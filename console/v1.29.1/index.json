[{"category":null,"content":"How to monitor JVM usage of Elasticsearch cluster nodes #  Introduction #  This article will introduce how to use the INFINI Console to monitor the JVM usage of Elasticsearch cluster nodes and generate alerts.\nPrepare #   Download and install the latest version of INFINI Console Register Elasticsearch cluster using INFINI Console  Create alerting rule #  Open INFINI Console in the browser, click on the left menu \u0026ldquo;Alerting\u0026rdquo; \u0026gt; Rules to enter the alerting management page, and then click New button to enter the Create Alerting Rule page. Follow these steps to create an alerting rule:\n Select the cluster (here you need to select the Elasticsearch cluster where the INFINI Console stores data, that is, the Elasticsearch cluster configured in the configuration file console.yml, if it is not registered to the INFINI Console, please register first) Input the alerting object .infini_metrics* (select the index under the Elasticsearch cluster, or enter the index pattern, because the monitoring data collected by the INFINI Console is stored in the index .infini_metrics) Input filter criteria (Elasticsearch query DSL) Here we need to filter the monitoring metrics category to node_stats and the metadata category to elasticsearch. The DSL is as follows:  { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;metadata.name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;node_stats\u0026quot; } } }, { \u0026quot;term\u0026quot;: { \u0026quot;metadata.category\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;elasticsearch\u0026quot; } } } ] } }  Select time field timestamp and statistical period for date histogram aggregation   Input the rule name Group settings (optional, multiple can be configured), set when statistical metrics need to be grouped, because all registered to INFINI Console The Elasticsearch cluster monitoring metrics are stored in the index .infini_metrics, so you need to group according to the cluster ID first, and then group according to the node ID, Here we choose metadata.labels.cluster_id and metadata.labels.node_id. Configure the alerting metrics, select the aggregation field payload.elasticsearch.node_stats.jvm.mem.heap_used_percent, and the statistics method p90. Configure the metrics formula (when more than one alerting metrics is configured, you need to set a formula to calculate the target metrics), where the formula fx is configured as a. Then set the value type of the variable a to the ratio Ratio. Configure the alerting conditions, configure three alerting conditions here, configure the P2(Medium) alerting when the JVM usage rate is greater than 50 for continuous one cycle; Configure continues one cycle when the JVM usage rate is greater than 90, trigger the P1(High) alerting; Configure the P0(Critical) alerting to be triggered when the JVM usage rate is greater than 95 for one cycle; Set the execution period, here is configured to execute a check every minute Set the event title, the event title is a template, you can use template variables, template syntax and template variable usage reference here Set the event content, the event content is a template, you can use template variables, template syntax and template variable usage reference here  Priority:{{.priority}} Timestamp:{{.timestamp | datetime_in_zone \u0026quot;Asia/Shanghai\u0026quot;}} RuleID:{{.rule_id}} EventID:{{.event_id}} {{range.results}} ClusterID:{{index.group_values ​​0}}; NodeID:{{index.group_values ​​1}}; JVM used percent: {{.result_value | to_fixed 2}}%; {{end}}  Turn on the configure alerting channel switch, and select add in the upper right corner to quickly select an alerting channel template to fill. For how to create an alerting channel template, please refer to here Set the silence period to 1 hour, that is, after the alerting rule is triggered, the notification message will only be sent once within an hour Set the receiving period, the default is 00:00-23:59, that is, you can receive notification messages throughout the day  After the settings are complete, click the Save button to submit.\nReceive alert notification message #  Wait for a while, and receive the DingTalk alerting message notification as follows:\nYou can see that the alert notification message displays the Elasticsearch cluster ID, node ID, and current JVM usage rate triggered by the current rule.\nView the alerting message center #  In addition to receiving external notification messages, the INFINI Console Alert Message Center also generates an alert message. Click menu Alerting \u0026gt; Alerting Center to enter\nSummary #  By using the INFINI Console alert function, you can easily monitor the JVM usage of Elasticsearch cluster nodes. After configuring alerting rule, Once any Elasticsearch node JVM usage exceeds a set threshold, an alert will be triggered and an alert message will be sent.\n","subcategory":null,"summary":"","tags":null,"title":"How to monitor JVM usage of Elasticsearch cluster nodes","url":"/console/v1.29.1/docs/tutorials/cluster_node_jvm_usage/"},{"category":null,"content":"How to monitor the CPU usage of Elasticsearch cluster nodes #  Introduction #  This article will introduce how to use the INFINI Console to monitor the disk usage of Elasticsearch cluster nodes and alert them.\nPrepare #   Download and install the latest version of INFINI Console Register Elasticsearch cluster using INFINI Console  Create alerting rule #  Open INFINI Console in the browser, click Alerting \u0026gt; Rules on the left menu to enter the alerting management page, and then click the New button to enter the alerting rule creation page. Follow these steps to create an alerting rule:\n Select the cluster (here you need to select the Elasticsearch cluster where the INFINI Console stores data, that is, the Elasticsearch cluster configured in the configuration file console.yml, if it is not registered to the INFINI Console, please register first) Input the alerting object .infini_metrics* (select the index under the Elasticsearch cluster, or enter the index pattern, because the monitoring data collected by the INFINI Console is stored in the index .infini_metrics) Input filter condition (Elasticsearch query DSL) Here we need to filter the monitoring metrics category to node_stats and the metadata category to elasticsearch. The DSL is as follows:  { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;metadata.name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;node_stats\u0026quot; } } }, { \u0026quot;term\u0026quot;: { \u0026quot;metadata.category\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;elasticsearch\u0026quot; } } } ] } }  Select time field timestamp and statistical period for date histogram aggregation   Input the rule name Group settings (optional, multiple can be configured), set when statistical metrics need to be grouped, because all registered to INFINI Console The Elasticsearch cluster monitoring metrics are stored in the index .infini_metrics, so you need to group according to the cluster ID first, and then group according to the node ID, Here we choose metadata.labels.cluster_id and metadata.labels.node_id. Configure the alerting metrics, select the aggregation field payload.elasticsearch.node_stats.process.cpu.percent, and the statistics method avg. Configure the metrics formula (when more than one alerting metrics is configured, you need to set a formula to calculate the target metrics), where the formula fx is configured as a. Then set the value type of the variable a to the ratio Ratio. Configure the alerting conditions, configure three alerting conditions here, and configure the P2(Medium) alerting when the CPU usage is greater than 80 for continuous one cycle; Configure the continue for one cycle when the CPU usage is greater than 90, trigger the P1(High) alerting; Configure the continuous period to trigger the P0(Critical) alerting when the CPU usage is greater than 95; Set the execution period, here is configured to execute a check every minute Set the event title, the event title is a template, you can use template variables, template syntax and template variable usage reference here Set the event content, the event content is a template, you can use template variables, template syntax and template variable usage reference here  Priority:{{.priority}} Timestamp:{{.timestamp | datetime_in_zone \u0026quot;Asia/Shanghai\u0026quot;}} RuleID:{{.rule_id}} EventID:{{.event_id}} {{range.results}} ClusterID:{{index.group_values ​​0}}; NodeID:{{index.group_values ​​1}}; CPU:{{.result_value | to_fixed 2}}%; {{end}}  Turn on the configure alerting channel switch, and select add in the upper right corner to quickly select an alerting channel template to fill. For how to create an alerting channel template, please refer to here Set the silence period to 1 hour, that is, after the alerting rule is triggered, the notification message will only be sent once within an hour Set the receiving period, the default is 00:00-23:59, that is, you can receive notification messages throughout the day  After the settings are complete, click the Save button to submit.\nReceive alert notification message #  Wait for a while, and receive the DingTalk alerting message notification as follows:\nYou can see that the alert notification message displays the Elasticsearch cluster ID, node ID, and current CPU usage triggered by the current rule.\nView the alerting message center #  In addition to receiving external notification messages, the INFINI Console Alert Message Center also generates an alert message. Click menu Alerting \u0026gt; Alerting Center to enter\nSummary #  By using the INFINI Console alerting function, you can easily monitor the CPU usage of Elasticsearch cluster nodes. After configuring the alerting rule, once the CPU usage of any Elasticsearch node exceeds the set threshold, an alert will be triggered and an alert message will be sent.\n","subcategory":null,"summary":"","tags":null,"title":"How to monitor the CPU usage of Elasticsearch cluster nodes","url":"/console/v1.29.1/docs/tutorials/cluster_node_cpu_usage/"},{"category":null,"content":"How to monitor slow query requests in Elasticsearch #  Introduction #  Many times, the Elasticsearch cluster will experience peak data writing or query traffic. At this time, the Elasticsearch cluster will be under a lot of pressure. Through the monitoring and alertinging of the delay of the Elasticsearch index query. This allows us to locate which indexes are the most stressed on the Elasticsearch cluster. This article will introduce how to use the INFINI Console alerting function to monitor the slow query request index in Elasticsearch.\nPrepare #   Download and install the latest version of INFINI Console Register Elasticsearch cluster using INFINI Console  Create alerting rule #  Open INFINI Console in the browser, click on the left menu \u0026ldquo;Alerting\u0026rdquo; \u0026gt; Rules to enter the alerting management page, and then click New button to enter the Create Alerting Rule page. Follow these steps to create an alerting rule:\n Select the cluster (here you need to select the Elasticsearch cluster where the INFINI Console stores data, that is, the Elasticsearch cluster configured in the configuration file console.yml, if it is not registered to the INFINI Console, please register first) Input the alerting object .infini_metrics* (select the index under the Elasticsearch cluster, or enter the index pattern, because the monitoring data collected by the INFINI Console is stored in the index .infini_metrics) Input filter criteria (Elasticsearch query DSL) Here we need to filter the monitoring metrics category to index_stats, and the index name cannot be _all, the DSL is as follows:  { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;metadata.name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;index_stats\u0026quot; } } }, { \u0026quot;term\u0026quot;: { \u0026quot;metadata.category\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;elasticsearch\u0026quot; } } } ], \u0026quot;must_not\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;metadata.labels.index_name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;_all\u0026quot; } } } ] } }  Select time field timestamp and statistical period for date histogram aggregation   Input the rule name Group settings (optional, multiple can be configured), set when statistical metrics need to be grouped, because all registered to INFINI Console The Elasticsearch cluster monitoring metrics are stored in the index .infini_metrics, so you need to group according to the cluster ID first, and then group according to the index name, Here we choose metadata.labels.cluster_id and metadata.labels.index_name Configure the alerting metrics, select the aggregation field payload.elasticsearch.index_stats.total.search.query_time_in_millis, and use the statistical method to derive derivative. Then add another alerting metrics, select the aggregation field payload.elasticsearch.index_stats.total.search.query_total, and the statistical method derivative. Configure the metrics formula (when more than one alerting metrics is configured, you need to set a formula to calculate the target metrics), where the formula fx is configured as a/b to calculate the delay, Configure the alerting conditions, three alerting conditions are configured here, and when the continue for one cycle delay is greater than 100, the P3(Low) alerting is triggered; Configure the Continue for one cycle delay when the delay is greater than 500, trigger the P1(High) alerting; Configure continue for one cycle when the delay is greater than 1000, trigger the P0(Critical) alerting; Set the execution period, here is configured to execute a check every minute Set the event title, the event title is a template, you can use template variables, template syntax and template variable usage reference here Set the event content, the event content is a template, you can use template variables, template syntax and template variable usage reference here  Priority:{{.priority}} Timestamp:{{.timestamp | datetime_in_zone \u0026quot;Asia/Shanghai\u0026quot;}} RuleID:{{.rule_id}} EventID:{{.event_id}} {{range.results}} ClusterID:{{index.group_values ​​0}}; Index name:{{index.group_values ​​1}}; Current value:{{.result_value | to_fixed 2}}ms; {{end}}  Turn on the configure alerting channel switch, and select add in the upper right corner to quickly select an alerting channel template to fill. For how to create an alerting channel template, please refer to here Set the silence period to 1 hour, that is, after the alerting rule is triggered, the notification message will only be sent once within an hour Set the receiving period, the default is 00:00-23:59, that is, you can receive notification messages throughout the day  After the settings are complete, click the Save button to submit.\nReceive alert notification message #  Wait for a while, and receive the DingTalk alerting message notification as follows:\nYou can see that the alerting notification message shows the Elasticsearch cluster ID, index name, and delay size of which query delay is too high.\nView the alerting message center #  In addition to receiving external notification messages, the INFINI Console Alert Message Center also generates an alert message. Click menu Alerting \u0026gt; Alerting Center to enter\nSummary #  By using the INFINI Console alerting function, you can easily monitor the slow index of the Elasticsearch cluster. After configuring alerting rule, Once any Elasticsearch index query latency is too high, an alert will be triggered and an alert message will be sent.\n","subcategory":null,"summary":"","tags":null,"title":"How to monitor slow query requests in Elasticsearch","url":"/console/v1.29.1/docs/tutorials/cluster_slow_request/"},{"category":null,"content":"How to monitor Elasticsearch cluster node disk usage #  Introduction #  When the system disk usage is too high, data cannot be written into the Elasticsearch cluster, which is likely to result in data loss. Therefore, monitor the Elasticsearch cluster. Node disk usage is necessary. This article will show you how to monitor your Elasticsearch cluster using INFINI Console alerts Node disk usage.\nPrepare #   Download and install the latest version of INFINI Console Register Elasticsearch cluster using INFINI Console  Create alerting rule #  Open INFINI Console in the browser, click on the left menu \u0026ldquo;Alerting\u0026rdquo; \u0026gt; Rules to enter the alerting management page, and then click New button to enter the Create Alerting Rule page. Follow these steps to create an alerting rule:\n Select the cluster (here you need to select the Elasticsearch cluster where the INFINI Console stores data, that is, the Elasticsearch cluster configured in the configuration file console.yml, if it is not registered to the INFINI Console, please register first) Input the alerting object .infini_metrics* (select the index under the Elasticsearch cluster, or enter the index pattern, because the monitoring data collected by the INFINI Console is stored in the index .infini_metrics) Input filter criteria (Elasticsearch query DSL) Here we need to filter the monitoring metrics category to node_stats, the DSL is as follows:  { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;metadata.name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;node_stats\u0026quot; } } } ] } }  Select time field timestamp and statistical period for date histogram aggregation   Input the rule name Group settings (optional, multiple can be configured), set when statistical metrics need to be grouped, because all registered to INFINI Console The Elasticsearch cluster monitoring metrics are stored in the index .infini_metrics, so you need to group according to the cluster ID first, and then group according to the node ID, Here we choose metadata.labels.cluster_id and metadata.labels.node_id Configure the alerting metrics, select the aggregation field payload.elasticsearch.node_stats.fs.total.free_in_bytes, and the statistics method avg. Then add another alerting metrics, select the aggregation field payload.elasticsearch.node_stats.fs.total.total_in_bytes, and the statistical method avg. Configure the metrics formula (when more than one alerting metrics is configured, you need to set a formula to calculate the target metrics), where the formula fx is configured as ((b-a)/b)*100, which means to use the total Disk space subtract remaining disk space to get disk used space, Then divide disk used space by total disk space and multiply by 100 to get disk usage Configure the alerting conditions, here configure three alerting conditions, configure the P2(Medium) alerting when the disk usage is greater than 80 for persisting for one period; Configure the continue for one period when the disk usage is greater than 90, trigger the P1(High) alerting; Configure persisting for a period when the disk usage rate is greater than 95, trigger the P0(Critical) alerting; Set the execution period, here is configured to execute a check every minute Set the event title, the event title is a template, you can use template variables, template syntax and template variable usage reference here Set the event content, the event content is a template, you can use template variables, template syntax and template variable usage reference here  Priority:{{.priority}} Timestamp:{{.timestamp | datetime}} RuleID:{{.rule_id}} EventID:{{.event_id}} {{range.results}} ClusterID: {{index.group_values ​​0}}; NodeID: {{index.group_values ​​1}}; Disk Usage:{{.result_value | to_fixed 2}}%; Free Storage: {{.relation_values.a | format_bytes 2}}; {{end}}  Turn on the configure alerting channel switch, and select add in the upper right corner to quickly select an alerting channel template to fill. For how to create an alerting channel template, please refer to here Set the silence period to 1 hour, that is, after the alerting rule is triggered, the notification message will only be sent once within an hour Set the receiving period, the default is 00:00-23:59, that is, you can receive notification messages throughout the day  After the settings are complete, click the Save button to submit.\nReceive alert notification message #  Wait for a while, and receive the DingTalk alerting message notification as follows:\nYou can see that the alert notification message displays the Elasticsearch cluster ID, node ID, and remaining disk space with high disk usage.\nView the alerting message center #  In addition to receiving external notification messages, the INFINI Console Alert Message Center also generates an alert message. Click menu Alerting \u0026gt; Alerting Center to enter\nSummary #  By using the INFINI Console alerting function, you can easily monitor the disk usage of Elasticsearch cluster nodes. After configuring alerting rule, Once the disk usage of any Elasticsearch node exceeds the set threshold, an alert will be triggered and an alert message will be sent.\n","subcategory":null,"summary":"","tags":null,"title":"How to monitor Elasticsearch cluster node disk usage","url":"/console/v1.29.1/docs/tutorials/cluster_node_disk_usage/"},{"category":null,"content":"How to Monitor Elasticsearch Cluster Health #  Introduction #  In many cases, the cluster health status of the Elasticsearch cluster will turn red for some reason. At this time, at least one primary shard in the Elasticsearch cluster is unallocated or lost. So it is necessary to monitor the health status of the Elasticsearch cluster. This article will introduce how to use the INFINI Console alerting feature to monitor the health of an Elasticsearch cluster.\nPrepare #   Download and install the latest version of INFINI Console Register Elasticsearch cluster using INFINI Console  Create alerting rule #  Open INFINI Console in the browser, click on the left menu \u0026ldquo;Alerting\u0026rdquo; \u0026gt; Rules to enter the alerting management page, and then click New button to enter the Create Alerting Rule page. Follow these steps to create an alerting rule:\n Select the cluster (here you need to select the Elasticsearch cluster where the INFINI Console stores data, that is, the Elasticsearch cluster configured in the configuration file console.yml, if it is not registered to the INFINI Console, please register first) Select the alerting object .infini_metrics (select the index under the Elasticsearch cluster, or enter the index pattern, because the monitoring data collected by the INFINI Console is stored in the index .infini_metrics) Input filter condition (Elasticsearch query DSL) Here we need to filter the data whose monitoring metrics category is cluster_health and the health status is red. The DSL is as follows:  { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;match\u0026quot;: { \u0026quot;payload.elasticsearch.cluster_health.status\u0026quot;: \u0026quot;red\u0026quot; } }, { \u0026quot;term\u0026quot;: { \u0026quot;metadata.name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;cluster_health\u0026quot; } } } ] } }  Select time field and stat period for date histogram aggregation   Input the rule name Group settings (optional, multiple can be configured), set when statistical metrics need to be grouped, because all registered to INFINI Console The Elasticsearch cluster monitoring metrics are stored in the index .infini_metrics, so they need to be grouped according to the cluster ID, Here we choose metadata.labels.cluster_id Configure the alerting metrics, select the aggregation field payload.elasticsearch.cluster_health.status, and the statistical method count Configure the alerting condition, configure the continue for one period and the aggregation result is greater than or equal to 1, that is, the Critical alerting is triggered Set the execution period, here is configured to execute a check every minute Set the event title, the event title is a template, you can use template variables, template syntax and template variable usage reference here Set the event content, the event content is a template, you can use template variables, template syntax and template variable usage reference here   Turn on the configure alerting channel switch, and select add in the upper right corner to quickly select an alerting channel template to fill. For how to create an alerting channel template, please refer to here Set the silence period to 1 hour, that is, after the alerting rule is triggered, the notification message will only be sent once within an hour Set the receiving period, the default is 00:00-23:59, that is, you can receive notification messages throughout the day  After the settings are complete, click the Save button to submit.\nSimulate trigger alerting rule #  Open the INFINI Console Dev tools (Ctrl+Shift+O) and enter the command as shown below:\nReceive alert notification message #  After waiting for about a minute, you will receive a DingTalk alerting notification as follows:\nYou can see that the alerting notification message shows the ID of the Elasticsearch cluster whose health status has turned red. Click the link below the message to view the alerting details as follows:\nView the alerting message center #  In addition to receiving external notification messages, the INFINI Console Alert Message Center also generates an alert message. Click menu Alerting \u0026gt; Alerting Center to enter\nSummary #  By using the INFINI Console alerting function, you can easily monitor the health status of the Elasticsearch cluster. After configuring alerting rule,As soon as any Elasticsearch cluster status turns red, an alert is triggered and an alert message is sent.\n","subcategory":null,"summary":"","tags":null,"title":"How to Monitor Elasticsearch Cluster Health","url":"/console/v1.29.1/docs/tutorials/cluster_health_change/"},{"category":null,"content":"Kubernetes Deployment #  INFINI Console supports deployment on Kubernetes using Helm charts.\nThe Chart Repository #  Chart repository: https://helm.infinilabs.com.\nUse the follow command add the repository:\nhelm repo add infinilabs https://helm.infinilabs.com Prerequisites #   K8S StorageClass  The default StorageClass of the Chart package is local-path, you can install it through here.\nIf you want use other StorageClass(installed), you can create a YAML file (eg. vaules.yaml) file that it contains the follow contents:\nstorageClassName: \\\u0026lt;storageClassName\\\u0026gt; and use it through -f.\nInstall #  helm install console infinilabs/console -n \u0026lt;namespace\u0026gt; Uninstall #  helm uninstall console -n \u0026lt;namespace\u0026gt; kubectl delete pvc console-data-console-0 console-config-console-0 -n \u0026lt;namespace\u0026gt; ","subcategory":null,"summary":"","tags":null,"title":"Kubernetes","url":"/console/v1.29.1/docs/getting-started/helm/"},{"category":null,"content":"User Management #  Introduction #  User Management includes CURD operations and reset password for user.\nCreate User #   User name is required, and it should be unique. Nick name, phone, email, is optional. Select one or more role. Tags is optional, it helps you group users.  Search User #  Input keywords and click the search button to query users.\nUpdate User #  Modify as needed, and then click the Save button to submit.\nReset User password #  Input the new password and then click the save button to reset the password.\n","subcategory":null,"summary":"","tags":null,"title":"User","url":"/console/v1.29.1/docs/reference/system/security/user/"},{"category":null,"content":"Role Management #  Introduction #  Role Management includes CURD operations for role. And INFINI Console has a builtin role named Administrator, it has all privileges , includes Platform and Data. The data role can help us control privileges of elasticsearch, includes elasticsearch api privileges which can be configured in the file config/permission.json of your setup path.\nCreate Platform Role #   Input role name, role name should be unique. Select feature privileges, can not be empty. Input a description if needed  All privilege represents both read and write permission, Read privilege represents only read permission, and None privilege represents no permission\nCreate Data Role #   Input role name, role name should be unique. Select one or more cluster, * represents all clusters. Config cluster api privileges, * represents all privileges. Config index api privileges, * represents all privileges. Input a description if needed  Search Role #  Input a keyword and click the search button to query roles.\nUpdate Platform Role #  Modify the role as needed, and then click the Save button to submit.\nUpdate Data Role #  Modify the role as needed, and then click the Save button to submit.\n","subcategory":null,"summary":"","tags":null,"title":"Role","url":"/console/v1.29.1/docs/reference/system/security/role/"},{"category":null,"content":"Container Deployment #  INFINI Console supports container deployment.\nDownloading an Image #  The images of INFINI Console are published at the official repository of Docker. The URL is as follows:\n https://hub.docker.com/r/infinilabs/console\nUse the following command to obtain the latest container image:\ndocker pull infinilabs/console:1.29.1-2000 Verifying the Image #  After downloading the image locally, you will notice that the container image of INFINI Console is very small, with a size less than 30 MB. So, the downloading is very fast.\n✗ docker images |grep \u0026quot;console\u0026quot; |grep \u0026quot;1.29.1-2000\u0026quot; REPOSITORY TAG IMAGE ID CREATED SIZE infinilabs/console 1.29.1-2000 8c27cd334e4c 47 minutes ago 26.4MB Starting the Console #  Use the following command to start the INFINI Console container:\ndocker run -p 9000:9000 infinilabs/console:1.29.1-2000 Docker Compose #  You can also use docker compose to manage container instances. Create one docker-compose.yml file as follows:\nversion: \u0026quot;3.5\u0026quot; services: infini-console: image: infinilabs/console:1.29.1-2000 ports: - 9000:9000 container_name: \u0026quot;infini-console\u0026quot; In the directory where the configuration file resides, run the following command to start INFINI Console.\n➜ docker-compose up   ","subcategory":null,"summary":"","tags":null,"title":"Docker","url":"/console/v1.29.1/docs/getting-started/docker/"},{"category":null,"content":"Container Deployment #  INFINI Agent supports container deployment.\nDownload Image #  The images of INFINI Agent are published at the official repository of Docker. The URL is as follows: https://hub.docker.com/r/infinilabs/agent\nRun the following command:\ndocker pull infinilabs/agent:1.29.1-2000 Verifying the Image #  After downloading the image locally, you will notice that the container image of INFINI Agent is very small, with a size less than 25 MB. So, the downloading is very fast.\n✗ docker images |grep \u0026quot;agent\u0026quot; |grep \u0026quot;1.29.1-2000\u0026quot; REPOSITORY TAG IMAGE ID CREATED SIZE infinilabs/agent 1.29.1-2000 c7bd9ad063d9 4 days ago 13.8MB Configuration #  Create a configuration file agent.yml to perform basic configuration as follows:\napi: enabled: true network: binding: 0.0.0.0:8080 metrics: enabled: true queue: metrics network: enabled: true summary: true details: true memory: metrics: - swap - memory disk: metrics: - ioqs - usage cpu: metrics: - idle - system - user - iowait - load elasticsearch: enabled: true agent_mode: true node_stats: true index_stats: true cluster_stats: true\nelasticsearch:\n name: default enabled: true endpoint: http://192.168.3.4:9200 monitored: false discovery: enabled: true  pipeline:\n name: metrics_ingest auto_start: true keep_running: true processor:  json_indexing: index_name: \u0026quot;.infini_metrics\u0026quot; elasticsearch: \u0026quot;default\u0026quot; input_queue: \u0026quot;metrics\u0026quot; output_queue: name: \u0026quot;metrics_requests\u0026quot; label: tag: \u0026quot;metrics\u0026quot; worker_size: 1 bulk_size_in_mb: 10   name: consume-metrics_requests auto_start: true keep_running: true processor:  bulk_indexing: bulk: compress: true batch_size_in_mb: 10 batch_size_in_docs: 5000 consumer: fetch_max_messages: 100 queues: type: indexing_merge when: cluster_available: [ \u0026quot;default\u0026quot; ]    agent: major_ip_pattern: \u0026quot;192.*\u0026quot; labels: env: dev tags: - linux - x86 - es7 - v7.5\npath.data: data path.logs: log\nagent.manager.endpoint: http://192.168.3.4:9000 Note: In the above configuration, replace the Elasticsearch configuration with the actual server connection address and authentication information.\nStarting #  Run the following command:\ndocker run -p 8080:8080 -v=`pwd`/agent.yml:/agent.yml infinilabs/agent:1.29.1-2000 Docker Compose #  You can also use docker compose to manage container instances. Create one docker-compose.yml file as follows:\nversion: \u0026quot;3.5\u0026quot; services: infini-agent: image: infinilabs/agent:1.29.1-2000 ports: - 8080:8080 container_name: \u0026quot;infini-agent\u0026quot; volumes: - ./agent.yml:/agent.yml\nvolumes: dist: Run the following command to start INFINI Agent.\n➜ docker-compose up Recreating infini-agent ... done Attaching to infini-agent infini-agent | _ ___ __ __ _____ infini-agent | /_\\ / _ \\ /__\\/\\ \\ \\/__ \\ infini-agent | //_\\\\ / /_\\//_\\ / \\/ / / /\\/ infini-agent | / _ \\/ /_\\\\//__/ /\\ / / / infini-agent | \\_/ \\_/\\____/\\__/\\_\\ \\/ \\/ infini-agent | infini-agent | [AGENT] A light-weight, powerful and high-performance elasticsearch agent. infini-agent | [AGENT] 0.1.0_SNAPSHOT#15, 2022-08-26 15:05:43, 2025-12-31 10:10:10, 164bd8a0d74cfd0ba5607352e125d72b46a1079e infini-agent | [08-31 09:11:45] [INF] [app.go:164] initializing agent. infini-agent | [08-31 09:11:45] [INF] [app.go:165] using config: /agent.yml. infini-agent | [08-31 09:11:45] [INF] [instance.go:72] workspace: /data/agent/nodes/cc7ibke5epac7314bf9g infini-agent | [08-31 09:11:45] [INF] [metrics.go:63] ip:172.18.0.2, host:bd9f43490911, labels:, tags: infini-agent | [08-31 09:11:45] [INF] [api.go:261] api listen at: http://0.0.0.0:8080 infini-agent | [08-31 09:11:45] [INF] [actions.go:367] elasticsearch [default] is available infini-agent | [08-31 09:11:45] [INF] [module.go:116] all modules are started infini-agent | [08-31 09:11:45] [INF] [manage.go:180] register agent to console infini-agent | [08-31 09:11:45] [INF] [app.go:334] agent is up and running now.   ","subcategory":null,"summary":"","tags":null,"title":"Container Deployment","url":"/console/v1.29.1/docs/reference/agent/docker/"},{"category":null,"content":"How to assign Elasticsearch index-level permissions to INFINI Console accounts #  Introduction #  This article will introduce the use of INFINI Console to limit an account to only have the management permissions of certain indexes in the Elasticsearch cluster\nPrepare #   Download and install the latest version of INFINI Console Enable INFINI Console Security Features Register at least two Elasticsearch clusters to the INFINI Console  Creating a Role #  Click System \u0026gt; Security Settings on the left menu of INFINI Console, and select the Role Tab page to enter the role management page.\nNew platform role platform_role #  Click the New button, select the platform role, and create a new platform role platform_role\nNew data role test_index_only #  Click the New button, select the data role, create a new data role test_index_only, and then configure the following:\n Select only es-v7140 for the cluster (restrict access to this role only to the Elasticsearch cluster es-v7140) Set index permissions to index only enter the index pattern test* (restrict the role to only index access permissions whose index name matches test*)  After the configuration is complete, click the Save button to submit.\nCreate Account #  Click the left menu of INFINI Console System \u0026gt; Security Settings, select the User Tab page to enter the Account Management page.\nNew account liming #  Click the New button to create a new account liming and assign the account roles platform_role, test_index_only\nClick the save button to submit after the creation is successful, save the account password\nLogin with administrator account #  After logging in with the administrator account, click the menu Data \u0026gt; Index Management, select the cluster es-v7140, and you can see:\nLogin with account liming #  After logging in with the account liming, click the menu Data \u0026gt; Index Management, select the cluster es-v7140, and then you can see:\nSummary #  By specifying the role\u0026rsquo;s Elasticsearch cluster permissions and indexing permissions, it is easy to precisely control user permissions down to the indexing level.\n","subcategory":null,"summary":"","tags":null,"title":"How to assign Elasticsearch index-level permissions to INFINI Console accounts","url":"/console/v1.29.1/docs/tutorials/role_with_index_limit/"},{"category":null,"content":"How to assign different Elasticsearch cluster access permissions to different INFINI Console accounts #  Introduction #  This article will introduce the use of INFINI Console to assign two different Elasticsearch cluster management permissions to two different accounts\nPrepare #   Download and install the latest version of INFINI Console Enable INFINI Console Security Features Register at least two Elasticsearch clusters to the INFINI Console  Creating a Role #  Click System \u0026gt; Security Settings on the left menu of INFINI Console, and select the Role Tab page to enter the role management page.\nNew platform role platform_role #  Click the New button, select the platform role, and create a new platform role platform_role. The operation steps are as follows:\n Input role name platform_role Expand all functional permissions Except for the security functions under the system settings, select the All permission for all other functions. Security feature under System Settings is set to None permission. Click the save button to submit  Selecting the All permission of a function represents the read and write operation permission of this function, Read means only have read permission, None means no permission for this function (the function is not available in the menu after the user logs in)\n   New data role es-v7171 #  Click the New button, select the data role, and create a new data role es-v7171\nNew data role es-v630 #  Click the New button, select the data role, create a new data role es-v630, the configuration is similar to the role es-v7171\nCreate Account #  Click the left menu of INFINI Console System \u0026gt; Security Settings, select the User Tab page to enter the Account Management page.\nNew account zhangsan #  Click the New button to create a new account zhangsan and assign the account role platform_role, es-v717\nClick the save button to submit after the creation is successful, save the account password\nNew account wangwu #  Click the New button to create a new account wangwu, and assign the account roles platform_role, es-v630, the configuration is similar to the account zhangsan\nLogin with administrator account #  After logging in with the administrator account, view the platform overview, and you can see all 13 registered clusters\nLogin with account zhangsan #  After logging in with the account zhangsan and viewing the platform overview, you can only see the cluster es-v7171\nLogin with account wangwu #  After logging in with the account zhangsan and viewing the platform overview, you can only see the cluster es-v630\nSummary #  By creating different roles and granting different Elasticsearch cluster permissions, and then assigning roles to users, we can quickly implement Grant different Elasticsearch cluster permissions to different users.\n","subcategory":null,"summary":"","tags":null,"title":"How to assign different Elasticsearch cluster access permissions to different INFINI Console accounts","url":"/console/v1.29.1/docs/tutorials/role_with_different_rights/"},{"category":null,"content":"How to easily create an Elasticsearch \u0026ldquo;guest\u0026rdquo; user #  Introduction #  In some cases, we want to share some functions or data with customers, but do not want the data to be modified. At this point we need to create a \u0026ldquo;guest\u0026rdquo; user. This article briefly describes how to create a \u0026ldquo;guest\u0026rdquo; user using the INFINI Console.\nPrepare #   Download and install the latest version of INFINI Console Enable INFINI Console Security Features  Creating a Role #  Click System \u0026gt; Security Settings on the left menu of INFINI Console, and select the Role Tab page to enter the role management page.\nNew platform role readonly #  Click the New button, select the platform role, and create a new platform role readonly. The operation steps are as follows:\n Input role name readonly Expand all functional permissions Read permission is selected for all other functions except the security functions under the system settings. Security feature under System Settings is set to None permission. Click the save button to submit  Selecting the All permission of a function represents the read and write operation permission of this function, Read means only have read permission, None means no permission for this function (the function is not available in the menu after the user logs in)\n   New data role es-v7171 #  Click the New button, select the data role, and create a new data role es-v7171. The operation steps are as follows:\n Input role name es-v7171 Cluster permission select cluster es-v7171 Click the save button to submit  New account guest #  Click the left menu of INFINI Console System \u0026gt; Security Settings, select the User Tab page to enter the Account Management page. Click the New button to create a new account guest and assign the account role readonly, es-v7171\nClick Save and submit. After the creation is successful, you can use the guest account to log in to the INFINI Console and only have read-only permissions.\n","subcategory":null,"summary":"","tags":null,"title":"How to easily create an Elasticsearch \"guest\" user","url":"/console/v1.29.1/docs/tutorials/create_readonly_account/"},{"category":null,"content":"Installing The Agent #  Before You Begin #  Install and keep INFINI Console running.\nInstall by Console generated script #  curl -sSL http://localhost:9000/agent/install.sh?token=cjctdrms4us1c6fu04ag |sudo bash -s -- -u https://release.infinilabs.com/agent/stable -v 0.6.0-262 -t /opt/agent  The -u and -v parameters indicate that the specified version of the Agent is downloaded from the specified URL, and the -t parameter indicates the installation path. In a networked environment, the \u0026ndash; and subsequent parameters can be ignored, and by default, the latest version of the Agent will be downloaded from the official website for installation.\n Container Deployment #  INFINI Agent also supports Docker container deployment.\nLearn More  Configuration #  Most of the configuration of INFINI Agent can be completed using agent.yml. After the configuration is modified, the agent program needs to be restarted to make the configuration take effect.\nAfter unzip the file and open agent.yml, you will see this:\nenv: LOGGING_ES_ENDPOINT: http://localhost:9200 LOGGING_ES_USER: admin LOGGING_ES_PASS: admin API_BINDING: \u0026quot;0.0.0.0:2900\u0026quot; path.data: data path.logs: log\napi: enabled: true network: binding: $[[env.API_BINDING]]\nomitted \u0026hellip; agent.manager.endpoint: http://192.168.3.4:9000 In most cases, you only need to config the LOGGING_ES_ENDPOINT, but if Elasticsearch has security authentication enabled, then configure the LOGGING_ES_USER and LOGGING_ES_PASS.\nThe user must have access to the cluster metadata, index metadata, and all indexes with .infini prefix.\nStarting the Agent #  Run the agent program to start INFINI Agent, as follows:\n _ ___ __ __ _____ /_\\ / _ \\ /__\\/\\ \\ \\/__ \\ //_\\\\ / /_\\//_\\ / \\/ / / /\\/ / _ \\/ /_\\\\//__/ /\\ / / / \\_/ \\_/\\____/\\__/\\_\\ \\/ \\/ [AGENT] A light-weight, powerful and high-performance elasticsearch agent. [AGENT] 0.1.0#14, 2022-08-26 14:09:29, 2025-12-31 10:10:10, 4489a8dff2b68501a0dd9ae15276cf5751d50e19 [08-31 15:52:07] [INF] [app.go:164] initializing agent. [08-31 15:52:07] [INF] [app.go:165] using config: /Users/INFINI/agent/agent-0.1.0-14-mac-arm64/agent.yml. [08-31 15:52:07] [INF] [instance.go:72] workspace: /Users/INFINI/agent/agent-0.1.0-14-mac-arm64/data/agent/nodes/cc7h5qitoaj25p2g9t20 [08-31 15:52:07] [INF] [metrics.go:63] ip:192.168.3.22, host:INFINI-MacBook.local, labels:, tags: [08-31 15:52:07] [INF] [api.go:261] api listen at: http://0.0.0.0:8080 [08-31 15:52:07] [INF] [module.go:116] all modules are started [08-31 15:52:07] [INF] [manage.go:180] register agent to console [08-31 15:52:07] [INF] [actions.go:367] elasticsearch [default] is available [08-31 15:52:07] [INF] [manage.go:203] registering, waiting for review [08-31 15:52:07] [INF] [app.go:334] agent is up and running now. If the above startup information is displayed, the agent is running successfully and listening on the responding port.\nBut now agent can\u0026rsquo;t work normally util it\u0026rsquo;s being added to INFINI Console. See Agent Manage\nShutting Down the Agent #  To shut down INFINI Agent, hold down Ctrl+C. The following information will be displayed:\n^C [AGENT] got signal: interrupt, start shutting down [08-31 15:57:13] [INF] [module.go:145] all modules are stopped [08-31 15:57:13] [INF] [app.go:257] agent now terminated. [AGENT] 0.1.0, uptime: 5m6.240314s  / // |/ // __// // |/ // / / // || // / / // || // / ////|/// ////|//_/\n©INFINI.LTD, All Rights Reserved. System Service #\n To run the INFINI Agent as a system service, run the following commands:\n➜ ./agent -service install Success ➜ ./agent -service start Success Uninstall service:\n➜ ./agent -service stop Success ➜ ./agent -service uninstall Success Manual Configuration #  If you want to manually configure the INFINI Agent to collect Elasticsearch logs and metrics, you can refer to the agent.yml. If you want to collect metrics and logs for other Elasticsearch clusters, you need to add the corresponding configuraiton under elasticsearch and pipeline configuration.\nIf you want to toggle off some metrics/logs collecting, set the corresponding pipeline.enabled to `false.\nCollect Elasticsearch Metrics #  Collect node stats:\n - name: collect_default_node_stats enabled: false auto_start: true keep_running: true retry_delay_in_ms: 10000 processor: - es_node_stats: elasticsearch: default Collect index stats:\n - name: collect_default_index_stats enabled: false auto_start: true keep_running: true retry_delay_in_ms: 10000 processor: - es_index_stats: elasticsearch: default Collect cluster stats:\n - name: collect_default_cluster_stats enabled: false auto_start: true keep_running: true retry_delay_in_ms: 10000 processor: - es_cluster_stats: elasticsearch: default Collect cluster health info:\n - name: collect_default_cluster_health enabled: false auto_start: true keep_running: true retry_delay_in_ms: 10000 processor: - es_cluster_health: elasticsearch: default Collect Elasticsearch Logs #  Collect the logs from the specified nodes, set the endpoint to the specified node in the elasticsearch configuration:\n - name: collect_default_es_logs enabled: false auto_start: true keep_running: true retry_delay_in_ms: 3000 processor: - es_logs_processor: queue_name: \u0026quot;logs\u0026quot; elasticsearch: default If you have multiple nodes running on the local host, add more elasticsearch and pipeline configurations:\nelasticsearch: # omitted ... - name: cluster-a-node-1 enabled: true endpoint: http://localhost:9202 monitored: false discovery: enabled: true omitted \u0026hellip;  name: collect_node_1_es_logs enabled: false auto_start: true keep_running: true retry_delay_in_ms: 3000 processor:  es_logs_processor: queue_name: \u0026quot;logs\u0026quot; elasticsearch: cluster-a-node-1 Collect Other Logs #     If es_logs_processor can\u0026rsquo;t provide the flexibility you need, or you want to collect other services' logs on the local host, you can use logs_processor to collect them. There\u0026rsquo;s a sample configuration to collect Elasticsearch logs in the agent.yml, you can modify it or add new configurations, and update the metadata and labels for better investigations later.\n - name: log_collect enabled: false auto_start: true keep_running: true retry_delay_in_ms: 3000 processor: - logs_processor: queue_name: \u0026quot;logs\u0026quot; logs_path: \u0026quot;/opt/es/elasticsearch-7.7.1/logs\u0026quot; # metadata for all log items metadata: category: elasticsearch # patterns are matched in order patterns: - pattern: \u0026quot;.*_server.json$\u0026quot; # file name pattern to match # log type, json/text/multiline type: json # metadata for matched files metadata: name: server # (json) timestamp fields in json message, match the first one timestamp_fields: [\u0026quot;timestamp\u0026quot;, \u0026quot;@timestamp\u0026quot;] # (json) remove fields with specified key path remove_fields: [ \u0026quot;type\u0026quot;, \u0026quot;cluster.name\u0026quot;, \u0026quot;cluster.uuid\u0026quot;, \u0026quot;node.name\u0026quot;, \u0026quot;node.id\u0026quot;, \u0026quot;timestamp\u0026quot;, \u0026quot;@timestamp\u0026quot;, ] - pattern: \u0026quot;gc.log$\u0026quot; # file name pattern to match # log type, json/text/multiline type: json # metadata for matched files metadata: name: gc # (text) regex to match timestamp in the log entries timestamp_patterns: - \u0026quot;\\\\d{4}-\\\\d{1,2}-\\\\d{1,2}T\\\\d{1,2}:\\\\d{1,2}:\\\\d{1,2}.\\\\d{3}\\\\+\\\\d{4}\u0026quot; - \u0026quot;\\\\d{4}-\\\\d{1,2}-\\\\d{1,2} \\\\d{1,2}:\\\\d{1,2}:\\\\d{1,2},\\\\d{3}\u0026quot; - \u0026quot;\\\\d{4}-\\\\d{1,2}-\\\\d{1,2}T\\\\d{1,2}:\\\\d{1,2}:\\\\d{1,2},\\\\d{3}\u0026quot; - pattern: \u0026quot;.*.log$\u0026quot; # file name pattern to match # log type, json/text/multiline type: multiline # (multiline) the pattern to match a new line line_pattern: '^\\[' # metadata for matched files metadata: name: server # (text) regex to match timestamp in the log entries timestamp_patterns: - \u0026quot;\\\\d{4}-\\\\d{1,2}-\\\\d{1,2}T\\\\d{1,2}:\\\\d{1,2}:\\\\d{1,2}.\\\\d{3}\\\\+\\\\d{4}\u0026quot; - \u0026quot;\\\\d{4}-\\\\d{1,2}-\\\\d{1,2} \\\\d{1,2}:\\\\d{1,2}:\\\\d{1,2},\\\\d{3}\u0026quot; - \u0026quot;\\\\d{4}-\\\\d{1,2}-\\\\d{1,2}T\\\\d{1,2}:\\\\d{1,2}:\\\\d{1,2},\\\\d{3}\u0026quot; ","subcategory":null,"summary":"","tags":null,"title":"Installing Agent","url":"/console/v1.29.1/docs/reference/agent/install/"},{"category":null,"content":"Agent Overview #  Overview #  INFINI Agent is a submodule of INFINI Console, charge of data scraping and Elasticsearch instance manage. it\u0026rsquo;s manage by INFINI Console. INFINI Agent supports mainstream operating systems and platforms. The program package is small, with no extra external dependency. So, the agent can be installed very rapidly.\nHighlights of Agent:\n Collect cluster health/cluster stats/node stats/index stats from Elasticsearch Collect local JSON and text logs from Elasticsearch. Collect host metrics. Upload all metrics and logs to Elasticsearch.  Add Agent #  Agent \u0026gt; Instance click Discover Agent。 Select the correct Agent and click Add Agents.\nDelete Agent #  Agent \u0026gt; Instance，click Delete, delete after confirmation。\nTask Settings #  Agent \u0026gt; Instance，click Task Setting，configure the task, then click Save\n","subcategory":null,"summary":"","tags":null,"title":"Overview","url":"/console/v1.29.1/docs/reference/agent/manage/manage/"},{"category":null,"content":"Installing the Console #  INFINI Console supports mainstream operating systems and platforms, the package is small, without any additional external dependencies, it should be very fast to install :)\nPreparation before Installation #  Prepare an Elasticsearch cluster that can store data. The required version is 5.3 or above, which is used for INFINI Console to store related data.\nInstallation Demo #    autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  Downloading #  Automatic install\ncurl -sSL http://get.infini.cloud | bash -s -- -p console  The above script can automatically download the latest version of the corresponding platform\u0026rsquo;s console and extract it to /opt/console\n  The optional parameters for the script are as follows:\n   -v [version number]（Default to use the latest version number）\n   -d [installation directory] (default installation to /opt/console)\n Manual install\nSelect a package for downloading in the following URL based on your operating system and platform:\n https://release.infinilabs.com/console/\nContainer Deployment #  INFINI Console also supports Docker container deployment.\nLearn More  Starting the Console #  The Console can be started by directly running the program (the mac version is used here, and the program file names of different platforms are slightly different), as follows:\n➜ ./console-mac-amd64 ___ __ ___ ___ / __\\/ / /___\\/\\ /\\ / \\ / / / / // // / \\ \\/ /\\ / / /__/ /__/ \\_//\\ \\_/ / /_// \\____|____|___/ \\___/___,' ___ ___ __ __ ___ __ __ / __\\/___\\/\\ \\ \\/ _\\ /___\\/ / /__\\ / / // // \\/ /\\ \\ // // / /_\\ / /__/ \\_// /\\ / _\\ \\/ \\_// /__//__ \\____|___/\\_\\ \\/ \\__/\\___/\\____|__/ [CONSOLE] INFINI Cloud Console, The easiest way to operate your own elasticsearch platform. [CONSOLE] 0.3.0_SNAPSHOT, 2022-03-31 10:26:41, 2023-12-31 10:10:10, fa04f6010144b7c5267c71ccaee30230ddf2432d [03-31 20:27:40] [INF] [app.go:174] initializing console. [03-31 20:27:40] [INF] [app.go:175] using config: /console-0.3.0_SNAPSHOT-447-mac-amd64/console.yml. [03-31 20:27:40] [INF] [instance.go:72] workspace: /console-0.3.0_SNAPSHOT-447-mac-amd64/data/console/nodes/c92psf1pdamk8rdhgqpg [03-31 20:27:40] [INF] [app.go:283] console is up and running now. [03-31 20:27:40] [INF] [elastic.go:136] loading [5] remote elasticsearch configs [03-31 20:27:40] [INF] [ui.go:197] ui listen at: http://0.0.0.0:9000 [03-31 20:27:40] [INF] [module.go:116] all modules are started Seeing the above startup information, it means that the Console has successfully run and listen on port 9000.\nShutting Down the Console #  To shut down INFINI Console, hold down Ctrl+C. The following information will be displayed:\n^C [CONSOLE] got signal: interrupt, start shutting down [03-31 20:33:10] [INF] [module.go:145] all modules are stopped [03-31 20:33:10] [INF] [app.go:267] console now terminated. [CONSOLE] 0.3.0_SNAPSHOT, uptime: 5m30.307832s  / // |/ // __// // |/ // / / // || // / / // || // / ////|/// ////|//_/\n©INFINI.LTD, All Rights Reserved. System Service #\n To run the Console as a background task, run the following commands:\n➜ ./console -service install Success ➜ ./console -service start Success Unloading the service is simple. To unload the service, run the following commands:\n➜ ./console -service stop Success ➜ ./console -service uninstall Success ","subcategory":null,"summary":"","tags":null,"title":"Installation","url":"/console/v1.29.1/docs/getting-started/install/"},{"category":null,"content":"Template variables #  Introduction #  When custom alerting triggers event content, in addition to the fixed copy written by yourself, the event title and event content also support template syntax. The rendering of the text can be achieved using fields in the event.\nVariables #  The syntax for rendering fields is {{ .fieldname }}, and the variable fields that can be used for template content rendering are as follows:\n   Field Name Type Descriction eg     rule_id string rule uuid c9f663tath2e5a0vksjg   rule_name string rule name High CPU usage   resource_id string resource uuid c9f663tath2e5a0vksjg   resource_name string resource name es-v716   event_id string identifier for check details c9f663tath2e5a0vksjx   timestamp number Millisecond timestamp 1654595042399   first_group_value string The first value of group_values in results c9aikmhpdamkiurn1vq0   first_threshold string The first value of threshold in results 90   priority string The highest priority in results critical   title string event title Node ({{.first_group_value}}) disk used \u0026gt;= 90%   message string event content EventID：{{.event_id}}; Cluster：{{.resource_name}}   results array result of groups    ┗ threshold array  [\u0026ldquo;90\u0026rdquo;]   ┗ priority string  high   ┗ group_values array  [\u0026ldquo;cluster-xxx\u0026rdquo;, \u0026ldquo;node-xxx\u0026rdquo;]   ┗ issue_timestamp number Millisecond timestamp 1654595042399   ┗ result_value float  91.2   ┗ relation_values map  {a:100, b:91.2}    Variable usage example #  Example 1:\n{\u0026quot;content\u0026quot;:\u0026quot;【Alerting】Event ID: {{.event_id}}, Cluster：{{.resource_name}}\u0026quot;} Example 2(array traversal):\n{{range .results}} Cluster ID: {{index .group_values 0}} {{end}} Template functions #  In addition to directly displaying the field value in the alerting event, it also supports the use of template functions to further process the field value to optimize the output.\nFunctions support extra parameters. When no parameters are required or passed, the following syntax can be used directly:\n{{ \u0026lt;field\u0026gt; | \u0026lt;function\u0026gt; }}\nSpecific examples are as follows:\nFunctions take no parameters:\nAlerting event trigger time:{{ .timestamp | datetime }} Functions take parameters:\nAlerting event trigger time:{{ .timestamp | datetime_in_zone \u0026quot;Asia/Shanghai\u0026quot; }} Use multiple functions in combination:\n{{.result_value | format_bytes 2 ｜ to_upper}} The complete list of template functions is as follows:\n   Functions params Descriction      to_fixed fixed number of decimal places The float type value retains N decimal places\nExample:{{.result_value | to_fixed 2}}\nOutput:10.35    format_bytes fixed number of decimal places Byte type numeric formatting\nExample:{{.result_value | format_bytes 2}}\nOutput:10.35gb    date  Convert timestamp to UTC date\nExample:{{.timestamp | date}}\nOutput:2022-05-01    date_in_zone Time zone Convert timestamp to current zone date\nExample:{{.timestamp | date_in_zone \u0026quot;Asia/Shanghai\u0026quot;}}\nOutput:2022-05-01    datetime  Convert timestamp to UTC time\nExample:{{.timestamp | datetime}}\nOutput:2022-05-01 10:10:10    datetime_in_zone Time zone Convert timestamp to current zone time\nExample:{{.timestamp | datetime_in_zone \u0026quot;Asia/Shanghai\u0026quot;}}\nOutput:2022-05-01 10:10:10    to_lower  Convert characters to lowercase\nExample:{{.resource_name | to_lower }}\nOutput:cluster    to_upper  Convert characters to uppercase\nExample:{{.resource_name | to_upper }}\nOutput:CLUSTER    add number Example: a+b\n{{.result_value | add 1 }}\nOutput：2    sub number Example: a - b\n{{sub .result_value 1 }}\nOutput：0    mul number Example: a _ b _ c\n{{mul .result_value 3 2 }}\nOutput：6    div number Example: a/b\n{{div .result_value 2 }}\nOutput：0.5     Common Template Syntax #  Array traversal：\n{{range .results}} priority: {{.priority}} {{end}} Get values by array subscript:\nExample: group_values = [\u0026ldquo;value1\u0026rdquo;,\u0026ldquo;value2\u0026rdquo;,\u0026ldquo;value3\u0026rdquo;]\n{{index .group_values 0}} # output: value1 {{index .group_values 2}} # output: value3 if conditional branch：\n{{if pipeline}} T1 {{else}} T0 {{end}} Example:\n{{if eq .priority \u0026quot;critical\u0026quot;}} \u0026quot;#C91010\u0026quot; {{else if eq .priority \u0026quot;high\u0026quot;}} \u0026quot;#EB4C21\u0026quot; {{else}} \u0026quot;#FFB449\u0026quot; {{end}} There is also a set of binary comparison operators defined as functions:\neq Returns the boolean truth of arg1 == arg2 ne Returns the boolean truth of arg1 != arg2 lt Returns the boolean truth of arg1 \u0026lt; arg2 le Returns the boolean truth of arg1 \u0026lt;= arg2 gt Returns the boolean truth of arg1 \u0026gt; arg2 ge Returns the boolean truth of arg1 \u0026gt;= arg2   A more complete example for Slack message ...  { \u0026quot;blocks\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;section\u0026quot;, \u0026quot;text\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;【test201】Alerting:\\n\u0026lt;http://localhost:8000/#/alerting/alert/{{.event_id}}|{{.title}}\u0026gt; \u0026lt;@username\u0026gt;\u0026quot; } }, { \u0026quot;type\u0026quot;: \u0026quot;section\u0026quot;, \u0026quot;text\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Timestamp:* {{.issue_timestamp | datetime}}\u0026quot; } } ], \u0026quot;attachments\u0026quot;: [ {{range .results}} { \u0026quot;color\u0026quot;: {{if eq .priority \u0026quot;critical\u0026quot;}} \u0026quot;#C91010\u0026quot; {{else if eq .priority \u0026quot;high\u0026quot;}} \u0026quot;#EB4C21\u0026quot; {{else}} \u0026quot;#FFB449\u0026quot; {{end}}, \u0026quot;blocks\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;section\u0026quot;, \u0026quot;fields\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Cluster:* {{index .group_values 0}}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Node:* {{index .group_values 1}}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Threshold:* {{index .threshold 0}}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Priority:* {{.priority}}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Monitoring value:* {{.result_value}}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Disk usage:* {{.relation_values.a | format_bytes 2 | to_upper}}\u0026quot; } ] } ] }, {{end}} ] }     More template syntax Click me\n","subcategory":null,"summary":"","tags":null,"title":"Template variables","url":"/console/v1.29.1/docs/reference/alerting/variables/"},{"category":null,"content":"Data View #  View list #  Creating and managing data views can help you better get data from Elasticsearch.\nCreate data view #  Step 1 Define the data view #   Input a view name Matching rules: Match the corresponding index, you can also use (*) to match multiple indexes.  Step 2 Configuration #    Select time field as time filter for view index\n  Created\n  Edit data view #  The page lists all fields that match the index, and you can set the Format, Popularity, etc. of the fields.\n","subcategory":null,"summary":"","tags":null,"title":"Data View","url":"/console/v1.29.1/docs/reference/data/view/"},{"category":null,"content":"Cluster Activities #  After registering a cluster, the activities of the cluster can be observed.\n","subcategory":null,"summary":"","tags":null,"title":"Cluster Activities","url":"/console/v1.29.1/docs/reference/platform/activities/"},{"category":null,"content":"Alerting Channels #  Introduction #  The alerting channel is used to configure the channel for sending notification messages when an alerting rule is triggered. Currently, webhook is supported.\nChannes list #  In the channels list, you can query the channels that have been added\nNew alerting channel #  Click the New button on the channels list page to enter the new alerting channel page\n Input channel name (required) Select channel type (currently only webhook is supported) Input the webhook address Select the method of HTTP request, the default is POST Add HTTP request headers as needed Configure the webhook request body Click the save button to submit  Update channel configuration #  Select the channel to be updated in the channels list and click the Edit button to enter the update channel configuration page\nFor operation reference, create an alerting channel\ndelete alerting channel #  Click the delete button in the alerting channels list table to confirm the second time, and execute the delete operation after confirming the deletion.\n","subcategory":null,"summary":"","tags":null,"title":"Alerting Channels","url":"/console/v1.29.1/docs/reference/alerting/channel/"},{"category":null,"content":"Discover #  Introduction #  In Discover, you can search and query the data under the index or view according to conditions such as time and fields. The data display methods include regular mode and Insight mode.\nSearch toolbar #  Index (View) #  Search Statement #  Time Range #  Field Filter #  Save Search #  Saved Search List #  Insight Mode Switch #  Insight Configuration #  Normal Mode #  Flexibly add fields to data content with multi-function charts in normal mode\nEdit, delete, etc. document data\nInsight Mode #  In Insight mode, visual charts will be pushed to display data according to data characteristics\nCharts can be added via push list\nEdit(Remove) chart widget\n","subcategory":null,"summary":"","tags":null,"title":"Discover","url":"/console/v1.29.1/docs/reference/data-insight/discover/"},{"category":null,"content":"Common Commands #  Introduction #  Common commands are used to save frequently used Elasticsearch requests in Dev toolss, so that if you need to use them later, Just use the LOAD command in the Dev tools to load, and it can be used quickly.\nSave frequently used commands #  Open the Dev tools (Ctrl+shift+o) in the upper right corner of the console, and select the Elasticsearch request to be saved in the Dev tools (Supports selecting multiple requests at one time and saving them as common commands), after selecting, click Save As Command in the toolbar to submit.\nLoad common commands #  In the Dev tools, input LOAD + saved command name keyword will automatically prompt related saved common commands, After selecting the command to be loaded, press the Input key to automatically load the corresponding common command.\nCommon command list #  In the list of common commands, you can query the saved common commands\nClick the name column of common commands in the list to view the specific information of common commands, and you can also modify the name and tag information\nDelete common commands #  Click the Delete button in the list of frequently used commands to confirm twice, and then execute the delete operation.\n","subcategory":null,"summary":"","tags":null,"title":"Common Commands","url":"/console/v1.29.1/docs/reference/dev-tools/command/"},{"category":null,"content":"Cluster Monitoring #  Introduction #  When the monitoring of the registered cluster is enabled, the INFINI Console will periodically collect data from the target cluster according to the corresponding configuration. Including some metrics at the cluster, node, and index level. These metrics can then be observed in cluster monitoring to understand the running status of the target cluster.\nList of Elasticsearch API permissions required for monitoring #  _cluster/health，_cluster/stats，_cat/shards, /_nodes/\u0026lt;node_id\u0026gt;/stats _cat/indices, _stats, _cluster/state, _nodes, _alias, _cluster/settings\nEnable cluster monitoring #  When registering the cluster or modifying the cluster configuration, you can see the following interface\nYou can see that there is a Monitored switch. When this switch is turned on, it means that the current cluster is monitored. When a cluster is registered, monitoring is enabled by default. The monitoring configuration includes cluster health metrics, cluster metrics, node metrics and index metrics. And you can set whether to open and the collection time interval respectively.\n The above are the settings for a single cluster. In the configuration file console.yaml, you can set the monitoring start and stop of all clusters. By default, you can see the following configuration in the configuration file:\nmetrics: enabled: true major_ip_pattern: \u0026quot;192.*\u0026quot; queue: metrics elasticsearch: enabled: true cluster_stats: true node_stats: true index_stats: true If metrics\u0026gt;enable is set to false, then all cluster monitoring is disabled; If metrics\u0026gt;elasticsearch\u0026gt;cluster_stats\u0026gt;enabled is set to false, then all The cluster will not collect related metrics at the cluster level.\n View cluster metrics monitoring #  After monitoring is enabled, you can view the monitoring information of the cluster in the monitoring report under the platform management on the left menu of INFINI Console, as follows:\nClick the Advanced tab to view more metrics at the cluster level;\nAs shown in the figure, you can specify multiple nodes in a cluster to view node-related metrics and compare them horizontally. By default, the top 5 node metrics are displayed (top 5 nodes are calculated based on the sum of the query qps and write qps of the node in the last 15 minutes). Switching to the index tab page here can also specify several related metrics to view the index, similar to the node. Switch to the Thread Pool tab to view the related metrics of the node\u0026rsquo;s thread pool.\nView node metrics monitoring #  Click the Nodes tab to view a list of cluster nodes.\nClick the node name in the list to view the monitoring of the specified node\nHere you can view the metrics monitoring information and related fragmentation information of a single node\nView index metrics monitoring #  Click the Indexes tab to see a list of cluster indexes.\nClick the node name in the list to view the monitoring of the specified index\nHere you can view the metrics monitoring information and related fragmentation information of a single node\n","subcategory":null,"summary":"","tags":null,"title":"Cluster Monitoring","url":"/console/v1.29.1/docs/reference/platform/monitoring/"},{"category":null,"content":"Cluster Management #  Introduction #  Cluster management can quickly and easily help us manage multiple Elasticsearch clusters across versions.\nCluster list #  The registered Elasticsearch cluster can be queried in the cluster list\nCluster registration #  The first step is to fill in the cluster address, and enable TLS and authentication as needed (you need to input a user name and password after enabling authentication).\nThe second step is to confirm the information\n Modify the cluster name and cluster description as needed; Whether to enable monitoring (enabled by default), after enabling monitoring, you can view various metrics of the Elasticsearch cluster in the console monitoring function Whether to enable Discovery (recommended), after enabling, the console will automatically discover all nodes in the cluster. When the configured cluster address is unavailable, the console will try to use the automatically discovered addresses available in other nodes to interact with Elasticsearch  Update cluster configuration #  Click the Edit button in the cluster list table to input the update interface\nModify the configuration as needed, then click the save button to submit\nDelete cluster #  Click the delete button in the cluster list table to confirm the second time. After confirming the deletion, execute the delete operation.\n","subcategory":null,"summary":"","tags":null,"title":"Cluster Management","url":"/console/v1.29.1/docs/reference/resource/cluster/"},{"category":null,"content":"Alias Management #  Alias list #  The alias list includes addition, deletion, modification, and search operations for aliases.\nNew alias #   Alias: Input an alias name Index: Select the target index corresponding to the alias, and use (*) to bind multiple indexes. Is Write Index: specify whether the selected index is writable. If the alias only binds one index, the index is writable by default; if multiple indexes are bound by (*), it is most necessary to specify one of the indexes as writable .  Alias and index relationship list #  Clicking the + button at the beginning of the alias list row will expand and display the index list bound to the alias, and at the same time, you can set and delete the relational binding update of the index.\n","subcategory":null,"summary":"","tags":null,"title":"Alias Management","url":"/console/v1.29.1/docs/reference/data/alias/"},{"category":null,"content":"Alerting Rules #  Introduction #  The alerting rules include the configuration of four parts: data source, metrics definition, trigger condition, and message notification\nAlerting rules list #  In the alerting rules list, you can query the alerting rules that have been added\nNew alerting rule #  Click the New button in the alerting rule list to enter the new alerting rule page\nConfigure data source #   Select a cluster (required) Select index, support input index pattern (required) Input elasticsearch query DSL query filter conditions (optional) Select time field (required) Select the statistical period (for time field aggregation, the default is one minute)  Configure alerting metrics and trigger conditions #   Input the rule name Add the grouped fields and group size as needed, you can add more than one for terms aggregation Select the metrics aggregation field and statistics type, you can configure more than one, when configuring more than one, you must configure a formula to calculate the final metrics Configure alerting trigger conditions  Select Metrics value Select Bucket diff  Select based on Doc diff or Content diff       Doc diff: The difference in the number of matching documents between two adjacent time buckets\nContent diff: Whether there’s a change in a group between two adjacent time buckets. A difference value of 1 indicates an increase, -1 indicates a decrease, and 0 indicates no change\n  Select execution check cycle Input the title of the alerting event (template, referenced by the title in the template variable, click here to learn about template syntax ) Input alerting event message (template, referenced by message in template variable, click here for template syntax )   Bucket Diff is a feature introduced in INFINI Console version 1.28.2. It can be used to detect differences in data across different time periods, such as checking if there’s an abnormal change in data volume during a specific time window.\n Configure message notification #   Configure notification channels, which can be reconfigured, or you can use the add button to select an already created channel as a template to quickly fill in, and support adding multiple Choose whether to enable notification upgrades as needed Select silence period (how often notification messages are sent) Configure notification sending time period Click the save button to submit  Update alerting rules #  Select the alerting rule to be updated in the alerting rules list and click the Edit button to enter the update alerting rules page\nDelete alerting rules #  Click the delete button in the alerting rule list table to confirm the second time. After confirming the deletion, execute the delete operation.\nImport of common rule templates #  Some common Alerting rules are listed below, and notification channels such as DingTalk, Enterprise WeChat, and Slack are configured. You only need to replace the custom variables specified in the template, and you can quickly import the rules through the DevTools tool of the Console.\n  Cluster Health Change to Red  Index Health Change to Red  Disk utilization is Too High  CPU utilization is Too High  JVM utilization is Too High  Shard Storage \u0026gt;= 55G  Elasticsearch node left cluster  Search latency is great than 500ms  Too Many Deleted Documents  ","subcategory":null,"summary":"","tags":null,"title":"Alerting Rules","url":"/console/v1.29.1/docs/reference/alerting/rule/"},{"category":null,"content":"Platform Overview #  Introduction #  You can view the main indicators at these levels(cluster, node, indice and host) to understand the operation status.\nCluster #  Node #  Indice #  Host #  Host data comes from INFINI Agent reporting and node discovery of elasticsearch.\nDiscover Host #  Click the button \u0026ldquo;Discover Host\u0026rdquo; on the right side of the host list，then click the button \u0026ldquo;add hosts\u0026rdquo; to add the host to the host list after checking.\n","subcategory":null,"summary":"","tags":null,"title":"Platform Overview","url":"/console/v1.29.1/docs/reference/platform/overview/"},{"category":null,"content":"Index management #  Index list #  The index list includes addition, deletion, modification, and lookup operations on indexes.\nNew index #  Input the new index name and index settings to complete the addition.\nIndex details #  You can view the index health status, number of shards, number of documents, storage size and other details, as well as view and modify Mappings and Edit settings.\n","subcategory":null,"summary":"","tags":null,"title":"Index Management","url":"/console/v1.29.1/docs/reference/data/indices/"},{"category":null,"content":"Gateway Management #  Gateway management can quickly and easily help us manage multiple gateway instances.\nGateway list #  The created gateway can be queried in the gateway list\nGateway registration #  The first step is to fill in the gateway address, and enable TLS and authentication as needed (you need to input a user name and password after enabling authentication).\nThe second step is to confirm the information, and modify the gateway name, tags, and description, as needed.\nUpdate gateway configuration #  Click the Edit button in the gateway list table to input the update interface\nModify the configuration as needed, then click the save button to submit\nDelete gateway #  Click the delete button in the gateway list table to confirm the second time. After confirming the deletion, execute the delete operation.\n","subcategory":null,"summary":"","tags":null,"title":"Gateway Management","url":"/console/v1.29.1/docs/reference/resource/gateway/"},{"category":null,"content":"Dev Tools #  Introduction #  Use Dev Tools to quickly write and execute Elasticsearch queries and other elasticsearch APIs. When installation verification is enabled, all requests will go through API level permission verification\nOpen Dev Tools #  Use the Ctrl+Shift+O shortcut to open or click the icon in the upper right corner of the console.\nExecute request shortcuts #  Command+Enter or Ctrl+Enter\nMulti-cluster multi-tab page support #  The Dev Tools supports the use of tab pages to open multiple clusters at the same time. Even if it is the same cluster, multiple clusters can be opened, and the status of the tab pages is independent. The tab page uses the cluster name as the title by default, and can be modified by double-clicking the tab page title. Below the Dev Tools is a status bar, and on the left is the health status, http address, and version information of the current cluster. On the right is the response status and duration of the elasticsearch interface request.\nView request header information #  After using the Dev Tools to execute the elasticsearch request, you can click the headers Tab page on the right to view the request header information.\n","subcategory":null,"summary":"","tags":null,"title":"Dev Tools","url":"/console/v1.29.1/docs/reference/dev-tools/dev-tools/"},{"category":null,"content":"Data Migration #  Create Migration Task #  Click on the Data Tools option in the left menu of the INFINI Console, then click the New button to create a migration task, as shown in the following image:\nConfigure Migration Clusters #  Select the cluster es-v5616 from the source cluster list and es-v710 from the destination cluster list.\nConfigure Migration Indices #  Click the Select Migration Indices button, as shown in the image below:\nHere, we select the index test and then click Confirm.\n The test index contains two types, which are automatically split into two indices.\n You can modify the target index name and document type on the right side of the table as needed. After selecting the index, click Next to perform index initialization, as shown in the following image:\nAfter expanding, you can see the mappings and settings. The mappings on the left side display the mappings of the source cluster index. You can click the middle button to copy them to the right side. Then click Auto Optimize for automatic optimization (compatibility optimization). After completing the settings, click Start to initialize the mappings and settings. If no settings are provided, it will be skipped automatically.\n If you have already initialized the index settings and mappings through other means, you can directly click Next to skip this step.\n After completing the index initialization, click Next to set the data range and partition settings for the migration task, as shown in the following image:\nConfigure Data Range #  If you need to filter the data migration, you can set the data range. In this case, we are performing a full data migration, so we won\u0026rsquo;t set it.\nConfigure Data Partition #  If an index has a large amount of data, you can configure data partitioning. Data partitioning divides the data into multiple segments based on the specified field and partitioning step. The system will treat each segment as a subtask for migrating the data. This way, even if an exception occurs during the migration of one segment, you only need to rerun that subtask.\nData partitioning currently supports partitioning based on date type fields (date) and numeric type fields (number). In the example above, we select the date type field now_widh_format for partitioning and set the partitioning step to 5 minutes (5m). Then click the Preview button to see the result. Based on the settings, it will generate 8 partitions (partitions with 0 documents will not generate subtasks). After confirming the partition settings based on the preview information, click Save to close the partition settings and proceed to the next step for runtime configuration.\nRuntime Configuration #  In general, use the default settings and select the previously registered gateway instance, Nebula, for the execution nodes. Then click Create Task.\nConfigure Incremental Data Migration #  If the data within the index is continuously being written (e.g., log data), you can configure incremental migration to continuously detect and migrate data from the source cluster to ensure synchronization with the destination cluster.\nWhen configuring the index, specify the incremental field (e.g., timestamp) and the data write delay (default is 15 minutes) for the index. The purpose of configuring the write delay is to prevent data loss in the destination cluster due to some data not being persisted when exporting from the source cluster:\nWhen creating the task, select Detect Incremental Data and set the detection interval (default is 15 minutes):\nAfter clicking Start, the task will import incremental data from the source cluster to the destination cluster every 15 minutes. To pause the incremental migration, you can click the Pause button. Clicking Resume will resume the incremental task, and the data written to the source cluster during the pause will be imported to the destination cluster as usual:\nStart Migration Task #  After successfully creating the migration task, you will see the task list, as shown in the following image:\nYou can see that the recently created task is listed. Click the Start button in the action bar on the right side of the table to begin the task.\n Before starting the task, make sure that if the migration index involves ILM configuration, the relevant index templates and ILM aliases are properly configured in the destination cluster.\n Click the Start button to initiate the migration task.\nView Migration Task Progress #  After the task is successfully started, click on Details to view the task execution status. By clicking the Refresh button to enable auto-refresh, you will see the following changes in the task details:\nThe blue squares indicate that the subtasks (partition tasks) are already running, while the gray squares indicate that the tasks have not started.\nIn the image above, you can see that the squares have turned green, indicating that the subtasks (partition tasks) have completed data migration. The migration progress for the index test-doc is 100%, and for the index test-doc1, it is 21.11.\nIn the image above, all the squares have turned green, indicating that the migration progress for all indices is 100%, indicating that the data migration is complete. If any square turns red during the migration, it means that an error has occurred. In such cases, you can click on the task square to view the error log in the progress information and identify the specific cause of the error.\n","subcategory":null,"summary":"","tags":null,"title":"Data Migration","url":"/console/v1.29.1/docs/reference/migration/migration/"},{"category":null,"content":"Data Comparison #  Create a Comparison Task #  Click on Data Tools in the left menu of the INFINI Console, then click on the New button to create a comparison task, as shown in the following image:\n Configure Comparison Clusters #  Next, select the cluster opensearch-v1.0 from the source cluster list, and select the cluster sy_cluster from the target cluster list:\n Configure Comparison Indices #  Click on the Select Comparison Index button. Here, we have chosen the index tv6-migration and click Confirm:\n Then, we can select the target index to compare, tv6-sy:\n Click Next.\nConfigure Filter Conditions and Partition Rules #  If you need to filter data from both the source and target indices, you can set the data range filter conditions, which will be applied to both indices simultaneously:\n You can configure partitioning of the data for comparison, which will help in identifying the source of the differing data. The partition rules also apply to both the source and target indices:\n Click Next.\nRuntime Settings #  We can set the runtime parameters for the task, which usually do not need to be adjusted. Select the nodes for the task to run on, and then click Create Task:\n Configure Incremental Data Comparison #  If the data within the index is continuously being written (e.g., in a log scenario), you can configure an incremental comparison task to continuously detect the differences between the source and target clusters.\nWhen configuring the index, specify the incremental fields (e.g., timestamp) and the data write delay (default: 15 minutes) for the index that receives incremental writes. The purpose of configuring the write delay is to prevent data discrepancies caused by some data not being persisted to disk when exporting:\n When creating the task, select Detect Incremental Data and set the detection interval (default: 15 minutes):\n Once the task starts, it will continuously compare the incremental data between the source and target clusters. If you want to pause the data comparison, click the Pause button to pause the incremental task. After clicking Resume, the data written during the pause will be verified as usual:\n Start the Comparison Task #  After successfully creating the comparison task, you will see the task list. You can select Start on the right-hand side to launch the newly created task:\n Click the Detail button to view the detailed information about the task:\n You can click the refresh button at the top right of the index\nlist to continuously update the progress information:\n If the data comparison is successful, the corresponding partition block will be displayed in green; otherwise, it will be displayed in red:\n If a block turns red during the comparison process, it indicates a failure. You can click on the View Log in the progress information of the task block to view the error log and locate the specific cause of the error.\n","subcategory":null,"summary":"","tags":null,"title":"Data Comparison","url":"/console/v1.29.1/docs/reference/migration/comparison/"},{"category":null,"content":"Dashboard #  Workspace List #  Click Tab +, view workspace list:\nCreate Workspace #  Click Create New in workspace list, enter the creation interface：\nWrokspace Setting #  Click Setting, enter the setting drawer box to configure information such as name, description, and data source conditions\nDelete Workspace #  Click delete icon in workspace list, and click \u0026lsquo;ok\u0026rsquo; in the confirmation box.\nSave Workspace #  Click Save, all configuration information of the current workspace can be saved\nAdd Workspace to tab #  Click on workspace to add it to the tab in workspace list\nRemove workspace from tab #  Click the edit button on the far right side of the label bar to enter editing mode:\nClick on the \u0026lsquo;X\u0026rsquo; icon on the tab to remove it\nAdd Widget #   Add widget  Click Add Widget, select the widget type and click add in the drawer:\n Import widget  Click \u0026lsquo;Import Widget\u0026rsquo;, click add to import the view\u0026rsquo;s widgets in the drawer:\nEdit Widget #  Click ··· in widget header, click Setting in dropdown menu:\nEdit Widget Setting in the drawer:\nClick Save to save widget setting to the workspace:\nRemove Widget #  Click ··· in widget header, click Remove in dropdown menu\nCopy Widget #  Click ··· in widget header, click Copy in dropdown menu\n","subcategory":null,"summary":"","tags":null,"title":"Dashboard","url":"/console/v1.29.1/docs/reference/data-insight/dashboard/"},{"category":null,"content":"Alerting center #  Introduction #  By default, the message center displays the alerting events that are currently occurring in the system, which is convenient for administrators to quickly preview the execution status of the system.\nMessage list #  The message list aggregates all triggered alerting events. If each alerting rule repeatedly triggers multiple alerting messages, only one will be aggregated and displayed here. Click the details to see more information.\nMessage details #  Click the Details button in the message list row and column to view the detailed content of the current alerting event message, including the basic information of the event message, the timing curve within the event trigger period, and the history of rule execution detection, etc., as shown in the following figure:\nIgnore warning messages #  If you think that the alerting event does not need to be processed or is not important, you can ignore it. After ignoring, the alerting message will not be displayed in the message list by default, but it can be queried by status filtering.\nOperation steps: Click the ignore button in the message list form to confirm the second time, fill in the ignore reason, and execute the ignore operation after submitting.\n","subcategory":null,"summary":"","tags":null,"title":"Alerting Center","url":"/console/v1.29.1/docs/reference/alerting/message/"}]