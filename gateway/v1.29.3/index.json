[{"category":null,"content":"translog #  Description #  The translog filter is used to save received requests to local files and compress them. It can record some or complete request logs for archiving and request replay.\nConfiguration Example #  A simple example is as follows:\nflow: - name: translog filter: - translog: max_file_age: 7 max_file_count: 10 Parameter Description #     Name Type Description     path string Root directory for log storage, which is the translog subdirectory in the gateway data directory by default   category string Level-2 subdirectory for differentiating different logs, which is default by default.   filename string Name of the log storage file, which is translog.log by default.   rotate.compress_after_rotate bool Whether to compress and archive files after scrolling. The default value is true.   rotate.max_file_age int Maximum number of days that archived files can be retained, which is 30 days by default.   rotate.max_file_count int Maximum number of archived files that can be retained, which is 100 by default.   rotate.max_file_size_in_mb int Maximum size of a single archived file, in bytes. The default value is 1024 MB.    ","subcategory":null,"summary":"","tags":null,"title":"translog","url":"/gateway/v1.29.3/docs/references/filters/translog/"},{"category":null,"content":"switch #  Description #  The switch filter is used to forward traffic to another flow along the requested path, to facilitate cross-cluster operations. No alternation is required for Elasticsearch clusters, and all APIs in each cluster can be accessed, including APIs used for index read/write and cluster operations.\nConfiguration Example #  A simple example is as follows:\nflow: - name: es1-flow filter: - elasticsearch: elasticsearch: es1 - name: es2-flow filter: - elasticsearch: elasticsearch: es2 - name: cross_cluste_search filter: - switch: path_rules: - prefix: \u0026quot;es1:\u0026quot; flow: es1-flow - prefix: \u0026quot;es2:\u0026quot; flow: es2-flow - elasticsearch: elasticsearch: dev #elasticsearch configure reference name In the above example, the index beginning with es1: is forwarded to the es1 cluster, the index beginning with es2: is forwarded to the es2 cluster, and unmatched indexes are forwarded to the dev cluster. Clusters of different versions can be controlled within one Kibana. See the following example.\n# GET es1:_cluster/health { \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;status\u0026quot; : \u0026quot;yellow\u0026quot;, \u0026quot;timed_out\u0026quot; : false, \u0026quot;number_of_nodes\u0026quot; : 1, \u0026quot;number_of_data_nodes\u0026quot; : 1, \u0026quot;active_primary_shards\u0026quot; : 37, \u0026quot;active_shards\u0026quot; : 37, \u0026quot;relocating_shards\u0026quot; : 0, \u0026quot;initializing_shards\u0026quot; : 0, \u0026quot;unassigned_shards\u0026quot; : 9, \u0026quot;delayed_unassigned_shards\u0026quot; : 0, \u0026quot;number_of_pending_tasks\u0026quot; : 0, \u0026quot;number_of_in_flight_fetch\u0026quot; : 0, \u0026quot;task_max_waiting_in_queue_millis\u0026quot; : 0, \u0026quot;active_shards_percent_as_number\u0026quot; : 80.43478260869566 } GET es2:_cluster/health { \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;status\u0026quot; : \u0026quot;yellow\u0026quot;, \u0026quot;timed_out\u0026quot; : false, \u0026quot;number_of_nodes\u0026quot; : 1, \u0026quot;number_of_data_nodes\u0026quot; : 1, \u0026quot;active_primary_shards\u0026quot; : 6, \u0026quot;active_shards\u0026quot; : 6, \u0026quot;relocating_shards\u0026quot; : 0, \u0026quot;initializing_shards\u0026quot; : 0, \u0026quot;unassigned_shards\u0026quot; : 6, \u0026quot;delayed_unassigned_shards\u0026quot; : 0, \u0026quot;number_of_pending_tasks\u0026quot; : 0, \u0026quot;number_of_in_flight_fetch\u0026quot; : 0, \u0026quot;task_max_waiting_in_queue_millis\u0026quot; : 0, \u0026quot;active_shards_percent_as_number\u0026quot; : 50.0 } You can run commands to achieve the same effect.\nroot@infini:/opt/gateway# curl -v 192.168.3.4:8000/es1:_cat/nodes * Trying 192.168.3.4... * TCP_NODELAY set * Connected to 192.168.3.4 (192.168.3.4) port 8000 (#0) \u0026gt; GET /es1:_cat/nodes HTTP/1.1 \u0026gt; Host: 192.168.3.4:8000 \u0026gt; User-Agent: curl/7.58.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Thu, 14 Oct 2021 10:37:39 GMT \u0026lt; content-type: text/plain; charset=UTF-8 \u0026lt; Content-Length: 45 \u0026lt; X-Backend-Cluster: dev1 \u0026lt; X-Backend-Server: 192.168.3.188:9299 \u0026lt; X-Filters: filters-\u0026gt;switch-\u0026gt;filters-\u0026gt;elasticsearch-\u0026gt;skipped \u0026lt; 192.168.3.188 48 38 5 cdhilmrstw * LENOVO * Connection #0 to host 192.168.3.4 left intact root@infini:/opt/gateway# curl -v 192.168.3.4:8000/es2:_cat/nodes * Trying 192.168.3.4... * TCP_NODELAY set * Connected to 192.168.3.4 (192.168.3.4) port 8000 (#0) \u0026gt; GET /es2:_cat/nodes HTTP/1.1 \u0026gt; Host: 192.168.3.4:8000 \u0026gt; User-Agent: curl/7.58.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Thu, 14 Oct 2021 10:37:48 GMT \u0026lt; content-type: text/plain; charset=UTF-8 \u0026lt; Content-Length: 146 \u0026lt; X-elastic-product: Elasticsearch \u0026lt; Warning: 299 Elasticsearch-7.14.0-dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1 \u0026quot;Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.14/security-minimal-setup.html to enable security.\u0026quot; \u0026lt; X-Backend-Cluster: dev \u0026lt; X-Backend-Server: 192.168.3.188:9216 \u0026lt; X-Filters: filters-\u0026gt;switch-\u0026gt;filters-\u0026gt;elasticsearch-\u0026gt;skipped \u0026lt; 192.168.3.188 26 38 3 cdfhilmrstw - node-714-1 192.168.3.188 45 38 3 cdfhilmrstw * LENOVO 192.168.3.188 43 38 4 cdfhilmrstw - node-714-2 * Connection #0 to host 192.168.3.4 left intact Parameter Description #\n    Name Type Description     path_rules array Matching rule based on the URL   path_rules.prefix string Prefix string for matching. It is recommended that the prefix string end with :. After matching, the URL prefix is removed from the traffic, which is then forwarded to the subsequent flow.   path_rules.flow string Name of the flow for processing a matched request   remove_prefix bool Whether to remove matched prefix string before request forwarding. The default value is true.   continue bool Whether to continue the flow after hit. Request returns immediately after it is set to false. The default value is false.   unescape bool Whether to unescape the url path. The default value is true.    ","subcategory":null,"summary":"","tags":null,"title":"switch","url":"/gateway/v1.29.3/docs/references/filters/switch/"},{"category":null,"content":"smtp #  Description #  The SMTP processor is used to send emails, supporting both plain text and HTML emails. It supports template variables and allows attachments to be embedded in the email body. The email message is comes from the pipeline context.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: send_email auto_start: true keep_running: true retry_delay_in_ms: 5000 processor: - consumer: consumer: fetch_max_messages: 1 max_worker_size: 200 num_of_slices: 1 idle_timeout_in_seconds: 30 queue_selector: keys: - email_messages processor: - smtp: idle_timeout_in_seconds: 1 server: host: \u0026quot;smtp.ym.163.com\u0026quot; port: 994 tls: true auth: username: \u0026quot;notify-test@infini.ltd\u0026quot; password: \u0026quot;xxx\u0026quot; sender: \u0026quot;notify-test@infini.ltd\u0026quot; recipients: # to: [\u0026quot;Test \u0026lt;medcl@infini.ltd\u0026gt;\u0026quot;] # cc: [\u0026quot;INFINI Labs \u0026lt;hello@infini.ltd\u0026gt;\u0026quot;] variables: #default variables, can be used in templates license_code: \u0026quot;N/A\u0026quot; templates: trial_license: subject: \u0026quot;$[[name]] 您好，请查收您的免费授权信息! [INFINI Labs]\u0026quot; # content_type: 'text/plain' # body: \u0026quot;$[[name]] 您好，请查收您的免费授权信息! [INFINI Labs]\u0026quot; content_type: 'text/html' body_file: '/Users/medcl/go/src/infini.sh/ops/assets/email_templates/send_trial_license.html' # attachments: #use cid in html: \u0026lt;img width=100 height=100 id=\u0026quot;1\u0026quot; src=\u0026quot;cid:myimg1\u0026quot;\u0026gt; # - file: '/Users/medcl/Desktop/WechatIMG2783.png' # content_type: 'image/png' # cid: 'myimg1' Message Format #  The SMTP filter retrieves email information to be sent from the context, such as the recipient, which email template to use, and the variable parameters for the email template. The message format is fixed, and the result is as follows:\n{ \u0026quot;template\u0026quot;: \u0026quot;trial_license\u0026quot;, \u0026quot;email\u0026quot;:[\u0026quot;medcl@example.com\u0026quot;], \u0026quot;variables\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;Medcl\u0026quot;, \u0026quot;company\u0026quot;: \u0026quot;INFINI Labs\u0026quot;, \u0026quot;phone\u0026quot;: \u0026quot;400-139-9200\u0026quot; } } The field template represents the template to be used from the configuration. email represents the recipient information of the email. variables defines the variable information to be used in the template.\nParameter Description #     Name Type Description     dial_timeout_in_seconds int Timeout duration for sending emails   server.host string Email server address   server.port int Email server port   tls bool Whether to enable TLS encryption for transmission   auth.username string Email server access username   auth.password string Email server access password   sender string Sender\u0026rsquo;s email address (defaults to auth.username)   recipients.to array Recipients' email addresses (optional)   recipients.cc array CC recipients' email addresses (optional)   recipients.bcc array BCC recipients' email addresses (optional)   templates[NAME].content_type string Email type, either text/plain or text/html   templates[NAME].subject string Email subject, supports template variables   templates[NAME].body string Email body, supports template variables   templates[NAME].body_file string Email body from a file, supports template variables   templates[NAME].attachments[i].cid string Attachment CID for referencing in the body   templates[NAME].attachments[i].file string Attachment file path   templates[NAME].attachments[i].content_type string Attachment file type, see: http://en.wikipedia.org/wiki/MIME   message_field string Source field for variables (defaults to message_field)   variable_start_tag string Variable tag prefix (defaults to $[[)   variable_end_tag string Variable tag suffix (defaults to ]])   variables array Built-in variables that can be overridden by context variables    ","subcategory":null,"summary":"","tags":null,"title":"smtp","url":"/gateway/v1.29.3/docs/references/processors/smtp/"},{"category":null,"content":"sleep #  Description #  The sleep filter is used to add a fixed delay to requests to reduce the speed.\nConfiguration Example #  A simple example is as follows:\nflow: - name: slow_query_logging_test filter: - sleep: sleep_in_million_seconds: 1024 Parameter Description #     Name Type Description     sleep_in_million_seconds int64 Delay to be added, in milliseconds    ","subcategory":null,"summary":"","tags":null,"title":"sleep","url":"/gateway/v1.29.3/docs/references/filters/sleep/"},{"category":null,"content":"set_response_header #  Description #  The set_response_header filter is used to set the header information used in responses.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_response_header filter: - set_response_header: headers: - Trial -\u0026gt; true - Department -\u0026gt; Engineering Parameter Description #     Name Type Description     headers map It uses -\u0026gt; to identify a key value pair and set header information.    ","subcategory":null,"summary":"","tags":null,"title":"set_response_header","url":"/gateway/v1.29.3/docs/references/filters/set_response_header/"},{"category":null,"content":"set_response #  Description #  The set_response filter is used to set response information to be returned for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_response filter: - set_response: status: 200 content_type: application/json body: '{\u0026quot;message\u0026quot;:\u0026quot;hello world\u0026quot;}' Parameter Description #     Name Type Description     status int Request status code, which is 200 by default.   content_type string Type of returned content   body string Returned structural body    ","subcategory":null,"summary":"","tags":null,"title":"set_response","url":"/gateway/v1.29.3/docs/references/filters/set_response/"},{"category":null,"content":"set_request_query_args #  Description #  The set_request_query_args filter is used to set the QueryString parameter information used for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_request_query_args filter: - set_request_query_args: args: - size -\u0026gt; 10 Parameter Description #     Name Type Description     args map It uses -\u0026gt; to identify a key value pair and set QueryString parameter information.    ","subcategory":null,"summary":"","tags":null,"title":"set_request_query_args","url":"/gateway/v1.29.3/docs/references/filters/set_request_query_args/"},{"category":null,"content":"set_request_header #  Description #  The set_request_header filter is used to set header information for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_request_header filter: - set_request_header: headers: - Trial -\u0026gt; true - Department -\u0026gt; Engineering Parameter Description #     Name Type Description     headers map It uses -\u0026gt; to identify a key value pair and set header information.    ","subcategory":null,"summary":"","tags":null,"title":"set_request_header","url":"/gateway/v1.29.3/docs/references/filters/set_request_header/"},{"category":null,"content":"set_hostname #  Description #  The set_hostname filter is used to set the host or domain name to be accessed in the request header.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_hostname filter: - set_hostname: hostname: api.infini.cloud Parameter Description #     Name Type Description     hostname string Host information    ","subcategory":null,"summary":"","tags":null,"title":"set_hostname","url":"/gateway/v1.29.3/docs/references/filters/set_hostname/"},{"category":null,"content":"set_context #  Description #  The set_context filter is used to set relevant information for the request context.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - set_response: body: '{\u0026quot;message\u0026quot;:\u0026quot;hello world\u0026quot;}' - set_context: context: # _ctx.request.uri: http://baidu.com # _ctx.request.path: new_request_path # _ctx.request.host: api.infinilabs.com # _ctx.request.method: DELETE # _ctx.request.body: \u0026quot;hello world\u0026quot; # _ctx.request.body_json.explain: true # _ctx.request.query_args.from: 100 # _ctx.request.header.ENV: dev # _ctx.response.content_type: \u0026quot;application/json\u0026quot; # _ctx.response.header.TIMES: 100 # _ctx.response.status: 419 # _ctx.response.body: \u0026quot;new_body\u0026quot; _ctx.response.body_json.success: true - dump: request: true Parameter Description #     Name Type Description     context map Request context and corresponding value    A list of supported context variables is provided below:\n   Name Type Description     _ctx.request.uri string Complete URL of a request   _ctx.request.path string Request path   _ctx.request.host string Request host   _ctx.request.method string Request method type   _ctx.request.body string Request body   _ctx.request.body_json.[JSON_PATH] string Path to the JSON request object   _ctx.request.query_args.[KEY] string URL query request parameter   _ctx.request.header.[KEY] string Request header information   _ctx.response.content_type string Request body type   _ctx.response.header.[KEY] string Response header information   _ctx.response.status int Returned status code   _ctx.response.body string Returned response body   _ctx.response.body_json.[JSON_PATH] string Path to the JSON response object    ","subcategory":null,"summary":"","tags":null,"title":"set_context","url":"/gateway/v1.29.3/docs/references/filters/set_context/"},{"category":null,"content":"set_basic_auth #  Description #  The set_basic_auth filter is used to configure the authentication information used for requests. You can use the filter to reset the authentication information used for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_basic_auth filter: - set_basic_auth: username: admin password: password Parameter Description #     Name Type Description     username string Username   password string Password    ","subcategory":null,"summary":"","tags":null,"title":"set_basic_auth","url":"/gateway/v1.29.3/docs/references/filters/set_basic_auth/"},{"category":null,"content":"sample #  Description #  The sample filter is used to sample normal traffic proportionally. In a massive query scenario, collecting logs of all traffic consumes considerable resources. Therefore, you are advised to perform sampling statistics and sample and analyze query logs.\nConfiguration Example #  A simple example is as follows:\nflow: - name: sample filter: - sample: ratio: 0.2 Parameter Description #     Name Type Description     ratio float Sampling ratio    ","subcategory":null,"summary":"","tags":null,"title":"sample","url":"/gateway/v1.29.3/docs/references/filters/sample/"},{"category":null,"content":"rewrite_to_bulk #  Description #  rewrite_to_bulk can analyze ordinary document creation and modification operations in Elasticsearch and rewrite them as Bulk batch requests.\nConfiguration Example #  Here is a simple example:\nflow: - name: replicate-primary-writes-to-backup-queue filter: - flow: flows: - set-auth-for-backup-flow - rewrite_to_bulk: # Rewrite docs create/update/delete operation to bulk request - bulk_reshuffle: # Handle bulk requests when: contains: _ctx.request.path: /_bulk elasticsearch: \u0026#34;backup\u0026#34; queue_name_prefix: \u0026#34;async_bulk\u0026#34; level: cluster # Cluster, node, index, shard partition_size: 10 fix_null_id: true - queue: # Handle non-bulk requests \u0026lt;1. send to non-bulk queue\u0026gt; queue_name: \u0026#34;backup\u0026#34; Parameter Description #     Name Type Description     auto_generate_doc_id bool If it\u0026rsquo;s a create operation and no document ID is specified, whether to auto-generate a document ID, default is true   prefix string Add a fixed prefix to UUID   type_removed bool _type was removed in latest elasticsearch version, , this option used to remove _type in bulk metadata    ","subcategory":null,"summary":"","tags":null,"title":"rewrite_to_bulk","url":"/gateway/v1.29.3/docs/references/filters/rewrite_to_bulk/"},{"category":null,"content":"retry_limiter #  Description #  The retry_limiter filter is used to judge whether the maximum retry count is reached for a request, to avert unlimited retries of a request.\nConfiguration Example #  A simple example is as follows:\nflow: - name: retry_limiter filter: - retry_limiter: queue_name: \u0026quot;deadlock_messages\u0026quot; max_retry_times: 3 Parameter Description #     Name Type Description     max_retry_times int Maximum retry count. The default value is 3.   queue_name string Name of a message queue, to which messages are output after the maximum retry count is reached   tag_on_success array Specified tag to be attached to request context after retry conditions are triggered    ","subcategory":null,"summary":"","tags":null,"title":"retry_limiter","url":"/gateway/v1.29.3/docs/references/filters/retry_limiter/"},{"category":null,"content":"response_status_filter #  Description #  The response_status_filter is used to filter traffic based on the status code responded by the back-end service.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - response_status_filter: message: \u0026quot;Request filtered!\u0026quot; exclude: - 404 include: - 200 - 201 - 500 Parameter Description #     Name Type Description     exclude array Response code for refusing to allow traffic to pass through   include array Response code for allowing traffic to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  ","subcategory":null,"summary":"","tags":null,"title":"response_status_filter","url":"/gateway/v1.29.3/docs/references/filters/response_status_filter/"},{"category":null,"content":"response_header_format #  Description #  The response_header_format filter is used to convert keys in response header information into lowercase letters.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - response_header_format: ","subcategory":null,"summary":"","tags":null,"title":"response_header_format","url":"/gateway/v1.29.3/docs/references/filters/response_header_format/"},{"category":null,"content":"response_header_filter #  Description #  The response_header_filter is used to filter traffic based on response header information.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: ... - response_header_filter: exclude: - INFINI-CACHE: CACHED The above example shows that a request is not allowed to pass through when the header information of the response contains INFINI-CACHE: CACHED.\nParameter Description #     Name Type Description     exclude array Response header information for refusing to allow traffic to pass through   include array Response header information for allowing traffic to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  ","subcategory":null,"summary":"","tags":null,"title":"response_header_filter","url":"/gateway/v1.29.3/docs/references/filters/response_header_filter/"},{"category":null,"content":"response_body_regex_replace #  Description #  The response_body_regex_replace filter is used to replace string content in a response by using a regular expression.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - echo: message: \u0026quot;hello infini\\n\u0026quot; - response_body_regex_replace: pattern: infini to: world The result output of the preceding example is hello world.\nParameter Description #     Name Type Description     pattern string Regular expression used for matching and replacement   to string Target string used for replacement    ","subcategory":null,"summary":"","tags":null,"title":"response_body_regex_replace","url":"/gateway/v1.29.3/docs/references/filters/response_body_regex_replace/"},{"category":null,"content":"request_user_limiter #  Description #  The request_user_limiter filter is used to control traffic by username.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_user_limiter: user: - elastic - medcl max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: \u0026quot;you reached our limit\u0026quot; The above configuration controls the traffic of users medcl and elastic and the allowable maximum QPS is 256 per second.\nParameter Description #     Name Type Description     user array Users who will participate in traffic control. If this parameter is not set, all users will participate in traffic control.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   burst_requests int Burst request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   burst_bytes int Burst request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_delay_in_ms int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached   log_warn_message bool Whether to log warn message    ","subcategory":null,"summary":"","tags":null,"title":"request_user_limiter","url":"/gateway/v1.29.3/docs/references/filters/request_user_limiter/"},{"category":null,"content":"request_user_filter #  Description #  When Elasticsearch conducts authentication in Basic Auth mode, the request_user_filter is used to filter requests by request username.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_user_filter: include: - \u0026quot;elastic\u0026quot; The above example shows that only requests from elastic are allowed to pass through.\nParameter Description #     Name Type Description     exclude array List of usernames, from which requests are refused to pass through   include array List of usernames, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  ","subcategory":null,"summary":"","tags":null,"title":"request_user_filter","url":"/gateway/v1.29.3/docs/references/filters/request_user_filter/"},{"category":null,"content":"request_reshuffle #  Description #  request_reshuffle can analyze non-bulk requests to Elasticsearch, archive them in a queue, and store them on disk first. This allows business-side requests to return quickly, decoupling the front-end writes from the back-end Elasticsearch cluster. request_reshuffle requires offline pipeline consumption tasks to work in conjunction.\nConfiguration Example #  Here is a simple example:\nflow: - name: backup-flow-request-reshuffle filter: - flow: flows: - set-auth-for-backup-flow - request_reshuffle: # Reshuffle none-bulk requests elasticsearch: \u0026#34;backup\u0026#34; queue_name_prefix: \u0026#34;request_reshuffle\u0026#34; partition_size: $[[env.REQUEST_RESHUFFLE_PARTITION_SIZE]] tag_on_success: [ \u0026#34;commit_message_allowed\u0026#34; ] Parameter Description #     Name Type Description     elasticsearch string Elasticsearch cluster instance name   queue_name_prefix string Queue name prefix, default is async_bulk, default Label is type:request_reshuffle   partition_size int In addition to level, partitioning is based on the document _id. This parameter sets the maximum partition size.   continue_after_reshuffle bool Whether to continue with subsequent processes after Reshuffle is complete, default is false   tag_on_success array Add specified tags to the request context after processing all bulk requests    ","subcategory":null,"summary":"","tags":null,"title":"request_reshuffle","url":"/gateway/v1.29.3/docs/references/filters/request_reshuffle/"},{"category":null,"content":"request_path_limiter #  Description #  The request_path_limiter filter is used to define traffic control rules for requests. It can implement index-level traffic control.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_path_limiter: message: \u0026quot;Hey, You just reached our request limit!\u0026quot; rules: - pattern: \u0026quot;/(?P\u0026lt;index_name\u0026gt;medcl)/_search\u0026quot; max_qps: 3 group: index_name - pattern: \u0026quot;/(?P\u0026lt;index_name\u0026gt;.*?)/_search\u0026quot; max_qps: 100 group: index_name In the above configuration, the query is performed against the medcl query, the allowable maximum QPS is 3, and the QPS is 100 for queries performed against other indexes.\nParameter Description #     Name Type Description     message string Message returned for a request, for which traffic control conditions are met   rules array Traffic control rule. Multiple rules can be configured, which are matched based on their configuration sequence. If a rule is matched earlier, the corresponding action is performed earlier.   rules.pattern string Regular expression rule used for URL path matching. One group name must be provided as the bucket key for traffic control.   rules.group string Group name defined in the regular expression, which is used to count the number of requests. Requests with the same group value are regarded as the same type of request.   rules.max_qps int Maximum QPS defined for each group of requests. When the actual value exceeds this value, the traffic control action is triggered.    ","subcategory":null,"summary":"","tags":null,"title":"request_path_limiter","url":"/gateway/v1.29.3/docs/references/filters/request_path_limiter/"},{"category":null,"content":"request_path_filter #  Description #  The request_path_filter is used to filter traffic based on request path.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_path_filter: must: #must match all rules to continue prefix: - /medcl contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl must_not: # any match will be filtered prefix: - /.kibana - /_security - /_security - /gateway_requests* - /.reporting - /_monitoring/bulk contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl should: prefix: - /medcl contain: - _search - _async_search suffix: - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl Parameter Description #     Name Type Description     must.* object Requests are allowed to pass through only when all conditions are met.   must_not.* object Requests are allowed to pass through only when none of the conditions are met.   should.* object Requests are allowed to pass through when any condition is met.   *.prefix array Whether a request begins with a specific character   *.suffix array Whether a request ends with a specific character   *.contain array Whether a request contains a specific character   *.wildcard array Whether a request meets pattern matching rules   *.regex array Whether a request meets regular expression matching rules   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If only the should condition is met, requests are allowed to pass through only when at least one item in should is met.\n","subcategory":null,"summary":"","tags":null,"title":"request_path_filter","url":"/gateway/v1.29.3/docs/references/filters/request_path_filter/"},{"category":null,"content":"request_method_filter #  Description #  The request_method_filter is used to filter traffic based on request method.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_method_filter: exclude: - PUT - POST include: - GET - HEAD - DELETE Parameter Description #     Name Type Description     exclude array Methods of requests that are refused to pass through   include array Methods of requests that are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  ","subcategory":null,"summary":"","tags":null,"title":"request_method_filter","url":"/gateway/v1.29.3/docs/references/filters/request_method_filter/"},{"category":null,"content":"request_host_limiter #  Description #  The request_host_limiter filter is used to control traffic based on the request host (domain name).\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_host_limiter: host: - api.elasticsearch.cn:8000 - logging.elasticsearch.cn:8000 max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: \u0026quot;you reached our limit\u0026quot; The above configuration controls the traffic used for accessing domain names api.elasticsearch.cn and logging.elasticsearch.cn and the maximum allowable QPS is 256 per second.\nParameter Description #     Name Type Description     host array Host domain names that will participate in traffic control. If this parameter is not set, all host domain names will participate in traffic control. If an accessed domain name contains a port ID, add the port ID here. For example, localhost:8080.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   burst_requests int Burst request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   burst_bytes int Burst request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_delay_in_ms int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached   log_warn_message bool Whether to log warn message    ","subcategory":null,"summary":"","tags":null,"title":"request_host_limiter","url":"/gateway/v1.29.3/docs/references/filters/request_host_limiter/"},{"category":null,"content":"request_host_filter #  Description #  The request_host_filter is used to filter requests based on a specified domain name or host name. It is suitable for scenarios in which there is only one IP address but access control is required for multiple domain names.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_host_filter: include: - domain-test2.com:8000 The above example shows that only requests that are used to access the domain name domain-test2.com:8000 are allowed to pass through.\nExample #  ✗ curl -k -u user:passwd http://domain-test4.com:8000/ -v  Trying 192.168.3.67\u0026hellip; TCP_NODELAY set Connected to domain-test4.com (192.168.3.67) port 8000 (#0) Server auth using Basic with user \u0026lsquo;medcl\u0026rsquo; \u0026gt; GET / HTTP/1.1 \u0026gt; Host: domain-test4.com:8000 \u0026gt; Authorization: Basic 123= \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: / \u0026gt; \u0026lt; HTTP/1.1 403 Forbidden \u0026lt; Server: INFINI \u0026lt; Date: Fri, 15 Jan 2021 13:53:01 GMT \u0026lt; Content-Length: 0 \u0026lt; FILTERED: true \u0026lt; Connection #0 to host domain-test4.com left intact Closing connection 0  ✗ curl -k -u user:passwd http://domain-test2.com:8000/ -v\n Trying 192.168.3.67\u0026hellip; TCP_NODELAY set Connected to domain-test2.com (192.168.3.67) port 8000 (#0) Server auth using Basic with user \u0026lsquo;medcl\u0026rsquo; \u0026gt; GET / HTTP/1.1 \u0026gt; Host: domain-test2.com:8000 \u0026gt; Authorization: Basic 123= \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: / \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Fri, 15 Jan 2021 13:52:53 GMT \u0026lt; Content-Type: application/json; charset=UTF-8 \u0026lt; Content-Length: 480 \u0026lt; UPSTREAM: 192.168.3.203:9200 \u0026lt; CACHE-HASH: a2902f950b4ade804b21a062257387ef \u0026lt; { \u0026quot;name\u0026quot; : \u0026quot;node3\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;pi\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;Z_HcN_6ESKWicV-eLsyU4g\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;6.4.2\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;04711c2\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2018-09-26T13:34:09.098244Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;7.4.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } Connection #0 to host domain-test2.com left intact Closing connection 0 Parameter Description #      Name Type Description     exclude array List of hosts, from which requests are refused to pass through   include array List of hosts, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  ","subcategory":null,"summary":"","tags":null,"title":"request_host_filter","url":"/gateway/v1.29.3/docs/references/filters/request_host_filter/"},{"category":null,"content":"request_header_filter #  Description #  The request_header_filter is used to filter traffic based on request header information.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_header_filter: include: - TRACE: true The above example shows that requests are allowed to pass through only when the headers of the requests contain TRACE: true.\ncurl 192.168.3.4:8000 -v -H 'TRACE: true' Parameter Description #     Name Type Description     exclude array Header information used to refuse to allow requests to pass through   include array Header information used to allow requests to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  ","subcategory":null,"summary":"","tags":null,"title":"request_header_filter","url":"/gateway/v1.29.3/docs/references/filters/request_header_filter/"},{"category":null,"content":"request_client_ip_limiter #  Description #  The request_client_ip_limiter filter is used to control traffic based on the request client IP address.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_client_ip_limiter: ip: #only limit for specify ips - 127.0.0.1 max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: \u0026quot;your ip reached our limit\u0026quot; The above configuration controls the traffic with the IP address of 127.0.0.1 and the allowable maximum QPS is 256 per second.\nParameter Description #     Name Type Description     ip array Client IP addresses that will participate in traffic control. If this parameter is not set, all IP addresses will participate in traffic control.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   burst_requests int Burst request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   burst_bytes int Burst request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_delay_in_ms int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached   log_warn_message bool Whether to log warn message    ","subcategory":null,"summary":"","tags":null,"title":"request_client_ip_limiter","url":"/gateway/v1.29.3/docs/references/filters/request_client_ip_limiter/"},{"category":null,"content":"request_client_ip_filter #  Description #  The request_client_ip_filter is used to filter traffic based on source user IP addresses of requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_client_ip_filter: exclude: - 192.168.3.67 The above example shows that requests from 192.168.3.67 are not allowed to pass through.\nThe following is an example of route redirection.\nflow: - name: echo filter: - echo: message: hello stanger - name: default_flow filter: - request_client_ip_filter: action: redirect_flow flow: echo exclude: - 192.168.3.67 Requests from 192.168.3.67 are redirected to another echo flow.\nParameter Description #     Name Type Description     exclude array List of IP arrays, from which requests are refused to pass through   include array List of IP arrays, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  ","subcategory":null,"summary":"","tags":null,"title":"request_client_ip_filter","url":"/gateway/v1.29.3/docs/references/filters/request_client_ip_filter/"},{"category":null,"content":"request_body_regex_replace #  Description #  The request_body_regex_replace filter is used to replace string content in a request body by using a regular expression.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_body_regex_replace: pattern: '\u0026quot;size\u0026quot;: 10000' to: '\u0026quot;size\u0026quot;: 100' - elasticsearch: elasticsearch: prod - dump: request: true The above example changes the size from 10000 to 100 in the request body sent to Elasticsearch. The filter can be used to dynamically fix errors or incorrect queries.\nThe test is as follows:\ncurl -XPOST \u0026quot;http://localhost:8000/myindex/_search\u0026quot; -H 'Content-Type: application/json' -d' { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} },\u0026quot;size\u0026quot;: 10000 }' The actual query is as follows:\n { \u0026quot;_index\u0026quot; : \u0026quot;gateway_requests\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;EH5bG3gBsbC2s3iWFzCF\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;tls\u0026quot; : false, \u0026quot;@timestamp\u0026quot; : \u0026quot;2021-03-10T08:57:30.645Z\u0026quot;, \u0026quot;conn_time\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;flow\u0026quot; : { \u0026quot;from\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;process\u0026quot; : [ \u0026quot;request_body_regex_replace\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;date_range_precision_tuning\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;elasticsearch\u0026quot;, \u0026quot;set_cache\u0026quot;, \u0026quot;||\u0026quot;, \u0026quot;request_logging\u0026quot; ], \u0026quot;relay\u0026quot; : \u0026quot;192.168.43.101-Quartz\u0026quot;, \u0026quot;to\u0026quot; : [ \u0026quot;localhost:9200\u0026quot; ] }, \u0026quot;id\u0026quot; : 3, \u0026quot;local_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;remote_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;request\u0026quot; : { \u0026quot;body_length\u0026quot; : 53, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot; { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} },\u0026quot;size\u0026quot;: 100 } \u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;content-type\u0026quot; : \u0026quot;application/json\u0026quot;, \u0026quot;User-Agent\u0026quot; : \u0026quot;curl/7.54.0\u0026quot;, \u0026quot;Accept\u0026quot; : \u0026quot;/\u0026quot;, \u0026quot;Host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;53\u0026quot; }, \u0026quot;host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:8000\u0026quot;, \u0026quot;method\u0026quot; : \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot; : \u0026quot;/myindex/_search\u0026quot;, \u0026quot;remote_addr\u0026quot; : \u0026quot;127.0.0.1:63309\u0026quot;, \u0026quot;started\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;uri\u0026quot; : \u0026quot;http://localhost:8000/myindex/_search\u0026quot; }, \u0026quot;response\u0026quot; : { \u0026quot;body_length\u0026quot; : 441, \u0026quot;cached\u0026quot; : false, \u0026quot;elapsed\u0026quot; : 9.878, \u0026quot;status_code\u0026quot; : 200, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot;{\u0026quot;took\u0026quot;:0,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;max_score\u0026quot;:1.0,\u0026quot;hits\u0026quot;:[{\u0026quot;_index\u0026quot;:\u0026quot;myindex\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;,\u0026quot;_id\u0026quot;:\u0026quot;c132mhq3r0otidqkac1g\u0026quot;,\u0026quot;_score\u0026quot;:1.0,\u0026quot;_source\u0026quot;:{\u0026quot;name\u0026quot;:\u0026quot;local\u0026quot;,\u0026quot;enabled\u0026quot;:true,\u0026quot;endpoint\u0026quot;:\u0026quot;http://localhost:9200\u0026quot;,\u0026quot;basic_auth\u0026quot;:{},\u0026quot;discovery\u0026quot;:{\u0026quot;refresh\u0026quot;:{}},\u0026quot;created\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;,\u0026quot;updated\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;}}]}}\u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;UPSTREAM\u0026quot; : \u0026quot;localhost:9200\u0026quot;, \u0026quot;process\u0026quot; : \u0026quot;request_body_regex_replace-\u0026gt;get_cache-\u0026gt;date_range_precision_tuning-\u0026gt;get_cache-\u0026gt;elasticsearch-\u0026gt;set_cache\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;441\u0026quot;, \u0026quot;content-type\u0026quot; : \u0026quot;application/json; charset=UTF-8\u0026quot;, \u0026quot;Server\u0026quot; : \u0026quot;INFINI\u0026quot;, \u0026quot;CLUSTER\u0026quot; : \u0026quot;dev\u0026quot; }, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:63310\u0026quot; } } } Parameter Description #\n    Name Type Description     pattern string Regular expression used for matching and replacement   to string Target string used for replacement    ","subcategory":null,"summary":"","tags":null,"title":"request_body_regex_replace","url":"/gateway/v1.29.3/docs/references/filters/request_body_regex_replace/"},{"category":null,"content":"request_body_json_set #  Description #  The request_body_json_set filter is used to modify a request body of the JSON format.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_body_json_set: path: - aggs.total_num.terms.field -\u0026gt; \u0026quot;name\u0026quot; - aggs.total_num.terms.size -\u0026gt; 3 - size -\u0026gt; 0 Parameter Description #     Name Type Description     path map It uses -\u0026gt; to identify the key value pair: JSON path and the value used for replacement.   ignore_missing bool Whether to ignore processing if the JSON path does not exist. The default value is false.    ","subcategory":null,"summary":"","tags":null,"title":"request_body_json_set","url":"/gateway/v1.29.3/docs/references/filters/request_body_json_set/"},{"category":null,"content":"request_body_json_del #  Description #  The request_body_json_del filter is used to delete some fields from a request body of the JSON format.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_body_json_del: path: - query.bool.should.[0] - query.bool.must Parameter Description #     Name Type Description     path array JSON path key value to be deleted   ignore_missing bool Whether to ignore processing if the JSON path does not exist. The default value is false.    ","subcategory":null,"summary":"","tags":null,"title":"request_body_json_del","url":"/gateway/v1.29.3/docs/references/filters/request_body_json_del/"},{"category":null,"content":"request_api_key_limiter #  Description #  The request_api_key_limiter filter is used to control traffic by API key.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_api_key_limiter: id: - VuaCfGcBCdbkQm-e5aOx max_requests: 1 action: drop # retry or drop message: \u0026quot;your api_key reached our limit\u0026quot; The above configuration controls the traffic with the API ID of VuaCfGcBCdbkQm-e5aOx and the allowable maximum QPS is 1 per second.\n➜ ~ curl localhost:8000 -H \u0026quot;Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw==\u0026quot; -v * Rebuilt URL to: localhost:8000/ * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw== \u0026gt; \u0026lt; HTTP/1.1 429 Too Many Requests \u0026lt; Server: INFINI \u0026lt; Date: Mon, 12 Apr 2021 15:14:52 GMT \u0026lt; content-type: text/plain; charset=utf-8 \u0026lt; content-length: 30 \u0026lt; process: request_api_key_limiter \u0026lt; * Connection #0 to host localhost left intact your api_key reached our limit% Parameter Description #     Name Type Description     id array IDs of APIs that will participate in traffic control. If this parameter is not set, all API keys will participate in traffic control.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   burst_requests int Burst request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   burst_bytes int Burst request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_delay_in_ms int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached   log_warn_message bool Whether to log warn message    ","subcategory":null,"summary":"","tags":null,"title":"request_api_key_limiter","url":"/gateway/v1.29.3/docs/references/filters/request_api_key_limiter/"},{"category":null,"content":"request_api_key_filter #  Description #  When Elasticsearch conducts authentication through API keys, the request_api_key_filter is used to filter requests based on request API ID.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_api_key_filter: message: \u0026quot;Request filtered!\u0026quot; exclude: - VuaCfGcBCdbkQm-e5aOx The above example shows that requests from VuaCfGcBCdbkQm-e5aOx will be rejected. See the following information.\n➜ ~ curl localhost:8000 -H \u0026quot;Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw==\u0026quot; -v * Rebuilt URL to: localhost:8000/ * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw== \u0026gt; \u0026lt; HTTP/1.1 403 Forbidden \u0026lt; Server: INFINI \u0026lt; Date: Mon, 12 Apr 2021 15:02:37 GMT \u0026lt; content-type: text/plain; charset=utf-8 \u0026lt; content-length: 17 \u0026lt; FILTERED: true \u0026lt; process: request_api_key_filter \u0026lt; * Connection #0 to host localhost left intact {\u0026quot;error\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Request filtered!\u0026quot;}% ➜ ~ Parameter Description #     Name Type Description     exclude array List of usernames, from which requests are refused to pass through   include array List of usernames, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  ","subcategory":null,"summary":"","tags":null,"title":"request_api_key_filter","url":"/gateway/v1.29.3/docs/references/filters/request_api_key_filter/"},{"category":null,"content":"replay #  Description #  The replay processor is used to replay requests recorded by the record filter.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: play_requests auto_start: true keep_running: false processor: - replay: filename: requests.txt schema: \u0026quot;http\u0026quot; host: \u0026quot;localhost:8000\u0026quot; Parameter Description #     Name Type Description     filename string Name of a file that contains replayed messages   schema string Request protocol type: http or https   host string Target server that receives requests, in the format of host:port    ","subcategory":null,"summary":"","tags":null,"title":"replay","url":"/gateway/v1.29.3/docs/references/processors/replay/"},{"category":null,"content":"redis_pubsub #  Description #  The redis filter is used to store received requests and response results to Redis message queues.\nConfiguration Example #  A simple example is as follows:\nflow: - name: redis_pubsub filter: - redis_pubsub: host: 127.0.0.1 port: 6379 channel: gateway response: true Parameter Description #     Name Type Description     host string Redis host name, which is localhost by default.   port int Redis port ID, which is 6379 by default.   password string Redis password   db int Default database of Redis, which is 0 by default.   channel string Name of a Redis message queue. It is mandatory and has no default value.   response bool Whether the response result is contained. The default value is true.    ","subcategory":null,"summary":"","tags":null,"title":"redis_pubsub","url":"/gateway/v1.29.3/docs/references/filters/redis_pubsub/"},{"category":null,"content":"redirect #  Description #  redirect filter used to redirect request to specify URL address。\nConfiguration Example #  A simple example is as follows:\nflow: - name: redirect filter: - redirect: uri: https://infinilabs.com Parameter Description #     Name Type Description     uri string The target URI   code int Status code，default 302    ","subcategory":null,"summary":"","tags":null,"title":"redirect","url":"/gateway/v1.29.3/docs/references/filters/redirect/"},{"category":null,"content":"record #  Description #  The record filter is used to record requests. Output requests can be copied to the console of Kibana for debugging.\nConfiguration Example #  A simple example is as follows:\nflow: - name: request_logging filter: - record: stdout: true filename: requests.txt Examples of the format of request logs output by the record filter are as follows:\nGET /_cluster/state/version,master_node,routing_table,metadata/* GET /_alias\nGET /_cluster/health\nGET /_cluster/stats\nGET /_nodes/0NSvaoOGRs2VIeLv3lLpmA/stats Parameter Description #\n    Name Type Description     filename string Filename of request logs stored in the data directory   stdout bool Whether the terminal also outputs the characters. The default value is false.    ","subcategory":null,"summary":"","tags":null,"title":"record","url":"/gateway/v1.29.3/docs/references/filters/record/"},{"category":null,"content":"ratio #  Description #  The ratio filter is used to forward normal traffic to another flow proportionally. It can implement canary release, traffic migration and export, or switch some traffic to clusters of different versions for testing.\nConfiguration Example #  A simple example is as follows:\nflow: - name: ratio_traffic_forward filter: - ratio: ratio: 0.1 flow: hello_world continue: true Parameter Description #     Name Type Description     ratio float Proportion of traffic to be migrated   action string The action when hit, can be drop or redirect_flow, default is redirect_flow   flow string New traffic processing flow   continue bool Whether to continue flow after hit. Request returns immediately after it is set to false. The default value is false.    ","subcategory":null,"summary":"","tags":null,"title":"ratio","url":"/gateway/v1.29.3/docs/references/filters/ratio/"},{"category":null,"content":"queue_consumer #  Description #  The queue_consumer processor is used to asynchronously consume requests in a queue and send the requests to Elasticsearch.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - queue_consumer: input_queue: \u0026quot;backup\u0026quot; elasticsearch: \u0026quot;backup\u0026quot; waiting_after: [ \u0026quot;backup_failure_requests\u0026quot;] worker_size: 20 when: cluster_available: [ \u0026quot;backup\u0026quot; ] Parameter Description #     Name Type Description     input_queue string Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.   idle_timeout_in_seconds int Timeout duration of the consumption queue, which is set to 1 by default.   elasticsearch string Name of a target cluster, to which requests are saved.   waiting_after array Data in the main queue can be consumed only after data in a specified queue is consumed.   failure_queue string Request that fails to be executed because of a back-end failure. The default value is %input_queue%-failure.   invalid_queue string Request, for which the returned status code is 4xx. The default value is %input_queue%-invalid.   compress bool Whether to compress requests. The default value is false.   safety_parse bool Whether to enable secure parsing, that is, no buffer is used and memory usage is higher. The default value is true.   doc_buffer_size bool Maximum document buffer size for the processing of a single request. You are advised to set it to be greater than the maximum size of a single document. The default value is 256*1024.    ","subcategory":null,"summary":"","tags":null,"title":"queue_consumer","url":"/gateway/v1.29.3/docs/references/processors/queue_consumer/"},{"category":null,"content":"queue #  Description #  The queue filter is used to save requests to a message queue.\nConfiguration Example #  Here is a simple example:\nflow: - name: queue filter: - queue: # Handle dirty_writes, second-commit queue_name: \u0026#34;primary_final_commit_log##$[[partition_id]]\u0026#34; labels: type: \u0026#34;primary_final_commit_log\u0026#34; partition_id: \u0026#34;$[[partition_id]]\u0026#34; message: \u0026#34;$[[_ctx.request.header.X-Replicated-ID]]#$[[_ctx.request.header.LAST_PRODUCED_MESSAGE_OFFSET]]#$[[_sys.unix_timestamp_of_now]]\u0026#34; when: equals: _ctx.request.header.X-Replicated: \u0026#34;true\u0026#34; Parameter Description #     Name Type Description     depth_threshold int Must be greater than the specified depth to be stored in the queue, default is 0   type string Specify the type of message queue, supports kafka and disk   queue_name string Message queue name   labels map Add custom labels to the newly created message queue topic   message string Custom message content, supports variables   save_last_produced_message_offset bool Whether to retain the Offset of the last successfully written message in the context for later use as a variable   last_produced_message_offset_key string Custom variable name for storing the Offset of the last successfully written message in the context, default is LAST_PRODUCED_MESSAGE_OFFSET    ","subcategory":null,"summary":"","tags":null,"title":"queue","url":"/gateway/v1.29.3/docs/references/filters/queue/"},{"category":null,"content":"merge_to_bulk #  Description #  The merge_to_bulk processor is used to consume pure JSON documents in the queue, and merge them into bulk requests and save them in the specified queue. It needs to be consumed with the consumer processor, and batch writes are used instead of single requests to improve write throughput.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: messages_merge_async_bulk_results auto_start: true keep_running: true singleton: true processor: - consumer: queue_selector: keys: - bulk_result_messages consumer: group: merge_to_bulk processor: - merge_to_bulk: elasticsearch: \u0026quot;logging\u0026quot; index_name: \u0026quot;.infini_async_bulk_results\u0026quot; output_queue: name: \u0026quot;merged_async_bulk_results\u0026quot; label: tag: \u0026quot;bulk_logging\u0026quot; worker_size: 1 bulk_size_in_mb: 10 Parameter Description #     名称 类型 说明     message_field string The field name in the context where messages from the queue are stored. Default is messages.   bulk_size_in_kb int Size of a bulk request, in KB.   bulk_size_in_mb int Size of a bulk request, in MB.   elasticsearch string Name of a target cluster, to which requests are saved.   index_name string Name of the index stored to the target cluster.   type_name string Name of the index type stored to the target cluster. It is set based on the cluster version. The value is doc for Elasticsearch versions earlier than v7 and _doc for versions later than v7.   output_queue.name string The name of output queue   output_queue.label map The labels assign to the output queue，with label type:merge_to_bulk builtin.    ","subcategory":null,"summary":"","tags":null,"title":"merge_to_bulk","url":"/gateway/v1.29.3/docs/references/processors/merge_to_bulk/"},{"category":null,"content":"logging #  Description #  The logging filter is used to asynchronously record requests to the local disk to minimize the delay of requests. In scenarios with heavy traffic, you are advised to use other request filters jointly to reduce the total number of logs.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - logging: queue_name: request_logging An example of a recorded request log is as follows:\n { \u0026quot;_index\u0026quot; : \u0026quot;gateway_requests\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;EH5bG3gBsbC2s3iWFzCF\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;tls\u0026quot; : false, \u0026quot;@timestamp\u0026quot; : \u0026quot;2021-03-10T08:57:30.645Z\u0026quot;, \u0026quot;conn_time\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;flow\u0026quot; : { \u0026quot;from\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;process\u0026quot; : [ \u0026quot;request_body_regex_replace\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;date_range_precision_tuning\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;elasticsearch\u0026quot;, \u0026quot;set_cache\u0026quot;, \u0026quot;||\u0026quot;, \u0026quot;request_logging\u0026quot; ], \u0026quot;relay\u0026quot; : \u0026quot;192.168.43.101-Quartz\u0026quot;, \u0026quot;to\u0026quot; : [ \u0026quot;localhost:9200\u0026quot; ] }, \u0026quot;id\u0026quot; : 3, \u0026quot;local_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;remote_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;request\u0026quot; : { \u0026quot;body_length\u0026quot; : 53, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot; { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} },\u0026quot;size\u0026quot;: 100 } \u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;content-type\u0026quot; : \u0026quot;application/json\u0026quot;, \u0026quot;User-Agent\u0026quot; : \u0026quot;curl/7.54.0\u0026quot;, \u0026quot;Accept\u0026quot; : \u0026quot;/\u0026quot;, \u0026quot;Host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;53\u0026quot; }, \u0026quot;host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:8000\u0026quot;, \u0026quot;method\u0026quot; : \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot; : \u0026quot;/myindex/_search\u0026quot;, \u0026quot;remote_addr\u0026quot; : \u0026quot;127.0.0.1:63309\u0026quot;, \u0026quot;started\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;uri\u0026quot; : \u0026quot;http://localhost:8000/myindex/_search\u0026quot; }, \u0026quot;response\u0026quot; : { \u0026quot;body_length\u0026quot; : 441, \u0026quot;cached\u0026quot; : false, \u0026quot;elapsed\u0026quot; : 9.878, \u0026quot;status_code\u0026quot; : 200, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot;{\u0026quot;took\u0026quot;:0,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;max_score\u0026quot;:1.0,\u0026quot;hits\u0026quot;:[{\u0026quot;_index\u0026quot;:\u0026quot;myindex\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;,\u0026quot;_id\u0026quot;:\u0026quot;c132mhq3r0otidqkac1g\u0026quot;,\u0026quot;_score\u0026quot;:1.0,\u0026quot;_source\u0026quot;:{\u0026quot;name\u0026quot;:\u0026quot;local\u0026quot;,\u0026quot;enabled\u0026quot;:true,\u0026quot;endpoint\u0026quot;:\u0026quot;http://localhost:9200\u0026quot;,\u0026quot;basic_auth\u0026quot;:{},\u0026quot;discovery\u0026quot;:{\u0026quot;refresh\u0026quot;:{}},\u0026quot;created\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;,\u0026quot;updated\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;}}]}}\u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;UPSTREAM\u0026quot; : \u0026quot;localhost:9200\u0026quot;, \u0026quot;process\u0026quot; : \u0026quot;request_body_regex_replace-\u0026gt;get_cache-\u0026gt;date_range_precision_tuning-\u0026gt;get_cache-\u0026gt;elasticsearch-\u0026gt;set_cache\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;441\u0026quot;, \u0026quot;content-type\u0026quot; : \u0026quot;application/json; charset=UTF-8\u0026quot;, \u0026quot;Server\u0026quot; : \u0026quot;INFINI\u0026quot;, \u0026quot;CLUSTER\u0026quot; : \u0026quot;dev\u0026quot; }, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:63310\u0026quot; } } } Parameter Description #\n    Name Type Description     queue_name string Name of a queue that stores request logs in the local disk   format_header_keys bool Whether to standardize the header and convert it into lowercase letters. The default value is false.   remove_authorization bool Whether to remove authorization information from the header. The default value is true.   max_request_body_size int Whether to truncate a very long request message. The default value is 1024, indicating that 1024 characters are retained.   max_response_body_size int Whether to truncate a very long response message. The default value is 1024, indicating that 1024 characters are retained.   min_elapsed_time_in_ms int Request filtering based on response time, that is, the minimum time (ms) for request logging. A request with time that exceeds this value will be logged.   bulk_stats_details bool Whether to record detailed index-based bulk request statistics. The default value is true.    ","subcategory":null,"summary":"","tags":null,"title":"logging","url":"/gateway/v1.29.3/docs/references/filters/logging/"},{"category":null,"content":"ldap_auth #  Description #  The ldap_auth filter is used to set authentication based on the Lightweight Directory Access Protocol (LDAP).\nConfiguration Example #  A simple example is as follows:\nflow: - name: ldap_auth filter: - ldap_auth: host: \u0026quot;ldap.forumsys.com\u0026quot; port: 389 bind_dn: \u0026quot;cn=read-only-admin,dc=example,dc=com\u0026quot; bind_password: \u0026quot;password\u0026quot; base_dn: \u0026quot;dc=example,dc=com\u0026quot; user_filter: \u0026quot;(uid=%s)\u0026quot; The above configuration uses an online free LDAP test server, the test user is tesla, and the password is password.\n➜ curl http://127.0.0.1:8000/ -u tesla:password { \u0026quot;name\u0026quot; : \u0026quot;192.168.3.7\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;ZGTwWtBfSLWRpsS1VKQDiQ\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;7.8.0\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;757314695644ea9a1dc2fecd26d1a43856725e65\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2020-06-14T19:35:50.234439Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;8.5.1\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;6.8.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;6.0.0-beta1\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } ➜ curl http://127.0.0.1:8000/ -u tesla:password1 Unauthorized% Parameter Description #     Name Type Description     host string Address of the LDAP server   port int Port of the LDAP server. The default value is 389.   tls bool Whether the LDAP server uses the Transport Layer Security (TLS) protocol. The default value is false.   bind_dn string Information about the user who performs the LDAP query   bind_password string Password for performing the LDAP query   base_dn string Root domain for filtering LDAP users   user_filter string Query condition for filtering LDAP users. The default value is (uid=%s).   uid_attribute string Attribute of a user ID. The default value is uid.   group_attribute string Attribute of a user group. The default value is cn.   attribute array List of attributes returned by the LDAP query   max_cache_items int The max number of cached items   cache_ttl duration The expired TTL of cached items，default 300s    ","subcategory":null,"summary":"","tags":null,"title":"ldap_auth","url":"/gateway/v1.29.3/docs/references/filters/ldap_auth/"},{"category":null,"content":"json_indexing #  Description #  The json_indexing processor is used to consume pure JSON documents in queues and store them to a specified Elasticsearch server.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;gateway_requests\u0026quot; elasticsearch: \u0026quot;dev\u0026quot; input_queue: \u0026quot;request_logging\u0026quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 Parameter Description #     Name Type Description     input_queue string Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.   idle_timeout_in_seconds int Timeout duration of the consumption queue, in seconds. The default value is 5.   bulk_size_in_kb int Size of a bulk request, in KB.   bulk_size_in_mb int Size of a bulk request, in MB.   elasticsearch string Name of a target cluster, to which requests are saved.   index_name string Name of the index stored to the target cluster.   type_name string Name of the index type stored to the target cluster. It is set based on the cluster version. The value is doc for Elasticsearch versions earlier than v7 and _doc for versions later than v7.    ","subcategory":null,"summary":"","tags":null,"title":"json_indexing","url":"/gateway/v1.29.3/docs/references/processors/json_indexing/"},{"category":null,"content":"javascript #  Description #  The javascript filter can be used to execute your own processing flow by crafting the scripts in javascript, which provide the ultimate flexibility.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - javascript: source: \u0026gt; function process(ctx) { var console = require('console'); console.log(\u0026quot;hello from javascript\u0026quot;); } The process in this script is a built-in function that handles incoming context and allows to write your custom business logic.\nIf the script is complex, it can be loaded from a file:\nflow: - name: test filter: - javascript: file: example.js The example.js is where the file located.\nParameter Description #     Name Type Description     source string Inline Javascript source code.   file string Path to a script file to load. Relative paths are interpreted as relative to the ${INSTANCE_DATA_PATH}/scripts directory.   params map A dictionary of parameters that are passed to the register of the script.    Context API #  The Context object passed to the process method has the following API. To learn more about context, please refer to Request Context.\n   Method Description     Get(string) Get a value from the context. If the key does not exist null is returned. If no key is provided then an object containing all fields is returned. eg: var value = event.Get(key);   Put(string, value) Put a value into the context. If the key was already set then the previous value is returned. It throws an exception if the key cannot be set because one of the intermediate values is not an object. eg: var old = event.Put(key, value);   Rename(string, string) Rename a key in the context. The target key must not exist. It returns true if the source key was successfully renamed to the target key. eg: var success = event.Rename(\u0026quot;source\u0026quot;, \u0026quot;target\u0026quot;);   Delete(string) Delete a field from the context. It returns true on success. eg: var deleted = event.Delete(\u0026quot;user.email\u0026quot;);   Tag(string) Append a tag to the tags field if the tag does not already exist. Throws an exception if tags exists and is not a string or a list of strings. eg: event.Tag(\u0026quot;user_event\u0026quot;);   AppendTo(string, string) AppendTo is a specialized Put method that converts the existing value to an array and appends the value if it does not already exist. If there is an existing value that’s not a string or array of strings then an exception is thrown. eg: event.AppendTo(\u0026quot;error.message\u0026quot;, \u0026quot;invalid file hash\u0026quot;);    Parameterization #  The following example describes how to use params to pass variables to scripts that can be loaded from files for easy reuse of program scripts.\nflow: - name: test filter: - javascript: params: keyword: [ \u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;, \u0026quot;scripts\u0026quot; ] source: \u0026gt; var console = require('console'); var params = {keyword: []}; function register(scriptParams) { params = scriptParams; } function process(ctx) { console.info(\u0026quot;keyword comes from params: [%s]\u0026quot;, params.keyword); } register is a built-in function that initializes external parameters.\n","subcategory":null,"summary":"","tags":null,"title":"javascript","url":"/gateway/v1.29.3/docs/references/filters/javascript/"},{"category":null,"content":"indexing_merge #  Description #  The indexing_merge processor is used to consume pure JSON documents in the queue, and merge them into bulk requests and save them in the specified queue. It needs to be consumed with the bulk_indexing processor, and batch writes are used instead of single requests to improve write throughput.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: indexing_merge auto_start: true keep_running: true processor: - indexing_merge: input_queue: \u0026quot;request_logging\u0026quot; elasticsearch: \u0026quot;logging-server\u0026quot; index_name: \u0026quot;infini_gateway_requests\u0026quot; output_queue: name: \u0026quot;gateway_requests\u0026quot; label: tag: \u0026quot;request_logging\u0026quot; worker_size: 1 bulk_size_in_mb: 10 - name: logging_requests auto_start: true keep_running: true processor: - bulk_indexing: bulk: compress: true batch_size_in_mb: 10 batch_size_in_docs: 5000 consumer: fetch_max_messages: 100 queues: type: indexing_merge when: cluster_available: [ \u0026quot;logging-server\u0026quot; ] Parameter Description #     Name Type Description     input_queue string Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.   idle_timeout_in_seconds int Timeout duration of the consumption queue, in seconds. The default value is 5.   bulk_size_in_kb int Size of a bulk request, in KB.   bulk_size_in_mb int Size of a bulk request, in MB.   elasticsearch string Name of a target cluster, to which requests are saved.   index_name string Name of the index stored to the target cluster.   type_name string Name of the index type stored to the target cluster. It is set based on the cluster version. The value is doc for Elasticsearch versions earlier than v7 and _doc for versions later than v7.   output_queue.name string The name of output queue   output_queue.label map The labels assign to the output queue，with label type:indexing_merge builtin.   failure_queue string The name of queue to save failure requests   invalid_queue string The name of queue to save invalid requests    ","subcategory":null,"summary":"","tags":null,"title":"indexing_merge","url":"/gateway/v1.29.3/docs/references/processors/indexing_merge/"},{"category":null,"content":"index_diff #  Description #  The index_diff processor is used to compare differences between two result sets.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - index_diff: diff_queue: \u0026quot;diff_result\u0026quot; buffer_size: 1 text_report: true #If data needs to be saved to Elasticsearch, disable the function and start the diff_result_ingest task of the pipeline. source_queue: 'source_docs' target_queue: 'target_docs' Parameter Description #     Name Type Description     source_queue string Name of source data   target_queue string Name of target data   diff_queue string Queue that stores difference results   buffer_size int Memory buffer size   keep_source bool Whether difference results contain document source information   text_report bool Whether to output results in text form    ","subcategory":null,"summary":"","tags":null,"title":"index_diff","url":"/gateway/v1.29.3/docs/references/processors/index_diff/"},{"category":null,"content":"http #  Description #  The http filter is used to forward requests to a specified HTTP server as a proxy.\nConfiguration Example #  A simple example is as follows:\nflow: - name: default_flow filter: - basic_auth: valid_users: medcl: passwd - http: schema: \u0026quot;http\u0026quot; #https or http #host: \u0026quot;192.168.3.98:5601\u0026quot; hosts: - \u0026quot;192.168.3.98:5601\u0026quot; - \u0026quot;192.168.3.98:5602\u0026quot; Parameter Description #     Name Type Description     schema string http or https   host string Target host address containing the port ID, for example, localhost:9200   hosts array Host address list. The addresses are tried in sequence after a fault occurs.   skip_failure_host bool Skip hosts in failure, default true   max_connection_per_node int The max connections per node, default 5000   max_response_size int The max length of response supported   max_retry_times int The max num of retries, default 0   retry_delay_in_ms int The latency before next retry in millisecond, default 1000   skip_cleanup_hop_headers bool Remove Hop-by-hop Headers   max_conn_wait_timeout duration The max time wait to create new connections, default 30s   max_idle_conn_duration duration The max duration of idle connections, default 30s   max_conn_duration duration The max duration of keepalived connections, default 0s   timeout duration Request timeout duration, default 30s   read_timeout duration Read request timeout duration, default 0s   write_timeout duration Write request timeout duration, default 0s   read_buffer_size int Read buffer size, default 16384   write_buffer_size int Write buffer size, default 16384   tls_insecure_skip_verify bool Skip the TLS verification, default true    ","subcategory":null,"summary":"","tags":null,"title":"http","url":"/gateway/v1.29.3/docs/references/filters/http/"},{"category":null,"content":"hash_mod #  Description #  The hash_mod filter is used to obtain a unique partition number using the hash modulo of the request\u0026rsquo;s context. It is generally used for subsequent request forwarding.\nConfiguration Example #  A simple example is as follows:\nflow: - name: default_flow filter: - hash_mod: # Hash requests to different queues source: \u0026#34;$[[_ctx.remote_ip]]_$[[_ctx.request.username]]_$[[_ctx.request.path]]\u0026#34; target_context_name: \u0026#34;partition_id\u0026#34; mod: 10 # Hash to 10 partitions add_to_header: true - set_context: context: _ctx.request.header.X-Replicated-ID: $[[_util.increment_id.request_number_id]]_$[[_util.generate_uuid]] _ctx.request.header.X-Replicated-Timestamp: $[[_sys.unix_timestamp_of_now]] _ctx.request.header.X-Replicated: \u0026#34;true\u0026#34; Parameter Description #     Name Type Description     source string Input for the hash, supports variable parameters   target_context_name string The primary key name to store the partition number in the context   mod int Maximum number of partitions   add_to_request_header bool Whether to add to the request header, default is true, creating X-Partition-ID and X-Partition-Size headers   add_to_response_header bool Whether to add to the response header, default is false    ","subcategory":null,"summary":"","tags":null,"title":"hash_mod","url":"/gateway/v1.29.3/docs/references/filters/hash_mod/"},{"category":null,"content":"flow_runner #  Description #  The flow_runner processor is used to asynchronously consume requests in a queue by using the processing flow used for online requests.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - flow_runner: input_queue: \u0026quot;primary_deadletter_requests\u0026quot; flow: primary-flow-post-processing when: cluster_available: [ \u0026quot;primary\u0026quot; ] Parameter Description #     Name Type Description     input_queue string Name of a subscribed queue   flow string Flow used to consume requests in consumption queues   commit_on_tag string A message is committed only when a specified tag exists in the context of the current request. The default value is blank, indicating that a message is committed immediately after the execution is complete.    ","subcategory":null,"summary":"","tags":null,"title":"flow_runner","url":"/gateway/v1.29.3/docs/references/processors/flow_runner/"},{"category":null,"content":"flow_replay #  Description #  The flow_replay processor is used to asynchronously consume requests in the queue and use the asynchronous processing process for online requests to perform consumption processing.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: backup-flow-request-reshuffle auto_start: true keep_running: true singleton: true retry_delay_in_ms: 10 processor: - consumer: max_worker_size: 100 queue_selector: labels: type: \u0026quot;primary_write_ahead_log\u0026quot; consumer: group: request-reshuffle fetch_max_messages: 10000 fetch_max_bytes: 20485760 fetch_max_wait_ms: 10000 processor: - flow_replay: flow: backup-flow-request-reshuffle commit_on_tag: \u0026quot;commit_message_allowed\u0026quot; Parameter Description #     Name Type Description     message_field string The context field name that store the message obtained from the queue, default messages.   flow string Specify the flow to consume request messages in the queue.   commit_on_tag string Only when the specified tag appears in the context of the current request will the message be committed. The default is empty, which means the commit will be executed once completed.         ","subcategory":null,"summary":"","tags":null,"title":"flow_replay","url":"/gateway/v1.29.3/docs/references/processors/flow_replay/"},{"category":null,"content":"flow #  Description #  The flow filter is used to redirect to or execute one or a series of other flows.\nConfiguration Example #  A simple example is as follows:\nflow: - name: flow filter: - flow: flows: - request_logging Context mapped flow:\nflow: - name: dns-flow filter: - flow: ignore_undefined_flow: true context_flow: context: _ctx.request.host context_parse_pattern: (?P\u0026lt;uuid\u0026gt;^[0-9a-z_\\-]+)\\. flow_id_template: flow_$[[uuid]] - set_response: status: 503 content_type: application/json body: '{\u0026quot;message\u0026quot;:\u0026quot;invalid HOST\u0026quot;}' More information about context, please refer to Context .\nParameter Description #     Name Type Description     flow string Flow ID, the definition of how requests will be executed   flows array Flow ID, in the array format. You can specify multiple flows, which are executed in sequence   ignore_undefined_flow bool Ignore the undefined flow   context_flow.context string The context to use for mapping flow_id   context_flow.context_parse_pattern string The regexp pattern used to extract named variables from context value   context_flow.flow_id_template string The string template used to rendering flow_id   context_flow.continue string Will continue next filter after executed the context mapped flow, default false    ","subcategory":null,"summary":"","tags":null,"title":"flow","url":"/gateway/v1.29.3/docs/references/filters/flow/"},{"category":null,"content":"elasticsearch_health_check #  Description #  The elasticsearch_health_check filter is used to detect the health status of Elasticsearch in traffic control mode. When a back-end fault occurs, the filter triggers an active cluster health check without waiting for the results of the default polling check of Elasticsearch. Traffic control can be configured to enable the filter to send check requests to the back-end Elasticsearch at a maximum of once per second.\nConfiguration Example #  A simple example is as follows:\nflow: - name: elasticsearch_health_check filter: - elasticsearch_health_check: elasticsearch: dev Parameter Description #     Name Type Description     elasticsearch string Cluster ID   interval int Minimum interval for executing requests, in seconds. The default value is 1.    ","subcategory":null,"summary":"","tags":null,"title":"elasticsearch_health_check","url":"/gateway/v1.29.3/docs/references/filters/elasticsearch_health_check/"},{"category":null,"content":"elasticsearch #  Description #  The elasticsearch filter is used to forward requests to back-end Elasticsearch clusters.\nConfiguration Example #  Before using the elasticsearch filter, define one Elasticsearch cluster configuration node as follows:\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 The following shows a flow configuration example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod The preceding example forwards requests to the prod cluster.\nAutomatic Update #  For a large cluster that contains many nodes, it is almost impossible to configure all back-end nodes individually. Instead, you only need to enable auto-discovery of back-end nodes on the Elasticsearch module. See the following example.\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 discovery: enabled: true refresh: enabled: true basic_auth: username: elastic password: pass Then, enable automatic configuration refresh on the filter. Now, all back-end nodes can be accessed and the status of online and offline nodes is automatically updated. See the following example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod refresh: enabled: true interval: 30s Setting the Weight #  If there are many back-end clusters, INFINI Gateway allows you to set different access weights for different nodes. See the following configuration example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod balancer: weight refresh: enabled: true interval: 30s weights: - host: 192.168.3.201:9200 weight: 10 - host: 192.168.3.202:9200 weight: 20 - host: 192.168.3.203:9200 weight: 30 In the above example, the traffic destined for an Elasticsearch cluster is distributed to the 203, 202, and 201 nodes at a ratio of 3：2：1.\nFiltering Node #  INFINI Gateway can also filter requests based on node IP address, label, or role to avoid sending requests to specific nodes, such as the master and cold nodes. See the following configuration example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod balancer: weight refresh: enabled: true interval: 30s filter: hosts: exclude: - 192.168.3.201:9200 include: - 192.168.3.202:9200 - 192.168.3.203:9200 tags: exclude: - temp: cold include: - disk: ssd roles: exclude: - master include: - data - ingest Parameter Description #     Name Type Description     elasticsearch string Name of an Elasticsearch cluster   max_connection_per_node int Maximum number of TCP connections that are allowed to access each node of an Elasticsearch cluster. The default value is 5000.   max_response_size int Maximum size of the message body returned in response to an Elasticsearch request. The default value is 100*1024*1024.   max_conn_wait_timeout duration Timeout duration for Elasticsearch to wait for an idle connection. The default value is 30s.   max_idle_conn_duration duration Idle duration of an Elasticsearch connection. The default value is 30s.   max_retry_times duration Limit the number of retries on Elasticsearch errors, default 0   max_conn_duration duration Duration of an Elasticsearch connection. The default value is 0s.   timeout duration Timeout duration to wait for the response. The default value is 30s. Warning: timeout will not terminate the request, it will continue in the background. If response time is too long and the connection pool is full, try to set read_timeout.   dial_timeout duration Timeout duration to wait for dialing the remote host. The default value is 3s.   read_timeout duration Read timeout duration of an Elasticsearch request. The default value is 0s.   write_timeout duration Write timeout duration of an Elasticsearch request. The default value is 0s.   read_buffer_size int Read cache size for an Elasticsearch request. The default value is 4096*4.   write_buffer_size int Write cache size for an Elasticsearch request. The default value is 4096*4.   tls_insecure_skip_verify bool Whether to ignore TLS certificate verification of an Elasticsearch cluster. The default value is true.   max_retry_times int The maximum number of retry attempts for requests. The default value is 5.   retry_on_backend_failure bool Whether to retry requests when backend failures occur. Used to switch to another available host. The default value is true.   retry_readonly_on_backend_failure bool Whether to retry readonly requests (e.g., GET/HEAD) on backend failure. This is generally safe as it does not risk data duplication or corruption. The default value is true.   retry_writes_on_backend_failure bool Whether to retry write operations (e.g., POST/PUT/PATCH) on backend failure. Use with caution, as retries can lead to duplicate writes. Recommended to use with additional filters. The default value is false.   retry_on_backend_busy bool Whether to retry requests when the backend is busy with status code 429. This helps handle temporary overloads or throttling. The default value is false.   retry_delay_in_ms int The delay in milliseconds between retry attempts. Does not apply when switching hosts. The default value is 1000.   balancer string Load balancing algorithm of a back-end Elasticsearch node. Currently, only the weight weight-based algorithm is available.   skip_metadata_enrich bool Whether to skip the processing of Elasticsearch metadata and not add X-* metadata to the header of the request and response   refresh.enable bool Whether to enable automatic refresh of node status changes, to perceive changes in the back-end Elasticsearch topology   refresh.interval int Interval of the node status refresh   weights array Priority of a back-end node. A node with a larger weight is assigned a higher proportion of request forwarding.   filter object Filtering rules for back-end Elasticsearch nodes. Rules can be set to forward requests to a specific node.   filter.hosts object Filtering based on the access address of Elasticsearch   filter.tags object Filtering based on the label of Elasticsearch   filter.roles object Filtering based on the role of Elasticsearch   filter.*.exclude array Conditions for excluding. Any matched node is denied handling requests as a proxy.   filter.*.include array Elasticsearch nodes that meet conditions are allowed to handle requests as a proxy. When the exclude parameter is not configured but include is configured, any condition in include must be met. Otherwise, the node is not allowed to handle requests as a proxy.    ","subcategory":null,"summary":"","tags":null,"title":"elasticsearch","url":"/gateway/v1.29.3/docs/references/filters/elasticsearch/"},{"category":null,"content":"dump_hash #  Description #  The dump_hash processor is used to export index documents of a cluster and calculate the hash value.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - dump_hash: #dump es1's doc indices: \u0026quot;medcl-dr3\u0026quot; scroll_time: \u0026quot;10m\u0026quot; elasticsearch: \u0026quot;source\u0026quot; query: \u0026quot;field1:elastic\u0026quot; fields: \u0026quot;doc_hash\u0026quot; output_queue: \u0026quot;source_docs\u0026quot; batch_size: 10000 slice_size: 5 Parameter Description #     Name Type Description     elasticsearch string Name of a target cluster   scroll_time string Scroll session timeout duration   batch_size int Scroll batch size, which is set to 5000 by default   slice_size int Slice size, which is set to 1 by default   sort_type string Document sorting type, which is set to asc by default   sort_field string Document sorting field   indices string Index   level string Request processing level, which can be set to cluster, indicating that node- and shard-level splitting are not performed on requests. It is applicable to scenarios in which there is a proxy in front of Elasticsearch.   query string Query filter conditions   fields string List of fields to be returned   sort_document_fields bool Whether to sort fields in _source before the hash value is calculated. The default value is false.   hash_func string Hash function, which can be set to xxhash32, xxhash64, or fnv1a. The default value is xxhash32.   output_queue string Name of a queue that outputs results    ","subcategory":null,"summary":"","tags":null,"title":"dump_hash","url":"/gateway/v1.29.3/docs/references/processors/dump_hash/"},{"category":null,"content":"dump #  Description #  The dump filter is used to dump relevant request information on terminals. It is mainly used for debugging.\nConfiguration Example #  A simple example is as follows:\nflow: - name: hello_world filter: - dump: request: true response: true Parameter Description #  The dump filter is relatively simple. After the dump filter is inserted into a required flow handling phase, the terminal can output request information about the phase, facilitating debugging.\n   Name Type Description     request bool Whether to output all complete request information   response bool Whether to output all complete response information   uri bool Whether to output the requested URI information   query_args bool Whether to output the requested parameter information   user bool Whether to output the requested user information   api_key bool Whether to output the requested API key information   request_header bool Whether to output the header information of the request   response_header bool Whether to output the header information of the response   status_code bool Whether to output the status code of the response   context array User-defined context information for output    Outputting Context #  You can use the context parameter to debug request context information. The following is an example of the configuration file.\nflow: - name: echo filter: - set_response: status: 201 content_type: \u0026quot;text/plain; charset=utf-8\u0026quot; body: \u0026quot;hello world\u0026quot; - set_response_header: headers: - Env -\u0026gt; Dev - dump: context: - _ctx.id - _ctx.tls - _ctx.remote_addr - _ctx.local_addr - _ctx.request.host - _ctx.request.method - _ctx.request.uri - _ctx.request.path - _ctx.request.body - _ctx.request.body_length - _ctx.request.query_args.from - _ctx.request.query_args.size - _ctx.request.header.Accept - _ctx.request.user - _ctx.response.status - _ctx.response.body - _ctx.response.content_type - _ctx.response.body_length - _ctx.response.header.Env Start the gateway and run the following command:\ncurl http://localhost:8000/medcl/_search\\?from\\=1\\\u0026amp;size\\=100 -d'{search:query123}' -v -u 'medcl:123' The gateway outputs the following information:\n---- dumping context ---- _ctx.id : 21474836481 _ctx.tls : false _ctx.remote_addr : 127.0.0.1:50925 _ctx.local_addr : 127.0.0.1:8000 _ctx.request.host : localhost:8000 _ctx.request.method : POST _ctx.request.uri : http://localhost:8000/medcl/_search?from=1\u0026amp;size=100 _ctx.request.path : /medcl/_search _ctx.request.body : {search:query123} _ctx.request.body_length : 17 _ctx.request.query_args.from : 1 _ctx.request.query_args.size : 100 _ctx.request.header.Accept : */* _ctx.request.user : medcl _ctx.response.status : 201 _ctx.response.body : hello world _ctx.response.content_type : text/plain; charset=utf-8 _ctx.response.body_length : 11 _ctx.response.header.Env : Dev ","subcategory":null,"summary":"","tags":null,"title":"dump","url":"/gateway/v1.29.3/docs/references/filters/dump/"},{"category":null,"content":"drop #  Description #  The drop filter is used to discard a message and end the processing of a request in advance.\nConfiguration Example #  A simple example is as follows:\nflow: - name: drop filter: - drop: ","subcategory":null,"summary":"","tags":null,"title":"drop","url":"/gateway/v1.29.3/docs/references/filters/drop/"},{"category":null,"content":"date_range_precision_tuning #  Description #  The date_range_precision_tuning filter is used to reset the time precision for time range query. After the precision is adjusted, adjacent repeated requests initiated within a short period of time can be easily cached. For scenarios with low time precision but a large amount of data, for example, if Kibana is used for report analysis, you can reduce the precision to cache repeated query requests to reduce the pressure of the back-end server and accelerate the front-end report presentation.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - date_range_precision_tuning: time_precision: 4 - get_cache: - elasticsearch: elasticsearch: dev - set_cache: Precision Description #  Queries sent by Kibana to Elasticsearch use the current time (Now) by default, which is accurate to milliseconds. You can set different precision levels to rewrite queries. See the following query example:\n{\u0026quot;range\u0026quot;:{\u0026quot;@timestamp\u0026quot;:{\u0026quot;gte\u0026quot;:\u0026quot;2019-09-26T08:21:12.152Z\u0026quot;,\u0026quot;lte\u0026quot;:\u0026quot;2020-09-26T08:21:12.152Z\u0026quot;,\u0026quot;format\u0026quot;:\u0026quot;strict_date_optional_time\u0026quot;} Set different precision levels. The query results after rewriting are as follows:\n   Precision New Query     0 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T00:00:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T23:59:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   1 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T00:00:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T09:59:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   2 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:00:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:59:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   3 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:20:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:29:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   4 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   5 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:10.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:19.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   6 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   7 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.100Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.199Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   8 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.150Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.159Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   9 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.152Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.152Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}    Parameter Description #     Name Type Description     time_precision int Precision length of time, that is, the digit length of displayed time. The default value is 4 and the valid range is from 0 to 9.   path_keywords array Keyword contained in a request. The time precision is reset only for requests that contain the keywords, to prevent parsing of unnecessary requests. The default values are _search and _async_search.    ","subcategory":null,"summary":"","tags":null,"title":"date_range_precision_tuning","url":"/gateway/v1.29.3/docs/references/filters/date_range_precision_tuning/"},{"category":null,"content":"dag #  Description #  The dag processor is used to manage the concurrent scheduling of tasks.\nConfiguration Example #  The following example defines a service named racing_example and auto_start is set to true. Processing units to be executed in sequence are set in processor, the dag processor supports concurrent execution of multiple tasks and the wait_all and first_win aggregation modes.\npipeline: - name: racing_example auto_start: true processor: - echo: #ready, set, go message: read,set,go - dag: mode: wait_all #first_win, wait_all parallel: - echo: #player1 message: player1 - echo: #player2 message: player2 - echo: #player3 message: player3 end: - echo: #checking score message: checking score - echo: #announce champion message: 'announce champion' - echo: #done message: racing finished The echo processor above is very simple and is used to output a specified message. This pipeline simulates a race scene, in which players 1, 2, and 3 run at the same time. After they run, the scores are calculated and the winner is announced, and finally the completion information is output. The output of the program is as follows:\n[10-12 14:59:22] [INF] [echo.go:36] message:read,set,go [10-12 14:59:22] [INF] [echo.go:36] message:player1 [10-12 14:59:22] [INF] [echo.go:36] message:player2 [10-12 14:59:22] [INF] [echo.go:36] message:player3 [10-12 14:59:22] [INF] [echo.go:36] message:checking score [10-12 14:59:22] [INF] [echo.go:36] message:announce champion [10-12 14:59:22] [INF] [echo.go:36] message:racing finished Parameter Description #     Name Type Description     mode string Aggregation mode of task results. The value first_win indicates that the program continues further execution after any of the concurrent tasks is completed, and the value wait_all indicates that the program continues further execution only after all concurrent tasks are completed.   parallel array Task array list, in which multiple subtasks are defined in sequence.   end array Task array list, which lists tasks to be executed after concurrent tasks are completed.    ","subcategory":null,"summary":"","tags":null,"title":"dag","url":"/gateway/v1.29.3/docs/references/processors/dag/"},{"category":null,"content":"context_switch #  Description #  context_switch filter can be used to use context variables for conditional judgment and achieve flexible jumps.\nConfiguration Example #  A simple example is as follows:\nflow: - name: context_switch filter: - context_switch: context: logging.month default_flow: echo_message_not_found switch: - case: [\u0026quot;02\u0026quot;,\u0026quot;01\u0026quot;] action: redirect_flow flow: echo_message_01_02 - case: [\u0026quot;03\u0026quot;] action: redirect_flow flow: echo_message_03 Parameter Description #     Name Type Description     context string The name of context   skip_error bool Whether to ignore the error and returned directly, such like the context variable does not exist   default_action string Set the default action，could be redirect_flow or drop，default redirect_flow   default_flow string Set the default flow   stringify_value bool Whether to stringify the value，default true。   continue bool Whether to continue the flow after hit. Request returns immediately after it is set to false. The default value is false.   switch array Switched by some cases   switch[i].case []string Matched criteria   switch[i].action string The action when met the case，could be redirect_flow or drop，default redirect_flow   switch[i].flow string When action is redirect_flow，the flow to redirect，or will use the default flow    ","subcategory":null,"summary":"","tags":null,"title":"context_switch","url":"/gateway/v1.29.3/docs/references/filters/context_switch/"},{"category":null,"content":"context_regex_replace #  Description #  The context_regex_replace filter is used to replace and modify relevant information in the request context by using regular expressions.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - context_regex_replace: context: \u0026quot;_ctx.request.path\u0026quot; pattern: \u0026quot;^/\u0026quot; to: \u0026quot;/cluster:\u0026quot; when: contains: _ctx.request.path: /_search - dump: request: true This example replaces curl localhost:8000/abc/_search in requests with curl localhost:8000/cluster:abc/_search.\nParameter Description #     Name Type Description     context string Request context and corresponding key   pattern string Regular expression used for matching and replacement   to string Target string used for replacement    A list of context variables that can be modified is provided below:\n   Name Type Description     _ctx.request.uri string Complete URL of a request   _ctx.request.path string Request path   _ctx.request.host string Request host   _ctx.request.body string Request body   _ctx.request.body_json.[JSON_PATH] string Path to the JSON request object   _ctx.request.query_args.[KEY] string URL query request parameter   _ctx.request.header.[KEY] string Request header information   _ctx.response.header.[KEY] string Response header information   _ctx.response.body string Returned response body   _ctx.response.body_json.[JSON_PATH] string Path to the JSON response object    ","subcategory":null,"summary":"","tags":null,"title":"context_regex_replace","url":"/gateway/v1.29.3/docs/references/filters/context_regex_replace/"},{"category":null,"content":"context_parse #  Description #  context_parse filter is used to extract fields from context variables and store them in the context。\nConfiguration Example #  A simple example is as follows:\nflow: - name: context_parse filter: - context_parse: context: _ctx.request.path pattern: ^\\/.*?\\d{4}\\.(?P\u0026lt;month\u0026gt;\\d{2})\\.(?P\u0026lt;day\u0026gt;\\d{2}).*? group: \u0026quot;parsed_index\u0026quot; In above flow, the context_parse can extract fields from request：/abd-2023.02.06-abc/_search，get two new fields: parsed_index.month and parsed_index.day。\nParameter Description #     Name Type Description     context string Context variable   pattern string The regular expression used to extract the field   skip_error bool Whether to ignore the error and returned directly, such like the context variable does not exist   group string Set the group name, which the extracted fields can be placed under a separate group    ","subcategory":null,"summary":"","tags":null,"title":"context_parse","url":"/gateway/v1.29.3/docs/references/filters/context_parse/"},{"category":null,"content":"context_limiter #  Description #  The context_limiter filter is used to control the traffic based on request context.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: default_flow filter: - context_limiter: max_requests: 1 action: drop context: - _ctx.request.path - _ctx.request.header.Host - _ctx.request.header.Env The above configuration combines three context variables (_ctx.request.path, _ctx.request.header.Host, and _ctx.request.header.Env) into a bucket for traffic control. The allowable maximum queries per second (QPS) is 1 per second. Subsequent requests out of the traffic control range are directly denied.\nParameter Description #     Name Type Description     context array Context variables, which form a bucket key   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   burst_requests int Burst request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   burst_bytes int Burst request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_delay_in_ms int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached   log_warn_message bool Whether to log warn message    ","subcategory":null,"summary":"","tags":null,"title":"context_limiter","url":"/gateway/v1.29.3/docs/references/filters/context_limiter/"},{"category":null,"content":"context_filter #  Description #  The context_filter is used to filter traffic by request context.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - context_filter: context: _ctx.request.path message: \u0026quot;request not allowed.\u0026quot; status: 403 must: #must match all rules to continue prefix: - /medcl contain: - _search suffix: - _search wildcard: - /*/_search regex: - ^/m[\\w]+dcl must_not: # any match will be filtered prefix: - /.kibana - /_security - /_security - /gateway_requests* - /.reporting - /_monitoring/bulk contain: - _refresh suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/\\.m[\\w]+dcl should: prefix: - /medcl contain: - _search - _async_search suffix: - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl Parameter Description #     Name Type Description     context string Context variable   exclude array List of variables used to refuse requests to pass through   include array List of variables used to allow requests to pass through   must.* object Requests are allowed to pass through only when all conditions are met.   must_not.* object Requests are allowed to pass through only when none of the conditions are met.   should.* object Requests are allowed to pass through when any condition is met.   *.prefix array Whether a request begins with a specific character   *.suffix array Whether a request ends with a specific character   *.contain array Whether a request contains a specific character   *.wildcard array Whether a request meets pattern matching rules   *.regex array Whether a request meets regular expression matching rules   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If only the should condition is met, requests are allowed to pass through only when at least one item in should is met.\n","subcategory":null,"summary":"","tags":null,"title":"context_filter","url":"/gateway/v1.29.3/docs/references/filters/context_filter/"},{"category":null,"content":"consumer #  Description #  The consumer processor is used to consume messages recorded in the queue without processing them. Its purpose is to provide an entry point for data consumption pipeline, which will be further processed by subsequent processors.\nConfiguration Example #  Here is a simple configuration example:\npipeline: - name: consume_queue_messages auto_start: true keep_running: true retry_delay_in_ms: 5000 processor: - consumer: consumer: fetch_max_messages: 1 max_worker_size: 200 num_of_slices: 1 idle_timeout_in_seconds: 30 queue_selector: keys: - email_messages processor: - xxx1: - xxx2: In the above example, it subscribes to and consumes the email_messages queue. The queue messages are stored in the context of the current pipeline. The consumer provides a processor parameter, which contains a series of processors that will be executed sequentially. If any processor encounters an error during execution, the consumer will exit without committing the batch of data.\nParameter Description #     Name Type Description     message_field string The field name in the context where messages from the queue are stored. Default is messages.   max_worker_size int The maximum number of workers allowed to run simultaneously. Default is 10.   num_of_slices int The number of parallel threads for consuming a single queue. Maximum slice size at runtime.   slices array Allowed slice numbers as an integer array.   queue_selector.labels map Filter a group of queues to be consumed based on labels, similar to queues configuration.   queue_selector.ids array Specifies the UUIDs of the queues to be consumed, as a string array.   queue_selector.keys array Specifies the unique key paths of the queues to be consumed, as a string array.   queues map Filter a group of queues to be consumed based on labels, similar to queue_selector.labels configuration.   waiting_after array Whether to wait for specified queues to finish consumption before starting. UUIDs of the queues, as a string array.   idle_timeout_in_seconds int Timeout duration for consuming queues. Default is 5 seconds.   detect_active_queue bool Whether to automatically detect new queues that meet the conditions. Default is true.   detect_interval int Time interval in milliseconds for automatically detecting new queues that meet the conditions. Default is 5000.   quiet_detect_after_idle_in_ms bool Idle interval in milliseconds to exit automatic detection. Default is 30000.   skip_empty_queue bool Whether to skip consuming empty queues. Default is true.   quit_on_eof_queue bool Automatically quit consuming when reaching the last message of a queue. Default is true.   consumer.source string Consumer source.   consumer.id string Unique identifier for the consumer.   consumer.name string Consumer name.   consumer.group string Consumer group name.   consumer.fetch_min_bytes int Minimum size in bytes for fetching messages. Default is 1.   consumer.fetch_max_bytes int Maximum size in bytes for fetching messages. Default is 10485760, which is 10MB.   consumer.fetch_max_messages int Maximum number of messages to fetch. Default is 1.   consumer.fetch_max_wait_ms int Maximum wait time in milliseconds for fetching messages. Default is 10000.   consumer.eof_retry_delay_in_ms int Waiting time in milliseconds for retrying when reaching the end of a file. Default is 500.    ","subcategory":null,"summary":"","tags":null,"title":"consumer","url":"/gateway/v1.29.3/docs/references/processors/consumer/"},{"category":null,"content":"clone #  Description #  The clone filter is used to clone and forward traffic to another handling flow. It can implement dual-write, multi-write, multi-DC synchronization, cluster upgrade, version switching, and other requirements.\nConfiguration Example #  A simple example is as follows:\nflow: - name: double_write filter: - clone: flows: - write_to_region_a - write_to_region_b #last one's response will be output to client - name: write_to_region_a filter: - elasticsearch: elasticsearch: es1 - name: write_to_region_b filter: - elasticsearch: elasticsearch: es2 The above example copies Elasticsearch requests to two different remote clusters.\nParameter Description #     Name Type Description     flows array Multiple traffic handling flows, which are executed one after another. The result of the last flow is output to the client.   continue bool Whether to continue the previous flow after traffic is migrated. The gateway returns immediately after it is set to false. The default value is false.    ","subcategory":null,"summary":"","tags":null,"title":"clone","url":"/gateway/v1.29.3/docs/references/filters/clone/"},{"category":null,"content":"cache #  Description #  The cache filter is composed of the get_cache and set_cache filters, which need to be used in combination. The cache filter is used to cache accelerated queries, prevent repeated requests, and reduce the query pressure of back-end clusters.\nget_cache Filter #  The get_cache filter is used to acquire previous messages from the cache and return them to the client, without needing to access the back-end Elasticsearch. It is intended to cache hotspot data.\nA configuration example is as follows:\nflow: - name: get_cache filter: - get_cache: pass_patterns: [\u0026quot;_cat\u0026quot;,\u0026quot;scroll\u0026quot;, \u0026quot;scroll_id\u0026quot;,\u0026quot;_refresh\u0026quot;,\u0026quot;_cluster\u0026quot;,\u0026quot;_ccr\u0026quot;,\u0026quot;_count\u0026quot;,\u0026quot;_flush\u0026quot;,\u0026quot;_ilm\u0026quot;,\u0026quot;_ingest\u0026quot;,\u0026quot;_license\u0026quot;,\u0026quot;_migration\u0026quot;,\u0026quot;_ml\u0026quot;,\u0026quot;_rollup\u0026quot;,\u0026quot;_data_stream\u0026quot;,\u0026quot;_open\u0026quot;, \u0026quot;_close\u0026quot;] Parameter Description #     Name Type Description     pass_patterns string Rule for ignoring the cache for a request. The cache is skipped when the URL contains any defined keyword.    set_cache Filter #  The set_cache filter is used to cache results returned through back-end query. Expiration time can be set for the cache.\nA configuration example is as follows:\nflow: - name: get_cache filter: - set_cache: min_response_size: 100 max_response_size: 1024000 cache_ttl: 30s max_cache_items: 100000 Parameter Description #     Name Type Description     cache_type string Cache type. It can be set to ristretto, ccache, or redis, and the default value is ristretto.   cache_ttl string Expiration time of the cache. The default value is 10s.   async_search_cache_ttl string Expiration time of the cache for storing asynchronous request results. The default value is 10m.   min_response_size int Minimum message body size that meets cache requirements. The default value is -1, indicating an unlimited value.   max_response_size int Maximum message body size that meets cache requirements. The default value is the maximum value of the int parameter.   max_cached_item int Maximum number of messages that can be cached. The default value is 1000000. The value is valid when the cache type is ccache.   max_cached_size int Maximum cache memory overhead. The default value is 1000000000, that is, 1 GB. The value is valid when the cache type is ristretto.   validated_status_code array Request status code that is allowed to be cached. The default value is 200,201,404,403,413,400,301.    Other Parameters #  If you want to ignore caching, you can define no_cache in the URL parameters to cause the gateway to ignore caching. For example:\ncurl http://localhost:8000/_search?no_cache=true ","subcategory":null,"summary":"","tags":null,"title":"cache","url":"/gateway/v1.29.3/docs/references/filters/cache/"},{"category":null,"content":"bulk_response_process #  Description #  The bulk_response_process filter is used to process bulk requests of Elasticsearch.\nConfiguration Example #  A simple example is as follows:\nflow: - name: bulk_response_process filter: - bulk_response_process: success_queue: \u0026quot;success_queue\u0026quot; tag_on_success: [\u0026quot;commit_message_allowed\u0026quot;] Parameter Description #     Name Type Description     invalid_queue string Name of the queue that saves an invalid request. It is mandatory.   failure_queue string Name of the queue that saves a failed request. It is mandatory.   save_partial_success_requests bool Whether to save partially successful requests in bulk requests. The default value is false.   success_queue string Queue that saves partially successful requests in the bulk requests   continue_on_error bool Whether to continue to execute subsequent filters after an error occurs on a bulk request. The default value is false.   message_truncate_size int Truncation length of a bulk request error log. The default value is 1024.   safety_parse bool Whether to use a secure bulk metadata parsing method. The default value is true.   doc_buffer_size int Buffer size when an insecure bulk metadata parsing method is adopted. The default value is 256 * 1024.   tag_on_success array Specified tag to be attached to request context after all bulk requests are processed.   tag_on_error array Specified tag to be attached to request context after an error occurs on a request.   tag_on_partial array Specified tag to be attached to request context after requests in a bulk request are partially executed successfully.   tag_on_failure array Specified tag to be attached to request context after some requests in a bulk request fail (retry is supported).   tag_on_invalid array Specified tag to be attached to request context after an invalid request error occurs.   success_flow string Flow executed upon successful request   invalid_flow string Flow executed upon invalid request   failure_flow string Flow executed upon failed request    ","subcategory":null,"summary":"","tags":null,"title":"bulk_response_process","url":"/gateway/v1.29.3/docs/references/filters/bulk_response_process/"},{"category":null,"content":"bulk_reshuffle #  Description #  The bulk_reshuffle filter is used to parse batch requests of Elasticsearch based on document, sort out documents as needed, and archive and store them in queues. After documents are stored, the filter can rapidly return service requests, thereby decoupling front-end writing from back-end Elasticsearch clusters. The bulk_reshuffle filter needs to be used in combination with offline pipeline consumption tasks.\nWhen passing through queues generated by the bulk_reshuffle filter, metadata carries \u0026quot;type\u0026quot;: \u0026quot;bulk_reshuffle\u0026quot; and Elasticsearch cluster information such as \u0026quot;elasticsearch\u0026quot;: \u0026quot;dev\u0026quot;, by default. You can call APIs on the gateway to check metadata defined in queues. See the following example.\ncurl http://localhost:2900/queue/stats { \u0026quot;queue\u0026quot;: { \u0026quot;disk\u0026quot;: { \u0026quot;async_bulk-cluster##dev\u0026quot;: { \u0026quot;depth\u0026quot;: 0, \u0026quot;metadata\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;dynamic\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;c71f7pqi4h92kki4qrvg\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;async_bulk-cluster##dev\u0026quot;, \u0026quot;label\u0026quot;: { \u0026quot;elasticsearch\u0026quot;: \u0026quot;dev\u0026quot;, \u0026quot;level\u0026quot;: \u0026quot;cluster\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bulk_reshuffle\u0026quot; } } } } } } Node-Level Asynchronous Submission #  INFINI Gateway is capable of locally calculating the target storage location of a back-end Elasticsearch cluster corresponding to each index document so as to precisely locate requests. A batch of bulk requests may contain the data of multiple back-end nodes. The bulk_reshuffle filter is used to shuffle normal bulk requests and reassemble them based on target nodes or shards. The purpose is to prevent Elasticsearch nodes from distributing received requests, so as to reduce the traffic and load between Elasticsearch clusters. The filter also prevents a single node from becoming a bottleneck and ensures balanced processing of all data nodes, thereby improving the overall index throughput of clusters.\nDefining a Flow #  A simple example is as follows:\nflow: - name: online_indexing_merge filter: - bulk_reshuffle: elasticsearch: prod level: node #cluster,node,shard,partition - elasticsearch: elasticsearch: prod refresh: enabled: true interval: 30s elastic: enabled: true remote_configs: false health_check: enabled: true interval: 30s availability_check: enabled: true interval: 60s metadata_refresh: enabled: true interval: 30s cluster_settings_check: enabled: false interval: 20s The above configuration indicates that bulk requests will be split and reassembled based on the target nodes corresponding to index documents. Data is sent to local disk queues first and then consumed and submitted through separate tasks to the target Elasticsearch nodes.\nThe benefit of this filter is that a failure occurring on the back-end Elasticsearch cluster will not affect indexing operations because requests are stored in disk queues of the gateway and the front-end indexing is decoupled from back-end clusters. Therefore, when the back-end Elasticsearch cluster encounters a failure, restarts, or initiates version upgrade, normal index operations will not be affected.  Configuring a Consumption Pipeline #  After the gateway sends requests to the disk, a consumption queue pipeline needs to be configured as follows to submit data:\npipeline: - name: bulk_request_ingest auto_start: true processor: - bulk_indexing: queues: type: bulk_reshuffle level: node One pipeline task named bulk_request_ingest is used and the filter conditions for queues of to-be-subscribed targets are type: bulk_reshuffle and level: node. You can also set the batch size for bulk submission. In this way, node-level requests received by INFINI Gateway will be automatically sent to the corresponding Elasticsearch node.\nShard-Level Asynchronous Submission #  Shard-level asynchronous submission is suitable for scenarios in which the data amount of a single index is large and needs to be processed independently. An index is split into shards and then bulk requests are submitted in the form of shards, which further improves the processing efficiency of back-end Elasticsearch nodes.\nThe configuration is as follows:\nDefining a Flow #  flow: - name: online_indexing_merge filter: - bulk_reshuffle: elasticsearch: prod level: shard - elasticsearch: elasticsearch: prod refresh: enabled: true interval: 30s Set the assembly and disassembly level to the shard type.\nDefining a Pipeline #  pipeline: - name: bulk_request_ingest auto_start: true processor: - bulk_indexing: queues: type: bulk_reshuffle level: shard Compared with the preceding node-level configuration, the level parameter is modified to listen to shard-type disk queues. If there are many indexes, excess local disk queues will cause extra overhead. You are advised to enable this mode only for specific indexes whose throughput needs to be optimized.\nParameter Description #     Name Type Description     elasticsearch string Name of an Elasticsearch cluster instance.   level string Shuffle level of a request, that is, cluster level. The default value is cluster. It can be set to cluster, node, index, or shard.   queue_name_prefix string The prefix of default queue，The default value is async_bulk   partition_size int Maximum partition size. Partitioning is performed by document _id on the basis of level.   fix_null_id bool Whether to automatically generate a random UUID if no document ID is specified in the bulk index request document. It is applicable to data of the log type. The default value is true.   continue_metadata_missing bool If the node or shard information required by the context does not exist, whether to continue or skip process，default false   continue_after_reshuffle bool Whether to continue with the process after finished the reshuffle process，default false   index_stats_analysis bool Whether to record index name statistics to request logs. The default value is true.   action_stats_analysis bool Whether to record bulk request statistics to request logs. The default value is true.   shards array Index shards that can be processed. The value is a character array, for example, \u0026quot;0\u0026quot;. All shards are processed by default, and you can set specific shards to be processed.   tag_on_success array Specified tag to be attached to request context after all bulk requests are processed.    ","subcategory":null,"summary":"","tags":null,"title":"bulk_reshuffle","url":"/gateway/v1.29.3/docs/references/filters/bulk_reshuffle/"},{"category":null,"content":"bulk_request_throttle #  Description #  The bulk_request_throttle filter is used to limit the speed of Bulk requests to Elasticsearch.\nConfiguration Example #  A simple example is as follows:\nflow: - name: bulk_request_mutate filter: - bulk_request_throttle: indices: test: max_requests: 5 action: drop message: \u0026quot;test writing too fast。\u0026quot; log_warn_message: true filebeat-*: max_bytes: 512 action: drop message: \u0026quot;filebeat indices writing too fast。\u0026quot; log_warn_message: true Parameter Description #     Name Type Description     indices map The indices which wanted to throttle   indices.[NAME].interval string The unit time interval for evaluating the speed limit, default 1s   indices.[NAME].max_requests int Maximum request count limit in the interval   indices.[NAME].burst_requests int Burst request count limit in the interval   indices.[NAME].max_bytes int Maximum request traffic limit in the interval   indices.[NAME].burst_bytes int Burst request traffic limit in the interval   indices.[NAME].action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   indices.[NAME].status string Status code returned after traffic control conditions are met. The default value is 429.   indices.[NAME].message string Rejection message returned for a request, for which traffic control conditions are met   indices.[NAME].retry_delay_in_ms int Interval for traffic control retry, in milliseconds. The default value is 10.   indices.[NAME].max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   indices.[NAME].failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached   indices.[NAME].log_warn_message bool Whether to log warn message    ","subcategory":null,"summary":"","tags":null,"title":"bulk_request_throttle","url":"/gateway/v1.29.3/docs/references/filters/bulk_request_throttle/"},{"category":null,"content":"bulk_request_mutate #  Description #  The bulk_request_mutate filter is used to mutate bulk requests of Elasticsearch.\nConfiguration Example #  A simple example is as follows:\nflow: - name: bulk_request_mutate filter: - bulk_request_mutate: fix_null_id: true generate_enhanced_id: true # fix_null_type: true # default_type: m-type # default_index: m-index # index_rename: # \u0026quot;*\u0026quot;: index-new # index1: index-new # index2: index-new # index3: index3-new # index4: index3-new # medcl-dr3: index3-new # type_rename: # \u0026quot;*\u0026quot;: type-new # type1: type-new # type2: type-new # doc: type-new # doc1: type-new \u0026hellip; Parameter Description #\n    Name Type Description     fix_null_type bool Whether to fix a request that does not carry _type. It is used in collaboration with the default_type parameter.   fix_null_id bool Whether to fix a request that does not carry _id and generate a random ID, for example, c616rhkgq9s7q1h89ig0   remove_type bool Whether to remove the _type parameter. Elasticsearch versions higher than 8.0 do not support the _type parameter.   generate_enhanced_id bool Whether to generate an enhanced ID, such as c616rhkgq9s7q1h89ig0-1635937734071093-10.   default_index string Default index name, which is used if no index name is specified in metadata   default_type string Default document type, which is used if no document type is specified in metadata   index_rename map Index name used for renaming. You can use * to overwrite all index names.   type_rename map Type used for renaming. You can use * to overwrite all type names.   pipeline string pipeline parameter of a specified bulk request   remove_pipeline bool Whether to remove the pipeline parameter from the bulk request   safety_parse bool Whether to use a secure bulk metadata parsing method. The default value is true.   doc_buffer_size int Buffer size when an insecure bulk metadata parsing method is adopted. The default value is 256 * 1024.    ","subcategory":null,"summary":"","tags":null,"title":"bulk_request_mutate","url":"/gateway/v1.29.3/docs/references/filters/bulk_request_mutate/"},{"category":null,"content":"bulk_indexing #  Description #  The bulk_indexing processor is used to asynchronously consume bulk requests in queues.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - bulk_indexing: queue_selector.labels: type: bulk_reshuffle level: cluster Parameter Description #     Name Type Description     elasticsearch string The default Elasticsearch cluster ID, which will be used if elasticsearch is not specified in the queue Labels   idle_timeout_in_seconds int Timeout duration of the consumption queue, which is set to 1 by default.   max_connection_per_node int Maximum number of connections allowed by the target node. The default value is 1.   max_worker_size int The maximum size of workers allowed to run at the same time, default 10   bulk.batch_size_in_kb int Size of a bulk request, in KB.   bulk.batch_size_in_mb int Size of a bulk request, in MB.   bulk.batch_size_in_docs int Num of docs in bulk request, default 1000   bulk.compress bool Whether to enable request compression.   bulk.retry_delay_in_seconds int Waiting time for request retry, default 1.   bulk.reject_retry_delay_in_seconds int Waiting time for request rejection, default 1.   bulk.max_retry_times int Maximum retry count.   bulk.invalid_queue string Queue for storing requests, for which 4xx is returned because of invalid requests.   bulk.dead_letter_queue string Request queue, for which the maximum retry count is exceeded.   bulk.remove_duplicated_newlines bool Whether to remove duplicated newlines in bulk requests   queue_selector.labels map A group of queues filtered by label, in which data needs to be consumed. alias queues   queue_selector.ids array Specifies the UUID of the queue to consume, an array of string   queue_selector.keys array Specifies the unique Key path of the queue to consume, string array   queues map A group of queues filtered by label, equals to queue_selector.labels   waiting_after array Whether to wait for the specified queue to finish consumption before starting consumption, UUID of the queue, string array   detect_active_queue bool Whether to automatically detect new queues that meet the conditions, default true   detect_interval bool The time interval for automatically detecting new queues that meet the conditions, in milliseconds, default 5000   num_of_slices int Threads consuming a single queue in parallel, maximum slice size at runtime   slices array Allowed slice numbers, int array   skip_info_missing bool Whether to ignore queue data consumption when conditions are not met, for example, the node, index, or shard information does not exist, that is, whether to consume queue data after information is obtained. The default value is false. Otherwise, one Elasticsearch node is selected to send requests.   skip_empty_queue bool Whether to skip consumption of empty queue, default true   consumer.source string consumer source   consumer.id string consumer UUID   consumer.name string consumer name   consumer.group string consumer group name   consumer.fetch_min_bytes int Minimum size in bytes to pull messages, default 1   consumer.fetch_max_bytes int The maximum byte size of the pull message, the default is 10485760, which is 10MB   consumer.fetch_max_messages int Pull the maximum number of messages, default 1   consumer.fetch_max_wait_ms int Pull maximum waiting time, in milliseconds, default 10000   consumer.eof_retry_delay_in_ms int Retry interval when hit EOF, default 500   bulk.response_handle.save_success_results bool Whether to save success results，default false   bulk.response_handle.output_bulk_stats bool Whether to save bulk stats, default false   bulk.response_handle.include_index_stats bool Whether to include index stats，default true   bulk.response_handle.include_action_stats bool Whether to include action stats，default true   bulk.response_handle.save_error_results bool Whether to save error results，default true   bulk.response_handle.include_error_details bool Whether to save dedicate request level error messages，default true   bulk.response_handle.max_error_details_count bool The max count of error details，default 50   bulk.response_handle.save_busy_results bool Whether to save 429 results，default true   bulk.response_handle.bulk_result_message_queue string The queue to save bulk results，default bulk_result_messages   bulk.response_handle.max_request_body_size int Max size of request body before truncated，default 10240   bulk.response_handle.max_response_body_size int Max size of response body before truncated，default 10240   bulk.response_handle.retry_rules.retry_429 bool Whether to retry 429 requests，default true   bulk.response_handle.retry_rules.retry_4xx bool Whether to retry 4xx (except 429) requests，default false ` |   bulk.response_handle.retry_rules.default bool Whether to retry other requests not specified in retry_rules, defualt true   bulk.response_handle.retry_rules.permitted.status []int Retry requests with specified status codes   bulk.response_handle.retry_rules.permitted.keyword []string Retry when response contains specified keywords   bulk.response_handle.retry_rules.denied.status []int Don\u0026rsquo;t retry requests with specified status codes   bulk.response_handle.retry_rules.denied.keyword []string Don\u0026rsquo;t retry when response contains specified keywords    ","subcategory":null,"summary":"","tags":null,"title":"bulk_indexing","url":"/gateway/v1.29.3/docs/references/processors/bulk_indexing/"},{"category":null,"content":"basic_auth #  Description #  The basic_auth filter is used to verify authentication information of requests. It is applicable to simple authentication.\nConfiguration Example #  A simple example is as follows:\nflow: - name: basic_auth filter: - basic_auth: valid_users: medcl: passwd medcl1: abc ... Parameter Description #     Name Type Description     valid_users map Username and password    ","subcategory":null,"summary":"","tags":null,"title":"basic_auth","url":"/gateway/v1.29.3/docs/references/filters/basic_auth/"},{"category":null,"content":"auto_generate_doc_id #  Description #  The auto_generate_doc_id filter is used to add a UUID (Universally Unique Identifier) to a document when creating a document without specifying the UUID explicitly. This is typically used when you don\u0026rsquo;t want the backend system to generate the ID automatically. For example, if you want to replicate the document between clusters, it\u0026rsquo;s better to assign a known ID to the document instead of letting each cluster generate its own ID. Otherwise, this could result in inconsistencies between clusters.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test_auto_generate_doc_id filter: - auto_generate_doc_id: Parameter Description #     Name Type Description     prefix string Add a fixed prefix to the uuid    ","subcategory":null,"summary":"","tags":null,"title":"auto_generate_doc_id","url":"/gateway/v1.29.3/docs/references/filters/auto_generate_doc_id/"},{"category":null,"content":"Other Configurations #  Advanced Usage #  Templates #  Example:\nconfigs.template: - name: \u0026quot;es_gw1\u0026quot; path: ./sample-configs/config_template.tpl variable: name: \u0026quot;es_gw1\u0026quot; binding_host: \u0026quot;0.0.0.0:8000\u0026quot; tls_on_entry: true elasticsearch_endpoint: \u0026quot;http://localhost:9200\u0026quot;    Name Type Description     configs.template array Configuration templates, can specify multiple templates with corresponding parameters   configs.template[].name string Name of the configuration   configs.template[].path string Template configuration path   configs.template[].variable map Template parameter settings, variables in the template are used as $[[variable_name]]    Environment Variables #  The Gateway supports the use of environment variables for flexible parameter control within the configuration.\nFirst, define the default values for environment variables in the configuration, as follows:\nenv: PROD_ES_ENDPOINT: http://localhost:9200 PROD_ES_USER: elastic PROD_ES_PASS: password Then, you can use environment variables in the configuration using the following syntax:\nelasticsearch: - name: prod enabled: true endpoints: - $[[env.PROD_ES_ENDPOINT]] discovery: enabled: false basic_auth: username: $[[env.PROD_ES_USER]] password: $[[env.PROD_ES_PASS]] Note that external environment variables take precedence over internal environment variable settings in the configuration. For example, to override environment variables when starting the program, use the following command:\nPROD_ES_ENDPOINT=http://1.1.1.1:9200 LOGGING_ES_ENDPOINT=http://2.2.2.2:9201 ./bin/gateway Path #  Common configuration for paths, including data, log and config directories.\nExample:\npath.data: data path.logs: log path.configs: \u0026#34;config\u0026#34;    Name Type Description     path.data string Data directory, default is data.   path.logs string Log directory, default is log.   path.configs string Configuration directory, default is config.    Log #  The configuration for logs.\nExample:\nlog: level: info debug: false    Name Type Description     log.level string Log level, default is info.   log.debug bool Whether to enable debug mode. When enabled, the program exits immediately in case of an exception, printing the complete stack trace. Used for debugging and fault localization. Default is false, do not enable in production as it may result in data loss.   log.format bool Log format, default is [%Date(01-02) %Time] [%LEV] [%File:%Line] %Msg%n. Format References.   log.disable_file_output bool Whether to disable local file log output, default is false. Use this in container environments if you don\u0026rsquo;t want local log output.    Configs #  Manage the configuration.\nExample:\nconfigs: auto_reload: true managed: true panic_on_config_error: false interval: \u0026#34;1s\u0026#34; servers: - \u0026#34;http://localhost:9000\u0026#34; max_backup_files: 5 soft_delete: false tls: enabled: false cert_file: /etc/ssl.crt key_file: /etc/ssl.key skip_insecure_verify: false    Name Type Description     configs.auto_reload bool Whether it supports dynamically loading configuration files under the path.configs path.   configs.managed bool Whether configuration management by the configuration center is supported.   configs.servers []string Configuration center address   configs.interval string Configuration synchronization interval   configs.soft_delete bool The deletion of configuration files is soft deletion, default is true.   configs.panic_on_config_error bool If there is an error in configuration loading, it will crash directly, default true   configs.max_backup_files int The maximum number of configuration file backups, default 10.   configs.valid_config_extensions []string Valid configuration file suffixes, default .tpl, .json, .yml, .yaml   configs.tls object TLS Configuration (Please refer to TLS)   configs.always_register_after_restart bool Whether to register after the instance is restarted. When the instance runs in the K8S environment, this parameter needs to be enabled.   configs.allow_generated_metrics_tasks bool Allow automatic generation of collection metrics tasks.   configs.ignored_path []string Paths of configuration files that need to be ignored.    Local Disk Queue #  Example:\ndisk_queue: upload_to_s3: true s3: server: my_blob_store location: cn-beijing-001 bucket: infini-store max_bytes_per_file: 102400    Name Type Description     disk_queue.min_msg_size int Minimum byte limit for a single message sent to the queue, default is 1   disk_queue.max_msg_size int Maximum byte limit for a single message sent to the queue, default is 104857600 (100MB)   disk_queue.sync_every_records int Synchronization interval in terms of the number of records, default is 1000   disk_queue.sync_timeout_in_ms int Synchronization interval in milliseconds, default is 1000 milliseconds   disk_queue.max_bytes_per_file int Maximum size of a single file in the local disk queue. If exceeded, a new file is created. Default is 104857600 (100MB)   disk_queue.max_used_bytes int Maximum allowed storage space used by the local disk queue   disk_queue.warning_free_bytes int Free storage space threshold for disk space warnings, default is 10737418240 (10GB)   disk_queue.reserved_free_bytes int Protected value for free storage space on disk. Once reached, the disk becomes read-only and no more writes are allowed. Default is 5368709120 (5GB)   disk_queue.auto_skip_corrupted_file bool Whether to automatically skip corrupted disk files, default is true   disk_queue.upload_to_s3 bool Whether to upload disk queue files to S3, default is false   disk_queue.s3.async bool Whether to asynchronously upload to the S3 server   disk_queue.s3.server string S3 server ID   disk_queue.s3.location string S3 server location   disk_queue.s3.bucket string S3 server bucket   disk_queue.retention.max_num_of_local_files int Maximum number of files to retain locally after uploading to S3, default is 3   disk_queue.compress.segment.enabled bool Whether to enable file-level compression, default is false    S3 #  Example:\ns3: my_blob_store: endpoint: \u0026quot;192.168.3.188:9000\u0026quot; access_key: \u0026quot;admin\u0026quot; access_secret: \u0026quot;gogoaminio\u0026quot;    Name Type Description     s3.[id].endpoint string S3 server address   s3.[id].access_key string S3 server key   s3.[id].access_secret string S3 server secret key   s3.[id].token string S3 server token information   s3.[id].ssl bool Whether S3 server uses TLS   s3.[id].skip_insecure_verify bool Whether to skip TLS certificate verification for S3 server    Kafka #  The Gateway supports using distributed Kafka as a backend queue. The related parameters are as follows.\n   Name Type Description     kafka.enabled bool Whether the Kafka module is enabled   kafka.default bool Whether the Kafka module is the default queue implementation   kafka.num_of_partition int Default number of partitions, default is 1   kafka.num_of_replica int Default number of replicas, default is 1   kafka.producer_batch_max_bytes int Maximum size of the batch to submit, default is 50 * 1024 * 1024   kafka.max_buffered_records int Maximum number of buffered request records, default is 10000   kafka.manual_flushing bool Whether to enable manual flushing, default is false   kafka.brokers []string Server address information   kafka.username string User information   kafka.password string Password information    Badger #  Badger is a lightweight disk-based KeyValue storage engine used by the Gateway to implement the KV module.\n   Name Type Description     badger.enabled bool Whether to enable the KV module implemented by Badger, default is true   badger.single_bucket_mode bool Whether Badger module uses single bucket mode, default is true   badger.sync_writes bool Whether Badger module uses synchronous writes, default is false   badger.mem_table_size int64 Size of the in-memory table used by Badger module, default is 10 * 1024 * 1024 (10485760)   badger.value_log_file_size int64 Size of Badger module\u0026rsquo;s log files, default is 1\u0026lt;\u0026lt;30 - 1 (1GB)   badger.value_log_max_entries int64 Maximum number of log entries for Badger module, default is 1000000   badger.value_threshold int64 Value threshold for Badger module\u0026rsquo;s log files, default is 1048576 (1MB)   badger.num_mem_tables int64 Number of in-memory tables for Badger module, default is 1   badger.num_level0_tables int64 Number of Level0 in-memory tables for Badger module, default is 1    Resource Limitations #     Name Type Description     resource_limit.cpu.max_num_of_cpus int Maximum number of CPU cores allowed to be used, Linux only with taskset command available.   resource_limit.cpu.affinity_list string CPU affinity settings, e.g., 0,2,5 or 0-8, Linux only with taskset command available.   resource_limit.memory.max_in_bytes string the max size of Memory to use, soft limit only    Network Configuration #  Common network configurations.\n   Name Type Description     *.network.host string Network address listened to by the service, for example, 192.168.3.10   *.network.port int Port address listened to by the service, for example, 8000   *.network.binding string Network binding address listened to by the service, for example, 0.0.0.0:8000   *.network.publish string External access address listened to by the service, for example, 192.168.3.10:8000   *.network.reuse_port bool Whether to reuse the network port for multi-process port sharing   *.network.skip_occupied_port bool Whether to automatically skip occupied ports    TLS Configuration #  Example:\nweb: enabled: true embedding_api: true network: binding: $[[env.SERV_BINDING]] tls: enabled: false skip_insecure_verify: true default_domain: \u0026quot;api.coco.rs\u0026quot; auto_issue: enabled: true email: \u0026quot;hello@infinilabs.com\u0026quot; include_default_domain: true domains: - \u0026quot;www.coco.rs\u0026quot; provider: tencent_dns: secret_id: $[[keystore.TENCENT_DNS_ID]] #./bin/coco keystore add TENCENT_DNS_ID secret_key: $[[keystore.TENCENT_DNS_KEY]] #./bin/coco keystore add TENCENT_DNS_KEY Common TLS configurations.\n   Name Type Description     *.tls.enabled bool Whether TLS secure transmission is enabled or not, can auto generate cert files if not specified any cert files   *.tls.ca_file string Path to the public CA cert of the TLS security certificate   *.tls.cert_file string Path to the public key of the TLS security certificate   *.tls.key_file string Path to the private key of the TLS security certificate   *.tls.skip_insecure_verify bool Whether to ignore TLS certificate verification   *.tls.default_domain string The default domain for auto generated certs   *.tls.skip_domain_verify bool Whether to skip domain verify or not   *.tls.client_session_cache_size int Set the max cache of ClientSessionState entries for TLS session resumption    Auto-Issue TLS Certificates #  Both the api and web modules support auto-issuing TLS certificates via Let\u0026rsquo;s Encrypt. This feature can be configured under *.tls.auto_issue:\n   Name Type Description     *.tls.auto_issue.enabled bool Enables automatic issuance of TLS certificates using Let\u0026rsquo;s Encrypt.   *.tls.auto_issue.path string Directory path where auto-issued certificates should be stored.   *.tls.auto_issue.email string Contact email for certificate issuance notifications and expiry warnings.   *.tls.auto_issue.include_default_domain bool Whether to include the default_domain in the list of domains for auto-issuance.   *.tls.auto_issue.domains []string List of additional domains for which TLS certificates will be issued.   *.tls.auto_issue.provider object Specifies the DNS provider configuration for DNS-based domain validation.    DNS Provider Configuration (Tencent Cloud) #  To support DNS-based verification with Tencent Cloud, configure the following within *.tls.auto_issue.provider:\n   Name Type Description     tencent_dns.secret_id string Secret ID for Tencent Cloud API access.   tencent_dns.secret_key string Secret Key for Tencent Cloud API access.    To set up and store the Tencent Cloud credentials securely, use the keystore commands:\n./bin/coco keystore add TENCENT_DNS_ID ./bin/coco keystore add TENCENT_DNS_KEY API #     Name Type Description     api.enabled bool Whether to enable the API module, default is true   api.network object Networking config, please refer to common network configuration section   api.tls object TLS config, please refer to common TLS configuration section   api.security object Security config for API module   api.security.enabled bool Whether security is enabled or not   api.security.username string The username for security   api.security.password string The password for security   api.cors.allowed_origins []string The list of origins a cross-domain request can be executed from   api.websocket object Websocket config for API module   api.websocket.enabled object Whether websocket is enabled or not   api.websocket.permitted_hosts []string The list of hosts that permitted to access the websocket service   api.websocket.skip_host_verify bool Whether websocket skip verify the host or not    Metrics #  Configure collection of system metrics.\nExample:\nmetrics: enabled: true queue: metrics network: enabled: true summary: true details: true memory: metrics: - swap - memory disk: metrics: - iops - usage cpu: metrics: - idle - system - user - iowait - load    Name Type Description     enabled bool Whether to enable system metrics collection, default true.   queue string The queue name of metrics collection.   network object The Configuration of network metrics collection.   network.enabled bool Whether to enable network metrics collection, default true.   network.summary bool Whether to collect network summary metircs.   network.sockets bool Whether to collect network socket metircs.   network.throughput bool Whether to collect network throughput metircs.   network.details bool Whether to accumulate network IO metrics.   network.interfaces []string Specify the network interfaces to be collected, and all interfaces are default.   memory object The Configuration of memory metrics collection.   memory.enabled bool Whether to enable memory metrics collection, default true   memory.metrics []string Specified collection metrics, optional swap，memory   disk object The Configuration of disk metrics collection.   disk.metrics []string Specified collection metrics, optional usage，iops   cpu object The Configuration of cpu metrics collection.   cpu.metrics []string Specified collection metrics, optional idle，system，user，iowait，load    Node #  The configuration of node.\nExample:\nnode: major_ip_pattern: \u0026#34;.*\u0026#34; labels: env: dev tags: - linux - x86 - es7    Name Type Description     major_ip_pattern string If there are multiple IPs on the host, use a pattern to control which IP is the primary one, which is used for reporting during registration.   labels map Custom lables   tags []string Custom tags    Misc #     Name Type Description     preference.pipeline_enabled_by_default map Whether pipelines are enabled by default. If set to false, each pipeline must be explicitly configured with enabled set to true   allow_multi_instance bool Whether is allowed to start multiple instances with the same program, default false   skip_instance_detect bool Whether is allowed to skip instance detection, default false   max_num_of_instances int The maximum number of instances that the same program can run simultaneously, default 5    ","subcategory":null,"summary":"","tags":null,"title":"Other Configurations","url":"/gateway/v1.29.3/docs/references/config/"},{"category":null,"content":"Use JavaScript for complex query rewriting #  Here is a use case：\n How does the gateway support cross-cluster search? I want to achieve: the input search request is lp:9200/index1/_search these indices are on three clusters, so need search across these clusters, how to use the gateways to switch to lp:9200/cluster01:index1,cluster02,index1,cluster03:index1/_search? we don\u0026rsquo;t want to change the application side, there are more than 100 indices, the index name not strictly named as index1, may be multiple indices together。\n Though INFINI Gateway provide a filter content_regex_replace can implement regular expression replacement, but in this case the variable need to replace with multi parameters. It is more complex, there is no direct way to implement by regexp match and replace, so how do we do that?\nJavascript filter #  The answer is yes, we do have a way, in the above case, in theory we only need to match the index name index1 and replace 3 times by adding prefix cluster01:, cluster02: and cluster03:,\nBy using INFINI Gateway\u0026rsquo;s JavaScript filter, we can implement this easily.\nActually no matter how complex the business logic is, it can be implemented through the scripts, not one line of script, then two lines.\nDefine the scripts #  Let\u0026rsquo;s create a script file under the scripts subdirectory of the gateway data directory, as follows:\n➜ gateway ✗ tree data data └── gateway └── nodes └── c9bpg0ai4h931o4ngs3g ├── kvdb ├── queue ├── scripts │ └── index_path_rewrite.js └── stats The content of this script is as follows:\nfunction process(context) { var originalPath = context.Get(\u0026quot;_ctx.request.path\u0026quot;); var matches = originalPath.match(/\\/?(.*?)\\/_search/) var indexNames = []; if(matches \u0026amp;\u0026amp; matches.length \u0026gt; 1) { indexNames = matches[1].split(\u0026quot;,\u0026quot;) } var resultNames = [] var clusterNames = [\u0026quot;cluster01\u0026quot;, \u0026quot;cluster02\u0026quot;] if(indexNames.length \u0026gt; 0) { for(var i=0; i\u0026lt;indexNames.length; i++){ if(indexNames[i].length \u0026gt; 0) { for(var j=0; j\u0026lt;clusterNames.length; j++){ resultNames.push(clusterNames[j]+\u0026quot;:\u0026quot;+indexNames[i]) } } } } if (resultNames.length\u0026amp;gt;0){ var newPath=\u0026amp;quot;/\u0026amp;quot;+resultNames.join(\u0026amp;quot;,\u0026amp;quot;)+\u0026amp;quot;/_search\u0026amp;quot;; context.Put(\u0026amp;quot;_ctx.request.path\u0026amp;quot;,newPath); }  } Like normal JavaScript, define a specific function process to handle context information inside the request, _ctx.request.path is a variable of the gateway’s built-in context to get the path of the request, and then use function context.Get(\u0026quot;_ctx.request.path\u0026quot;) to access this field inside the script.\nIn the script we used general regular expression for matching and characters process, did some character stitching, got a new path variable newPath , and finally used context.Put(\u0026quot;_ctx.request.path\u0026quot;,newPath) to update the request path back to context.\nFor more information about fields of request context please visit: Request Context\nGateway Configuration #  Next, create a gateway configuration and reference the script using a javascript filter as follows\nentry: - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 flow:\n name: default_flow filter:  dump: context: - _ctx.request.path javascript: file: index_path_rewrite.js dump: context:  _ctx.request.path   elasticsearch: elasticsearch: dev router:   name: my_router default_flow: default_flow  elasticsearch:\n name: dev enabled: true schema: http hosts:  192.168.3.188:9206 In the above example, a javascript filter with file specified as index_path_rewrite.js, and two dump filters are used for debugging, also used one elasticsearch filter to forward requests to ElasticSearch for queries.\n    Start Gateway #  Let\u0026rsquo;s start the gateway to have a test:\n➜ gateway ✗ ./bin/gateway ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, 2022-04-18 07:11:09, 2023-12-31 10:10:10, 8062c4bc6e57a3fefcce71c0628d2d4141e46953 [04-19 11:41:29] [INF] [app.go:174] initializing gateway. [04-19 11:41:29] [INF] [app.go:175] using config: /Users/medcl/go/src/infini.sh/gateway/gateway.yml. [04-19 11:41:29] [INF] [instance.go:72] workspace: /Users/medcl/go/src/infini.sh/gateway/data/gateway/nodes/c9bpg0ai4h931o4ngs3g [04-19 11:41:29] [INF] [app.go:283] gateway is up and running now. [04-19 11:41:30] [INF] [api.go:262] api listen at: http://0.0.0.0:2900 [04-19 11:41:30] [INF] [entry.go:312] entry [my_es_entry] listen at: http://0.0.0.0:8000 [04-19 11:41:30] [INF] [module.go:116] all modules are started [04-19 11:41:30] [INF] [actions.go:349] elasticsearch [dev] is available Testing #\n Run the following query to verify the query results, as shown below:\ncurl localhost:8000/abc,efg/_search You can see debugging information output by the gateway through the dump filter\n---- DUMPING CONTEXT ---- _ctx.request.path : /abc,efg/_search ---- DUMPING CONTEXT ---- _ctx.request.path : /cluster01:abc,cluster02:abc,cluster01:efg,cluster02:efg/_search The query criteria have been rewritten according to our requirements,Nice!\nRewrite the DSL #  All right, we did change the request url, is that also possible to change the request body, like the search QueryDSL?\nLet\u0026rsquo;s do this:\nfunction process(context) { var originalDSL = context.Get(\u0026quot;_ctx.request.body\u0026quot;); if (originalDSL.length \u0026gt;0){ var jsonObj=JSON.parse(originalDSL); jsonObj.size=123; jsonObj.aggs= { \u0026quot;test1\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;size\u0026quot;: 10 } } } context.Put(\u0026quot;_ctx.request.body\u0026quot;,JSON.stringify(jsonObj)); } } Testing:\n curl -XPOST localhost:8000/abc,efg/_search -d'{\u0026quot;query\u0026quot;:{}}' Output:\n---- DUMPING CONTEXT ---- _ctx.request.path : /abc,efg/_search _ctx.request.body : {\u0026quot;query\u0026quot;:{}} [04-19 18:14:24] [INF] [reverseproxy.go:255] elasticsearch [dev] hosts: [] =\u0026gt; [192.168.3.188:9206] ---- DUMPING CONTEXT ---- _ctx.request.path : /abc,efg/_search _ctx.request.body : {\u0026quot;query\u0026quot;:{},\u0026quot;size\u0026quot;:123,\u0026quot;aggs\u0026quot;:{\u0026quot;test1\u0026quot;:{\u0026quot;terms\u0026quot;:{\u0026quot;field\u0026quot;:\u0026quot;abc\u0026quot;,\u0026quot;size\u0026quot;:10}}}} Look, we just unlock the new world, agree?\nConclusion #  By using the Javascript filter in INFINI Gateway, it can be very flexible and easily to perform the complex logical operations and rewrite the Elasticsearch QueryDSL to meet your business needs.\n","subcategory":null,"summary":"","tags":null,"title":"Use JavaScript for complex query rewriting","url":"/gateway/v1.29.3/docs/tutorial/path_rewrite_by_javascript/"},{"category":null,"content":"Unified access indices from different clusters in Kibana #  Now there is such a demand, customers need to divide the data according to the business dimension, the index is split into three different clusters, to split the large cluster into multiple small clusters have many benefits, such as reduced coupling, bringing benefits to cluster availability and stability, but also to avoid the impact of a single business hotspot to affect other services, although splitting the cluster is a very common way to play, but the management is not so convenient, especially when querying data, it may be need to access the three sets of clusters separately APIs, even to switch between three different sets of Kibana to access the cluster\u0026rsquo;s data, is there a way to seamlessly unite them together?\nA gateway! #  The answer is naturally yes, by switching the Elasticsearch address of Kibana to the address of the INFINI Gateway, we can intelligently route requests according to the index, that is, when accessing different business indexes, they will be intelligently routed to different clusters, as shown in the following figure:\nAbove, we have three different indexes：\n apm-* erp-* mall-*  Each corresponds to three different sets of Elasticsearch clusters:\n ES1-APM ES2-ERP ES3-MALL  Now let\u0026rsquo;s see how to configure the INFINI Gateway to meet this business requirements:\nConfigure clusters #  First configure the connection information for the three clusters.\nelasticsearch: - name: es1-apm enabled: true endpoints: - http://192.168.3.188:9206 - name: es2-erp enabled: true endpoints: - http://192.168.3.188:9207 - name: es3-mall enabled: true endpoints: - http://192.168.3.188:9208 Configure Flow #  then, we define three flows that are used to access three different Elasticsearch clusters, as shown below:\nflow: - name: es1-flow filter: - elasticsearch: elasticsearch: es1-apm - name: es2-flow filter: - elasticsearch: elasticsearch: es2-erp - name: es3-flow filter: - elasticsearch: elasticsearch: es3-mall Then define a flow for path rule and forwarding, as follows:\n - name: default-flow filter: - switch: remove_prefix: false path_rules: - prefix: \u0026quot;apm-\u0026quot; flow: es1-flow - prefix: \u0026quot;erp-\u0026quot; flow: es2-flow - prefix: \u0026quot;mall-\u0026quot; flow: es3-flow - flow: #default flow flows: - es1-flow Match different indexes based on the index prefix in the request path and forward to different flows.\nConfigure Router #  Next, we define the routing information as follows:\nrouter: - name: my_router default_flow: default-flow Point to the default flow defined above to unify the processing of requests.\nConfigure Entrypoint #  Finally, we define a service that listening on port 8000 to provide unified access to Kibana, as follows:\nentry: - name: es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 Full Configuration #  The final complete configuration is as follows:\npath.data: data path.logs: log entry:\n name: es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000  flow:\n name: default-flow filter:  switch: remove_prefix: false path_rules: - prefix: \u0026quot;apm-\u0026quot; flow: es1-flow - prefix: \u0026quot;erp-\u0026quot; flow: es2-flow - prefix: \u0026quot;mall-\u0026quot; flow: es3-flow flow: #default flow flows: - es1-flow   name: es1-flow filter:  elasticsearch: elasticsearch: es1-apm   name: es2-flow filter:  elasticsearch: elasticsearch: es2-erp   name: es3-flow filter:  elasticsearch: elasticsearch: es3-mall    router:\n name: my_router default_flow: default-flow  elasticsearch:\n name: es1-apm enabled: true endpoints:  http://192.168.3.188:9206   name: es2-erp enabled: true endpoints:  http://192.168.3.188:9207   name: es3-mall enabled: true endpoints:  http://192.168.3.188:9208 Start Gateway #     Start the gateway as follows:\n➜ gateway git:(master) ✗ ./bin/gateway -config sample-configs/elasticsearch-route-by-index.yml  / _ \\ /\\ /__ /__/ / /\\ \\ /\\ /_/\n/ ////\\ / //\\ \\ / / //\\_ / / /\\/ _ / / //__ \\ /\\ / _ /\n____/_/ _// __/ / /_/ _/_/\n[GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, 2022-04-20 08:23:56, 2023-12-31 10:10:10, 51650a5c3d6aaa436f3c8a8828ea74894c3524b9 [04-21 13:41:21] [INF] [app.go:174] initializing gateway. [04-21 13:41:21] [INF] [app.go:175] using config: /Users/medcl/go/src/infini.sh/gateway/sample-configs/elasticsearch-route-by-index.yml. [04-21 13:41:21] [INF] [instance.go:72] workspace: /Users/medcl/go/src/infini.sh/gateway/data/gateway/nodes/c9bpg0ai4h931o4ngs3g [04-21 13:41:21] [INF] [app.go:283] gateway is up and running now. [04-21 13:41:21] [INF] [api.go:262] api listen at: http://0.0.0.0:2900 [04-21 13:41:21] [INF] [reverseproxy.go:255] elasticsearch [es1-apm] hosts: [] =\u0026gt; [192.168.3.188:9206] [04-21 13:41:21] [INF] [reverseproxy.go:255] elasticsearch [es2-erp] hosts: [] =\u0026gt; [192.168.3.188:9207] [04-21 13:41:21] [INF] [reverseproxy.go:255] elasticsearch [es3-mall] hosts: [] =\u0026gt; [192.168.3.188:9208] [04-21 13:41:21] [INF] [actions.go:349] elasticsearch [es2-erp] is available [04-21 13:41:21] [INF] [actions.go:349] elasticsearch [es1-apm] is available [04-21 13:41:21] [INF] [entry.go:312] entry [es_entry] listen at: http://0.0.0.0:8000 [04-21 13:41:21] [INF] [module.go:116] all modules are started [04-21 13:41:21] [INF] [actions.go:349] elasticsearch [es3-mall] is available [04-21 13:41:55] [INF] [reverseproxy.go:255] elasticsearch [es1-apm] hosts: [] =\u0026gt; [192.168.3.188:9206] After the gateway successfully started, you can access the target Elasticsearch cluster through the gateway’s IP+ port 8000.\nTesting #  Let\u0026rsquo;s start with the API access test, as follows:\n➜ ~ curl http://localhost:8000/apm-2022/_search -v * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET /apm-2022/_search HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Thu, 21 Apr 2022 05:45:44 GMT \u0026lt; content-type: application/json; charset=UTF-8 \u0026lt; Content-Length: 162 \u0026lt; X-elastic-product: Elasticsearch \u0026lt; X-Backend-Cluster: es1-apm \u0026lt; X-Backend-Server: 192.168.3.188:9206 \u0026lt; X-Filters: filters-\u0026gt;elasticsearch \u0026lt; * Connection #0 to host localhost left intact {\u0026quot;took\u0026quot;:142,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:{\u0026quot;value\u0026quot;:0,\u0026quot;relation\u0026quot;:\u0026quot;eq\u0026quot;},\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}}% You can see that apm-2022 points to the backend ES1-APM cluster.\nTo continue testing, access to the ERP index as follows:\n➜ ~ curl http://localhost:8000/erp-2022/_search -v * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET /erp-2022/_search HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Thu, 21 Apr 2022 06:24:46 GMT \u0026lt; content-type: application/json; charset=UTF-8 \u0026lt; Content-Length: 161 \u0026lt; X-Backend-Cluster: es2-erp \u0026lt; X-Backend-Server: 192.168.3.188:9207 \u0026lt; X-Filters: filters-\u0026gt;switch-\u0026gt;filters-\u0026gt;elasticsearch-\u0026gt;skipped \u0026lt; * Connection #0 to host localhost left intact {\u0026quot;took\u0026quot;:12,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:{\u0026quot;value\u0026quot;:0,\u0026quot;relation\u0026quot;:\u0026quot;eq\u0026quot;},\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}}% Great!\nLet\u0026rsquo;s continue testing, access to the mall index as follows:\n➜ ~ curl http://localhost:8000/mall-2022/_search -v * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET /mall-2022/_search HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Thu, 21 Apr 2022 06:25:08 GMT \u0026lt; content-type: application/json; charset=UTF-8 \u0026lt; Content-Length: 134 \u0026lt; X-Backend-Cluster: es3-mall \u0026lt; X-Backend-Server: 192.168.3.188:9208 \u0026lt; X-Filters: filters-\u0026gt;switch-\u0026gt;filters-\u0026gt;elasticsearch-\u0026gt;skipped \u0026lt; * Connection #0 to host localhost left intact {\u0026quot;took\u0026quot;:8,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:5,\u0026quot;successful\u0026quot;:5,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:0,\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}}% Perfect!\nAnother Option #  Besides using the switch filter, it is possible to use the path rules of the router itself, as shown in the following example configuration:\nflow: - name: default_flow filter: - echo: message: \u0026quot;hello world\u0026quot; - name: mall_flow filter: - echo: message: \u0026quot;hello mall indices\u0026quot; - name: apm_flow filter: - echo: message: \u0026quot;hello apm indices\u0026quot; - name: erp_flow filter: - echo: message: \u0026quot;hello erp indices\u0026quot; router: - name: my_router default_flow: default_flow rules: - method: - \u0026quot;*\u0026quot; pattern: - \u0026quot;/apm-{suffix:.*}/\u0026quot; - \u0026quot;/apm-{suffix:.*}/{any:.*}\u0026quot; flow: - apm_flow - method: - \u0026quot;*\u0026quot; pattern: - \u0026quot;/erp-{suffix:.*}/\u0026quot; - \u0026quot;/erp-{suffix:.*}/{any:.*}\u0026quot; flow: - erp_flow - method: - \u0026quot;*\u0026quot; pattern: - \u0026quot;/mall-{suffix:.*}/\u0026quot; - \u0026quot;/mall-{suffix:.*}/{any:.*}\u0026quot; flow: - mall_flow INFINI Gateway has many powerful features, and there are many ways to achieve your need, go explore it by yourself.\nModify Kibana Configuration #  Modify the Kibana configuration file kibana.yml, replace the address of Elasticsearch with the gateway address (HTTP: 192.168.3.200/8000), as shown below:\nelasticsearch.hosts: [\u0026quot;http://192.168.3.200:8000\u0026quot;] Restart the Kibana。\nVisit Kibana #  As you can see, in the Kibana developer tool we can already perform read and write operations from three different clusters as if it were one cluster.\nConclusion #  Through the INFINI Gateway, we can be very flexible for online traffic editing, dynamic combine requests of different cluster operations together on the fly.\n","subcategory":null,"summary":"","tags":null,"title":"Unified access indexes from different clusters in Kibana","url":"/gateway/v1.29.3/docs/tutorial/routing_to_cluser_by_index/"},{"category":null,"content":"Integration with Prometheus #  Infini Gateway supports outputting metrics in Prometheus format, which is convenient for integration with Prometheus.\nStats API #  Access Gateway\u0026rsquo;s API endpoint, with URL parameter as below:\nhttp://localhost:2900/stats?format=prometheus ➜ ~ curl http://localhost:2900/stats\\?format\\=prometheus buffer_fasthttp_resbody_buffer_acquired{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 1 buffer_stats_acquired{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 7 buffer_stats_max_count{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 0 system_cpu{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 0 buffer_bulk_request_docs_acquired{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 1 buffer_fasthttp_resbody_buffer_inuse{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 0 stats_gateway_request_bytes{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 0 system_mem{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 31473664 ... By providing parameter format=prometheus, the Prometheus format will be outputted.\nConfigure Prometheus #  Edit config file: prometheus.yml\n# my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093\nLoad rules once and periodically evaluate them according to the global \u0026lsquo;evaluation_interval\u0026rsquo;. rule_files:\n- \u0026quot;first_rules.yml\u0026quot; - \u0026quot;second_rules.yml\u0026quot; A scrape configuration containing exactly one endpoint to scrape: Here it\u0026rsquo;s Prometheus itself. scrape_configs:\n job_name: \u0026quot;prometheus\u0026quot; scrape_interval: 5s metrics_path defaults to \u0026lsquo;/metrics\u0026rsquo; metrics_path: /stats params: format: [\u0026lsquo;prometheus\u0026rsquo;]\nscheme defaults to \u0026lsquo;http\u0026rsquo;. static_configs:\n targets: [\u0026quot;localhost:2900\u0026quot;] labels: group: \u0026lsquo;infini\u0026rsquo; Start Prometheus #     The metrics success scraped.\nThen you can continuously check the running status of the gateway.\n","subcategory":null,"summary":"","tags":null,"title":"Integration with Prometheus","url":"/gateway/v1.29.3/docs/tutorial/prometheus_integration/"},{"category":null,"content":"Integrate with Elasticsearch-Hadoop #  Elasticsearch-Hadoop utilizes a seed node to access all back-end Elasticsearch nodes by default. The hotspots and requests may be improperly allocated. To improve the resource utilization of back-end Elasticsearch nodes, you can implement precision routing for the access to Elasticsearch nodes through INFINI Gateway.\nWrite Acceleration #  If you import data by using Elasticsearch-Hadoop, you can modify the following parameters of Elasticsearch-Hadoop to access INFINI Gateway, so as to improve the write throughput:\n   Name Type Description     es.nodes string List of addresses used to access the gateway, for example, localhost:8000,localhost:8001   es.nodes.discovery bool When it is set to false, the sniff mode is not adopted and only the configured back-end nodes are accessed.   es.nodes.wan.only bool When it is set to true, it indicates the proxy mode, in which data is forcibly sent through the gateway address.   es.batch.size.entries int Batch document quantity. Set the parameter to a larger value to improve throughput, for example, 5000.   es.batch.size.bytes string Batch transmission size. Set the parameter to a larger value to improve throughput, for example, 20mb.   es.batch.write.refresh bool Set it to false to prevent active refresh and improve throughput.    Related Link #    Elasticsearch-Hadoop Configuration Parameter Document  ","subcategory":null,"summary":"","tags":null,"title":"Integrate with Elasticsearch-Hadoop","url":"/gateway/v1.29.3/docs/tutorial/es-hadoop_integration/"},{"category":null,"content":"Handle Count Structure of Different Elasticsearch Versions #  To optimize performance in Elasticsearch 7.0 and later versions, search result matches are not accurately counted and the search result response body is adjusted. This will inevitably cause incompatibility with existing code. How can the problem be fixed quickly?\nStructure Diff #  The search structure difference is as follows:\nThe search structure used by Elasticsearch before version 7.0 is as follows. total shows a specific value.\n{ \u0026quot;took\u0026quot;: 53, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: 0, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] } } The search structure used by Elasticsearch 7.0 and later versions is as follows. total shows a group of description scope objects.\n{ \u0026quot;took\u0026quot;: 3, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: 1, \u0026quot;hits\u0026quot;: [] } } Parameters Provided by Elasticsearch #  Elasticsearch 7.0 provides a parameter to accurately control the count. In other words, rest_total_hits_as_int=true can be added to the query request URL parameter so that the old structure is used. It is disabled by default.\nDocument URL: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html\nHowever, you need to modify the program to add this parameter, and you may need to adjust the back-end code, front-end paging, and presentation. The modification workload may not be small.\nUsing INFINI Gateway for Quick Fixing #  If you do not want to modify the program, you can use INFINI Gateway to quickly repair the query and add query parameters to a search query. In addition, INFINI Gateway can be used to limit the request sources for which query parameters are to be added. For example, request sources can be adjusted only for specific service calling parties. The following uses the curl command as an example to add query parameters only to queries from the curl debugging.\nentry: - name: es_entrypoint enabled: true router: default network: binding: 0.0.0.0:8000 router:\n name: default default_flow: main_flow  flow:\n name: main_flow filter:  set_request_query_args: args:  rest_total_hits_as_int -\u0026gt; true when: and:  contains: _ctx.request.path: \u0026quot;_search\u0026quot; equals: _ctx.request.header.User-Agent: \u0026quot;curl/7.54.0\u0026quot;     record: stdout: true elasticsearch: elasticsearch: es-server dump: response_body: true    elasticsearch:\n name: es-server enabled: true endpoints:  http://192.168.3.188:9206    The final effect is as follows:\nFigure 1 shows the search result returned after the gateway is accessed through a browser. Figure 2 shows the search result returned by the curl command. The User-Agent header information can match the curl command and only parameters are added to the search conditions to avoid affecting other requests.\n","subcategory":null,"summary":"","tags":null,"title":"Handle Count Structure of Different Elasticsearch Versions","url":"/gateway/v1.29.3/docs/tutorial/fix_count_in_search_response/"},{"category":null,"content":"Enable HTTPS/TLS + Basic Auth for Elasticsearch easily #  If you have multiple Elasticsearch versions or your version is out of date, or if you do not set TLS or identity, then anyone can directly access Elasticsearch. You can use INFINI Gateway to quickly fix this issue.\nDefine an Elasticsearch resource #  Let\u0026rsquo;s define the Elasticsearch resources, config as bellow：\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 The prod refer to http://192.168.3.201:9200\nAnd then, we will need to use a filter to forward requests to that Elasticsearch，which name is prod：\n - elasticsearch: elasticsearch: prod For more options of this elasticsearch filter, please refer to documentation： elasticsearch filter\nAdd basic_auth filter #  In order to perform access control of elasticsearch, we are using a basic_auth filter for example:\n - basic_auth: valid_users: medcl: passwd The only valid user defined in above configuration.\nEnable TLS #  Enable auth, but do not enable the TLS, it is useless, because HTTP is a clear text transmission protocol, which can easily leak the passwords, enable the TLS is quite simple, jut define a entry as below:\n - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 tls: enabled: true You can try visit https://localhost:8000 to access the prod Elasticsearch cluster now。\nNote that the listening address here is \u0026lsquo;0.0.0.0\u0026rsquo;, which means that the IP on all the network cards on the machine are listening. For security reasons, you may need to change to listen only on local addresses or specified NIC IP addresses.\nCompatible with HTTP access #  If there are legacy systems that cannot switch to HTTPS, we can leverage gateway to provide plain HTTP access too:\n - name: my_unsecure_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8001 tls: enabled: false By visit http://localhost:8001 you can access the prod cluster too。\nFull configuration #  elasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 entry:\n name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 tls: enabled: true name: my_unsecure_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8001 tls: enabled: false  flow:\n name: default_flow filter:  basic_auth: valid_users: medcl: passwd elasticsearch: elasticsearch: prod router:   name: my_router default_flow: default_flow Showcase #   You will a valid user to access Elasticsearch now：\n","subcategory":null,"summary":"","tags":null,"title":"Enable HTTPS/TLS + Basic Auth for Elasticsearch easily","url":"/gateway/v1.29.3/docs/tutorial/proxy_elasticsearch/"},{"category":null,"content":"Adding a TLS and Basic Security for Kibana #  If you have multiple Kibana versions or your Kibana version is out of date, or if you do not set TLS or identity, then anyone can directly access Kibana. You can use the INFINI Gateway to quickly fix this issue.\nUsing the HTTP Filter to Forward Requests #   - http: schema: \u0026quot;http\u0026quot; #https or http host: \u0026quot;192.168.3.188:5602\u0026quot; Adding Authentication #   - basic_auth: valid_users: medcl: passwd Replacing Static Resources in the Router #   - method: - GET pattern: - \u0026quot;/plugins/kibanaReact/assets/illustration_integrations_lightmode.svg\u0026quot; flow: - replace_logo_flow Enabling TLS #   - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 tls: enabled: true Complete Configuration #  entry: - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 tls: enabled: true flow:\n name: logout_flow filter:  set_response: status: 401 body: \u0026quot;Success logout!\u0026quot; drop:   name: replace_logo_flow filter:  redirect: uri: https://elasticsearch.cn/uploads/event/20211120/458c74ca3169260dbb2308dd06ef930a.png   name: default_flow filter:  basic_auth: valid_users: medcl: passwd http: schema: \u0026quot;http\u0026quot; #https or http host: \u0026quot;192.168.3.188:5602\u0026quot; router:   name: my_router default_flow: default_flow rules:  method:  GET POST pattern: \u0026quot;/_logout\u0026quot; flow: logout_flow   method:  GET pattern: \u0026quot;/plugins/kibanaReact/assets/illustration_integrations_lightmode.svg\u0026quot; flow: replace_logo_flow Effect #       To access Kibana through INFINI Gateway, you need to log in as follows:\nAfter login, you will find that resources in Kibana are also replaced. See the figure below.\nProspect #  We can explore other benefits of by using INFINI Gateway, for example, we can use the INFINI Gateway to replace the static assets, like logo, JS, and CSS style in Kibana, or use the combination of JS and CSS to dynamically add navigation and pages or advanced visualization.\n","subcategory":null,"summary":"","tags":null,"title":"Adding a TLS and Basic Security for Kibana","url":"/gateway/v1.29.3/docs/tutorial/proxy_kibana/"},{"category":null,"content":"Benchmark Testing #  You are advised to use the Elasticsearch-dedicated benchmark tool Loadgen to test the gateway performance.\nHighlights of Loadgen:\n Robust performance Lightweight and dependency-free Random selection of template-based parameters High concurrency Balanced traffic control at the benchmark end Validate server responses.   Download URL: http://release.infinilabs.com/loadgen/\n Loadgen #  Loadgen is easy to use. After the tool is downloaded and decompressed, two files are obtained: one executable program and one configuration file loadgen.yml. An example of the configuration file is as follows:\nenv: ES_USERNAME: elastic ES_PASSWORD: elastic runner: # total_rounds: 1 no_warm: false log_requests: false assert_invalid: false assert_error: false variables: - name: ip type: file path: test/ip.txt - name: user type: file path: test/user.txt - name: id type: sequence - name: uuid type: uuid - name: now_local type: now_local - name: now_utc type: now_utc - name: now_unix type: now_unix requests: - request: method: GET basic_auth: username: $[[env.ES_USERNAME]] password: $[[env.ES_PASSWORD]] url: http://localhost:8000/medcl/_search body: '{ \u0026quot;query\u0026quot;: {\u0026quot;match\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;$[[user]]\u0026quot; }}}' Runner Configurations #  By default, loadgen will run under the benchmarking mode, repeating through all the requests during the specified duration (-d). If you only need to test the responses, setting runner.total_rounds: 1 will let loadgen run for only once.\nHTTP Headers Canonization #  By default, loadgen will canonilize the HTTP response header keys received from the server side (user-agent: xxx -\u0026gt; User-Agent: xxx). If you need to assert the header keys exactly, you can set runner.disable_header_names_normalizing: true to disable this behavior.\nUsage of Variables #  In the above configuration, variables is used to define variable parameters and variables are identified by name. In a constructed request, $[[Variable name]] can be used to access the value of the variable. Supported variable types are as follows:\n   Type Description Parameters     file Load variables from file path: the path of the data files\ndata: a list of values, will get appended to the end of the data specified by path file   list Defined variables inline use data to define a string array   sequence 32-bit Variable of the auto incremental numeric type from: the minimum of the values\nto: the maximum of the values   sequence64 64-bit Variable of the auto incremental numeric type from: the minimum of the values\nto: the maximum of the values   range Variable of the range numbers, support parameters from and to to define the range from: the minimum of the values\nto: the maximum of the values   random_array Generate a random array from the variable specified by variable_key variable_key: the variable name for the source of array values\nsize: the size of array\nsquare_bracket: true/false, whether to add [] for the outputed array\nstring_bracket: the string to surround the outputed elements.   uuid Variable of the UUID character type    now_local Current time and local time zone    now_utc Current time and UTC time zone    now_unix Current time and Unix timestamp    now_with_format Current time，support parameter format to customize the output format， eg: 2006-01-02T15:04:05-0700 format: the format of the time output ( Example)    Examples #  Variable parameters of the file type are loaded from an external text file. One variable parameter occupies one line. When one variable of the file type is accessed, one variable value is taken randomly. An example of the variable format is as follows:\n➜ loadgen git:(master) ✗ cat test/user.txt medcl elastic Tips about how to generate a random string of fixed length, such as 1024 per line:\nLC_CTYPE=C tr -dc A-Za-z0-9_\\!\\@\\#\\$\\%\\^\\\u0026amp;\\*\\(\\)-+= \u0026lt; /dev/random | head -c 1024 \u0026gt;\u0026gt; 1k.txt Environment Variables #  loadgen supporting loading and using environment variables in loadgen.yml, you can specify the default values in env configuration. loadgen will overwrite the variables at runtime if they\u0026rsquo;re also specified by the command-line environment.\nThe environment variables can be access by $[[env.ENV_KEY]]:\n# Default values for the environment variables. env: ES_USERNAME: elastic ES_PASSWORD: elastic ES_ENDPOINT: http://localhost:8000 requests: - request: method: GET basic_auth: username: $[[env.ES_USERNAME]] # Use environment variables password: $[[env.ES_PASSWORD]] # Use environment variables url: $[[env.ES_ENDPOINT]]/medcl/_search # Use environment variables body: '{ \u0026quot;query\u0026quot;: {\u0026quot;match\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;$[[user]]\u0026quot; }}}' Request Definition #  The requests node is used to set requests to be executed by Loadgen in sequence. Loadgen supports fixed-parameter requests and requests constructed using template-based variable parameters. The following is an example of a common query request.\nrequests: - request: method: GET basic_auth: username: elastic password: pass url: http://localhost:8000/medcl/_search?q=name:$[[user]] In the above query, Loadgen conducts queries based on the medcl index and executes one query based on the name field. The value of each request is from the random variable user.\nSimulating Bulk Ingestion #  It is very easy to use Loadgen to simulate bulk ingestion. Configure one index operation in the request body and then use the body_repeat_times parameter to randomly replicate several parameterized requests to complete the preparation of a batch of requests. See the following example.\n - request: method: POST basic_auth: username: test password: testtest url: http://localhost:8000/_bulk body_repeat_times: 1000 body: | { \u0026quot;index\u0026quot; : { \u0026quot;_index\u0026quot; : \u0026quot;medcl-y4\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;$[[uuid]]\u0026quot; } } { \u0026quot;id\u0026quot; : \u0026quot;$[[id]]\u0026quot;,\u0026quot;field1\u0026quot; : \u0026quot;$[[user]]\u0026quot;,\u0026quot;ip\u0026quot; : \u0026quot;$[[ip]]\u0026quot;,\u0026quot;now_local\u0026quot; : \u0026quot;$[[now_local]]\u0026quot;,\u0026quot;now_unix\u0026quot; : \u0026quot;$[[now_unix]]\u0026quot; } Response Assertions #  You can use the assert configuration to check the response values. assert now supports most of all the condition checkers of INFINI Gateway.\nrequests: - request: method: GET basic_auth: username: elastic password: pass url: http://localhost:8000/medcl/_search?q=name:$[[user]] assert: equals: _ctx.response.status: 201 The response value can be accessed from the _ctx value, currently it contains these values:\n   Parameter Description     _ctx.response.status HTTP response status code   _ctx.response.header HTTP response headers   _ctx.response.body HTTP response body text   _ctx.response.body_json If the HTTP response body is a valid JSON string, you can access the JSON fields by body_json   _ctx.elapsed The time elapsed since request sent to the server (milliseconds)    If the request failed (e.g. the host is not reachable), loadgen will record it under Number of Errors as part of the testing output. If you configured runner.assert_error: true, loadgen will exit as exit(2) when there\u0026rsquo;re any requests failed.\nIf the assertion failed, loadgen will record it under Number of Invalid as part of the testing output and skip the subsequent requests in this round. If you configured runner.assert_invalid: true, loadgen will exit as exit(1) when there\u0026rsquo;re any assertions failed.\nDynamic Variable Registration #  Each request can use register to dynamically set the variables based on the response value, a common usage is to update the parameters of the later requests based on the previous responses.\nIn the below example, we\u0026rsquo;re registering the response value _ctx.response.body_json.test.settings.index.uuid of the $[[env.ES_ENDPOINT]]/test to the index_id variable, then we can access it by $[[index_id]].\nrequests: - request: method: GET url: $[[env.ES_ENDPOINT]]/test assert: equals: _ctx.response.status: 200 register: - index_id: _ctx.response.body_json.test.settings.index.uuid Benchmark Test #  Run Loadgen to perform the benchmark test as follows:\n➜ loadgen git:(master) ✗ ./bin/loadgen -d 30 -c 100 -compress __ ___ _ ___ ___ __ __ / / /___\\/_\\ / \\/ _ \\ /__\\/\\ \\ \\ / / // ///_\\\\ / /\\ / /_\\//_\\ / \\/ / / /__/ \\_// _ \\/ /_// /_\\\\//__/ /\\ / \\____|___/\\_/ \\_/___,'\\____/\\__/\\_\\ \\/ [LOADGEN] A http load generator and testing suit. [LOADGEN] 1.0.0_SNAPSHOT, 83f2cb9, Sun Jul 4 13:52:42 2021 +0800, medcl, support single item in dict files [07-19 16:15:00] [INF] [instance.go:24] workspace: data/loadgen/nodes/0 [07-19 16:15:00] [INF] [loader.go:312] warmup started [07-19 16:15:00] [INF] [app.go:306] loadgen now started. [07-19 16:15:00] [INF] [loader.go:316] [GET] http://localhost:8000/medcl/_search [07-19 16:15:00] [INF] [loader.go:317] status: 200,\u0026lt;nil\u0026gt;,{\u0026quot;took\u0026quot;:1,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:{\u0026quot;value\u0026quot;:0,\u0026quot;relation\u0026quot;:\u0026quot;eq\u0026quot;},\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}} [07-19 16:15:00] [INF] [loader.go:316] [GET] http://localhost:8000/medcl/_search?q=name:medcl [07-19 16:15:00] [INF] [loader.go:317] status: 200,\u0026lt;nil\u0026gt;,{\u0026quot;took\u0026quot;:1,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:{\u0026quot;value\u0026quot;:0,\u0026quot;relation\u0026quot;:\u0026quot;eq\u0026quot;},\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}} [07-19 16:15:01] [INF] [loader.go:316] [POST] http://localhost:8000/_bulk [07-19 16:15:01] [INF] [loader.go:317] status: 200,\u0026lt;nil\u0026gt;,{\u0026quot;took\u0026quot;:120,\u0026quot;errors\u0026quot;:false,\u0026quot;items\u0026quot;:[{\u0026quot;index\u0026quot;:{\u0026quot;_index\u0026quot;:\u0026quot;medcl-y4\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;,\u0026quot;_id\u0026quot;:\u0026quot;c3qj9123r0okahraiej0\u0026quot;,\u0026quot;_version\u0026quot;:1,\u0026quot;result\u0026quot;:\u0026quot;created\u0026quot;,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:2,\u0026quot;successful\u0026quot;:1,\u0026quot;failed\u0026quot;:0},\u0026quot;_seq_no\u0026quot;:5735852,\u0026quot;_primary_term\u0026quot;:3,\u0026quot;status\u0026quot;:201}}]} [07-19 16:15:01] [INF] [loader.go:325] warmup finished\n5253 requests in 32.756483336s, 524.61KB sent, 2.49MB received\n[Loadgen Client Metrics] Requests/sec:\t175.10 Request Traffic/sec:\t17.49KB Total Transfer/sec:\t102.34KB Avg Req Time:\t5.711022ms Fastest Request:\t440.448µs Slowest Request:\t3.624302658s Number of Errors:\t0 Number of Invalid:\t0 Status 200:\t5253\n[Estimated Server Metrics] Requests/sec:\t160.37 Transfer/sec:\t93.73KB Avg Req Time:\t623.576686ms Loadgen executes all requests once to warm up before the formal benchmark test. If an error occurs, a prompt is displayed, asking you whether to continue. The warm-up request results are also output to the terminal. After execution, an execution summary is output. You can set runner.no_warm: true to skip the warm-up stage.\n The final results of Loadgen are the cumulative statistics after all requests are executed, and they may be inaccurate. You are advised to start the Kibana dashboard to check all operating indicators of Elasticsearch in real time.\n CLI Parameters #  Loadgen cyclically executes requests defined in the configuration file. By default, Loadgen runs for 5s and then automatically exits. If you want to prolong the running time or increase the concurrency, you can set the tool\u0026rsquo;s startup parameters. The help commands are as follows:\n➜ loadgen git:(master) ✗ ./bin/loadgen --help Usage of ./bin/loadgen: -c int Number of concurrent threads (default 1) -compress Compress requests with gzip -config string the location of config file, default: loadgen.yml (default \u0026quot;loadgen.yml\u0026quot;) -d int Duration of tests in seconds (default 5) -debug run in debug mode, loadgen will quit with panic error -l int Limit total requests (default -1) -log string the log level,options:trace,debug,info,warn,error (default \u0026quot;info\u0026quot;) -r int Max requests per second (fixed QPS) (default -1) -v\tversion Limiting the Client Workload #  You can use Loadgen and set the CLI parameter -r to restrict the number of requests that can be sent by the client per second, so as to evaluate the response time and load of Elasticsearch under fixed pressure. See the following example.\n➜ loadgen git:(master) ✗ ./bin/loadgen -d 30 -c 100 -r 100  Note: The client throughput limit may not be accurate enough in the case of massive concurrencies.\n Limiting the Total Number of Requests #  You can set the -l parameter to control the total number of requests that can be sent by the client, so as to generate a fixed number of documents. Modify the configuration as follows:\nrequests: - request: method: POST basic_auth: username: test password: testtest url: http://localhost:8000/medcl-test/doc2/_bulk body_repeat_times: 1 body: | { \u0026quot;index\u0026quot; : { \u0026quot;_index\u0026quot; : \u0026quot;medcl-test\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;$[[uuid]]\u0026quot; } } { \u0026quot;id\u0026quot; : \u0026quot;$[[id]]\u0026quot;,\u0026quot;field1\u0026quot; : \u0026quot;$[[user]]\u0026quot;,\u0026quot;ip\u0026quot; : \u0026quot;$[[ip]]\u0026quot; } Configured parameters use the content of only one document for each request. Then, the system executes Loadgen.\n./bin/loadgen -config loadgen-gw.yml -d 600 -c 100 -l 50000 After execution, 50000 records are added for the Elasticsearch index medcl-test.\nUsing Auto Incremental IDs to Ensure the Document Sequence #  If the IDs of generated documents need to increase regularly to facilitate comparison, you can use the auto incremental IDs of the sequence type as the primary key and avoid using random numbers in the content. See the following example.\nrequests: - request: method: POST basic_auth: username: test password: testtest url: http://localhost:8000/medcl-test/doc2/_bulk body_repeat_times: 1 body: | { \u0026quot;index\u0026quot; : { \u0026quot;_index\u0026quot; : \u0026quot;medcl-test\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;$[[id]]\u0026quot; } } { \u0026quot;id\u0026quot; : \u0026quot;$[[id]]\u0026quot; } Reuse variables in Request Context #  In a request, we might want use the same variable value, such as the routing parameter to control the shard destination, also store the field in the JSON document. You can use runtime_variables to set request-level variables, or runtime_body_line_variables to define request-body-level variables. If the request body set body_repeat_times, each line will be different, as shown in the following example:\nvariables: - name: id type: sequence - name: uuid type: uuid - name: now_local type: now_local - name: now_utc type: now_utc - name: now_unix type: now_unix - name: suffix type: range from: 10 to: 15 requests: - request: method: POST runtime_variables: batch_no: id runtime_body_line_variables: routing_no: uuid basic_auth: username: ingest password: password #url: http://localhost:8000/_search?q=$[[id]] url: http://192.168.3.188:9206/_bulk body_repeat_times: 10 body: | { \u0026quot;create\u0026quot; : { \u0026quot;_index\u0026quot; : \u0026quot;test-$[[suffix]]\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;$[[uuid]]\u0026quot; , \u0026quot;routing\u0026quot; : \u0026quot;$[[routing_no]]\u0026quot; } } { \u0026quot;id\u0026quot; : \u0026quot;$[[uuid]]\u0026quot;,\u0026quot;routing_no\u0026quot; : \u0026quot;$[[routing_no]]\u0026quot;,\u0026quot;batch_number\u0026quot; : \u0026quot;$[[batch_no]]\u0026quot;, \u0026quot;random_no\u0026quot; : \u0026quot;$[[suffix]]\u0026quot;,\u0026quot;ip\u0026quot; : \u0026quot;$[[ip]]\u0026quot;,\u0026quot;now_local\u0026quot; : \u0026quot;$[[now_local]]\u0026quot;,\u0026quot;now_unix\u0026quot; : \u0026quot;$[[now_unix]]\u0026quot; } We defined the batch_no variable to represent the same batch number in a batch of documents, and the routing_no variable to represent the routing value at each document level.\nCustomize Header #  requests: - request: method: GET url: http://localhost:8000/test/_search headers: - Agent: \u0026quot;Loadgen-1\u0026quot; disable_header_names_normalizing: false By default, loadgen will canonilize the HTTP header keys before sending the request (user-agent: xxx -\u0026gt; User-Agent: xxx), if you need to set the header keys exactly as is, set disable_header_names_normalizing: true.\nRunning Testing Suites #  If you want to run multiple loadgen test cases against different environments, you can use loadrun to ease the process.\n Download URL: http://release.infinilabs.com/loadrun/\n After the tool is downloaded and decompressed, two files are obtained: one executable program loadrun and one configuration file loadrun.yml. An example of the configuration file is as follows:\nenv: LR_LOADGEN_CMD: ./bin/loadgen # The path to the executable of loadgen tool. LR_TEST_DIR: ./testing # The path to the test cases. # If you want to start gateway dynamically and automatically: LR_GATEWAY_CMD: ./bin/gateway # The path to the executable of INFINI Gateway LR_GATEWAY_HOST: 0.0.0.0:18000 # The binding host of the INFINI Gateway LR_GATEWAY_API_HOST: 0.0.0.0:19000 # The binding host of the INFINI Gateway API server # Set up other envrionments for the gateway and loadgen LR_ELASTICSEARCH_ENDPOINT: http://localhost:19201 CUSTOM_ENV: myenv tests: # The relative path of test cases under `LR_TEST_DIR` # # - gateway.yml: (Optional) the configuration to start the INFINI Gateway dynamically. # - loadgen.yml: the configuration to run the loadgen tool. # # The environments set in `env` section will be passed to the INFINI Gateway and loadgen. - path: cases/gateway/echo/echo_with_context Environment Variables #  loadrun controls the INFINI Gateway and loadgen by environment variables, you can set these environment variables in the env section of the YAML configuration. The following variables are required:\n   Variable Description     LR_TEST_DIR The parent directory of all test cases   LR_LOADGEN_CMD The path to the loadgen executable    If you need loadrun to start INFINI Gateway instances dynamically, the following variables are required:\n   Variable Description     LR_GATEWAY_CMD The path to the INFINI Gateway executable   LR_GATEWAY_HOST The binding host:port of INFINI Gateway entry   LR_GATEWAY_API_HOST The binding host:port of INFINI Gateway API    Preparing Test Cases #  You can add multiple test cases under the tests section, each path points to the relative path of the test case directory. loadrun will read the configuration gateway.yml (optional) and loadgen.yml in each directory.\nAll environment variables configured in the env section can be accessed within gateway.yml and loadgen.yml.\nA sample configuraiton of gateway.yml:\npath.data: data path.logs: log entry:\n name: my_es_entry enabled: true router: my_router max_concurrency: 200000 network: binding: $[[env.LR_GATEWAY_HOST]]  flow:\n name: hello_world filter:  echo: message: \u0026lsquo;hello world\u0026rsquo; router:   name: my_router default_flow: hello_world A sample configuraiton of loadgen.yml:\n  runner: total_rounds: 1 no_warm: true log_requests: true assert_invalid: true assert_error: true requests: - request: method: GET url: http://$[[env.LR_GATEWAY_HOST]]/ assert: and: - equals: _ctx.response.status: 200 - equals: _ctx.response.body: 'hello world' Executing the Test Cases #\n Once the loadrun.yml is ready, you can run loadrun with the command below:\nloadrun -config ./loadrun.yml loadrun will execute the test cases one by one, and output the testing result:\n __ ___ _ ___ ___ __ __ / / /___\\/_\\ / \\/ _ \\ /__\\/\\ \\ \\ / / // ///_\\\\ / /\\ / /_\\//_\\ / \\/ / / /__/ \\_// _ \\/ /_// /_\\\\//__/ /\\ / \\____|___/\\_/ \\_/___,'\\____/\\__/\\_\\ \\/ [LOAD-RUNNER] A testing suite runner [LOAD-RUNNER] 1.0.0_SNAPSHOT#001, 2023-02-21 02:49:57, 2023-12-31 10:10:10, b427e3657a1336b4839a7eff59f79f8e334f3934 [02-21 10:50:05] [INF] [app.go:192] initializing loadrun [02-21 10:50:05] [INF] [app.go:193] using config: /Users/kassian/Workspace/infini/src/infini.sh/testing/suites/dev.yml [02-21 10:50:05] [INF] [instance.go:78] workspace: /Users/kassian/Workspace/infini/src/infini.sh/testing/data/loadrun/nodes/cfpihf15k34iqhpd4d00 [02-21 10:50:05] [INF] [app.go:399] loadrun is up and running now. [2023-02-21 10:50:05][TEST][SUCCESS] [setup/loadgen/cases/dummy] duration: 105(ms)\n1 requests in 68.373875ms, 0.00bytes sent, 0.00bytes received\n[Loadgen Client Metrics] Requests/sec: 0.20 Request Traffic/sec: 0.00bytes Total Transfer/sec: 0.00bytes Avg Req Time: 5s Fastest Request: 68.373875ms Slowest Request: 68.373875ms Number of Errors: 0 Number of Invalid: 0 Status 200: 1\n[Estimated Server Metrics] Requests/sec: 14.63 Transfer/sec: 0.00bytes Avg Req Time: 68.373875ms\n[2023-02-21 10:50:06][TEST][FAILED] [setup/gateway/cases/echo/echo_with_context/] duration: 1274(ms) #0 request, GET http://$[[env.LR_GATEWAY_HOST]]/any/, assertion failed, skiping subsequent requests 1 requests in 1.255678s, 0.00bytes sent, 0.00bytes received\n[Loadgen Client Metrics] Requests/sec: 0.20 Request Traffic/sec: 0.00bytes Total Transfer/sec: 0.00bytes Avg Req Time: 5s Fastest Request: 1.255678s Slowest Request: 1.255678s Number of Errors: 1 Number of Invalid: 1 Status 0: 1\n[Estimated Server Metrics] Requests/sec: 0.80 Transfer/sec: 0.00bytes Avg Req Time: 1.255678s\n\n","subcategory":null,"summary":"","tags":null,"title":"Benchmark Testing","url":"/gateway/v1.29.3/docs/getting-started/benchmark/"},{"category":null,"content":"Request Context #  What Is Context #  Context is the entry for INFINI Gateway to access relevant information in the current running environment, such as the request source and configuration. You can use the _ctx keyword to access relevant fields, for example, _ctx.request.uri, which indicates the requested URL.\nEmbedded Request Context #  The embedded _ctx context objects of an HTTP request mainly include the following:\n   Name Type Description     id uint64 Unique ID of the request   tls bool Whether the request is a TLS request   remote_ip string Source IP of the client   remote_addr string Source IP address of the client, including port   local_ip string Gateway local IP address   local_addr string Gateway local IP address, including port   elapsed int64 Time that the request has been executed (ms)   request.* object Request description   response.* object Response description    request #  The request object has the following attributes:\n   Name Type Description     to_string string Complete HTTP request in text form   host string Accessed destination host name/domain name   method string Request type   uri string Complete URL of request   path string Request path   query_args map URL request parameter   username string Name of the user who initiates the request   password string Password of the user   header map Header parameter   body string Request body   body_json object JSON request body object   body_length int Request body length    If the request body data submitted by the client is in JSON format, you can use body_json to access the data. See the following example.\ncurl -u tesla:password -XGET \u0026quot;http://localhost:8000/medcl/_search?pretty\u0026quot; -H 'Content-Type: application/json' -d' { \u0026quot;query\u0026quot;:{ \u0026quot;bool\u0026quot;:{ \u0026quot;must\u0026quot;:[{\u0026quot;match\u0026quot;:{\u0026quot;name\u0026quot;:\u0026quot;A\u0026quot;}},{\u0026quot;match\u0026quot;:{\u0026quot;age\u0026quot;:18}}] } }, \u0026quot;size\u0026quot;:900, \u0026quot;aggs\u0026quot;: { \u0026quot;total_num\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;name1\u0026quot;, \u0026quot;size\u0026quot;: 1000000 } } } }' In JSON data, . is used to identify the path. If the data is an array, you can use [Subscript] to access a specified element, for example, you can use a dump filter for debugging as follows:\n - name: cache_first filter: - dump: context: - _ctx.request.body_json.size - _ctx.request.body_json.aggs.total_num.terms.field - _ctx.request.body_json.query.bool.must.[1].match.age The output is as follows:\n_ctx.request.body_json.size : 900 _ctx.request.body_json.aggs.total_num.terms.field : name1 _ctx.request.body_json.query.bool.must.[1].match.age : 18 response #  The response object has the following attributes:\n   Name Type Description     to_string string Complete HTTP response in text form   status int Request status code   header map Header parameter   content_type string Response body type   body string Response body   body_json object JSON request body object   body_length int Response body length    System Context #  The _sys.* object has the following attributes:\n   Name Type Description     hostname string Hostname of the gateway deployed   month_of_now int Month of now, range from [1,12]   weekday_of_now int Weekday of now, range from [0,6], 0 is Sunday   day_of_now int Day of now   hour_of_now int Hour of now, range from [0,23]   minute_of_now int Minute of now, range from [0,59]   second_of_now int Second of now, range from [0,59]   unix_timestamp_of_now int64 Unix timestamp of the current time   unix_timestamp_milli_of_now int64 Unix timestamp of the current time in milliseconds    Utility Context #  These are utility context can ube used for quickly obtaining relevant parameters. The object _util.* has the following properties:\n   Name Type Description     generate_uuid string Retrieve a random UUID string parameter   increment_id string Retrieve an auto-incrementing numeric identifier. Default bucket is default, customize bucket are supported, e.g., _util.increment_id.mybucket    ","subcategory":null,"summary":"","tags":null,"title":"Request Context","url":"/gateway/v1.29.3/docs/references/context/"},{"category":null,"content":"Elasticsearch #  Defining a Resource #  INFINI Gateway supports multi-cluster access and different versions. Each cluster serves as one Elasticsearch back-end resource and can be subsequently used by INFINI Gateway in multiple locations. See the following example.\nelasticsearch: - name: local enabled: true endpoint: https://127.0.0.1:9200 - name: dev enabled: true endpoint: https://192.168.3.98:9200 basic_auth: username: elastic password: pass - name: prod enabled: true endpoint: http://192.168.3.201:9200 discovery: enabled: true refresh: enabled: true interval: 10s basic_auth: username: elastic password: pass The above example defines a local development test cluster named local and a development cluster named dev. Authentication is enabled in the development cluster, in which corresponding usernames and passwords are also defined. In addition, one production cluster named prod is defined, and the auto node topology discovery and update of the cluster are enabled through the discovery parameter.\nParameter Description #     Name Type Description      name string Name of an Elasticsearch cluster    project string project name    location.provider string the service provider region info of the cluster    location.region string the region info of the cluster    location.dc string the data center info of the cluster    location.rack string the rack info of the cluster    labels map cluster labels    tags array cluster tags    enabled bool Whether the cluster is enabled    endpoint string Elasticsearch access address, for example, http://localhost:9200    endpoints array List of Elasticsearch access addresses. Multiple entry addresses are supported for redundancy.    schema string Protocol type: http or https    host string Elasticsearch host, in the format of localhost:9200. Either the host or endpoint configuration mode can be used.    hosts array Elasticsearch host list. Multiple entry addresses are supported for redundancy.    request_timeout int Request timeout duration, in seconds, default 30    request_compress bool Whether to enable Gzip compression    basic_auth object Authentication information    basic_auth.username string Username    basic_auth.password string Password    discovery object Cluster discovery settings    discovery.enabled bool Whether to enable cluster topology discovery    discovery.refresh object Cluster topology update settings    discovery.refresh.enabled bool Whether to enable auto cluster topology update    discovery.refresh.interval string Interval of auto cluster topology update    traffic_control object Node-level overall traffic control of the cluster    traffic_control.enabled bool Whether to enabled traffic control    traffic_control.max_bytes_per_node int Maximum allowable number of request bytes per second    traffic_control.max_qps_per_node int Maximum allowable number of requests per second, regardless of read or write requests    traffic_control.max_connection_per_node int Maximum allowable number of connections per node    traffic_control.max_wait_time_in_ms int In case of throttled, the maximum allowable waiting time in ms, the default is 10000    allow_access_when_master_not_found bool Still allow access to visit this elasticsearch when it is in error master_not_discovered_exception , default value is false     ","subcategory":null,"summary":"","tags":null,"title":"Elasticsearch","url":"/gateway/v1.29.3/docs/references/elasticsearch/"},{"category":null,"content":"Handling Flow #  Flow Definition #  Requests received by each gateway are handled through a series of processes and then results are returned to the client. A process is called a flow in INFINI Gateway. See the following example.\nflow: - name: hello_world filter: - echo: message: \u0026quot;hello gateway\\n\u0026quot; repeat: 1 - name: not_found filter: - echo: message: '404 not found\\n' repeat: 1 The above example defines two flows: hello_world and not_found. Each flow uses a filter named echo to output a string. A series of filters can be defined in each flow and they are executed in the defined sequence.\nSyntax Description #  INFINI Gateway defines a flow in the stipulated format and supports flexible conditional parameters for logical judgment. The specific format is defined as follows:\nflow: - name: \u0026lt;flow_name\u0026gt; filter: - \u0026lt;filter_name\u0026gt;: when: \u0026lt;condition\u0026gt; \u0026lt;parameters\u0026gt; - \u0026lt;filter_name\u0026gt;: when: \u0026lt;condition\u0026gt; \u0026lt;parameters\u0026gt; ... In the format defined above, filter_name indicates the name of a filter, which is used to execute a specific task. condition below when is used to define specific conditional parameters for executing the task, and the filter task is skipped when the conditions are not met. In parameters, parameters related to the filter are set, and the parameters are separated by the line feed character.\nConditional Judgment #  Complex logical judgments can be defined in a flow of INFINI Gateway so that a filter can be executed only when certain conditions are met. See the following example.\nfilter: - if: \u0026lt;condition\u0026gt; then: - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; ... else: - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; ... Parameter Description #     Name Type Description     then array A series of filters to be executed only when conditions defined in condition are met.   else array A set of filters to be executed only when the conditions are not met. You do not have to set it.    You can use if to make conditional judgment and logical selection in the case of multiple filters and use when to determine whether to execute a single filter.\nCondition Type #  For various condition defined in a flow, you can use the current request context to judge whether a specific condition is met so as to achieve logical processing. The conditions support the combination of Boolean expressions (AND, NOT, and OR). The complete list of condition types is as follows:\n equals contains prefix suffix regexp range network exists in queue_has_lag consumer_has_lag cluster_available or and not  equals #  The equals condition is used to judge whether the content of a field is the specified value. It is used for the exact match of characters and digits.\nThe following example determines whether the request method is of the GET type and _ctx is a specific keyword for accessing the request context:\nequals: _ctx.request.method: GET contains #  The contains condition is used to judge whether the content of a field contains a specific character value. Only support string field.\nThe following example judges whether the returned response body contains an error keyword:\ncontains: _ctx.response.body: \u0026quot;error\u0026quot; prefix #  Use the prefix condition to determine whether the contents of a field begin with a specific character value, Only support string field.\nThe following example determines that the returned request path starts with a specific index name:\nprefix: _ctx.request.path: \u0026quot;/filebeat\u0026quot; suffix #  Use the suffix condition to determine whether the content of a field ends with a specific character value. Only support string field.\nThe following example determines whether the request is a search request:\nsuffix: _ctx.request.path: \u0026quot;/_search\u0026quot; regexp #  The regexp condition is used to judge whether the content of a field meets the matching rules of a regular expression. Only support string field.\nThe following example judges whether the request URI is a query request:\nregexp: _ctx.request.uri: \u0026quot;.*/_search\u0026quot; range #  The range condition is used to judge whether the value of a field meets a specific range. It supports the lt, lte, gt, and gte types and only numeric fields are supported.\nThe following example judges the range of the status code:\nrange: _ctx.response.code: gte: 400 The following combination example judges the range of the response byte size:\nrange: _ctx.request.body_length.gte: 100 _ctx.request.body_length.lt: 5000 network #  If the value of a field is an IP address, you can use the network condition to judge whether the field meets a specific network range, whether it supports standard IPv4 or IPv6, whether it supports the classless inter-domain routing (CIDR) expression, or whether it uses an alias in the following range:\n   Name Description     loopback Matches the local loopback network address. Range: 127.0.0.0/8 or ::1/128.   unicast Matches global unicast addresses defined in RFC 1122, RFC 4632, and RFC 4291, except the IPv4 broadcast address (255.255.255.255) but including private address ranges.   multicast Matches the broadcast address.   interface_local_multicast Matches the local multicast address of an IPv6 interface.   link_local_unicast Matches the link-local unicast address.   link_local_multicast Matches the link-local broadcast address.   private Matches the private address range defined in RFC 1918 (IPv4) and RFC 4193 (IPv6).   public Matches public addresses other than the local address, unspecified address, IPv4 broadcast address, link-local unicast address, link-local multicast address, interface local multicast address, or private address.   unspecified Matches an unspecified address (IPv4 address 0.0.0.0 or IPv6 address ::).    The following example matches the local network address:\nnetwork: _ctx.request.client_ip: private The following example specifies a subnet:\nnetwork: _ctx.request.client_ip: '192.168.3.0/24' An array is supported and it is judged that the condition is met when any value in the array is met.\nnetwork: _ctx.request.client_ip: ['192.168.3.0/24', '10.1.0.0/8', loopback] exists #  You can use the exists condition to judge whether a field exists. It supports the use of one or more character fields. See the following example:\nexists: ['_ctx.request.user'] in #  You can use the in condition to judge whether a field has any value in a specified array. It supports a single field and the character and numeric types.\nThe following example judges the returned status code.\nin: _ctx.response.status: [ 403,404,200,201 ] queue_has_lag #  The queue_has_lag condition is used to judge whether one or more local disk queues are stacked with messages.\nqueue_has_lag: [ \u0026quot;prod\u0026quot;, \u0026quot;prod-500\u0026quot; ] When the queue type is FIFO, If you want to set the depth of a queue to be greater than a specified depth, add \u0026gt;queue depth to the end of the queue name. See the following example:\nqueue_has_lag: [ \u0026quot;prod\u0026gt;10\u0026quot;, \u0026quot;prod-500\u0026gt;10\u0026quot; ] The above example shows that the condition is met only when the queue depth exceeds 10.\nconsumer_has_lag #  The consumer_has_lag condition is used to judge whether delay and message stacking occur in the consumer of a queue.\nconsumer_has_lag: queue: \u0026quot;primary-partial-success_bulk_requests\u0026quot; group: \u0026quot;my-group\u0026quot; name: \u0026quot;my-consumer-1\u0026quot; cluster_available #  The cluster_available condition is used to judge the service availability of one or more Elasticsearch clusters. See the following example:\ncluster_available: [\u0026quot;prod\u0026quot;] or #  The or condition is used to combine multiple optional conditions in the following format:\nor: - \u0026lt;condition1\u0026gt; - \u0026lt;condition2\u0026gt; - \u0026lt;condition3\u0026gt; ... See the following example:\nor: - equals: _ctx.response.code: 304 - equals: _ctx.response.code: 404 and #  The and condition is used to combine multiple necessary conditions in the following format:\nand: - \u0026lt;condition1\u0026gt; - \u0026lt;condition2\u0026gt; - \u0026lt;condition3\u0026gt; ... See the following example:\nand: - equals: _ctx.response.code: 200 - equals: _ctx.status: OK You can combine the and and or conditions flexibly. See the following example:\nor: - \u0026lt;condition1\u0026gt; - and: - \u0026lt;condition2\u0026gt; - \u0026lt;condition3\u0026gt; not #  If you want to negate a condition, use the not condition in the following format:\nnot: \u0026lt;condition\u0026gt; See the following example:\nnot: equals: _ctx.status: OK ","subcategory":null,"summary":"","tags":null,"title":"Handling Flow","url":"/gateway/v1.29.3/docs/references/flow/"},{"category":null,"content":"System Optimization #  The operating system of the server where INFINI Gateway is installed needs to be optimized to ensure that INFINI Gateway runs in the best possible state. The following uses Linux as an example.\nSystem Parameters #  sudo tee /etc/security/limits.d/21-infini.conf \u0026lt;\u0026lt;-'EOF' * soft nofile 1048576 * hard nofile 1048576 * soft memlock unlimited * hard memlock unlimited root soft nofile 1048576 root hard nofile 1048576 root soft memlock unlimited root hard memlock unlimited EOF Kernel Optimization #  cat \u0026lt;\u0026lt; SETTINGS | sudo tee /etc/sysctl.d/70-infini.conf fs.file-max=10485760 fs.nr_open=10485760 vm.max_map_count=262144 net.core.somaxconn=65535 net.core.netdev_max_backlog=65535 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.core.rmem_max=4194304 net.core.wmem_max=4194304\nnet.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind=1 net.ipv4.ip_local_port_range = 1024 65535 net.ipv4.conf.default.accept_redirects = 0 net.ipv4.conf.default.rp_filter = 1 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.send_redirects = 0 net.ipv4.tcp_tw_reuse=1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_max_tw_buckets = 300000 net.ipv4.tcp_timestamps=1 net.ipv4.tcp_syncookies=1 net.ipv4.tcp_max_syn_backlog=65535 net.ipv4.tcp_synack_retries=0 net.ipv4.tcp_keepalive_intvl = 30 net.ipv4.tcp_keepalive_time = 900 net.ipv4.tcp_keepalive_probes = 3 net.ipv4.tcp_fin_timeout = 10 net.ipv4.tcp_max_orphans = 131072 net.ipv4.tcp_rmem = 4096 4096 16777216 net.ipv4.tcp_wmem = 4096 4096 16777216 net.ipv4.tcp_mem = 786432 3145728 4194304 SETTINGS Run the following command to check whether configuration parameters are valid.\nsysctl -p Restart the operating system to make the configuration take effect.\n","subcategory":null,"summary":"","tags":null,"title":"System Optimization","url":"/gateway/v1.29.3/docs/getting-started/optimization/"},{"category":null,"content":"Helm Charts #  INFINI Gateway supports deployment on K8s by using helm chart .\nThe Chart Repository #  Chart repository: https://helm.infinilabs.com.\nUse the follow command add the repository:\nhelm repo add infinilabs https://helm.infinilabs.com Prerequisites #   K8S StorageClass  The default StorageClass of the Chart package is local-path, you can install it through here.\nIf you want use other StorageClass(installed), you can create a YAML file (eg. vaules.yaml) file that it contains the follow contents:\nstorageClassName: \\\u0026lt;storageClassName\\\u0026gt; and use it through -f.\n Storage Cluster  The default Storage Cluster of the Chart package is Easysearch, you can install it through here.\nNote: The username and password of easysearch in the Chart package is default, if you change it, you can adjust this by modifying the cluster connection below。 Gateway also support other cluster (eg. Elasticsearch、Opensearch)，you can create a YAML file (eg. vaules.yaml) file that it contains the follow contents:\nenv: # connection address of the logging cluster loggingEsEndpoint: ****** # username of the logging cluster loggingEsUser: ****** # password of the logging cluster\u0026#39;s user loggingEsPass: ****** # connection address of the production cluster prodEsEndpoint: ****** # username of the production cluster prodEsUser: ****** # password of the production cluster\u0026#39;s user prodEsPass: ****** and use it through -f.\nInstall #  helm install gateway infinilabs/gateway -n \u0026lt;namespace\u0026gt; Uninstall #  helm uninstall gateway -n \u0026lt;namespace\u0026gt; kubectl delete pvc gateway-data-gateway-0 -n \u0026lt;namespace\u0026gt; ","subcategory":null,"summary":"","tags":null,"title":"Kubernetes Deployment","url":"/gateway/v1.29.3/docs/getting-started/helm/"},{"category":null,"content":"","subcategory":null,"summary":"","tags":null,"title":"Hardware Specifications","url":"/gateway/v1.29.3/docs/overview/hardware/"},{"category":null,"content":"Container Deployment #  INFINI Gateway supports container deployment.\nInstallation Demo #    start-at=\u0026quot;49\u0026quot; speed=\u0026quot;1\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  Downloading an Image #  The images of INFINI Gateway are published at the official repository of Docker. The URL is as follows:\n https://hub.docker.com/r/infinilabs/gateway\nUse the following command to obtain the latest container image:\ndocker pull infinilabs/gateway:1.29.3-2018 Verifying the Image #  After downloading the image locally, you will notice that the container image of INFINI Gateway is very small, with a size less than 25 MB. So, the downloading is very fast.\n✗ docker images |grep \u0026quot;gateway\u0026quot; |grep \u0026quot;1.29.3-2018\u0026quot; REPOSITORY TAG IMAGE ID CREATED SIZE infinilabs/gateway 1.29.3-2018 fdae74b64e1a 47 minutes ago 23.5MB Creating Configuration #  Create a configuration file gateway.yml to perform basic configuration as follows:\npath.data: data path.logs: log entry:\n name: my_es_entry enabled: true router: my_router max_concurrency: 200000 network: binding: 0.0.0.0:8000  flow:\n name: simple_flow filter:  elasticsearch: elasticsearch: dev    router:\n name: my_router default_flow: simple_flow  elasticsearch:\n name: dev enabled: true endpoint: http://localhost:9200 basic_auth: username: test password: testtest Note: In the above configuration, replace the Elasticsearch configuration with the actual server connection address and authentication information.\n  Starting the Gateway #  Use the following command to start the INFINI Gateway container:\ndocker run -p 2900:2900 -p 8000:8000 -v=`pwd`/gateway.yml:/gateway.yml infinilabs/gateway:1.29.3-2018 Verifying the Gateway #  If the gateway runs properly, the following information is displayed:\n➜ /tmp docker run -p 2900:2900 -p 8000:8000 -v=`pwd`/gateway.yml:/gateway.yml infinilabs/gateway:1.29.3-2018 ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.26.0, b61758c, Mon Dec 28 14:32:02 2023 +0800, medcl, no panic by default [12-30 05:26:41] [INF] [instance.go:24] workspace: data/gateway/nodes/0 [12-30 05:26:41] [INF] [runner.go:59] pipeline: primary started with 1 instances [12-30 05:26:41] [INF] [entry.go:257] entry [es_gateway] listen at: http://0.0.0.0:8000 [12-30 05:26:41] [INF] [app.go:247] gateway now started. [12-30 05:26:45] [INF] [reverseproxy.go:196] elasticsearch [prod] endpoints: [] =\u0026gt; [192.168.3.201:9200] If you want the container to run in the background, append the parameter -d as follows:\ndocker run -d -p 2900:2900 -p 8000:8000 -v=`pwd`/gateway.yml:/gateway.yml infinilabs/gateway:1.29.3-2018 Access the URL http://localhost:8000/ from the CLI or browser. The Elasticsearch can be accessed normally. See the following information.\n➜ /tmp curl -v http://localhost:8000/ * Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 8000 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Wed, 30 Dec 2020 05:12:39 GMT \u0026lt; Content-Type: application/json; charset=UTF-8 \u0026lt; Content-Length: 480 \u0026lt; UPSTREAM: 192.168.3.201:9200 \u0026lt; { \u0026quot;name\u0026quot; : \u0026quot;node1\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;pi\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;Z_HcN_6ESKWicV-eLsyU4g\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;6.4.2\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;04711c2\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2018-09-26T13:34:09.098244Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;7.4.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } * Connection #0 to host localhost left intact * Closing connection 0 Docker Compose #\n You can also use docker compose to manage container instances. Create one docker-compose.yml file as follows:\nversion: \u0026quot;3.5\u0026quot; services: infini-gateway: image: infinilabs/gateway:1.29.3-2018 ports: - 2900:2900 - 8000:8000 container_name: \u0026quot;infini-gateway\u0026quot; volumes: - ../gateway.yml:/gateway.yml\nvolumes: dist: In the directory where the configuration file resides, run the following command to start INFINI Gateway.\n➜ docker-compose up Starting infini-gateway ... done Attaching to infini-gateway infini-gateway | ___ _ _____ __ __ __ _ infini-gateway | / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ infini-gateway | / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ infini-gateway | / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ infini-gateway | \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ infini-gateway | infini-gateway | [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. infini-gateway | [GATEWAY] 1.0.0_SNAPSHOT, b61758c, Mon Dec 28 14:32:02 2020 +0800, medcl, no panic by default infini-gateway | [12-30 13:24:16] [INF] [instance.go:24] workspace: data/gateway/nodes/0 infini-gateway | [12-30 13:24:16] [INF] [api.go:244] api server listen at: http://0.0.0.0:2900 infini-gateway | [12-30 13:24:16] [INF] [runner.go:59] pipeline: primary started with 1 instances infini-gateway | [12-30 13:24:16] [INF] [entry.go:257] entry [es_gateway] listen at: http://0.0.0.0:8000 infini-gateway | [12-30 13:24:16] [INF] [app.go:247] gateway now started.   ","subcategory":null,"summary":"","tags":null,"title":"Container Deployment","url":"/gateway/v1.29.3/docs/getting-started/docker/"},{"category":null,"content":"Service Router #  INFINI Gateway judges the flow direction based on routers. A typical example of router configuration is as follows:\nrouter: - name: my_router default_flow: default_flow tracing_flow: request_logging rules: - method: - PUT - POST pattern: - \u0026quot;/_bulk\u0026quot; - \u0026quot;/{index_name}/_bulk\u0026quot; flow: - bulk_process_flow Router involves several important terms:\n Flow: Handling flow of a request. Flows can be defined in three places in a router. default_flow: Default handling flow, which is the main flow of service handling. Request forwarding, filtering, and caching are performed in this flow. tracing_flow: Flow used to track the request status. It is independent of the default_flow. This flow is used to log requests and collect statistics. rules: Requests are distributed to specific handling flows according to matching rules. Regular expressions can be used to match the methods and paths of requests.  Parameter Description #     Name Type Description     name string Route name   default_flow string Name of the default request handling flow   tracing_flow string Name of the flow used to trace a request   rules array List of routing rules to be applied in the array sequence   rules.method string Method type of a request. The GET, HEAD, POST, PUT, PATCH, DELETE, CONNECT, OPTIONS, and TRACE types are supported and * indicates any type.   rules.pattern string URL path matching rule of a request. Patterns are supported and overlapping matches are not allowed.   rules.flow string Flow to be executed after rule matching. Multiple flows can be combined and they are executed sequentially.   permitted_client_ip_list string array Specified IP list will be allowed access to the gateway service, in order to permit specify user or application.   denied_client_ip_list string array Specified IP list will not allowed access to the gateway service, in order to prevent specify user or application.    Pattern Syntax #     Syntax Description Example     {Variable name} Variable with a name /{name}   {Variable name:regexp} Restricts the matching rule of the variable by using a regular expression. /{name:[a-zA-Z]}   {Variable name:*} Any path after matching. It can be applied only to the end of a pattern. /{any:*}    Examples:\nPattern: /user/{user} /user/gordon match /user/you match /user/gordon/profile no match /user/ no match\nPattern with suffix: /user/{user}_admin\n/user/gordon_admin match /user/you_admin match /user/you no match /user/gordon/profile no match /user/gordon_admin/profile no match /user/ no match\nPattern: /src/{filepath:*}\n/src/ match /src/somefile.go match /src/subdir/somefile.go match Notes:\n A pattern must begin with /. Any match is only used as the last rule.  Permit IPs #  If you only want some specific IP to access the gateway, you can configure it in the route section, and the request will be directly rejected during the link establishment process. In the following example, 133.37.55.22 will be allowed to access the gateway, and the rest of the IP will be denied.\nrouter: - name: my_router default_flow: async_bulk ip_access_control: enabled: true client_ip: permitted: - 133.37.55.22 Block IPs #  If you want to block specify know ip to access gateway, you can configure the ip list in the router section, the requests will be denied during the TCP connection establishment。 for below example，the ip 133.37.55.22 will be blocked:\nrouter: - name: my_router default_flow: async_bulk ip_access_control: enabled: true client_ip: denied: - 133.37.55.22 ","subcategory":null,"summary":"","tags":null,"title":"Service Router","url":"/gateway/v1.29.3/docs/references/router/"},{"category":null,"content":"Nearest Cluster Access Across Two Cloud Providers #  Service Requirements #  To ensure the high availability of the Elasticsearch service, Zuoyebang deploys a single Elasticsearch cluster on both Baidu Cloud and Huawei Cloud and requires that service requests be sent to the nearest cloud first.\nDeployment of a Single Elasticsearch Cluster on Dual Clouds #  The Elasticsearch cluster uses an architecture with master nodes separated from data nodes. Currently, the main cloud is used to accommodate two master nodes and the other cloud is used to accommodate another master node. The main consideration is that infrastructure failures are mostly dedicated line failures, and the overall breakdown of a provider\u0026rsquo;s cloud rarely occurs. Therefore, the main cloud is configured. When a dedicated line failure occurs, the Elasticsearch cluster on the main cloud is read/write and service traffic can be switched to the main cloud.\nThe configuration is as follows:\nFirst, complete the following settings on the master nodes:\ncluster.routing.allocation.awareness.attributes: zone_id cluster.routing.allocation.awareness.force.zone_id.values: zone_baidu,zone_huawei Then, perform the following settings on data nodes on Baidu Cloud:\nnode.attr.zone_id: zone_baidu Perform the following settings on data nodes on Huawei Cloud:\nnode.attr.zone_id: zone_huawei Indexes are created using one copy, which can ensure that the same copy of data exists on Baidu Cloud and Huawei Cloud.\nThe service access mode is shown as follows:\n Baidu Cloud service -\u0026gt; Baidu lb -\u0026gt; INFINI Gateway (Baidu Cloud) -\u0026gt; Elasticsearch (data nodes on Baidu Cloud) Huawei Cloud service -\u0026gt; Huawei lb -\u0026gt; INFINI Gateway (Huawei Cloud) -\u0026gt; Elasticsearch (data nodes on Huawei Cloud)  Configuring INFINI Gateway #  Elasticsearch uses the Preference parameter to set the request access priority. Set the default Preference parameter for requests on INFINI Gateway inside the two clouds so that requests inside each cloud are sent to data nodes in the local cloud first, thereby implementing nearest access of requests.\nThe specific configuration on INFINI Gateway inside Baidu Cloud is as follows (the configuration on INFINI Gateway inside Huawei Cloud is basically the same and is not provided here):\npath.data: data path.logs: log entry:\n name: es-test enabled: true router: default network: binding: 0.0.0.0:9200 reuse_port: true  router:\n name: default default_flow: es-test  flow:\n name: es-test filter:  set_request_query_args: args: - preference -\u0026gt; _prefer_nodes:node-id-of-data-baidu01,node-id-of-data-baidu02 #Set _prefer_nodes of Preference to all Baidu data nodes(use node_id of these nodes) so that Baidu Cloud service accesses the nodes of Baidu Cloud first, thereby avoiding cross-cloud access to the maximum extent and enabling the service to run more smoothly. when: contains: _ctx.request.path: /_search elasticsearch: elasticsearch: default refresh: enabled: true interval: 10s roles: include: - data #Set it to data so that requests are sent only to the data node. tags: include: - zone_id: zone_baidu #Requests are forwarded only to nodes in Baidu Cloud.    elasticsearch:\n name: default enabled: true endpoint: http://10.10.10.10:9200 allow_access_when_master_not_found: true discovery: enabled: true refresh: enabled: true interval: 10s basic_auth: username: elastic password: elastic Summary and Benefits #   Retrospect of Failures Before INFINI Gateway Is Introduced #  When Baidu Cloud service accesses the Elasticsearch cluster, it pulls daily incremental data from the Hive cluster and synchronizes it to Elasticsearch. Some tasks may fail and data is synchronized again. As a result, some data is pulled from the Elasticsearch node inside Huawei Cloud to the Hive cluster of Baidu Cloud. The huge amount of data triggers an alarm about cross-cloud dedicated line traffic monitoring. Online services, MySQL, Redis, and Elasticsearch use the same dedicated line. The impact of the failures is huge. The temporary solution is to add the Preference parameter to the service modification statement so that the services only pull local cloud data, reducing the occupancy of the dedicated line. The service transformation and maintenance costs are high. In addition, DBA has worries that there are omissions in service transformation, the Preference parameter is ignored for new services, and later adjustment costs are high. These are always risk points.\nBenefits of INFINI Gateway #  After INFINI Gateway is added to the original architecture, services can preferentially access the local cloud with the service code not modified. In this way, CPU resources of the server are fully utilized and all CPU resources of each node are used.\n Author: Zhao Qing, former DBA of NetEase, mainly involved in the O\u0026amp;M of Oracle, MySQL, Redis, Elasticsearch, Tidb, OB, and other components, as well as O\u0026amp;M automation, platform-based application, and intelligence. Now he is working in Zuoyebang.\n ","subcategory":null,"summary":"","tags":null,"title":"Nearest Cluster Access Across Two Cloud Providers","url":"/gateway/v1.29.3/docs/user-cases/stories/a_cross_region_cluster_access_locality/"},{"category":null,"content":"Active Merging of Index Segments #  INFINI Gateway has an index segment merging service, which can actively merge index segment files to improve query speed. The index segment merging service supports sequential processing of multiple indexes and tracks the status of the merging task, thereby preventing cluster slowdown caused by concurrent operations of massive index segment merging tasks.\nEnabling the Service #  Modify the gateway.yml configuration file by adding the following configuration:\nforce_merge: enabled: false elasticsearch: dev min_num_segments: 20 max_num_segments: 1 indices: - index_name The parameters are described as follows:\n   Name Type Description     enabled bool Whether the module is enabled, which is set to false by default.   elasticsearch string ID of an Elasticsearch cluster, on which index segment merging is performed   min_num_segments int Minimum number of shards in an index for active shard merging. The value is based on indexes.   max_num_segments int The maximum number of segment files that can be generated after segment files in a shard are merged   indices array List of indexes that need shard merging   discovery object Auto-discovery of index-related settings   discovery.min_idle_time string Minimum time span for judging whether segment merging conditions are met. The default value is 1d.   discovery.interval string Interval for detecting whether segment merging is required   discovery.rules array Index matching rules used in automatic index detection   discovery.rules.index_pattern string Pattern of indexes that need index segment file merging   discovery.rules.timestamp_fields array List of fields representing the index timestamp    ","subcategory":null,"summary":"","tags":null,"title":"Index Segment Merging","url":"/gateway/v1.29.3/docs/references/modules/force_merge/"},{"category":null,"content":"How an Insurance Group Improved the Indexing Speed by 200x Times #  Challenges #  A large insurance group places common database fields in Elasticsearch to improve the query performance for its policy query service. The cluster is deployed on 14 physical machines, with 4 Elasticsearch instances deployed on each physical machine. The whole cluster has more than 9 billion pieces of data. The storage size of index primary shards is close to 5 TB, and about 600 million pieces of incremental data are updated every day. Due to the service particularity, all the service data across the country is stored in one index, resulting in up to 210 shards in the single index. The bulk rebuilding task is executed in parallel by Spark. The average write speed is about 2,000–3,000 pieces per second. One incremental rebuilding operation may take 2–3 days. Service data updating causes a large delay and the lengthy rebuilding also affects service access during normal time periods. The technical team had tried hard to optimize Elasticsearch and also the Spark write end for several rounds, but did not get any progress in the indexing speed improvement.\nScenario #  The analysis shows that the cluster performance is good. However, after write requests in a single batch are received by Elasticsearch, they need to be encapsulated and forwarded according to the node where the primary shard is located. There are too many service index shards and each data node eventually gets a very small number of request documents. One bulk write request of the client is divided into hundreds of small bulk requests. According to the short board theory of the barrel, the processing speed of the slowest node slows down the whole bulk write operation. INFINI Gateway knows where a document should go to.\nINFINI Gateway is capable of splitting and merging requests in advance. It splits and merges requests in advance, sends the requests to local queues based on the target node, and then writes the requests to the target Elasticsearch cluster through the queue consumption program to convert random bulk requests into sequential requests that are precisely delivered. See the figure below.\nAfter receiving a request from Spark, INFINI Gateway first stores the request on the local disk to prevent data loss. Meanwhile, INFINI Gateway can locally calculate the routing information of each document and the target data nodes. The new data writing architecture is shown in the figure below.\nAfter INFINI Gateway is used to receive write requests from Spark, the write throughput of the entire cluster is significantly improved. Spark accomplishes the data writing task in less than 15 minutes, and it takes only 20 minutes for the gateway to receive requests and write them into Elasticsearch. The CPU resources of the server are fully utilized and all the CPU resources of each node are used.\nUser Benefits #   The Indexing Speed Is Improved by 20,000%\n After INFINI Gateway is used as the intermediate acceleration layer, the index rebuilding cycle of the group\u0026rsquo;s policy service is reduced from 2–3 days to about 20 minutes, the 600 million pieces of daily incremental data can also be rebuilt very quickly, and the peak index write QPS can exceed 300,000. In a word, INFINI Gateway greatly shortens the index rebuilding cycle, reduces data latency, enhances the consistency of online data, and ensures the normal use of the query service.\n","subcategory":null,"summary":"","tags":null,"title":"How an Insurance Group Improved the Indexing Speed by 200x Times","url":"/gateway/v1.29.3/docs/user-cases/stories/indexing_speedup_for_big_index_rebuild/"},{"category":null,"content":"Document-Level Index Diff Between Two Elasticsearch Clusters #  INFINI Gateway is able to compare differences between two different indexes in the same or different clusters. In scenarios in which application dual writes, CCR, or other data replication solutions are used, differences can be periodically compared to ensure data consistency.\nFunction Demonstration #    start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  How Is This Feature Configured? #  Setting a Target Cluster #  Modify the gateway.yml configuration file by setting two cluster resources source and target and adding the following configuration:\nelasticsearch: - name: source enabled: true endpoint: http://localhost:9200 basic_auth: username: test password: testtest - name: target enabled: true endpoint: http://localhost:9201 basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions username: test password: testtest Configuring a Contrast Task #  Add a service pipeline to handle the index document pulling and contrast of two clusters as follows:\npipeline: - name: index_diff_service auto_start: true keep_running: true processor: - dag: parallel: - dump_hash: #dump es1's doc indices: \u0026quot;medcl-test\u0026quot; scroll_time: \u0026quot;10m\u0026quot; elasticsearch: \u0026quot;source\u0026quot; output_queue: \u0026quot;source_docs\u0026quot; batch_size: 10000 slice_size: 5 - dump_hash: #dump es2's doc indices: \u0026quot;medcl-test\u0026quot; scroll_time: \u0026quot;10m\u0026quot; batch_size: 10000 slice_size: 5 elasticsearch: \u0026quot;target\u0026quot; output_queue: \u0026quot;target_docs\u0026quot; end: - index_diff: diff_queue: \u0026quot;diff_result\u0026quot; buffer_size: 1 text_report: true #If data needs to be saved to Elasticsearch, disable the function and start the diff_result_ingest task of the pipeline. source_queue: 'source_docs' target_queue: 'target_docs' In the above configuration, dump_hash is concurrently used to pull the medcl-a index of the source cluster and fetch the medcl-b index of the target cluster, and output results to terminals in text form.\nOutputting Results to Elasticsearch #  If there are many difference results, you can save them to the Elasticsearch cluster, set the text_report parameter of the above index_diff processing unit to false, and add the following configuration:\npipeline: - name: diff_result_ingest auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;diff_result\u0026quot; elasticsearch: \u0026quot;source\u0026quot; input_queue: \u0026quot;diff_result\u0026quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 #in MB Finally, import the dashboard to Kibana to achieve the following effect:\n","subcategory":null,"summary":"","tags":null,"title":"Document-Level Index Diff Between Two Elasticsearch Clusters","url":"/gateway/v1.29.3/docs/tutorial/index_diff/"},{"category":null,"content":"Configuration #  The configuration of INFINI Gateway can be modified in multiple ways.\nCLI Parameters #  INFINI Gateway provides the following CLI parameters:\n✗ ./bin/gateway --help Usage of ./bin/gateway: -config string the location of config file, default: gateway.yml (default \u0026quot;gateway.yml\u0026quot;) -debug run in debug mode, gateway will quit with panic error -log string the log level,options:trace,debug,info,warn,error (default \u0026quot;info\u0026quot;) -v version The parameters are described as follows:\n config: Specifies the name of a configuration file. The default configuration file name is gateway.yml in the directory where the currently executed command is located. If your configuration file is stored elsewhere, you can specify the parameter to select it. daemon: Switches the gateway to the background. It needs to be used jointly with pidfile to save the process ID and facilitate subsequent process operations.  Configuration File #  Most of the configuration of INFINI Gateway can be completed using gateway.yml. After the configuration is modified, the gateway program needs to be restarted to make the configuration take effect.\nDefining an Entry #  Each gateway must expose at least one service entrance to receive operation requests of services. In INFINI Gateway, the service entrance is called an entry, which can be defined using the following parameters:\nentry: - name: es_gateway enabled: true router: default network: binding: 0.0.0.0:8000 The above configuration defines one service entry named es_gateway, the address listened to is 0.0.0.0:8000, and one router named default is used to process requests.\nDefining a Router #  INFINI Gateway judges the flow direction based on routers. A typical example of router configuration is as follows:\nrouter: - name: default default_flow: cache_first This example defines one router named default, which is also the main flow for service handling. Request forwarding, filtering, caching, and other operations are performed in this flow.\nDefining a Flow #  One request flow defines a series of work units for request handling. It adopts a typical pipeline work mode. One typical configuration example is as follows:\nflow: - name: cache_first filter: - get_cache: - elasticsearch: elasticsearch: prod - set_cache: The configuration example defines a flow named cache_first, which uses three different filters: get_cache, elasticsearch, and set_cache. These filters are executed in their configuration sequence. Note that each filter name must be appended with one colon (:). The processing results of the filters are as follows:\n get_cache: This filter is mainly used to get data from the cache. If the same request has been received before and data is cached in the cache, which is within the validity period, this filter can directly take and return the cached data immediately, without further processing. elasticsearch: This filter is used to forward requests to back-end Elasticsearch clusters and further transfer responses returned by Elasticsearch. set_cache: This filter caches execution results to the local memory. It has some parameter restrictions such as the status code and request size, and expiration time is set for the filter so that results in the cache can be used directly when the same request is received next time. It is generally used together with get_cache.  Defining a Resource #  Resources here refer to Elasticsearch back-end server resources. INFINI Gateway supports multiple Elasticsearch clusters. It can forward requests to different clusters and supports blue/green deployment and smooth evolution under canary deployment of requests. The following example shows how to define an Elasticsearch back-end resource.\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 discovery: enabled: true refresh: enabled: true basic_auth: username: elastic password: pass The endpoint parameter is used to set the access address for Elasticsearch. If identity authentication is enabled for Elasticsearch, you can use basic_auth to specify the username and password and the user must have the permission to obtain cluster status information. The discover parameter is used to enable automatic discovery to automatically detect the status of back-end nodes and identify new and offline nodes.\nAfter these basic configurations have been completed, INFINI Gateway can normally handle Elasticsearch requests as a proxy. For details about parameters of each component, see the Reference.\n","subcategory":null,"summary":"","tags":null,"title":"Configuring the Gateway","url":"/gateway/v1.29.3/docs/getting-started/configuration/"},{"category":null,"content":"Service Entry #  Defining an Entry #  Each gateway must expose at least one service entrance to receive operation requests of services. In INFINI Gateway, the service entrance is called an entry, which can be defined using the following parameters:\nentry: - name: es_gateway enabled: true router: default network: binding: 0.0.0.0:8000 reuse_port: true tls: enabled: false The network.binding parameter can be used to specify the IP address and port to be bound and listened to after the service is started. INFINI Gateway supports port reuse, that is, multiple INFINI Gateways can share the same IP address and port. In this way, server resources can be fully utilized and the configuration of different gateway processes can be modified dynamically (you can start multiple processes, and then restart the processes in sequence after modifying the configuration), without interrupting normal client requests.\nFor each request sent to the entry, requested traffic is routed by router. Rules are defined for router separately so that the rules are used in different entry settings. In entry, the router parameter can be used to specify the router rules to be used and default is defined here.\nTLS Configuration #  TLS transmission encryption can be seamlessly enabled on INFINI Gateway. You can switch to HTTPS communication mode by setting tls.enabled to true. INFINI Gateway can automatically generate certification files.\nINFINI Gateway also allows you to define the path of the certification file. The configuration is as follows:\nentry: - name: es_gateway enabled: true router: default network: binding: 0.0.0.0:8000 reuse_port: true tls: enabled: true cert_file: /etc/ssl.crt key_file: /etc/ssl.key skip_insecure_verify: false Multiple Services #  INFINI Gateway can listen on multiple service entries at the same time. The listened address, protocol, and router of each service entry can be separately defined to meet different service requirements. The following shows a configuration example.\nentry: - name: es_ingest enabled: true router: ingest_router network: binding: 0.0.0.0:8000 - name: es_search enabled: true router: search_router network: binding: 0.0.0.0:9000 The above example defines a service entry named es_ingest to listen on the address 0.0.0.0:8000, and all requests are processed through ingest_router. In the example, one es_search service is also defined, the listening port is 9000, and search_router is used for request processing to implement read/write separation of services. In addition, different service entries can be defined for different back-end Elasticsearch clusters, and the gateway can forward requests as a proxy.\nIPv6 Support #  INFINI Gateway support to binding to IPv6 address，for example:\nentry: - name: es_ingest enabled: true router: ingest_router network: # binding: \u0026quot;[ff80::4e2:7fb6:7db6:a839%en0]:8000\u0026quot; binding: \u0026quot;[::]:8000\u0026quot; Parameter Description #     Name Type Description     name string Name of a service entry   enabled bool Whether the entry is enabled   max_concurrency int Maximum concurrency connection number, which is 10000 by default.   router string Router name   network object Relevant network configuration   tls object TLS secure transmission configuration   network.host string Network address listened to by the service, for example, 192.168.3.10   network.port int Port address listened to by the service, for example, 8000   network.binding string Network binding address listened to by the service, for example, 0.0.0.0:8000   network.publish string External access address listened to by the service, for example, 192.168.3.10:8000   network.reuse_port bool Whether to reuse the network port for multi-process port sharing   network.skip_occupied_port bool Whether to automatically skip occupied ports   tls.enabled bool Whether TLS secure transmission is enabled   tls.cert_file string Path to the public key of the TLS security certificate   tls.key_file string Path to the private key of the TLS security certificate   tls.skip_insecure_verify bool Whether to ignore TLS certificate verification    ","subcategory":null,"summary":"","tags":null,"title":"Service Entry","url":"/gateway/v1.29.3/docs/references/entry/"},{"category":null,"content":"Installing the Gateway #  INFINI Gateway supports mainstream operating systems and platforms. The program package is small, with no extra external dependency. So, the gateway can be installed very rapidly.\nInstallation Demo #    autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  Downloading #  Automatic install\ncurl -sSL http://get.infini.cloud | bash -s -- -p gateway  The above script can automatically download the latest version of the corresponding platform\u0026rsquo;s gateway and extract it to /opt/gateway\n  The optional parameters for the script are as follows:\n   -v [version number]（Default to use the latest version number）\n   -d [installation directory] (default installation to /opt/gateway)\n Manual install\nSelect a package for downloading in the following URL based on your operating system and platform:\n https://release.infinilabs.com/\nContainer Deployment #  INFINI Gateway also supports Docker container deployment.\nLearn More  Verifying the Installation #  After downloading and decompressing INFINI Gateway installation package, run the following command to check whether the installation package is effective:\n✗ ./bin/gateway -v gateway 1.0.0_SNAPSHOT 2021-01-03 22:45:28 6a54bb2 If the above version information is displayed, the gateway program is in good condition.\nStarting the Gateway #  Run the gateway program as an administrator to start INFINI Gateway, as follows:\n➜ sudo ./bin/gateway ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, 4daf6e9, Mon Jan 11 11:40:44 2021 +0800, medcl, add response_header_filter [01-11 16:43:31] [INF] [instance.go:24] workspace: data/gateway/nodes/0 [01-11 16:43:31] [INF] [api.go:255] api server listen at: http://0.0.0.0:2900 [01-11 16:43:31] [INF] [runner.go:59] pipeline: primary started with 1 instances [01-11 16:43:31] [INF] [runner.go:59] pipeline: nodes_index started with 1 instances [01-11 16:43:31] [INF] [entry.go:262] entry [es_gateway] listen at: https://0.0.0.0:8000 [01-11 16:43:32] [INF] [floating_ip.go:170] floating_ip listen at: 192.168.3.234, echo port: 61111 [01-11 16:43:32] [INF] [app.go:254] gateway now started. If the above startup information is displayed, the gateway is running successfully and listening on specified port.\nAccessing the Gateway #  The back-end Elasticsearch service can be accessed using a browser or other clients through the gateway that serves as a proxy:\nShutting Down the Gateway #  To shut down INFINI Gateway, hold down Ctrl+C. The following information will be displayed:\n^C [GATEWAY] got signal: interrupt, start shutting down [01-11 16:44:41] [INF] [app.go:303] gateway now terminated. [GATEWAY] 1.0.0_SNAPSHOT, uptime: 1m10.550336s Thanks for using GATEWAY, have a good day! System Service #\n To run the data platform of INFINI Gateway as a background task, run the following commands:\n➜ ./gateway -service install Success ➜ ./gateway -service start Success Unloading the service is simple. To unload the service, run the following commands:\n➜ ./gateway -service stop Success ➜ ./gateway -service uninstall Success Customize service name:\nsudo SERVICE_NAME=mygw ./bin/gateway -service install sudo SERVICE_NAME=mygw ./bin/gateway -service start sudo SERVICE_NAME=mygw ./bin/gateway -service stop sudo SERVICE_NAME=mygw ./bin/gateway -service uninstall INFINI Gateway has been completely installed. Next, configure the gateway.\nConfiguring INFINI Gateway  ","subcategory":null,"summary":"","tags":null,"title":"Installing the Gateway","url":"/gateway/v1.29.3/docs/getting-started/install/"},{"category":null,"content":"Floating IP #  The embedded floating IP feature of INFINI Gateway can implement dual-node hot standby and failover. INFINI Gateway innately provides high availability for L4 network traffic, and no extra software and devices are required to prevent proxy service interruption caused by downtime or network failures.\nNote:\n This feature supports only Mac OS and Linux OS. The gateway must run as the user root. This feature relies on the ping and ifconfig commands of the target system. Therefore, ensure that related packages are installed by default. The network interface cards (NICs) of a group of gateways, on which floating IP is enabled, should belong to the same subnet, and devices on the Intranet can communicate with each other in broadcast mode (the actual IP address and floating IP address of a gateway need to be different only in the last bit, for example, 192.168.3.x).   Function Demonstration #    Youtube  Bilibili  What Is a Floating IP? #  INFINI Gateway achieves high availability by using a floating IP, which is also called a virtual IP or dynamic IP. Each server must have an IP address for communication and the IP address of a server is usually static and allocated in advance. If the server malfunctions, the IP address and the services deployed on the server are inaccessible. A floating IP address is usually a public and routable IP address that is not automatically allocated to a physical device. The project manager can temporarily allocate this dynamic IP address to one or more physical devices. The physical devices have automatically assigned static IP addresses for communicating with devices on the Intranet. This Intranet uses private addresses that are not routable. Services of physical devices on the Intranet can be identified and accessed by external networks only through the floating IP address.\nWhy Is a Floating IP Needed? #  One typical floating IP switching scenario is that, when a device bound with a floating IP address malfunctions, the floating IP address floats to another device on the network. The new device immediately replaces the faulty device to provide services externally. This creates high availability for network services. For service consumers, only the floating IP needs to be specified. Floating IPs are very useful. In certain scenarios, for example, only one service IP address is allowed for the client or SDK, which means that the IP address must be highly available. INFINI Gateway can effectively solve this problem. When two independent INFINI Gateway servers are used, you are advised to deploy them on independent physical servers. The two INFINI Gateways work in dual-node hot standby mode. If any of the gateways malfunction, front-end services can still be accessed.\nEnabling Floating IP #  To enable the floating IP feature of INFINI Gateway, modify the gateway.yml configuration file by adding the following configuration:\nfloating_ip: enabled: true INFINI Gateway can automatically detect NIC device information and bind the virtual IP address to the Intranet communication port. It is very intelligent and easy to use. By default, the IP address to be listened to is *.*.*.234 in the network segment, to which the machine belongs. Assume that the physical IP address of the machine is 192.168.3.35. The default floating IP address is 192.168.3.234. This default IP address is only used to facilitate configuration and quick startup. If you need to use a user-defined floating IP address, supplement complete parameters.\nRelated Parameter Settings #  The following is an example of configuration parameters about floating IP:\nfloating_ip: enabled: true ip: 192.168.3.234 netmask: 255.255.255.0 interface: en1 The parameters are described as follows:\n   Name Type Description     enabled bool Whether floating IP is enabled, which is set to false by default.   interface string NIC device name. If this parameter is not specified, the name of the first device that listens to the first non-local address is selected. If a server has multiple NIC cards, you are advised to manually set this parameter.   ip string Listened floating IP address, which is *.*.*.234 in the network segment, to which the current physical NIC belongs. You are advised to manually set the floating IP address. The floating IP address cannot conflict with an existing IP address.   local_ip string The physical IP address   netmask string Subnet mask of the floating IP address, which is the subnet mask of the NIC or 255.255.255.0 by default.   echo.port int The ports between gateway nodes for heartbeat detection, make sure that connect to this port are allowed, default 61111   echo.dial_timeout_in_ms int Timeout for heartbeat detection dialing, default 10000   echo.timeout_in_ms int Timeout for heartbeat detection, default 10000    ","subcategory":null,"summary":"","tags":null,"title":"Floating IP","url":"/gateway/v1.29.3/docs/references/modules/floating_ip/"},{"category":null,"content":"Elasticsearch Search/Request Log Analysis/Audit #  INFINI Gateway can track and record all requests that pass through the gateway and analyze requests sent to Elasticsearch, to figure out request performance and service running status.\nGateway configuration modification #  After extracting the Extreme Gateway installation package, there will be a default configuration file called ‘gateway.yml’. With a simple modification, traffic analysis can be achieved. Typically, only this section needs to be modified. The configuration items after this will reference the defined content through variables.\nenv: LOGGING_ES_ENDPOINT: http://localhost:9200 LOGGING_ES_USER: elastic LOGGING_ES_PASS: password PROD_ES_ENDPOINT: http://localhost:9200 PROD_ES_USER: elastic PROD_ES_PASS: password GW_BINDING: \u0026quot;0.0.0.0:8000\u0026quot; API_BINDING: \u0026quot;0.0.0.0:2900\u0026quot; The above configuration defines two ES clusters and the gateway’s listener information.\n LOGGING_ES_ENDPOINT : Define the access information of the log cluster, and all request logs will be written to this cluster. PROD_ES_ENDPOINT : Define the access information of the production cluster, the gateway will proxy this cluster. *_ES_USER and *_ES_PASS : Define the authentication information of the cluster. API_BINDING : Define the address and port that the gateway API service listens on. GW_BINDING Define the address and port that the gateway proxy service listens on.  In the test environment, the log cluster and production cluster can be the same.\nPlease make sure that requests to access the ES cluster are sent to the address and port that the gateway proxy service is listening on.\nThe gateway comes with a cache function. If you need to enable this feature, please modify the default_flow configuration as follows:\n - name: default_flow filter: - get_cache: - elasticsearch: elasticsearch: prod max_connection_per_node: 1000 - set_cache: INFINI Easysearch #  INFINI easysearch supports higher compression rates, which is more conducive to saving disk space.\nIf the log cluster is using INFINI easysearch, it is important to install the index-management plugin.\nPlease click here to view the plugin installation documentation\nbin/easysearch-plugin install index-management After installing the plugin, restart for it to take effect.\nConfigure the index template. #  If you are already using INFINI Console, you can skip configuring the index lifecycle and index template because they have already been automatically created.\nExecute the following command on the log cluster to create an index template for the log index.\n Please note that you may need to modify the above template settings before executing, for example, by adding the routing.allocation.require parameter to specify the node attribute where the index is created.\n   Click to expand the Elasticsearch’s template definition ...  PUT _template/.infini_requests_logging-rollover { \u0026quot;order\u0026quot;: 100000, \u0026quot;index_patterns\u0026quot;: [ \u0026quot;.infini_requests_logging*\u0026quot; ], \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;format\u0026quot;: \u0026quot;7\u0026quot;, \u0026quot;lifecycle\u0026quot;: { \u0026quot;name\u0026quot; : \u0026quot;ilm_.infini_metrics-30days-retention\u0026quot;, \u0026quot;rollover_alias\u0026quot; : \u0026quot;.infini_requests_logging\u0026quot; }, \u0026quot;codec\u0026quot;: \u0026quot;best_compression\u0026quot;, \u0026quot;number_of_shards\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;translog\u0026quot;: { \u0026quot;durability\u0026quot;: \u0026quot;async\u0026quot; } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;dynamic_templates\u0026quot;: [ { \u0026quot;strings\u0026quot;: { \u0026quot;mapping\u0026quot;: { \u0026quot;ignore_above\u0026quot;: 256, \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot;: \u0026quot;string\u0026quot; } } ], \u0026quot;properties\u0026quot;: { \u0026quot;request\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;body\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } }, \u0026quot;response\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;body\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } }, \u0026quot;timestamp\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot; } } }, \u0026quot;aliases\u0026quot;: {} } DELETE .infini_requests_logging-00001 PUT .infini_requests_logging-00001 { \u0026quot;settings\u0026quot;: { \u0026quot;index.lifecycle.rollover_alias\u0026quot;:\u0026quot;.infini_requests_logging\u0026quot; , \u0026quot;refresh_interval\u0026quot;: \u0026quot;5s\u0026quot; }, \u0026quot;aliases\u0026quot;:{ \u0026quot;.infini_requests_logging\u0026quot;:{ \u0026quot;is_write_index\u0026quot;:true } } }\n  \n  Click to expand INFINI Easysearch’s template definition for a 50% reduction in storage ...  PUT _template/.infini_requests_logging-rollover { \u0026quot;order\u0026quot;: 100000, \u0026quot;index_patterns\u0026quot;: [ \u0026quot;.infini_requests_logging*\u0026quot; ], \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;format\u0026quot;: \u0026quot;7\u0026quot;, \u0026quot;lifecycle\u0026quot;: { \u0026quot;name\u0026quot; : \u0026quot;ilm_.infini_metrics-30days-retention\u0026quot;, \u0026quot;rollover_alias\u0026quot; : \u0026quot;.infini_requests_logging\u0026quot; }, \u0026quot;codec\u0026quot;: \u0026quot;ZSTD\u0026quot;, \u0026quot;source_reuse\u0026quot;: true， \u0026quot;number_of_shards\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;translog\u0026quot;: { \u0026quot;durability\u0026quot;: \u0026quot;async\u0026quot; } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;dynamic_templates\u0026quot;: [ { \u0026quot;strings\u0026quot;: { \u0026quot;mapping\u0026quot;: { \u0026quot;ignore_above\u0026quot;: 256, \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot;: \u0026quot;string\u0026quot; } } ], \u0026quot;properties\u0026quot;: { \u0026quot;request\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;body\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } }, \u0026quot;response\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;body\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } }, \u0026quot;timestamp\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot; } } }, \u0026quot;aliases\u0026quot;: {} } DELETE .infini_requests_logging-00001 PUT .infini_requests_logging-00001 { \u0026quot;settings\u0026quot;: { \u0026quot;index.lifecycle.rollover_alias\u0026quot;:\u0026quot;.infini_requests_logging\u0026quot; , \u0026quot;refresh_interval\u0026quot;: \u0026quot;5s\u0026quot; }, \u0026quot;aliases\u0026quot;:{ \u0026quot;.infini_requests_logging\u0026quot;:{ \u0026quot;is_write_index\u0026quot;:true } } }\n  \n Configuring the Index Lifecycle #   Click to expand the definition of the index lifecycle ...  PUT _ilm/policy/ilm_.infini_metrics-30days-retention { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;hot\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;0ms\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;rollover\u0026quot;: { \u0026quot;max_age\u0026quot;: \u0026quot;30d\u0026quot;, \u0026quot;max_size\u0026quot;: \u0026quot;50gb\u0026quot; }, \u0026quot;set_priority\u0026quot;: { \u0026quot;priority\u0026quot;: 100 } } }, \u0026quot;delete\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;30d\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;delete\u0026quot;: { } } } } } }     Importing the Dashboard #  Download the latest dashboard INFINI-Gateway-7.9.2-2021-01-15.ndjson.zip for Kibana 7.9 and import it into Kibana of the dev cluster as follows:\nStarting the Gateway #  Start the gateway.\n➜ ./bin/gateway ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, a17be4c, Wed Feb 3 00:12:02 2021 +0800, medcl, add extra retry for bulk_indexing [02-03 13:51:35] [INF] [instance.go:24] workspace: data/gateway/nodes/0 [02-03 13:51:35] [INF] [api.go:255] api server listen at: http://0.0.0.0:2900 [02-03 13:51:35] [INF] [runner.go:59] pipeline: request_logging_index started with 1 instances [02-03 13:51:35] [INF] [entry.go:267] entry [es_gateway] listen at: http://0.0.0.0:8000 [02-03 13:51:35] [INF] [app.go:297] gateway now started. Modifying Application Configuration #\n Replace the Elasticsearch address with the gateway address for applications directed to the Elasticsearch address (such as Beats, Logstash, and Kibana). Assume that the gateway IP address is 192.168.3.98. Modify the Kibana configuration as follows:\n# The Kibana server's name. This is used for display purposes. #server.name: \u0026quot;your-hostname\u0026quot; The URLs of the Elasticsearch instances to use for all your queries. elasticsearch.hosts: [\u0026quot;https://192.168.3.98:8000\u0026quot;] elasticsearch.customHeaders: { \u0026quot;app\u0026quot;: \u0026quot;kibana\u0026quot; }\nWhen this setting\u0026rsquo;s value is true Kibana uses the hostname specified in the server.host setting. When the value of this setting is false, Kibana uses the hostname of the host that connects to this Kibana instance. #elasticsearch.preserveHost: true\nKibana uses an index in Elasticsearch to store saved searches, visualizations and dashboards. Kibana creates a new index if the index doesn\u0026rsquo;t already exist. #kibana.index: \u0026quot;.kibana\u0026quot;\nThe default application to load. #kibana.defaultAppId: \u0026quot;home\u0026quot; Save the configuration and restart Kibana.\nChecking the Results #  All requests that access Elasticsearch through the gateway can be monitored.\n","subcategory":null,"summary":"","tags":null,"title":"Elasticsearch Search Requests Analysis/Audit","url":"/gateway/v1.29.3/docs/tutorial/request-logging/"},{"category":null,"content":"","subcategory":null,"summary":"","tags":null,"title":"Counterpart Comparison","url":"/gateway/v1.29.3/docs/overview/-comparison/"},{"category":null,"content":"Rewrite Your Elasticsearch Requests OnTheFly #  In some cases, you may find that the QueryDSL generated by the service code is unreasonable. The general practice is to modify the service code and publish it online. If the launch of a new version takes a long time (for example, the put-into-production window is not reached, major network operation closure is in progress, or additional code needs to be submitted to go live), a large number of tests need to be performed. However, faults in the production environment need to be rectified immediately and customers have no time to wait. What should be done in that case?\nDon\u0026rsquo;t worry. You can use INFINI Gateway to dynamically repair queries.\nExample #  See the following query example:\nGET _search { \u0026quot;size\u0026quot;: 1000000 , \u0026quot;explain\u0026quot;: true } The size parameter is set to a very large value and the problem is not found at the beginning. With more and more data generated, too much returned data is bound to cause a sharp decline in performance. In addition, enabling the explain parameter will create unnecessary performance overhead and this function is generally used only during development and debugging.\nBy adding the request_body_json_set filter to the gateway, you can dynamically replace the value of the specified request body JSON PATH. The configuration for the above example is as follows:\nflow: - name: rewrite_query filter: - request_body_json_set: path: - explain -\u0026gt; false - size -\u0026gt; 10 - dump_request_body: - elasticsearch: elasticsearch: dev Set the explain and size parameters again. The query is rewritten in the following format before it is sent to Elasticsearch:\n{ \u0026quot;size\u0026quot;: 10, \u0026quot;explain\u0026quot;: false } The problem is successfully fixed in in-service mode.\nAnother Example #  Look at the following query example. The programmer who writes the code writes the name of the field to be queried by mistake. The name should be name but is written as name1. The size parameter is set to a very large value.\nGET medcl/_search { \u0026quot;aggs\u0026quot;: { \u0026quot;total_num\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;name1\u0026quot;, \u0026quot;size\u0026quot;: 1000000 } } } } The system goes live but a problem arises when a query is conducted. For this problem, you can add the following filter configuration to the gateway request flow:\nflow: - name: rewrite_query filter: - request_body_json_set: path: - aggs.total_num.terms.field -\u0026gt; \u0026quot;name\u0026quot; - aggs.total_num.terms.size -\u0026gt; 10 - size -\u0026gt; 0 - dump_request_body: - elasticsearch: elasticsearch: dev In the above configuration, we can replace the data of the JSON request body through its path, and add one parameter not to return the query document because only aggregated results are required.\nAnother Example #  The user query is as follows:\n{ \u0026quot;query\u0026quot;:{ \u0026quot;bool\u0026quot;:{ \u0026quot;should\u0026quot;:[{\u0026quot;term\u0026quot;:{\u0026quot;isDel\u0026quot;:0}},{\u0026quot;match\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;order\u0026quot;}}] } } } Now you want to replace the term query with the equivalent range query as follows:\n{ \u0026quot;query\u0026quot;:{ \u0026quot;bool\u0026quot;:{ \u0026quot;should\u0026quot;:[{ \u0026quot;range\u0026quot;: { \u0026quot;isDel\u0026quot;: {\u0026quot;gte\u0026quot;: 0,\u0026quot;lte\u0026quot;: 0 }}},{\u0026quot;match\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;order\u0026quot;}}] } } } Use the following configuration:\nflow: - name: rewrite_query filter: - request_body_json_del: path: - query.bool.should.[0] - request_body_json_set: path: - query.bool.should.[1].range.isDel.gte -\u0026gt; 0 - query.bool.should.[1].range.isDel.lte -\u0026gt; 0 - dump_request_body: - elasticsearch: elasticsearch: dev In the above configuration, one request_body_json_del filter is used to delete the first element from the Should query, that is, the Term subquery to be replaced. There is only one Match query left. One Should subquery is added, and the added subscript should be 1. Set the attributes of the Range query.\nFurther Improvement #  In the above examples, queries are directly replaced. In general, you may need to make a judgment about whether to replace the query, for example, replacement may only be performed when the _ctx.request.body_json.query.bool.should.[0].term.isDel JSON field exists. The conditional judgment of the gateway is very flexible and the configuration is as follows:\nflow: - name: cache_first filter: - if: and: - exists: ['_ctx.request.body_json.query.bool.should.[0].term.isDel'] then: - request_body_json_del: path: - query.bool.should.[0] - request_body_json_set: path: - query.bool.should.[1].range.isDel.gte -\u0026gt; 0 - query.bool.should.[1].range.isDel.lte -\u0026gt; 0 - dump_request_body: - elasticsearch: elasticsearch: dev The feature is superb!\n","subcategory":null,"summary":"","tags":null,"title":"Rewrite Your Elasticsearch Requests OnTheFly","url":"/gateway/v1.29.3/docs/tutorial/online_query_rewrite/"},{"category":null,"content":"Protect Elasticsearch from Apache Log4j Vulnerability #  CVE Address\n https://github.com/advisories/GHSA-jfh8-c2jp-5v3q\nVulnerability Description\nApache Log4j is a very popular open source logging toolkit used for the Java runtime environment. Many Java frameworks including Elasticsearch of the latest version, use this component. Therefore, the scope of impact is huge.\nThe latest vulnerability existing in the execution of Apache Log4j\u0026rsquo;s remote code was revealed recently. Attackers can construct malicious requests and utilize this vulnerability to execute arbitrary code on a target server. As a result, the server can be controlled by hackers, who can then conduct page tampering, data theft, mining, extortion, and other behaviors. Users who use this component are advised to immediately initiate emergency response for fixing.\nBasically, if a log output by Log4j contains the keyword ${, the log is replaced as a variable and then the variable operation is executed. Attackers can maliciously construct log content to make Java processes to execute arbitrary commands, achieving the attack purpose.\nVulnerability Level: very urgent\nThe vulnerability is caused by the lookup function provided by Log4j2. This function allows developers to read configurations in the environment by using a number of protocols. However, the input is not strictly judged in the implementation, resulting in the vulnerability.\nImpact Scope: Java products: Apache Log4j 2.x \u0026lt; 2.15.0-rc2\nAttack Detection\nYou can check logs for jndi:ldap://, jndi:rmi, and other characters to find out possible attacks.\nHandling Method #  If Elasticsearch does not support configuration modification, Jar package replacement of Log4j, or cluster restart, you can use INFINI Gateway to intercept requests, replace parameters, and even directly block requests. You can use INFINI Gateway to check parameters in requests sent to Elasticsearch and replace or reject content that contains the sensitive keyword ${. In this way, INFINI Gateway can prevent the execution of malicious attack commands during Log4j logging after attack-contained requests are sent to Elasticsearch, thereby preventing attacks.\nReference Configuration #  Download the latest 1.29.3-2018 version: https://release.infinilabs.com/gateway/stable/\nThe context_filter filter of INFINI Gateway can be used to detect the keywords of the request context _ctx.request.to_string and filter out malicious traffic, thereby blocking attacks.\npath.data: data path.logs: log entry:\n name: es_entrypoint enabled: true router: default max_concurrency: 20000 network: binding: 0.0.0.0:8000  router:\n name: default default_flow: main_flow  flow:\n name: main_flow filter:  context_filter: context: _ctx.request.to_string action: redirect_flow status: 403 flow: log4j_matched_flow must_not: # any match will be filtered regex: - ${.?} - \u0026quot;%24%7B.?%7D\u0026quot; #urlencode contain: - \u0026quot;jndi:\u0026quot; - \u0026quot;jndi:ldap:\u0026quot; - \u0026quot;jndi:rmi:\u0026quot; - \u0026quot;jndi%3A\u0026quot; #urlencode - \u0026quot;jndi%3Aldap%3A\u0026quot; #urlencode - \u0026quot;jndi%3Armi%3A\u0026quot; #urlencode elasticsearch: elasticsearch: es-server   name: log4j_matched_flow filter:  echo: message: \u0026lsquo;Apache Log4j 2, Boom!\u0026rsquo;    elasticsearch:\n name: es-server enabled: true endpoints:  http://localhost:9200 Use urlencode to convert the test command ${java:os} into %24%7Bjava%3Aos%7D.\n    Request calling execution result when requests do not need to pass through the gateway:\n~% curl 'http://localhost:9200/index1/_search?q=%24%7Bjava%3Aos%7D' {\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;index_not_found_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;no such index\u0026quot;,\u0026quot;resource.type\u0026quot;:\u0026quot;index_or_alias\u0026quot;,\u0026quot;resource.id\u0026quot;:\u0026quot;index1\u0026quot;,\u0026quot;index_uuid\u0026quot;:\u0026quot;_na_\u0026quot;,\u0026quot;index\u0026quot;:\u0026quot;index1\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;index_not_found_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;no such index\u0026quot;,\u0026quot;resource.type\u0026quot;:\u0026quot;index_or_alias\u0026quot;,\u0026quot;resource.id\u0026quot;:\u0026quot;index1\u0026quot;,\u0026quot;index_uuid\u0026quot;:\u0026quot;_na_\u0026quot;,\u0026quot;index\u0026quot;:\u0026quot;index1\u0026quot;},\u0026quot;status\u0026quot;:404}% Logs on Elasticsearch are as follows:\n[2021-12-11T01:49:50,303][DEBUG][r.suppressed ] path: /index1/_search, params: {q=Mac OS X 10.13.4 unknown, architecture: x86_64-64, index=index1} org.elasticsearch.index.IndexNotFoundException: no such index at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.infe(IndexNameExpressionResolver.java:678) ~[elasticsearch-5.6.15.jar:5.6.15] at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.innerResolve(IndexNameExpressionResolver.java:632) ~[elasticsearch-5.6.15.jar:5.6.15] at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:580) ~[elasticsearch-5.6.15.jar:5.6.15] The logs above show that q=${java:os} in query conditions is executed and is changed to q=Mac OS X 10.13.4 unknown, architecture: x86_64-64, index=index1.\nRequest calling execution result when requests need to pass through the gateway:\nmedcl@Medcl:~% curl 'http://localhost:8000/index1/_search?q=%24%7Bjava%3Aos%7D' Apache Log4j 2, Boom!% The logs above show that requests are filtered out.\nYou can try other commands to check whether malicious requests are intercepted:\n#{java:vm} ~% curl 'http://localhost:9200/index/_search?q=%24%7Bjava%3Avm%7D' [2021-12-11T02:36:04,764][DEBUG][r.suppressed ] [Medcl-2.local] path: /index/_search, params: {q=OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode), index=index} ~% curl \u0026lsquo;http://localhost:8000/index/_search?q=%24%7Bjava%3Avm%7D\u0026rsquo; Apache Log4j 2, Boom!%\n#{jndi:rmi://localhost:1099/api} ~% curl \u0026lsquo;http://localhost:9200/index/_search?q=%24%7Bjndi%3Armi%3A%2F%2Flocalhost%3A1099%2Fapi%7D\u0026rsquo; 2021-12-11 03:35:06,493 elasticsearch[YOmFJsW][search][T#3] ERROR An exception occurred processing Appender console java.lang.SecurityException: attempt to add a Permission to a readonly Permissions object\n~% curl \u0026lsquo;http://localhost:8000/index/_search?q=%24%7Bjndi%3Armi%3A%2F%2Flocalhost%3A1099%2Fapi%7D\u0026rsquo; Apache Log4j 2, Boom!% \nThe benefits of using INFINI Gateway is that no change needs to be made to the Elasticsearch server, especially in large-scale cluster scenarios. The flexible INFINI Gateway can significantly reduce workload, improve efficiency, shorten the security processing time, and reduce enterprise risks.\n ","subcategory":null,"summary":"","tags":null,"title":"Protect Elasticsearch from Apache Log4j Vulnerability","url":"/gateway/v1.29.3/docs/tutorial/log4j2_filtering/"},{"category":null,"content":"echo #  Description #  The echo filter is used to output specified characters in the returned result. It is often used for debugging.\nFunction Demonstration #    autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  Configuration Example #  A simple example is as follows:\nflow: - name: hello_world filter: - echo: message: \u0026quot;hello infini\\n\u0026quot; The echo filter allows you to set the number of times that same characters can be output repeatedly. See the following example.\n... - echo: message: \u0026quot;hello gateway\\n\u0026quot; repeat: 3 ... Parameter Description #     Name Type Description     message string Characters to be output，default .   messages []string Characters list to be output   status int HTTP Status，default 200   repeat int Number of repetition times   continue bool Whether to continue further filters，default true   response bool Whether to output to HTTP response，default true   stdout bool Whether the terminal also outputs the characters. The default value is false.   logging bool Whether to output as logging，default false   logging_level string The logging level for output logs，default info    ","subcategory":null,"summary":"","tags":null,"title":"echo","url":"/gateway/v1.29.3/docs/references/filters/echo/"}]