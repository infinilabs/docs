<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Offline Processor on INFINI Gateway</title><link>/gateway/v1.27.0/docs/references/processors/</link><description>Recent content in Offline Processor on INFINI Gateway</description><generator>Hugo -- gohugo.io</generator><atom:link href="/gateway/v1.27.0/docs/references/processors/index.xml" rel="self" type="application/rss+xml"/><item><title>bulk_indexing</title><link>/gateway/v1.27.0/docs/references/processors/bulk_indexing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/bulk_indexing/</guid><description>bulk_indexing # Description # The bulk_indexing processor is used to asynchronously consume bulk requests in queues.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - bulk_indexing: queue_selector.labels: type: bulk_reshuffle level: cluster Parameter Description # Name Type Description elasticsearch string The default Elasticsearch cluster ID, which will be used if elasticsearch is not specified in the queue Labels idle_timeout_in_seconds int Timeout duration of the consumption queue, which is set to 1 by default.</description></item><item><title>consumer</title><link>/gateway/v1.27.0/docs/references/processors/consumer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/consumer/</guid><description>consumer # Description # The consumer processor is used to consume messages recorded in the queue without processing them. Its purpose is to provide an entry point for data consumption pipeline, which will be further processed by subsequent processors.
Configuration Example # Here is a simple configuration example:
pipeline: - name: consume_queue_messages auto_start: true keep_running: true retry_delay_in_ms: 5000 processor: - consumer: consumer: fetch_max_messages: 1 max_worker_size: 200 num_of_slices: 1 idle_timeout_in_seconds: 30 queue_selector: keys: - email_messages processor: - xxx1: - xxx2: In the above example, it subscribes to and consumes the email_messages queue.</description></item><item><title>dag</title><link>/gateway/v1.27.0/docs/references/processors/dag/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/dag/</guid><description>dag # Description # The dag processor is used to manage the concurrent scheduling of tasks.
Configuration Example # The following example defines a service named racing_example and auto_start is set to true. Processing units to be executed in sequence are set in processor, the dag processor supports concurrent execution of multiple tasks and the wait_all and first_win aggregation modes.
pipeline: - name: racing_example auto_start: true processor: - echo: #ready, set, go message: read,set,go - dag: mode: wait_all #first_win, wait_all parallel: - echo: #player1 message: player1 - echo: #player2 message: player2 - echo: #player3 message: player3 end: - echo: #checking score message: checking score - echo: #announce champion message: 'announce champion' - echo: #done message: racing finished The echo processor above is very simple and is used to output a specified message.</description></item><item><title>dump_hash</title><link>/gateway/v1.27.0/docs/references/processors/dump_hash/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/dump_hash/</guid><description>dump_hash # Description # The dump_hash processor is used to export index documents of a cluster and calculate the hash value.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - dump_hash: #dump es1's doc indices: &amp;quot;medcl-dr3&amp;quot; scroll_time: &amp;quot;10m&amp;quot; elasticsearch: &amp;quot;source&amp;quot; query: &amp;quot;field1:elastic&amp;quot; fields: &amp;quot;doc_hash&amp;quot; output_queue: &amp;quot;source_docs&amp;quot; batch_size: 10000 slice_size: 5 Parameter Description # Name Type Description elasticsearch string Name of a target cluster scroll_time string Scroll session timeout duration batch_size int Scroll batch size, which is set to 5000 by default slice_size int Slice size, which is set to 1 by default sort_type string Document sorting type, which is set to asc by default sort_field string Document sorting field indices string Index level string Request processing level, which can be set to cluster, indicating that node- and shard-level splitting are not performed on requests.</description></item><item><title>flow_replay</title><link>/gateway/v1.27.0/docs/references/processors/flow_replay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/flow_replay/</guid><description>flow_replay # Description # The flow_replay processor is used to asynchronously consume requests in the queue and use the asynchronous processing process for online requests to perform consumption processing.
Configuration Example # A simple example is as follows:
pipeline: - name: backup-flow-request-reshuffle auto_start: true keep_running: true singleton: true retry_delay_in_ms: 10 processor: - consumer: max_worker_size: 100 queue_selector: labels: type: &amp;quot;primary_write_ahead_log&amp;quot; consumer: group: request-reshuffle fetch_max_messages: 10000 fetch_max_bytes: 20485760 fetch_max_wait_ms: 10000 processor: - flow_replay: flow: backup-flow-request-reshuffle commit_on_tag: &amp;quot;commit_message_allowed&amp;quot; Parameter Description # Name Type Description message_field string The context field name that store the message obtained from the queue, default messages.</description></item><item><title>flow_runner</title><link>/gateway/v1.27.0/docs/references/processors/flow_runner/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/flow_runner/</guid><description>flow_runner # Description # The flow_runner processor is used to asynchronously consume requests in a queue by using the processing flow used for online requests.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - flow_runner: input_queue: &amp;quot;primary_deadletter_requests&amp;quot; flow: primary-flow-post-processing when: cluster_available: [ &amp;quot;primary&amp;quot; ] Parameter Description # Name Type Description input_queue string Name of a subscribed queue flow string Flow used to consume requests in consumption queues commit_on_tag string A message is committed only when a specified tag exists in the context of the current request.</description></item><item><title>index_diff</title><link>/gateway/v1.27.0/docs/references/processors/index_diff/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/index_diff/</guid><description>index_diff # Description # The index_diff processor is used to compare differences between two result sets.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - index_diff: diff_queue: &amp;quot;diff_result&amp;quot; buffer_size: 1 text_report: true #If data needs to be saved to Elasticsearch, disable the function and start the diff_result_ingest task of the pipeline. source_queue: 'source_docs' target_queue: 'target_docs' Parameter Description # Name Type Description source_queue string Name of source data target_queue string Name of target data diff_queue string Queue that stores difference results buffer_size int Memory buffer size keep_source bool Whether difference results contain document source information text_report bool Whether to output results in text form</description></item><item><title>indexing_merge</title><link>/gateway/v1.27.0/docs/references/processors/indexing_merge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/indexing_merge/</guid><description>indexing_merge # Description # The indexing_merge processor is used to consume pure JSON documents in the queue, and merge them into bulk requests and save them in the specified queue. It needs to be consumed with the bulk_indexing processor, and batch writes are used instead of single requests to improve write throughput.
Configuration Example # A simple example is as follows:
pipeline: - name: indexing_merge auto_start: true keep_running: true processor: - indexing_merge: input_queue: &amp;quot;request_logging&amp;quot; elasticsearch: &amp;quot;logging-server&amp;quot; index_name: &amp;quot;infini_gateway_requests&amp;quot; output_queue: name: &amp;quot;gateway_requests&amp;quot; label: tag: &amp;quot;request_logging&amp;quot; worker_size: 1 bulk_size_in_mb: 10 - name: logging_requests auto_start: true keep_running: true processor: - bulk_indexing: bulk: compress: true batch_size_in_mb: 10 batch_size_in_docs: 5000 consumer: fetch_max_messages: 100 queues: type: indexing_merge when: cluster_available: [ &amp;quot;logging-server&amp;quot; ] Parameter Description # Name Type Description input_queue string Name of a subscribed queue worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.</description></item><item><title>json_indexing</title><link>/gateway/v1.27.0/docs/references/processors/json_indexing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/json_indexing/</guid><description>json_indexing # Description # The json_indexing processor is used to consume pure JSON documents in queues and store them to a specified Elasticsearch server.
Configuration Example # A simple example is as follows:
pipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: &amp;quot;gateway_requests&amp;quot; elasticsearch: &amp;quot;dev&amp;quot; input_queue: &amp;quot;request_logging&amp;quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 Parameter Description # Name Type Description input_queue string Name of a subscribed queue worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.</description></item><item><title>merge_to_bulk</title><link>/gateway/v1.27.0/docs/references/processors/merge_to_bulk/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/merge_to_bulk/</guid><description>merge_to_bulk # Description # The merge_to_bulk processor is used to consume pure JSON documents in the queue, and merge them into bulk requests and save them in the specified queue. It needs to be consumed with the consumer processor, and batch writes are used instead of single requests to improve write throughput.
Configuration Example # A simple example is as follows:
pipeline: - name: messages_merge_async_bulk_results auto_start: true keep_running: true singleton: true processor: - consumer: queue_selector: keys: - bulk_result_messages consumer: group: merge_to_bulk processor: - merge_to_bulk: elasticsearch: &amp;quot;logging&amp;quot; index_name: &amp;quot;.</description></item><item><title>queue_consumer</title><link>/gateway/v1.27.0/docs/references/processors/queue_consumer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/queue_consumer/</guid><description>queue_consumer # Description # The queue_consumer processor is used to asynchronously consume requests in a queue and send the requests to Elasticsearch.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - queue_consumer: input_queue: &amp;quot;backup&amp;quot; elasticsearch: &amp;quot;backup&amp;quot; waiting_after: [ &amp;quot;backup_failure_requests&amp;quot;] worker_size: 20 when: cluster_available: [ &amp;quot;backup&amp;quot; ] Parameter Description # Name Type Description input_queue string Name of a subscribed queue worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.</description></item><item><title>replay</title><link>/gateway/v1.27.0/docs/references/processors/replay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/replay/</guid><description>replay # Description # The replay processor is used to replay requests recorded by the record filter.
Configuration Example # A simple example is as follows:
pipeline: - name: play_requests auto_start: true keep_running: false processor: - replay: filename: requests.txt schema: &amp;quot;http&amp;quot; host: &amp;quot;localhost:8000&amp;quot; Parameter Description # Name Type Description filename string Name of a file that contains replayed messages schema string Request protocol type: http or https host string Target server that receives requests, in the format of host:port</description></item><item><title>smtp</title><link>/gateway/v1.27.0/docs/references/processors/smtp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/gateway/v1.27.0/docs/references/processors/smtp/</guid><description>smtp # Description # The SMTP processor is used to send emails, supporting both plain text and HTML emails. It supports template variables and allows attachments to be embedded in the email body. The email message is comes from the pipeline context.
Configuration Example # A simple example is as follows:
pipeline: - name: send_email auto_start: true keep_running: true retry_delay_in_ms: 5000 processor: - consumer: consumer: fetch_max_messages: 1 max_worker_size: 200 num_of_slices: 1 idle_timeout_in_seconds: 30 queue_selector: keys: - email_messages processor: - smtp: idle_timeout_in_seconds: 1 server: host: &amp;quot;smtp.</description></item></channel></rss>