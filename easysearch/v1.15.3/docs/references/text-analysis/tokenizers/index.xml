<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>词元生成器 on INFINI Easysearch</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/</link><description>Recent content in 词元生成器 on INFINI Easysearch</description><generator>Hugo -- gohugo.io</generator><atom:link href="/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/index.xml" rel="self" type="application/rss+xml"/><item><title>N-gram 词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/n-gram/</guid><description>N-gram 词元生成器 # N-gram 词元生成器会将文本拆分为指定长度的重叠 n-gram（固定长度为 n 的字符序列）。当你希望实现部分单词匹配或自动补全搜索功能时，这个分词器特别有用，因为它会生成原始输入文本的子字符串（ n-gram 字符串）。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 n-gram 词元生成器的分词器。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_ngram_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 3, &amp;quot;max_gram&amp;quot;: 4, &amp;quot;token_chars&amp;quot;: [&amp;quot;letter&amp;quot;, &amp;quot;digit&amp;quot;] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_ngram_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_ngram_tokenizer&amp;quot; } } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_ngram_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Search&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ {&amp;quot;token&amp;quot;: &amp;quot;Sea&amp;quot;,&amp;quot;start_offset&amp;quot;: 0,&amp;quot;end_offset&amp;quot;: 3,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 0}, {&amp;quot;token&amp;quot;: &amp;quot;Sear&amp;quot;,&amp;quot;start_offset&amp;quot;: 0,&amp;quot;end_offset&amp;quot;: 4,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 1}, {&amp;quot;token&amp;quot;: &amp;quot;ear&amp;quot;,&amp;quot;start_offset&amp;quot;: 1,&amp;quot;end_offset&amp;quot;: 4,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 2}, {&amp;quot;token&amp;quot;: &amp;quot;earc&amp;quot;,&amp;quot;start_offset&amp;quot;: 1,&amp;quot;end_offset&amp;quot;: 5,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 3}, {&amp;quot;token&amp;quot;: &amp;quot;arc&amp;quot;,&amp;quot;start_offset&amp;quot;: 2,&amp;quot;end_offset&amp;quot;: 5,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 4}, {&amp;quot;token&amp;quot;: &amp;quot;arch&amp;quot;,&amp;quot;start_offset&amp;quot;: 2,&amp;quot;end_offset&amp;quot;: 6,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 5}, {&amp;quot;token&amp;quot;: &amp;quot;rch&amp;quot;,&amp;quot;start_offset&amp;quot;: 3,&amp;quot;end_offset&amp;quot;: 6,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 6} ] } 参数说明 # N-gram 词元生成器可以使用以下参数进行配置。</description></item><item><title>UAX URL 邮件词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/uax-url-email/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/uax-url-email/</guid><description>UAX URL 邮件词元生成器 # 除了常规文本之外，UAX URL 邮件（uax_url_email）词元生成器还专门用于处理网址、电子邮件地址和域名。它基于 Unicode 文本分割算法（ UAX #29），这使得它能够正确地对复杂文本（包括网址和电子邮件地址）进行分词处理。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 UAX URL 邮件词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;uax_url_email_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;uax_url_email&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_uax_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;uax_url_email_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_uax_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_uax_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Contact us at support@example.</description></item><item><title>关键词词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/keyword/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/keyword/</guid><description>关键词词元生成器 # 关键字（keyword）词元生成器接收文本并将其原封不动地作为单个词元输出。当你希望输入内容保持完整时，比如在管理像姓名、产品代码或电子邮件地址这类结构化数据时，这个词元生成器就特别有用。
关键字词元生成器可以与词元过滤器搭配使用来处理文本。例如，对文本进行规范化处理或去除多余的字符。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用关键字词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_keyword_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_keyword_analyzer&amp;quot; } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_keyword_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch Example&amp;quot; } 返回内容会是包含原始内容的单个词元：
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Easysearch Example&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 18, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] } 参数说明 # 关键字词元生成器可以使用以下参数进行配置。</description></item><item><title>前缀 n 元词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/edge-n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/edge-n-gram/</guid><description>前缀 n 元词元生成器 # 前缀 n 元词元生成器会从每个单词的开头生成部分单词词元，也就是 n 元组。它会根据指定的字符分割文本，并在定义的最小和最大长度范围内生成词元。这种词元生成器在实现即输即搜（search-as-you-type）功能时特别有用。
前缀 n 元组非常适合用于自动补全搜索，在这种搜索中单词的顺序可能会有所不同，例如在搜索产品名称或地址时。有关更多信息，请参阅 “自动补全” 相关内容。不过，对于像电影或歌曲标题这种顺序固定的文本，完成建议器(completion suggester)可能会更准确。
默认情况下，前缀 n 元词元生成器生成的词元最小长度为 1，最大长度为 2。例如，当分析文本 Easysearch 时，默认配置会生成 “E” 和 “Ea” 这两个 n 元组。这些短的 n 元组常常会匹配到太多不相关的词条，所以调整 n 元组的长度，对词元生成器进行优化是很有必要的。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用前缀 n 元词元生成器的分词器。该词元生成器生成长度为 3 到 6 个字符的词元，且将字母和符号都视为有效的词元字符。
PUT /edge_n_gram_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;my_custom_tokenizer&amp;quot; } }, &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_custom_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 3, &amp;quot;max_gram&amp;quot;: 6, &amp;quot;token_chars&amp;quot;: [ &amp;quot;letter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>匹配词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/pattern/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/pattern/</guid><description>匹配词元生成器 # 匹配（pattern）词元生成器是一种高度灵活的词元生成器，它允许你根据自定义的 Java 正则表达式将文本拆分为词元。与使用 Lucene 正则表达式的简单匹配（simple_pattern）词元生成器和简单分割匹配（simple_pattern_split）词元生成器不同，匹配词元生成器能够处理更复杂、更细致的正则表达式模式，从而让你对文本的分词方式拥有更强的掌控力。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用匹配词元生成器的分词器。该分词器会在 -、_ 或 . 字符处对文本进行分割。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_pattern_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;[-_.]&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_pattern_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch-2024_v1.</description></item><item><title>字母词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/letter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/letter/</guid><description>字母词元生成器 # 字母(letter)词元生成器会根据任何非字母字符将文本拆分成单词。它在处理许多欧洲语种时效果良好，但对于一些亚洲语种却不太有效，因为在亚洲语种中，单词之间不是由空格来分隔的。
参考样例 # 下面的示例请求创建了一个名为 my_index 的新索引，并配置了一个使用字母词元生成器的分词器。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_letter_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;letter&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_letter_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST _analyze { &amp;quot;tokenizer&amp;quot;: &amp;quot;letter&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Cats 4EVER love chasing butterflies!&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Cats&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 4, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;EVER&amp;quot;, &amp;quot;start_offset&amp;quot;: 6, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;love&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;chasing&amp;quot;, &amp;quot;start_offset&amp;quot;: 16, &amp;quot;end_offset&amp;quot;: 23, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;butterflies&amp;quot;, &amp;quot;start_offset&amp;quot;: 24, &amp;quot;end_offset&amp;quot;: 35, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 4 } ] }</description></item><item><title>字符组词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/character-group/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/character-group/</guid><description>字符组词元生成器 # 字符组（char_group）词元生成器使用特定字符作为分隔符将文本拆分为词元。它适用于需要简单直接进行分词的场景，为基于词元生成器的匹配模式提供了一种更简单的替代方案，避免了额外的复杂性。
参考样例 # 以下示例请求创建了一个名为my_index的新索引，并配置了一个带有 char_group 字符组词元生成器的分词器。该词元生成器会依据空格、连字符 - 和冒号 : 来分割文本。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_char_group_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;char_group&amp;quot;, &amp;quot;tokenize_on_chars&amp;quot;: [ &amp;quot;whitespace&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;:&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_char_group_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_char_group_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_char_group_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_char_group_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Fast-driving cars: they drive fast!</description></item><item><title>小写词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/lowercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/lowercase/</guid><description>小写词元生成器 # 小写（lowercase）词元生成器会在空白处将文本拆分成词条，然后把所有词条转换为小写形式。从功能上来说，这与配置一个字母词元生成器并搭配一个小写词元过滤器的效果是一样的。不过，使用小写词元生成器效率更高，因为分词操作是在一步之内完成的。
参考样例 # 以下示例请求创建了一个名为 my-lowercase-index 的新索引，并配置了一个使用小写词元生成器的分词器：
PUT /my-lowercase-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_lowercase_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_lowercase_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_lowercase_tokenizer&amp;quot; } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my-lowercase-index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_lowercase_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;This is a Test. Easysearch 123!&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;this&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 4, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 5, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 9, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;start_offset&amp;quot;: 10, &amp;quot;end_offset&amp;quot;: 14, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 16, &amp;quot;end_offset&amp;quot;: 26, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 4 } ] }</description></item><item><title>标准词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/standard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/standard/</guid><description>标准词元生成器 # 标准（standard）词元生成器是 Easysearch 中的默认分词器。它基于单词边界，采用一种基于语法的方法对文本进行分词，这种方法能够识别字母、数字以及标点等其他字符。它具有高度的通用性，适用于多种语言，它使用了 Unicode 文本分割规则（ UAX#29）来将文本分割成词元。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用标准词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_standard_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;standard&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_standard_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_standard_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch is powerful, fast, and scalable.&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;powerful&amp;quot;, &amp;quot;start_offset&amp;quot;: 14, &amp;quot;end_offset&amp;quot;: 22, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;fast&amp;quot;, &amp;quot;start_offset&amp;quot;: 24, &amp;quot;end_offset&amp;quot;: 28, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;and&amp;quot;, &amp;quot;start_offset&amp;quot;: 30, &amp;quot;end_offset&amp;quot;: 33, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 4 }, { &amp;quot;token&amp;quot;: &amp;quot;scalable&amp;quot;, &amp;quot;start_offset&amp;quot;: 34, &amp;quot;end_offset&amp;quot;: 42, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 5 } ] } 参数说明 # 标准词元生成器可以使用以下参数进行配置。</description></item><item><title>空格词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/whitespace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/whitespace/</guid><description>空格词元生成器 # 空格（whitespace）词元生成器会依据空白字符（如空格、制表符和换行符）对文本进行拆分。它将由空白字符分隔开的每个单词都视为一个词元，并且不会执行诸如转换为小写形式或去除标点符号等额外的规范化操作。
参考样例 # 以下示例请求创建一个名为 my_index 的新索引，并配置一个使用空格词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;whitespace_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;whitespace&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_whitespace_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_whitespace_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_whitespace_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch is fast! Really fast.&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;fast!</description></item><item><title>简单分割匹配词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/simple-pattern-split/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/simple-pattern-split/</guid><description>简单分割匹配词元生成器 # 简单分割匹配（simple_pattern_split）词元生成器使用正则表达式将文本分割成词元。该正则表达式定义了用于确定文本分割位置的模式。文本中任何匹配该模式的部分都会被用作分隔符，分隔符之间的文本则成为一个词元。当你想定义分隔符，并根据某个模式对文本的其余部分进行分词时，可以使用此词元生成器。
该词元生成器仅将输入文本中匹配到的部分（基于正则表达式）用作分隔符或边界，以将文本分割成词条。匹配到的部分不会包含在生成的词元中。例如，如果将词元生成器配置为在点号（.）处分割文本，输入文本为 one.two.three，那么生成的词元就是 one、two 和 three。点号本身不会包含在生成的词条中。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用简单分割匹配词元生成器的分词器。该分词器被配置为在连字符（-）处分割文本：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_pattern_split_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;simple_pattern_split&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;-&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_split_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_pattern_split_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_split_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_split_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;2024-10-09&amp;quot; } 返回内容包含产生的词元</description></item><item><title>经典词元生成器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/classic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/classic/</guid><description>经典词元生成器 # 经典（classic）词元生成器会解析文本，并应用英语语法规则将文本拆分为词元。它包含以下特定的逻辑来处理匹配规则：
首字母缩写词 电子邮件地址 域名 某些类型的标点符号 这种词元生成器最适合处理英语文本。对于其他语言，尤其是那些具有不同语法结构的语言，它可能无法产生最佳效果。
经典词元生成器按如下方式解析文本：
标点符号：在大多数标点符号处拆分文本，并移除标点字符。后面不跟空格的点号会被视为词元的一部分。 连字符：在连字符处拆分单词，但当词元中存在数字时除外。当词元中存在数字时，该词元不会被拆分，而是被当作产品编号处理。 电子邮件：识别电子邮件地址和主机名，并将它们作为单个词元保留。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用经典词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_classic_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;classic&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_classic_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_classic_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;For product AB3423, visit X&amp;amp;Y at example.</description></item><item><title>路径词元分词器</title><link>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/path-hierarchy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.3/docs/references/text-analysis/tokenizers/path-hierarchy/</guid><description>路径词元分词器 # 路径（path_hierarchy）词元分词器可以把文件路径（或类似的层次结构）的文本按照各个层级分解为词元。当处理诸如文件路径、网址或其他有分隔符的层次结构数据时，这个词元生成器特别有用。
参考样例 # 以下示例请求创建一个名为 my_index 的新索引，并配置一个使用路径词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_path_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;path_hierarchy&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_path_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_path_tokenizer&amp;quot; } } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_path_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;/users/john/documents/report.txt&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;/users&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;/users/john&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 11, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;/users/john/documents&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 21, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;/users/john/documents/report.</description></item></channel></rss>