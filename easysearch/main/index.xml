<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>INFINI Easysearch</title><link>/easysearch/main/</link><description>Recent content on INFINI Easysearch</description><generator>Hugo -- gohugo.io</generator><atom:link href="/easysearch/main/index.xml" rel="self" type="application/rss+xml"/><item><title>部署 Operator</title><link>/easysearch/main/docs/getting-started/install/operator/deploy_operator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/operator/deploy_operator/</guid><description>部署 Easysearch Operator # Easysearch Operator 只能在 k8s 环境下部署安装，请准备好一套 k8s 环境
部署前准备 # k8s 环境
要求Kubernetes 1.9以上版本，自 1.9 版本以后，StatefulSet成为了在Kubernetes中管理有状态应用的标准方式。 StorageClass
StorageClass 允许集群管理员定义多种存储方案，如快速的 SSD、标准的硬盘，或者其他的存储系统。无需手动预先创建存储资源，用户只需要在 PersistentVolumeClaim (PVC) 中指定需要的 StorageClass，存储资源就可以根据需求动态地创建。 ServiceAccount
创建一个 ServiceAccount 用于 Easysearch Operator 获取和操作 k8s 资源 apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/name: serviceaccount app.kubernetes.io/instance: controller-manager-sa app.kubernetes.io/component: rbac app.kubernetes.io/created-by: k8s-operator app.kubernetes.io/part-of: k8s-operator app.kubernetes.io/managed-by: kustomize name: controller-manager # ServiceAccount 的名字是 controller-manager namespace: default ClusterRole
创建 ClusterRole，用于定义访问 k8s 集群的角色权限 展开查看完整代码 .</description></item><item><title>本地配置</title><link>/easysearch/main/docs/references/security/configuration/yaml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/configuration/yaml/</guid><description>本地配置 # 通过安全模块的本地 YAML 配置文件可以方便的管理默认的内置用户或 隐藏的保留资源，例如 admin 管理员用户。不过通过 INFINI Console 或者 REST API 来创建其他用户、角色、映射、操作组和租户可能更容易。
user.yml # 此文件包含您要添加到内部用户数据库的默认初始用户。
配置文件里面的密码不能是明文，必须使用 Hash 之后的密码，通过命令 ./bin/hash_password.sh -p &amp;lt;new-password&amp;gt; 可以生成一个密码哈希。
_meta: type: &amp;#34;user&amp;#34; config_version: 2 admin: hash: &amp;#34;$2y$12$ZXx5R8NfuW2TYPOdGNY7a.43WKKBMCtN9aJywYWjAz9i11w7SrkqG&amp;#34; reserved: true external_roles: - &amp;#34;admin&amp;#34; description: &amp;#34;Default admin user&amp;#34; readonly: hash: &amp;#34;$2y$12$d9I16.5qpYhhsbiGN4zqdeA4k6BeMl/yEKRvTo3gzxFp8UC57EgJ.&amp;#34; reserved: false external_roles: - &amp;#34;readall&amp;#34; description: &amp;#34;Default readonly user&amp;#34; role.yml # 此文件包含要添加到内置数据库的默认初始角色。除了一些元数据之外，这个文件默认是是空的，因为系统已经了内置了若干角色，可以根据需要进行角色扩展。
complex-role: reserved: false hidden: false cluster: - &amp;#34;read&amp;#34; - &amp;#34;cluster:monitor/nodes/stats&amp;#34; - &amp;#34;cluster:monitor/task/get&amp;#34; indices: - names: - &amp;#34;kibana_sample_data_*&amp;#34; query: &amp;#39;{&amp;#34;match&amp;#34;: {&amp;#34;FlightDelay&amp;#34;: true}}&amp;#39; field_security: - &amp;#34;~FlightNum&amp;#34; field_mask: - &amp;#34;Carrier&amp;#34; privileges: - &amp;#34;read&amp;#34; static: false _meta: type: &amp;#34;roles&amp;#34; config_version: 2 role_mapping.</description></item><item><title>后端配置</title><link>/easysearch/main/docs/references/security/configuration/backend/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/configuration/backend/</guid><description>后端配置 # 配置安全模块的第一步是确定如何验证用户身份。尽管 Easysearch 本身可以充当一个内部用户数据库，但许多人更喜欢集成企业现有的身份认证体系，例如 LDAP 服务器，或两者组合。设置身份验证和授权服务端的主要配置文件位于 config/security/config.yml。它定义了安全模块如何检索用户凭据、如何验证这些凭据以及如何从后端系统获取其他角色（可选）。
config.yml 主要包含三大部分：
security: dynamic: http: ... authc: ... authz: ... HTTP # 配置 http 具有以下格式：
anonymous_auth_enabled: &amp;lt;true|false&amp;gt; 可以选择是否开启匿名访问，如果禁用匿名身份验证，则至少在 authc里面提供一个认证后端，否则安全模块将不予初始化，默认为 false。
认证 # 认证配置 authc 具有以下格式：
&amp;lt;name&amp;gt;: http_enabled: &amp;lt;true|false&amp;gt; transport_enabled: &amp;lt;true|false&amp;gt; order: &amp;lt;integer&amp;gt; http_authenticator: ... authentication_backend: ... 配置 authc 里面的每一项被称为 身份验证域。它指定来在何处获取用户凭据以及应针对哪个后端对它们进行身份验证。
您可以使用多个身份验证域。每个身份验证域都有一个名称（例如，basic_auth_internal）、enabled 开关和排序参数 order。该顺序使将身份验证域链接在一起成为可能。安全模块按您提供的顺序依次使用它们。如果用户成功通过一个域进行了身份验证，安全模块将跳过剩余的验证域。
http_authenticator 指定要在 HTTP 层上使用的身份验证方法。
以下是在 HTTP 层上定义身份验证器的语法：
http_authenticator: type: &amp;lt;type&amp;gt; challenge: &amp;lt;true|false&amp;gt; config: ... 参数 type 支持以下几种类型:</description></item><item><title>证书配置</title><link>/easysearch/main/docs/references/security/configuration/tls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/configuration/tls/</guid><description>配置 TLS 证书 # Easysearch 可以通过启用 TLS 传输加密来保护您数据的网络传输安全。 TLS 的相关设置要在配置文件 easysearch.yml 里面进行。主要包括两个部分的配置：传输层和 HTTP 层。传输层的 TLS 是必需的，HTTP 层的 TLS 的配置是可选的。
默认的配置如下：
security.ssl.transport.cert_file: instance.crt security.ssl.transport.key_file: instance.key security.ssl.transport.ca_file: ca.crt security.ssl.http.enabled: true security.ssl.http.cert_file: instance.crt security.ssl.http.key_file: instance.key security.ssl.http.ca_file: ca.crt 一键生成证书 # 启用 TLS 需要设置证书才能工作，通过执行命令 ./bin/initialize.sh 可以一键生成 TLS 证书，如下：
➜ ./bin/initialize.sh Generating RSA private key, 2048 bit long modulus .......................+++ ...............................+++ e is 65537 (0x10001) Generating RSA private key, 2048 bit long modulus .......................................................................................................................................................................................+++ .</description></item><item><title>系统索引</title><link>/easysearch/main/docs/references/security/configuration/system-indices/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/configuration/system-indices/</guid><description>系统索引 # Easysearch 默认的身份信息存放在一个受保护的系统索引里面，名称为：.security， 将索引设置为系统索引可以对该索引的数据进行额外的保护，因为即使您的用户帐户对所有索引具有读取权限，也无法直接访问此系统索引中的数据。
您可以在 easysearch.yml 中添加其它您希望需要受到保护的索引。
security.system_indices.enabled: true security.system_indices.indices: [&amp;#34;.infini-*&amp;#34;] 如果要访问系统索引，必须使用管理员证书的方式来进行： 配置管理证书:
curl -k --cert ./admin.crt --key ./admin.key -XGET &amp;#39;https://localhost:9200/.security/_search&amp;#39; 另一种方法是从每个节点上的 security.system_indices.index 列表中删除该索引，然后重新启动 Easysearch 即可正常操作该索引。</description></item><item><title>部署 Easysearch</title><link>/easysearch/main/docs/getting-started/install/operator/deploy_easysearch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/operator/deploy_easysearch/</guid><description>部署Easysearch Operator # 这里我们准备部署一个 3 节点的Easysearch 集群，准备 three-nodes-easysearch-cluster.yaml 文件，文件内容如下所示，并对关键字段都进行了注释。
apiVersion: infinilabs.infinilabs.com/v1 kind: SearchCluster # 自定义的资源类型 metadata: name: threenodes # Easysearch 集群的名称 namespace: default # Easysearch 集群所在的命名空间 spec: # 规格 security: # 安全相关 config: adminSecret: # admin证书配置 name: easysearch-admin-certs adminCredentialsSecret: # 账户密码配置 name: threenodes-admin-password tls: # tls 协议配置，包括节点间的transport，以及访问集群的http http: # 访问集群http配置 generate: false # 是否需要集群自动生成证书 secret: # 自定义证书配置 name: easysearch-certs transport: # 集群间访问配置 generate: false perNode: false # 是否给每一个节点配置证书 secret: # 自定义证书配置 name: easysearch-certs nodesDn: [&amp;#34;CN=Easysearch_Node&amp;#34;] adminDn: [&amp;#34;CN=Easysearch_Admin&amp;#34;] general: # 通用配置 snapshotRepositories: # s3 快照配置 - name: s3_repository # 配置的s3快照的名称 type: s3 # 快照类型 settings: # 快照配置 bucket: es-operator-bucket # s3中的桶，需提前建好 access_key: minioadmin # 访问s3密钥 secret_key: minioadmin endpoint: http://192.</description></item><item><title>用户角色</title><link>/easysearch/main/docs/references/security/access-control/users-roles/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/access-control/users-roles/</guid><description>角色与用户 # 安全模块包括一个内部用户数据库。使用此数据库代替外部身份验证系统（如 LDAP 或 Active Directory）或作为外部身份验证系统的补充。
角色是控制对群集的访问的核心方式。角色包含集群范围权限、特定于索引的权限、文档和字段级安全性以及租户的任意组合。然后，将用户映射到这些角色，以便用户获得这些权限。
除非您需要创建新的 只读或隐藏用户，我们强烈建议使用 REST API 来创建新的用户、角色和角色映射。.yml 文件用于初始设置，而不是持续使用。
创建用户 # user.yml # 参照 本地文件配置。
REST API # 参照 创建用户。
创建角色 # role.yml # 参照 本地文件配置。
REST API # 参照 创建角色。
映射用户到角色 # role_mapping.yml # 参照 本地文件配置。
REST API # 参照 创建角色映射。</description></item><item><title>搭建集群</title><link>/easysearch/main/docs/references/management/cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/cluster/</guid><description>搭建集群 # 在深入研究 Easysearch 以及搜索和聚合数据之前，你首先需要创建一个 Easysearch 集群。
Easysearch 可以作为一个单节点或多节点集群运行。一般来说，配置两者的步骤是非常相似的。本页演示了如何创建和配置一个多节点集群，但只需做一些小的调整，你可以按照同样的步骤创建一个单节点集群。
要根据你的要求创建和部署一个 Easysearch 集群，重要的是要了解节点发现和集群形成是如何工作的，以及哪些设置对它们有影响。
有许多方法可以设计一个集群。下面的插图显示了一个基本架构。
这是一个四节点的集群，有一个专用的主节点，一个专用的协调节点，还有两个数据节点，这两个节点是主节点，也是用来摄取数据的。
下表提供了节点类型的简要描述。
节点类型 描述 最佳实践 Master 管理集群的整体运作并跟踪集群的状态。这包括创建和删除索引，跟踪加入和离开集群的节点，检查集群中每个节点的健康状况（通过运行 ping 请求），并将分片分配给节点。 在三个不同区域的三个专用主节点是几乎所有生产用例的正确方法。这可以确保你的集群永远不会失去法定人数。两个节点在大部分时间都是空闲的，除非一个节点宕机或需要一些维护。 Data 存储和搜索数据。在本地分片上执行所有与数据有关的操作（索引、搜索、聚合）。这些是你的集群的工作节点，需要比其他任何节点类型更多的磁盘空间。 当你添加数据节点时，保持它们在各区之间的平衡。例如，如果你有三个区，以三的倍数添加数据节点，每个区一个。我们建议使用存储和内存重的节点。 默认情况下，每个节点是一个主节点和数据节点。决定节点的数量，分配节点类型，并为每个节点类型选择硬件，取决于你的使用情况。你必须考虑到一些因素，如你想保留数据的时间，你的文件的平均大小，你的典型工作负载（索引、搜索、聚合），你的预期性价比，你的风险容忍度，等等。
在你评估所有这些要求之后，我们建议你使用一个管理工具。要开始使用 INFINI Console，请参阅 INFINI Console 文档。
本页演示了如何处理不同的节点类型。它假设你有一个类似于前面插图的四节点集群。
前提条件 # 在你开始之前，你必须在你的所有节点上安装和配置 Easysearch。有关可用选项的信息，请参见 安装和配置。
完成后，使用 SSH 连接到每个节点，然后打开 config/easysearch.yml 文件。
你可以在这个文件中为你的集群设置所有的配置。
Step 1: 命名集群 # 为集群指定一个唯一的名字。如果你不指定集群名称，它将被默认设置为 easysearch。设置一个描述性的集群名称很重要，特别是如果你想在一个网络内运行多个集群。
要指定集群名称，请修改下面一行。
#cluster.name: my-application to</description></item><item><title>文档权限</title><link>/easysearch/main/docs/references/security/access-control/document-level-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/access-control/document-level-security/</guid><description>文档级权限 # 文档级权限允许您将角色限制为索引中文档的一部分子集。
参考设置 # 文档级权限使用 Easysearch 查询 DSL 来定义角色授予对哪些文档的访问权限。
{ &amp;#34;bool&amp;#34;: { &amp;#34;must&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;public&amp;#34;: &amp;#34;true&amp;#34; } } } } 上面的查询指定了该角色访问的文档里面，其字段 public 必须匹配 true。
指定字段 query 并设置为将上面的查询，并对查询字符串进行转义，最后的角色设置如下：
PUT _security/role/public_data { &amp;#34;cluster&amp;#34;: [ &amp;#34;*&amp;#34; ], &amp;#34;indices&amp;#34;: [{ &amp;#34;names&amp;#34;: [ &amp;#34;pub*&amp;#34; ], &amp;#34;query&amp;#34;: &amp;#34;{\&amp;#34;term\&amp;#34;: { \&amp;#34;public\&amp;#34;: true}}&amp;#34;, &amp;#34;privileges&amp;#34;: [ &amp;#34;read&amp;#34; ] }] } 上面的查询也可以根据需要写的很复杂，但是我们建议保持简单，以最大程度地减少文档级安全功能对集群的性能影响。
参数替换 # 查询过程中可以利用上下文变量，可根据当前用户的属性来强制实施规则替换。例如 ${user.name} 将替换为当前用户的名称。
如下规则允许用户读取字段 readable_by 为其用户名值的任何文档：
PUT _security/role/user_data { &amp;#34;cluster&amp;#34;: [ &amp;#34;*&amp;#34; ], &amp;#34;indices&amp;#34;: [{ &amp;#34;names&amp;#34;: [ &amp;#34;pub*&amp;#34; ], &amp;#34;query&amp;#34;: &amp;#34;{\&amp;#34;term\&amp;#34;: { \&amp;#34;readable_by\&amp;#34;: \&amp;#34;${user.</description></item><item><title>CAT API</title><link>/easysearch/main/docs/references/management/catapis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/catapis/</guid><description>cat API # 您可以使用紧凑且对齐的文本 （CAT） API 以易于理解的表格格式获取有关集群的基本统计信息。cat API 是一个人类可读的接口，它返回纯文本而不是传统的 JSON。
使用 cat API，您可以回答诸如哪个节点是选定的主节点、集群处于什么状态、每个索引中有多少文档等问题。
要查看 cat API 中的可用操作，请使用以下命令：
GET _cat 还可以在查询中使用以下字符串参数。
参数 描述 ?v 通过向列添加标题使输出更详细。它还添加了一些格式，以帮助将每列对齐在一起。此页面上的所有示例都包含 v 参数。 ?help 列出给定操作的默认标头和其他可用标头。 ?h 将输出限制为特定标头。 ?format 以 JSON、YAML 或 CBOR 格式输出结果。 ?sort 按指定列对输出进行排序。 要查看每列表示的内容，请使用 ?v 参数：
GET _cat/&amp;lt;operation_name&amp;gt;?v 要查看所有可用的标头，请使用 ?help 参数：
GET _cat/&amp;lt;operation_name&amp;gt;?help 要将输出限制为标头的子集，请使用 ?h 参数：</description></item><item><title>增删改查</title><link>/easysearch/main/docs/references/document/index-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/document/index-data/</guid><description>增删改查 # 您可以使用 REST API 对数据进行索引。存在两个 API：索引 API 和 _bulk API。
对于新数据增量到达的情况（例如，来自小型企业的客户订单），您可以使用索引 API 在文档到达时单独添加文档。对于数据流不太频繁的情况（例如，每周更新一次营销网站），您可能更希望生成一个文件并将其发送到 _bulk API。对于大量文档，将请求汇总在一起并使用 _bulk API 可提供优异的性能。然而，如果您的文档非常庞大，您可能需要单独对它们进行索引。
索引介绍 # 在搜索数据之前，必须对其进行索引。索引是搜索引擎组织数据以便快速检索的方法。生成的结构称为索引。
在 Easysearch 中，数据的基本单位是 JSON 文档。在索引中，Easysearch 使用唯一的 ID 标识每个文档。
对索引 API 的请求如下所示：
PUT &amp;lt;index&amp;gt;/_doc/&amp;lt;id&amp;gt; { &amp;#34;A JSON&amp;#34;: &amp;#34;document&amp;#34; } 对 _bulk API 的请求看起来有点不同，因为您在批量数据中指定了索引和 ID：
POST _bulk { &amp;#34;index&amp;#34;: { &amp;#34;_index&amp;#34;: &amp;#34;&amp;lt;index&amp;gt;&amp;#34;, &amp;#34;_id&amp;#34;: &amp;#34;&amp;lt;id&amp;gt;&amp;#34; } } { &amp;#34;A JSON&amp;#34;: &amp;#34;document&amp;#34; } 批量数据采用 _bulk API 操作必须符合特定的格式，该格式要求在每行（包括最后一行）的末尾都有一个换行符（ \n ）。这是基本格式：
Action and metadata\n Optional document\n Action and metadata\n Optional document\n 文档是可选的，因为 删除 操作不需要文档。其他操作（ 索引 、 创建 和 更新 ）都需要文档。如果您特别希望在文档已存在的情况下操作失败，请使用 创建 操作而不是 索引 操作。</description></item><item><title>字段权限</title><link>/easysearch/main/docs/references/security/access-control/field-level-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/access-control/field-level-security/</guid><description>字段级权限 # 字段级权限允许您控制用户可以查看哪些文档的字段。就像 文档级权限，您可以通过角色中的索引控制访问。
包括或排除字段 # 配置字段级权限时，有两个选项：包括或排除字段。如果包含字段，则用户在检索文档时 只能看到 这些字段。例如，如果您包含 actors、title 和 year 字段，则搜索结果可能如下所示：
POST movies/_doc/1 { &amp;#34;year&amp;#34;: 2013, &amp;#34;title&amp;#34;: &amp;#34;Rush&amp;#34;, &amp;#34;actors&amp;#34;: [ &amp;#34;Daniel Brühl&amp;#34;, &amp;#34;Chris Hemsworth&amp;#34;, &amp;#34;Olivia Wilde&amp;#34; ] } 如果是排除字段，则用户在检索文档时会看到除这些字段之外的所有内容。例如，如果排除这些相同的字段，则相同的搜索结果可能如下所示：
POST movies/_doc/2 { &amp;#34;directors&amp;#34;: [ &amp;#34;Ron Howard&amp;#34; ], &amp;#34;plot&amp;#34;: &amp;#34;A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.&amp;#34;, &amp;#34;genres&amp;#34;: [ &amp;#34;Action&amp;#34;, &amp;#34;Biography&amp;#34;, &amp;#34;Drama&amp;#34;, &amp;#34;Sport&amp;#34; ] } 您可以使用配置文件 role.yml 和 REST API 来指定字段级安全设置。</description></item><item><title>ASCII 折叠分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/ascii-folding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/ascii-folding/</guid><description>ASCII 折叠分词过滤器 # ASCII 折叠(asciifolding)分词过滤器将非 ASCII 字符转换为与其最接近的 ASCII 等效字符。例如，“é” 变为 “e”，“ü” 变为 “u”，“ñ” 变为 “n”。这个过程被称为&amp;quot;音译&amp;quot;。
ASCII 折叠分词过滤器有许多优点：
增强搜索灵活性：用户在输入查询内容时常常会省略重音符号或特殊字符。ASCII 折叠分词过滤器可确保即使是这样的查询也仍然能返回相关结果。 规范化：通过确保重音字符始终被转换为其 ASCII 等效字符，使索引编制过程标准化。 国际化：对于包含多种语言和字符集的应用程序特别有用。 尽管 ASCII 折叠分词过滤器可以简化搜索，但它也可能导致特定信息的丢失，尤其是当数据集中重音字符和非重音字符之间的区别很重要的时候。
参数说明 # 你可以使用 preserve_original 参数来配置 ASCII 折叠分词过滤器。将此参数设置为 true 时，会在词元流中同时保留原始词元及其 ASCII 折叠后的版本。当你希望在搜索查询中同时匹配一个词项的原始版本（带重音符号）和规范化版本（不带重音符号）时，这一点会特别有用。该参数的默认值为 false。
参考样例 # 以下示例请求创建了一个名为 example_index 的新索引，并定义了一个分词器，该分词器使用了 ASCII 折叠过滤器，且将 preserve_original 参数设置为 true：
PUT /example_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;custom_ascii_folding&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;asciifolding&amp;quot;, &amp;quot;preserve_original&amp;quot;: true } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_ascii_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;custom_ascii_folding&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>CJK 二元分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/cjk-bigram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/cjk-bigram/</guid><description>CJK 二元分词过滤器 # CJK 二元(cjk_bigram)分词过滤器是专门为处理东亚语言（如中文、日文和韩文，简称 CJK）而设计的，这些语言通常不使用空格来分隔单词。二元组是指在词元字符串中两个相邻元素的序列，这些元素可以是字符或单词。对于 CJK 语言来说，二元组有助于近似确定单词边界，并捕捉那些能够传达意义的重要字符对。
参数说明 # CJK 二元分词过滤器可以通过两个参数进行配置：ignore_scripts（忽略字符集）和 output_unigrams（输出一元组）。
ignore_scripts（忽略字符集） # CJK 二元分词过滤器会忽略所有非 CJK 字符集（如拉丁字母或西里尔字母等书写系统），并且仅将 CJK 文本分词为二元组。可使用此选项指定要忽略的 CJK 字符集。该选项有以下有效值：
han（汉字）：汉字字符集处理汉字。汉字是用于中国、日本和韩国书面语言的意符文字。该过滤器可帮助处理文本相关任务，例如对用中文、日文汉字或韩文汉字书写的文本进行分词、规范化或词干提取。 hangul（韩文）：韩文（谚文）字符集处理韩文字符，这些字符是韩语所特有的，不存在于其他东亚字符集中。 hiragana（平假名）：平假名字符集处理平假名，平假名是日语书写系统中使用的两种音节文字之一。平假名通常用于日语的本土词汇、语法元素以及某些形式的标点符号。 katakana（片假名）：片假名字符集处理片假名，片假名是日语的另一种音节文字。片假名主要用于外来借词、拟声词、科学名称以及某些日语词汇。 output_unigrams（输出一元组） # 当此选项设置为 true 时，会同时输出一元组（单个字符）和二元组。默认值为 false。
参考样例 # 以下示例请求创建了一个名为 devanagari_example_index 的新索引，并定义了一个分词器，该分词器使用了 cjk_bigram_filter 过滤器，且将 ignored_scripts 参数设置为 katakana（片假名）：
PUT /cjk_bigram_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;cjk_bigrams_no_katakana&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;cjk_bigrams_no_katakana_filter&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;cjk_bigrams_no_katakana_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;cjk_bigram&amp;quot;, &amp;quot;ignored_scripts&amp;quot;: [ &amp;quot;katakana&amp;quot; ], &amp;quot;output_unigrams&amp;quot;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>CJK 宽度分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/cjk-width/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/cjk-width/</guid><description>CJK 宽度分词过滤器 # CJK 宽度(cjk_width)分词过滤器通过将全角 ASCII 字符转换为其标准（半角）ASCII 等效字符，并将半角片假名字符转换为全角等效字符，来对中文、日文和韩文（CJK）词元进行规范化处理。
转换全角 ASCII 字符 # 在 CJK 文本中，ASCII 字符（如字母和数字）可能会以全角形式出现，占据两个半角字符的空间。全角 ASCII 字符在东亚排版中通常用于与 CJK 字符的宽度对齐。然而，出于索引和搜索的目的，这些全角字符需要规范化为其标准（半角）ASCII 等效字符。
以下示例说明了 ASCII 字符的规范化过程：
全角: ＡＢＣＤＥ １２３４５ 规范化 (半角): ABCDE 12345 转换半角片假名字符 # CJK 宽度分词过滤器会将半角片假名字符转换为对应的全角片假名字符，全角片假名是日语文本中使用的标准形式。如下例所示，这种规范化处理对于文本处理和搜索的一致性至关重要：
半角 katakana: ｶﾀｶﾅ 规范化 (全角) katakana: カタカナ 参考样例 # 以下示例请求创建了一个名为 cjk_width_example_index 的新索引，并定义了一个包含 cjk_width 过滤器的分词器：
PUT /cjk_width_example_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;cjk_width_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;cjk_width&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>Exists 查询</title><link>/easysearch/main/docs/references/search/term/exists/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term/exists/</guid><description>Exists 查询 # 使用 exists 查询来搜索包含特定字段的文档。
如果出现以下任一情况，索引值将不会存在于文档字段中：
该字段在映射中指定了 &amp;quot;index&amp;quot; : false 。 源 JSON 中的字段为 null 或 [] 。 字段值的长度超过了映射中 ignore_above 的设置。 字段值格式错误，并且映射中定义了 ignore_malformed 。 索引值将在以下情况下存在于文档字段中：
该值是一个包含一个或多个 null 元素和一个或多个非 null 元素的数组（例如， [&amp;quot;one&amp;quot;, null] ）。 该值是一个空字符串（ &amp;quot;&amp;quot; 或 &amp;quot;-&amp;quot; ）。 该值是一个自定义的 null_value ，如字段映射中所定义。 参考样例 # 例如，假设索引包含以下两个文档：
PUT testindex/_doc/1 { &amp;quot;title&amp;quot;: &amp;quot;The wind rises&amp;quot; } PUT testindex/_doc/2 { &amp;quot;title&amp;quot;: &amp;quot;Gone with the wind&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;A 1939 American epic historical film&amp;quot; } 以下查询搜索包含 description 字段的文档：</description></item><item><title>HTML 剥离字符过滤器</title><link>/easysearch/main/docs/references/text-analysis/character-filters/html-strip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/character-filters/html-strip/</guid><description>HTML 剥离字符过滤器 # HTML 剥离（html_strip）字符过滤器会从输入文本中移除 HTML 标签（例如 &amp;lt;div&amp;gt;、&amp;lt;p&amp;gt; 和 &amp;lt;a&amp;gt; 等）并输出纯文本。该过滤器可以配置保留某些标签，或者配置把特定的 HTML 标签实体（如 &amp;amp;nbsp;）解码为空格。
参考样例 # 以下请求展示将 html_strip 字符过滤器应用于文本：
GET /_analyze { &amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;, &amp;quot;char_filter&amp;quot;: [ &amp;quot;html_strip&amp;quot; ], &amp;quot;text&amp;quot;: &amp;quot;&amp;lt;p&amp;gt;Commonly used calculus symbols include &amp;amp;alpha;, &amp;amp;beta; and &amp;amp;theta; &amp;lt;/p&amp;gt;&amp;quot; } 返回内容中包含的词元里，可以看到 HTML 字符已被转换为它们的解码后的值：
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;\nCommonly used calculus symbols include α, β and θ \n&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 74, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] } 参数说明 # html_strip 字符过滤器可以使用以下参数进行配置。</description></item><item><title>IDs 查询</title><link>/easysearch/main/docs/references/search/term/ids/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term/ids/</guid><description>IDs 查询 # 使用 ids 查询在 _id 字段中搜索具有一个或多个特定文档 ID 值的文档。例如，以下查询请求 ID 为 34229 和 91296 的文档：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;ids&amp;quot;: { &amp;quot;values&amp;quot;: [ 34229, 91296 ] } } } 参数说明 # 查询接受以下参数。
参数 数据类型 描述 values Array of strings 要搜索的文档 ID。必填。 boost Float 一个浮点值，用于指定此字段相对于相关性分数的权重。值高于 1.0 会增加字段的相关性。值介于 0.0 和 1.0 之间会降低字段的相关性。默认值为 1.0。</description></item><item><title>IK 分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/ik-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/ik-analyzer/</guid><description>IK 分词器 # IK 分词器是一款专为处理中文文本设计的分词器，高效且智能。支持 ik_smart 和 ik_max_word 两种分词模式。
IK 分词器安装 # IK 分词插件安装命令如下：
bin/easysearch-plugin install analysis-ik 同时也需要安装 ingest-common 插件：
bin/easysearch-plugin install ingest-common 如果觉得比较麻烦，也可以直接使用 Easysearch 的 bundle 包安装部署。
使用样例 # 下面的命令样例展示了 IK 的使用方式。
# 1.创建索引 PUT index_ik # 2.创建映射关系 POST index_ik/_mapping { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;, &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_smart&amp;quot; } } } # 3.写入文档 POST index_ik/_create/1 {&amp;quot;content&amp;quot;:&amp;quot;美国留给伊拉克的是个烂摊子吗&amp;quot;} POST index_ik/_create/2 {&amp;quot;content&amp;quot;:&amp;quot;公安部：各地校车将享最高路权&amp;quot;} POST index_ik/_create/3 {&amp;quot;content&amp;quot;:&amp;quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&amp;quot;} POST index_ik/_create/4 {&amp;quot;content&amp;quot;:&amp;quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&amp;quot;} # 4.</description></item><item><title>Intervals 查询</title><link>/easysearch/main/docs/references/search/full-text/intervals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/full-text/intervals/</guid><description>Intervals 查询 # 间隔查询根据匹配词的邻近度和顺序来匹配文档。它将一组匹配规则应用于指定字段中的词。该查询生成跨越文本中词的最小间隔序列。你可以组合间隔并按父源进行过滤。
考虑一个包含以下文档的索引：
PUT testindex/_doc/1 { &amp;quot;title&amp;quot;: &amp;quot;key-value pairs are efficiently stored in a hash table&amp;quot; } PUT /testindex/_doc/2 { &amp;quot;title&amp;quot;: &amp;quot;store key-value pairs in a hash map&amp;quot; } 例如，以下查询搜索包含短语 key-value pairs （词之间没有间隔）后跟 hash table 或 hash map 的文档：
GET /testindex/_search { &amp;quot;query&amp;quot;: { &amp;quot;intervals&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;all_of&amp;quot;: { &amp;quot;ordered&amp;quot;: true, &amp;quot;intervals&amp;quot;: [ { &amp;quot;match&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;key-value pairs&amp;quot;, &amp;quot;max_gaps&amp;quot;: 0, &amp;quot;ordered&amp;quot;: true } }, { &amp;quot;any_of&amp;quot;: { &amp;quot;intervals&amp;quot;: [ { &amp;quot;match&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;hash table&amp;quot; } }, { &amp;quot;match&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;hash map&amp;quot; } } ] } } ] } } } } } 该查询返回两个文档：</description></item><item><title>IP 范围聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/ip-range/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/ip-range/</guid><description>IP 范围聚合 # ip_range IP 范围聚合用于 IP 地址。它适用于 ip 类型字段。您可以在 CIDR 表示法中定义 IP 范围和掩码。
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;access&amp;quot;: { &amp;quot;ip_range&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;ip&amp;quot;, &amp;quot;ranges&amp;quot;: [ { &amp;quot;from&amp;quot;: &amp;quot;1.0.0.0&amp;quot;, &amp;quot;to&amp;quot;: &amp;quot;126.158.155.183&amp;quot; }, { &amp;quot;mask&amp;quot;: &amp;quot;1.0.0.0/8&amp;quot; } ] } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;access&amp;quot; : { &amp;quot;buckets&amp;quot; : [ { &amp;quot;key&amp;quot; : &amp;quot;1.0.0.0/8&amp;quot;, &amp;quot;from&amp;quot; : &amp;quot;1.0.0.0&amp;quot;, &amp;quot;to&amp;quot; : &amp;quot;2.0.0.0&amp;quot;, &amp;quot;doc_count&amp;quot; : 98 }, { &amp;quot;key&amp;quot; : &amp;quot;1.</description></item><item><title>Keyword 字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/string-field-type/keyword/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/string-field-type/keyword/</guid><description>Keyword 字段类型 # keyword 字段类型包含未经分析的字符串。它只允许精确的大小写敏感匹配。
默认情况下，keyword 字段既被索引（因为 index 已启用）也存储在磁盘上（因为 doc_values 已启用）。为了减少磁盘空间，您可以通过将 index 设置为 false 来指定不索引 keyword 字段。
如果您需要对字段进行全文搜索，请将其映射为 text 类型。
代码样例 # 以下查询创建了一个带有 keyword 字段的映射。将 index 设置为 false 指定将 genre 字段存储在磁盘上，并使用 doc_values 检索它：
PUT movies { &amp;#34;mappings&amp;#34; : { &amp;#34;properties&amp;#34; : { &amp;#34;genre&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;keyword&amp;#34;, &amp;#34;index&amp;#34; : false } } } } 参数说明 # 下表列出了 keyword 字段类型接受的参数。所有参数都是可选的。
参数 描述 boost 浮点值，指定此字段对相关性分数的权重。大于 1.</description></item><item><title>KStem 分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/kstem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/kstem/</guid><description>KStem 分词过滤器 # KStem 分词过滤器是一种词干提取过滤器，用于将单词还原为其词根形式。该过滤器是一种专为英语设计的轻量级算法词干提取器，它执行以下词干提取操作：
将复数形式的单词还原为单数形式。 将不同的动词时态转换为其基本形式。 去除常见的派生词尾，例如 “-ing” 或 “-ed”。 KStem 分词过滤器等同于配置了 light_english 语言的词干提取器过滤器。与其他词干提取过滤器（如词干提取器 porter_stem）相比，它提供了更为保守的词干提取方式。
KStem 分词过滤器基于 Lucene 的 KStemFilter。如需了解更多信息，请参阅 Lucene 文档。
参考样例 # 以下示例请求创建了一个名为 my_kstem_index 的新索引，并配置了一个带有 KStem 过滤器的分词器：
PUT /my_kstem_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;kstem_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;kstem&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_kstem_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;kstem_filter&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_kstem_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>n-gram 分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/n-gram/</guid><description>n-gram 分词过滤器 # n-gram 分词过滤器是一种强大的工具，用于将文本拆分为更小的组件，即 n-gram，这有助于提升部分匹配和模糊搜索能力。它通过将一个词元拆分成指定长度的子字符串来工作。这些过滤器在搜索应用程序中很常用，可用于支持自动补全、部分匹配以及容错拼写搜索。
参数说明 # n-gram 分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_gram 可选 整数 n-gram 的最小长度。默认值为 1。 max_gram 可选 整数 n-gram 的最大长度。默认值为 2。 preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 false。 参考样例 # 以下示例请求创建了一个名为“ngram_example_index”的新索引，并配置了一个带有 n-gram 过滤器的分词器：
PUT /ngram_example_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;ngram_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 2, &amp;quot;max_gram&amp;quot;: 3 } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;ngram_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;ngram_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>N-gram 词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/n-gram/</guid><description>N-gram 词元生成器 # N-gram 词元生成器会将文本拆分为指定长度的重叠 n-gram（固定长度为 n 的字符序列）。当你希望实现部分单词匹配或自动补全搜索功能时，这个分词器特别有用，因为它会生成原始输入文本的子字符串（ n-gram 字符串）。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 n-gram 词元生成器的分词器。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_ngram_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 3, &amp;quot;max_gram&amp;quot;: 4, &amp;quot;token_chars&amp;quot;: [&amp;quot;letter&amp;quot;, &amp;quot;digit&amp;quot;] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_ngram_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_ngram_tokenizer&amp;quot; } } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_ngram_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Search&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ {&amp;quot;token&amp;quot;: &amp;quot;Sea&amp;quot;,&amp;quot;start_offset&amp;quot;: 0,&amp;quot;end_offset&amp;quot;: 3,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 0}, {&amp;quot;token&amp;quot;: &amp;quot;Sear&amp;quot;,&amp;quot;start_offset&amp;quot;: 0,&amp;quot;end_offset&amp;quot;: 4,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 1}, {&amp;quot;token&amp;quot;: &amp;quot;ear&amp;quot;,&amp;quot;start_offset&amp;quot;: 1,&amp;quot;end_offset&amp;quot;: 4,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 2}, {&amp;quot;token&amp;quot;: &amp;quot;earc&amp;quot;,&amp;quot;start_offset&amp;quot;: 1,&amp;quot;end_offset&amp;quot;: 5,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 3}, {&amp;quot;token&amp;quot;: &amp;quot;arc&amp;quot;,&amp;quot;start_offset&amp;quot;: 2,&amp;quot;end_offset&amp;quot;: 5,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 4}, {&amp;quot;token&amp;quot;: &amp;quot;arch&amp;quot;,&amp;quot;start_offset&amp;quot;: 2,&amp;quot;end_offset&amp;quot;: 6,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 5}, {&amp;quot;token&amp;quot;: &amp;quot;rch&amp;quot;,&amp;quot;start_offset&amp;quot;: 3,&amp;quot;end_offset&amp;quot;: 6,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 6} ] } 参数说明 # N-gram 词元生成器可以使用以下参数进行配置。</description></item><item><title>Rank 字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/rank/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/rank/</guid><description>Rank 字段类型 # 下表列出了 Easysearch 支持的所有 rank 字段类型。
字段数据类型 描述 rank_feature 提升或降低文档的相关性得分。 rank_features 提升或降低文档的相关性得分。用于特征列表稀疏的情况。 Rank feature 和 rank features 字段只能使用 rank feature 查询进行查询。它们不支持聚合或排序。
Rank feature 字段类型 # Rank feature 字段类型使用正浮点值来提升或降低文档在 rank_feature 查询中的相关性得分。默认情况下，该值会提升相关性得分。要降低相关性得分，请将可选参数 positive_score_impact 设置为 false。
示例 # 创建一个包含 rank feature 字段的映射：
PUT chessplayers { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;name&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34; }, &amp;#34;rating&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;rank_feature&amp;#34; }, &amp;#34;age&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;rank_feature&amp;#34;, &amp;#34;positive_score_impact&amp;#34;: false } } } } 索引三个文档，其中包含一个提升得分的 rank_feature 字段（rating）和一个降低得分的 rank_feature 字段（age）：</description></item><item><title>Span 前序匹配</title><link>/easysearch/main/docs/references/search/span/span-first/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/span/span-first/</guid><description>Span 前序匹配 # span_first 查询匹配从字段开头开始并在指定位置结束的跨度。当您想要查找出现在文档开头附近的词项或短语时，此查询很有用。
例如，您可以使用 span_first 查询来执行以下搜索：
查找在字段的最初几个词中出现的特定词项的文档。 确保某些短语出现在文本的开头或附近 仅在模式出现在指定距离内时匹配 参考样例 # 以下查询搜索词干“dress”出现在描述的前 4 个位置：
GET /clothing/_search { &amp;quot;query&amp;quot;: { &amp;quot;span_first&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;span_term&amp;quot;: { &amp;quot;description.stemmed&amp;quot;: &amp;quot;dress&amp;quot; } }, &amp;quot;end&amp;quot;: 4 } } } 该查询匹配文档 1 和 2：
文档 1 和 2 在第三位置包含单词 dress （“长袖连衣裙…”和“漂亮的连衣裙”）。索引单词的起始位置为 0，因此单词“dress”位于位置 2。 单词 dress 的位置必须小于 4 ，如 end 参数指定。 { &amp;quot;took&amp;quot;: 13, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 2, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 0.</description></item><item><title>Span 包含于查询</title><link>/easysearch/main/docs/references/search/span/span-within/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/span/span-within/</guid><description>Span 包含于查询 # span_within 查询匹配被另一个 span 查询所包围的跨度。它是 span_containing 的相反操作： span_containing 返回包含较小跨度的较大跨度，而 span_within 返回被较大跨度包围的较小跨度。
例如，您可以使用 span_within 查询来：
查找出现在较长短语中的较短短语。 匹配在特定上下文中出现的词项。 识别被较大模式包围的小模式。 参考样例 # 以下查询在包含“shirt”和“long”的跨度中搜索单词“dress”：
GET /clothing/_search { &amp;quot;query&amp;quot;: { &amp;quot;span_within&amp;quot;: { &amp;quot;little&amp;quot;: { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;dress&amp;quot; } }, &amp;quot;big&amp;quot;: { &amp;quot;span_near&amp;quot;: { &amp;quot;clauses&amp;quot;: [ { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;shirt&amp;quot; } }, { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;long&amp;quot; } } ], &amp;quot;slop&amp;quot;: 2, &amp;quot;in_order&amp;quot;: false } } } } } 该查询匹配文档 1 的原因是：</description></item><item><title>Span 包含查询</title><link>/easysearch/main/docs/references/search/span/span-containing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/span/span-containing/</guid><description>Span 包含查询 # span_containing 查询会在较大的文本模式（如短语或一组单词）的边界内查找包含较小文本模式的匹配项。可以将其视为仅在特定更大的上下文中出现时才查找单词或短语。
例如，您可以使用 span_containing 查询来执行以下搜索：
查找单词“quick”，但仅当它出现在同时提到狐狸和行为的句子中时。 确保某些词项出现在其他词项的上下文中——而不仅仅是在文档的任何地方。 搜索在更长的有意义的短语中出现的特定单词。 参考样例 # 以下查询搜索在包含“silk”和“dress”词语的较大词组（不一定按该顺序）中，与“red”一词出现且彼此之间不超过 5 个词的情况：
GET /clothing/_search { &amp;quot;query&amp;quot;: { &amp;quot;span_containing&amp;quot;: { &amp;quot;little&amp;quot;: { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;red&amp;quot; } }, &amp;quot;big&amp;quot;: { &amp;quot;span_near&amp;quot;: { &amp;quot;clauses&amp;quot;: [ { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;silk&amp;quot; } }, { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;dress&amp;quot; } } ], &amp;quot;slop&amp;quot;: 5, &amp;quot;in_order&amp;quot;: false } } } } } 该查询匹配文档 1 的原因是：
它找到一个词组，其中“silk”和“dress”出现且彼此之间不超过 5 个词（“…dress in red silk…”）。词语“silk”和“dress”彼此之间相隔 2 个词（它们之间有 2 个词）。 在这个更大的跨度内，它找到了“red”这个词。 { &amp;quot;took&amp;quot;: 4, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 1, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 1.</description></item><item><title>Span 多词项查询</title><link>/easysearch/main/docs/references/search/span/span-multi-term/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/span/span-multi-term/</guid><description>Span 多词项查询 # span_multi 查询允许您将多词查询（如 wildcard 、 fuzzy 、 prefix 、 range 或 regexp ）包装为 span 查询。这使您能够在其他 span 查询中使用这些更灵活的匹配查询。
例如，您可以使用 span_multi 查询来：
查找具有相同前缀的词语，并与其他词语靠近。 匹配跨度内单词的模糊变体。 在跨度查询中使用正则表达式。 span_multi 查询可能匹配多个词。为了避免过度内存使用，您可以：
为多词查询设置 rewrite 参数。 使用 topterms* 重写方法。 如果你仅使用 span_multi 进行 prefix 查询，请考虑为文本字段启用 index_prefixes 选项。这将自动将字段上的任何 prefix 查询重写为匹配索引前缀的单词查询。 参考样例 # span_multi 查询使用以下语法来包装 prefix 查询：
&amp;quot;span_multi&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;prefix&amp;quot;: { &amp;quot;description&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;flutter&amp;quot; } } } } 以下查询搜索以“dress”开头的单词，在彼此最多 5 个单词的距离内靠近任何形式的“sleeve”：</description></item><item><title>Span 或查询</title><link>/easysearch/main/docs/references/search/span/span-or/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/span/span-or/</guid><description>Span 或查询 # span_or 查询组合多个 span 查询，并匹配它们 span 的并集。如果其中至少一个包含的 span 查询匹配，则发生匹配。
例如，您可以使用 span_or 查询来：
查找匹配多个模式中的任意一个的 span。 将不同的 span 模式作为备选项组合。 在一个查询中匹配多个 span 变体。 参考样例 # 以下查询搜索“formal collar”或“button collar”在彼此 2 个词距离内出现：
GET /clothing/_search { &amp;quot;query&amp;quot;: { &amp;quot;span_or&amp;quot;: { &amp;quot;clauses&amp;quot;: [ { &amp;quot;span_near&amp;quot;: { &amp;quot;clauses&amp;quot;: [ { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;formal&amp;quot; } }, { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;collar&amp;quot; } } ], &amp;quot;slop&amp;quot;: 0, &amp;quot;in_order&amp;quot;: true } }, { &amp;quot;span_near&amp;quot;: { &amp;quot;clauses&amp;quot;: [ { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;button&amp;quot; } }, { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;collar&amp;quot; } } ], &amp;quot;slop&amp;quot;: 2, &amp;quot;in_order&amp;quot;: true } } ] } } } 该查询在指定的 slop 距离内匹配文档 1（“…formal collar…”）和文档 3（“…button-down collar…”）。</description></item><item><title>Span 排除查询</title><link>/easysearch/main/docs/references/search/span/span-not/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/span/span-not/</guid><description>Span 排除查询 # span_not 查询会排除与另一个 span 查询重叠的跨度。您还可以指定在排除的跨度之前或之后不允许匹配的距离范围。
例如，您可以使用 span_not 查询来：
查找除在特定短语中出现时的词项外。 除非它们靠近特定词项，否则匹配跨度。 排除在特定距离内出现的其他模式匹配。 参考样例 # 以下查询搜索单词“dress”，但当它出现在短语“dress shirt”中时不搜索：
GET /clothing/_search { &amp;quot;query&amp;quot;: { &amp;quot;span_not&amp;quot;: { &amp;quot;include&amp;quot;: { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;dress&amp;quot; } }, &amp;quot;exclude&amp;quot;: { &amp;quot;span_near&amp;quot;: { &amp;quot;clauses&amp;quot;: [ { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;dress&amp;quot; } }, { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;shirt&amp;quot; } } ], &amp;quot;slop&amp;quot;: 0, &amp;quot;in_order&amp;quot;: true } } } } } 该查询匹配文档 2，因为它包含单词“dress”（“Beautiful long dress…”）。文档 1 未匹配，因为它包含短语“dress shirt”，该短语被排除。文档 3 和 4 未匹配，因为它们包含单词“dress”的变体（“dressed”和“dresses”），并且查询是在原始字段中进行的。</description></item><item><title>Span 词项查询</title><link>/easysearch/main/docs/references/search/span/span-term/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/span/span-term/</guid><description>Span 词项查询 # span_term 查询是最基本的 span 查询，它匹配包含单个词的 span。它是更复杂的 span 查询的构建模块。
例如，您可以使用 span_term 查询来：
查找可用于其他 span 查询的精确词匹配。 匹配特定单词同时保持位置信息。 创建可与其他 span 查询组合的基本 span。 参考样例 # 以下查询搜索确切的词“formal”：
GET /clothing/_search { &amp;quot;query&amp;quot;: { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;formal&amp;quot; } } } 或者，您可以在 value 参数中指定搜索词：
GET /clothing/_search { &amp;quot;query&amp;quot;: { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;formal&amp;quot; } } } } 您也可以指定 boost 值来提升文档得分：
GET /clothing/_search { &amp;quot;query&amp;quot;: { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;formal&amp;quot;, &amp;quot;boost&amp;quot;: 2 } } } } 该查询匹配文档 1 和文档 2，因为它们包含确切的词项“formal”。位置信息被保留以用于其他 span 查询。</description></item><item><title>Span 跨字段查询</title><link>/easysearch/main/docs/references/search/span/span-field-masking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/span/span-field-masking/</guid><description>Span 跨字段查询 # field_masking_span 查询允许 span 查询通过“掩饰”查询的真实字段来匹配不同字段。这在处理多字段（相同内容使用不同分词器索引）或需要跨不同字段运行 span 查询（如 span_near 或 span_or ，这通常是不允许的）时特别有用。
例如，您可以使用 field_masking_span 查询来：
匹配原始字段及其词干版本中的词项。 在一个 span 操作中组合不同字段的 span 查询。 使用不同分词器索引的相同内容进行操作。 在使用字段遮罩时，相关性分数是根据遮罩字段的特性（范数）计算的，而不是实际搜索的字段。这意味着如果遮罩字段与被搜索字段具有不同的属性（如长度或提升值），您可能会收到意外的评分结果。
参考样例 # 以下查询在词干化字段中搜索单词“long”，并查找“sleeve”一词的变体附近：
GET /clothing/_search { &amp;quot;query&amp;quot;: { &amp;quot;span_near&amp;quot;: { &amp;quot;clauses&amp;quot;: [ { &amp;quot;span_term&amp;quot;: { &amp;quot;description&amp;quot;: &amp;quot;long&amp;quot; } }, { &amp;quot;field_masking_span&amp;quot;: { &amp;quot;query&amp;quot;: { &amp;quot;span_term&amp;quot;: { &amp;quot;description.stemmed&amp;quot;: &amp;quot;sleev&amp;quot; } }, &amp;quot;field&amp;quot;: &amp;quot;description&amp;quot; } } ], &amp;quot;slop&amp;quot;: 1, &amp;quot;in_order&amp;quot;: true } } } 查询匹配文档 1 和文档 4：</description></item><item><title>Span 邻近查询</title><link>/easysearch/main/docs/references/search/span/span-near/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/span/span-near/</guid><description>Span 邻近查询 # span_near 查询匹配彼此靠近的跨度。您可以指定跨度之间的距离，并指定它们是否需要按特定顺序出现。
例如，您可以使用 span_near 查询来：
查找彼此之间距离在特定范围内的词项。 匹配词语按特定顺序出现的短语。 查找文本中彼此靠近的相关概念。 参考样例 # 以下查询搜索任何形式的“sleeve”和“long”相邻出现，顺序不限：
GET /clothing/_search { &amp;quot;query&amp;quot;: { &amp;quot;span_near&amp;quot;: { &amp;quot;clauses&amp;quot;: [ { &amp;quot;span_term&amp;quot;: { &amp;quot;description.stemmed&amp;quot;: &amp;quot;sleev&amp;quot; } }, { &amp;quot;span_term&amp;quot;: { &amp;quot;description.stemmed&amp;quot;: &amp;quot;long&amp;quot; } } ], &amp;quot;slop&amp;quot;: 1, &amp;quot;in_order&amp;quot;: false } } } 该查询匹配文档 1（“Long-sleeved…”）和文档 2（“…long fluttered sleeves…”）。在文档 1 中，词语是相邻的，而在文档 2 中，它们在指定的 slop 距离 1 内（它们之间有一个词）。
{ &amp;quot;took&amp;quot;: 3, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 2, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 0.</description></item><item><title>SQL-JDBC</title><link>/easysearch/main/docs/references/sql/sql-jdbc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/sql/sql-jdbc/</guid><description>SQL-JDBC 驱动 # Easysearch 的 SQL jdbc 驱动程序是一个独立、直连的纯 Java 驱动程序，可将 JDBC 调用转换为 Easysearch SQL。
安装 # JDBC 驱动 可以从官网下载：https://release.infinilabs.com/easysearch/archive/plugins/sql-jdbc-1.7.1.jar
在 gradle 项目中安装 # 需要把sql-jdbc-1.x.x.jar 集成到本地 gradle 项目的libs目录，假设项目名称叫 jdbc-test
将下载的sql-jdbc jar 包放到 jdbc-test/libs/ 下：
在 项目 build.gradle 添加依赖
implementation files(&amp;lsquo;libs/sql-jdbc-1.0.0.jar&amp;rsquo;)
完整的build.gradle 配置：
plugins { id 'java' } group 'org.example' version '1.0-SNAPSHOT' repositories { mavenCentral() flatDir { dirs 'libs' } } dependencies { implementation files('libs/sql-jdbc-1.0.0.jar') testImplementation 'org.junit.jupiter:junit-jupiter-api:5.7.0' testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.7.0' } test { useJUnitPlatform() } 初始化 # String url = &amp;quot;jdbc:easysearch://https://localhost:9210&amp;quot;; Properties properties = new Properties(); properties.</description></item><item><title>Term 精确查询</title><link>/easysearch/main/docs/references/search/term/term/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term/term/</guid><description>Term 精确查询 # 使用 term 查询在字段中搜索确切的词项。例如，以下查询搜索包含确切的行号的行：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;term&amp;quot;: { &amp;quot;line_id&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;61809&amp;quot; } } } } 当文档被索引时， text 字段会被分词。分词包括对文本进行分词和转换为小写，并移除标点符号。与分词查询文本的 match 查询不同， term 查询仅匹配确切的词项，因此可能不会返回具有相关性的结果。避免在 text 字段上使用 term 查询。更多信息，请参阅精确查询与全文检索的对比。
您可以在 case_insensitive 参数中指定查询应不区分大小写：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;term&amp;quot;: { &amp;quot;speaker&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;HAMLET&amp;quot;, &amp;quot;case_insensitive&amp;quot;: true } } } } 返回内容包含匹配的文档，无论大小写是否有差异：
&amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 1582, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 2, &amp;quot;hits&amp;quot;: [ { &amp;quot;_index&amp;quot;: &amp;quot;shakespeare&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;32700&amp;quot;, &amp;quot;_score&amp;quot;: 2, &amp;quot;_source&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;line&amp;quot;, &amp;quot;line_id&amp;quot;: 32701, &amp;quot;play_name&amp;quot;: &amp;quot;Hamlet&amp;quot;, &amp;quot;speech_number&amp;quot;: 9, &amp;quot;line_number&amp;quot;: &amp;quot;1.</description></item><item><title>Text 字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/string-field-type/text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/string-field-type/text/</guid><description>Text 字段类型 # text 字段类型包含经过分词器分析的字符串。它用于全文搜索，因为它允许部分匹配。对多个词条的搜索可以匹配其中的一部分而不是全部。根据分词器的不同，搜索结果可以是大小写不敏感的、词干化的、去除停用词的、应用同义词的等等。
如果您需要进行精确值搜索，请将字段映射为 keyword 类型。
代码样例 # 创建一个带有 text 字段的映射：
PUT movies { &amp;#34;mappings&amp;#34; : { &amp;#34;properties&amp;#34; : { &amp;#34;title&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;text&amp;#34; } } } } 参数说明 # 下表列出了 text 字段类型接受的参数。所有参数都是可选的。
参数 描述 analyzer 用于此字段的分词器。默认情况下，它将在索引时和搜索时使用。要在搜索时覆盖它，请设置 search_analyzer 参数。默认是 standard 分词器，它使用基于语法的分词，并基于 Unicode 文本分段算法。 boost 浮点值，指定此字段对相关性分数的权重。大于 1.0 的值会增加字段的相关性。0.0 到 1.0 之间的值会降低字段的相关性。默认值为 1.0。 eager_global_ordinals 指定是否应在刷新时立即加载全局序号。如果该字段经常用于聚合，此参数应设置为 true。默认值为 false。 fielddata 布尔值，指定是否访问此字段的已分析标记以进行排序、聚合和脚本编写。默认值为 false。 fielddata_frequency_filter JSON 对象，指定仅将文档频率在 min 和 max 值之间的已分析标记加载到内存中（以绝对数字或百分比提供）。频率按段计算。参数：min、max、min_segment_size。默认加载所有已分析的标记。 fields 要以多种方式索引同一个字符串（例如，作为 keyword 和 text），请提供 fields 参数。您可以指定字段的一个版本用于搜索，另一个版本用于排序和聚合。 index 布尔值，指定字段是否应可搜索。默认值为 true。 index_options 指定要存储在索引中用于搜索和突出显示的信息。有效值：docs（仅文档编号）、freqs（文档编号和词频）、positions（文档编号、词频和词位置）、offsets（文档编号、词频、词位置以及开始和结束字符偏移量）。默认值为 positions。 index_phrases 布尔值，指定是否单独索引 2-gram。2-gram 是此字段字符串中两个连续单词的组合。导致精确短语查询更快但索引更大。当不删除停用词时效果最好。默认值为 false。 index_prefixes JSON 对象，指定单独索引词条前缀。前缀中的字符数在 min_chars 和 max_chars 之间（包含）。导致前缀搜索更快但索引更大。可选参数：min_chars、max_chars。默认 min_chars 为 2，max_chars 为 5。 meta 接受此字段的元数据。 norms 布尔值，指定在计算相关性分数时是否应使用字段长度。默认值为 false。 position_increment_gap 当文本字段被分析时，它们被分配位置。如果一个字段包含一个字符串数组，并且这些位置是连续的，这将导致可能跨不同数组元素匹配。为防止这种情况，在连续的数组元素之间插入一个人工间隙。您可以通过指定整数 position_increment_gap 来更改此间隙。注意：如果 slop 大于 position_element_gap，可能会发生跨不同数组元素的匹配。默认值为 100。 similarity 用于计算相关性分数的排名算法。默认值为 BM25。 term_vector 布尔值，指定是否应存储此字段的词向量。默认值为 no。 词向量参数 # 词向量在分析过程中产生。它包含：</description></item><item><title>Top 汇总</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/top-hits/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/top-hits/</guid><description>Top 汇总 # top_hits 指标是一种多值指标汇总，它根据聚合字段的相关性评分对匹配文档进行排名。
您可以指定以下选项：
from : 命中的起始位置。 size : 返回命中的最大数量。默认值为 3。 sort : 匹配命中的排序方式。默认情况下，命中的排序依据聚合查询的相关性得分。 以下示例返回数据中的前 5 个产品：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;top_hits_products&amp;quot;: { &amp;quot;top_hits&amp;quot;: { &amp;quot;size&amp;quot;: 5 } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;top_hits_products&amp;quot; : { &amp;quot;hits&amp;quot; : { &amp;quot;total&amp;quot; : { &amp;quot;value&amp;quot; : 4675, &amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot; : 1.0, &amp;quot;hits&amp;quot; : [ { &amp;quot;_index&amp;quot; : &amp;quot;sample_data_ecommerce&amp;quot;, &amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;, &amp;quot;_id&amp;quot; : &amp;quot;glMlwXcBQVLeQPrkHPtI&amp;quot;, &amp;quot;_score&amp;quot; : 1.</description></item><item><title>UAX URL 邮件词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/uax-url-email/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/uax-url-email/</guid><description>UAX URL 邮件词元生成器 # 除了常规文本之外，UAX URL 邮件（uax_url_email）词元生成器还专门用于处理网址、电子邮件地址和域名。它基于 Unicode 文本分割算法（ UAX #29），这使得它能够正确地对复杂文本（包括网址和电子邮件地址）进行分词处理。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 UAX URL 邮件词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;uax_url_email_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;uax_url_email&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_uax_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;uax_url_email_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_uax_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_uax_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Contact us at support@example.</description></item><item><title>中位数绝对偏差聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/median-absolute-deviation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/median-absolute-deviation/</guid><description>中位数绝对偏差聚合 # median_absolute_deviation 中位数绝对偏差聚合是一个单值指标聚合。中位数绝对偏差是一种变异性指标，用于衡量相对于中位数的离散程度。
与依赖平方误差项的标准偏差相比，中位数绝对偏差受异常值的影响较小，适用于描述非正态分布的数据。
中位数绝对偏差按以下方式计算：
median_absolute_deviation = median( | x&amp;lt;sub&amp;gt;i&amp;lt;/sub&amp;gt; - median(x&amp;lt;sub&amp;gt;i&amp;lt;/sub&amp;gt;) | )
由于内存限制，Easysearch 估计 median_absolute_deviation ，而不是直接计算它。这种估计在计算上很昂贵。您可以调整估计精度和性能之间的权衡。有关更多信息，请参阅调整估计精度。
参数说明 # median_absolute_deviation 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 要计算中位数绝对偏差的数值字段的名称。 missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，则从估计中排除具有缺失值的文档。 compression 可选 Numeric 一个调整估计精度和性能之间平衡的参数。 compression 的值必须大于 0 。默认值为 1000 。 参考样例 # 以下示例计算数据集中 DistanceMiles 字段的绝对中位数偏差：
GET sample_data_flights/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;median_absolute_deviation_DistanceMiles&amp;quot;: { &amp;quot;median_absolute_deviation&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;DistanceMiles&amp;quot; } } } } 返回内容 # 如以下返回内容所示，聚合返回了 median_absolute_deviation_DistanceMiles 变量中绝对中位数偏差的估计值：</description></item><item><title>保留类型分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/keep-types/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/keep-types/</guid><description>保留类型分词过滤器 # 保留类型（keep_types）分词过滤器是一种用于文本分析的分词过滤器，它可用于控制保留或丢弃哪些词元类型。不同的分词器会产生不同的词元类型，例如 &amp;lt;HOST&amp;gt;、&amp;lt;NUM&amp;gt; 或 &amp;lt;ALPHANUM&amp;gt;。
分词器keyword）、简单匹配（simple_pattern）和简单匹配拆分（simple_pattern_split）分词器不支持保留类型分词过滤器，因为这些分词器不支持词元类型属性。
参数说明 # 保留类型分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 types 必需 字符串列表 要保留或丢弃的词元类型列表（由 mode 参数决定）。 mode 可选 字符串 是要包含(include)还是排除(exclude) types 中指定的词元类型。默认值为 include（包含）。 参考样例 # 以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有保留类型过滤器的分析器：
PUT /test_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;keep_types_filter&amp;quot;] } }, &amp;quot;filter&amp;quot;: { &amp;quot;keep_types_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keep_types&amp;quot;, &amp;quot;types&amp;quot;: [&amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>修剪分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/trim/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/trim/</guid><description>修剪分词过滤器 # 修剪(trim)分词过滤器会从词元中去除前导和尾随的空白字符。
许多常用的分词器，例如标准(standard)分词器、关键字(keyword)分词器和空白(whitespace)分词器，在分词过程中会自动去除前导和尾随的空白字符。当使用这些分词器时，无需额外配置修剪分词过滤器。
参考样例 # 以下示例请求创建了一个名为 my_pattern_trim_index 的新索引，并配置了一个带有修剪过滤器和匹配词元生成器的分词器。其中，匹配词元生成器不会去除词元的前导和尾随空白字符。
PUT /my_pattern_trim_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_trim_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;trim&amp;quot; } }, &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_pattern_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;,&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_trim_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_pattern_tokenizer&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_trim_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
GET /my_pattern_trim_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_trim_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot; Easysearch , is , powerful &amp;quot; } 返回内容包含产生的词元</description></item><item><title>值计数聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/value-count/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/value-count/</guid><description>值计数聚合 # value_count 值计数聚合是一个单值指标聚合，用于计算聚合所基于的值的数量。 例如，您可以将 value_count 指标与 avg 指标一起使用来查找聚合使用多少个数字来计算平均值。
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;number_of_values&amp;quot;: { &amp;quot;value_count&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;taxful_total_price&amp;quot; } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;number_of_values&amp;quot; : { &amp;quot;value&amp;quot; : 4675 } } }</description></item><item><title>停用词分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/stop-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/stop-analyzer/</guid><description>停用词分词器 # 停用词（stop）分词器会在文本中移除预定义的停用词。该分词器由一个小写词元生成器和一个停用词词元过滤器组成。
参数说明 # 你可以使用以下参数来配置一个停用词分词器。
参数 必填/可选 数据类型 描述 stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_stop_index 并使用停词器分词器的索引：
PUT /my_stop_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;stop&amp;quot; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于停用词分词器：
PUT /my_custom_stop_analyzer_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_stop_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;lowercase&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;stop&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_stop_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>停用词分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/stop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/stop/</guid><description>停用词分词过滤器 # 停用词（stop）分词过滤器用于在分词过程中从词元流中去除常见词汇（也称为停用词）。停用词通常是冠词和介词，比如“a” 或 “for” 。这些词在搜索查询中意义不大，并且常常被排除在外，以提高搜索效率和相关性。
默认的英语停用词列表包含以下单词：a、an、and、are、as、at、be、but、by、for、if、in、into、is、it、no、not、of、on、or、such、that、the、their、then、there、these、they、this、to、was、will 以及 with。
参数说明 # 停用词分词过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 stopwords 可选 字符串 既可以指定自定义的停用词数组，也可以指定一种语言以获取预定义的 Lucene 停用词列表。可指定的语言如下：
- _arabic_
- _armenian_
- _basque_
- _bengali_
- _brazilian_（巴西葡萄牙语）
- _bulgarian_
- _catalan_
- _cjk_（中文、日语和韩语）
- _czech_
- _danish_
- _dutch_
- _english_（默认值）
- _estonian_
- _finnish_
- _french_
- _galician_
- _german_
- _greek_
- _hindi_
- _hungarian_</description></item><item><title>全匹配查询</title><link>/easysearch/main/docs/references/search/match-all/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/match-all/</guid><description>全匹配查询 # match_all 查询返回所有文档。如果需要返回整个文档集，这个查询在测试大量文档集时很有用。
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;match_all&amp;quot;: {} } } match_all 查询有一个 match_none 的对应查询，这个对应查询很少有用：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;match_none&amp;quot;: {} } } 参数说明 # 全匹配和全不匹配查询都接受以下参数。所有参数都是可选的。
参数 数据类型 描述 boost Float 一个浮点数值，用于指定该字段对相关性分数的权重。大于 1.0 的值会增加字段的权重。介于 0.0 和 1.0 之间的值会降低字段的权重。默认值为 1.0。 _name String 用于查询标签的查询名称。可选。</description></item><item><title>全局聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/global/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/global/</guid><description>全局聚合 # global 全局聚合让你能跳出过滤聚合的聚合上下文。即使你包含了一个缩小文档集的过滤查询， global 聚合仍然对所有文档进行聚合，就好像过滤查询不存在一样。它忽略 filter 聚合，并隐式地假设 match_all 查询。
以下示例返回索引中所有文档的 taxful_total_price 字段的 avg 值：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;query&amp;quot;: { &amp;quot;range&amp;quot;: { &amp;quot;taxful_total_price&amp;quot;: { &amp;quot;lte&amp;quot;: 50 } } }, &amp;quot;aggs&amp;quot;: { &amp;quot;total_avg_amount&amp;quot;: { &amp;quot;global&amp;quot;: {}, &amp;quot;aggs&amp;quot;: { &amp;quot;avg_price&amp;quot;: { &amp;quot;avg&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;taxful_total_price&amp;quot; } } } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;total_avg_amount&amp;quot; : { &amp;quot;doc_count&amp;quot; : 4675, &amp;quot;avg_price&amp;quot; : { &amp;quot;value&amp;quot; : 75.05542864304813 } } } } 你可以看到， taxful_total_price 字段的平均值是 75.</description></item><item><title>关键词分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/keyword-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/keyword-analyzer/</guid><description>关键词分词器 # 关键词（keyword）分词器根本不会对文本进行分词。相反，它将整个输入视为单个词元，不会将其拆分成单个的词项。关键词分词器常用于包含电子邮件地址、网址或产品 ID 的字段，以及其他不需要进行分词的情况。
参考样例 # 以下命令创建一个名为 my_keyword_index 使用关键词分词器的索引：
PUT /my_keyword_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;keyword&amp;quot; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于关键词分词器：
PUT /my_custom_keyword_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_keyword_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot; } } } } } 产生的词元 # 以下请求来检查使用该分词器生成的词元：
POST /my_custom_keyword_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_keyword_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Just one token&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Just one token&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 14, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] }</description></item><item><title>关键词标记分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/keyword-marker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/keyword-marker/</guid><description>关键词标记分词过滤器 # 关键词标记（keyword_marker）分词过滤器用于防止某些词元被词干提取器或其他过滤器修改。该过滤器通过将指定的词元标记为关键词来实现这一点，这样就能避免词干提取或其他方式的处理。这确保了特定的单词能保持其原始形式。
参数说明 # 关键词标记分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 ignore_case 可选 布尔值 在匹配关键词时是否忽略字母大小写。默认值为false。 keywords 若未设置 keywords_path 或 keywords_pattern 则为必需 字符串列表 要标记为关键词的词元列表。 keywords_path 若未设置 keywords 或 keywords_pattern 则为必需 字符串 关键词列表的路径（相对于配置目录或绝对路径）。 keywords_pattern 若未设置 keywords 或 keywords_path 则为必需 字符串 用于匹配要标记为关键词的词元的正则表达式。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词标记过滤器的分词器。该过滤器将单词 example 标记为关键词：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;keyword_marker_filter&amp;quot;, &amp;quot;stemmer&amp;quot;] } }, &amp;quot;filter&amp;quot;: { &amp;quot;keyword_marker_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword_marker&amp;quot;, &amp;quot;keywords&amp;quot;: [&amp;quot;example&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>关键词词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/keyword/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/keyword/</guid><description>关键词词元生成器 # 关键字（keyword）词元生成器接收文本并将其原封不动地作为单个词元输出。当你希望输入内容保持完整时，比如在管理像姓名、产品代码或电子邮件地址这类结构化数据时，这个词元生成器就特别有用。
关键字词元生成器可以与词元过滤器搭配使用来处理文本。例如，对文本进行规范化处理或去除多余的字符。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用关键字词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_keyword_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_keyword_analyzer&amp;quot; } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_keyword_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch Example&amp;quot; } 返回内容会是包含原始内容的单个词元：
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Easysearch Example&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 18, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] } 参数说明 # 关键字词元生成器可以使用以下参数进行配置。</description></item><item><title>关键词重复分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/keyword-repeat/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/keyword-repeat/</guid><description>关键词重复分词过滤器 # 关键词重复（keyword_repeat）分词过滤器会将词元的关键词版本发松到词元流中。当你希望在经过进一步的词元转换（例如词干提取或同义词扩展）之后，既保留原始词元，又保留其修改后的版本时，通常会使用这个过滤器。这些重复的词元使得原始的、未改变的词元版本能够与修改后的版本一起保留在最终的分析结果中。
关键词重复分词过滤器应该放置在词干提取过滤器之前。词干提取并非应用于每个词元，因此在词干提取之后，你可能会在同一位置得到重复的词元。为了去除重复词元，可以在词干提取器之后使用 remove_duplicates 分词过滤器。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词重复过滤器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_kstem&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;kstem&amp;quot; }, &amp;quot;my_lowercase&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_lowercase&amp;quot;, &amp;quot;keyword_repeat&amp;quot;, &amp;quot;my_kstem&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Stopped quickly&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;stopped&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;stop&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;quickly&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;quick&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 } ] } 你可以通过在 _analyze 查询中添加以下参数，进一步研究关键词重复分词过滤器的影响：</description></item><item><title>写入数据文本向量化</title><link>/easysearch/main/docs/references/ai-integration/ingest-text-embedding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/ai-integration/ingest-text-embedding/</guid><description>写入数据文本向量化 # Easysearch 使用摄取管道 ingest pipeline 中的一系列处理器，可以对写入的数据进行处理，并且支持对文本进行向量化，本文档介绍如何在 Easysearch 中使用 text_embedding 处理器对写入数据进行向量化。
先决条件 # 支持与 OpenAI API 兼容的 embedding 接口，支持 Ollama embedding 接口。
需要安装 Easysearch 的 knn 和 ai 插件。
在生产环境中使用数据采集时，您的集群应至少包含一个节点，且该节点的节点角色权限设置为 ingest 。
创建带有向量字段的索引 # 首先，需要创建一个包含 knn mapping 的索引，text_vector 是存储向量的字段，向量维度是 768。
PUT /my-index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text_vector&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;knn_dense_float_vector&amp;quot;, &amp;quot;knn&amp;quot;: { &amp;quot;dims&amp;quot;: 768, &amp;quot;model&amp;quot;: &amp;quot;lsh&amp;quot;, &amp;quot;similarity&amp;quot;: &amp;quot;cosine&amp;quot;, &amp;quot;L&amp;quot;: 99, &amp;quot;k&amp;quot;: 1 } } } } } 创建或更新 text_embedding 处理器 # 请求路径：</description></item><item><title>函数评分查询</title><link>/easysearch/main/docs/references/search/compound/function-score/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/compound/function-score/</guid><description>函数评分查询 # 如果您需要更改结果中返回的文档的相关性评分，请使用 function_score 查询。 function_score 查询定义了一个查询和一个或多个函数，这些函数可以应用于所有结果或结果的一部分，以重新计算它们的相关性评分。
使用一个评分函数 # 最基础的 function_score 查询示例使用一个函数来重新计算分数。以下查询使用一个 weight 函数将所有相关性分数加倍。此函数适用于所有结果文档，因为没有在 function_score 中指定 query 参数：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;function_score&amp;quot;: { &amp;quot;weight&amp;quot;: &amp;quot;2&amp;quot; } } } 将评分函数应用于文档子集 # 要将评分函数应用于文档子集，在函数中提供一个查询：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;function_score&amp;quot;: { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;play_name&amp;quot;: &amp;quot;Hamlet&amp;quot; } }, &amp;quot;weight&amp;quot;: &amp;quot;2&amp;quot; } } } 支持的功能 # function_score 查询类型支持以下功能：
内置: weight ：将文档得分乘以一个预定义的增强因子。 random_score ：提供一个对单个用户一致的随机得分，但不同用户之间得分不同。 field_value_factor : 使用指定文档字段的值来重新计算分数。 衰减函数（ gauss 、 exp 和 linear ）：使用指定的衰减函数重新计算分数。 自定义： script_score : 使用脚本对文档进行评分。 权重函数 # 当您使用 weight 函数时，原始的相关性分数会乘以 weight 的浮点值：</description></item><item><title>分组排序聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/bucket-sort/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/bucket-sort/</guid><description>分组排序聚合 # bucket_sort 分组排序聚合是一个父聚合，它对其父多存储分组聚合生成的存储分组进行排序或截断。
在 bucket_sort 聚合中，您可以按多个字段对存储分组进行排序，每个字段都有自己的排序顺序。可以按存储分组的键、文档计数或子聚合中的值进行排序。您还可以使用 from 和 size 参数来截断结果，无论是否进行排序。
有关指定排序顺序的信息，请参阅排序结果 。
参数说明 # bucket_sort 聚合采用以下参数。
参数 必需/可选 数据类型 描述 gap_policy 可选 String 要应用于缺失数据的策略。有效值为 skip 和 insert_zeros。默认值为 skip。请参阅数据差距 。 sort 可选 String 要排序的字段列表。请参阅排序结果 。 from 可选 String 要返回的第一个结果的索引。必须是非负整数。默认值为 0。请参阅 from 和 size 参数 。 size 可选 String 要返回的最大结果数。必须是正整数。请参阅 from 和 size 参数 。 您必须至少提供一个 sort、from 和 size。</description></item><item><title>分组聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/terms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/terms/</guid><description>分组聚合 # terms 聚合会动态为字段中的每个唯一词条创建一个分组。
以下示例使用 terms 聚合来查找网络日志数据中每个响应代码的文档数量：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;response_codes&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;response.keyword&amp;quot;, &amp;quot;size&amp;quot;: 10 } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;response_codes&amp;quot; : { &amp;quot;doc_count_error_upper_bound&amp;quot; : 0, &amp;quot;sum_other_doc_count&amp;quot; : 0, &amp;quot;buckets&amp;quot; : [ { &amp;quot;key&amp;quot; : &amp;quot;200&amp;quot;, &amp;quot;doc_count&amp;quot; : 12832 }, { &amp;quot;key&amp;quot; : &amp;quot;404&amp;quot;, &amp;quot;doc_count&amp;quot; : 801 }, { &amp;quot;key&amp;quot; : &amp;quot;503&amp;quot;, &amp;quot;doc_count&amp;quot; : 441 } ] } } } 值以 key 键返回。 doc_count 指定每个分组中的文档数量。默认情况下，分组按 doc-count 的降序排列。</description></item><item><title>分隔式负载分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/delimited-payload/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/delimited-payload/</guid><description>分隔式负载分词过滤器 # 分隔式负载（delimited_payload）分词过滤器用于在分词过程中解析包含负载的词元。例如，字符串 red|1.5 fast|2.0 car|1.0 会被解析为词元 red（负载为 1.5）、fast（负载为 2.0）和 car（负载为 1.0）。当你的词元包含额外的关联数据（如权重、分数或其他数值），且你打算将这些数据用于评分或自定义查询逻辑时，这个过滤器就特别有用。该过滤器可以处理不同类型的负载，包括整数、浮点数和字符串，并将负载（额外的元数据）附加到词元上。
在文本分词时，分隔式负载分词过滤器会解析每个词元，提取负载并将其附加到词元上。后续在查询中可以使用这个负载来影响评分、提升权重或实现其他自定义行为。
负载以 Base64 编码的字符串形式存储。默认情况下，负载不会随词元一起在查询响应中返回。若要返回负载，你必须配置额外的参数。更多信息，请参阅“带有负载存储的分词示例”。
参数说明 # 分隔式负载分词过滤器有两个参数：
参数 必需/可选 数据类型 描述 encoding 可选 字符串 指定附加到词元的负载的数据类型。这决定了在分析和查询期间如何解释负载数据。
有效值为：
- float：使用 IEEE 754 格式将负载解释为 32 位浮点数（例如，car\|2.5 中的 2.5）。
- identity：将负载解释为字符序列（例如，在 user\|admin 中，admin 被解释为字符串）。
- int：将负载解释为 32 位整数（例如，priority \| 1中的1）。
默认值为 float。 delimiter 可选 字符串 指定在输入文本中分隔词元及其负载的字符。默认值为竖线字符（\|）。 不带负载存储的分词示例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有分隔式负载过滤器的分词器：</description></item><item><title>创建一个自定义分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/creating-a-custom-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/creating-a-custom-analyzer/</guid><description>创建一个自定义分词器 # 要创建一个自定义分词器，需要指定以下组成内容：
字符过滤器（零个或多个） 词元生成器（一个） 词元过滤器（零个或多个） 相关配置 # 以下参数可用于配置自定义分词器。
参数 必填/可选 描述 type 可选 分词器类型。默认值为custom。你也可以给这个参数指定一个预构建的分词器。 tokenizer 必填 每个分词器必须要有一个词元生成器。 char_filter 可选 要包含在分词器中的字符过滤器列表。 filter 可选 要包含在分词器中的分词过滤器列表。 position_increment_gap 可选 在对包含多个词项内容的文本字段建立索引时，应用于各词项之间的额外间隔。有关更多信息，请参阅位置间隔增量。默认值为 100。 参考样例 # 以下示例展示了各种自定义分词器的配置。
自定义分词器用于去除 HTML 格式标签 # 以下示例中的分词器在分词之前会从文本中移除 HTML 格式标签：
PUT simple_html_strip_analyzer_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;html_strip_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;char_filter&amp;quot;: [&amp;quot;html_strip&amp;quot;], &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;] } } } } } 使用以下请求来查看使用该分词器生成的词元：</description></item><item><title>别名操作</title><link>/easysearch/main/docs/references/document/index-alias/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/document/index-alias/</guid><description>索引别名 # 别名是可以指向一个或多个索引的虚拟索引名称。
如果数据分布在多个索引中，而不是跟踪要查询的索引，则可以创建别名并进行查询。
例如，如果要将日志存储到基于月份的索引中，并且经常查询前两个月的日志，则可以创建一个 last_2_months 别名，并每月更新其指向的索引。
您可以随时更改别名指向的索引，所以在应用程序中使用别名引用索引可以让您在不停机的情况下重新索引数据。
创建索引别名 # 要创建索引别名，请使用 POST 请求:
POST _aliases 使用 actions 方法指定要执行的操作列表。此命令创建名为 alias1 的别名，并将 index-1 添加到此别名：
POST _aliases { &amp;#34;actions&amp;#34;: [ { &amp;#34;add&amp;#34;: { &amp;#34;index&amp;#34;: &amp;#34;index-1&amp;#34;, &amp;#34;alias&amp;#34;: &amp;#34;alias1&amp;#34; } } ] } 您应该看到以下响应:
{ &amp;#34;acknowledged&amp;#34;: true } 如果此请求失败，请确保要添加到别名的索引已存在.
要检查 alias1 是否引用 index-1 ，请运行以下命令
GET alias1 索引别名添加与删除操作 # 您可以在同一 别名 操作中执行多个操作。
例如，以下命令删除 index-1 并将 index-2 添加到 alias1 ：
POST _aliases { &amp;#34;actions&amp;#34;: [ { &amp;#34;remove&amp;#34;: { &amp;#34;index&amp;#34;: &amp;#34;index-1&amp;#34;, &amp;#34;alias&amp;#34;: &amp;#34;alias1&amp;#34; } }, { &amp;#34;add&amp;#34;: { &amp;#34;index&amp;#34;: &amp;#34;index-2&amp;#34;, &amp;#34;alias&amp;#34;: &amp;#34;alias1&amp;#34; } } ] } add 和 remove 操作以原子方式发生，这意味着 alias1 不会同时指向 index-1 和 index-2 .</description></item><item><title>前缀 n 元词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/edge-n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/edge-n-gram/</guid><description>前缀 n 元词元生成器 # 前缀 n 元词元生成器会从每个单词的开头生成部分单词词元，也就是 n 元组。它会根据指定的字符分割文本，并在定义的最小和最大长度范围内生成词元。这种词元生成器在实现即输即搜（search-as-you-type）功能时特别有用。
前缀 n 元组非常适合用于自动补全搜索，在这种搜索中单词的顺序可能会有所不同，例如在搜索产品名称或地址时。有关更多信息，请参阅 “自动补全” 相关内容。不过，对于像电影或歌曲标题这种顺序固定的文本，完成建议器(completion suggester)可能会更准确。
默认情况下，前缀 n 元词元生成器生成的词元最小长度为 1，最大长度为 2。例如，当分析文本 Easysearch 时，默认配置会生成 “E” 和 “Ea” 这两个 n 元组。这些短的 n 元组常常会匹配到太多不相关的词条，所以调整 n 元组的长度，对词元生成器进行优化是很有必要的。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用前缀 n 元词元生成器的分词器。该词元生成器生成长度为 3 到 6 个字符的词元，且将字母和符号都视为有效的词元字符。
PUT /edge_n_gram_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;my_custom_tokenizer&amp;quot; } }, &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_custom_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 3, &amp;quot;max_gram&amp;quot;: 6, &amp;quot;token_chars&amp;quot;: [ &amp;quot;letter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>前缀查询</title><link>/easysearch/main/docs/references/search/term/prefix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term/prefix/</guid><description>前缀查询 # 使用 prefix 查询可以搜索以特定前缀开头的词。例如，以下查询会搜索 speaker 字段包含以 KING H 开头的词的文档。
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;prefix&amp;quot;: { &amp;quot;speaker&amp;quot;: &amp;quot;KING H&amp;quot; } } } 为了提供参数，您可以使用与前面的查询相同的形式，并使用以下扩展语法
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;prefix&amp;quot;: { &amp;quot;speaker&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;KING H&amp;quot; } } } } 参数说明 # 查询接受字段名称（ &amp;lt;field&amp;gt; ）作为顶级参数：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;prefix&amp;quot;: { &amp;quot;&amp;lt;field&amp;gt;&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;sample&amp;quot;, ... } } } } &amp;lt;field&amp;gt; 接受以下参数。除了 value 之外，所有参数都是可选的。
参数 数据类型 描述 value String 在由 指定的字段中搜索的词项。 boost Float 一个浮点值，用于指定该字段对相关性评分的权重。值大于 1.</description></item><item><title>加权</title><link>/easysearch/main/docs/references/search/compound/boosting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/compound/boosting/</guid><description>加权 # 如果你搜索“pitcher”这个词，你的结果可能既与棒球运动员有关，也与盛液体的容器有关。在棒球语境下搜索时，你可能想通过使用 must_not 子句完全排除包含“glass”或“water”的搜索结果。然而，如果你想保留这些结果但降低它们的关联度，可以使用 boosting 查询。
一个 boosting 查询返回与 positive 查询匹配的文档。在这些文档中，与 negative 查询也匹配的文档的关联度得分会降低（它们的关联度得分会乘以负的提升因子）。
参考用例 # 考虑一个包含两个文档的索引，你以如下方式索引：
PUT testindex/_doc/1 { &amp;quot;article_name&amp;quot;: &amp;quot;The greatest pitcher in baseball history&amp;quot; } PUT testindex/_doc/2 { &amp;quot;article_name&amp;quot;: &amp;quot;The making of a glass pitcher&amp;quot; } 使用以下匹配查询来搜索包含单词“pitcher”的文档：
GET testindex/_search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;article_name&amp;quot;: &amp;quot;pitcher&amp;quot; } } } 返回的两个文档具有相同的相关性分数：
{ &amp;quot;took&amp;quot;: 5, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 2, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 0.</description></item><item><title>加权平均聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/weighted-avg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/weighted-avg/</guid><description>加权平均聚合 # weighted_avg 聚合计算跨文档数值的加权平均值。当您想计算平均值，但希望某些数据点的权重大于其他数据点时，此功能非常有用。
加权平均值使用公式 $\frac{\sum_{i=1}^n value_i \cdot weight_i}{\sum_{i=1}^n weight_i}$ 计算。
参数说明 # weighted_avg 聚合采用以下参数。
参数 必需/可选 描述 value 必需 定义如何获取要计算平均值的数值。需要 field 或 script 。 weight 必需 定义如何获取每个值的权重。需要 field 或 script 。 format 可选 DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。 value_type 可选 使用脚本或未映射字段时的值的类型提示。 可以在 value 或 weight 内指定以下参数。
参数 必需/可选 描述 field 可选 用于值或权重的文档字段。 missing 可选 字段缺失时使用的默认值或权重。请参阅缺失值。 参考样例 # 首先，创建索引并索引一些数据。请注意，产品 C 缺少 rating 和 num_reviews 字段：</description></item><item><title>包装器</title><link>/easysearch/main/docs/references/search/specialized/wrapper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/specialized/wrapper/</guid><description>包装器 # wrapper 查询允许您以 Base64 编码的 JSON 格式提交完整的查询。当查询必须嵌入到仅支持字符串值的上下文中时，它非常有用。
仅当需要管理系统约束时才使用此查询。为了提高可读性和可维护性，最好尽可能使用基于 JSON 的标准查询。
参考样例 # 使用以下映射创建名为 products 的索引：
PUT /products { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; } } } } 索引示例文档：
POST /products/_bulk { &amp;quot;index&amp;quot;: { &amp;quot;_id&amp;quot;: 1 } } { &amp;quot;title&amp;quot;: &amp;quot;Wireless headphones with noise cancellation&amp;quot; } { &amp;quot;index&amp;quot;: { &amp;quot;_id&amp;quot;: 2 } } { &amp;quot;title&amp;quot;: &amp;quot;Bluetooth speaker&amp;quot; } { &amp;quot;index&amp;quot;: { &amp;quot;_id&amp;quot;: 3 } } { &amp;quot;title&amp;quot;: &amp;quot;Over-ear headphones with rich bass&amp;quot; } 以 Base64 格式编码以下查询：</description></item><item><title>匹配布尔前缀查询</title><link>/easysearch/main/docs/references/search/full-text/match-bool-prefix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/full-text/match-bool-prefix/</guid><description>匹配布尔前缀查询 # match_bool_prefix 匹配布尔前缀查询分析提供的搜索字符串，并从字符串的词项中创建一个布尔查询。它将除最后一个词项外的每个词项作为完整单词进行匹配。最后一个词项用作前缀。 match_bool_prefix 查询返回包含完整单词词项或以前缀词项开头的词项的文档，顺序不限。
以下示例展示了一个基本的 match_bool_prefix 查询：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;match_bool_prefix&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;the wind&amp;quot; } } } 要传递额外参数，您可以使用扩展语法：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;match_bool_prefix&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;the wind&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;stop&amp;quot; } } } } 参数说明 # 例如，考虑一个包含以下文档的索引：
PUT testindex/_doc/1 { &amp;quot;title&amp;quot;: &amp;quot;The wind rises&amp;quot; } PUT testindex/_doc/2 { &amp;quot;title&amp;quot;: &amp;quot;Gone with the wind&amp;quot; } 以下 match_bool_prefix 查询会搜索整个词 rises 以及以 wi 开头的词，顺序不限：</description></item><item><title>匹配替换分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/pattern-replace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/pattern-replace/</guid><description>匹配替换分词过滤器 # 匹配替换（pattern_replace）分词过滤器允许您使用正则表达式来修改词元。此过滤器会将词元中的模式替换为指定的值，这使您在对词元进行索引之前，能够灵活地转换或规范化词元。当您在分析过程中需要清理或规范文本时，该过滤器尤其有用。
参数说明 # 匹配替换分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 pattern 必需 字符串 一个正则表达式模式，用于匹配需要被替换的文本。 all 可选 布尔值 是否替换所有匹配的模式。如果为 false，则仅替换第一个匹配项。默认值为 true。 replacement 可选 字符串 用于替换匹配模式的字符串。默认值为空字符串。 参考样例 # 以下示例请求创建了一个名为 text_index 的新索引，并配置了一个带有匹配替换过滤器的分词器，用于将包含数字的词元替换为字符串 [NUM]：
PUT /text_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;number_replace_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;\\d+&amp;quot;, &amp;quot;replacement&amp;quot;: &amp;quot;[NUM]&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;number_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;number_replace_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>匹配替换字符过滤器</title><link>/easysearch/main/docs/references/text-analysis/character-filters/pattern-replace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/character-filters/pattern-replace/</guid><description>匹配替换字符过滤器 # 匹配替换（pattern_replace）字符过滤器使你能够使用正则表达式来定义文本匹配替换的模式。对于文本转换的高阶需求场景，尤其是在处理复杂的字符串模式时，它是一种的灵活工具。
这个过滤器会用替换符合匹配模式的所有匹配项，从而可以轻松地对输入文本进行替换、删除或复杂的修改。你可以在分词之前使用它对输入内容进行规范化处理。
参考样例 # 为了规范电话号码，你可以使用正则表达式 [\\s()-]+去替换号码里的特殊格式：
[]：定义一个字符类，意味着它将匹配方括号内的任意一个字符。 \\s：匹配任何空白字符，如空格、制表符或换行符。 ()：匹配字面意义上的括号（( 或 )）。 -：匹配字面意义上的连字符（-）。 +：指定该模式应匹配前面字符的一次或多次出现。 模式 [\\s()-]+ 将匹配由一个或多个空白字符、括号或连字符组成的任意序列，并将其从输入文本中移除。这确保了电话号码得到规范处理，结果将仅包含数字。
以下请求通过移除空格、连字符和括号来规范电话号码：
GET /_analyze { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;char_filter&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;[\\s()-]+&amp;quot;, &amp;quot;replacement&amp;quot;: &amp;quot;&amp;quot; } ], &amp;quot;text&amp;quot;: &amp;quot;(555) 123-4567&amp;quot; } 返回内容中包含生成的词元：
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;5551234567&amp;quot;, &amp;quot;start_offset&amp;quot;: 1, &amp;quot;end_offset&amp;quot;: 14, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] } 参数说明 # pattern_replace 字符过滤器必须使用以下参数进行配置。
参数 必需/可选 数据类型 描述 pattern 必需 字符串 用于匹配输入文本部分内容的正则表达式。过滤器会识别并匹配此模式以执行替换操作。 replacement 可选 字符串 用于替换匹配内容的字符串。使用空字符串（&amp;quot;&amp;quot;）可移除匹配到的文本。默认值为空字符串（&amp;quot;&amp;quot;）。 创建自定义分词器 # 以下请求创建一个索引，该索引带有一个配置了 pattern_replace 字符过滤器的自定义分词器。此过滤器会从数字中移除货币符号以及千位分隔符（包括欧洲的 “.</description></item><item><title>匹配查询</title><link>/easysearch/main/docs/references/search/full-text/match/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/full-text/match/</guid><description>匹配查询 # 使用 match 查询在特定文档字段上执行全文搜索。如果你在 text 字段上运行 match 查询， match 查询会分析提供的搜索字符串，并返回匹配字符串中任意词的文档。如果你在精确值字段上运行 match 查询，它会返回匹配精确值的文档。搜索精确值字段的推荐方式是使用过滤（ filter ）查询，因为与普通查询不同，过滤（ filter ）查询会被缓存。
以下示例展示了在 title 中对 wind 的基本 match 查询：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;wind&amp;quot; } } } 通过传递其他参数，您可以使用扩展语法：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;wind&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;stop&amp;quot; } } } } 参考样例 # 在以下示例中，您将使用包含以下文档的索引：
PUT testindex/_doc/1 { &amp;quot;title&amp;quot;: &amp;quot;Let the wind rise&amp;quot; } PUT testindex/_doc/2 { &amp;quot;title&amp;quot;: &amp;quot;Gone with the wind&amp;quot; } PUT testindex/_doc/3 { &amp;quot;title&amp;quot;: &amp;quot;Rise is gone&amp;quot; } 运算符 # 如果对 text 字段运行 match 查询，文本将使用 analyzer 参数中指定的分词器进行分析。然后，将生成的词元组合为使用 operator 参数中指定的操作符构成的布尔查询。默认操作符是 OR ，因此查询 wind rise 被改为 wind OR rise 。在这个示例中，此查询返回文档 1–3，因为每个文档都有一个与查询匹配的词元。要指定 and 操作符，请使用以下查询：</description></item><item><title>匹配模式分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/pattern-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/pattern-analyzer/</guid><description>匹配模式分词器 # 匹配模式（pattern）分词器允许你定义一个自定义分词器，该分词器使用正则表达式（regex）将输入文本分割成词元。它还提供了应用正则表达式、将词元转换为小写以及过滤掉停用词的功能。
参数说明 # 匹配模式分词器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 pattern 可选 字符串 用于对输入内容进行分词匹配的 Java 正则表达式。默认值是 \W+。 flags 可选 字符串 以竖线分隔的 Java 正则表达式的字符串标志，这些标志会修改正则表达式的行为。 lowercase 可选 布尔值 是否将词元转换为小写。默认值为 true。 stopwords 可选 字符串或字符串列表 个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_pattern_index 并使用匹配模式分词器的索引：
PUT /my_pattern_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;\\W+&amp;quot;, &amp;quot;lowercase&amp;quot;: true, &amp;quot;stopwords&amp;quot;: [&amp;quot;and&amp;quot;, &amp;quot;is&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>匹配短语前缀查询</title><link>/easysearch/main/docs/references/search/full-text/match-phrase-prefix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/full-text/match-phrase-prefix/</guid><description>匹配短语前缀查询 # 使用 match_phrase_prefix 查询来指定要匹配的短语。包含您指定短语的文档将被返回。短语中的最后一个部分词被解释为前缀，因此任何包含以该短语和最后一个词的前缀开头的短语的文档都将被返回。
与匹配短语类似，但会从查询字符串中的最后一个词创建一个前缀查询。
对于 match_phrase_prefix 和 match_bool_prefix 查询之间的差异，请参阅 match_bool_prefix 和 match_phrase_prefix 查询。
以下示例展示了一个基本的 match_phrase_prefix 查询：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;match_phrase_prefix&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;the wind&amp;quot; } } } 要传递附加参数，您可以使用扩展语法：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;match_phrase_prefix&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;the wind&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;stop&amp;quot; } } } } 参考用例 # 例如，创建一个包含以下文档的索引：
PUT testindex/_doc/1 { &amp;quot;title&amp;quot;: &amp;quot;The wind rises&amp;quot; } PUT testindex/_doc/2 { &amp;quot;title&amp;quot;: &amp;quot;Gone with the wind&amp;quot; } 以下 match_phrase_prefix 查询会搜索完整单词 wind ，后跟一个以 ri 开头的单词：</description></item><item><title>匹配短语查询</title><link>/easysearch/main/docs/references/search/full-text/match-phrase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/full-text/match-phrase/</guid><description>匹配短语查询 # 使用 match_phrase 查询来匹配包含指定顺序中确切的短语的文档。您可以通过提供 slop 参数来增加短语匹配的灵活性。
match_phrase 查询创建一个匹配词项序列的短语查询。
以下示例展示了一个基本的 match_phrase 查询：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;match_phrase&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;the wind&amp;quot; } } } 要传递额外的参数，您可以使用扩展语法：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;match_phrase&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;the wind&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;stop&amp;quot; } } } } 参考用例 # 例如，创建一个包含以下文档的索引：
PUT testindex/_doc/1 { &amp;quot;title&amp;quot;: &amp;quot;The wind rises&amp;quot; } PUT testindex/_doc/1 { &amp;quot;title&amp;quot;: &amp;quot;The wind rises&amp;quot; } 以下 match_phrase 查询搜索短语 wind rises ，其中单词 wind 后面跟着单词 rises ：</description></item><item><title>匹配词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/pattern/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/pattern/</guid><description>匹配词元生成器 # 匹配（pattern）词元生成器是一种高度灵活的词元生成器，它允许你根据自定义的 Java 正则表达式将文本拆分为词元。与使用 Lucene 正则表达式的简单匹配（simple_pattern）词元生成器和简单分割匹配（simple_pattern_split）词元生成器不同，匹配词元生成器能够处理更复杂、更细致的正则表达式模式，从而让你对文本的分词方式拥有更强的掌控力。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用匹配词元生成器的分词器。该分词器会在 -、_ 或 . 字符处对文本进行分割。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_pattern_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;[-_.]&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_pattern_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch-2024_v1.</description></item><item><title>十进制数字分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/decimal-digit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/decimal-digit/</guid><description>十进制数字分词过滤器 # 十进制数字（decimal_digit）分词过滤器用于将各种字符集中的十进制数字字符（0 到 9）规范化为它们对应的 ASCII 字符。当你希望在文本分析中确保所有数字都能被统一处理，而不管这些数字是以何种字符集书写时，这个过滤器就非常有用。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有十进制数字过滤器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_decimal_digit_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;decimal_digit&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;my_decimal_digit_filter&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;123 ١٢٣ १२३&amp;quot; } text分词：
“123”（ASCII 数字） “١٢٣”（阿拉伯 - 印度数字） “१२३”（梵文数字） 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;123&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 3, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;123&amp;quot;, &amp;quot;start_offset&amp;quot;: 4, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;123&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 11, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>单词分隔符分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/word-delimiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/word-delimiter/</guid><description>单词分隔符分词过滤器 # 单词分隔符(word_delimiter)分词过滤器用于根据预定义的字符拆分词元，还能根据可定制规则对词元进行可选的规范化处理。
我们建议尽可能使用 word_delimiter_graph 过滤器而非 word_delimiter 过滤器，因为 word_delimiter 过滤器有时会生成无效的词元图。
word_delimiter 过滤器可用于从零件编号或产品 ID 等复杂标识符中去除标点符号。在这种情况下，它最好与关键字分词器配合使用。对于带连字符的单词，建议使用同义词图(synonym_graph)分词过滤器而非 word_delimiter 过滤器，因为用户搜索这些词项时，常常既会搜索带连字符的形式，也会搜索不带连字符的形式。
默认情况下，该过滤器应用以下规则：
描述 输入 输出 将非字母数字字符视为分隔符 ultra-fast ultra, fast 去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder 当字母大小写发生转换时拆分词元 Easysearch Easy, search 当字母和数字之间发生转换时拆分词元 T1000 T, 1000 去除词元末尾的所有格形式（&amp;lsquo;s） John's John 重要提示：不要将该过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并影响诸如 catenate_all 或 preserve_original 等选项的效果。我们建议将此过滤器与关键(keyword)字分词器或空白(whitespace)分词器配合使用。
参数说明 # 你可以使用以下参数配置单词分隔符分词过滤器。</description></item><item><title>单词分隔符图分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/word-delimiter-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/word-delimiter-graph/</guid><description>单词分隔符图分词过滤器 # 单词分隔符图(word_delimiter_graph)分词过滤器用于依据预定义的字符对词元进行拆分，还能基于可定制的规则对词元进行可选的规范化处理。
单词分隔符图分词过滤器可用于去除零件编号或产品 ID 这类复杂标识符中的标点符号。在这种情况下，它最好与关键字分词器搭配使用。对于带有连字符的单词，建议使用同义词图分词过滤器而非单词分隔符图分词过滤器，因为用户在搜索这些词时，往往既会搜索带连字符的形式，也会搜索不带连字符的形式。
默认情况下，该过滤器会应用以下规则：
描述 输入 输出 将非字母数字字符视为分隔符 ultra-fast ultra, fast 去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder 当字母大小写发生转换时拆分词元 Easysearch Easy, search 当字母和数字之间发生转换时拆分词元 T1000 T, 1000 去除词元末尾的所有格形式（&amp;lsquo;s） John's John 重要的是，不要将此过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并且会影响诸如 catenate_all 或 preserve_original 之类的选项。我们建议将此过滤器与关键字(keyword)分词器或空白(whitespace)分词器搭配使用。
参数说明 # 你可以使用以下参数来配置单词分隔符图分词过滤器。
参数 必需/可选 数据类型 描述 adjust_offsets 可选 布尔值 决定是否要为拆分或连接后的词元重新计算词元偏移量。若为 true，过滤器会调整词元偏移量，以准确呈现词元在词元流中的位置。这种调整能确保词元在文本中的位置与处理后的修改形式相匹配，这对高亮显示或短语查询等应用特别有用。若为 false，偏移量保持不变，这可能会导致处理后的词元映射回原始文本位置时出现错位。如果你的分词器使用了像 trim 这类会改变词元长度但不改变偏移量的过滤器，建议将此参数设为 false。默认值为 true。 catenate_all 可选 布尔值 从一系列字母数字部分生成连接后的词元。例如，quick-fast-200 会变成 [ quickfast200, quick, fast, 200 ]。默认值为 false。 catenate_numbers 可选 布尔值 连接数字序列。例如，10-20-30 会变成 [ 102030, 10, 20, 30 ]。默认值为 false。 catenate_words 可选 布尔值 连接字母单词。例如，high-speed-level 会变成 [ highspeedlevel, high, speed, level ]。默认值为 false。 generate_number_parts 可选 布尔值 若为 true，输出中会包含纯数字词元（仅由数字组成的词元）。默认值为 true。 generate_word_parts 可选 布尔值 若为 true，输出中会包含纯字母词元（仅由字母字符组成的词元）。默认值为 true。 ignore_keywords 可选 布尔值 是否处理标记为关键字的词元。默认值为 false。 preserve_original 可选 布尔值 在输出中，除了生成的词元外，还保留原始词元（可能包含非字母数字分隔符）。例如，auto-drive-300 会变成 [ auto - drive - 300, auto, drive, 300 ]。如果为 true，该过滤器会生成索引不支持的多位置词元，因此请勿在索引分词器中使用此过滤器，或者在该过滤器之后使用 flatten_graph 过滤器。默认值为 false。 protected_words 可选 字符串数组 指定不应被拆分的词元。 protected_words_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含不应被分隔的词元，词元需分行列出。 split_on_case_change 可选 布尔值 在连续字母大小写不同（一个为小写，另一个为大写）的位置拆分词元。例如，“EasySearch” 会变成 [ Easy, Search ]。默认值为 true。 split_on_numerics 可选 布尔值 在连续字母和数字的位置拆分词元。例如，v8engine 会变成 [ v, 8, engine ]。默认值为 true。 stem_english_possessive 可选 布尔值 去除英语所有格结尾，如 &amp;lsquo;s。默认值为 true。 type_table 可选 字符串数组 一个自定义映射，用于指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。例如，要将连字符（-）视为字母数字字符，可指定 [&amp;quot;- =&amp;gt; ALPHA&amp;quot;]，这样单词就不会在连字符处拆分。有效类型有：</description></item><item><title>去重分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/remove-duplicates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/remove-duplicates/</guid><description>去重分词过滤器 # 去重（remove_duplicates）分词过滤器用于去除在分词过程中在相同位置生成的重复词元。
参考样例 # 以下示例请求创建了一个带有 keyword_repeat（关键词重复）分词过滤器的索引。该过滤器会在每个词元自身所在的相同位置添加该词元的关键词版本，然后使用 kstem（K 词干提取）过滤器来创建该词元的词干形式。
PUT /example-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;keyword_repeat&amp;quot;, &amp;quot;kstem&amp;quot; ] } } } } } 使用以下请求来分析字符串 “Slower turtle”。
GET /example-index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;custom_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Slower turtle&amp;quot; } 返回内容中在同一位置包含了两次词元 “turtle”。
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;slower&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;slow&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;turtle&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;turtle&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 } ] } 可以通过在索引设置中添加一个去重分词过滤器来移除重复的词元。</description></item><item><title>反向嵌套聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/reverse-nested/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/reverse-nested/</guid><description>反向嵌套聚合 # 您可以将嵌套文档中的值聚合到其父文档中；这种聚合称为 reverse_nested 。您可以使用 reverse_nested 在按嵌套对象中的字段分组后，聚合父文档中的字段。 reverse_nested 聚合将“连接回”根页面，并为您的各种变体获取 load_time 。
reverse_nested 聚合是嵌套聚合中的一个子聚合。它接受一个名为 path 的选项。此选项定义 Easysearch 在计算聚合时在文档层次结构中向后退多少步。
GET logs/_search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;response&amp;quot;: &amp;quot;200&amp;quot; } }, &amp;quot;aggs&amp;quot;: { &amp;quot;pages&amp;quot;: { &amp;quot;nested&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;pages&amp;quot; }, &amp;quot;aggs&amp;quot;: { &amp;quot;top_pages_per_load_time&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;pages.load_time&amp;quot; }, &amp;quot;aggs&amp;quot;: { &amp;quot;comment_to_logs&amp;quot;: { &amp;quot;reverse_nested&amp;quot;: {}, &amp;quot;aggs&amp;quot;: { &amp;quot;min_load_time&amp;quot;: { &amp;quot;min&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;pages.load_time&amp;quot; } } } } } } } } } } 返回内容</description></item><item><title>反转分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/reverse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/reverse/</guid><description>反转分词过滤器 # 反转（reverse）分词过滤器会反转每个词元中字符的顺序，这样在分析过程中，后缀信息就会位于反转后词元的开头。
这对于基于后缀的搜索很有用：
反转分词过滤器在你需要进行基于后缀的搜索时很有帮助，例如以下场景：
后缀匹配：根据单词的后缀来搜索单词，比如识别以特定后缀结尾的单词（例如 -tion 或 -ing）。 文件扩展名搜索：通过文件的扩展名来搜索文件，例如 .txt 或 .jpg。 自定义排序或排名：通过反转词元，你可以基于后缀实现独特的排序或排名逻辑。 后缀自动补全：实现基于后缀而非前缀的自动补全建议。 参考说明 # 以下示例请求创建了一个名为 my-reverse-index 的新索引，并配置了一个带有反转过滤器的分词器。
PUT /my-reverse-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;reverse_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;reverse&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_reverse_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;reverse_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
GET /my-reverse-index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_reverse_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;hello world&amp;quot; } 返回内容包含产生的词元</description></item><item><title>各类语种分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/language-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/language-analyzer/</guid><description>各类语种分词器 # Easysearch 在分词器选项中支持以下语种：阿拉伯语（arabic）、亚美尼亚语（armenian）、巴斯克语（basque）、孟加拉语（bengali）、巴西葡萄牙语（brazilian）、保加利亚语（bulgarian）、加泰罗尼亚语（catalan）、捷克语（czech）、丹麦语（danish）、荷兰语（dutch）、英语（english）、爱沙尼亚语（estonian）、芬兰语（finnish）、法语（french）、加利西亚语（galician）、德语（german）、希腊语（greek）、印地语（hindi）、匈牙利语（hungarian）、印度尼西亚语（indonesian）、爱尔兰语（irish）、意大利语（italian）、拉脱维亚语（latvian）、立陶宛语（lithuanian）、挪威语（norwegian）、波斯语（persian）、葡萄牙语（portuguese）、罗马尼亚语（romanian）、俄语（russian）、索拉尼语（sorani）、西班牙语（spanish）、瑞典语（swedish）、土耳其语（turkish）以及泰语（thai）。
当你映射索引时要使用该分词器，需在查询中指定相应的值。例如，要使用法语语种分词器来映射您的索引，为分词器字段指定值 french：
&amp;quot;analyzer&amp;quot;: &amp;quot;french&amp;quot; 参考样例 # 以下请求指定了一个名为 my-index 的索引，其中 content 字段被配置为多字段，并且一个名为 french 的子字段配置了french语种分词器
PUT my-index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;fields&amp;quot;: { &amp;quot;french&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;french&amp;quot; } } } } } } 也可以使用以下查询为整个索引配置默认的french分词器：
PUT my-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;default&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;french&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; } } } }</description></item><item><title>同义词分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/synonym/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/synonym/</guid><description>同义词分词过滤器 # 同义词(synonym)分词过滤器允许将多个术语映射到单个术语，或在单词之间创建等价组，从而提高搜索的灵活性。
参数说明 # 同义词分词过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。 synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。 lenient 可选 布尔值 加载规则配置时是否忽略异常。默认值为 false。 format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：
- solr
- wordnet
默认值为 solr。 expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。
例如：
如果同义词定义为 &amp;quot;quick, fast&amp;quot; 且 expand 设置为 true，则同义词规则配置如下：</description></item><item><title>同义词图分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/synonym-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/synonym-graph/</guid><description>同义词图分词过滤器 # 同义词图(synonym_graph)分词过滤器是同义词分词过滤器的更高级版本。它支持多词同义词，并能跨多个词元处理同义词，这使得它非常适合处理短语或者那些词元之间关系很重要的场景。
参数说明 # 同义词图分词过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。 synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。 lenient 可选 布尔值 在加载规则配置时是否忽略异常。默认值为 false。 format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：
- solr
- wordnet
默认值为 solr。 expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。
例如：
若同义词定义为 “quick, fast” 且 expand 设置为 true，则同义词规则配置如下：</description></item><item><title>唯一分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/unique/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/unique/</guid><description>唯一分词过滤器 # 唯一(unique)分词过滤器可确保在分词过程中仅保留唯一的词元，它会去除在单个字段或文本块中出现的重复词元。
参数说明 # 唯一分词过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 only_on_same_position 可选 布尔值 如果设置为 true，该分词过滤器将充当去重分词过滤器，仅移除位于相同位置的词元。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 unique_example 的新索引，并配置了一个带有唯一过滤器的分词器。
PUT /unique_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;unique_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;unique&amp;quot;, &amp;quot;only_on_same_position&amp;quot;: false } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;unique_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;unique_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>地理中心点</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/geocentroid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/geocentroid/</guid><description>地理中心点 # geo_centroid 地理中心点的聚合计算一组 geo_point 值的地理中心或焦点。它将中心位置作为纬度-经度对返回。
参数说明 # geo_centroid 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 包含计算中心的地理点的字段的名称。 参考样例 # 以下示例返回数据中每个订单的 geo_centroid 的 geoip.location 。每个 geoip.location 都是一个地理点：
GET /sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;centroid&amp;quot;: { &amp;quot;geo_centroid&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;geoip.location&amp;quot; } } } } 返回内容 # 返回内容包括一个 centroid 对象，该对象具有 lat 和 lon 属性，表示所有索引数据点的中心位置：
{ &amp;quot;took&amp;quot;: 35, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 4675, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: null, &amp;quot;hits&amp;quot;: [] }, &amp;quot;aggregations&amp;quot;: { &amp;quot;centroid&amp;quot;: { &amp;quot;location&amp;quot;: { &amp;quot;lat&amp;quot;: 35.</description></item><item><title>地理多边形查询</title><link>/easysearch/main/docs/references/search/geo-and-xy/geopolygon/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/geo-and-xy/geopolygon/</guid><description>地理多边形查询 # 地理多边形查询返回包含指定多边形内地理点的文档。如果一个文档包含多个地理点，只要至少有一个地理点匹配查询，该文档就匹配查询。
多边形通过坐标形式的顶点列表指定。与为地理形状字段指定多边形不同，多边形不需要闭合（指定第一个和最后一个点相同是不必要的）。尽管点不需要遵循顺时针或逆时针顺序，但建议您按照这两种顺序之一列出它们。这将确保正确捕获多边形。
搜索的文档字段必须映射为 geo_point 。
参考样例 # 创建一个映射，将 point 字段映射为 geo_point ：
PUT /testindex1 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;point&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;geo_point&amp;quot; } } } } 索引一个地理点，指定其纬度和经度：
PUT testindex1/_doc/1 { &amp;quot;point&amp;quot;: { &amp;quot;lat&amp;quot;: 73.71, &amp;quot;lon&amp;quot;: 41.32 } } 搜索包含指定 geo_polygon 的 point 对象的文档：
GET /testindex1/_search { &amp;quot;query&amp;quot;: { &amp;quot;bool&amp;quot;: { &amp;quot;must&amp;quot;: { &amp;quot;match_all&amp;quot;: {} }, &amp;quot;filter&amp;quot;: { &amp;quot;geo_polygon&amp;quot;: { &amp;quot;point&amp;quot;: { &amp;quot;points&amp;quot;: [ { &amp;quot;lat&amp;quot;: 74.</description></item><item><title>地理形状查询</title><link>/easysearch/main/docs/references/search/geo-and-xy/geoshape/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/geo-and-xy/geoshape/</guid><description>地理形状查询 # 使用地理形状（geoshape）查询来搜索包含地理点（geopoint）或地理形状字段（geoshape）的文档。您可以使用查询中定义的地理形状或使用预索引的地理形状来过滤文档。
搜索的文档字段必须映射为 geo_point 或 geo_shape 。
空间关系 # 当您向地理形状查询提供地理形状时，文档中的地理点和地理形状字段将使用以下空间关系与提供的形状进行匹配。
关系 描述 支持地理字段类型 INTERSECTS （默认）匹配与查询中提供的形状相交的 geopoint 或 geoshape 的文档。 geo_point, geo_shape DISJOINT 匹配与查询中提供的形状不相交的 geoshape 的文档。 geo_shape WITHIN 匹配完全位于查询中提供的形状内的 geoshape 的文档。 geo_shape CONTAINS 匹配完全包含查询中提供的形状的文档。 geo_shape 在 geoshape 查询中定义形状 # 您可以在 geoshape 查询中通过在查询时提供新的形状定义或引用另一个索引中预先索引的形状名称来定义形状以过滤文档。
使用新的形状定义 # 为了向地理形状查询提供新的形状，请在 geo_shape 字段中定义它。您必须以 GeoJSON 格式定义地理形状。</description></item><item><title>地理距离查询</title><link>/easysearch/main/docs/references/search/geo-and-xy/geodistance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/geo-and-xy/geodistance/</guid><description>地理距离查询 # 地理距离查询返回包含地理点的文档，这些地理点位于提供的地理点指定距离内。如果一个文档包含多个地理点，只要至少有一个地理点与查询匹配，该文档就符合查询条件。
搜索的文档字段必须映射为 geo_point 。
参考样例 # 创建一个映射，将 point 字段映射为 geo_point ：
PUT testindex1 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;point&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;geo_point&amp;quot; } } } } 索引一个地理点，指定其纬度和经度：
PUT testindex1/_doc/1 { &amp;quot;point&amp;quot;: { &amp;quot;lat&amp;quot;: 74.00, &amp;quot;lon&amp;quot;: 40.71 } } 搜索距离指定的 point 内包含指定 distance 对象的文档：
GET /testindex1/_search { &amp;quot;query&amp;quot;: { &amp;quot;bool&amp;quot;: { &amp;quot;must&amp;quot;: { &amp;quot;match_all&amp;quot;: {} }, &amp;quot;filter&amp;quot;: { &amp;quot;geo_distance&amp;quot;: { &amp;quot;distance&amp;quot;: &amp;quot;50mi&amp;quot;, &amp;quot;point&amp;quot;: { &amp;quot;lat&amp;quot;: 73.</description></item><item><title>地理距离聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/geo-distance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/geo-distance/</guid><description>地理距离聚合 # geo_distance 地理距离聚合根据与一个起始 geo_point 字段距离将文档分组到同心圆中。它与 range 聚合相同，只是它作用于地理位置。
例如，你可以使用 geo_distance 聚合来查找你 1 公里范围内的所有披萨店。搜索结果仅限于你指定的 1 公里半径范围内，但你还可以添加另一个在 2 公里范围内找到的结果。
您只能对映射为 geo_point 的字段使用 geo_distance 聚合。
点是一个单一的地理坐标，例如您智能手机显示的当前位置。在 OpenSearch 中，点表示如下：
{ &amp;quot;location&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;point&amp;quot;, &amp;quot;coordinates&amp;quot;: { &amp;quot;lat&amp;quot;: 83.76, &amp;quot;lon&amp;quot;: -81.2 } } } 您还可以将纬度和经度指定为数组 [-81.20, 83.76] 或字符串 &amp;quot;83.76, -81.20&amp;quot;
此表列出了 geo_distance 聚合的相关字段：
字段 必需/可选 描述 field 必需 指定您要处理的地理点字段。 origin 必需 指定用于计算距离的地理点。 ranges 必需 指定一组范围，根据文档与目标点的距离收集文档。 unit 可选 定义 ranges 数组中使用的单位。 unit 默认为 m （米），但你可以切换到其他单位，如 km （千米）、 mi （英里）、 in （英寸）、 yd （码）、 cm （厘米）和 mm （毫米）。 distance_type 可选 指定 OpenSearch 如何计算距离。默认值为 sloppy_arc （更快但精度较低），也可以设置为 arc （较慢但最精确）或 plane （最快但精度最低）。由于误差范围较大，仅适用于小地理区域使用 plane 。 语法如下：</description></item><item><title>地理边界框查询</title><link>/easysearch/main/docs/references/search/geo-and-xy/geo-bounding-box/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/geo-and-xy/geo-bounding-box/</guid><description>地理边界框查询 # 要搜索包含地理点字段的文档，请使用地理边界框查询。地理边界框查询返回地理点位于查询中指定的边界框内的文档。如果一个文档包含多个地理点，只要至少有一个地理点位于边界框内，该文档就与查询匹配。
参考样例 # 您可以使用地理边界框查询来搜索包含地理点的文档。
创建一个映射，将 point 字段映射为 geo_point ：
PUT testindex1 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;point&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;geo_point&amp;quot; } } } } 以经纬度作为对象索引三个地理点：
PUT testindex1/_doc/1 { &amp;quot;point&amp;quot;: { &amp;quot;lat&amp;quot;: 74.00, &amp;quot;lon&amp;quot;: 40.71 } } PUT testindex1/_doc/2 { &amp;quot;point&amp;quot;: { &amp;quot;lat&amp;quot;: 72.64, &amp;quot;lon&amp;quot;: 22.62 } } PUT testindex1/_doc/3 { &amp;quot;point&amp;quot;: { &amp;quot;lat&amp;quot;: 75.00, &amp;quot;lon&amp;quot;: 28.00 } } 搜索所有文档，并筛选出查询中定义的矩形内的点所在的文档：
GET testindex1/_search { &amp;quot;query&amp;quot;: { &amp;quot;bool&amp;quot;: { &amp;quot;must&amp;quot;: { &amp;quot;match_all&amp;quot;: {} }, &amp;quot;filter&amp;quot;: { &amp;quot;geo_bounding_box&amp;quot;: { &amp;quot;point&amp;quot;: { &amp;quot;top_left&amp;quot;: { &amp;quot;lat&amp;quot;: 75, &amp;quot;lon&amp;quot;: 28 }, &amp;quot;bottom_right&amp;quot;: { &amp;quot;lat&amp;quot;: 73, &amp;quot;lon&amp;quot;: 41 } } } } } } } 返回内容包含匹配的文档：</description></item><item><title>地理边界聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/geobounds/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/geobounds/</guid><description>地理边界聚合 # geo_bounds 地理边界聚合是一个多值聚合，用于计算包含一组 geo_point 或 geo_shape 对象的地理边界框。边界框以十进制编码的经纬度（lat-lon）对形式返回，作为矩形的左上角和右下角顶点。
参数说明 # geo_bounds 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 计算地理边界所使用的包含地理点或地理形状的字段的名称。 wrap_longitude 可选 Boolean 是否允许边界框与国际日期变更线重叠。默认值为 true 。 参考样例 # 以下示例查询数据中每个订单的 geo_bounds 的 geoip.location （每个 geoip.location 是一个地理点）：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;geo&amp;quot;: { &amp;quot;geo_bounds&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;geoip.location&amp;quot; } } } } 返回内容 # 如以下示例响应所示，聚合返回包含 geoip.</description></item><item><title>基数聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/cardinality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/cardinality/</guid><description>基数聚合 # cardinality 聚合是一种单值度量聚合，用于计算字段的唯一值或不同值的数量。
基数计数为近似值。有关更多信息，请参阅下面的控制精度 。
参数说明 # cardinality 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需的 String 估计基数的字段。 precision_threshold 可选 Numeric 阈值，低于该阈值的计数预计接近准确值。有关更多信息，请参阅控制精度 。 execution_hint 可选 String 如何运行聚合，该参数会影响资源使用和聚合效率。有效值为 ordinals 和 direct 。 missing 可选 与 field 类型相同 用于存储字段缺失实例的 bucket。如果未提供，则忽略缺失值。 参考样例 # 以下示例请求查找数据中唯一产品 ID 的数量：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;unique_products&amp;quot;: { &amp;quot;cardinality&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;products.</description></item><item><title>复合聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/composite/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/composite/</guid><description>复合聚合 # composite 复合聚合基于一个或多个文档字段或源创建分组。 composite 聚合为每个单独源值的组合创建一个分组。默认情况下，如果一个或多个字段中缺少值，则这些组合会从结果中省略。
每个源有四种类型的聚合之一：
terms 类型按唯一（通常是 String ）值分组。 histogram 类型按指定宽度数值分组。 date_histogram 类型按指定宽度的日期或时间范围分组。 geotile_grid 类型按指定分辨率将地理点分组到网格中。 composite 聚合通过将其源键组合到分组中来工作。生成的分组是有序的，跨源(Across)和源内部(Within)都是：
Across：分组按聚合请求中源的顺序嵌套。 Within:每个源中值的顺序决定了该源的分组顺序。排序方式根据源类型适当选择，可以是字母顺序、数字顺序、日期时间顺序或地理切片顺序。 考虑一下来自马拉松参与者索引的这些字段：
{... &amp;quot;city&amp;quot;: &amp;quot;Albuquerque&amp;quot;, &amp;quot;place&amp;quot;: &amp;quot;Bronze&amp;quot; ...} {... &amp;quot;city&amp;quot;: &amp;quot;Boston&amp;quot;, ...} {... &amp;quot;city&amp;quot;: &amp;quot;Chicago&amp;quot;, &amp;quot;place&amp;quot;: &amp;quot;Bronze&amp;quot; ...} {... &amp;quot;city&amp;quot;: &amp;quot;Albuquerque&amp;quot;, &amp;quot;place&amp;quot;: &amp;quot;Gold&amp;quot; ...} {... &amp;quot;city&amp;quot;: &amp;quot;Chicago&amp;quot;, &amp;quot;place&amp;quot;: &amp;quot;Silver&amp;quot; ...} {... &amp;quot;city&amp;quot;: &amp;quot;Boston&amp;quot;, &amp;quot;place&amp;quot;: &amp;quot;Bronze&amp;quot; ...} {... &amp;quot;city&amp;quot;: &amp;quot;Chicago&amp;quot;, &amp;quot;place&amp;quot;: &amp;quot;Gold&amp;quot; ...} 假设请求指定源如下：
... &amp;quot;sources&amp;quot;: [ { &amp;quot;marathon_city&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;city&amp;quot; }}}, { &amp;quot;participant_medal&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;place&amp;quot; }}} ], .</description></item><item><title>多字段查询</title><link>/easysearch/main/docs/references/search/full-text/multi-match/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/full-text/multi-match/</guid><description>多字段查询 # 多字段查询与匹配查询类似。您可以使用 multi_match 查询来搜索多个字段。
^ 会“提升”某些字段的权重。提升是乘数，用于使一个字段中的匹配比其他字段中的匹配更重要。在以下示例中，title字段中匹配 “wind” 的权重比 plot 字段中匹配的权重高 _score 四倍：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;multi_match&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;wind&amp;quot;, &amp;quot;fields&amp;quot;: [&amp;quot;title^4&amp;quot;, &amp;quot;plot&amp;quot;] } } } 结果是，像《The Wind Rises》和《Gone with the Wind》这样的电影出现在搜索结果的顶部附近，而像《Twister》这样的电影，其剧情简介中可能包含“wind”字，则出现在底部附近。
您可以在字段名中使用通配符。例如，以下查询将搜索 speaker 字段以及所有以 play_ 开头的字段，例如 play_name 或 play_title ：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;multi_match&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;hamlet&amp;quot;, &amp;quot;fields&amp;quot;: [&amp;quot;speaker&amp;quot;, &amp;quot;play_*&amp;quot;] } } } 如果您不提供 fields 参数， multi_match 查询将搜索 index.query. Default_field 设置中指定的字段，该设置默认为 * 。默认行为是提取映射中所有适用于词级查询的字段，过滤元数据字段，并将所有提取的字段组合起来构建查询。
查询中的子句最大数量由 indices.</description></item><item><title>多样化采样聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/diversified-sampler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/diversified-sampler/</guid><description>多样化采样聚合 # diversified_sampler 多样化采样聚合允许你通过去重包含相同 field 的文档来减少样本池分布的偏差。它通过使用 max_docs_per_value 和 field 设置来实现，这些设置限制了在分片上收集的 field 的最大文档数。 max_docs_per_value 设置是一个可选参数，用于确定每个 field 将返回的最大文档数。此设置的默认值为 1 。
与 sampler 聚合类似，你可以使用 shard_size 设置来控制在任何单个分片上收集的最大文档数，如下面的示例所示：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;sample&amp;quot;: { &amp;quot;diversified_sampler&amp;quot;: { &amp;quot;shard_size&amp;quot;: 1000, &amp;quot;field&amp;quot;: &amp;quot;response.keyword&amp;quot; }, &amp;quot;aggs&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;agent.keyword&amp;quot; } } } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;sample&amp;quot; : { &amp;quot;doc_count&amp;quot; : 3, &amp;quot;terms&amp;quot; : { &amp;quot;doc_count_error_upper_bound&amp;quot; : 0, &amp;quot;sum_other_doc_count&amp;quot; : 0, &amp;quot;buckets&amp;quot; : [ { &amp;quot;key&amp;quot; : &amp;quot;Mozilla/5.</description></item><item><title>多词项查询</title><link>/easysearch/main/docs/references/search/term/terms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term/terms/</guid><description>多词项查询 # 使用 terms 多词项查询在同一字段中搜索多个词项。例如，以下查询搜索具有 ID 61809 和 61810 的文档：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;line_id&amp;quot;: [ &amp;quot;61809&amp;quot;, &amp;quot;61810&amp;quot; ] } } } 如果文档与数组中的任何词项匹配，则会返回该文档。
默认情况下， terms 查询中允许的最大词项数量为 65,536。要更改最大词项数量，请更新 index.max_terms_count 设置。
为了更好的查询性能，请传递包含已排序词项的长期数组（按 UTF-8 字节值升序排序）。
根据高亮器类型和查询中词项的数量，高亮显示词项查询结果的能力可能无法保证。
参数说明 # 该查询接受以下参数。所有参数都是可选的。
参数 数据类型 描述 &amp;lt;field&amp;gt; String 要搜索的字段。只有当文档的字段值与查询中至少一个词项完全匹配（包括正确的空格和大小写）时，该文档才会出现在结果中。 boost Float 一个浮点数值，用于指定该字段对相关性分数的权重。大于 1.0 的值会增加字段的权重。介于 0.0 和 1.0 之间的值会降低字段的权重。默认值为 1.0。 _name String 查询标签的查询名称。可选。 value_type String 指定用于过滤的值类型。有效值为 default 和 bitmap 。如果省略，则值默认为 default 。 条件查找 # 条件查找功能会检索单个文档的字段值并将其用作搜索词。您可以使用条件查找功能来搜索大量词项。</description></item><item><title>多路复用分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/multiplexer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/multiplexer/</guid><description>多路复用分词过滤器 # 多路复用(multiplexer)分词过滤器允许你通过应用不同的过滤器来创建同一词元的多个版本。当你想要以多种方式分析同一个词元时，这非常有用。例如，你可能希望使用不同的词干提取、同义词或 n-gram 过滤器来分析一个词元，并将所有生成的词元一起使用。这个分词过滤器的工作方式是复制分词流，并对每个副本应用不同的过滤器。
多路复用分词过滤器会从分词流中移除重复的词元。
多路复用分词过滤器不支持多词同义词(synonym)过滤器、同义词图(synonym_graph)分词过滤器或组合词（shingle）分词过滤器，因为这些过滤器不仅需要分析当前词元，还需要分析后续的词元，以便正确确定如何转换输入内容。
参数说明 # 多路复用分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 filters 可选 字符串列表 要应用于分词流每个副本的分词过滤器的逗号分隔列表。默认值为空列表。 preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 true。 参考样例 # 以下示例请求创建了一个名为 multiplexer_index 的新索引，并配置了一个带有多路复用过滤器的分词器：
PUT /multiplexer_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;english_stemmer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;english&amp;quot; }, &amp;quot;synonym_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;synonym&amp;quot;, &amp;quot;synonyms&amp;quot;: [ &amp;quot;quick,fast&amp;quot; ] }, &amp;quot;multiplexer_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;multiplexer&amp;quot;, &amp;quot;filters&amp;quot;: [&amp;quot;english_stemmer&amp;quot;, &amp;quot;synonym_filter&amp;quot;], &amp;quot;preserve_original&amp;quot;: true } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;multiplexer_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;multiplexer_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>多过滤聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/filters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/filters/</guid><description>多过滤聚合 # filters 聚合与 filter 聚合相同，但它允许你使用多个过滤器聚合。 filter 聚合结果为一个分组，而 filters 聚合会返回多个分组，每个定义的过滤器对应一个分组。
要为所有未匹配任何过滤器查询的文档创建一个分组，将 other_bucket 属性设置为 true ：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;200_os&amp;quot;: { &amp;quot;filters&amp;quot;: { &amp;quot;other_bucket&amp;quot;: true, &amp;quot;filters&amp;quot;: [ { &amp;quot;term&amp;quot;: { &amp;quot;response.keyword&amp;quot;: &amp;quot;200&amp;quot; } }, { &amp;quot;term&amp;quot;: { &amp;quot;machine.os.keyword&amp;quot;: &amp;quot;osx&amp;quot; } } ] }, &amp;quot;aggs&amp;quot;: { &amp;quot;avg_amount&amp;quot;: { &amp;quot;avg&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;bytes&amp;quot; } } } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;200_os&amp;quot; : { &amp;quot;buckets&amp;quot; : [ { &amp;quot;doc_count&amp;quot; : 12832, &amp;quot;avg_amount&amp;quot; : { &amp;quot;value&amp;quot; : 5897.</description></item><item><title>大写分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/uppercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/uppercase/</guid><description>大写分词过滤器 # 大写(uppercase)分词过滤器用于在分析过程中将所有词元（单词）转换为大写形式。
参考样例 # 以下示例请求创建了一个名为 uppercase_example 的新索引，并配置了一个带有大写过滤器的分词器。
PUT /uppercase_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;uppercase_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;uppercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;uppercase_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;uppercase_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
GET /uppercase_example/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;uppercase_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch is powerful&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;EASYSEARCH&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;IS&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;POWERFUL&amp;quot;, &amp;quot;start_offset&amp;quot;: 14, &amp;quot;end_offset&amp;quot;: 22, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>子关联聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/children/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/children/</guid><description>子关联聚合 # children 子关联聚合是一种存储分组聚合，它根据索引中定义的父子关系创建包含子文档的单个存储分组。
子关联聚合与 join 字段类型配合使用，以聚合与父文档关联的子文档。
子关联聚合标识与特定子关系名称匹配的子文档，而 parent 父聚合标识具有匹配子文档的父文档。这两个聚合都采用子关系名称作为输入。
参数说明 # children 聚合采用以下参数。
参数 必需/可选 数据类型 描述 type 必填 String join 字段中的子类型的名称。这标识了要使用的父子关系。 参考样例 # 以下示例构建一个包含三名员工的小型公司数据库。每个员工记录都与父部门记录具有子联接关系。
首先，创建一个 company 索引，其中包含一个将部门（父级）映射到员工（子级）的联接字段：
PUT /company { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;join_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;join&amp;quot;, &amp;quot;relations&amp;quot;: { &amp;quot;department&amp;quot;: &amp;quot;employee&amp;quot; } }, &amp;quot;department_name&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;employee_name&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;salary&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;double&amp;quot; }, &amp;quot;hire_date&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;date&amp;quot; } } } } 接下来，使用三个部门和三名员工填充数据。下表显示了父子分配。</description></item><item><title>子查询</title><link>/easysearch/main/docs/references/search/joining/has-child/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/joining/has-child/</guid><description>子查询 # has_child 查询返回匹配特定查询的子文档的父文档。您可以通过使用连接字段类型在相同索引中的文档之间建立父子关系。
has_child 查询比其他查询慢，因为它执行了连接操作。随着指向不同父文档的匹配子文档数量的增加，性能会降低。您搜索中的每个 has_child 查询都可能显著影响查询性能。如果您优先考虑速度，请避免使用此查询或尽可能限制其使用。
参考样例 # 在您运行一个 has_child 查询之前，您的索引必须包含一个连接字段，以便建立父子关系。索引映射请求使用以下格式：
PUT /example_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;relationship_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;join&amp;quot;, &amp;quot;relations&amp;quot;: { &amp;quot;parent_doc&amp;quot;: &amp;quot;child_doc&amp;quot; } } } } } 在这个例子中，您将配置一个包含代表产品和其品牌的文档的索引。
首先，创建索引并建立 brand 和 product 之间的父子关系：
PUT testindex1 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;product_to_brand&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;join&amp;quot;, &amp;quot;relations&amp;quot;: { &amp;quot;brand&amp;quot;: &amp;quot;product&amp;quot; } } } } } 创建两个父（品牌）文档：
PUT testindex1/_doc/1 { &amp;quot;name&amp;quot;: &amp;quot;Luxury brand&amp;quot;, &amp;quot;product_to_brand&amp;quot; : &amp;quot;brand&amp;quot; } PUT testindex1/_doc/2 { &amp;quot;name&amp;quot;: &amp;quot;Economy brand&amp;quot;, &amp;quot;product_to_brand&amp;quot; : &amp;quot;brand&amp;quot; } 索引三个子（产品）文档：</description></item><item><title>字母词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/letter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/letter/</guid><description>字母词元生成器 # 字母(letter)词元生成器会根据任何非字母字符将文本拆分成单词。它在处理许多欧洲语种时效果良好，但对于一些亚洲语种却不太有效，因为在亚洲语种中，单词之间不是由空格来分隔的。
参考样例 # 下面的示例请求创建了一个名为 my_index 的新索引，并配置了一个使用字母词元生成器的分词器。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_letter_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;letter&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_letter_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST _analyze { &amp;quot;tokenizer&amp;quot;: &amp;quot;letter&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Cats 4EVER love chasing butterflies!&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Cats&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 4, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;EVER&amp;quot;, &amp;quot;start_offset&amp;quot;: 6, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;love&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;chasing&amp;quot;, &amp;quot;start_offset&amp;quot;: 16, &amp;quot;end_offset&amp;quot;: 23, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;butterflies&amp;quot;, &amp;quot;start_offset&amp;quot;: 24, &amp;quot;end_offset&amp;quot;: 35, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 4 } ] }</description></item><item><title>字符串查询</title><link>/easysearch/main/docs/references/search/full-text/query-string/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/full-text/query-string/</guid><description>字符串查询 # 一个 query_string 查询根据查询字符串语法解析查询字符串。它提供了创建强大而简洁的查询的功能，这些查询可以包含通配符并搜索多个字段。
使用 query_string 查询的搜索不会返回嵌套文档。要搜索嵌套字段，请使用 nested 查询。
查询字符串查询具有严格的语法，在语法无效时会返回错误。因此，它不适合搜索框应用程序。对于不太严格的替代方案，可以考虑使用 simple_query_string 查询。如果你不需要查询语法支持，使用 match 查询。
字符串查询语法 # 字符串查询语法基于 Apache Lucene 查询语法。
在以下情况下，您可以使用查询字符串语法：
在一个 query_string 查询中，例如：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;query_string&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;the wind AND (rises OR rising)&amp;quot; } } } 如果你使用 HTTP 请求查询参数进行搜索，例如：
GET _search?q=wind 字符串查询由词项和运算符组成。词项是一个单词（例如，在查询 wind rises 中，词项是 wind 和 rises ）。如果多个词项被引号包围，它们被视为一个短语，其中单词按出现的顺序匹配（例如， &amp;ldquo;wind rises&amp;rdquo; ）。运算符（如 OR 、 AND 和 NOT ）指定用于解释查询字符串中文本的布尔逻辑。</description></item><item><title>字符组词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/character-group/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/character-group/</guid><description>字符组词元生成器 # 字符组（char_group）词元生成器使用特定字符作为分隔符将文本拆分为词元。它适用于需要简单直接进行分词的场景，为基于词元生成器的匹配模式提供了一种更简单的替代方案，避免了额外的复杂性。
参考样例 # 以下示例请求创建了一个名为my_index的新索引，并配置了一个带有 char_group 字符组词元生成器的分词器。该词元生成器会依据空格、连字符 - 和冒号 : 来分割文本。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_char_group_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;char_group&amp;quot;, &amp;quot;tokenize_on_chars&amp;quot;: [ &amp;quot;whitespace&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;:&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_char_group_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_char_group_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_char_group_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_char_group_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Fast-driving cars: they drive fast!</description></item><item><title>存储分组脚本聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/bucket-script/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/bucket-script/</guid><description>存储分组脚本聚合 # bucket_script 存储分组脚本聚合是一个父管道聚合，它执行脚本以跨一组存储分组执行每个存储分组的数字计算。使用 bucket_script 聚合对分分组聚合中的多个指标执行自定义数值计算。例如，您可以：
计算派生指标和复合指标。 使用 if/else 语句应用条件逻辑。 计算特定于业务的 KPI，例如自定义评分指标。 参数说明 # bucket_script 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 Object 一个变量名称到分分组指标的映射，用于识别脚本中使用的指标。这些指标必须是数值型。参见脚本变量 。 script 必需 String 或 Object 要执行的脚本。可以是内联脚本、存储脚本或脚本文件。脚本可以访问通过 buckets_path 参数定义的变量名。必须返回一个数值。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认值为 skip 。详见 数据空缺。 format 可选 String 一个 DecimalFormat 格式化字符串。将在聚合的 value_as_string 参数中返回格式化后的输出。 脚本变量 # buckets_path 参数将脚本变量名称映射到父聚合的指标。然后可以在脚本中使用这些变量。</description></item><item><title>存储分组选择聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/bucket-selector/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/bucket-selector/</guid><description>存储分组选择聚合 # bucket_selector 存储分组选择聚合是一个父管道聚合，它评估脚本以确定直方图 （或 date_histogram ）聚合返回的存储分组是否应包含在最终结果中。
与创建新值的管道聚合不同，bucket_selector 聚合充当筛选器，根据指定的条件保留或删除整个存储分组。使用此聚合可根据存储分组的计算指标筛选存储分组。
参数说明 # bucket_selector 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 Object 变量名称到分分组指标的映射，用于标识要在脚本中使用的指标。指标必须是数字。请参阅脚本变量 。 script 必需 String 或 Object 要执行的脚本。可以是内联脚本、存储脚本或脚本文件。该脚本可以访问 buckets_path 参数中定义的变量名称。必须返回布尔值。返回 false 的存储分组将从最终输出中删除。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认值为 skip 。详见 数据空缺。 参考样例 # 以下示例创建间隔为一周的日期直方图。sum 子聚合计算每周所有销售额的总和。最后，bucket_selector 聚合会筛选生成的每周存储分组，删除所有总值不超过 75,000 美元的存储分组：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;sales_per_week&amp;quot;: { &amp;quot;date_histogram&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;order_date&amp;quot;, &amp;quot;calendar_interval&amp;quot;: &amp;quot;week&amp;quot; }, &amp;quot;aggs&amp;quot;: { &amp;quot;weekly_sales&amp;quot;: { &amp;quot;sum&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;taxful_total_price&amp;quot;, &amp;quot;format&amp;quot;: &amp;quot;$#,###.</description></item><item><title>导数聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/derivative/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/derivative/</guid><description>导数聚合 # derivative 导数聚合是一个父聚合，用于计算聚合每个分组的一阶和二阶导数。
对于有序的分组序列， derivative 将当前分组和前一个分组中的指标值之差近似为一阶导数。
参数说明 # derivative 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。 示例：一阶导数 # 以下示例创建一个每月间隔的日期直方图。 sum 子聚合计算每个月所有字节的和。最后， derivative 聚合计算 sum 子聚合的一阶导数。一阶导数估计为当前月份和上个月字节数之间的差值：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;sales_per_month&amp;quot;: { &amp;quot;date_histogram&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;@timestamp&amp;quot;, &amp;quot;calendar_interval&amp;quot;: &amp;quot;month&amp;quot; }, &amp;quot;aggs&amp;quot;: { &amp;quot;number_of_bytes&amp;quot;: { &amp;quot;sum&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;bytes&amp;quot; } }, &amp;quot;bytes_deriv&amp;quot;: { &amp;quot;derivative&amp;quot;: { &amp;quot;buckets_path&amp;quot;: &amp;quot;number_of_bytes&amp;quot; } } } } } } 返回内容显示了为第二和第三个分组计算出的导数：</description></item><item><title>小写分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/lowercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/lowercase/</guid><description>小写分词过滤器 # 小写(lowercase)分词过滤器用于将分词流中的所有字符转换为小写，从而使搜索不区分大小写。
参数 # 小写分词过滤器可以使用以下参数进行配置。
参数 必填/可选 描述 language 可选 指定一个特定语言的分词过滤器。有效值为：
- 希腊语 greek
- 爱尔兰语irish
- 土耳其语turkish。
默认值是 Lucene 的小写过滤器。 参考样例 # 以下示例请求创建了一个名为 custom_lowercase_example 的新索引。它配置了一个带有小写过滤器的分析器，并指定语言为希腊语。
PUT /custom_lowercase_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;greek_lowercase_example&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;greek_lowercase&amp;quot;] } }, &amp;quot;filter&amp;quot;: { &amp;quot;greek_lowercase&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;greek&amp;quot; } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>小写词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/lowercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/lowercase/</guid><description>小写词元生成器 # 小写（lowercase）词元生成器会在空白处将文本拆分成词条，然后把所有词条转换为小写形式。从功能上来说，这与配置一个字母词元生成器并搭配一个小写词元过滤器的效果是一样的。不过，使用小写词元生成器效率更高，因为分词操作是在一步之内完成的。
参考样例 # 以下示例请求创建了一个名为 my-lowercase-index 的新索引，并配置了一个使用小写词元生成器的分词器：
PUT /my-lowercase-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_lowercase_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_lowercase_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_lowercase_tokenizer&amp;quot; } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my-lowercase-index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_lowercase_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;This is a Test. Easysearch 123!&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;this&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 4, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 5, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 9, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;start_offset&amp;quot;: 10, &amp;quot;end_offset&amp;quot;: 14, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 16, &amp;quot;end_offset&amp;quot;: 26, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 4 } ] }</description></item><item><title>嵌套查询</title><link>/easysearch/main/docs/references/search/joining/nested/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/joining/nested/</guid><description>嵌套查询 # nested 查询充当其他查询的包装器，用于搜索嵌套字段。嵌套字段对象被视为单独的文档进行搜索。如果对象匹配搜索条件， nested 查询将返回根级别的父文档。
参考样例 # 在运行 nested 查询之前，您的索引必须包含一个嵌套字段。
要配置一个包含嵌套字段的示例索引，请发送以下请求：
PUT /testindex { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;patient&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;nested&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;name&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;age&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;integer&amp;quot; } } } } } } 接下来，将一个文档索引到示例索引中：
PUT /testindex/_doc/1 { &amp;quot;patient&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;John Doe&amp;quot;, &amp;quot;age&amp;quot;: 56 } } 要搜索嵌套的 patient 字段，请将您的查询包裹在 nested 查询中，并提供 path 给嵌套字段：
GET /testindex/_search { &amp;quot;query&amp;quot;: { &amp;quot;nested&amp;quot;: { &amp;quot;path&amp;quot;: &amp;quot;patient&amp;quot;, &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;patient.</description></item><item><title>嵌套聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/nested/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/nested/</guid><description>嵌套聚合 # nested 聚合让你能够对嵌套对象内的字段进行聚合。 nested 类型是对象数据类型的特殊版本，它允许对象数组以独立于彼此的方式进行索引，从而可以独立于彼此进行查询
使用 object 类型，所有数据都存储在同一个文档中，因此搜索匹配可以跨越子文档。例如，想象一个 logs 索引，其中 pages 映射为 object 数据类型：
PUT logs/_doc/0 { &amp;quot;response&amp;quot;: &amp;quot;200&amp;quot;, &amp;quot;pages&amp;quot;: [ { &amp;quot;page&amp;quot;: &amp;quot;landing&amp;quot;, &amp;quot;load_time&amp;quot;: 200 }, { &amp;quot;page&amp;quot;: &amp;quot;blog&amp;quot;, &amp;quot;load_time&amp;quot;: 500 } ] } Easysearch 合并所有看起来像这样的实体关系的子属性：
{ &amp;quot;logs&amp;quot;: { &amp;quot;pages&amp;quot;: [&amp;quot;landing&amp;quot;, &amp;quot;blog&amp;quot;], &amp;quot;load_time&amp;quot;: [&amp;quot;200&amp;quot;, &amp;quot;500&amp;quot;] } } 所以，如果你想要用 pages=landing 和 load_time=500 搜索这个索引，即使 load_time 的 landing 值为 200，这个文档也符合条件。
如果你想要确保不会发生这种跨对象匹配，将字段映射为 nested 类型：
PUT logs { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;pages&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;nested&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;page&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;load_time&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;double&amp;quot; } } } } } } 嵌套文档允许你索引相同的 JSON 文档，但会保持你的页面在不同的 Lucene 文档中，使得只有 pages=landing 和 load_time=200 这样的搜索能返回预期结果。内部上，嵌套对象将数组中的每个对象索引为一个单独的隐藏文档，这意味着每个嵌套对象都可以独立于其他对象进行查询。</description></item><item><title>布尔查询</title><link>/easysearch/main/docs/references/search/compound/bool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/compound/bool/</guid><description>布尔查询 # 一个布尔（ bool ）查询可以将多个查询子句组合成一个高级查询。这些子句通过布尔逻辑组合起来，以在结果中找到匹配的文档。
在布尔（ bool ）查询中使用以下查询子句：
子句 行为 must 逻辑 and 运算符。结果必须匹配此子句中的所有查询。 must_not 逻辑 not 运算符。所有匹配项都将被排除在结果之外。如果 must_not 包含多个子句，则只返回不匹配任何这些子句的文档。例如， &amp;quot;must_not&amp;quot;:[{clause_A}, {clause_B}] 等同于 NOT(A OR B) 。 should 逻辑 or 运算符。结果必须匹配至少一个查询。匹配更多 should 子句会增加文档的相关性分数。您可以使用 minimum_should_match 参数设置必须匹配的最小查询数量。如果一个查询包含 must 或 filter 子句，默认 minimum_should_match 值为 0。否则，默认 minimum_should_match 值为 1。 filter 逻辑 and 运算符，在应用查询之前首先应用于减少您的数据集。过滤器子句中的查询是一个是或否选项。如果文档匹配查询，则它将出现在结果中；否则，它将不会出现。过滤器查询的结果通常会被缓存以允许更快的返回。使用过滤器查询根据精确匹配、范围、日期或数字来过滤结果。 一个布尔查询具有以下结构：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;bool&amp;quot;: { &amp;quot;must&amp;quot;: [ {} ], &amp;quot;must_not&amp;quot;: [ {} ], &amp;quot;should&amp;quot;: [ {} ], &amp;quot;filter&amp;quot;: {} } } } 例如，假设您在 Easysearch 集群中索引了莎士比亚的全部作品。您希望构建一个满足以下要求的单个查询：</description></item><item><title>常用词组分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/common-grams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/common-grams/</guid><description>常用词组分词过滤器 # 常用词组(common_grams)分词过滤器通过保留文本中常见的短语（常用词组）来提高搜索的相关性。当处理某些语言或数据集时，特定的单词组合经常作为一个整体出现，如果将它们当作单独的词元处理，可能会影响搜索的相关性，此时该过滤器就非常有用。如果输入字符串中常用词，这个分词过滤器会同时生成它们的一元词组（单个词）和二元词组（两个词的组合）。
使用这个分词过滤器可以保持常用短语的完整性，从而提高搜索的相关性。这有助于更准确地匹配查询内容，尤其是对于频繁出现的单词组合。它还能通过减少不相关的匹配结果来提高搜索的精度。
使用此过滤器时，你必须谨慎选择并维护常用词(common_words)列表。
参数说明 # 常用词组分词过滤器可通过以下参数进行配置：
参数 必需/可选 数据类型 描述 common_words 必需 字符串列表 一个应被视为常用词词组的单词列表。如果 common_words 参数是一个空列表，common_grams 分词过滤器将成为一个无操作过滤器，即它根本不会修改输入的词元。 ignore_case 可选 布尔值 指示过滤器在匹配常用词时是否应忽略大小写差异。默认值为 false。 query_mode 可选 布尔值 当设置为 true 时，应用以下规则：
- 从 common_words 生成的一元词组（单个词）不包含在输出中。
- 非常用词后跟常用词形成的二元词组会保留在输出中。
- 如果非常用词的一元词组后面紧接着一个常用词，则该一元词组会被排除。
- 如果非常用词出现在文本末尾且前面是一个常用词，其一元词组不包含在输出中。 参考样例 # 以下示例请求创建了一个名为 my_common_grams_index 的新索引，并配置了一个使用常用词组（common_grams）过滤器的分析器。
PUT /my_common_grams_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_common_grams_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;common_grams&amp;quot;, &amp;quot;common_words&amp;quot;: [&amp;quot;a&amp;quot;, &amp;quot;in&amp;quot;, &amp;quot;for&amp;quot;], &amp;quot;ignore_case&amp;quot;: true, &amp;quot;query_mode&amp;quot;: true } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_common_grams_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>常量化评分查询</title><link>/easysearch/main/docs/references/search/compound/constant-score/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/compound/constant-score/</guid><description>常量化评分查询 # 如果您需要返回包含某个词的文档，而不管该词出现多少次，您可以使用 constant_score 查询。 constant_score 查询包装一个过滤器查询，并将结果中的所有文档的关联分数设置为 boost 参数的值。因此，所有返回的文档具有相同的关联分数，并且不考虑词频/逆文档频率（TF/IDF）。过滤器查询不会计算关联分数。此外，Easysearch 会缓存常用的过滤器查询以提高性能。
参考样例 # 使用以下查询返回 shakespeare 索引中包含单词“Hamlet”的文档：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;constant_score&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;text_entry&amp;quot;: &amp;quot;Hamlet&amp;quot; } }, &amp;quot;boost&amp;quot;: 1.2 } } } 结果中的所有文档都被分配了 1.2 的相关性分数：
{ &amp;quot;took&amp;quot;: 8, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 96, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 1.2, &amp;quot;hits&amp;quot;: [ { &amp;quot;_index&amp;quot;: &amp;quot;shakespeare&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;32535&amp;quot;, &amp;quot;_score&amp;quot;: 1.</description></item><item><title>平图化分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/flatten-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/flatten-graph/</guid><description>平图化分词过滤器 # 平图化（flatten_graph）分词过滤器用于处理在图结构中同一位置生成多个词元时出现的复杂词元关系。一些分词过滤器，比如同义词图（synonym_graph）和词分隔符图（word_delimiter_graph），会生成多位置词元——即相互重叠或跨越多个位置的词元。这些词元图对于搜索查询很有用，但在索引过程中却不被直接支持。平图化分词过滤器会将多位置词元解析为一个线性的词元序列。对图进行扁平化处理可确保与索引过程兼容。
词元图的扁平化是一个有损耗的过程。只要有可能，就应避免使用平图化过滤器。相反，应仅在搜索分词器中应用图分词过滤器，这样就无需使用平图化分词过滤器了。
参考样例 # 以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有平图化分词过滤器的分词器：
PUT /test_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_index_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_custom_filter&amp;quot;, &amp;quot;flatten_graph&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;my_custom_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;word_delimiter_graph&amp;quot;, &amp;quot;catenate_all&amp;quot;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /test_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_index_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch helped many employers&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;helped&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 17, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;many&amp;quot;, &amp;quot;start_offset&amp;quot;: 18, &amp;quot;end_offset&amp;quot;: 22, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;employers&amp;quot;, &amp;quot;start_offset&amp;quot;: 23, &amp;quot;end_offset&amp;quot;: 32, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 } ] }</description></item><item><title>平均分组聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/avg-bucket/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/avg-bucket/</guid><description>平均分组聚合 # avg_bucket 平均分组聚合是一个同级聚合，它计算上一个聚合的每个分组中的指标平均值。
指定的指标必须是数值型的，且同级聚合必须是多分组聚合。
参数说明 # avg_bucket 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合存储分组的路径。请参阅存储分组路径 。 gap_policy 可选 String 要应用于缺失数据的策略。有效值为 skip 和 insert_zeros。默认值为 skip。请参阅数据差距 。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出 。 参考样例 # 以下示例创建间隔为一个月的日期直方图。sum 子聚合计算每个月的字节总和。最后，avg_bucket 聚合根据这些总和计算每月的平均字节数：
POST sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;visits_per_month&amp;quot;: { &amp;quot;date_histogram&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;@timestamp&amp;quot;, &amp;quot;interval&amp;quot;: &amp;quot;month&amp;quot; }, &amp;quot;aggs&amp;quot;: { &amp;quot;sum_of_bytes&amp;quot;: { &amp;quot;sum&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;bytes&amp;quot; } } } }, &amp;quot;avg_monthly_bytes&amp;quot;: { &amp;quot;avg_bucket&amp;quot;: { &amp;quot;buckets_path&amp;quot;: &amp;quot;visits_per_month&amp;gt;sum_of_bytes&amp;quot; } } } } 返回内容 # 聚合返回每月存储分组的平均字节数：</description></item><item><title>平均聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/average/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/average/</guid><description>平均聚合 # avg 指标是一个单值指标，它返回某个字段的平均值。
参数说明 # avg 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 计算平均值的字段。 missing 可选 Float 要分配给字段缺失实例的值。默认情况下， avg 会在计算中忽略缺失值。 参考样例 # 以下示例请求计算示例数据中 taxful_total_price 字段的平均值：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;avg_taxful_total_price&amp;quot;: { &amp;quot;avg&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;taxful_total_price&amp;quot; } } } } 返回内容 # 返回内容包含 taxful_total_price 的平均值：
{ &amp;quot;took&amp;quot;: 85, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 4675, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: null, &amp;quot;hits&amp;quot;: [] }, &amp;quot;aggregations&amp;quot;: { &amp;quot;avg_taxful_total_price&amp;quot;: { &amp;quot;value&amp;quot;: 75.</description></item><item><title>序列差分聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/serial-diff/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/serial-diff/</guid><description>序列差分聚合 # serial_diff 序列差分聚合是一个父级管道聚合，用于计算当前分组中指标值与上一个分组中指标值之间的差值。它将结果存储在当前分组中。
使用 serial_diff 聚合来计算具有指定滞后的时间段之间的变化。 lag 参数（一个正整数值）指定要从中减去当前值的哪个先前分组的值。默认的 lag 值是 1 ，这意味着 serial_diff 从当前分组中的值减去立即前一个分组中的值。
参数说明 # serial_diff 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。 lag 可选 Integer 用于从当前数据分组中减去的历史数据分组。必须是正整数。默认为 1 。 参考样例 # 以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， serial_diff 聚合计算这些总和之间的月度字节差异：</description></item><item><title>归一化分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/normalization/</guid><description>归一化分词过滤器 # 归一化(normalization)分词过滤器旨在以减少文本差异（尤其是特殊字符差异）的方式对文本进行调整和简化。它主要用于通过对特定语言中的字符进行标准化处理，来应对书写上的差异。
以下是可用的归一化分词过滤器：
阿拉伯语归一化: arabic_normalization 德语归一化: german_normalization 印地语归一化: hindi_normalization 印度语系归一化: indic_normalization 索拉尼语归一化: sorani_normalization 波斯语归一化: persian_normalization 斯堪的纳维亚语归一化: scandinavian_normalization 斯堪的纳维亚语折叠处理归一化: scandinavian_folding 塞尔维亚语归一化: serbian_normalization 参考样例 # 以下示例请求创建了一个名为 german_normalizer_example 的新索引，并配置了一个带有 german_normalization的分词器：
PUT /german_normalizer_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;german_normalizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;german_normalization&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;german_normalizer_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;german_normalizer&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /german_normalizer_example/_analyze { &amp;quot;text&amp;quot;: &amp;quot;Straße München&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;german_normalizer_analyzer&amp;quot; } 返回内容包含产生的词元</description></item><item><title>截断分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/truncate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/truncate/</guid><description>截断分词过滤器 # 截断(truncate)分词过滤器用于缩短超过指定长度的词元。它会将词元修剪至最大字符数，确保超过该限制的词元被截断。
参数说明 # 截断分词过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 length 可选 整数 指定生成的词元的最大长度。默认值为 10。 参考样例 # 以下示例请求创建了一个名为 truncate_example 的新索引，并配置了一个带有截断过滤器的分词器。
PUT /truncate_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;truncate_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;truncate&amp;quot;, &amp;quot;length&amp;quot;: 5 } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;truncate_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;truncate_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>扩展统计分组聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/extended-stats/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/extended-stats/</guid><description>扩展统计分组聚合 # extended_stats_bucket 扩展统计分组聚合是 stats_bucket 同级聚合的更全面的版本。除了 stats_bucket 提供的基本统计度量外， extended_stats_bucket 还计算以下指标：
平方和 方差 总体方差 抽样方差 标准差 总体标准差 抽样标准差 标准差界限： ** 上限 ** 下限 ** 种群上限 ** 种群下限 ** 采样上限 ** 采样下限 标准差和方差是总体统计量；它们分别始终等于总体标准差和方差。
std_deviation_bounds 对象定义了一个范围，该范围在均值（默认为两个标准差）的上方和下方跨越指定的标准差数量。此对象始终包含在输出中，但仅对正态分布的数据才有意义。在解释这些值之前，请验证您的数据集是否遵循正态分布。
指定的指标必须是数值型，并且同级聚合必须是多分组聚合。
参数说明 # extended_stats_bucket 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。 sigma 可选 Double 非负） 用于计算 std_deviation_bounds 区间的均值上方和下方的标准差数量。默认值为 2 。参见 extended_stats 中定义范围。 参考样例 # 以下示例创建一个以一个月为间隔的日期直方图。 sum 子聚合计算每个月的字节总和。最后， extended_stats_bucket 聚合返回这些总和的扩展统计信息：</description></item><item><title>扩展统计聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/extended-stats/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/extended-stats/</guid><description>扩展统计聚合 # extended_stats 扩展统计聚合是统计数据 stats 聚合的更全面版本。除了统计数据提供的基本统计指标外，extended_stats 还计算以下内容：
平方和 方差 总体方差 抽样方差 标准差 总体标准差 抽样标准差 标准差界限： ** 上限 ** 下限 ** 总体上限 ** 总体下限 ** 抽样上限 ** 抽样下限 其中标准差和方差是总体统计量；它们分别始终等于总体标准差和方差。
std_deviation_bounds 对象定义了一个范围，该范围在均值（默认为两个标准差）的上方和下方跨越指定的标准差数量。此对象始终包含在输出中，但仅对正态分布的数据才有意义。在解释这些值之前，请验证您的数据集是否遵循正态分布。
参数说明 # extended_stats 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 返回扩展统计信息的字段名称。 sigma 可选 Double（非负） 计算 std_deviation_bounds 区间所使用的均值上下标准差的数量。默认值为 2 。 missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，包含缺失值的文档将不会出现在扩展统计中。 参考样例 # 以下示例请求数据中 taxful_total_price 的扩展统计信息：</description></item><item><title>拼音分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/pinyin-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/pinyin-analyzer/</guid><description>拼音分词器 # 总体介绍 # pinyin-analyzer 拼音分词器能够在索引阶段将 中文字符实时转写为拼音，并在检索阶段对拼音、汉字及混输查询进行统一匹配。借助该插件，你可以轻松实现：
支持 全拼、首字母、全拼拼接 等多种检索方式； 保留非中文字符，实现「中英混输」搜索； 借助 token filter 在分词链中灵活组合不同策略； 在联想输入、排序、聚合等场景下提升中文用户体验。 适用于人名、地名、品牌、歌曲、商品等多种中文文本检索场景。
参数说明 # 下表整理了 pinyin 分词器 / 过滤器支持的全部可选参数及默认值。
参数 说明 默认值 keep_first_letter 仅保留每个汉字的拼音首字母。例如 刘德华 → ldh true keep_separate_first_letter 将首字母分开存储，提高命中率。例如 刘德华 → l, d, h，注意：这可能会由于词频增加而增加查询模糊性。 false limit_first_letter_length 首字母结果最长长度 16 keep_full_pinyin 保留每个汉字的全拼。如 刘德华 → liu, de, hua true keep_joined_full_pinyin 将全拼连接在一起。如 刘德华 → liudehua false keep_none_chinese 保留非中文字符（数字/字母等） true keep_none_chinese_together 将连续非中文字符作为整体保留。例如， DJ 音乐家 变成 DJ 、 yin 、 yue 、 jia 。当设置为 false 时， DJ 音乐家 变成 D 、 J 、 yin 、 yue 、 jia 。注意： keep_none_chinese 应该先启用。 true keep_none_chinese_in_first_letter 在首字母结果中保留非中文字。例如， 刘德华 AT2016 变成 ldhat2016 。符 true keep_none_chinese_in_joined_full_pinyin 在拼接全部拼音时保留非中文字。例如， 刘德华 2016 变成 liudehua2016 。符 false none_chinese_pinyin_tokenize 将非中文字符中可能的拼音继续拆分例如， liudehuaalibaba13zhuanghan 变成 liu 、 de 、 hua 、 a 、 li 、 ba 、 ba 、 13 、 zhuang 、 han 。注意： keep_none_chinese 和 keep_none_chinese_together 应该先启用。 true keep_original 同时保留原始文本 false lowercase 对非中文字符强制小写 true trim_whitespace 去除首尾空格 true remove_duplicated_term 开启时，移除重复术语以节省索引空间。例如， de 的 变为 de 。注意：位置相关的查询可能会受影响。积 false ignore_pinyin_offset 忽略 offset 以允许重叠 token。在 6.</description></item><item><title>指纹分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/fingerprint-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/fingerprint-analyzer/</guid><description>指纹分词器 # 指纹（fingerprint）分词器会创建一个文本指纹。该分词器对从输入生成的词项（词元）进行排序和去重，然后使用一个分隔符将它们连接起来。它通常用于数据去重，对于包含相同单词的相似输入，无论单词顺序如何，它都可以产生相同的输出结果。
指纹分词器由以下组件组成：
标准分词生成器 词元小写化过滤器 ASCII 词元过滤器 停用词词元过滤器 指纹词元过滤器 参数说明 # 指纹分词器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 separator 可选 字符串 在对词项进行词元生成、排序和去重后，用于连接这些词项的字符。默认值为一个空格（ ）。 max_output_size 可选 整数 定义输出词元的最大大小。如果连接后的指纹超过此大小，将被截断。默认值为 255。 stopwords 可选 字符串或字符串列表 自定义的停用词列表。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_custom_fingerprint_index 带有指纹分词器的索引：
PUT /my_custom_fingerprint_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_fingerprint_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;fingerprint&amp;quot;, &amp;quot;separator&amp;quot;: &amp;quot;-&amp;quot;, &amp;quot;max_output_size&amp;quot;: 50, &amp;quot;stopwords&amp;quot;: [&amp;quot;to&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;over&amp;quot;, &amp;quot;and&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_fingerprint_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查使用该分词器生成的词元：</description></item><item><title>指纹分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/fingerprint/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/fingerprint/</guid><description>指纹分词过滤器 # 指纹（fingerprint）分词过滤器用于对文本进行标准化和去重处理。当文本处理的唯一性或者一致性至关重要时，这个过滤器特别有用。指纹分词过滤器通过以下步骤处理文本以实现这一目的：
小写转换：将所有文本转换为小写。 分词：把文本拆分成词元。 排序：按字母顺序排列这些词元。 去重：消除重复的词元。 合并词元：将这些词元合并成一个字符串，通常使用空格或其他指定的分隔符进行连接。 参数说明 # 指纹分词过滤器可以使用以下两个参数进行配置。
参数 必需/可选 数据类型 描述 max_output_size 可选 整数 限制生成的指纹字符串的长度。如果拼接后的字符串超过了 max_output_size，过滤器将不会产生任何输出，从而得到一个空词元。默认值是 255 separator 可选 字符串 定义在词元排序和去重后，用于将词元连接成单个字符串。默认是空格（&amp;quot; &amp;quot;）。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有指纹分词过滤器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_fingerprint&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;fingerprint&amp;quot;, &amp;quot;max_output_size&amp;quot;: 200, &amp;quot;separator&amp;quot;: &amp;quot;-&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_fingerprint&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>捕获匹配分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/pattern-capture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/pattern-capture/</guid><description>捕获匹配分词过滤器 # 捕获匹配(pattern_capture)分词过滤器是一种功能强大的过滤器，它使用正则表达式根据特定模式来捕获和提取文本的部分内容。当你想要提取词元的特定部分，例如电子邮件域名、话题标签或数字，并将其重新用于进一步的分析或索引编制时，这个过滤器会非常有用。
参数说明 # 捕获匹配分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 patterns 必需 字符串数组 用于捕获文本部分的正则表达式数组。 preserve_original 必需 布尔值 是否在输出中保留原始词元。默认值为 true。 参考样例 # 以下示例请求创建了一个名为 email_index 的新索引，并配置了一个带有捕获匹配过滤器的分词器，以便从电子邮件地址中提取本地部分和域名。
PUT /email_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;email_pattern_capture&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern_capture&amp;quot;, &amp;quot;preserve_original&amp;quot;: true, &amp;quot;patterns&amp;quot;: [ &amp;quot;^([^@]+)&amp;quot;, &amp;quot;@(.+)$&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;email_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;uax_url_email&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;email_pattern_capture&amp;quot;, &amp;quot;lowercase&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>排序功能</title><link>/easysearch/main/docs/references/search/specialized/rank-feature/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/specialized/rank-feature/</guid><description>排序功能 # 使用 rank_feature 查询根据文档中的数值（如相关性分数、人气或新鲜度）提升文档分数。如果你希望使用数值特征微调相关性排名，这种查询非常理想。与全文检索不同， rank_feature 仅关注数值信号；在复合查询（如 bool ）中与其他查询结合时效果最佳。
rank_feature 查询要求目标字段映射为 rank_feature 字段类型。这可以启用内部优化的评分，从而实现快速高效的提升。
分数影响取决于字段值以及可选的 saturation 、 log 或 sigmoid 函数。这些函数在查询时动态应用以计算最终文档分数；它们不会更改或存储文档中的任何值。
参数说明 # rank_feature 查询支持以下参数。
参数 数据类型 必需/可选 描述 field String 必需 一个 rank_feature 或 rank_features 字段，用于影响文档评分。 boost Float 可选 应用于评分的乘数。默认值为 1.0 。0 到 1 之间的值会降低评分；大于 1 的值会提高评分。 saturation Object 可选 对特征值应用饱和函数。随着值的增加，增益会增长，但在 pivot 之后会趋于平稳。如果没有提供其他函数，则使用此默认函数。一次只能使用 saturation 、 log 或 sigmoid 中的一个函数。 log Object 可选 使用基于字段值的对数评分函数。适用于大范围值的场景。一次只能使用 saturation 、 log 或 sigmoid 中的一个函数。 sigmoid Object 可选 对评分影响应用 S 形曲线，由 pivot 和 exponent 控制。一次只能使用 saturation 、 log 或 sigmoid 中的一个函数。 positive_score_impact Boolean 可选 当 false 时，较低值会获得更高的评分。适用于像价格这样的特征，其中较小值更好。作为映射的一部分定义。默认值为 true 。 参考样例 # 以下示例展示了如何定义和使用 rank_feature 字段来影响文档评分。</description></item><item><title>搜索分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/search-analyzers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/search-analyzers/</guid><description>搜索分词器 # 搜索分词器是在查询时指定的，当你对text字段进行全文匹配查询时，它会被用于分析被查询字符串。
搜索分词器的生效流程 # 在查询时确定对查询字符串使用哪种分词器，Easysearch 会按以下顺序检查以下参数：
查询的 analyzer 参数 字段的 search_analyzer 映射参数 索引设置 analysis.analyzer.default_search 字段的 analyzer 映射参数 standard 分词器（默认分词器） 在大多数情况下，没有必要指定与索引分词器不同的搜索分词器，而且这样做可能会对搜索结果的相关性产生负面影响，或者导致出现意想不到的搜索结果。
为查询内容指定搜索分词器 # 在查询时，你可以在 analyzer 字段中指定想要使用的分词器：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;text_entry&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;speak the truth&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;english&amp;quot; } } } } 为字段指定搜索分词器 # 在创建索引映射时，你可以为每个文本字段提供 search_analyzer 参数。在提供 search_analyzer 时，你还必须提供 analyzer 参数，该参数指定在索引创建时的写入索引分词器内应用。
例如，以下请求将 simple 分词器指定为 text_entry 字段的索引分词器，将 whitespace 分词器指定为该字段的搜索分词器：
PUT testindex { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text_entry&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;simple&amp;quot;, &amp;quot;search_analyzer&amp;quot;: &amp;quot;whitespace&amp;quot; } } } } 为索引指定默认的搜索分词器 # 如果你希望在搜索时使用同一个分词器来解析所有的查询，你可以在 analysis.</description></item><item><title>数据脱敏</title><link>/easysearch/main/docs/references/security/access-control/field-masking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/access-control/field-masking/</guid><description>数据脱敏 # 如果您的数据里面包含一些敏感信息，除了通过 字段级权限 来进行访问控制，您还可以通过混淆字段里面的内容来进行脱敏。目前，字段数据脱敏仅适用于基于字符串的字段，支持加密哈希和正则替换字段的内容。
字段脱敏与字段级权限一起可以在相同的角色级别和索引级别上工作。您可以允许某些角色查看明文格式的敏感字段，并为其他角色脱敏这些字段。带有脱敏字段的搜索结果可能如下所示：
{ &amp;#34;_index&amp;#34;: &amp;#34;movies&amp;#34;, &amp;#34;_type&amp;#34;: &amp;#34;_doc&amp;#34;, &amp;#34;_source&amp;#34;: { &amp;#34;year&amp;#34;: 2013, &amp;#34;directors&amp;#34;: [&amp;#34;Ron Howard&amp;#34;], &amp;#34;title&amp;#34;: &amp;#34;ca998e768dd2e6cdd84c77015feb29975f9f498a472743f159bec6f1f1db109e&amp;#34; } } 设置盐值 # 可以在 easysearch.yml 设置一个随机字符串:
security.compliance.salt: abcdefghijklmnopqrstuvqxyz1234567890 属性 说明 security.compliance.salt 生成哈希值时要使用的盐值。必须至少为 32 个字符。仅允许使用 ASCII 字符。选填。 配置脱敏字段 # role.yml # masked_movie: cluster: [] indices: - names: - movies privileges: - &amp;#34;read&amp;#34; field_mask: - &amp;#34;genres&amp;#34; - &amp;#34;title&amp;#34; REST API # 参照 创建角色.</description></item><item><title>日期直方图聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/date-histogram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/date-histogram/</guid><description>日期直方图聚合 # date_histogram 日期直方图聚合使用日期计算来为时间序列数据生成直方图。
例如，您可以找到您的网站每月有多少次访问：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;logs_per_month&amp;quot;: { &amp;quot;date_histogram&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;@timestamp&amp;quot;, &amp;quot;interval&amp;quot;: &amp;quot;month&amp;quot; } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;logs_per_month&amp;quot; : { &amp;quot;buckets&amp;quot; : [ { &amp;quot;key_as_string&amp;quot; : &amp;quot;2020-10-01T00:00:00.000Z&amp;quot;, &amp;quot;key&amp;quot; : 1601510400000, &amp;quot;doc_count&amp;quot; : 1635 }, { &amp;quot;key_as_string&amp;quot; : &amp;quot;2020-11-01T00:00:00.000Z&amp;quot;, &amp;quot;key&amp;quot; : 1604188800000, &amp;quot;doc_count&amp;quot; : 6844 }, { &amp;quot;key_as_string&amp;quot; : &amp;quot;2020-12-01T00:00:00.000Z&amp;quot;, &amp;quot;key&amp;quot; : 1606780800000, &amp;quot;doc_count&amp;quot; : 5595 } ] } } 返回内容包含三个月的日志。如果你绘制这些值，你可以看到你的网站每月请求流量的峰值和低谷。</description></item><item><title>日期范围聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/date-range/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/date-range/</guid><description>日期范围聚合 # date_range 日期范围聚合在概念上与 range 聚合相同，只是它允许执行日期计算。例如，你可以获取过去 10 天内的所有文档。为了使日期更易读，可以使用 format 参数包含格式：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;number_of_bytes&amp;quot;: { &amp;quot;date_range&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;@timestamp&amp;quot;, &amp;quot;format&amp;quot;: &amp;quot;MM-yyyy&amp;quot;, &amp;quot;ranges&amp;quot;: [ { &amp;quot;from&amp;quot;: &amp;quot;now-10d/d&amp;quot;, &amp;quot;to&amp;quot;: &amp;quot;now&amp;quot; } ] } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;number_of_bytes&amp;quot; : { &amp;quot;buckets&amp;quot; : [ { &amp;quot;key&amp;quot; : &amp;quot;03-2021-03-2021&amp;quot;, &amp;quot;from&amp;quot; : 1.6145568E12, &amp;quot;from_as_string&amp;quot; : &amp;quot;03-2021&amp;quot;, &amp;quot;to&amp;quot; : 1.615451329043E12, &amp;quot;to_as_string&amp;quot; : &amp;quot;03-2021&amp;quot;, &amp;quot;doc_count&amp;quot; : 0 } ] } } }</description></item><item><title>映射字符过滤器</title><link>/easysearch/main/docs/references/text-analysis/character-filters/mapping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/character-filters/mapping/</guid><description>映射字符过滤器 # 映射（mapping）字符过滤器接受一个用于字符替换的键值对映射。每当该过滤器遇到与某个键匹配的字符串时，它就会用相应的值来替换这些字符。替换值可以是空字符串。
该过滤器采用贪婪匹配方式，这意味着会匹配最长的匹配结果。
在分词过程之前，需要进行特定文本替换的场景下，映射字符过滤器会很有帮助。
参考样例 # 以下请求配置了一个映射字符过滤器，该过滤器可将罗马数字（如 I、II 或 III）转换为对应的阿拉伯数字（1、2 和 3）：
GET /_analyze { &amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;, &amp;quot;char_filter&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;mapping&amp;quot;, &amp;quot;mappings&amp;quot;: [ &amp;quot;I =&amp;gt; 1&amp;quot;, &amp;quot;II =&amp;gt; 2&amp;quot;, &amp;quot;III =&amp;gt; 3&amp;quot;, &amp;quot;IV =&amp;gt; 4&amp;quot;, &amp;quot;V =&amp;gt; 5&amp;quot; ] } ], &amp;quot;text&amp;quot;: &amp;quot;I have III apples and IV oranges&amp;quot; } 返回内容中包含一个词元，其中罗马数字已被替换为阿拉伯数字：
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;1 have 3 apples and 4 oranges&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 32, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] } 参数说明 # 你可以使用以下任意一个参数来配置键值映射。</description></item><item><title>最大值聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/maximum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/maximum/</guid><description>最大值聚合 # max 最大值指标是一个单值指标，返回字段的最大值。
max 聚合使用 double （双精度）表示来比较数值字段。对于包含 long 或 unsigned_long 超过 2^53 的整数值的字段，结果应被视为近似值，因为 double 的尾数中的有效位数是 53。
参数说明 # max 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 计算最大值的字段名称。 missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，则包含缺失值的文档将从聚合中排除。 参考样例 # 以下示例请求在数据中查找最昂贵的商品——即 base_unit_price 值最大的商品：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;max_base_unit_price&amp;quot;: { &amp;quot;max&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;products.base_unit_price&amp;quot; } } } } 返回内容 # 如以下示例返回内容所示，聚合返回 products.</description></item><item><title>最大分组聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/max-bucket/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/max-bucket/</guid><description>最大分组聚合 # max_bucket 最大分组聚合是一个同级聚合，用于计算先前聚合中每个分组中某个指标的最大值。
指定的指标必须是数值型，并且同级聚合必须是多分组聚合。
参数说明 # max_bucket 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。 参考样例 # 以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， max_bucket 聚合找到最大值——这些分组中最大的那个：
POST sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;visits_per_month&amp;quot;: { &amp;quot;date_histogram&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;@timestamp&amp;quot;, &amp;quot;interval&amp;quot;: &amp;quot;month&amp;quot; }, &amp;quot;aggs&amp;quot;: { &amp;quot;sum_of_bytes&amp;quot;: { &amp;quot;sum&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;bytes&amp;quot; } } } }, &amp;quot;max_monthly_bytes&amp;quot;: { &amp;quot;max_bucket&amp;quot;: { &amp;quot;buckets_path&amp;quot;: &amp;quot;visits_per_month&amp;gt;sum_of_bytes&amp;quot; } } } } 返回内容 # max_bucket 聚合返回跨多个分组的指定指标的最大值。在这个示例中，它计算了 sum_of_bytes 指标在 visits_per_month 中的每月最大字节数。 value 字段显示了在所有分组中找到的最大值。 keys 数组包含观察到该最大值的分组的键。它是一个数组，因为多个分组可以具有相同最大值。在这种情况下，所有匹配的分组键都会被包含。这确保了即使多个时间段（或分组项）具有相同最大值，结果也是准确的：</description></item><item><title>最小值分组聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/min-bucket/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/min-bucket/</guid><description>最小值分组聚合 # min_bucket 最小值分组聚合是一个同级聚合，用于计算先前聚合中每个分组中某个指标的最小值。
指定的指标必须是数值型，并且同级聚合必须是多分组聚合。
参数说明 # min_bucket 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。 参考样例 # 以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， min_bucket 聚合找到最小值——这些分组中最小的一个：
POST sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;visits_per_month&amp;quot;: { &amp;quot;date_histogram&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;@timestamp&amp;quot;, &amp;quot;interval&amp;quot;: &amp;quot;month&amp;quot; }, &amp;quot;aggs&amp;quot;: { &amp;quot;sum_of_bytes&amp;quot;: { &amp;quot;sum&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;bytes&amp;quot; } } } }, &amp;quot;min_monthly_bytes&amp;quot;: { &amp;quot;min_bucket&amp;quot;: { &amp;quot;buckets_path&amp;quot;: &amp;quot;visits_per_month&amp;gt;sum_of_bytes&amp;quot; } } } } 返回内容 # min_bucket 聚合返回跨多个分组的指定指标的最小值。在这个示例中，它计算了 sum_of_bytes 指标在 visits_per_month 中的每月最小字节数。 value 字段显示了在所有分组中找到的最小值。 keys 数组包含观察到该最小值的分组的键。它是一个数组，因为多个分组可以具有相同的最小值。在这种情况下，所有匹配的分组键都会被包含。这确保了即使多个时间段（或分组项）具有相同的最小值，结果也是准确的：</description></item><item><title>最小值聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/minimum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/minimum/</guid><description>最小值聚合 # min 最小值指标是一个单值指标，返回字段的最小值。
min 聚合使用 double （双精度）表示来比较数值字段。对于包含 long 或 unsigned_long 且绝对值大于 2 53 的字段，结果应被视为近似值，因为 double 尾数中的有效位数是 53。
参数说明 # min 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 计算最小值的字段名称。 missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，则包含缺失值的文档将从聚合中排除。 参考样例 # 以下示例请求在样本数据中查找最便宜的商品——即 base_unit_price 值最小的商品：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;min_base_unit_price&amp;quot;: { &amp;quot;min&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;products.base_unit_price&amp;quot; } } } } 返回内容 # 如以下示例返回所示，聚合返回了 products.</description></item><item><title>最小匹配</title><link>/easysearch/main/docs/references/search/minimum-should-match/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/minimum-should-match/</guid><description>最小匹配 # minimum_should_match 参数可用于全文搜索，并指定文档必须匹配的最小词项数量才能在搜索结果中返回。
以下示例要求文档至少匹配三个搜索词中的两个才能作为搜索结果返回：
GET /shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;text_entry&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;prince king star&amp;quot;, &amp;quot;minimum_should_match&amp;quot;: &amp;quot;2&amp;quot; } } } } 在这个示例中，查询有三个可选子句，它们通过 OR 结合，因此文档必须匹配 prince 和 king ，或者 prince 和 star ，或者 king 和 star 。
参数值说明 # 您可以指定 minimum_should_match 参数为以下值之一。
值类型 示例 描述 非负整数 2 一个文档必须匹配这个数量的可选子句。 负整数 -1 一个文档必须匹配可选子句总数减去这个数。 非负百分比 70% 一个文档必须匹配可选子句总数的这个百分比。要匹配的子句数向下取整到最接近的整数。 负百分比 -30% 一个文档可以有这个百分比的不匹配的可选子句。文档允许不匹配的子句数向下取整到最接近的整数。 组合 2&amp;lt;75% n&amp;lt;p% 格式的表达式。如果可选子句的数量小于或等于 n ，文档必须匹配所有可选子句。如果可选子句的数量大于 n ，则文档必须匹配 p 百分比的可选子句。 多种组合 3&amp;lt;-1 5&amp;lt;50% 用空格分隔的多个组合。每个条件适用于 &amp;lt; 符号左侧数字更多的可选子句数量。在这个例子中，如果有三个或更少可选子句，文档必须匹配所有它们。如果有四或五个可选子句，文档必须匹配所有但一个。如果有 6 个或更多可选子句，文档必须匹配 50% 的它们。 设 n 为文档必须匹配的可选子句数量。当 n 计算为百分比时，如果 n 小于 1，则使用 1。如果 n 大于可选子句的数量，则使用可选子句的数量。</description></item><item><title>最小哈希分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/min-hash/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/min-hash/</guid><description>最小哈希分词过滤器 # 最小哈希（min_hash）分词过滤器用于基于最小哈希近似算法为词元生成哈希值，这对于检测文档间的相似度很有用。最小哈希分词过滤器会为一组词元（通常来自一个经过分词的字段）生成哈希值。
参数说明 # 最小哈希分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 hash_count 可选 整数 为每个词元生成的哈希值数量。增加此值通常会提高相似度估计的准确性，但会增加计算成本。默认值为 1。 bucket_count 可选 整数 要使用的哈希桶数量。这会影响哈希的粒度。更多的桶能提供更细的粒度并减少哈希冲突，但需要更多内存。默认值为 512。 hash_set_size 可选 整数 每个桶中要保留的哈希数量。这会影响哈希质量。更大的集合大小可能会带来更好的相似度检测效果，但会消耗更多内存。默认值为 1。 with_rotation 可选 布尔值 当设置为 true 时，如果 hash_set_size 为 1，过滤器会用从其右侧按环形顺序找到的第一个非空桶的值填充空桶。如果 bucket_count 参数大于 1，此设置会自动默认为 true；否则，默认为 false。 参考样例 # 以下示例请求创建了一个名为 minhash_index 的新索引，并配置了一个带有最小哈希过滤器的分词器：
PUT /minhash_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;minhash_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;min_hash&amp;quot;, &amp;quot;hash_count&amp;quot;: 3, &amp;quot;bucket_count&amp;quot;: 512, &amp;quot;hash_set_size&amp;quot;: 1, &amp;quot;with_rotation&amp;quot;: false } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;minhash_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;minhash_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>条件分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/condition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/condition/</guid><description>条件分词过滤器 # 条件(condition)分词过滤器是一种特殊类型的过滤器，它允许你根据特定标准有条件地应用其他分词过滤器。这使得在文本分析过程中，你能更精准地控制何时应用特定的分词过滤器。你可以配置多个过滤器，并且只有当满足你定义的条件时，这些过滤器才会被应用。该分词过滤器在进行特定语言处理和特殊字符处理时非常有用。
参数说明 # 要使用条件分词过滤器，必须配置两个参数，具体如下：
参数 必需/可选 数据类型 描述 filter 必需 数组 当满足由 script 参数定义的指定条件时，指定应对词元应用哪个分词过滤器。 script 必需 对象 配置一个内联脚本，该脚本定义了要应用 filter 参数中指定的过滤器所需满足的条件（仅接受内联脚本）。 参考样例 # 以下示例请求创建了一个名为 my_conditional_index 的新索引，并配置了一个带有条件过滤器的分词器。该过滤器会对任何包含字符序列 “um” 的词元应用小写（lowercase）字母过滤器。
PUT /my_conditional_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_conditional_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;condition&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;], &amp;quot;script&amp;quot;: { &amp;quot;source&amp;quot;: &amp;quot;token.getTerm().toString().contains('um')&amp;quot; } } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_conditional_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>析取最大查询</title><link>/easysearch/main/docs/references/search/compound/disjunction-max/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/compound/disjunction-max/</guid><description>析取最大查询 # 析取最大（ dis_max ）查询返回与一个或多个查询子句匹配的任何文档。对于与多个查询子句匹配的文档，相关性得分设置为所有匹配查询子句中的最高相关性得分。
当返回的文档的相关性分数相同时，您可以使用 tie_breaker 参数来增加匹配多个查询子句的文档的权重。
参考样例 # 考虑一个包含两个文档的索引，您按照以下方式索引这些文档：
PUT testindex1/_doc/1 { &amp;quot;title&amp;quot;: &amp;quot; The Top 10 Shakespeare Poems&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Top 10 sonnets of England's national poet and the Bard of Avon&amp;quot; } PUT testindex1/_doc/2 { &amp;quot;title&amp;quot;: &amp;quot;Sonnets of the 16th Century&amp;quot;, &amp;quot;body&amp;quot;: &amp;quot;The poems written by various 16-th century poets&amp;quot; } 使用 dis_max 查询来搜索包含单词“莎士比亚诗歌”的文档
GET testindex1/_search { &amp;quot;query&amp;quot;: { &amp;quot;dis_max&amp;quot;: { &amp;quot;queries&amp;quot;: [ { &amp;quot;match&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;Shakespeare poems&amp;quot; }}, { &amp;quot;match&amp;quot;: { &amp;quot;body&amp;quot;: &amp;quot;Shakespeare poems&amp;quot; }} ] } } } 返回内容包含两个文档：</description></item><item><title>查询和过滤上下文</title><link>/easysearch/main/docs/references/search/query-filter-context/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/query-filter-context/</guid><description>查询和过滤上下文 # 查询由查询子句组成，这些子句可以在过滤（filter）上下文或查询上下文中运行。 在过滤（filter）上下文中的查询子句会询问“文档是否匹配查询子句？”并返回匹配的文档。在查询上下文中的查询子句会询问“文档与查询子句匹配程度如何？”，返回匹配的文档，并以相关性分数的形式提供每个文档的相关性。
相关性分数 # 相关性分数衡量文档与查询的匹配程度。它是一个正浮点数，Easysearch 会记录在每个文档的 _score 元数据字段中：
&amp;quot;hits&amp;quot; : [ { &amp;quot;_index&amp;quot; : &amp;quot;shakespeare&amp;quot;, &amp;quot;_id&amp;quot; : &amp;quot;32437&amp;quot;, &amp;quot;_score&amp;quot; : 18.781435, &amp;quot;_source&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;line&amp;quot;, &amp;quot;line_id&amp;quot; : 32438, &amp;quot;play_name&amp;quot; : &amp;quot;Hamlet&amp;quot;, &amp;quot;speech_number&amp;quot; : 3, &amp;quot;line_number&amp;quot; : &amp;quot;1.1.3&amp;quot;, &amp;quot;speaker&amp;quot; : &amp;quot;BERNARDO&amp;quot;, &amp;quot;text_entry&amp;quot; : &amp;quot;Long live the king!&amp;quot; } }, ... 一个更高的分数表示文档更相关。虽然不同的查询类型计算相关性分数的方式不同，但所有查询类型都会考虑查询子句是在过滤（filter）上下文还是查询上下文中运行。
在查询上下文中使用你想影响相关性分数的查询子句，并在过滤（filter）上下文中使用所有其他查询子句。
过滤上下文 # 在过滤上下文中的查询子句会问“文档是否匹配查询子句？”，这个问题有二元答案。例如，如果你有一个包含学生数据的索引，你可以使用过滤上下文来回答以下关于学生的疑问:
学生的 honors 状态是否设置为 true ？ 学生的 graduation_year 是否在 2020–2022 范围内？ 使用过滤上下文时，Easysearch 会返回匹配的文档，而不会计算相关性分数。因此，您应该使用过滤上下文来处理具有精确值的字段。</description></item><item><title>标准分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/standard-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/standard-analyzer/</guid><description>标准分词器 # 标准（standard）分词器是在未指定其他分词器时默认使用的分词器。它旨在为通用文本处理提供一种基础且高效的方法。
该分词器由以下词元生成器和词元过滤器组成：
标准（standard）词元生成器：去除大部分标点符号，并依据空格和其他常见分隔符对文本进行分割。 小写（lowercase）词元过滤器：将所有词元转换为小写，以确保匹配时不区分大小写。 停用词（stop）词元过滤器：从分词后的输出中移除常见的停用词，例如 “the”、“is” 和 “and”。 参考样例 # 以下命令创建一个名为 my_standard_index 并使用标准分词器的索引：
PUT /my_standard_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot; } } } } 参数说明 # 你可以使用以下参数来配置标准分词器。
参数 必填/可选 数据类型 描述 max_token_length 可选 整数 设置生成的词元的最大长度。如果超过此长度，词元将按照 max_token_length 中配置的长度拆分为多个词元。默认值为 255。 stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 配置自定义分词器 # 以下命令配置一个索引，该索引带有一个等同于标准分词器的自定义分词器：</description></item><item><title>标准词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/standard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/standard/</guid><description>标准词元生成器 # 标准（standard）词元生成器是 Easysearch 中的默认分词器。它基于单词边界，采用一种基于语法的方法对文本进行分词，这种方法能够识别字母、数字以及标点等其他字符。它具有高度的通用性，适用于多种语言，它使用了 Unicode 文本分割规则（ UAX#29）来将文本分割成词元。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用标准词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_standard_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;standard&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_standard_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_standard_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch is powerful, fast, and scalable.&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;powerful&amp;quot;, &amp;quot;start_offset&amp;quot;: 14, &amp;quot;end_offset&amp;quot;: 22, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;fast&amp;quot;, &amp;quot;start_offset&amp;quot;: 24, &amp;quot;end_offset&amp;quot;: 28, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;and&amp;quot;, &amp;quot;start_offset&amp;quot;: 30, &amp;quot;end_offset&amp;quot;: 33, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 4 }, { &amp;quot;token&amp;quot;: &amp;quot;scalable&amp;quot;, &amp;quot;start_offset&amp;quot;: 34, &amp;quot;end_offset&amp;quot;: 42, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 5 } ] } 参数说明 # 标准词元生成器可以使用以下参数进行配置。</description></item><item><title>模糊查询</title><link>/easysearch/main/docs/references/search/term/fuzzy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term/fuzzy/</guid><description>模糊查询 # 模糊查询用于搜索包含与搜索词相似的词条的文档，相似度在允许的最大 Damerau-Levenshtein 距离范围内。Damerau-Levenshtein 距离衡量将一个词条变为另一个词条所需的一字符变化的数量。这些变化包括：
Replacements: 替换，cat 变为 bat Insertions: 插入，cat 变为 cats Deletions: 删除，cat 变为 at Transpositions: 转换，cat 变为 act 模糊查询会生成一个包含所有可能扩展的搜索词列表，这些扩展在 Damerau-Levenshtein 距离内。你可以在 max_expansions 字段中指定此类扩展的最大数量。查询然后会搜索匹配任何扩展的文档。如果你将 transpositions 参数设置为 false ，则搜索将使用经典的 Levenshtein 距离。
以下示例查询搜索发言者 HALET （误写为 HAMLET ）。未指定最大编辑距离，因此使用默认的 AUTO 编辑距离：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;fuzzy&amp;quot;: { &amp;quot;speaker&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;HALET&amp;quot; } } } } 返回内容包含所有发言者为 HAMLET 的文档。
以下示例查询使用高级参数搜索单词 HALET ：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;fuzzy&amp;quot;: { &amp;quot;speaker&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;HALET&amp;quot;, &amp;quot;fuzziness&amp;quot;: &amp;quot;2&amp;quot;, &amp;quot;max_expansions&amp;quot;: 40, &amp;quot;prefix_length&amp;quot;: 0, &amp;quot;transpositions&amp;quot;: true, &amp;quot;rewrite&amp;quot;: &amp;quot;constant_score&amp;quot; } } } } 参数说明 # 查询接受字段名称（ ）作为顶级参数：</description></item><item><title>正则查询</title><link>/easysearch/main/docs/references/search/term/regexp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term/regexp/</guid><description>正则查询 # 使用 regexp 正则查询来搜索符合正则表达式的词项。有关编写正则表达式的更多信息，请参见正则表达式语法。
以下查询搜索以任何大写或小写字母开头的任何词项 amlet ：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;regexp&amp;quot;: { &amp;quot;play_name&amp;quot;: &amp;quot;[a-zA-Z]amlet&amp;quot; } } } 请注意以下重要事项：
正则表达式应用于字段中的词条（即，标记/token），而不是整个字段。 默认情况下，正则表达式的最大长度为 1,000 个字符。要更改最大长度，请更新 index.max_regex_length 设置。 正则表达式使用 Lucene 语法，这与更标准的实现有所不同。请充分测试以确保获得预期的结果。要了解更多信息，请参阅 Lucene 文档。 为了提高正则表达式查询的性能，避免使用没有前缀或后缀的通配符模式，例如 .* 或 .*?+ 。 regexp 查询可能会非常耗时，并且需要将 search.allow_expensive_queries 设置为 true 。在频繁执行 regexp 查询之前，请测试其对集群性能的影响，并考虑使用其他可能达到类似效果的查询。 通配符字段类型构建了一个特别为通配符和正则表达式查询设计的索引。 参数说明 # 查询接受字段名称（ &amp;lt;field&amp;gt; ）作为顶级参数：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;regexp&amp;quot;: { &amp;quot;&amp;lt;field&amp;gt;&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;[Ss]ample&amp;quot;, ... } } } } &amp;lt;field&amp;gt; 接受以下参数。除了 value 之外，所有参数都是可选的。</description></item><item><title>正则表达式语法</title><link>/easysearch/main/docs/references/search/regex-syntax/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/regex-syntax/</guid><description>正则表达式语法 # 正则表达式（regex）是一种使用特殊符号和运算符定义搜索模式的方法。这些模式允许你在字符串中匹配字符序列。
在 Easysearch 中，你可以在以下查询类型中使用正则表达式：
regexp query_string Easysearch 使用 Apache Lucene 正则表达式引擎，该引擎有自己的语法和限制。它不使用 Perl 兼容正则表达式（PCRE），因此某些熟悉的正则表达式功能可能会表现不同或不受支持。
在 regexp 和 query_string 查询之间进行选择 # regexp 和 query_string 查询都支持正则表达式，但它们的行为不同，适用于不同的使用场景。
特性 regexp 查询 query_string 查询 模式匹配 正则表达式模式必须匹配整个字段值 正则表达式模式可以匹配字段中的任何部分 flags 支持 flags 启用可选正则表达式运算符 flags 不支持 查询类型 词级查询（未评分） 全文检索（评分和解析） 最佳使用场景 对关键字或精确字段进行严格模式匹配 使用支持正则表达式模式的灵活查询字符串在分析字段中进行搜索 复杂查询组合 仅限于正则表达式模式 支持 AND 、 OR 、通配符、字段、提升值以及其他功能。参见查询字符串查询。 保留字符 # Lucene 的正则表达式引擎支持所有 Unicode 字符。然而，以下字符被视为特殊运算符：</description></item><item><title>求和分组聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/sum-bucket/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/sum-bucket/</guid><description>求和分组聚合 # sum_bucket 聚合是一个同级聚合，用于计算先前聚合中每个分组中指标的总和。
指定的指标必须是数值型，并且同级聚合必须是多分组聚合。
参数说明 # sum_bucket 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。 参考样例 # 以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， sum_bucket 聚合通过汇总这些总和来计算每个月的总字节数：
POST sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;visits_per_month&amp;quot;: { &amp;quot;date_histogram&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;@timestamp&amp;quot;, &amp;quot;interval&amp;quot;: &amp;quot;month&amp;quot; }, &amp;quot;aggs&amp;quot;: { &amp;quot;sum_of_bytes&amp;quot;: { &amp;quot;sum&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;bytes&amp;quot; } } } }, &amp;quot;sum_monthly_bytes&amp;quot;: { &amp;quot;sum_bucket&amp;quot;: { &amp;quot;buckets_path&amp;quot;: &amp;quot;visits_per_month&amp;gt;sum_of_bytes&amp;quot; } } } } 返回内容 # 该聚合返回所有月度分组中的字节总和：</description></item><item><title>求和聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/sum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/sum/</guid><description>求和聚合 # sum 求和聚合是一种单值指标聚合，计算字段中匹配文档中提取的数值的总和。此聚合常用于计算诸如收入、数量或持续时间等指标的总计。
参数说明 # sum 聚合接受以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 聚合的字段。必须是数值字段。 script 可选 Object 用于计算聚合自定义值的脚本。可以替代或与 field 结合使用。 missing 可选 Number 缺少目标字段时使用的默认值。 参考样例 # 以下示例演示了如何计算物流索引中记录的交付总重量。
创建一个索引：
PUT /deliveries { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;shipment_id&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;weight_kg&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;double&amp;quot; } } } } 添加示例文档：
POST /deliveries/_bulk?refresh=true {&amp;quot;index&amp;quot;: {}} {&amp;quot;shipment_id&amp;quot;: &amp;quot;S001&amp;quot;, &amp;quot;weight_kg&amp;quot;: 12.</description></item><item><title>波特词干分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/porter-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/porter-stem/</guid><description>波特词干分词过滤器 # 波特词干(porter_stem)分词过滤器会将单词还原为其基本（或词干）形式，并去除单词中常见的后缀，这有助于通过单词的词根来匹配相似的单词。例如，单词“running”会被词干提取为“run”。此分词过滤器主要用于英语，并基于 波特词干提取算法进行词干提取操作。
参考样例 # 以下示例请求创建了一个名为 my_stem_index 的新索引，并配置了一个带有波特词干提取过滤器的分词器。
PUT /my_stem_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_porter_stem&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;porter_stem&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;porter_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_porter_stem&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_stem_index/_analyze { &amp;quot;text&amp;quot;: &amp;quot;running runners ran&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;porter_analyzer&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;run&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;runner&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;ran&amp;quot;, &amp;quot;start_offset&amp;quot;: 16, &amp;quot;end_offset&amp;quot;: 19, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>渗透查询</title><link>/easysearch/main/docs/references/search/specialized/percolate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/specialized/percolate/</guid><description>渗透查询 # 使用 percolate 渗透查询来查找与给定文档匹配的已存储查询。此操作与常规搜索相反：常规搜索是查找与查询匹配的文档，而 percolate 查询是查找与文档匹配的查询。 percolate 查询通常用于告警、通知和反向搜索用例。
在使用 percolate 查询时，请考虑以下几点：
您可以在线提供文档进行 percolate 操作，或者从索引中获取现有文档。 文档和存储的查询必须使用相同的字段名称和类型。 您可以结合使用透查、过滤和评分来构建复杂的匹配系统。 percolate 查询被视为昂贵的查询，并且只有在集群设置 search.allow_expensive_queries 被设置为 true （默认值）时才会运行。如果此设置是 false ， percolate 查询将被拒绝。 percolate 查询在各种实时匹配场景中非常有用。一些常见的用例包括：
电子商务通知：用户可以注册对产品的兴趣，例如，“有新的苹果笔记本电脑时通知我”。当新产品文档被索引时，系统会找到所有匹配保存的查询的用户并发送警报。 工作警报：求职者根据首选的工作标题或地点保存查询，新的职位发布将与这些查询匹配以触发警报。 安全和警报系统：Percolate 传入的日志或事件数据与保存的规则或异常模式进行匹配。 新闻筛选：将传入的文章与保存的主题配置文件进行匹配，以分类或提供相关内容。 工作过程 # 保存的查询存储在一个特殊的 percolator 字段类型中。 文档会与所有保存的查询进行比较。 每个匹配的查询都会返回其 _id 。 如果启用了高亮显示，匹配的文本片段也会被返回。 如果发送了多个文档， _percolator_document_slot 会显示匹配的文档。 参考样例 # 以下示例展示了如何存储 percolate 查询，并使用不同方法对它们进行测试。
首先，创建一个索引并配置其 mappings 字段类型以存储保存的查询：
PUT /my_percolator_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;query&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot; }, &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; } } } } 在 title 字段中添加一个匹配“apple”的查询：</description></item><item><title>父 ID 查询</title><link>/easysearch/main/docs/references/search/joining/parent-id/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/joining/parent-id/</guid><description>父 ID 查询 # parent_id 查询返回具有指定 ID 的父文档的子文档。您可以通过使用连接字段类型在相同索引中的文档之间建立父子关系。
参考样例 # 在您运行一个 parent_id 查询之前，您的索引必须包含一个连接字段，以便建立父子关系。索引映射请求使用以下格式：
PUT /example_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;relationship_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;join&amp;quot;, &amp;quot;relations&amp;quot;: { &amp;quot;parent_doc&amp;quot;: &amp;quot;child_doc&amp;quot; } } } } } 对于此示例，首先配置一个包含代表产品和其品牌的文档的索引，这些文档在 has_child 查询示例中有所描述。
要搜索特定父文档的子文档，请使用 parent_id 查询。以下查询返回具有 ID 1 的父文档的子文档（产品）：
GET testindex1/_search { &amp;quot;query&amp;quot;: { &amp;quot;parent_id&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;product&amp;quot;, &amp;quot;id&amp;quot;: &amp;quot;1&amp;quot; } } } 返回子产品：
{ &amp;quot;took&amp;quot;: 57, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 1, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 0.</description></item><item><title>父查询</title><link>/easysearch/main/docs/references/search/joining/has-parent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/joining/has-parent/</guid><description>父查询 # has_parent 查询返回匹配特定查询的父文档的子文档。您可以通过使用连接字段类型在相同索引中的文档之间建立父子关系。
has_parent 查询比其他查询慢，因为它执行了连接操作。随着匹配的父文档数量的增加，性能会降低。您搜索中的每个 has_parent 查询都可能显著影响查询性能。如果您优先考虑速度，请避免使用此查询或尽可能限制其使用。
参考样例 # 在您运行一个 has_parent 查询之前，您的索引必须包含一个连接字段，以便建立父子关系。索引映射请求使用以下格式：
PUT /example_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;relationship_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;join&amp;quot;, &amp;quot;relations&amp;quot;: { &amp;quot;parent_doc&amp;quot;: &amp;quot;child_doc&amp;quot; } } } } } 对于这个示例，首先配置一个包含代表产品和其品牌的文档的索引，这些文档如查询示例 has_child 中所述。
要搜索父项的子项，请使用 has_parent 查询。以下查询返回与查询 economy 匹配的品牌生产的子文档（产品）：
GET testindex1/_search { &amp;quot;query&amp;quot; : { &amp;quot;has_parent&amp;quot;: { &amp;quot;parent_type&amp;quot;:&amp;quot;brand&amp;quot;, &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot; : { &amp;quot;name&amp;quot;: &amp;quot;economy&amp;quot; } } } } } 返回由该品牌生产的所有产品：
{ &amp;quot;took&amp;quot;: 11, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 2, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 1, &amp;quot;hits&amp;quot;: [ { &amp;quot;_index&amp;quot;: &amp;quot;testindex1&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;4&amp;quot;, &amp;quot;_score&amp;quot;: 1, &amp;quot;_routing&amp;quot;: &amp;quot;2&amp;quot;, &amp;quot;_source&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;Electronic watch&amp;quot;, &amp;quot;sales_count&amp;quot;: 300, &amp;quot;product_to_brand&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;product&amp;quot;, &amp;quot;parent&amp;quot;: &amp;quot;2&amp;quot; } } }, { &amp;quot;_index&amp;quot;: &amp;quot;testindex1&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;5&amp;quot;, &amp;quot;_score&amp;quot;: 1, &amp;quot;_routing&amp;quot;: &amp;quot;2&amp;quot;, &amp;quot;_source&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;Digital watch&amp;quot;, &amp;quot;sales_count&amp;quot;: 100, &amp;quot;product_to_brand&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;product&amp;quot;, &amp;quot;parent&amp;quot;: &amp;quot;2&amp;quot; } } } ] } } 检索内部命中项 # 返回与查询匹配的父文档，请提供 inner_hits 参数：</description></item><item><title>父级聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/parent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/parent/</guid><description>父级聚合 # parent 父级聚合是一个分组聚合，根据您索引中定义的父子关系创建一个包含父级文档的分组。此聚合使您能够对具有相同匹配子级文档的父级文档执行分析，从而实现强大的层次结构数据分析。
parent 聚合与 join 字段类型一起工作，该字段类型在同一个索引中的文档内建立父子关系。
parent 聚合识别具有匹配子文档的父文档，而 children 聚合识别匹配特定子关系的子文档。这两种聚合都使用子关系名称作为输入。
参数说明 # parent 聚合具有以下参数：
参数 必需/可选 数据类型 描述 type 必填 String join 字段中的子类型名称。 参考样例 # 以下示例构建了一个包含三名员工的小公司数据库。每个员工记录都与一个父部门记录存在 join 子关系。
首先，创建一个 company 索引，其中包含一个 join 字段，该字段将部门（父级）映射到员工（子级）：
PUT /company { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;join_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;join&amp;quot;, &amp;quot;relations&amp;quot;: { &amp;quot;department&amp;quot;: &amp;quot;employee&amp;quot; } }, &amp;quot;department_name&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;employee_name&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;salary&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;double&amp;quot; }, &amp;quot;hire_date&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;date&amp;quot; } } } } 接下来，用三个部门和三个员工填充数据。父子关系在以下表格中展示。</description></item><item><title>百分位排名聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/percentile-ranks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/percentile-ranks/</guid><description>百分位排名聚合 # percentile_ranks 百分比排名聚合估计低于或等于给定阈值的观测值百分比。这对于了解特定值在值分布中的相对位置很有用。 例如，您可以使用百分位排名聚合来学习交易金额 45 与数据集中其他交易值相比如何。百分位排名聚合返回一个值，如 82.3 ，这意味着 82.3% 的交易额低于或等于 45 。
参数说明 # percentile_ranks 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 用于计算百分位数的数值字段。 values 必需 Array of doubles 用于计算百分位数的值。 keyed 可选 Boolean 如果设置为 false ，则将结果作为数组返回。否则将结果作为 JSON 对象返回。默认值为 true 。 tdigest.compression 可选 Double 控制 tdigest 算法的准确性和内存使用。参见使用 tdigest 进行精度调整。 hdr.number_of_significant_value_digits 可选 Integer HDR 直方图的精度设置。参见 HDR 直方图。 missing 可选 Numeric 当文档中目标字段缺失时使用的默认值。 script 可选 Object 用于计算自定义值而不是使用字段的脚本。支持内联和存储脚本。 参考样例 # 首先，创建一个示例索引：</description></item><item><title>百分位数分组聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/percentiles-bucket/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/percentiles-bucket/</guid><description>百分位数分组聚合 # percentiles_bucket 百分位数分组聚合是一个同级聚合，用于计算分位数的位置。
percentiles_bucket 聚合精确计算分位数，不使用近似或插值。每个分位数都返回为目标分位数小于或等于的最近值。
percentiles_bucket 聚合需要将整个值列表临时保存在内存中，即使对于大型数据集也是如此。相比之下， percentiles 指标聚合使用更少的内存，但会近似百分比。
指定的指标必须是数值型，并且同级聚合必须是多分组聚合。
参数说明 # percentiles_bucket聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。 percents 可选 List 一个包含任意数量数值百分比值的列表，这些值将被包含在输出中。有效值为 0.0 到 100.0（含）。默认为 [1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0] 。 keyed 可选 Boolean 是否将输出格式化为字典，而不是键值对对象数组。默认为 true （以键值对格式化输出）。 参考样例 # 以下示例创建一个以一周为间隔的日期直方图。 sum 子聚合为每周汇总 taxful_total_price 。最后， percentiles_bucket 聚合计算这些汇总的每周百分位数值：</description></item><item><title>百分位聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/percentile/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/percentile/</guid><description>百分位聚合 # percentiles 百分位聚合估计数值字段在给定百分位处的值。这对于理解分布边界很有用。
例如， load_time 的 95th 百分位 = 120ms 表示 95% 的值小于或等于 120 毫秒。
与 cardinality 指标类似， percentile 指标也是近似的。
参数说明 # percentiles 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 用于计算百分位数的数值字段。 percents 可选 Array of doubles 返回百分位数列表。默认为 [1, 5, 25, 50, 75, 95, 99] 。 keyed 可选 Boolean 如果设置为 false ，则将结果作为数组返回。否则将结果作为 JSON 对象返回。默认值为 true 。 tdigest.</description></item><item><title>直方图聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/histogram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/histogram/</guid><description>直方图聚合 # histogram 直方图聚合根据指定的间隔对文档进行分组。
使用 histogram 聚合，您可以非常轻松地可视化给定范围内文档中值的分布。
以下示例将 number_of_bytes 字段按 10,000 个间隔进行分组：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;number_of_bytes&amp;quot;: { &amp;quot;histogram&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;bytes&amp;quot;, &amp;quot;interval&amp;quot;: 10000 } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;number_of_bytes&amp;quot; : { &amp;quot;buckets&amp;quot; : [ { &amp;quot;key&amp;quot; : 0.0, &amp;quot;doc_count&amp;quot; : 13372 }, { &amp;quot;key&amp;quot; : 10000.0, &amp;quot;doc_count&amp;quot; : 702 } ] } } 参数说明 # histogram 聚合支持以下参数。
参数 必需/可选 数据类型 描述 interval 必填 Numeric 构造每个分组所使用的字段值宽度。</description></item><item><title>省略符号分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/apostrophe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/apostrophe/</guid><description>省略符号分词过滤器 # 省略符号（apostrophe）分词过滤器的主要功能是移除所有格形式的撇号以及跟在撇号后面的所有内容。这在分析那些大量使用撇号的语言的文本时非常有用，比如土耳其语。在土耳其语中，撇号用于将词根与后缀分隔开来，这些后缀包括所有格后缀、格标记以及其他语法词尾。
参考样例 # 以下示例请求创建了一个名为 custom_text_index 的新索引，在索引设置中配置了一个自定义分词器，并在映射中使用该分词器：
PUT /custom_text_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;apostrophe&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;custom_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /custom_text_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;custom_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;John's car is faster than Peter's bike&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;john&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;car&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;faster&amp;quot;, &amp;quot;start_offset&amp;quot;: 14, &amp;quot;end_offset&amp;quot;: 20, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;than&amp;quot;, &amp;quot;start_offset&amp;quot;: 21, &amp;quot;end_offset&amp;quot;: 25, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 4 }, { &amp;quot;token&amp;quot;: &amp;quot;peter&amp;quot;, &amp;quot;start_offset&amp;quot;: 26, &amp;quot;end_offset&amp;quot;: 33, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 5 }, { &amp;quot;token&amp;quot;: &amp;quot;bike&amp;quot;, &amp;quot;start_offset&amp;quot;: 34, &amp;quot;end_offset&amp;quot;: 38, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 6 } ] } 内置的省略符号分词过滤器并不适用于像法语这样的语言，在法语中撇号会出现在单词的开头。例如，C'est l'amour de l'école 这句话使用该过滤器分词后将会得到四个词元：“C”、“l”、“de” 和 “l”。</description></item><item><title>省略词分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/elision/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/elision/</guid><description>省略词分词过滤器 # 省略词（Elision）分词过滤器用于从某些语言的单词中去除省略的字符。省略现象通常出现在像法语这样的语言中，在这些语言里，单词常常会发生缩合，并与后面的单词结合，常见的方式是省略一个元音字母，并用一个撇号来替代。
省略词分词过滤器已经在以下语言分词器中预先配置好了：加泰罗尼亚语/catalan、法语/french、爱尔兰语/irish和意大利语/italian。
参数说明 # 自定义省略词分词过滤器可使用以下参数进行配置。
参数 必需/可选 数据类型 描述 articles 若未配置 articles_path 则为必需 字符串数组 定义当某些冠词或短词作为省略形式的一部分出现时，哪些应该被移除。 articles_path 若未配置 articles 则为必需 字符串 指定在分析过程中应被移除的自定义冠词列表的路径。 articles_case 可选 布尔值 指定在匹配省略形式时，该过滤器是否区分大小写。默认值为 false。 参考样例 # 法语中默认的省略形式集合包括 l'、m'、t'、qu'、n'、s'、j'、d'、c'、jusqu'、quoiqu'、lorsqu' 和 puisqu'。你可以通过配置 french_elision 分词过滤器来更新这个集合。以下示例请求创建了一个名为 french_texts 的新索引，并配置了一个带有 french_elision 过滤器的分词器：
PUT /french_texts { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;french_elision&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;elision&amp;quot;, &amp;quot;articles&amp;quot;: [ &amp;quot;l&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;m&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;s&amp;quot;, &amp;quot;j&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;french_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;french_elision&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;french_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>矩阵统计聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/matrix-stats/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/matrix-stats/</guid><description>矩阵统计聚合 # matrix_stats 矩阵统计聚合是一个多值指标聚合，以矩阵形式为两个或多个字段生成协方差统计。
matrix_stats 聚合不支持脚本。
参数说明 # matrix_stats 聚合采用以下参数。
参数 必需/可选 数据类型 描述 field 必需 String 用于计算矩阵统计的一组字段。 missing 可选 Object 用于替代缺失值的值。默认情况下，会忽略缺失值。参见缺失值。 mode 可选 String 用作多值或数组字段样本的值。允许的值是 avg 、 min 、 max 、 sum 和 median 。默认是 avg 。 参考样例 # 以下示例返回数据中 taxful_total_price 和 products.base_price 字段的统计信息：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;matrix_stats_taxful_total_price&amp;quot;: { &amp;quot;matrix_stats&amp;quot;: { &amp;quot;fields&amp;quot;: [&amp;quot;taxful_total_price&amp;quot;, &amp;quot;products.</description></item><item><title>移动函数聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/moving-function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/moving-function/</guid><description>移动函数聚合 # moving_fn 移动函数聚合是一个父级管道聚合，它在滑动窗口上执行脚本。滑动窗口在从父级 histogram 或 date histogram 聚合中提取的一系列值上移动。窗口一次向右移动一个分组； moving_fn 每次窗口移动时都会运行脚本。
使用 moving_fn 聚合在滑动窗口内的数据上执行任何数值计算。你可以使用 moving_fn 用于以下目的：
趋势分析 异常值检测 自定义时间序列分析 自定义平滑算法 数字信号处理 (DSP) 参数说明 # moving_fn 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。 script 必需 String 或 Object 为每个数据窗口计算值的脚本。可以是内联脚本、存储脚本或脚本文件。该脚本可以访问在 buckets_path 参数中定义的变量名。 window 必需 Integer 滑动窗口中的分组的数量。必须是正整数。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。 shift 可选 Integer 窗口要移动的分组的数量。可以是正数（向未来的分组右移）或负数（向过去的分组左移）。默认是 0 ，将窗口立即放置在当前分组的左侧。参见移动窗口。 移动函数的工作原理 # moving_fn 聚合操作在有序分组序列上的滑动窗口上。从父聚合中的第一个分组开始， moving_fn 执行以下操作：</description></item><item><title>移动平均聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/moving-avg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/moving-avg/</guid><description>移动平均聚合 # 一个 moving_avg 移动平均聚合是一个父级管道聚合，它计算有序数据集中窗口（相邻子集）内指标的一系列平均值。
要创建一个 moving_avg 聚合，您首先创建一个 histogram 或 date_histogram 聚合。然后，您可以选择在直方图聚合中嵌入一个指标聚合。最后，您在直方图中嵌入 moving_avg 聚合，并将 buckets_path 参数设置为要跟踪的嵌入指标。
窗口的大小是窗口中连续数据值的数量。在每次迭代中，算法计算窗口中所有数据点的平均值，然后向前滑动一个数据值，排除上一个窗口的第一个值，并包含下一个窗口的第一个值。
例如，给定数据 [1, 5, 8, 23, 34, 28, 7, 23, 20, 19] ，一个窗口大小为 5 的移动平均如下：
(1 + 5 + 8 + 23 + 34) / 5 = 14.2 (5 + 8 + 23 + 34 + 28) / 5 = 19.6 (8 + 23 + 34 + 28 + 7) / 5 = 20 .</description></item><item><title>稀有分组聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/rare-terms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/rare-terms/</guid><description>稀有分组聚合 # rare_terms 稀有分组聚合是一个分组聚合，用于识别数据集中的不常见词项。与 terms 聚合（查找最常见的词项）不同， rare_terms 聚合查找出现频率最低的词项。 rare_terms 聚合适用于异常检测、长尾分析和异常报告等应用。
可以使用 terms 通过按升序计数排序（ &amp;ldquo;order&amp;rdquo;: {&amp;ldquo;count&amp;rdquo;: &amp;ldquo;asc&amp;rdquo;} ）来搜索不常见的值。然而，我们强烈不建议这种做法，因为在涉及多个分片时，它可能导致不准确的结果。一个全局上不常见的词项可能不会在每个单个分片上显得不常见，或者可能完全不在某些分片返回的最不常见结果中。相反，一个在某个分片上出现频率较低的词项可能在另一个分片上很常见。在这两种情况下，分片级别的聚合可能会遗漏稀有词项，导致整体结果不正确。我们建议使用 rare_terms 聚合代替 terms 聚合，它专门设计用于更准确地处理这些情况。
近似结果 # 计算 rare_terms 聚合的精确结果需要编译所有分片上的值完整映射，这需要过多的运行时内存。因此， rare_terms 聚合结果被近似处理。
rare_terms 计算中的大多数错误是假阴性或“遗漏”的值，这些值定义了聚合检测测试的灵敏度。 rare_terms 聚合使用 CuckooFilter 算法以实现适当的灵敏度和可接受的内存使用平衡。有关 CuckooFilter 算法的描述，请参阅这篇论文。
控制灵敏度 # rare_terms 聚合算法中的灵敏度误差被衡量为被遗漏的稀有值的比例，或 false negatives/target values 。例如，如果聚合在包含 5,000 个稀有值的数据集中遗漏了 100 个稀有值，灵敏度误差为 100/5000 = 0.02 ，或 2%。
您可以调整 precision 参数在 rare_terms 聚合中来控制灵敏度和内存使用之间的权衡。
这些因素也会影响灵敏度和内存的权衡：
唯一值的总数 数据集中稀有项的比例 以下指南可以帮助你决定使用哪个 precision 值。</description></item><item><title>空格分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/whitespace-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/whitespace-analyzer/</guid><description>空格分词器 # 空格（whitespace）分词器仅基于空白字符（例如，空格和制表符）将文本拆分为词元。比如转换为小写形式或移除停用词这样的转换操作，它都不会应用，因此文本的原始大小写形式会被保留，并且标点符号也会作为词元的一部分包含在内。
参考样例 # 以下命令创建一个名为 my_whitespace_index 并使用空格分词器的索引：
PUT /my_whitespace_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;whitespace&amp;quot; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于空格分词器：
PUT /my_custom_whitespace_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_whitespace_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_whitespace_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>空格词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/whitespace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/whitespace/</guid><description>空格词元生成器 # 空格（whitespace）词元生成器会依据空白字符（如空格、制表符和换行符）对文本进行拆分。它将由空白字符分隔开的每个单词都视为一个词元，并且不会执行诸如转换为小写形式或去除标点符号等额外的规范化操作。
参考样例 # 以下示例请求创建一个名为 my_index 的新索引，并配置一个使用空格词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;whitespace_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;whitespace&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_whitespace_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_whitespace_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_whitespace_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch is fast! Really fast.&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;fast!</description></item><item><title>简单分割匹配词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/simple-pattern-split/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/simple-pattern-split/</guid><description>简单分割匹配词元生成器 # 简单分割匹配（simple_pattern_split）词元生成器使用正则表达式将文本分割成词元。该正则表达式定义了用于确定文本分割位置的模式。文本中任何匹配该模式的部分都会被用作分隔符，分隔符之间的文本则成为一个词元。当你想定义分隔符，并根据某个模式对文本的其余部分进行分词时，可以使用此词元生成器。
该词元生成器仅将输入文本中匹配到的部分（基于正则表达式）用作分隔符或边界，以将文本分割成词条。匹配到的部分不会包含在生成的词元中。例如，如果将词元生成器配置为在点号（.）处分割文本，输入文本为 one.two.three，那么生成的词元就是 one、two 和 three。点号本身不会包含在生成的词条中。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用简单分割匹配词元生成器的分词器。该分词器被配置为在连字符（-）处分割文本：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_pattern_split_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;simple_pattern_split&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;-&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_split_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_pattern_split_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_split_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_split_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;2024-10-09&amp;quot; } 返回内容包含产生的词元</description></item><item><title>简单分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/simple-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/simple-analyzer/</guid><description>简单分词器 # 简单（simple）分词器是一种非常基础的分词器，它会将文本中的非字母字符串拆分成词项，并将这些词项转换为小写形式。与标准分词器不同的是，简单分词器将除字母字符之外的所有内容都视为分隔符，这意味着它不会把数字、标点符号或特殊字符识别为词元的一部分。
参考样例 # 以下命令创建一个名为 my_simple_index 并使用简单分词器的索引：
PUT /my_simple_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;simple&amp;quot; } } } } 配置自定义分词器 # 以下命令配置了一个索引，该索引带有一个自定义分词器，这个自定义分词器等同于添加了 html_strip 字符过滤器的简单分词器：
PUT /my_custom_simple_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;char_filter&amp;quot;: { &amp;quot;html_strip&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;html_strip&amp;quot; } }, &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_lowercase_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_simple_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;char_filter&amp;quot;: [&amp;quot;html_strip&amp;quot;], &amp;quot;tokenizer&amp;quot;: &amp;quot;my_lowercase_tokenizer&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_simple_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>简单字符串查询</title><link>/easysearch/main/docs/references/search/full-text/simple-query-string/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/full-text/simple-query-string/</guid><description>简单字符串查询 # 使用 simple_query_string 类型在查询字符串中直接指定由正则表达式分隔的多个参数。简单查询字符串的语法比查询字符串宽松，因为它会丢弃字符串中的任何无效部分，并且不会因无效语法而返回错误。
此查询使用简单语法根据特殊运算符解析查询字符串，并将字符串拆分为词项。解析后，查询会独立分析每个词项，然后返回匹配的文档。
以下查询对 title 字段执行模糊搜索：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;simple_query_string&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;\&amp;quot;rises wind the\&amp;quot;~4 | *ising~2&amp;quot;, &amp;quot;fields&amp;quot;: [&amp;quot;title&amp;quot;] } } } 简单字符串语法 # 查询字符串由词项和运算符组成。词项是一个单词（例如，在查询 wind rises 中，词项是 wind 和 rises ）。如果多个词项被引号包围，它们被视为一个短语，其中单词按出现的顺序匹配（例如， &amp;ldquo;wind rises&amp;rdquo; ）。 + 、 | 和 - 等运算符指定用于解释查询字符串中文本的布尔逻辑。
操作符 # 简单查询字符串语法支持以下运算符。
操作符 描述 + 作为 AND 操作符。 | 作为 OR 操作符。 * 在词尾使用时，表示前缀查询。 &amp;quot; 将多个词括起来组成短语（例如， &amp;quot;wind rises&amp;quot; ）。 (, ) 为优先级包装子句（例如， wind + (rises | rising) ）。 ~n 在词后面使用时（例如，wnid~3 ），设置 fuzziness 。在短语后面使用时，设置 slop 。 - 否定该词。 所有前面的操作符都是保留字符。要将其作为原始字符而不是操作符引用，用反斜杠转义它们中的任何一个。在发送 JSON 请求时，使用 \\ 转义保留字符（因为反斜杠字符本身也是保留的，你必须用另一个反斜杠转义反斜杠）。</description></item><item><title>简繁转换分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/stconvert-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/stconvert-analyzer/</guid><description>简繁转换分词器 # stconvert-analyzer 简繁体转换分词器，可在索引与查询阶段将 简体中文 与 繁体中文 之间进行双向转换，解决两种文字体系混合检索的问题。
参数说明 # 参数 说明 默认值 convert_type 转换方向，可选：s2t（简 → 繁），t2s（繁 → 简） s2t keep_both 是否同时保留转换前后两种 token false delimiter 当 keep_both=true 时，两种 token 之间的分隔符 , 使用介绍 # 映射创建
PUT /stconvert/ { &amp;#34;settings&amp;#34; : { &amp;#34;analysis&amp;#34; : { &amp;#34;analyzer&amp;#34; : { &amp;#34;tsconvert&amp;#34; : { &amp;#34;tokenizer&amp;#34; : &amp;#34;tsconvert&amp;#34; } }, &amp;#34;tokenizer&amp;#34; : { &amp;#34;tsconvert&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;stconvert&amp;#34;, &amp;#34;delimiter&amp;#34; : &amp;#34;#&amp;#34;, &amp;#34;keep_both&amp;#34; : false, &amp;#34;convert_type&amp;#34; : &amp;#34;t2s&amp;#34; } }, &amp;#34;filter&amp;#34;: { &amp;#34;tsconvert&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;stconvert&amp;#34;, &amp;#34;delimiter&amp;#34; : &amp;#34;#&amp;#34;, &amp;#34;keep_both&amp;#34; : false, &amp;#34;convert_type&amp;#34; : &amp;#34;t2s&amp;#34; } }, &amp;#34;char_filter&amp;#34; : { &amp;#34;tsconvert&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;stconvert&amp;#34;, &amp;#34;convert_type&amp;#34; : &amp;#34;t2s&amp;#34; } } } } } 分词测试</description></item><item><title>类似查询</title><link>/easysearch/main/docs/references/search/specialized/more-like-this/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/specialized/more-like-this/</guid><description>类似查询 # 使用 more_like_this 查询查找与一个或多个给定文档相似的文档。这对于推荐引擎、内容发现以及识别数据集中的相关项目很有用。
more_like_this 查询分析输入文档或文本，并选择最能描述它们的词项。然后，它搜索包含这些重要词项的其他文档。
前提条件 # 在使用 more_like_this 查询之前，请确保您目标字段已索引，且其数据类型为 text 或 keyword 。
如果您在 like 部分引用文档，Easysearch 需要访问其内容。这通常通过 _source 字段完成，该字段默认启用。如果 _source 被禁用，您必须单独存储这些字段，或配置它们以保存 term_vector 数据。
在索引文档时保存 term_vector 信息可以大大加速 more_like_this 查询，因为引擎可以直接检索重要词项，而无需在查询时重新分析字段文本。
示例：无词向量优化 # 使用以下映射创建名为 articles-basic 的索引：
PUT /articles-basic { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; } } } } 添加示例文档：
POST /articles-basic/_bulk { &amp;quot;index&amp;quot;: { &amp;quot;_id&amp;quot;: 1 }} { &amp;quot;title&amp;quot;: &amp;quot;Exploring the Sahara Desert&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Sand dunes and vast landscapes.</description></item><item><title>精确查询与全文检索的对比</title><link>/easysearch/main/docs/references/search/term-vs-full-text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term-vs-full-text/</guid><description>精确查询与全文检索的对比 # 您可以使用精确查询和全文检索来搜索文本，但精确查询通常用于搜索结构化数据，而全文检索则用于全文搜索。精确查询与全文检索的主要区别在于，精确查询搜索文档中的确切指定词项，而全文检索分析查询字符串。下表总结了精确查询与全文检索之间的差异。
精确查询 全文检索 描述 精确查询回答哪些文档匹配查询。 全文检索回答文档与查询匹配的程度。 分词器 搜索词没有被分析。这意味着精确查询会按照你输入的搜索词进行搜索。 搜索词由在索引特定文档字段时使用的相同分词器进行分析。这意味着您的搜索词会经历与文档字段相同的分词过程。 相关性 词级查询仅返回匹配的文档，而不根据相关性分数对它们进行排序。它们仍然计算相关性分数，但这个分数是所有返回文档相同的。 全文检索为每个匹配计算相关性分数，并按相关性分数降序对结果进行排序。 用例 当你需要匹配精确值（如数字、日期或标签）且不需要按相关性排序时，使用词级查询。 使用全文检索来匹配文本字段，并根据大小写和词干变化等因素进行相关性排序。 Easysearch 使用 BM25 排序算法来计算相关性分数。欲了解更多信息，请参阅 Okapi BM25。
我应该使用全文检索还是精确查询？ # 为了说明全文检索和精确查询的区别，考虑以下两个搜索特定文本短语的示例。莎士比亚的全部作品在一个 Easysearch 集群中被索引。
示例：短语搜索 # 在这个示例中，你将在 text_entry 字段中搜索莎士比亚的全部作品中短语“To be, or not to be”。
首先，使用精确查询进行此搜索：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;term&amp;quot;: { &amp;quot;text_entry&amp;quot;: &amp;quot;To be, or not to be&amp;quot; } } } 返回内容不包含任何匹配项，hits 为 0：</description></item><item><title>系统日志</title><link>/easysearch/main/docs/references/management/logs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/logs/</guid><description>系统日志 # Easysearch 日志包含监控群集操作和故障排除问题的重要信息。日志的位置因安装类型而异：
在 Docker 上，Easysearch 将大多数日志写入控制台，并将其余日志存储在 Easysearch/logs/ 中。 tarball 安装也使用 easysearch/logs/ 。 在 RPM 和 Debian 安装上， Easysearch 将日志写入 /var/log/Easysearch/ 。 日志可作为 .log （纯文本）和 .json 文件使用。
应用程序日志 # 对于其应用程序日志，Easysearch 使用 Apache Log4j 2 其内置日志级别（从最低到最高）为 TRACE 、 DEBUG 、 INFO 、 WARN 、 ERROR 和 FATAL 。默认 Easysearch 日志级别为 INFO 。
您可以更改各个 Easysearch 模块的日志级别，而不是更改默认日志级别（ logger.level ）：
PUT /_cluster/settings { &amp;#34;persistent&amp;#34; : { &amp;#34;logger.org.easysearch.index.reindex&amp;#34; : &amp;#34;DEBUG&amp;#34; } } 此示例更改后，Easysearch 在重新索引操作期间会发出更详细的日志：</description></item><item><title>索引分词器</title><link>/easysearch/main/docs/references/text-analysis/analyzers/index-analyzers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/analyzers/index-analyzers/</guid><description>索引分词器 # 索引分词器是在索引创建时指定的，在文档写入时对文本字段进行分词。
写入索引分词器的生效流程 # 为了确定在对文档建立索引时对某个字段使用哪种分词器，Easysearch 会按以下顺序检查以下参数：
字段的分词器映射参数，即analyzer analysis.analyzer.default 索引设置 standard标准分词器（默认分词器） 在指定索引分词器时，请记住，在大多数情况下，为索引中的每个文本字段指定一个分词器效果最佳。使用相同的分词器在文本字段（在建立索引时）和查询字符串（在查询时），可确保搜索所使用的词项与存储在索引中的词项相同。
为字段指定索引分词器 # 在创建索引映射时，可以为每个文本字段提供 analyzer（分词器）参数。例如，以下请求为 text_entry 字段指定了 simple 简单分词器：
PUT testindex { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text_entry&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;simple&amp;quot; } } } } 为索引指定默认索引分词器 # 如果想对索引中的所有文本字段使用相同的分词器，则可以在索引设置中按如下方式进行指定analysis.analyzer.default：
PUT testindex { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;default&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;simple&amp;quot; } } } } } 如果您未指定默认分词器，那么将使用standard标准分词器。</description></item><item><title>索引压缩</title><link>/easysearch/main/docs/references/document/index-compression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/document/index-compression/</guid><description>索引压缩 # 索引编码 # 索引编码决定索引的存储字段如何被压缩和存储在磁盘上。索引编码由静态的 index.codec 设置来控制，该设置指定压缩算法。这个设置会影响索引分片的大小和索引操作的性能。
Easysearch 提供了多种基于索引编码的压缩方案，以降低索引的存储成本。
default – 该编码使用LZ4算法和预设字典，优先考虑性能而非压缩比。与best_compression相比，它提供更快的索引和搜索操作，但可能导致更大的索引/分片大小。如果在索引设置中未提供编码，则默认使用LZ4作为压缩算法。
best_compression – 该编码底层使用zlib算法进行压缩。它能实现高压缩比，从而减小索引大小。然而，这可能会增加索引操作期间的额外CPU使用，并可能随后导致较高的索引和搜索延迟。
从 Easysearch 1.1 开始，增加了基于 Zstandard 压缩算法的新编码方式。这种算法在压缩比和速度之间提供了良好的平衡。
ZSTD 与默认编解码器相比，该编解码器提供了与best_compression编解码器相当的压缩比，CPU使用合理，索引和搜索性能也有所提高。
source 复用 # source_reuse： 启用 source_reuse 配置项能够去除 _source 字段中与 doc_values 或倒排索引重复的部分，从而有效减小索引总体大小，这个功能对日志类索引效果尤其明显。
source_reuse 支持对以下数据类型进行压缩：keyword，integer，long，short，boolean，float，half_float，double，geo_point，ip， 如果是 text 类型，需要默认启用 keyword 类型的 multi-field 映射。 以上类型必须启用 doc_values 映射（默认启用）才能压缩。
使用限制 # 当索引里包含 nested 类型映射，或插件额外提供的数据类型时，不能启用 source_reuse，例如 knn 索引。
压缩效果对比 # Easysearch 压缩效果对比如下
使用 Nginx 日志作为数据样本 就 Elasticsearch 6.4.3 和 Easysearch 1.</description></item><item><title>索引模板</title><link>/easysearch/main/docs/references/management/index-templates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/index-templates/</guid><description>索引模板 # 索引模板允许您使用预定义的映射和设置初始化新索引。例如，如果连续索引日志数据，可以定义一个索引模板，以便所有这些索引都具有相同数量的碎片和副本。
创建模板 # 要创建索引模板，请使用 POST 请求：
POST _index_template 此命令创建一个名为 daily_logs 的模板，并将其应用于名称与正则表达式 logs-2023-01-* 匹配的任何新索引，还将其添加到 my_log 别名中：
PUT _index_template/daily_logs { &amp;#34;index_patterns&amp;#34;: [ &amp;#34;logs-2023-01-*&amp;#34; ], &amp;#34;template&amp;#34;: { &amp;#34;aliases&amp;#34;: { &amp;#34;my_logs&amp;#34;: {} }, &amp;#34;settings&amp;#34;: { &amp;#34;number_of_shards&amp;#34;: 2, &amp;#34;number_of_replicas&amp;#34;: 1 }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;timestamp&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&amp;#34; }, &amp;#34;value&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;double&amp;#34; } } } } } 您应该看到以下响应：
{ &amp;#34;acknowledged&amp;#34;: true } 如果创建名为 logs-2023-01-01 的索引，可以看到它具有模板中的映射和设置：
PUT logs-2023-01-01 GET logs-2023-01-01 { &amp;#34;logs-2023-01-01&amp;#34;: { &amp;#34;aliases&amp;#34;: { &amp;#34;my_logs&amp;#34;: {} }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;timestamp&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&amp;#34; }, &amp;#34;value&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;double&amp;#34; } } }, &amp;#34;settings&amp;#34;: { &amp;#34;index&amp;#34;: { &amp;#34;creation_date&amp;#34;: &amp;#34;1673588860779&amp;#34;, &amp;#34;number_of_shards&amp;#34;: &amp;#34;2&amp;#34;, &amp;#34;number_of_replicas&amp;#34;: &amp;#34;1&amp;#34;, &amp;#34;uuid&amp;#34;: &amp;#34;S1vMSMDHSAuS2IzPcOHpOA&amp;#34;, &amp;#34;version&amp;#34;: { &amp;#34;created&amp;#34;: &amp;#34;7110199&amp;#34; }, &amp;#34;provided_name&amp;#34;: &amp;#34;logs-2023-01-01&amp;#34; } } } } 与此模式匹配的任何其他索引&amp;mdash; logs-2023-01-02 、 logs-2033-01-03等&amp;mdash; 都将继承相同的映射和设置。</description></item><item><title>累积和聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/cumulative-sum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/cumulative-sum/</guid><description>累积和聚合 # cumulative_sum 累积和聚合是一个父聚合，用于计算上一个聚合的存储分组的累积总和。
累积和是给定序列的部分和的序列。例如，序列 {a，b，c,...} 的累积和为 a、a+b、a+b+c 等。您可以使用累积总和来可视化字段随时间的变化率。
参数说明 # cumulative_sum 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合存储分组的路径。请参阅存储分组路径 。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。 参考样例 # 以下示例创建间隔为一个月的日期直方图。sum 子聚合计算每个月所有字节的总和。最后，cumulative_sum 聚合计算每个月存储分组的累积字节数：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;sales_per_month&amp;quot;: { &amp;quot;date_histogram&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;@timestamp&amp;quot;, &amp;quot;calendar_interval&amp;quot;: &amp;quot;month&amp;quot; }, &amp;quot;aggs&amp;quot;: { &amp;quot;no-of-bytes&amp;quot;: { &amp;quot;sum&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;bytes&amp;quot; } }, &amp;quot;cumulative_bytes&amp;quot;: { &amp;quot;cumulative_sum&amp;quot;: { &amp;quot;buckets_path&amp;quot;: &amp;quot;no-of-bytes&amp;quot; } } } } } } 返回内容</description></item><item><title>经典分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/classic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/classic/</guid><description>经典分词过滤器 # 经典（classic）分词过滤器的主要功能是与经典词元生成器配合使用。它通过应用以下常见的转换操作来处理词元，这些操作有助于文本分析和搜索：
移除所有格词尾，例如 “’s” 。比如，“John’s” 会变为 “John”。 从首字母缩略词中移除句点。例如，“D.A.R.P.A.” 会变为 “DARPA”。 参考样例 # 以下示例请求创建了一个名为 custom_classic_filter 的新索引，并配置了一个使用经典分词过滤器的分词器。
PUT /custom_classic_filter { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_classic&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;classic&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;classic&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /custom_classic_filter/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;custom_classic&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;John's co-operate was excellent.&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;John&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;APOSTROPHE&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;co&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 9, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;operate&amp;quot;, &amp;quot;start_offset&amp;quot;: 10, &amp;quot;end_offset&amp;quot;: 17, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;was&amp;quot;, &amp;quot;start_offset&amp;quot;: 18, &amp;quot;end_offset&amp;quot;: 21, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;excellent&amp;quot;, &amp;quot;start_offset&amp;quot;: 22, &amp;quot;end_offset&amp;quot;: 31, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 4 } ] }</description></item><item><title>经典词元生成器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/classic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/classic/</guid><description>经典词元生成器 # 经典（classic）词元生成器会解析文本，并应用英语语法规则将文本拆分为词元。它包含以下特定的逻辑来处理匹配规则：
首字母缩写词 电子邮件地址 域名 某些类型的标点符号 这种词元生成器最适合处理英语文本。对于其他语言，尤其是那些具有不同语法结构的语言，它可能无法产生最佳效果。
经典词元生成器按如下方式解析文本：
标点符号：在大多数标点符号处拆分文本，并移除标点字符。后面不跟空格的点号会被视为词元的一部分。 连字符：在连字符处拆分单词，但当词元中存在数字时除外。当词元中存在数字时，该词元不会被拆分，而是被当作产品编号处理。 电子邮件：识别电子邮件地址和主机名，并将它们作为单个词元保留。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用经典词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_classic_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;classic&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_classic_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_classic_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;For product AB3423, visit X&amp;amp;Y at example.</description></item><item><title>统计分组聚合</title><link>/easysearch/main/docs/references/aggregation/pipeline-aggregations/stats-bucket/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/pipeline-aggregations/stats-bucket/</guid><description>统计分组聚合 # stats_bucket 统计分组聚合是一个同级聚合，它为先前聚合的分组返回各种统计信息（ count 、 min 、 max 、 avg 和 sum ）。
指定的指标必须是数值型，并且同级聚合必须是多分组聚合。
参数说明 # stats_bucket 聚合采用以下参数。
参数 必需/可选 数据类型 描述 buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。 gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。 format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。 参考样例 # 以下示例创建一个以一个月为间隔的日期直方图。 sum 子聚合计算每个月所有字节的总和。最后， stats_bucket 聚合从这些总和中返回 count 、 avg 、 sum 、 min 和 max 统计信息：</description></item><item><title>统计聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/stats/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/stats/</guid><description>统计聚合 # stats 聚合是一个多值指标聚合，用于计算数值数据的汇总。这种聚合有助于快速了解数值字段的分布情况。它可以直接作用于字段，应用脚本来派生值，或处理缺少字段的文档。 stats 聚合返回五个值：
count : 收集到的值的数量 min : 最低值 max : 最高值 sum : 所有值的总和 avg : 值的平均数（总和除以数量） 参数说明 # stats 聚合支持以下可选参数。
参数 必需/可选 数据类型 描述 field 必需 String 要聚合的字段。必须是数值字段。 script 可选 Object 用于计算聚合自定义值的脚本。可单独使用或与 field 一起使用。 missing 可选 Number 用于缺少目标字段的文档的默认值。 参考样例 # 以下示例计算 stats 聚合的电力使用情况。
创建一个名为 power_usage 的索引，并添加包含给定小时内消耗的千瓦时 (kWh) 数量的文档：</description></item><item><title>缺省聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/missing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/missing/</guid><description>缺省聚合 # 如果你的索引中的文档完全不包含聚合字段，或者聚合字段的值为 NULL，请使用 missing 参数指定这些文档应该放入的分组的名称。
以下示例将任何缺失的值添加到名为“N/A”的分组中：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;response_codes&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;response.keyword&amp;quot;, &amp;quot;size&amp;quot;: 10, &amp;quot;missing&amp;quot;: &amp;quot;N/A&amp;quot; } } } } 由于 min_doc_count 参数的默认值为 1， missing 参数在其响应中不会返回任何分组。将 min_doc_count 参数设置为 0 以在响应中查看“N/A”分组：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;response_codes&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;response.keyword&amp;quot;, &amp;quot;size&amp;quot;: 10, &amp;quot;missing&amp;quot;: &amp;quot;N/A&amp;quot;, &amp;quot;min_doc_count&amp;quot;: 0 } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;response_codes&amp;quot; : { &amp;quot;doc_count_error_upper_bound&amp;quot; : 0, &amp;quot;sum_other_doc_count&amp;quot; : 0, &amp;quot;buckets&amp;quot; : [ { &amp;quot;key&amp;quot; : &amp;quot;200&amp;quot;, &amp;quot;doc_count&amp;quot; : 12832 }, { &amp;quot;key&amp;quot; : &amp;quot;404&amp;quot;, &amp;quot;doc_count&amp;quot; : 801 }, { &amp;quot;key&amp;quot; : &amp;quot;503&amp;quot;, &amp;quot;doc_count&amp;quot; : 441 }, { &amp;quot;key&amp;quot; : &amp;quot;N/A&amp;quot;, &amp;quot;doc_count&amp;quot; : 0 } ] } } }</description></item><item><title>脚本分数查询</title><link>/easysearch/main/docs/references/search/specialized/script-score/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/specialized/script-score/</guid><description>脚本分数查询 # 使用 script_score 查询通过脚本自定义分数计算。对于昂贵的评分函数，您可以使用 script_score 查询仅计算已过滤的返回文档的分数。
参考样例 # 例如，以下请求创建一个包含一个文档的索引：
PUT testindex1/_doc/1 { &amp;quot;name&amp;quot;: &amp;quot;John Doe&amp;quot;, &amp;quot;multiplier&amp;quot;: 0.5 } 您可以使用 match 查询返回所有在 name 字段中包含 John 的文档：
GET testindex1/_search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;John&amp;quot; } } } 在返回内容中，文档 1 的得分为 0.2876821 ：
{ &amp;quot;took&amp;quot;: 7, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 1, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 0.</description></item><item><title>脚本指标聚合</title><link>/easysearch/main/docs/references/aggregation/metric-aggregations/scripted-metric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/metric-aggregations/scripted-metric/</guid><description>脚本指标聚合 # scripted_metric 脚本指标聚合是一个多值指标聚合，它返回根据指定脚本计算的指标。脚本有四个阶段， init 、 map 、 combine 和 reduce ，每个聚合按顺序运行这些阶段，组合来自文档的结果。
所有四个脚本共享一个可变对象，称为 state ，该对象由你定义。 state 在 init 、 map 和 combine 阶段时对每个分片是局部的。结果被传递到 states 数组中用于 reduce 阶段。因此，每个分片的 state 在分片在 reduce 步骤中组合之前是独立的。
参数说明 # scripted_metric 聚合采用以下参数。
参数 必需/可选 数据类型 描述 init_script 可选 String 一个脚本，在每个分片上处理任何文档之前执行一次。用于设置初始 state （例如，在 state 对象中初始化计数器或列表）。如果没有提供， state 在每个分片上开始时是一个空对象。 map_script 必需 String 一个脚本，针对聚合收集的每个文档执行。此脚本根据文档的数据更新 state 。例如，您可以检查字段的值，然后递增计数器或在 state 中计算运行总和。 combine_script 必需 String 一个脚本，在每个分片上处理完所有文档后由 map_script 执行一次。此脚本将分片的 state 聚合为单个结果发送回协调节点。此脚本用于完成一个分片的计算（例如，汇总存储在 state 中的计数器或总计）。脚本应返回其分片的汇总值或结构。 reduce_script 必需 String 一个脚本在接收到所有分片的合并结果后，在协调节点上执行一次。这个脚本接收一个特殊变量 states ，它是一个包含每个分片从 combine_script 输出的数组。 reduce_script 遍历状态并生成最终的聚合输出（例如，添加分片总和或合并计数的映射）。 reduce_script 返回的值是在聚合结果中报告的值。 params 可选 Object 除 reduce_script 外，所有脚本都可以访问用户定义的参数。 可返回的类型 # 脚本可以在内部使用任何有效的操作和对象。然而，存储在 state 或从任何脚本返回的数据必须属于允许的类型之一。这个限制存在是因为中间 state 需要在节点之间发送。允许的类型如下：</description></item><item><title>脚本查询</title><link>/easysearch/main/docs/references/search/specialized/script/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/specialized/script/</guid><description>脚本查询 # 使用 script 查询基于 Painless 脚本语言编写的自定义条件来过滤文档。此查询返回脚本评估结果为 true 的文档，从而实现无法使用标准查询表达的高级过滤逻辑。
script 查询计算成本高，应谨慎使用。仅在必要时使用，并确保 search.allow_expensive_queries 已启用（默认为 true ）。有关更多信息，请参阅昂贵查询。
参考样例 # 使用以下映射创建一个名为 products 的索引：
PUT /products { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;price&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;float&amp;quot; }, &amp;quot;rating&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;float&amp;quot; } } } } 使用以下请求索引示例文档：
POST /products/_bulk { &amp;quot;index&amp;quot;: { &amp;quot;_id&amp;quot;: 1 } } { &amp;quot;title&amp;quot;: &amp;quot;Wireless Earbuds&amp;quot;, &amp;quot;price&amp;quot;: 99.99, &amp;quot;rating&amp;quot;: 4.5 } { &amp;quot;index&amp;quot;: { &amp;quot;_id&amp;quot;: 2 } } { &amp;quot;title&amp;quot;: &amp;quot;Bluetooth Speaker&amp;quot;, &amp;quot;price&amp;quot;: 79.</description></item><item><title>范围查询</title><link>/easysearch/main/docs/references/search/term/range/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term/range/</guid><description>范围查询 # 您可以使用 range 范围查询搜索字段中的值范围。
要搜索 line_id 值为 &amp;gt;= 10 和 &amp;lt;= 20 的文档，请使用以下请求：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;range&amp;quot;: { &amp;quot;line_id&amp;quot;: { &amp;quot;gte&amp;quot;: 10, &amp;quot;lte&amp;quot;: 20 } } } } 运算符 # 范围查询中的字段参数接受以下可选运算符参数：
gte：大于或等于 gt：大于 lte：小于或等于 lt：小于 日期字段 # 您可以对包含日期的字段使用范围查询。例如，假设您有一个products索引，并且想要查找 2019 年添加的所有产品：
GET products/_search { &amp;quot;query&amp;quot;: { &amp;quot;range&amp;quot;: { &amp;quot;created&amp;quot;: { &amp;quot;gte&amp;quot;: &amp;quot;2019/01/01&amp;quot;, &amp;quot;lte&amp;quot;: &amp;quot;2019/12/31&amp;quot; } } } } 日期格式 # 要在查询中使用字段映射格式以外的日期格式，请在 format 字段中指定它。</description></item><item><title>范围聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/range/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/range/</guid><description>范围聚合 # range 范围聚合允许你为每个分组定义范围。
例如，你可以找到在 1000 和 2000 之间、2000 和 3000 之间以及 3000 和 4000 之间的字节数。在 range 参数中，你可以将范围定义为数组对象。
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;number_of_bytes_distribution&amp;quot;: { &amp;quot;range&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;bytes&amp;quot;, &amp;quot;ranges&amp;quot;: [ { &amp;quot;from&amp;quot;: 1000, &amp;quot;to&amp;quot;: 2000 }, { &amp;quot;from&amp;quot;: 2000, &amp;quot;to&amp;quot;: 3000 }, { &amp;quot;from&amp;quot;: 3000, &amp;quot;to&amp;quot;: 4000 } ] } } } } 响应包含 from 键值，并排除 to 键值：
... &amp;quot;aggregations&amp;quot; : { &amp;quot;number_of_bytes_distribution&amp;quot; : { &amp;quot;buckets&amp;quot; : [ { &amp;quot;key&amp;quot; : &amp;quot;1000.</description></item><item><title>规范化</title><link>/easysearch/main/docs/references/text-analysis/normalizers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/normalizers/</guid><description>规范化 # 规范化的功能与分词器类似，但它仅输出单个词元。它不包含分词器，并且只能包含特定类型的字符过滤器和词元过滤器。这些过滤器只能执行字符级别的操作，例如字符或模式替换，而不能对整个词元进行操作。这意味着不支持用同义词替换词元或进行词干提取。
规范化在关键字搜索（即基于词项的查询）中很有用，因为它允许你对任何给定的输入运行词元过滤器和字符过滤器。例如，它使得能够将传入的查询 “Naïve” 与索引词项 “naive” 进行匹配。
考虑以下示例：
创建一个带有自定义规范化的新索引：
PUT /sample-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;normalizer&amp;quot;: { &amp;quot;normalized_keyword&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;char_filter&amp;quot;: [], &amp;quot;filter&amp;quot;: [ &amp;quot;asciifolding&amp;quot;, &amp;quot;lowercase&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;approach&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;, &amp;quot;normalizer&amp;quot;: &amp;quot;normalized_keyword&amp;quot; } } } } 索引一个文档
POST /sample-index/_doc/ { &amp;quot;approach&amp;quot;: &amp;quot;naive&amp;quot; } 以下查询与该文档匹配。这是预期的结果：
GET /sample-index/_search { &amp;quot;query&amp;quot;: { &amp;quot;term&amp;quot;: { &amp;quot;approach&amp;quot;: &amp;quot;naive&amp;quot; } } } 但这个查询同样也与该文档匹配：</description></item><item><title>词保留分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/keep-words/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/keep-words/</guid><description>词保留分词过滤器 # 词保留（keep_words）分词过滤器旨在在分析过程中仅保留特定的词。如果你有大量文本，但只对某些关键字或术语感兴趣，那么这个过滤器就会很有用。
参数说明 # 词保留分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 keep_words 若未配置 keep_words_path 则为必需 字符串列表 要保留的词的列表。 keep_words_path 若未配置 keep_words 则为必需 字符串 包含要保留的词列表的文件的路径。 keep_words_case 可选 布尔值 在比较时是否将所有词转换为小写。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有词保留过滤器的分词器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_keep_word&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;keep_words_filter&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;keep_words_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keep&amp;quot;, &amp;quot;keep_words&amp;quot;: [&amp;quot;example&amp;quot;, &amp;quot;world&amp;quot;, &amp;quot;easysearch&amp;quot;], &amp;quot;keep_words_case&amp;quot;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词典复合词分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/dictionary-decompounder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/dictionary-decompounder/</guid><description>词典复合词分词过滤器 # 词典复合词（dictionary_decompounder）分词过滤器用于根据预定义的词典将复合词拆分为其组成部分。该过滤器对于德语、荷兰语或芬兰语等复合词较为常见的语言特别有用，因为将复合词拆分可以提高搜索的相关性。词典复合词分词过滤器会根据已知词列表判断每个词元（单词）是否可以拆分为更小的词元。如果该词元可以拆分为已知单词，过滤器就会为该词元生成子词元。
参数说明 # 词典复合词分词过滤器具有以下参数：
参数 必需/可选 数据类型 描述 word_list 除非配置了 word_list_path，否则为必需 字符串数组 过滤器用于拆分复合词的单词词典。 word_list_path 除非配置了 word_list，否则为必需 字符串 包含词典单词的文本文件的文件路径。可以接受绝对路径或相对于配置(config)目录的相对路径。词典文件必须采用 UTF-8 编码，且每个单词必须单独占一行。 min_word_size 可选 整数 会被考虑进行拆分的整个复合词的最小长度。如果复合词短于该值，则不会进行拆分。默认值为 5。 min_subword_size 可选 整数 任何子词的最小长度。如果子词短于该值，则不会包含在输出中。默认值为 2。 max_subword_size 可选 整数 任何子词的最大长度。如果子词长于该值，则不会包含在输出中。默认值为 15。 only_longest_match 可选 布尔值 如果设置为 true，则仅返回最长匹配的子词。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 decompound_example 的新索引，并配置了一个带有词典复合词过滤器的分词器：</description></item><item><title>词干提取</title><link>/easysearch/main/docs/references/text-analysis/stemming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/stemming/</guid><description>词干提取 # 词干提取是将单词还原为其词根或基本形式（即词干）的过程。这项技术可确保在搜索操作中，单词的不同变体都能匹配到相应结果。例如，单词 “running”（跑步，现在分词形式）、“runner”（跑步者，名词形式）和 “ran”（跑步，过去式）都可以还原为词干 “run”（跑步，原形），这样一来，搜索这些词中的任何一个都能返回相关结果。
在自然语言中，由于动词变位、名词复数变化或词的派生等原因，单词常常以各种形式出现。词干提取在以下方面提升了搜索操作的效果：
提高搜索召回率：通过将不同的单词形式匹配到同一个词干，词干提取增加了检索到的相关文档的数量。 减小索引大小：仅存储单词的词干形式可以减少搜索索引的总体大小。 词干提取是通过在分词器中使用词元过滤器来配置的。一个分词器包含以下组件：
字符过滤器：在分词之前修改字符流。 词元生成器：将文本拆分为词元（通常是单词）。 词元过滤器：在分词之后修改词元，例如，应用词干提取操作。 使用内置词元过滤器进行词干提取的示例 # 要实现词干提取，你可以配置一个内置的词元过滤器，比如 porter_stem 或 kstem 过滤器。
波特词干提取算法（ Porter stemming algorithm）是一种常用于英语的词干提取算法。
创建带有自定义分词器的索引 # 以下示例请求创建了一个名为 my_stemming_index 的新索引，并配置了一个使用 porter_stem 词元过滤器的分词器：
PUT /my_stemming_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_stemmer_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;porter_stem&amp;quot; ] } } } } } 此配置包含以下内容：
标准分词器：根据单词边界将文本拆分为词项。 小写字母过滤器：将所有词元转换为小写形式。 波特词干过滤器（porter_stem 过滤器）：将单词还原为它们的词根形式。 测试分词器 # 为了检验词干提取的效果，使用之前配置好的自定义分词器来分析一段示例文本：</description></item><item><title>词干提取分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/stemmer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/stemmer/</guid><description>词干提取分词过滤器 # 词干提取(stemmer)分词过滤器会将单词缩减为其词根或基本形式（也称为词干stem）。
参数说明 # 词干提取分词过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：
阿拉伯语：arabic 亚美尼亚语：armenian 巴斯克语：basque 孟加拉语：bengali 巴西葡萄牙语：brazilian 保加利亚语：bulgarian 加泰罗尼亚语：catalan 捷克语：czech 丹麦语：danish 荷兰语：dutch、dutch_kp 英语：english（默认）、light_english、lovins、minimal_english、porter2、possessive_english 爱沙尼亚语：estonian 芬兰语：finnish、light_finnish 法语：light_french、french、minimal_french 加利西亚语：galician、minimal_galician（仅复数处理步骤） 德语：light_german、german、german2、minimal_german 希腊语：greek 印地语：hindi 匈牙利语：hungarian、light_hungarian 印尼语：indonesian 爱尔兰语：irish 意大利语：light_italian、italian 库尔德语（索拉尼方言）：sorani 拉脱维亚语：latvian 立陶宛语：lithuanian 挪威语（书面挪威语）：norwegian、light_norwegian、minimal_norwegian 挪威语（新挪威语）：light_nynorsk、minimal_nynorsk 葡萄牙语：light_portuguese、minimal_portuguese、portuguese、portuguese_rslp 罗马尼亚语：romanian 俄语：russian、light_russian 西班牙语：light_spanish、spanish 瑞典语：swedish、light_swedish 土耳其语：turkish 你也可以使用 name 参数作为 language 参数的别名。如果两个参数都被设置，name 参数将被忽略。
参考样例 # 以下示例请求创建了一个名为 my-stemmer-index 的新索引，并配置了一个带有词干提取过滤器的分词器。
PUT /my-stemmer-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_english_stemmer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;english&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_stemmer_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_english_stemmer&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词干覆盖分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/stemmer-override/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/stemmer-override/</guid><description>词干覆盖分词过滤器 # 词干覆盖（stemmer_override）分词过滤器允许你定义自定义的词干提取规则，这些规则可以覆盖像波特（Porter）或雪球（Snowball）等默认词干提取器的行为。当你希望对某些特定的单词应用特殊的词干提取方式，而这些单词可能无法被标准的词干提取算法正确处理时，这个过滤器就会非常有用。
参数说明 # 词干覆盖分词过滤器必须使用以下参数中的一个进行配置。
参数 数据类型 描述 rules 字符串 直接在设置中定义覆盖规则。 rules_path 字符串 指定包含自定义规则（映射）的文件的路径。该路径可以是绝对路径，也可以是相对于配置目录的相对路径。 参考样例 # 以下示例请求创建了一个名为 my-index 的新索引，并配置了一个带有词干覆盖过滤器的分词器。
PUT /my-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_stemmer_override_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer_override&amp;quot;, &amp;quot;rules&amp;quot;: [ &amp;quot;running, runner =&amp;gt; run&amp;quot;, &amp;quot;bought =&amp;gt; buy&amp;quot;, &amp;quot;best =&amp;gt; good&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_stemmer_override_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词片分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/shingle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/shingle/</guid><description>词片分词过滤器 # 词片(shingle)分词过滤器用于从输入文本中生成词 n 元组（即词片）。例如，对于字符串 “slow green turtle”，词片过滤器会创建以下一元词片和二元词片：“slow”、“slow green”、“green”、“green turtle” 以及 “turtle”。
这个分词过滤器常常与其他过滤器结合使用，通过对短语而不是单个词元进行索引来提高搜索的准确性。
参数说明 # 词片分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_shingle_size 可选 整数 要连接的词元的最小数量。默认值为 2。 max_shingle_size 可选 整数 要连接的词元的最大数量。默认值为 2。 output_unigrams 可选 布尔值 是否将一元词（单个词元）包含在输出中。默认值为 true。 output_unigrams_if_no_shingles 可选 布尔值 如果未生成词片，是否输出一元词。默认值为 false。 token_separator 可选 字符串 用于将词元连接成词片的分隔符。默认值为一个空格（ ）。 filler_token 可选 字符串 插入到词元之间的空位置或间隙中的词元。默认值为下划线（_）。 如果 output_unigrams 和 output_unigrams_if_no_shingles 都设置为 true，则 output_unigrams_if_no_shingles 将被忽略。</description></item><item><title>词项集查询</title><link>/easysearch/main/docs/references/search/term/terms-set/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term/terms-set/</guid><description>词项集查询 # 使用 terms_set 词项集查询，您可以在指定字段中搜索匹配一定数量的精确词的文档。与 terms 查询类似，您可以指定返回文档所需的匹配词的最小数量。您可以直接在索引字段中指定这个数量，也可以通过脚本指定。
例如，假设有一个索引，其中包含学生的姓名和他们所选的课程。在设置该索引的映射时，您需要提供一个数值字段，以指定返回文档所需的最小匹配项数量：
PUT students { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;name&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;classes&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;min_required&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;integer&amp;quot; } } } } 接下来，索引两个与学生相关的文档：
PUT students/_doc/1 { &amp;quot;name&amp;quot;: &amp;quot;Mary Major&amp;quot;, &amp;quot;classes&amp;quot;: [ &amp;quot;CS101&amp;quot;, &amp;quot;CS102&amp;quot;, &amp;quot;MATH101&amp;quot; ], &amp;quot;min_required&amp;quot;: 2 } PUT students/_doc/2 { &amp;quot;name&amp;quot;: &amp;quot;John Doe&amp;quot;, &amp;quot;classes&amp;quot;: [ &amp;quot;CS101&amp;quot;, &amp;quot;MATH101&amp;quot;, &amp;quot;ENG101&amp;quot; ], &amp;quot;min_required&amp;quot;: 2 } 现在搜索已经修读了以下至少两门课程的学生： CS101 ， CS102 ， MATH101 ：</description></item><item><title>谓词分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/predicate-token-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/predicate-token-filter/</guid><description>谓词分词过滤器 # 谓词分词过滤器(predicate_token_filter)会根据自定义脚本中定义的条件来评估词元是应该保留还是丢弃。词元的评估是在分析谓词上下文中进行的。此过滤器仅支持内联 Painless 脚本。
参数说明 # 谓词分词过滤器有一个必需参数：script。该参数提供一个条件，用于评估词元是否应被保留。
参考样例 # 以下示例请求创建了一个名为 predicate_index 的新索引，并配置了一个带有谓词分词过滤器的分词器。该过滤器指定仅输出长度超过 7 个字符的词元。
PUT /predicate_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_predicate_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;, &amp;quot;script&amp;quot;: { &amp;quot;source&amp;quot;: &amp;quot;token.term.length() &amp;gt; 7&amp;quot; } } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;predicate_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_predicate_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /predicate_index/_analyze { &amp;quot;text&amp;quot;: &amp;quot;The Easysearch community is growing rapidly&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;predicate_analyzer&amp;quot; } 返回内容包含产生的词元</description></item><item><title>距离特征查询</title><link>/easysearch/main/docs/references/search/specialized/distance-feature/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/specialized/distance-feature/</guid><description>距离特征查询 # 使用 distance_feature 查询来提升与特定日期或地理位置更近的文档的相关性。这可以帮助你在搜索结果中优先显示更近期的或附近的内容。例如，你可以为近期生产的产品分配更高的权重，或提升最接近用户指定位置的项目。
你可以将此查询应用于包含日期或位置数据的字段。它通常用于 bool 查询的 should 子句中，以改进相关性评分而不过滤掉结果。
配置索引 # 在使用 distance_feature 查询之前，请确保您的索引至少包含以下字段类型之一：date,date_nanos,geo_point
在此示例中，您将配置 opening_date 和 coordinates 字段，用于运行距离特征查询：
PUT /stores { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;opening_date&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;date&amp;quot; }, &amp;quot;coordinates&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;geo_point&amp;quot; } } } } 向索引中添加示例文档：
PUT /stores/_doc/1 { &amp;quot;store_name&amp;quot;: &amp;quot;Green Market&amp;quot;, &amp;quot;opening_date&amp;quot;: &amp;quot;2025-03-10&amp;quot;, &amp;quot;coordinates&amp;quot;: [74.00, 40.70] } PUT /stores/_doc/2 { &amp;quot;store_name&amp;quot;: &amp;quot;Fresh Foods&amp;quot;, &amp;quot;opening_date&amp;quot;: &amp;quot;2025-04-01&amp;quot;, &amp;quot;coordinates&amp;quot;: [73.98, 40.75] } PUT /stores/_doc/3 { &amp;quot;store_name&amp;quot;: &amp;quot;City Organics&amp;quot;, &amp;quot;opening_date&amp;quot;: &amp;quot;2021-04-20&amp;quot;, &amp;quot;coordinates&amp;quot;: [74.</description></item><item><title>路径词元分词器</title><link>/easysearch/main/docs/references/text-analysis/tokenizers/path-hierarchy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/tokenizers/path-hierarchy/</guid><description>路径词元分词器 # 路径（path_hierarchy）词元分词器可以把文件路径（或类似的层次结构）的文本按照各个层级分解为词元。当处理诸如文件路径、网址或其他有分隔符的层次结构数据时，这个词元生成器特别有用。
参考样例 # 以下示例请求创建一个名为 my_index 的新索引，并配置一个使用路径词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_path_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;path_hierarchy&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_path_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_path_tokenizer&amp;quot; } } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_path_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;/users/john/documents/report.txt&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;/users&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;/users/john&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 11, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;/users/john/documents&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 21, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;/users/john/documents/report.</description></item><item><title>边缘 n 元分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/edge-n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/edge-n-gram/</guid><description>边缘 n 元分词过滤器 # 边缘 n 元（edge_ngram）分词过滤器与 n 元（ngram）分词过滤器非常相似，二者都会将特定字符串拆分成不同长度的子字符串。不过，边缘 n 元分词过滤器仅从词元的开头（边缘）生成 n 元子字符串。在自动补全或前缀匹配等场景中，它特别有用，因为在这些场景里，你希望在用户输入单词或短语时就能匹配词项的开头部分。
参数说明 # 边缘 n 元分词过滤器可使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_gram 可选 整数 要生成的 n 元词项的最小长度。默认值为1。 max_gram 可选 整数 要生成的 n 元词项的最大长度。对于边缘 n 元分词过滤器，默认值为1；对于自定义分词过滤器，默认值为2。避免将此参数设置为较低的值。如果该值设置得过低，将只会生成非常短的 n 元词项，并且可能找不到搜索词。例如，将max_gram设置为 3，并且对单词“banana”建立索引，那么生成的最长词元将是“ban”。如果用户搜索“banana”，将不会返回任何匹配结果。你可以使用截断truncate分词过滤器作为搜索分析器来降低这种风险。 preserve_original 可选 布尔值 将原始词元包含在输出中。默认值为false。 参考样例 # 以下示例请求创建了一个名为 edge_ngram_example 的新索引，并配置了一个带有边缘 n 元过滤器的分词器：
PUT /edge_ngram_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_edge_ngram&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 3, &amp;quot;max_gram&amp;quot;: 4 } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;my_edge_ngram&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>过滤聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/filter/</guid><description>过滤聚合 # 一个 filter 过滤聚合是一个查询子句，就像一个搜索查询一样 — match 或 term 或 range 。您可以使用 filter 聚合在创建分组之前将整个文档集缩小到特定的文档集。
以下示例展示了 avg 聚合在过滤上下文中运行的情况。 avg 聚合仅聚合与 range 查询匹配的文档：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;low_value&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;range&amp;quot;: { &amp;quot;taxful_total_price&amp;quot;: { &amp;quot;lte&amp;quot;: 50 } } }, &amp;quot;aggs&amp;quot;: { &amp;quot;avg_amount&amp;quot;: { &amp;quot;avg&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;taxful_total_price&amp;quot; } } } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;low_value&amp;quot; : { &amp;quot;doc_count&amp;quot; : 1633, &amp;quot;avg_amount&amp;quot; : { &amp;quot;value&amp;quot; : 38.</description></item><item><title>通配符查询</title><link>/easysearch/main/docs/references/search/term/wildcard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/term/wildcard/</guid><description>通配符查询 # 使用 wildcard 通配符查询来搜索匹配通配符模式的词项。通配符查询支持以下操作符。
操作符 描述 * 匹配零个或多个字符。 ? 匹配任意单个字符。 case_insensitive 若 true 为真，则通配符查询不区分大小写；若 false 为真，则通配符查询区分大小写。默认情况下 false 为真（区分大小写）。 若进行区分大小写的搜索，查找以 H 开头且以 Y 结尾的词，可使用以下请求：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;wildcard&amp;quot;: { &amp;quot;speaker&amp;quot;: { &amp;quot;value&amp;quot;: &amp;quot;H*Y&amp;quot;, &amp;quot;case_insensitive&amp;quot;: false } } } } 如果你将 * 更改为 ? ，则不会有任何匹配，因为 ? 引用的是一个单一字符。
通配符查询通常速度较慢，因为它们需要遍历大量的词项。避免在查询的开头使用通配符字符，因为这在资源和时间方面都可能是一项非常昂贵的操作。
通配符(wildcard)字段类型构建了一个特别为通配符和正则表达式查询设计的索引。
参数说明 # 查询接受字段名称（ &amp;lt;field&amp;gt; ）作为顶级参数：</description></item><item><title>邻接矩阵聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/adjacency-matrix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/adjacency-matrix/</guid><description>邻接矩阵聚合 # adjacency_matrix 邻接矩阵聚合允许你定义过滤表达式，并返回一个交集矩阵，矩阵中的每个非空单元格代表一个分组。你可以找到落入任何过滤器组合中的文档数量。
使用 adjacency_matrix 聚合通过将数据可视化为图形来发现概念之间的关联。
例如，下面查询可以分析不同制造公司之间的关联关系：
GET sample_data_ecommerce/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;interactions&amp;quot;: { &amp;quot;adjacency_matrix&amp;quot;: { &amp;quot;filters&amp;quot;: { &amp;quot;grpA&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;manufacturer.keyword&amp;quot;: &amp;quot;Low Tide Media&amp;quot; } }, &amp;quot;grpB&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;manufacturer.keyword&amp;quot;: &amp;quot;Elitelligence&amp;quot; } }, &amp;quot;grpC&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;manufacturer.keyword&amp;quot;: &amp;quot;Oceanavigations&amp;quot; } } } } } } } 返回内容
{ ... &amp;quot;aggregations&amp;quot; : { &amp;quot;interactions&amp;quot; : { &amp;quot;buckets&amp;quot; : [ { &amp;quot;key&amp;quot; : &amp;quot;grpA&amp;quot;, &amp;quot;doc_count&amp;quot; : 1553 }, { &amp;quot;key&amp;quot; : &amp;quot;grpA&amp;amp;grpB&amp;quot;, &amp;quot;doc_count&amp;quot; : 590 }, { &amp;quot;key&amp;quot; : &amp;quot;grpA&amp;amp;grpC&amp;quot;, &amp;quot;doc_count&amp;quot; : 329 }, { &amp;quot;key&amp;quot; : &amp;quot;grpB&amp;quot;, &amp;quot;doc_count&amp;quot; : 1370 }, { &amp;quot;key&amp;quot; : &amp;quot;grpB&amp;amp;grpC&amp;quot;, &amp;quot;doc_count&amp;quot; : 299 }, { &amp;quot;key&amp;quot; : &amp;quot;grpC&amp;quot;, &amp;quot;doc_count&amp;quot; : 1218 } ] } } } 让我们更仔细地查看结果</description></item><item><title>采样聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/sampler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/sampler/</guid><description>采样聚合 # 如果你正在聚合大量文档，可以使用 sampler 聚合将范围缩小到一小部分文档，从而获得更快的响应。 sampler 聚合通过选择得分最高的文档来选取样本。
结果是大致的，但能很好地反映真实数据的分布。 sampler 聚合显著提高了查询性能，但估计的响应并不完全可靠。
基本语法是：
“aggs”: { &amp;quot;SAMPLE&amp;quot;: { &amp;quot;sampler&amp;quot;: { &amp;quot;shard_size&amp;quot;: 100 }, &amp;quot;aggs&amp;quot;: {...} } } 分片大小属性 # shard_size 属性告诉 Easysearch 每个分片最多收集多少文档。
以下示例将每个分片上收集的文档数量限制为 1,000，然后使用 terms 聚合对文档进行分组：
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;aggs&amp;quot;: { &amp;quot;sample&amp;quot;: { &amp;quot;sampler&amp;quot;: { &amp;quot;shard_size&amp;quot;: 1000 }, &amp;quot;aggs&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;agent.keyword&amp;quot; } } } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;sample&amp;quot; : { &amp;quot;doc_count&amp;quot; : 1000, &amp;quot;terms&amp;quot; : { &amp;quot;doc_count_error_upper_bound&amp;quot; : 0, &amp;quot;sum_other_doc_count&amp;quot; : 0, &amp;quot;buckets&amp;quot; : [ { &amp;quot;key&amp;quot; : &amp;quot;Mozilla/5.</description></item><item><title>重写参数</title><link>/easysearch/main/docs/references/search/rewrite-parameter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/rewrite-parameter/</guid><description>重写参数 # 像 wildcard 、 prefix 、 regexp 、 fuzzy 和 range 这样的多词查询在内部会重组成一组词。 rewrite 参数允许你控制这些词重写的执行和评分。
当多词查询扩展成很多词（例如 prefix: &amp;quot;error*&amp;quot; 匹配数百个词）时，它们在内部会转换成 term 查询。这个过程可能会有以下缺点：
超出 indices.query.bool.max_clause_count 限制（默认是 1024 ）。 影响匹配文档的评分计算方式。 根据所使用的重写方法，影响内存和延迟。 rewrite 参数让你能够控制多词查询的内部行为。
模式 评分规则 性能 注释 constant_score 所有匹配具有相同分数 最佳 默认模式，适合过滤器 scoring_boolean 基于 TF/IDF 中等 完整相关性评分 constant_score_boolean 相同分数但使用布尔结构 中等 与 must_not 或 minimum_should_match 一起使用 top_terms_N 在顶部 N 个词上使用 TF/IDF 高效 截断扩展 top_terms_boost_N 静态提升 快速 较低准确度 top_terms_blended_freqs_N 混合评分 平衡 最佳评分/效率权衡 可用的重写方法 # 下表总结了可用的重写方法。</description></item><item><title>重点分组聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/significant-terms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/significant-terms/</guid><description>重点分组聚合 # significant_terms 聚合功能可以帮助你在相对于索引中其他数据的过滤子集中识别不寻常或有趣的分组出现情况。
前景集是指你进行过滤的文档集合，背景集是指索引中所有文档的集合。 significant_terms 聚合会检查前景集中的所有文档，并与背景集中的文档进行对比，从而为重要出现情况找到相应的分数。
在示例网络日志数据中，每个文档都有一个包含访客 user-agent 的字段。此示例搜索来自 iOS 操作系统的所有请求。对这一前景集进行常规的 terms 聚合返回 Firefox，因为它在这个分组内有最多的文档数量。另一方面， significant_terms 聚合返回 Internet Explorer（IE），因为 IE 在前景集中的出现频率显著高于背景集。
GET sample_data_logs/_search { &amp;quot;size&amp;quot;: 0, &amp;quot;query&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;machine.os.keyword&amp;quot;: [ &amp;quot;ios&amp;quot; ] } }, &amp;quot;aggs&amp;quot;: { &amp;quot;significant_response_codes&amp;quot;: { &amp;quot;significant_terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;agent.keyword&amp;quot; } } } } 返回内容
... &amp;quot;aggregations&amp;quot; : { &amp;quot;significant_response_codes&amp;quot; : { &amp;quot;doc_count&amp;quot; : 2737, &amp;quot;bg_count&amp;quot; : 14074, &amp;quot;buckets&amp;quot; : [ { &amp;quot;key&amp;quot; : &amp;quot;Mozilla/4.0 (compatible; MSIE 6.</description></item><item><title>重点文本聚合</title><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/significant-text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/significant-text/</guid><description>重点文本聚合 # significant_text 聚合与 significant_terms 聚合类似，但它适用于原始文本字段。重要文本通过统计分析测量前景集和背景集之间流行度的变化。例如，当你搜索其股票缩写 TSLA 时，它可能会建议 Tesla。
significant_text 聚合会动态重新分析源文本，过滤掉重复段落、模板化的页眉和页脚等噪声数据，这些数据可能会扭曲结果。
重新分析高基数数据集可能是一项非常耗费 CPU 的操作。我们建议在采样聚合中使用 significant_text 聚合来将分析限制在少量最匹配文档中，例如 200。
您可以设置以下参数：
min_doc_count - 返回匹配超过配置数量顶部命中结果。我们不建议将 min_doc_count 设置为 1，因为它倾向于返回拼写错误或错别字。找到一个以上的词项实例有助于加强显著性不是偶然事件的结果。默认值 3 用于提供最小证据权重。 shard_size - 设置高值会增加稳定性（和准确性），但会牺牲计算性能。 shard_min_doc_count - 如果你的文本包含许多低频词，而你又不关心这些词（例如拼写错误），那么你可以将 shard_min_doc_count 参数设置为在分片级别上过滤候选词，以合理地确保即使合并本地显著文本频率也不会达到所需的 min_doc_count 。默认值为 1，直到你显式设置它之前没有影响。我们建议将此值设置得远低于 min_doc_count 值。 假设你在一个 Easysearch 集群中索引了莎士比亚的全部作品。你可以在 text_entry 字段中找到与“breathe”相关的显著文本：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;text_entry&amp;quot;: &amp;quot;breathe&amp;quot; } }, &amp;quot;aggregations&amp;quot;: { &amp;quot;my_sample&amp;quot;: { &amp;quot;sampler&amp;quot;: { &amp;quot;shard_size&amp;quot;: 100 }, &amp;quot;aggregations&amp;quot;: { &amp;quot;keywords&amp;quot;: { &amp;quot;significant_text&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;text_entry&amp;quot;, &amp;quot;min_doc_count&amp;quot;: 4 } } } } } } 返回内容</description></item><item><title>长度分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/length/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/length/</guid><description>长度分词过滤器 # 长度(length)分词过滤器用于从词元流中移除那些不符合指定长度标准（最小和最大长度值）的词元。
参数说明 # 长度分词过滤器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 min 可选 整数 词元的最小长度。默认值为 0。 max 可选 整数 词元的最大长度。默认值为整数类型的最大值（2147483647）。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有长度过滤器的分词器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;only_keep_4_to_10_characters&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;length_4_to_10&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;length_4_to_10&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;length&amp;quot;, &amp;quot;min&amp;quot;: 4, &amp;quot;max&amp;quot;: 10 } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>限制分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/limit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/limit/</guid><description>限制分词过滤器 # 限制（limit）分词过滤器用于限制分词链通过的词元数量。
参数说明 # 限制分词过滤器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 max_token_count 可选 整数 要生成的词元的最大数量。默认值为 1。 consume_all_tokens 可选 布尔值 （专家级设置）即使结果超过 max_token_count，也会使用来自词元生成器的所有词元。当设置此参数时，输出仍然只包含 max_token_count 指定数量的词元。不过，词元生成器生成的所有词元都会被处理。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有限制过滤器的分析器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;three_token_limit&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;custom_token_limit&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;custom_token_limit&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;limit&amp;quot;, &amp;quot;max_token_count&amp;quot;: 3 } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>雪球算法分词过滤器</title><link>/easysearch/main/docs/references/text-analysis/token-filters/snowball/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/text-analysis/token-filters/snowball/</guid><description>雪球算法分词过滤器 # 雪球算法（snowball）分词过滤器是一种基于 雪球算法的词干提取过滤器。它支持多种语言，并且比波特词干提取算法更加高效和准确。
参数说明 # 雪球分词过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：
阿拉伯语（Arabic） 亚美尼亚语（Armenian） 巴斯克语（Basque） 加泰罗尼亚语（Catalan） 丹麦语（Danish） 荷兰语（Dutch） 英语（English，默认值） 爱沙尼亚语（Estonian） 芬兰语（Finnish） 法语（French） 德语（German） 德语 2（German2） 匈牙利语（Hungarian） 意大利语（Italian） 爱尔兰语（Irish） Kp 立陶宛语（Lithuanian） 洛文斯（Lovins） 挪威语（Norwegian） 波特（Porter） 葡萄牙语（Portuguese） 罗马尼亚语（Romanian） 俄语（Russian） 西班牙语（Spanish） 瑞典语（Swedish） 土耳其语（Turkish） 参考样例 # 以下示例请求创建了一个名为 my-snowball-index 的新索引，并配置了一个带有雪球过滤器的分词器。
PUT /my-snowball-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_snowball_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;snowball&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;English&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_snowball_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_snowball_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>搜索请求文本向量化</title><link>/easysearch/main/docs/references/ai-integration/search-text-embedding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/ai-integration/search-text-embedding/</guid><description>搜索请求文本向量化 # Easysearch 使用搜索管道的 semantic_query_enricher 处理器，协助 semantic query，将文本转为向量。
先决条件 # 服务兼容性 需满足以下任一条件：
支持与 OpenAI API 兼容的 embedding 接口 支持 Ollama embedding 接口 插件要求 必须安装 Easysearch 的以下插件：
knn ai 数据准备 需预先完成：
创建向量索引 写入向量数据 参考 写入数据文本向量化 创建或更新 semantic_query_enricher 处理器 # PUT /_search/pipeline/default_model_pipeline { &amp;quot;rewrite_processors&amp;quot;: [ { &amp;quot;semantic_query_enricher&amp;quot; : { &amp;quot;tag&amp;quot;: &amp;quot;tag1&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Sets the default embedding model&amp;quot;, &amp;quot;url&amp;quot;: &amp;quot;https://api.</description></item><item><title>聚合查询</title><link>/easysearch/main/docs/references/sql/aggregations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/sql/aggregations/</guid><description>聚合查询 # 介绍 # 聚合函数作用于一组值。它们通常与GROUP BY子句一起使用，将值分组为子集。
GROUP BY 子句 # GROUP BY 表达式可以是：
标识符：Identifier 序数：Ordinal 表达式：Expression 标识符 # group by 表达式可以是标识符：
os&amp;gt; SELECT gender, sum(age) FROM accounts GROUP BY gender; fetched rows / total rows = 2/2 +----------+------------+ | gender | sum(age) | |----------+------------| | F | 28 | | M | 101 | +----------+------------+ 序数 # group by 表达式可以是序数：
os&amp;gt; SELECT gender, sum(age) FROM accounts GROUP BY 1; fetched rows / total rows = 2/2 +----------+------------+ | gender | sum(age) | |----------+------------| | F | 28 | | M | 101 | +----------+------------+ group by 表达式可以是一个表达式。</description></item><item><title>身份模拟</title><link>/easysearch/main/docs/references/security/access-control/run-as/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/access-control/run-as/</guid><description>身份模拟 # 用户模拟允许具备特定权限的用户以另外的身份来进行集群的访问。
用户模拟可用于测试和故障排除，或允许系统服务安全地充当用户。
在 REST 接口或 TCP 传输层上都可以进行用户模拟。
REST 接口 # 要允许一个用户模拟另一个用户，请将以下内容添加到 easysearch.yml :
security.authcz.rest_impersonation_user: &amp;lt;AUTHENTICATED_USER&amp;gt;: - &amp;lt;IMPERSONATED_USER_1&amp;gt; - &amp;lt;IMPERSONATED_USER_2&amp;gt; 模拟用户字段支持通配符。将其设置为 * 允许 AUTHENTICATED_USER 来模拟任意用户。
传输层配置 # 类似的配置方法如下：
security.authcz.impersonation_dn: &amp;#34;CN=spock,OU=client,O=client,L=Test,C=DE&amp;#34;: - worf 模拟其他用户 # 要模拟其他用户，请向系统提交请求，并将 HTTP 标头 security_run_as 设置为要模拟的用户的名称。例如：
curl -XGET -u &amp;#39;admin:xxxxxxxxxxxx&amp;#39; -k -H &amp;#34;security_run_as: user_1&amp;#34; https://localhost:9200/_security/authinfo?pretty</description></item><item><title>重建数据</title><link>/easysearch/main/docs/references/management/reindex-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/reindex-data/</guid><description>重新索引数据 # 创建索引后，如果您需要进行广泛的更改，例如为每个文档添加一个新字段或合并多个索引以形成一个新的索引，而不是删除索引，使更改脱机，然后重新索引数据，则可以使用 reindex 操作。
使用 reindex 操作，可以将通过查询选择的所有文档或文档子集复制到另一个索引。重新索引是一个 POST 操作。在最基本的形式中，指定源索引和目标索引。
重新编制索引可能是一项昂贵的操作，具体取决于源索引的大小。我们建议您通过将 number_of_replicas 设置为 0 来禁用目标索引中的副本，并在重新索引过程完成后重新启用它们。
重新索引所有文档 # 您可以将所有文档从一个索引复制到另一个索引。
首先需要使用所需的字段映射和设置创建目标索引，或者可以从源索引中复制这些映射和设置：
PUT destination { &amp;#34;mappings&amp;#34;:{ &amp;#34;Add in your desired mappings&amp;#34; }, &amp;#34;settings&amp;#34;:{ &amp;#34;Add in your desired settings&amp;#34; } } reindex 命令将所有文档从源索引复制到目标索引：
POST _reindex { &amp;#34;source&amp;#34;:{ &amp;#34;index&amp;#34;:&amp;#34;source&amp;#34; }, &amp;#34;dest&amp;#34;:{ &amp;#34;index&amp;#34;:&amp;#34;destination&amp;#34; } } 如果尚未创建目标索引，则 reindex 操作将使用默认配置创建新的目标索引。
从远程群集 reindex # 您可以从远程集群中的索引复制文档。使用 remote 选项指定远程主机名和所需的登录凭据。
此命令会到达远程集群，使用用户名和密码登录，并将所有文档从该远程集群中的源索引复制到本地集群中的目标索引：
POST _reindex { &amp;#34;source&amp;#34;:{ &amp;#34;remote&amp;#34;:{ &amp;#34;host&amp;#34;:&amp;#34;https://&amp;lt;REST_endpoint_of_remote_cluster&amp;gt;:9200&amp;#34;, &amp;#34;username&amp;#34;:&amp;#34;YOUR_USERNAME&amp;#34;, &amp;#34;password&amp;#34;:&amp;#34;YOUR_PASSWORD&amp;#34; } }, &amp;#34;dest&amp;#34;:{ &amp;#34;index&amp;#34;:&amp;#34;destination&amp;#34; } } 您可以指定以下选项：</description></item><item><title>全文搜索</title><link>/easysearch/main/docs/references/sql/fulltext/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/sql/fulltext/</guid><description>全文搜索 # 全文搜索是对存储的单个文档进行搜索，这与基于数据库中的原始文本的常规搜索有所区别。全文搜索尝试通过检查每个文档中的所有单词来匹配搜索条件。 在 Easysearch 中，提供的全文查询使你能够搜索在索引过程中分析的文本字段。
Match Query # 在 Easysearch 中，Match 查询是执行全文搜索的标准查询。MATCHQUERY 和 MATCH_QUERY 都是用于执行匹配查询的函数。
示例 1 # 这两个函数都可以接受字段名称作为第一个参数，文本作为第二个参数。
SQL query:
POST /_sql { &amp;quot;query&amp;quot; : &amp;quot;&amp;quot;&amp;quot; SELECT account_number, address FROM accounts WHERE MATCH_QUERY(address, 'Holmes') &amp;quot;&amp;quot;&amp;quot; } 解释：
{ &amp;quot;from&amp;quot; : 0, &amp;quot;size&amp;quot; : 200, &amp;quot;query&amp;quot; : { &amp;quot;bool&amp;quot; : { &amp;quot;filter&amp;quot; : [ { &amp;quot;bool&amp;quot; : { &amp;quot;must&amp;quot; : [ { &amp;quot;match&amp;quot; : { &amp;quot;address&amp;quot; : { &amp;quot;query&amp;quot; : &amp;quot;Holmes&amp;quot;, &amp;quot;operator&amp;quot; : &amp;quot;OR&amp;quot;, &amp;quot;prefix_length&amp;quot; : 0, &amp;quot;max_expansions&amp;quot; : 50, &amp;quot;fuzzy_transpositions&amp;quot; : true, &amp;quot;lenient&amp;quot; : false, &amp;quot;zero_terms_query&amp;quot; : &amp;quot;NONE&amp;quot;, &amp;quot;auto_generate_synonyms_phrase_query&amp;quot; : true, &amp;quot;boost&amp;quot; : 1.</description></item><item><title>文本向量化</title><link>/easysearch/main/docs/references/ai-integration/text-embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/ai-integration/text-embeddings/</guid><description>[已废弃] 文本向量化 # 本文档描述的功能已不再支持，将在下个版本删除，请使用新的 写入数据文本向量化替代。
本文档介绍如何在 Easysearch 中集成和使用预先部署的 Ollama 服务来生成文本嵌入向量。
先决条件 # 需要预先部署好 Ollama 服务，现阶段集成的服务版本是 0.5.4。
可以用以下命令测试服务是否正常：
curl http://localhost:11434/api/embed -d '{ &amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text:latest&amp;quot;, &amp;quot;input&amp;quot;: &amp;quot;Why is the sky blue?&amp;quot; }' 配置 Ollama 服务 # 可以通过 ollama_url 配置项指定 Ollama 服务的地址。您可以通过以下 API 查看当前配置：
GET _cluster/settings?flat_settings=true&amp;amp;include_defaults=true&amp;amp;filter_path=*.ollama_url 如果没有修改，会输出默认值：
{ &amp;quot;defaults&amp;quot;: { &amp;quot;ollama_url&amp;quot;: &amp;quot;http://localhost:11434&amp;quot; } } REST API # POST /_ai/embed { &amp;quot;model&amp;quot;: &amp;quot;模型名称&amp;quot;, &amp;quot;input&amp;quot;: &amp;quot;文本内容&amp;quot; } 请求示例 # POST /_ai/embed { &amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text:latest&amp;quot;, &amp;quot;input&amp;quot;: &amp;quot;Llamas are members of the camelid family&amp;quot; } 批量生成 Embeddings # 可以一次为多个文本生成嵌入向量。</description></item><item><title>跨集群搜索</title><link>/easysearch/main/docs/references/security/access-control/cross-cluster-search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/access-control/cross-cluster-search/</guid><description>跨集群搜索 # 跨集群搜索正是它听起来的样子：它允许集群中的任何节点对其他集群执行搜索请求。Easysearch 支持开箱即用的跨集群搜索。
身份验证流程 # 当跨集群搜索通过 协调集群 访问 远程集群 时：
安全模块对协调集群上的用户进行身份验证。 安全模块在协调集群上获取用户的后端角色。 请求调用（包括经过身份验证的用户）将转发到远程集群。 在远程群集上评估用户的权限。 远程群集和协调集群可以分别配置不同的身份验证和授权配置，但我们建议在两者上使用相同的设置。
权限信息 # 要查询远程集群上的索引，除了 READ 或 SEARCH 权限外，用户还需要具有以下索引权限：
indices:admin/shards/search_shards role.yml 样例配置 # humanresources: cluster: - CLUSTER_COMPOSITE_OPS_RO indices: &amp;#34;humanresources&amp;#34;: &amp;#34;*&amp;#34;: - READ - indices:admin/shards/search_shards # needed for CCS 配置流程 # 分别启动两个集群，如下：
➜ curl -k &amp;#39;https://localhost:9200/_cluster/health?pretty&amp;#39; -u admin:xxxxxxxxxxxx { &amp;#34;cluster_name&amp;#34; : &amp;#34;easysearch&amp;#34;, &amp;#34;status&amp;#34; : &amp;#34;green&amp;#34;, &amp;#34;timed_out&amp;#34; : false, &amp;#34;number_of_nodes&amp;#34; : 1, &amp;#34;number_of_data_nodes&amp;#34; : 1, &amp;#34;active_primary_shards&amp;#34; : 1, &amp;#34;active_shards&amp;#34; : 1, &amp;#34;relocating_shards&amp;#34; : 0, &amp;#34;initializing_shards&amp;#34; : 0, &amp;#34;unassigned_shards&amp;#34; : 0, &amp;#34;delayed_unassigned_shards&amp;#34; : 0, &amp;#34;number_of_pending_tasks&amp;#34; : 0, &amp;#34;number_of_in_flight_fetch&amp;#34; : 0, &amp;#34;task_max_waiting_in_queue_millis&amp;#34; : 0, &amp;#34;active_shards_percent_as_number&amp;#34; : 100.</description></item><item><title>过滤查询处理器</title><link>/easysearch/main/docs/references/search/search-pipelines/search-processors/filter-query-processor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/search-pipelines/search-processors/filter-query-processor/</guid><description>过滤查询处理器（filter query processor） # 版本引入：1.14.0
filter_query 查询重写处理器用于拦截搜索请求，并向该请求中添加一个额外的查询条件，从而对搜索结果进行过滤。当你不希望重写应用程序中已有的查询语句，但又需要对结果进行额外过滤时，此功能非常有用。
请求体字段 # 下表列出了所有可用的请求字段。
字段 数据类型 说明 query 对象 使用 Easysearch 查询领域特定语言（DSL）编写的查询语句。必填。 tag 字符串 处理器的唯一标识符。可选。 description 字符串 对该处理器的描述信息。可选。 ignore_failure 布尔值 若为 true，则当此处理器执行失败时，Easysearch 将忽略该错误并继续执行搜索管道中的其余处理器。可选，默认值为 false。 示例 # 以下示例演示如何在搜索管道中使用 filter_query 处理器。
准备工作 # 创建一个名为 my_index 的索引，并索引两个文档：一个公开，一个私有：
POST /my_index/_doc/1 { &amp;quot;message&amp;quot;: &amp;quot;This is a public message&amp;quot;, &amp;quot;visibility&amp;quot;: &amp;quot;public&amp;quot; } POST /my_index/_doc/2 { &amp;quot;message&amp;quot;: &amp;quot;This is a private message&amp;quot;, &amp;quot;visibility&amp;quot;: &amp;quot;private&amp;quot; } 创建搜索管道 # 以下请求创建一个名为 my_pipeline 的搜索管道，其中包含一个 filter_query 查询重写处理器。该处理器使用 term 查询，仅返回可见性为“public”的文档：</description></item><item><title>Easysearch-client</title><link>/easysearch/main/docs/release-notes/client/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/release-notes/client/</guid><description>版本发布日志 # 这里是 INFINI Easysearch-client 历史版本发布的相关说明。
2.0.2(2024-08-13) # Improvements # 升级相关依赖项至安全版本 2.0.0(2024-04-17) # Breaking changes # Features # 发布全新的 Easysearch java 客户端 2.0 版本。 客户端经过完全重构，更加轻量级，避免冗余的第三方依赖。 为常用 Easysearch API 提供强类型的请求和响应。 API 均支持阻塞和异步方式。 使用流式构建器和函数式模式，以在创建复杂嵌套结构时编写简洁易读的代码。 通过使用 Jackson 无缝集成应用程序类。 自带 Java 低级别 REST 客户端，处理所有传输级别的问题：HTTP 连接池、重试、节点发现等。 Bug fix # Improvements # 1.0.1(2023-11-14) # Breaking changes # Features # 正式发布 Easysearch Java 客户端。这一里程碑式的更新为开发人员带来了前所未有的便利性，使得与 Easysearch 集群的交互变得更加简洁和直观。现在，通过 Easysearch-client 客户端，开发者可以直接使用 Java 方法和数据结构来进行交互，而不再需要依赖于传统的 HTTP 方法和 JSON。这一变化大大简化了操作流程，使得数据管理和索引更加高效。高级客户端的功能范围包括处理数据操作，管理集群，包括查看和维护集群的健康状态，并对 Security 模块全面兼容。它提供了一系列 API，用于管理角色、用户、权限、角色映射和账户。这意味着安全性和访问控制现在可以更加细粒度地管理，确保了数据的安全性和合规性。 Bug fix # Improvements #</description></item><item><title>任务管理</title><link>/easysearch/main/docs/references/management/tasksapis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/tasksapis/</guid><description>任务管理 # 任务是在集群中运行的任何操作。例如，搜索图书数据集以查找标题或作者姓名是一项任务。将自动创建任务以监视集群的运行状况和性能。有关集群中当前执行的所有任务的详细信息，可以使用 tasks API 操作。
以下请求返回有关所有任务的信息：
GET _tasks 通过包含任务 ID，您可以获得特定任务的信息。请注意，任务 ID 由节点的标识字符串和任务的数字 ID 组成。例如，如果节点的标识串是 nodestring ，任务的数字标识是 1234 ，则任务 ID 是 nodestring:1234 。您可以通过运行 tasks 操作来查找此信息。
GET _tasks/&amp;lt;task_id&amp;gt; 请注意，如果任务完成运行，它将不会作为请求的一部分返回。对于一个需要稍长时间才能完成的任务的示例，可以在较大的文档上运行 _reindex API 操作，然后运行 tasks 。
Sample Response
{ &amp;#34;nodes&amp;#34;: { &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;easy-node1&amp;#34;, &amp;#34;transport_address&amp;#34;: &amp;#34;30.18.0.3:9300&amp;#34;, &amp;#34;host&amp;#34;: &amp;#34;30.18.0.3&amp;#34;, &amp;#34;ip&amp;#34;: &amp;#34;30.18.0.3:9300&amp;#34;, &amp;#34;roles&amp;#34;: [&amp;#34;data&amp;#34;, &amp;#34;ingest&amp;#34;, &amp;#34;master&amp;#34;, &amp;#34;remote_cluster_client&amp;#34;], &amp;#34;tasks&amp;#34;: { &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ:17416&amp;#34;: { &amp;#34;node&amp;#34;: &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ&amp;#34;, &amp;#34;id&amp;#34;: 17416, &amp;#34;type&amp;#34;: &amp;#34;transport&amp;#34;, &amp;#34;action&amp;#34;: &amp;#34;cluster:monitor/tasks/lists&amp;#34;, &amp;#34;start_time_in_millis&amp;#34;: 1613599752458, &amp;#34;running_time_in_nanos&amp;#34;: 994000, &amp;#34;cancellable&amp;#34;: false, &amp;#34;headers&amp;#34;: {} }, &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ:17413&amp;#34;: { &amp;#34;node&amp;#34;: &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ&amp;#34;, &amp;#34;id&amp;#34;: 17413, &amp;#34;type&amp;#34;: &amp;#34;transport&amp;#34;, &amp;#34;action&amp;#34;: &amp;#34;indices:data/write/bulk&amp;#34;, &amp;#34;start_time_in_millis&amp;#34;: 1613599752286, &amp;#34;running_time_in_nanos&amp;#34;: 30846500, &amp;#34;cancellable&amp;#34;: false, &amp;#34;parent_task_id&amp;#34;: &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ:17366&amp;#34;, &amp;#34;headers&amp;#34;: {} }, &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ:17366&amp;#34;: { &amp;#34;node&amp;#34;: &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ&amp;#34;, &amp;#34;id&amp;#34;: 17366, &amp;#34;type&amp;#34;: &amp;#34;transport&amp;#34;, &amp;#34;action&amp;#34;: &amp;#34;indices:data/write/reindex&amp;#34;, &amp;#34;start_time_in_millis&amp;#34;: 1613599750929, &amp;#34;running_time_in_nanos&amp;#34;: 1529733100, &amp;#34;cancellable&amp;#34;: true, &amp;#34;headers&amp;#34;: {} } } } } } 您还可以在查询中使用以下参数。</description></item><item><title>复杂查询</title><link>/easysearch/main/docs/references/sql/complex/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/sql/complex/</guid><description>复杂查询 # 子查询 (Subquery) # 子查询 (subquery) 是一个完整的 SELECT 语句，它被用在另一个语句中，并用括号括起来。从 explain 输出中，您可以注意到一些子查询实际上被转换为等效的联接查询来执行。
示例 1：表子查询 # SQL query:
POST /_sql { &amp;quot;query&amp;quot; : &amp;quot;&amp;quot;&amp;quot; SELECT a1.firstname, a1.lastname, a1.balance FROM accounts a1 WHERE a1.account_number IN ( SELECT a2.account_number FROM accounts a2 WHERE a2.balance &amp;gt; 10000 ) &amp;quot;&amp;quot;&amp;quot; } 解释：
{ &amp;quot;Physical Plan&amp;quot; : { &amp;quot;Project [ columns=[a1.balance, a1.firstname, a1.lastname] ]&amp;quot; : { &amp;quot;Top [ count=200 ]&amp;quot; : { &amp;quot;BlockHashJoin[ conditions=( a1.</description></item><item><title>混合搜索</title><link>/easysearch/main/docs/references/ai-integration/hybrid-search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/ai-integration/hybrid-search/</guid><description>混合搜索 # 混合搜索结合了关键词搜索和语义搜索，以提升搜索相关性。要实现混合搜索，您需要建立一个在搜索时运行的搜索管道。 该管道会在中间阶段拦截搜索结果，并通过处理流程对文档分数进行归一化和组合处理。
要使用混合搜索，您需要配置搜索管道，添加混合排序处理器。
混合排序处理器 # 混合排序处理器 hybrid_ranker_processor 是一种重排序处理器，运行在搜索执行的查询阶段和获取阶段之间。它会拦截查询阶段的结果，然后使用 倒数排序融合算法（RRF，Reciprocal Rank Fusion） 来合并不同查询子句，最终生成排序后的搜索结果列表。
适用场景：
需要融合不同搜索技术（如关键词和语义搜索）的结果 子查询的原始分数不可直接比较（如 BM25 和 KNN） 算法原理 # RRF 是一种多查询融合方法，其核心计算逻辑为：
对每个文档在不同子结果集给出的排名取倒数（如排名第 k 则得分为 1/(k+60)） 将各子结果集的倒数得分相加，生成统一排序分数 按最终分数降序输出结果集 RRF 的通用计算公式如下（其中 k 为平滑常数，默认 60，query_j_rank 表示混合查询中某文档在第 j 种查询方法返回结果中的排名）：
rankScore(document_i) = sum(1/(k + query_1_rank), 1/(k + query_2_rank), ..., 1/(k + query_j_rank)) 请求体字段 # 下表列出了所有可用的请求字段。
字段 数据类型 说明 combination.</description></item><item><title>重命名字段处理器</title><link>/easysearch/main/docs/references/search/search-pipelines/search-processors/rename-field-processor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/search-pipelines/search-processors/rename-field-processor/</guid><description>重命名字段处理器（Rename Field Processor） # 版本引入：1.14.0
rename_field 结果增强处理器用于拦截搜索响应，并对指定字段进行重命名。当你索引中的字段名称与应用程序使用的名称不一致时，该功能非常有用。例如，当索引中使用了新的字段名称，但应用程序仍期望接收旧字段名称时，可通过 rename_field 处理器在返回响应前完成字段名的映射，实现平滑过渡和向后兼容。
请求体字段 # 下表列出了该处理器支持的所有配置字段。
字段 数据类型 说明 field 字符串 要重命名的原始字段名。必填。 target_field 字符串 新的字段名称。必填。 tag 字符串 处理器的唯一标识符。可选。 description 字符串 对该处理器的描述信息。可选。 ignore_failure 布尔值 若为 true，当此处理器执行失败时，Easysearch 将忽略错误并继续执行搜索管道中的其余处理器。可选，默认值为 false。 示例 # 以下示例演示如何在搜索管道中使用 rename_field 处理器。
准备工作 # 创建一个名为 my_index 的索引，并索引一个包含 message 字段的文档：
POST /my_index/_doc/1 { &amp;#34;message&amp;#34;: &amp;#34;This is a public message&amp;#34;, &amp;#34;visibility&amp;#34;: &amp;#34;public&amp;#34; } 创建搜索管道 # 以下请求创建一个名为 my_pipeline 的搜索管道，其中包含一个 rename_field 结果增强处理器，用于将字段 message 重命名为 notification：</description></item><item><title>Easysearch</title><link>/easysearch/main/docs/release-notes/easysearch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/release-notes/easysearch/</guid><description>版本发布日志 # 这里是 INFINI Easysearch 历史版本发布的相关说明。
Latest (In development) # Breaking changes # Features # Bug fix # Improvements # 1.15.3 (2025-09-28) # Breaking changes # Features # 为索引操作添加新的菜单项 节点详情新增树状统计字段存储 添加热点线程页面 Bug fix # Improvements # 1.15.2 (2025-09-21) # Breaking changes # Features # 集群页面新增「临时配置」、「限流限速」以及「其他配置」页面 索引详情页面新增「限流」页面 Bug fix # Improvements # 重构索引详情页面的「设置」页面 去掉 ILM 配置索引的前缀，并兼容旧索引 index-management 从plugin 移动到 modules 精简证书错误时的日志输出 1.</description></item><item><title>基础查询</title><link>/easysearch/main/docs/references/sql/basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/sql/basics/</guid><description>基础查询 # 介绍 # SQL 中的 SELECT 语句是从 Easysearch 索引中检索数据的最常见查询。在本文档中，只涵盖涉及单个索引和查询的简单 SELECT 语句。 SELECT 语句包括 SELECT、FROM、WHERE、GROUP BY、HAVING、ORDER BY 和 LIMIT 子句。其中，SELECT 和 FROM 是指定要获取哪些字段以及它们应该从哪个索引获取的基础。 其它所有子句都是可选的，根据您的需求使用。请继续阅读以了解它们的详细描述、语法和用例。
语法 # SELECT 语句的语法如下：
SELECT [ALL | DISTINCT] (* | expression) [[AS] alias] [, ...] FROM index_name [WHERE predicates] [GROUP BY expression [, ...] [HAVING predicates]] [ORDER BY expression [ASC | DESC] [NULLS {FIRST | LAST}] [, ...]] [LIMIT [offset, ] size] 尽管不支持批量执行多个查询语句，但仍然允许以分号 ; 结束。例如，你可以运行 SELECT * FROM accounts; 而不会遇到问题。这对于支持其他工具生成的查询，如 Microsoft Excel 或 BI 工具，非常有用。</description></item><item><title>快速开始</title><link>/easysearch/main/docs/references/client/java-client/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/client/java-client/</guid><description>Easysearch Java API Client 使用文档 # 简介 # Easysearch Java API Client 是 Easysearch 的官方 Java 客户端，提供了简洁、强大且类型安全的 API 接口。
全新重构的 2.0.x 版本，更轻量级的设计，移除冗余依赖。 兼容 Easysearch 各个版本。 为常用 Easysearch API 提供强类型的请求和响应。 API 均支持阻塞和异步方式。 使用流式构建器和函数式模式，以在创建复杂嵌套结构时编写简洁易读的代码。 通过使用 Jackson 无缝集成应用程序类。 快速开始 # 本页指导您完成Java客户端的安装过程，展示了如何实例化客户端，以及如何使用它执行基本的 Easysearch 操作。
安装 # easysearch-client 已经发布到 Maven https://mvnrepository.com/artifact/com.infinilabs/easysearch-client/2.0.2
安装需要 jdk8 或以上版本
easysearch-client 使用 Jackson 将业务代码和客户端 api 进行集成。
在 Maven 项目中安装 # 相比 1.x 版本的客户端，新版客户端的安装更加简单，只需在您项目的 pom 文件的 dependencies 区域添加以下依赖以引入客户端</description></item><item><title>脚本处理器</title><link>/easysearch/main/docs/references/search/search-pipelines/search-processors/script-processor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/search-pipelines/search-processors/script-processor/</guid><description>脚本处理器（Script Processor） # 版本引入：1.14.0
script 查询重写处理器用于拦截搜索请求，并在请求中添加一个内联的 Painless 脚本，该脚本会在接收到请求时执行。脚本仅能操作以下请求字段：
from size explain version seq_no_primary_term track_scores track_total_hits min_score terminate_after profile 请求体字段 # 下表列出了该处理器支持的所有配置字段。
字段 数据类型 说明 source 内联脚本 要执行的脚本代码。必填。 lang 字符串 脚本语言。可选，默认为 painless，目前仅支持 painless。 tag 字符串 处理器的唯一标识符，用于调试或跟踪。可选。 description 字符串 对该处理器的描述信息。可选。 ignore_failure 布尔值 若为 true，当此处理器执行失败时，Easysearch 将忽略错误并继续执行后续处理器。可选，默认值为 false。 示例 # 以下请求创建一个名为 explain_one_result 的搜索管道，其中包含一个 script 查询重写处理器。该脚本的作用是：当请求返回多个结果时自动关闭 explain 功能，因为 explain 是一项开销较大的操作；仅在返回单个结果时启用。</description></item><item><title>API 使用</title><link>/easysearch/main/docs/references/client/client-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/client/client-api/</guid><description>Easysearch Java API Client 使用文档 # 管理索引 # 使用客户端对索引进行管理
String index = &amp;#34;test1&amp;#34;; if (client.indices().exists(r -&amp;gt; r.index(index)).value()) { LOGGER.info(&amp;#34;Deleting index &amp;#34; + index); DeleteIndexResponse deleteIndexResponse = client.indices().delete(new DeleteIndexRequest.Builder().index(index).build()); LOGGER.info(deleteIndexResponse.toString()); } LOGGER.info(&amp;#34;Creating index &amp;#34; + index); CreateIndexResponse createIndexResponse = client.indices().create(req -&amp;gt; req.index(index)); CloseIndexResponse closeIndexResponse = client.indices().close(req -&amp;gt; req.index(index)); OpenResponse openResponse = client.indices().open(req -&amp;gt; req.index(index)); RefreshResponse refreshResponse = client.indices().refresh(req -&amp;gt; req.index(index)); FlushResponse flushResponse = client.indices().flush(req -&amp;gt; req.index(index)); ForcemergeResponse forcemergeResponse = client.</description></item><item><title>API 接口</title><link>/easysearch/main/docs/references/security/access-control/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/access-control/api/</guid><description>API # 通过 REST API 可以管理用户、角色、角色映射、权限集合和租户。
API 的访问控制 # 您可以控制哪些角色可以访问安全相关的 API，在配置文件 easysearch.yml:
security.restapi.roles_enabled: [&amp;#34;&amp;lt;role&amp;gt;&amp;#34;, ...] 如果希望阻止访问特定的 API：
security.restapi.endpoints_disabled.&amp;lt;role&amp;gt;.&amp;lt;endpoint&amp;gt;: [&amp;#34;&amp;lt;method&amp;gt;&amp;#34;, ...] 参数 endpoint 可以是:
PRIVILEGE ROLE ROLE_MAPPING USER CONFIG CACHE 参数 method 可以是:
GET PUT POST DELETE PATCH 例如，以下配置授予三个角色对 REST API 的访问权限，但随后会阻止 test-role 发送 PUT, POST, DELETE, 或 PATCH 到 _security/role 或 _security/user :
security.restapi.roles_enabled: [&amp;#34;superuser&amp;#34;, &amp;#34;security&amp;#34;, &amp;#34;test-role&amp;#34;] security.restapi.endpoints_disabled.test-role.ROLE: [&amp;#34;PUT&amp;#34;, &amp;#34;POST&amp;#34;, &amp;#34;DELETE&amp;#34;, &amp;#34;PATCH&amp;#34;] security.restapi.endpoints_disabled.test-role.USER: [&amp;#34;PUT&amp;#34;, &amp;#34;POST&amp;#34;, &amp;#34;DELETE&amp;#34;, &amp;#34;PATCH&amp;#34;] 要为 API 配置 使用 PUT 和 PATCH 方法，请将以下行添加到 easysearch.</description></item><item><title>Date nanoseconds 字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/date-field-type/date-nanos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/date-field-type/date-nanos/</guid><description>Date nanoseconds 字段类型 # 日期纳秒字段类型与 日期 字段类型类似，它存储一个日期。然而，date 以毫秒分辨率存储日期，而 date_nanos 以纳秒分辨率存储日期。日期以 long 值的形式存储，表示自纪元以来的纳秒数。因此，支持的日期范围大约是 1970-2262 年。
对 date_nanos 字段的查询被转换为对字段值的 long 表示形式的范围查询。然后使用字段上设置的格式将存储的字段和聚合结果转换为字符串。
date_nano 字段支持 date 支持的所有格式和参数。你可以使用 || 分隔的多种格式。
对于 date_nanos 字段，你可以使用 strict_date_optional_time_nanos 格式来保留纳秒值。如果你在将字段映射为 date_nanos 时没有指定格式，默认格式是 strict_date_optional_time||epoch_millis，它允许你以 strict_date_optional_time 或 epoch_millis 格式传递值。strict_date_optional_time 格式支持纳秒的日期，但 epoch_millis 格式仅支持毫秒的日期。
示例 # 创建一个具有 strict_date_optional_time_nanos 格式的 date_nanos 类型的 date 字段的映射：
PUT testindex/_mapping { &amp;#34;properties&amp;#34;: { &amp;#34;date&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;date_nanos&amp;#34;, &amp;#34;format&amp;#34; : &amp;#34;strict_date_optional_time_nanos&amp;#34; } } } 将两个文档写入到索引中：
PUT testindex/_doc/1 { &amp;#34;date&amp;#34;: &amp;#34;2022-06-15T10:12:52.</description></item><item><title>Date 字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/date-field-type/date/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/date-field-type/date/</guid><description>Date 字段类型 # 在 Easysearch 中，日期可以表示为以下几种形式：
一个长整型值，对应自纪元以来的毫秒数（必须为非负数）。日期在内部以此形式存储。 一个格式化的字符串。 一个整数值，对应自纪元以来的秒数（必须为非负数）。 要表示日期范围，可以使用 date range 字段类型。
代码样例 # 创建一个有两种日期格式的 date 字段
PUT testindex { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;release_date&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;date&amp;quot;, &amp;quot;format&amp;quot; : &amp;quot;strict_date_optional_time||epoch_millis&amp;quot; } } } } 参数说明 # 下表列出了日期字段类型支持的参数，所有参数均为可选项。 参数 描述 默认值 boost 浮点值，指定字段对相关性评分的权重。值大于 1.0 增加相关性，0.0 到 1.0 降低相关性。 1.0 doc_values 布尔值，指定是否将字段存储到磁盘，以便用于聚合、排序或 script 脚本操作。 false format 用于解析日期的格式。 strict_date_time_no_millis || strict_date_optional_time || epoch_millis ignore_malformed 布尔值，指定是否忽略格式错误的值而不抛出异常。 false index 布尔值，指定字段是否可搜索。 true locale 指定基于区域和语言的日期表示格式。 ROOT（区域和语言中立的本地设置） meta 接受字段的元数据。 null_value 指定替代 null 的值，必须与字段类型一致。如果未指定，字段值为 null 时会被视为缺失值。 null store 布尔值，指定字段值是否单独存储并可从 _source 字段外检索。 false 格式 # Easysearch 提供内置的日期格式，但您也可以自定义日期格式。</description></item><item><title>Flat 对象字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/object-field-type/flattened/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/object-field-type/flattened/</guid><description>Flat 对象字段类型 # 在 Easysearch 中，您不需要在索引文档之前指定映射。如果您不指定映射，Easysearch 会使用动态映射自动映射文档中的每个字段及其子字段。当您摄取诸如日志之类的文档时，您可能事先不知道每个字段的子字段名称和类型。在这种情况下，动态映射所有新的子字段可能会快速导致&amp;quot;映射爆炸&amp;quot;，其中不断增长的字段数量可能会降低集群的性能。
Flat 对象字段类型通过将整个 JSON 对象视为字符串来解决这个问题。可以使用标准的点路径表示法访问 JSON 对象中的子字段，但它们不会被索引成单独的字段以供快速查找。
点表示法（a.b）中的字段名最大长度为 2^24 − 1。
Flat 对象字段类型提供以下优势：
高效读取：获取性能类似于关键字字段。 内存效率：将整个复杂的 JSON 对象存储在一个字段中而不索引其所有子字段，可以减少索引中的字段数量。 空间效率：Easysearch 不会为 flat 对象中的子字段创建倒排索引，从而节省空间。 迁移兼容性：您可以将数据从支持类似 flat 字段的数据库系统迁移到 Easysearch。 当字段及其子字段主要用于读取而不是用作搜索条件时，应将字段映射为 flat 对象，因为子字段不会被索引。当对象具有大量字段或您事先不知道内容时，flat 对象非常有用。
Flat 对象支持带有和不带有点路径表示法的精确匹配查询。有关支持的查询类型的完整列表，请参见支持的查询。
在文档中搜索特定嵌套字段的值可能效率低下，因为它可能需要对索引进行完整扫描，这可能是一个昂贵的操作。
Flat 对象不支持：
特定类型的解析。 数值运算，如数值比较或数值排序。 文本分析。 高亮显示。 使用点表示法(a.b)的子字段聚合。 按子字段过滤。 支持的查询 # Flat 对象字段类型支持以下查询：
Term Terms Terms set Prefix Range Match Multi-match Query string Simple query string Exists Wildcard 限制 # 以下限制适用于 Easysearch 中的 flat 对象：</description></item><item><title>Flat 对象字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/object-field-type/flattened_text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/object-field-type/flattened_text/</guid><description>flattened_text 字段类型 # Introduced 1.10.0
flattened_text 类型是一种特殊的数据结构，适用于存储和查询嵌套层次的数据，同时保留类似于 text 类型的灵活搜索特性，例如分词和全文匹配。它在处理结构化或者半结构化数据时非常有用，例如 JSON 对象或动态键值对映射。
定义映射 # { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;flattened_text&amp;#34; } } } 特性 # 扁平化存储
将嵌套的 JSON 对象转换为扁平结构 保留完整的路径信息 支持点号访问内部字段 文本分析
支持标准分词器 支持短语查询 支持全文搜索功能 内部索引结构 每个 flattened_text 字段在 lucene 层面会创建多个子字段:
{field} - 存储所有键 {field}._value - 存储所有值 {field}._valueAndPath - 存储 &amp;ldquo;path=value&amp;rdquo; 格式 索引示例 # PUT my_index/_doc/1 { &amp;quot;my_field&amp;quot;: { &amp;quot;key1&amp;quot;: { &amp;quot;subkey1&amp;quot;: { &amp;quot;subkey2&amp;quot;: &amp;quot;quick brown fox&amp;quot; } } } } 查询示例 # 精确路径匹配 # // Match Query - 指定完整路径 GET my_index/_search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;my_field.</description></item><item><title>Geopoint 地理点字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/geo-filed-type/geo-point/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/geo-filed-type/geo-point/</guid><description>Geopoint 地理点字段类型 # Geopoint 地理点字段类型包含由纬度 latitude 和经度 longitude 指定的地理点。
代码示例 # 创建一个带有 Geopoint 地理点字段类型的映射：
PUT testindex1 { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;point&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;geo_point&amp;#34; } } } } 地理点格式 # Geopoint 地理点可以用以下格式索引：
包含纬度和经度的对象 PUT testindex1/_doc/1 { &amp;#34;point&amp;#34;: { &amp;#34;lat&amp;#34;: 40.71, &amp;#34;lon&amp;#34;: 74.00 } } 写入包含纬度,经度 的文档 PUT testindex1/_doc/2 { &amp;#34;point&amp;#34;: &amp;#34;40.71,74.00&amp;#34; } geohash 格式的文档 PUT testindex1/_doc/3 { &amp;#34;point&amp;#34;: &amp;#34;txhxegj0uyp3&amp;#34; } [经度, 纬度] 格式的数组 PUT testindex1/_doc/4 { &amp;#34;point&amp;#34;: [74.</description></item><item><title>Geoshape 地理形状字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/geo-filed-type/geo-shape/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/geo-filed-type/geo-shape/</guid><description>Geoshape 地理形状字段类型 # Geoshape 地理形状字段类型包含地理形状，例如多边形或地理点的集合。为了索引地理形状，Easysearch 会将形状分割成三角形网格，并将每个三角形存储在 BKD 树中。这提供了 10^-7 度的精度，代表了接近完美的空间分辨率。这个过程的性能主要受到您正在索引的多边形顶点数量多少的影响。
代码样例 # 创建一个带有地理形状字段类型的映射：
PUT testindex { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;location&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;geo_shape&amp;#34; } } } } 格式说明 # 地理形状可以用以下格式索引：
GeoJSON Well-Known Text (WKT) 在 GeoJSON 和 WKT 中，坐标必须在坐标数组中按照 经度, 纬度 的顺序指定。注意在这种格式中经度是在前面的。
地理形状类型 # 下表描述了可能的地理形状类型以及它们与 GeoJSON 和 WKT 类型的关系。
Easysearch 类型 GeoJSON 类型 WKT 类型 描述 point Point POINT 由纬度和经度指定的地理点。Easysearch 使用世界大地测量系统 (WGS84) 坐标。 linestring LineString LINESTRING 由两个或更多点指定的线。可以是直线或连接的线段路径。 polygon Polygon POLYGON 由坐标形式的顶点列表指定的多边形。多边形必须是闭合的，这意味着最后一个点必须与第一个点相同。因此，要创建一个 n 边形，需要 n+1 个顶点。最少需要四个顶点，这会创建一个三角形。 multipoint MultiPoint MULTIPOINT 不连接的离散相关点的数组。 multilinestring MultiLineString MULTILINESTRING 线串的数组。 multipolygon MultiPolygon MULTIPOLYGON 多边形的数组。 geometrycollection GeometryCollection GEOMETRYCOLLECTION 可能是不同类型的地理形状的集合。 envelope N/A BBOX 由左上和右下顶点指定的边界矩形。 Point 点位 # 一个点代表着由经度和纬度指定的单个坐标对。</description></item><item><title>ID 属性</title><link>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/id/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/id/</guid><description>ID 属性 # Easysearch 中的每个文档都有一个唯一的 _id 字段。此字段已被索引，允许您使用 GET API 或 ids 查询 检索文档。
如果您未提供 _id 值，则 Easysearch 会自动为文档生成一个。
以下示例请求创建一个名为 test-index1 的索引，并添加两个具有不同 _id 值的文档：
PUT test-index1/_doc/1 { &amp;quot;text&amp;quot;: &amp;quot;Document with ID 1&amp;quot; } PUT test-index1/_doc/2?refresh=true { &amp;quot;text&amp;quot;: &amp;quot;Document with ID 2&amp;quot; } 您可以使用 _id 字段查询文档，如以下示例请求所示：
GET test-index1/_search { &amp;quot;query&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;_id&amp;quot;: [&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;] } } } 返回 _id 值为 1 和 2 的两个文档：
{ &amp;quot;took&amp;quot;: 10, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 2, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 1, &amp;quot;hits&amp;quot;: [ { &amp;quot;_index&amp;quot;: &amp;quot;test-index1&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;_score&amp;quot;: 1, &amp;quot;_source&amp;quot;: { &amp;quot;text&amp;quot;: &amp;quot;Document with ID 1&amp;quot; } }, { &amp;quot;_index&amp;quot;: &amp;quot;test-index1&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;2&amp;quot;, &amp;quot;_score&amp;quot;: 1, &amp;quot;_source&amp;quot;: { &amp;quot;text&amp;quot;: &amp;quot;Document with ID 2&amp;quot; } } ] } _id 字段的限制 # 虽然 _id 字段可以在各种查询中使用，但它在聚合、排序和脚本中的使用受到限制。如果您需要对 _id 字段进行排序或聚合，建议将 _id 内容复制到另一个启用了 doc_values 的字段中。</description></item><item><title>IP 地址字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/ip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/ip/</guid><description>IP 地址字段类型 # IP 字段类型用于存储 IPv4 或 IPv6 格式的 IP 地址。
要表示 IP 地址范围，可以使用 IP 范围字段类型
参考代码 # 创建一个有 IP 地址的 mapping
PUT testindex { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;ip_address&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;ip&amp;quot; } } } } 索引一个有 IP 地址的文档
PUT testindex/_doc/1 { &amp;quot;ip_address&amp;quot; : &amp;quot;10.24.34.0&amp;quot; } 查询一个特定 IP 地址的索引
GET testindex/_doc/1 { &amp;quot;query&amp;quot;: { &amp;quot;term&amp;quot;: { &amp;quot;ip_address&amp;quot;: &amp;quot;10.24.34.0&amp;quot; } } } 搜索 IP 地址及其关联的网络掩码 # 您可以使用无类别域间路由 (CIDR) 表示法查询索引中的 IP 地址。在 CIDR 表示法中，通过斜杠 / 分隔 IP 地址和前缀长度（0–32）。例如，前缀长度为 24 表示匹配所有具有相同前 24 位的 IP 地址。</description></item><item><title>Join 字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/object-field-type/join/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/object-field-type/join/</guid><description>Join 字段类型 # Join 字段类型用于在同一索引中的文档之间建立父/子关系。
代码样例 # 模拟创建一个映射来建立一个产品和其品牌之间的父/子关系：
PUT testindex1 { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;product_to_brand&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;join&amp;#34;, &amp;#34;relations&amp;#34;: { &amp;#34;brand&amp;#34;: &amp;#34;product&amp;#34; } } } } } 索引一个父文档：
PUT testindex1/_doc/1 { &amp;#34;name&amp;#34;: &amp;#34;Brand 1&amp;#34;, &amp;#34;product_to_brand&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;brand&amp;#34; } } 您也可以使用更简单的格式：
PUT testindex1/_doc/1 { &amp;#34;name&amp;#34;: &amp;#34;Brand 1&amp;#34;, &amp;#34;product_to_brand&amp;#34;: &amp;#34;brand&amp;#34; } 在索引子文档时，您需要指定 routing 查询参数，因为同一父/子层级中的父文档和子文档必须索引在同一分片上。每个子文档在 parent 字段中引用其父文档的 ID。
为每个父文档索引两个子文档：
PUT testindex1/_doc/3?routing=1 { &amp;#34;name&amp;#34;: &amp;#34;Product 1&amp;#34;, &amp;#34;product_to_brand&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;product&amp;#34;, &amp;#34;parent&amp;#34;: &amp;#34;1&amp;#34; } } PUT testindex1/_doc/4?</description></item><item><title>match_only_text 字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/match_only_text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/match_only_text/</guid><description>match_only_text 字段类型 # Introduced 1.10.0
简介 # match_only_text 是一个为全文搜索优化的字段类型，是 text 类型的变体。它通过省略词条位置、词频和规范化信息来减少存储需求,适合对存储成本敏感但仍需要基本全文搜索功能的场景。
主要特点 # 存储优化:
不存储位置信息 不存储词频信息 不存储规范化信息 显著减少索引大小 评分机制:
禁用评分计算 所有匹配文档得分统一为 1.0 查询支持:
支持大多数查询类型 不支持 interval 查询 不支持 span 查询 支持但不优化短语查询 使用场景 # 适合用于:
需要快速查找包含特定词条的文档 对存储成本敏感的大数据集 不需要复杂相关性排序的场景 不适合用于:
需要基于相关性排序的查询 依赖词条位置或顺序的查询 需要精确短语匹配的场景 映射示例 # PUT my_index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;title&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;match_only_text&amp;#34; } } } } 参数配置 # 参数 说明 默认值 analyzer 分析器设置 standard boost 评分提升因子 1.</description></item><item><title>Nested 字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/object-field-type/nested/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/object-field-type/nested/</guid><description>Nested 字段类型 # Nested 字段类型是一种特殊的 对象字段类型。
任何对象字段都可以包含一个对象数组。数组中的每个对象都会被动态映射为对象字段类型并以扁平化形式存储。这意味着数组中的对象会被分解成单独的字段，每个字段在所有对象中的值会被存储在一起。有时需要使用 Nested 嵌套类型来将嵌套对象作为一个整体保存，以便您可以对其关联性执行搜索。
扁平化形式 # 默认情况下，每个嵌套对象都被动态映射为对象字段类型。任何对象字段都可以包含一个对象数组。
PUT testindex1/_doc/100 { &amp;#34;patients&amp;#34;: [ {&amp;#34;name&amp;#34;: &amp;#34;John Doe&amp;#34;, &amp;#34;age&amp;#34;: 56, &amp;#34;smoker&amp;#34;: true}, {&amp;#34;name&amp;#34;: &amp;#34;Mary Major&amp;#34;, &amp;#34;age&amp;#34;: 85, &amp;#34;smoker&amp;#34;: false} ] } 当这些对象被存储时，它们会被扁平化，因此它们的内部表示形式具有每个字段的所有值的数组：
{ &amp;#34;patients.name&amp;#34;: [&amp;#34;John Doe&amp;#34;, &amp;#34;Mary Major&amp;#34;], &amp;#34;patients.age&amp;#34;: [56, 85], &amp;#34;patients.smoker&amp;#34;: [true, false] } 一些查询会在这种表示形式中正确工作。如果您搜索年龄大于 75 或者 吸烟的病人 &amp;quot;patients.smoker&amp;quot;: true，文档 id 100 应该匹配。
GET testindex1/_search { &amp;#34;query&amp;#34;: { &amp;#34;bool&amp;#34;: { &amp;#34;should&amp;#34;: [ { &amp;#34;term&amp;#34;: { &amp;#34;patients.</description></item><item><title>Object 对象字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/object-field-type/object/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/object-field-type/object/</guid><description>Object 对象字段类型 # 对象字段类型包含一个 JSON 对象（一组名称/值对）。JSON 对象中的值可以是另一个 JSON 对象。在映射对象字段时不需要指定 object 作为类型，因为 object 是默认类型。
代码示例 # 创建一个带有对象字段的映射：
PUT testindex1/_mappings { &amp;#34;properties&amp;#34;: { &amp;#34;patient&amp;#34;: { &amp;#34;properties&amp;#34; : { &amp;#34;name&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;text&amp;#34; }, &amp;#34;id&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;keyword&amp;#34; } } } } } 索引一个包含对象字段的文档：
PUT testindex1/_doc/1 { &amp;#34;patient&amp;#34;: { &amp;#34;name&amp;#34; : &amp;#34;John Doe&amp;#34;, &amp;#34;id&amp;#34; : &amp;#34;123456&amp;#34; } } 嵌套对象在内部存储为扁平的键/值对。要引用嵌套对象中的字段，使用 parent field.child field（例如，patient.id）。
搜索 ID 为 123456 的患者信息：</description></item><item><title>Percolator 过滤器字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/percolator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/percolator/</guid><description>Percolator 过滤器字段类型 # Percolator 字段类型将该字段视为查询处理。任何 JSON 对象字段都可以标记为 Percolator 字段。通常，文档被索引并用于搜索，而 Percolator 字段存储搜索条件，稍后通过 Percolate 查询将匹配文档到该条件。
参考代码 # 客户正在搜索价格在 400 美元或以下的桌子，并希望为此搜索创建警报。 创建一个映射，为查询字段分配一个 percolator 字段类型：
PUT testindex1 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;search&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;query&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot; } } }, &amp;quot;price&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;float&amp;quot; }, &amp;quot;item&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; } } } } 索引一个查询
PUT testindex1/_doc/1 { &amp;quot;search&amp;quot;: { &amp;quot;query&amp;quot;: { &amp;quot;bool&amp;quot;: { &amp;quot;filter&amp;quot;: [ { &amp;quot;match&amp;quot;: { &amp;quot;item&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;table&amp;quot; } } }, { &amp;quot;range&amp;quot;: { &amp;quot;price&amp;quot;: { &amp;quot;lte&amp;quot;: 400.</description></item><item><title>Search-as-you-type 字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/autocomplete-field-type/search-as-you-type/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/autocomplete-field-type/search-as-you-type/</guid><description>Search-as-you-type 字段类型 # search-as-you-type 字段类型通过前缀和中缀补全提供边输入边搜索的功能。
代码样例 # 将字段映射为 search-as-you-type 类型时，会为该字段创建 n-gram 子字段，其中 n 的范围为 [2, max_shingle_size]。此外，还会创建一个索引前缀子字段。
创建一个 search-as-you-type 的映射字段
PUT books { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;suggestions&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;search_as_you_type&amp;quot; } } } } 除了创建 suggestions 字段外，还会生成 suggestions._2gram、suggestions._3gram 和 suggestions._index_prefix 字段。
以下是使用 search-as-you-type 字段索引文档的示例：
PUT books/_doc/1 { &amp;quot;suggestions&amp;quot;: &amp;quot;one two three four&amp;quot; } 要匹配任意顺序的词项，可以使用 bool_prefix 或 multi-match 查询。
这些查询会将搜索词项按顺序匹配的文档排名提高，而将词项顺序不一致的文档排名降低。
GET books/_search { &amp;quot;query&amp;quot;: { &amp;quot;multi_match&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;tw one&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;bool_prefix&amp;quot;, &amp;quot;fields&amp;quot;: [ &amp;quot;suggestions&amp;quot;, &amp;quot;suggestions.</description></item><item><title>二进制字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/binary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/binary/</guid><description>二进制字段类型 # 二进制字段类型包含以 Base64 编码存储的二进制值，这些值不可被搜索。
参考代码 # 创建包含二进制字段的映射
PUT testindex { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;binary_value&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;binary&amp;quot; } } } } 索引一个二进制值的文档
PUT testindex/_doc/1 { &amp;quot;binary_value&amp;quot; : &amp;quot;bGlkaHQtd29rfx4=&amp;quot; } 使用 = 作为填充字符。不允许嵌入换行符。
参数说明 # 以下参数均为可选参数
doc_values：布尔值，指定字段是否应存储在磁盘上，以便用于聚合、排序或 script 操作。可选，默认为 false。 store：布尔值，指定字段值是否应存储，并可从 _source 字段中单独检索。可选，默认为 false。</description></item><item><title>元数据属性</title><link>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/meta/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/meta/</guid><description>Meta 元数据属性 # _meta 字段是一个映射属性，允许您为索引映射附加自定义元数据。您的应用程序可以使用这些元数据来存储与您的用例相关的信息，如版本控制、所有权、分类或审计。
用法 # 您可以在创建新索引或更新现有索引的映射时定义 _meta 字段，如以下示例所示：
PUT my-index { &amp;quot;mappings&amp;quot;: { &amp;quot;_meta&amp;quot;: { &amp;quot;application&amp;quot;: &amp;quot;MyApp&amp;quot;, &amp;quot;version&amp;quot;: &amp;quot;1.2.3&amp;quot;, &amp;quot;author&amp;quot;: &amp;quot;John Doe&amp;quot; }, &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; } } } } 在此示例中，添加了三个自定义元数据字段：application、version 和 author。您的应用程序可以使用这些字段来存储有关索引的任何相关信息，例如它所属的应用程序、应用程序版本或索引的作者。
您可以使用 Put Mapping API 操作更新 _meta 字段，如以下示例所示：
PUT my-index/_mapping { &amp;quot;_meta&amp;quot;: { &amp;quot;application&amp;quot;: &amp;quot;MyApp&amp;quot;, &amp;quot;version&amp;quot;: &amp;quot;1.3.0&amp;quot;, &amp;quot;author&amp;quot;: &amp;quot;Jane Smith&amp;quot; } } 检索元数据信息 # 您可以使用 Get Mapping API 操作检索索引的 _meta 信息，如以下示例所示：</description></item><item><title>分词器参数</title><link>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/analyzer/</guid><description>Analyzer 分词器参数 # 分析器 analyzer 映射参数用于定义在索引和搜索期间应用于文本字段的文本分析过程，即分词器的作用过程。
代码样例 # 以下示例配置定义了一个名为 my_custom_analyzer 的自定义分词器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_stop_filter&amp;quot;, &amp;quot;my_stemmer&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;my_stop_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stop&amp;quot;, &amp;quot;stopwords&amp;quot;: [&amp;quot;the&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;and&amp;quot;, &amp;quot;or&amp;quot;] }, &amp;quot;my_stemmer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;english&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_text_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_analyzer&amp;quot;, &amp;quot;search_analyzer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;search_quote_analyzer&amp;quot;: &amp;quot;my_custom_analyzer&amp;quot; } } } } 在此示例中，my_custom_analyzer 使用标准分词器，将所有标记转换为小写，应用自定义停用词过滤器，并应用英语词干提取器。</description></item><item><title>别名字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/alias/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/alias/</guid><description>Alias 别名字段类型 # 别名字段类型为现有字段创建另一个名称。您可以在搜索和字段功能的 API 操作中使用别名字段，但存在一些例外情况。要设置别名，必须在 path 参数中指定原始字段名称。
参考代码 # PUT movies { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;year&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;date&amp;quot; }, &amp;quot;release_date&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;alias&amp;quot;, &amp;quot;path&amp;quot; : &amp;quot;year&amp;quot; } } } } 参数说明 # path：指向原始字段的完整路径，包括所有父对象。例如，parent.child.field_name。此参数为必填项。 别名（Alias）字段 # 别名（Alias）字段必须遵循以下规则：
一个别名字段只能引用一个原始字段。 在嵌套对象中，别名必须与原始字段位于相同的嵌套层级。 要更改别名引用的字段，需要更新映射配置。但请注意，之前存储的 Percolator 查询中的别名仍会继续引用原始字段，不会自动更新为新的字段引用。
原始字段 # 别名的原始字段必须遵守以下规则：
原始字段必须在别名字段创建之前定义。 原始字段不能是对象类型，也不能是另一个别名字段。 可以使用别名字段的搜索 API # 您可以在以下搜索 API 的只读操作中使用别名：</description></item><item><title>动态映射参数</title><link>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/dynamic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/dynamic/</guid><description>Dynamic 动态映射参数 # dynamic 参数指定是否可以动态地将新检测到的字段添加到映射中。它接受下表中列出的参数。
参数 描述 true 指定可以动态地将新字段添加到映射中。默认值为 true。 false 指定不能动态地将新字段添加到映射中。如果检测到新字段，则不会对其进行索引或搜索，但可以从 _source 字段中检索。 strict 当检测到文档中有新字段时，索引操作失败，抛出异常。 strict_allow_templates 如果新字段匹配映射中预定义的动态模板，则添加新字段。 示例：创建 dynamic 设置为 true 的索引 # 通过以下命令创建一个 dynamic 设置为 true 的索引：
PUT testindex1 { &amp;#34;mappings&amp;#34;: { &amp;#34;dynamic&amp;#34;: true } } 通过以下命令，索引一个包含两个字符串字段的对象字段 patient 的文档：
PUT testindex1/_doc/1 { &amp;#34;patient&amp;#34;: { &amp;#34;name&amp;#34; : &amp;#34;John Doe&amp;#34;, &amp;#34;id&amp;#34; : &amp;#34;123456&amp;#34; } } 通过以下命令确认映射按预期工作：</description></item><item><title>向量字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/knn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/knn/</guid><description>k-NN 向量字段类型 # 关于向量 # 在索引文档和运行查询时都需要指定向量类型。在这两种情况下，您都使用相同的 JSON 结构来定义向量类型。每个向量类型还有一个简写形式，这在使用不支持嵌套文档的工具时会很方便。以下示例展示了如何在索引向量时指定它们。
knn_dense_float_vector 密集向量类型 # 假设您已经定义了一个映射，其中 my_vec 的类型为 knn_dense_float_vector。
POST /my-index/_doc { &amp;#34;my_vec&amp;#34;: { &amp;#34;values&amp;#34;: [0.1, 0.2, 0.3, ...] # 1 } } POST /my-index/_doc { &amp;#34;my_vec&amp;#34;: [0.1, 0.2, 0.3, ...] # 2 } 说明 # 1 向量中所有浮点值的 JSON 列表。长度应与映射中的dims匹配。 2 #1 的简写形式。
knn_sparse_bool_vector 稀疏向量类型 # 假设您已经定义了一个映射，其中 my_vec 的类型为 knn_sparse_bool_vector。
POST /my-index/_doc { &amp;#34;my_vec&amp;#34;: { &amp;#34;true_indices&amp;#34;: [1, 3, 5, .</description></item><item><title>向量查询</title><link>/easysearch/main/docs/references/search/knn_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/knn_api/</guid><description>向量查询 # 使用 kNN 检索 API 来进行向量查询。
先决条件 # 要运行 kNN 搜索，必须安装 knn 插件，参考 插件安装 。
创建 Mapping 和 Setting # 在索引向量之前，首先定义一个 Mapping，指定向量数据类型、索引模型和模型的参数。这决定了索引向量支持哪些查询。 并指定 index.knn 为 true ，这是为了启用近似相似度模型。
从1.11.1 版本开始，index.knn 已弃用，创建 knn 索引时，不再配置 index.knn 参数。
请求示例 # PUT /knn-test { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_vec&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;knn_dense_float_vector&amp;quot;, &amp;quot;knn&amp;quot;: { &amp;quot;dims&amp;quot;: 50, &amp;quot;model&amp;quot;: &amp;quot;lsh&amp;quot;, &amp;quot;similarity&amp;quot;: &amp;quot;cosine&amp;quot;, &amp;quot;L&amp;quot;: 99, &amp;quot;k&amp;quot;: 1 } } } } } 参数说明 # my_vec 存储向量的字段名称 knn_dense_float_vector 表示数据类型为密集型浮点向量.</description></item><item><title>字段名称</title><link>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/field-names/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/field-names/</guid><description>Field names 字段名称 # _field_names 字段索引包含非空值的字段名称。可以使用 exists 查询来识别指定字段是否具有非空值的文档。
但是，只有当 doc_values 和 norms 都被禁用时，_field_names 才会索引字段名称。如果启用了 doc_values 或 norms 中的任何一个，则 exists 查询仍然可以工作，但不会依赖 _field_names 字段。
映射示例 # PUT testindex { &amp;quot;mappings&amp;quot;: { &amp;quot;_field_names&amp;quot;: { &amp;quot;enabled&amp;quot;: &amp;quot;true&amp;quot; }, &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;doc_values&amp;quot;: false, &amp;quot;norms&amp;quot;: false }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;doc_values&amp;quot;: true, &amp;quot;norms&amp;quot;: false }, &amp;quot;price&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;float&amp;quot;, &amp;quot;doc_values&amp;quot;: false } } } }</description></item><item><title>字段复制参数</title><link>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/copy_to/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/copy_to/</guid><description>Copy_to 字段复制参数 # copy_to 参数允许您将多个字段的值复制到单个字段中。如果您经常跨多个字段搜索，此参数会很有用，因为这样可以达到搜索一组字段的效果。
只有字段值被复制，而不是分词器产生的词项。原始的 _source 字段保持不变，并且可以使用 copy_to 参数将相同的值复制到多个字段。但是，字段间不支持递归复制；相反，应该直接使用 copy_to 从源字段复制到多个目标字段。
代码样例 # 以下示例使用 copy_to 参数通过产品的名称和描述进行搜索，并将这些值复制到单个字段中：
PUT my-products-index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;name&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;copy_to&amp;quot;: &amp;quot;product_info&amp;quot; }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;copy_to&amp;quot;: &amp;quot;product_info&amp;quot; }, &amp;quot;product_info&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;price&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;float&amp;quot; } } } } PUT my-products-index/_doc/1 { &amp;quot;name&amp;quot;: &amp;quot;Wireless Headphones&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;High-quality wireless headphones with noise cancellation&amp;quot;, &amp;quot;price&amp;quot;: 99.99 } PUT my-products-index/_doc/2 { &amp;quot;name&amp;quot;: &amp;quot;Bluetooth Speaker&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Portable Bluetooth speaker with long battery life&amp;quot;, &amp;quot;price&amp;quot;: 49.</description></item><item><title>布尔字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/boolean/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/boolean/</guid><description>布尔字段类型 # 布尔字段类型接受 true 或 false 值，也支持字符串形式的 &amp;ldquo;true&amp;rdquo; 或 &amp;ldquo;false&amp;rdquo;。此外，还可以使用空字符串 &amp;quot;&amp;quot; 表示 false 值。
参考代码 # 创建一个由 a,b,c 三个布尔字段组成的 mapping
PUT testindex { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;a&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;boolean&amp;quot; }, &amp;quot;b&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;boolean&amp;quot; }, &amp;quot;c&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;boolean&amp;quot; } } } } 索引由布尔值组成的文档
PUT testindex/_doc/1 { &amp;quot;a&amp;quot; : true, &amp;quot;b&amp;quot; : &amp;quot;true&amp;quot;, &amp;quot;c&amp;quot; : &amp;quot;&amp;quot; } 因此，字段 a 和 b 将被设置为 true，而字段 c 将被设置为 false。</description></item><item><title>强制类型转换参数</title><link>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/coerce/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/coerce/</guid><description>Coerce 强制类型转换参数 # coerce 映射参数控制数据在索引期间如何将其值转换为预期的字段数据类型。此参数让您可以验证数据是否按照预期的字段类型正确格式化和索引。这提高了搜索结果的准确性。
代码样例 # 以下示例演示如何使用 coerce 映射参数。
启用 coerce 去索引文档 # PUT products { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;price&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;integer&amp;quot;, &amp;quot;coerce&amp;quot;: true } } } } PUT products/_doc/1 { &amp;quot;name&amp;quot;: &amp;quot;Product A&amp;quot;, &amp;quot;price&amp;quot;: &amp;quot;19.99&amp;quot; } 在此示例中，price 字段被定义为 integer 类型，且 coerce 设置为 true。在索引文档时，字符串值 19.99 被强制转换为整数 19。
禁用 coerce 的文档索引 # PUT orders { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;quantity&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;integer&amp;quot;, &amp;quot;coerce&amp;quot;: false } } } } PUT orders/_doc/1 { &amp;quot;item&amp;quot;: &amp;quot;Widget&amp;quot;, &amp;quot;quantity&amp;quot;: &amp;quot;10&amp;quot; } 在此示例中，quantity 字段被定义为 integer 类型，且 coerce 设置为 false。在索引文档时，字符串值 10 不会被强制转换，由于类型不匹配，文档写入会被拒绝。</description></item><item><title>快照生命周期管理</title><link>/easysearch/main/docs/references/management/slm_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/slm_api/</guid><description>快照生命周期管理 # 使用快照管理（SLM）API 自动创建快照。
创建快照策略 # 请求示例 # 每天上午 8 点自动创建一份快照,快照名称格式为 yyyy-MM-dd-HH:mm ，存储在 my_backup 快照仓库 每天凌晨 1 点自动删除最早 7 天前创建的快照、超过 21 个的快照以及保留至少 7 个快照 快照创建和删除的时间限制均为 1 小时 curl -XPOST -uadmin:admin -H 'Content-Type: application/json' 'https://localhost:9200/_slm/policies/daily-policy' -d ' { &amp;quot;description&amp;quot;: &amp;quot;每日快照策略&amp;quot;, &amp;quot;creation&amp;quot;: { &amp;quot;schedule&amp;quot;: { &amp;quot;cron&amp;quot;: { &amp;quot;expression&amp;quot;: &amp;quot;0 8 * * *&amp;quot;, &amp;quot;timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot; } }, &amp;quot;time_limit&amp;quot;: &amp;quot;1h&amp;quot; }, &amp;quot;deletion&amp;quot;: { &amp;quot;schedule&amp;quot;: { &amp;quot;cron&amp;quot;: { &amp;quot;expression&amp;quot;: &amp;quot;0 1 * * *&amp;quot;, &amp;quot;timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot; } }, &amp;quot;condition&amp;quot;: { &amp;quot;max_age&amp;quot;: &amp;quot;7d&amp;quot;, &amp;quot;max_count&amp;quot;: 21, &amp;quot;min_count&amp;quot;: 7 }, &amp;quot;time_limit&amp;quot;: &amp;quot;1h&amp;quot; }, &amp;quot;snapshot_config&amp;quot;: { &amp;quot;date_format&amp;quot;: &amp;quot;yyyy-MM-dd-HH:mm&amp;quot;, &amp;quot;date_format_timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot;, &amp;quot;indices&amp;quot;: &amp;quot;*&amp;quot;, &amp;quot;repository&amp;quot;: &amp;quot;my_backup&amp;quot;, &amp;quot;ignore_unavailable&amp;quot;: &amp;quot;true&amp;quot;, &amp;quot;include_global_state&amp;quot;: &amp;quot;false&amp;quot;, &amp;quot;partial&amp;quot;: &amp;quot;true&amp;quot;, &amp;quot;metadata&amp;quot;: { &amp;quot;any_key&amp;quot;: &amp;quot;any_value&amp;quot; } } }' 示例响应 # { &amp;quot;_id&amp;quot;: &amp;quot;daily-policy-sm-policy&amp;quot;, &amp;quot;_version&amp;quot;: 1, &amp;quot;_seq_no&amp;quot;: 0, &amp;quot;_primary_term&amp;quot;: 1, &amp;quot;sm_policy&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;daily-policy&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;每日快照策略&amp;quot;, &amp;quot;schema_version&amp;quot;: 17, &amp;quot;creation&amp;quot;: { &amp;quot;schedule&amp;quot;: { &amp;quot;cron&amp;quot;: { &amp;quot;expression&amp;quot;: &amp;quot;0 8 * * *&amp;quot;, &amp;quot;timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot; } }, &amp;quot;time_limit&amp;quot;: &amp;quot;1h&amp;quot; }, &amp;quot;deletion&amp;quot;: { &amp;quot;schedule&amp;quot;: { &amp;quot;cron&amp;quot;: { &amp;quot;expression&amp;quot;: &amp;quot;0 1 * * *&amp;quot;, &amp;quot;timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot; } }, &amp;quot;condition&amp;quot;: { &amp;quot;max_age&amp;quot;: &amp;quot;7d&amp;quot;, &amp;quot;min_count&amp;quot;: 7, &amp;quot;max_count&amp;quot;: 21 }, &amp;quot;time_limit&amp;quot;: &amp;quot;1h&amp;quot; }, &amp;quot;snapshot_config&amp;quot;: { &amp;quot;indices&amp;quot;: &amp;quot;*&amp;quot;, &amp;quot;metadata&amp;quot;: { &amp;quot;any_key&amp;quot;: &amp;quot;any_value&amp;quot; }, &amp;quot;ignore_unavailable&amp;quot;: &amp;quot;true&amp;quot;, &amp;quot;date_format_timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot;, &amp;quot;include_global_state&amp;quot;: &amp;quot;false&amp;quot;, &amp;quot;date_format&amp;quot;: &amp;quot;yyyy-MM-dd-HH:mm&amp;quot;, &amp;quot;repository&amp;quot;: &amp;quot;my_backup&amp;quot;, &amp;quot;partial&amp;quot;: &amp;quot;true&amp;quot; }, &amp;quot;schedule&amp;quot;: { &amp;quot;interval&amp;quot;: { &amp;quot;start_time&amp;quot;: 1685348095913, &amp;quot;period&amp;quot;: 1, &amp;quot;unit&amp;quot;: &amp;quot;Minutes&amp;quot; } }, &amp;quot;enabled&amp;quot;: true, &amp;quot;last_updated_time&amp;quot;: 1685348095938, &amp;quot;enabled_time&amp;quot;: 1685348095909 } } 获取策略 # 获取所有 SLM 策略</description></item><item><title>忽略属性</title><link>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/ignored/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/ignored/</guid><description>Ignored 忽略属性 # _ignored 字段帮助您管理文档中与格式错误数据相关的问题。由于在 索引映射中启用了 ignore_malformed 设置，此字段用于存储在数据索引过程中被忽略的字段名称。
_ignored 字段允许您搜索和识别包含被忽略字段的文档，以及被忽略的具体字段名称。这对于故障排除很有用。
您可以使用 term、terms 和 exists 查询来查询 _ignored 字段。
只有当索引映射中启用了 ignore_malformed 设置时，才会填充 _ignored 字段。如果 ignore_malformed 设置为 false（默认值），则格式错误的字段将导致整个文档被拒绝，并且不会填写 _ignored 字段。
以下示例展示了如何使用 _ignored 字段：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;exists&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;_ignored&amp;quot; } } } 使用 _ignored 字段的索引请求示例 # 以下示例向 test-ignored 索引添加一个新文档，其中 ignore_malformed 设置为 true，这样在数据索引时不会抛出错误：
PUT test-ignored { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;length&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;long&amp;quot;, &amp;quot;ignore_malformed&amp;quot;: true } } } } POST test-ignored/_doc { &amp;quot;title&amp;quot;: &amp;quot;correct text&amp;quot;, &amp;quot;length&amp;quot;: &amp;quot;not a number&amp;quot; } GET test-ignored/_search { &amp;quot;query&amp;quot;: { &amp;quot;exists&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;_ignored&amp;quot; } } } 示例返回内容 # { &amp;quot;took&amp;quot;: 42, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 1, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 1, &amp;quot;hits&amp;quot;: [ { &amp;quot;_index&amp;quot;: &amp;quot;test-ignored&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;qcf0wZABpEYH7Rw9OT7F&amp;quot;, &amp;quot;_score&amp;quot;: 1, &amp;quot;_ignored&amp;quot;: [ &amp;quot;length&amp;quot; ], &amp;quot;_source&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;correct text&amp;quot;, &amp;quot;length&amp;quot;: &amp;quot;not a number&amp;quot; } } ] } } 忽略指定字段 # 您可以使用 term 查询来查找特定字段被忽略的文档，如以下示例请求所示：</description></item><item><title>数值字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/numeric-field/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/numeric-field/</guid><description>Numeric 字段类型 # 下表列出了 Easysearch 支持的所有数字字段类型。
字段数据类型 描述 byte 有符号的 8 位整数。最小值为 -128，最大值为 127。 double 双精度 64 位 IEEE 754 浮点数。最小值为 2^−1074，最大值为 (2 − 2^−52) · 2^1023。有效位数为 53，有效数字位为 15.95。 float 单精度 32 位 IEEE 754 浮点数。最小值为 2^−149，最大值为 (2 − 2^−23) · 2^127。有效位数为 24，有效数字位为 7.22。 half_float 半精度 16 位 IEEE 754 浮点数。最小值为 2^−24，最大值为 65504。有效位数为 11，有效数字位为 3.31。 integer 有符号的 32 位整数。最小值为 -2^31，最大值为 2^31 - 1。 long 有符号的 64 位整数。最小值为 -2^63，最大值为 2^63 - 1。 unsigned_long 无符号的 64 位整数。最小值为 0，最大值为 2^64 - 1。 short 有符号的 16 位整数。最小值为 -2^15，最大值为 2^15 - 1。 scaled_float 一个浮点值，它会被乘以双精度缩放因子并存储为长整型值。 Integer、long、float 和 double 字段类型都有对应的 范围字段类型。</description></item><item><title>文档列值参数</title><link>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/doc_values/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/doc_values/</guid><description>doc_values 文档值参数 # 默认情况下，Easysearch 会为搜索目的索引大多数字段的字段值。doc_values 参数启用文档到词项的正排查找，用于排序、聚合和脚本等操作。
doc_values 参数接受以下选项。
选项 Option 描述 true true 启用字段的 doc_values。默认值为 true。 false false 禁用字段的 doc_values。 示例：创建启用和禁用 doc_values 的索引 # 以下示例请求创建一个索引，其中一个字段启用 doc_values，另一个字段禁用：
PUT my-index-001 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;status_code&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;session_id&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;, &amp;quot;doc_values&amp;quot;: false } } } }</description></item><item><title>是否启用参数</title><link>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/enabled/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/enabled/</guid><description>Enabled 启用参数 # enabled 参数允许您控制 Easysearch 是否解析字段的内容。此参数可以应用于顶级映射定义和对象字段。
enabled 参数接受以下值：
参数 描述 true 字段被解析和索引。默认值为 true。 false 字段不被解析或索引，但仍可从 _source 字段中检索。当 enabled 设置为 false 时，Easysearch 将字段的值存储在 _source 字段中，但不索引或解析其内容。这对于您想要存储但不需要搜索、排序或聚合的字段很有用。 示例：使用 enabled 参数 # 在以下示例请求中，session_data 字段被禁用。Easysearch 将其内容存储在 _source 字段中，但不对其进行索引或解析：
PUT my-index-002 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;user_id&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;last_updated&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;date&amp;quot; }, &amp;quot;session_data&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;enabled&amp;quot;: false } } } }</description></item><item><title>权重参数设置</title><link>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/boost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/mapping-parameters/boost/</guid><description>Boost 权重参数 # boost 映射参数用于在搜索查询期间增加或减少字段的相关性分数。它允许您在计算文档的整体相关性分数时，对特定字段应用更多或更少的权重。
boost 参数作为字段分数的乘数应用。例如，如果一个字段的 boost 值为 2，则该字段的分数的权重将翻倍。相反，boost 值为 0.5 将使该字段的分数的权重减半。
代码样例 # 以下是在 Easysearch 映射中使用 boost 参数的示例：
PUT my-index1 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;boost&amp;quot;: 2 }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;boost&amp;quot;: 1 }, &amp;quot;tags&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;, &amp;quot;boost&amp;quot;: 1.5 } } } } 在此示例中，title 字段的提升值为 2，这意味着它对整体相关性分数的权重是描述字段（提升值为 1）的两倍。tags 字段的提升值为 1.5，因此它的权重是描述字段的一倍半。
当您想要对某些字段赋予更多权重时，boost 参数特别有用。例如，您可能想要将 title 字段的权重提升得比 description 字段更高，因为标题更能文档的相关性。
boost 参数是一个乘法因子，而不是加法因子。这意味着与具有较低权重值的字段相比，具有较高权重值的字段对整体相关性分数的影响将不成比例地大。在使用 boost 参数时，建议您从小值（1.5 或 2）开始，并测试其对搜索结果的影响。过高的权重值可能会扭曲相关性分数，并导致意外或不理想的搜索结果。</description></item><item><title>源文档属性</title><link>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/source/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/source/</guid><description>Source 源文档属性 # _source 字段包含已索引的原始 JSON 文档主体。虽然此字段不可搜索，但它会被存储，以便在执行获取请求（如 get 和 search）时可以返回完整文档。
禁用_source # 您可以通过将 enabled 参数设置为 false 来禁用 _source 字段，如以下示例所示：
PUT sample-index1 { &amp;quot;mappings&amp;quot;: { &amp;quot;_source&amp;quot;: { &amp;quot;enabled&amp;quot;: false } } } 禁用 _source 字段可能会影响某些功能的可用性，例如 update、update_by_query 和 reindex API，以及使用原始索引文档查询或聚合的能力。
包含或排除某些字段 # 您可以使用 includes 和 excludes 参数选择 _source 字段的内容。如以下示例：
PUT logs { &amp;quot;mappings&amp;quot;: { &amp;quot;_source&amp;quot;: { &amp;quot;includes&amp;quot;: [ &amp;quot;*.count&amp;quot;, &amp;quot;meta.*&amp;quot; ], &amp;quot;excludes&amp;quot;: [ &amp;quot;meta.description&amp;quot;, &amp;quot;meta.other.*&amp;quot; ] } } } 这些字段不会存储在 _source 中，但您仍然可以搜索它们，因为数据仍然被索引。</description></item><item><title>索引属性</title><link>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/</guid><description>Index 索引属性 # 当跨多个索引进行查询时，您可能需要根据文档所在的索引来过滤结果。index 字段根据文档的索引来匹配文档。
以下示例创建两个索引，products 和 customers，并向每个索引添加一个文档：
PUT products/_doc/1 { &amp;quot;name&amp;quot;: &amp;quot;Widget X&amp;quot; } PUT customers/_doc/2 { &amp;quot;name&amp;quot;: &amp;quot;John Doe&amp;quot; } 然后，您可以查询这两个索引，并使用 _index 属性过滤结果，如以下示例请求所示：
GET products,customers/_search { &amp;quot;query&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;_index&amp;quot;: [&amp;quot;products&amp;quot;, &amp;quot;customers&amp;quot;] } }, &amp;quot;aggs&amp;quot;: { &amp;quot;index_groups&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;_index&amp;quot;, &amp;quot;size&amp;quot;: 10 } } }, &amp;quot;sort&amp;quot;: [ { &amp;quot;_index&amp;quot;: { &amp;quot;order&amp;quot;: &amp;quot;desc&amp;quot; } } ], &amp;quot;script_fields&amp;quot;: { &amp;quot;index_name&amp;quot;: { &amp;quot;script&amp;quot;: { &amp;quot;lang&amp;quot;: &amp;quot;painless&amp;quot;, &amp;quot;source&amp;quot;: &amp;quot;doc['_index'].</description></item><item><title>索引生命周期管理</title><link>/easysearch/main/docs/references/management/ilm_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/ilm_api/</guid><description>索引生命周期管理 # 索引生命周期管理（Index Lifecycle Management, ILM）为您提供了一种集成化、自动化的方式来高效管理时序数据。 通过配置 ILM 策略，您可以根据性能、可用性与数据保留需求，自动执行索引的滚动、归档和清理等操作。
典型应用场景 # 自动滚动生成新索引：当现有索引达到指定大小或文档数量时，自动创建新索引； 周期性轮换索引：按天、周或月创建新索引，并将历史索引归档； 强制数据保留策略：自动删除过期索引，确保合规与存储成本可控。 从 1.15.2 版本开始，index-management 已经成为 modules 的一部分，不需要单独安装插件。
创建策略 # 引入版本 1.0
创建一个策略。
请求示例 # PUT _ilm/policy/ilm_test { &amp;quot;policy&amp;quot;: { &amp;quot;phases&amp;quot;: { &amp;quot;hot&amp;quot;: { &amp;quot;min_age&amp;quot;: &amp;quot;0ms&amp;quot;, &amp;quot;actions&amp;quot;: { &amp;quot;rollover&amp;quot;: { &amp;quot;max_age&amp;quot;: &amp;quot;10m&amp;quot;, &amp;quot;max_size&amp;quot;: &amp;quot;1mb&amp;quot;, &amp;quot;max_docs&amp;quot;: 100 }, &amp;quot;set_priority&amp;quot;: { &amp;quot;priority&amp;quot;: 100 } } }, &amp;quot;delete&amp;quot;: { &amp;quot;min_age&amp;quot;: &amp;quot;15m&amp;quot;, &amp;quot;actions&amp;quot;: { &amp;quot;delete&amp;quot;: { } } } } } } 示例响应 # { &amp;quot;acknowledged&amp;quot;: true } 应用生命周期策略到索引模板 要让策略生效，需要在索引模板中指定策略名称和滚动索引的别名。</description></item><item><title>自动补全字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/autocomplete-field-type/completion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/autocomplete-field-type/completion/</guid><description>Completion 自动补全字段类型 # 自动补全字段类型通过补全建议器提供自动补全功能。补全建议器是一个前缀建议器，所以它只匹配文本的开头部分。补全建议器会创建一个内存中的数据结构，这提供了更快的查找速度，但会导致内存使用增加。在使用此功能之前，你需要将所有可能的补全项上传到索引中。
代码样例 # 创建一个包含补全字段的映射：
PUT chess_store { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;suggestions&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;completion&amp;#34; }, &amp;#34;product&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;keyword&amp;#34; } } } } 将建议内容索引到 Easysearch 中：
PUT chess_store/_doc/1 { &amp;#34;suggestions&amp;#34;: { &amp;#34;input&amp;#34;: [&amp;#34;Books on openings&amp;#34;, &amp;#34;Books on endgames&amp;#34;], &amp;#34;weight&amp;#34; : 10 } } 参数 # 下表列出了补全字段接受的参数。
参数 描述 input 可能的补全项列表，可以是字符串或字符串数组。不能包含 \u0000 (null),\u001f (信息分隔符一) 或 \u001e (信息分隔符二)。必需。 weight 用于对建议进行排序的正整数或正整数字符串。可选。 可以按以下方式索引多个建议：</description></item><item><title>范围字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/range-field-type/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/range-field-type/</guid><description>范围（Range）字段类型 # 以下表格列出了 Easysearch 支持的所有范围字段类型。
字段数据类型 描述 integer_range 整数值范围。 long_range 长整型值范围。 double_range 双精度浮点值范围。 float_range 浮点值范围。 ip_range IPv4 或 IPv6 地址范围，起始和结束地址可使用不同格式。 date_range 日期值范围，起始和结束日期可采用不同格式。内部以 64 位无符号整数存储，自纪元以来的毫秒数表示。 参考代码 # 创建一个有双精度浮点数范围字段和日期范围字段的映射
PUT testindex { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;gpa&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;double_range&amp;quot; }, &amp;quot;graduation_date&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;date_range&amp;quot;, &amp;quot;format&amp;quot; : &amp;quot;strict_year_month||strict_year_month_day&amp;quot; } } } } 索引一个包含这两个字段的文档</description></item><item><title>资源扩容</title><link>/easysearch/main/docs/getting-started/install/operator/resource_manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/operator/resource_manager/</guid><description>资源扩容 # 查看当前 cpu、mem 和磁盘资源情况
kubectl get sts/threenodes-masters -o yaml resources: requests: cpu: &amp;#34;1&amp;#34; memory: 3Gi limits: cpu: &amp;#34;1&amp;#34; memory: 5Gi resources: requests: storage: 30Gi volumeMode: Filesystem 磁盘（磁盘扩容依赖于实际的 StorageClass，需要 StorageClass 本身支持扩容）
修改 Operator yaml 文件，执行 apply 操作
resources: requests: cpu: &amp;#34;1&amp;#34; memory: 4Gi limits: cpu: &amp;#34;2&amp;#34; memory: 6Gi resources: requests: storage: 50Gi 滚动更新中： 从 threenodes-masters-0 开始更新
threenodes-masters-0 更新完毕后，依次更新 threenodes-masters-1、threenodes-masters-2
最终全部更新完毕
查看更新后的资源情况：
可以发现，结果与预期的一致</description></item><item><title>跨集群复制</title><link>/easysearch/main/docs/references/management/ccr_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/ccr_api/</guid><description>跨集群复制 # 使用跨集群复制 API 管理跨集群复制。
在跨集群复制中，可以将数据索引到一个领导者索引，然后 Easysearch 将这些数据复制到一个或多个只读的跟随者索引。所有在领导者上进行的后续操作都会在跟随者上复制，例如创建、更新或删除文档。
先决条件 # 1.11.1 版本之前，leader 和 follower 集群都必须安装 cross-cluster-replication 插件和 index-management 插件，1.11.1 版本开始，已经内置了 cross-cluster-replication 模块。 从1.15.2版本开始，cross-cluster-replication 和 index-management 都已经内置到 modules，不再需要安装。 如果 follower 集群的 easysearch.yml 文件中覆盖了 node.roles，确保它也包括 remote_cluster_client 角色，默认启用。 node.roles: [&amp;lt;other_roles&amp;gt;, remote_cluster_client] 权限 # 确保安全功能在两个集群上都启用或都禁用。如果启用了安全功能，确保非管理员用户被映射到适当的权限，以便他们可以执行复制操作。 部署示例集群 # 在本地起 2 个单节点的 easysearch 测试集群，分别是 follower-application (9201 端口) 和 leader-application (9200 端口) 在 easysearch.yml 添加 discovery.type: single-node 如果启用 security 功能，确保 2 个集群的证书互信，测试环境可以直接合并 2 个节点的 ca 证书： 例如 cat ca.</description></item><item><title>路由属性</title><link>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/routing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/metadata-field/routing/</guid><description>Routing 路由属性 # Easysearch 使用哈希算法将文档路由到索引中的特定分片。默认情况下，文档的 _id 字段用作路由值，但您也可以为每个文档指定自定义路由值。
默认路由 # 以下是 Easysearch 的默认路由公式。_routing 值是文档的 _id。
shard_num = hash(_routing) % num_primary_shards 自定义路由 # 您可以在索引文档时指定自定义路由值，如以下示例所示：
PUT sample-index1/_doc/1?routing=JohnDoe1 { &amp;quot;title&amp;quot;: &amp;quot;This is a document&amp;quot; } 在此示例中，文档使用的路由值是 JohnDoe1 而不是默认的 _id 。
在检索、删除或更新文档时，您必须提供相同的路由值，如以下示例所示：
GET sample-index1/_doc/1?routing=JohnDoe1 通过路由查询 # 您可以使用 _routing 字段根据文档的路由值进行查询，如以下示例所示。此查询仅搜索与 JohnDoe1 路由值关联的分片：
GET sample-index1/_search { &amp;quot;query&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;_routing&amp;quot;: [ &amp;quot;JohnDoe1&amp;quot; ] } } } 设置路由为必需项 # 您可以使索引上的所有 CRUD 操作都必需提供路由值，如以下示例。如果您尝试在不提供路由值的情况下索引文档，Easysearch 将抛出异常。</description></item><item><title>Wildcard 字段类型</title><link>/easysearch/main/docs/references/mappings-and-field-types/field-types/wildcard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/mappings-and-field-types/field-types/wildcard/</guid><description>Wildcard 字段类型 # wildcard（通配符）字段是keyword（关键字）字段的一种变体，专为任意子字符串和正则表达式匹配而设计。
当您的内容由&amp;quot;字符串&amp;quot;而非&amp;quot;文本&amp;quot;组成时，应使用wildcard字段。示例包括非结构化日志行和计算机代码。
wildcard字段类型的索引方式与keyword字段类型不同。keyword字段将原始字段值写入索引，而wildcard字段类型则将字段值拆分为长度小于或等于3的子字符串，并将这些子字符串写入索引。例如，字符串test被拆分为t、te、tes、e、es和est这些子字符串。
在搜索时，将查询模式中所需的子字符串与索引进行匹配以生成候选文档，然后根据查询中的模式对这些文档进行过滤。例如，对于搜索词test，OpenSearch执行索引搜索tes AND est。如果搜索词包含少于三个字符，OpenSearch会使用长度为一或二的字符子字符串。对于每个匹配的文档，如果源值为test，则该文档将出现在结果中。这样可以排除误报值，如nikola tesla felt alternating current was best。
通常，精确匹配查询（如 term或 terms查询）在wildcard字段上的表现不如在keyword字段上有效， 而 wildcard、 prefix和 regexp查询在wildcard字段上表现更好。
示例 # 创建带有 wildcard 字段的映射：
PUT logs { &amp;#34;mappings&amp;#34; : { &amp;#34;properties&amp;#34; : { &amp;#34;log_line&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;wildcard&amp;#34; } } } } 参数 # 以下表格列出了 wildcard 字段可用的所有参数。 `
参数 描述 doc_values 布尔值，指定该字段是否应存储在磁盘上，以便用于聚合、排序或脚本操作。默认值为 false。 ignore_above 长度超过此整数值的任何字符串都不会被索引。默认值为 2147483647。 normalizer 用于预处理索引和搜索值的标准化器。默认情况下，不进行标准化，使用原始值。您可以使用 lowercase 标准化器在该字段上执行不区分大小写的匹配。 null_value 用于替代 null 的值。必须与字段类型相同。如果未指定此参数，则当字段值为 null 时，该字段将被视为缺失。默认值为 null。</description></item><item><title>查询模版</title><link>/easysearch/main/docs/references/search/search-template/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/search-template/</guid><description>查询模版 # 您可以将全文查询转换为查询模版，以接受用户输入并将其动态插入到查询中。
例如，如果您使用 Easysearch 作为应用程序或网站的后端搜索引擎，则可以从搜索栏或表单字段接收用户查询，并将其作为参数传递到查询模版中。这样，创建 Easysearch 查询的语法就从最终用户那里抽象出来了。
当您编写代码将用户输入转换为 Easysearch 查询时，可以使用查询模版简化代码。如果需要将字段添加到搜索查询中，只需修改模板即可，而无需更改代码。
查询模版使用 Mustache 语言。有关所有语法选项的列表，请参阅 Mustache 手册。
创建查询模版 # 查询模版有两个组件：查询和参数。参数是放置在变量中的用户输入值。在 Mustache 符号中，变量用双括号表示。当在查询中遇到类似 {% raw %}{{var}}{% endraw %} 的变量时，Easysearch 会转到 params 部分，查找名为 var 的参数，并用指定的值替换它。
您可以编写应用程序代码，询问用户要搜索什么，然后在运行时将该值插入 params 对象中。
此命令定义了一个查询模版，用于按名称查找播放。查询中的 {% raw %}{{play_name}}{% endraw %} 被值 Henry IV 替换：
GET _search/template { &amp;#34;source&amp;#34;: { &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;play_name&amp;#34;: &amp;#34;{% raw %}{{play_name}}{% endraw %}&amp;#34; } } }, &amp;#34;params&amp;#34;: { &amp;#34;play_name&amp;#34;: &amp;#34;Henry IV&amp;#34; } } 此模板在整个集群上运行搜索。</description></item><item><title>异步搜索</title><link>/easysearch/main/docs/references/search/async_search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/async_search/</guid><description>异步搜索 # 搜索大量数据可能会花费很长时间，尤其是当你在热节点或者多个远程集群中进行搜索时。
Easysearch 中的异步搜索允许你发送在后台运行的搜索请求。你可以监控这些搜索的进度，并且在部分结果可用时获取这些部分结果。在搜索完成之后，你可以保存结果以便日后查看。
先决条件 # Easysearch 从 1.11.1 版本开始，内置支持异步搜索。
REST API # 引入版本 1.11.0
要执行异步搜索，请向 /{index}/_async_search 发送请求，并在请求正文中包含您的查询：
POST test-index/_asynch_search 可以指定以下选项。
选项 描述 默认值 是否必填 wait_for_completion_timeout 计划等待结果的时间。在此时间内，您可以像在普通搜索中一样查看所获得的结果。您可以根据ID轮询剩余的结果。最大值为300秒。 1秒 否 keep_on_completion 搜索完成后，您是否希望将结果保存在集群中。您可以在稍后查看存储的结果。 false 否 keep_alive 结果在集群中保存的时间。例如，2d 表示结果在集群中存储48小时。保存的搜索结果在此时间段结束后或如果搜索被取消时将被删除。请注意，这包括查询执行时间。如果查询超过此时间，进程将自动取消该查询。 12小时 否 index 要搜索的索引名称。可以是单个名称、用逗号分隔的索引列表，或索引名称的通配符表达式。 集群中的所有索引 否 请求示例 # POST test-index/_async_search?wait_for_completion_timeout=1ms&amp;amp;keep_on_completion=true { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;张三&amp;quot; } } } 示例响应 # { &amp;quot;id&amp;quot;: &amp;quot;FmFqN0llTXlKVHF5cnV1NGdVNUlPancEMzMzMBRaOUNxU3BVQlRIdzczZmJfNnZtRQIyMA==&amp;quot;, &amp;quot;state&amp;quot;: &amp;quot;RUNNING&amp;quot;, &amp;quot;start_time_in_millis&amp;quot;: 1740714470020, &amp;quot;expiration_time_in_millis&amp;quot;: 1740800870020, &amp;quot;response&amp;quot;: { &amp;quot;took&amp;quot;: 0, &amp;quot;timed_out&amp;quot;: false, &amp;quot;num_reduce_phases&amp;quot;: 0, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 0, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;max_score&amp;quot;: null, &amp;quot;hits&amp;quot;: [] } } } 响应参数 # 选项 描述 id 异步搜索的ID。使用此ID来监控搜索进度、获取其部分结果和/或删除结果。如果异步搜索在超时期限内完成，响应中不包含ID，因为结果未存储在集群中。 state 指定搜索是仍在运行还是已经完成，以及结果是否在集群中持久保存。可能的状态有 RUNNING（运行中）、SUCCEEDED（成功）、FAILED（失败）、PERSISTING（正在持久化）、PERSIST_SUCCEEDED（持久化成功）、PERSIST_FAILED（持久化失败）、CLOSED（已关闭）和 STORE_RESIDENT（存储驻留）。 start_time_in_millis 开始时间，单位为毫秒。 expiration_time_in_millis 过期时间，单位为毫秒。 took 搜索运行的总时长。 response 实际的搜索响应。 num_reduce_phases 协调节点从分片响应批次中聚合结果的次数（默认值为5）。如果与上次检索到的结果相比，此数字增加，您可以预期搜索响应中将包含额外的结果。 total 执行搜索的分片总数。 successful 协调节点成功接收到的分片响应数量。 aggregations 分片到目前为止已完成的聚合部分结果。 获取部分结果 # 提交异步搜索请求后，您可以使用在异步搜索响应中看到的ID请求部分响应。</description></item><item><title>定点查询</title><link>/easysearch/main/docs/references/search/pit_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/search/pit_api/</guid><description>定点查询 # 定点查询，也称为 Point in Time 搜索，具有与常规搜索相同的功能，不同之处在于 PIT 搜索作用于较旧的数据集，而常规搜索作用于实时数据集。PIT 搜索不绑定于特定查询，因此您可以在同一个冻结在时间点上的数据集上运行不同的查询。
您可以使用创建 PIT API 来创建 PIT。当您为一组索引创建 PIT 时，Easysearch 会锁定这些索引的一组段，使它们在时间上冻结。在底层，此 PIT 所需的资源不会被修改或删除。 如果作为 PIT 一部分的段被合并，Easysearch 会在 PIT 创建时通过 keep_alive 参数指定的时间段内保留这些段的副本。
创建 PIT 操作会返回一个 PIT ID，您可以使用该 ID 在冻结的数据集上运行多个查询。即使索引继续摄取数据并修改或删除文档，PIT 引用的数据自 PIT 创建以来不会发生变化。当您的查询包含 PIT ID 时， 您不需要将索引传递给搜索，因为它将使用该 PIT。使用 PIT ID 的搜索在多次运行时将产生完全相同的结果。
创建 PIT # 创建一个 PIT。查询参数 keep_alive 是必需的；它指定了保持 PIT 的时间长度。
端点 # POST /&amp;lt;target_indexes&amp;gt;/_pit?keep_alive=1h&amp;amp;routing=&amp;amp;expand_wildcards=&amp;amp;preference= 路径参数 # 参数 数据类型 描述 target_indexes 字符串 PIT 的目标索引名称。可以包含以逗号分隔的列表或通配符索引模式。 查询参数 # 参数 数据类型 描述 keep_alive 时间 保持 PIT 的时间长度。每次使用搜索 API 访问 PIT 时，PIT 的生命周期都会延长一段等于 keep_alive 参数的时间。必需。 preference 字符串 用于执行搜索的节点或分片。可选。默认为随机。 routing 字符串 指定将搜索请求路由到特定分片。可选。默认为文档的 _id。 expand_wildcards 字符串 可匹配通配符模式的索引类型。支持逗号分隔的值。有效值如下：</description></item><item><title>备份还原</title><link>/easysearch/main/docs/references/management/snapshot-restore/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/snapshot-restore/</guid><description>备份还原 # 快照是集群索引和状态的备份。状态包括集群设置、节点信息、索引设置和分片的信息。
快照有两个主要用途：
从故障中恢复
例如，如果集群运行状况变为红色，则可以从快照恢复红色索引。
从一个群集迁移到另一个群集
例如，如果您要从概念验证迁移到生产集群，您可以拍摄前者的快照并在后者上进行恢复。
关于快照(snapshots) # 快照不是即时的。它们需要时间来完成，并不代表集群的完美时间点视图。当快照正在进行时，您仍然可以为文档编制索引并向集群发出其他请求，但快照中通常不包括新文档和对现有文档的更新。快照包括 Easysearch 启动快照时存在的主碎片。根据快照线程池的大小，快照中可能会在稍微不同的时间包含不同的碎片。
Easysearch 快照是增量的，这意味着它们只存储自上次成功快照以来已更改的数据。频繁快照和不频繁快照之间的磁盘使用率差异通常很小。
换句话说，一周内每小时拍摄一次快照（总共拍摄 168 个快照）可能不会比周末拍摄一个快照占用更多的磁盘空间。此外，拍摄快照的频率越高，完成快照所需的时间越短。一些 Easysearch 用户每半小时拍摄一次快照。
如果需要删除快照，请确保使用 Easysearch API，而不是导航到存储位置并清除文件。集群中的增量快照通常共享大量相同的数据；使用 API 时， Easysearch 仅删除其他快照未使用的数据。 {: .tip }
注册快照存储库 # 在拍摄快照之前，必须“注册”快照存储库。快照存储库只是一个存储位置：共享文件系统、 Amazon S3 、 Hadoop 分布式文件系统（HDFS）、 Azure 存储等。
Shared file system # 要将共享文件系统用作快照存储库，请将其添加到 easysearch.yml： path.repo: [&amp;#34;/mnt/snapshots&amp;#34;] 在 RPM 和 Debian 安装中，您可以安装文件系统。如果您使用 Docker 安装，请在启动集群之前，将文件系统添加到 docker-compose.yml 中的每个节点：
volumes: - /Users/jdoe/snapshots:/mnt/snapshots T1.</description></item><item><title>写入限流</title><link>/easysearch/main/docs/references/management/throttling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/throttling/</guid><description>写入限流 # Easysearch 支持节点级别和分片级别的写入限流功能，可以将 bulk 操作对集群的压力，限制在可接受的范围。
最低版本 # 1.8.0
限流参数设置 # 以下是 Easysearch 集群级别的限流设置，并且是动态的，您可以更改此功能的默认行为，而无需重新启动集群。
限流参数说明
名称 类型 说明 默认值 cluster.throttle.node.write boolean 是否启用节点级别限流 false cluster.throttle.node.write.max_requests int 限定时间范围内单个节点允许的最大写入请求次数 0 cluster.throttle.node.write.max_bytes 字符串 限定时间范围内单个节点允许的最大写入请求字节数（kb, mb, gb 等） 0mb cluster.throttle.node.write.action 字符串 触发限速之后的处理动作，分为 retry 和 drop 两种，默认为 drop drop cluster.throttle.node.write.interval int 节点级别评估限速的单位时间间隔，默认为 1s 1 cluster.throttle.shard.write boolean 是否启用分片级别限流 false cluster.</description></item><item><title>可搜索快照</title><link>/easysearch/main/docs/references/management/searchable_snapshot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/searchable_snapshot/</guid><description>可搜索快照 # 可搜索快照索引在实时搜索时从快照存储库中按需读取数据，而不是在恢复时将所有索引数据下载到集群存储中。由于索引数据仍然保持在快照格式中存储在存储库中，因此可搜索快照索引本质上是只读的。 任何尝试写入可搜索快照索引的操作都会导致错误。
可搜索快照功能采用了诸如在集群节点中缓存频繁使用的数据段以及删除集群节点中最不常使用的数据段等技术，以便为频繁使用的数据段腾出空间。从快照下载的数据段存储在块存储中，与集群节点的通用索引并存。 因此，集群节点的计算能力在索引、本地搜索和存储在低成本对象存储，例如 Amazon Simple Storage Service（Amazon S3）上的快照数据段之间共享。 尽管集群节点的资源利用效率要高得多，但大量的任务将导致快照搜索变得较慢且持续时间较长。节点的本地存储也用于缓存快照数据。
将节点配置为使用可搜索快照 # 只有角色为 search 的节点才能进行快照搜索，要启用可搜索快照功能，请在您的 easysearch.yml 文件中创建一个节点，并将节点角色定义为 &amp;ldquo;search&amp;rdquo;：
node.name: snapshots-node node.roles: [search] 如果您正在运行 Docker，可以通过在您的 docker-compose.yml 文件中添加以下行来创建一个具有搜索节点角色的节点：
- node.roles: [search] 创建可搜索的快照索引 # 可搜索的快照索引是通过使用 _restore API 并指定 remote_snapshot 存储类型来创建的。
storage_type:
local 表示所有快照的元数据和索引数据都将下载到本地存储。 remote_snapshot 表示快照的元数据将下载到集群，但远程存储库将保持索引数据的权威存储。数据将根据需要下载和缓存以提供查询服务。为了使用 remote_snapshot 类型还原快照，集群中至少必须配置一个节点具有 search 节点角色。 使用示例 # 下面我们以 MinIO 作为快照存储仓库，Minio 是专为云应用程序开发人员和 DevOps 构建的对象存储服务器，与 Amazon S3 对象存储兼容。
使用 _snapshot API 注册 MinIO 存储库</description></item><item><title>使用时间范围合并策略优化时序索引</title><link>/easysearch/main/docs/references/management/time-series-Index-optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/time-series-Index-optimization/</guid><description>使用时间范围合并策略优化时序索引 # 在处理时序数据（如日志、监控指标、事件流）时，数据通常具有明显的时间先后顺序。Easysearch 底层的 Lucene Segment 合并是保证搜索性能和资源效率的关键操作。 然而，默认的合并策略（如 TieredMergePolicy）主要基于 Segment 的大小和删除文档比例来决定合并哪些 Segment，它并不感知数据的时间属性。
对于时序场景，这种默认策略可能导致：
冷热数据混合合并：较旧的（冷）数据 Segment 可能与较新的（热）数据 Segment 合并，导致不必要的 I/O 和 CPU 开销，因为冷数据通常访问频率低，合并它们带来的收益有限。
查询性能影响：跨时间范围的大 Segment 可能降低某些按时间范围过滤的查询效率。
为了解决这些问题，Easysearch 从 1.12.1 版本开始引入了基于时间范围的合并策略 (TimeRangeMergePolicy)，专门为时序索引优化 Segment 合并行为。
最低版本 # 1.12.1
核心概念：TimeRangeMergePolicy # TimeRangeMergePolicy 是一种特殊的合并策略，它在选择要合并的 Segment 时，除了考虑大小、删除比例等因素外，优先考虑 Segment 所覆盖的时间范围。
其核心思想是：
时间优先：倾向于合并时间上相邻的 Segment。
保留时间分区：尽量避免将时间跨度很大的 Segment 合并在一起，保持数据的“时间局部性”。
优先合并新数据：通常，新写入的数据变化更频繁（包括更新、删除），优先合并较新的 Segment 有助于更快地回收空间和优化最新数据的查询性能。
如何启用 # 要为你的时序索引启用时间范围合并策略，你需要更新索引的设置，指定用于时间排序的字段名。
步骤： # 确认时间字段：确保你的索引 Mapping 中有一个合适的日期或时间戳类型的字段（如 @timestamp、event_time 等），并且该字段准确反映了数据的时间属性。 更新索引设置：使用 Index Settings API 来设置 index.</description></item><item><title>数据汇总</title><link>/easysearch/main/docs/references/management/rollup_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/rollup_api/</guid><description>数据汇总 # 数据汇总或上卷（Rollup），对于时序场景类的数据，往往会有大量的非常详细的聚合指标，随着时间的图推移，存储将持续增长。汇总功能可以将旧的、细粒度的数据汇总为粗粒度格式以进行长期存储。通过将数据汇总到一个单一的文档中，可以大大降低历史数据的存储成本。 Easysearch 的 rollup 具备一些独特的优势，可以自动对 rollup 索引进行滚动而不用依赖其他 API 去单独设置，并且在进行聚合查询时支持直接搜索原始索引，做到了对业务端的搜索代码完全兼容，从而对用户无感知。
支持的聚合类型 # 对数值类型字段支持的聚合
avg sum max min value_count percentiles 对 keyword 类型字段提供 terms 聚合。
对 date 类型字段 除了 date_histogram 聚合，还支持 date_range 聚合。(v1.10.0)
查询 rollup 数据时，增加支持 Filter aggregation，某些场景可以用来替代 query 过滤数据。(v1.10.1)
增加针对个别字段自定义 special_metrics 指标的配置项。 (v1.10.1)
增加支持 Bucket sort aggregation。 (v1.10.1)
混合查询原始索引和 rollup 索引时，返回的 response 里增加了 origin 参数，表示包含 rollup 数据。(v1.10.1)
Rollup 查询 API 提供了 debug 参数，显示 Easysearch 内部执行的查询语句。(v1.10.1)</description></item><item><title>权限列表</title><link>/easysearch/main/docs/references/security/access-control/permissions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/security/access-control/permissions/</guid><description>权限列表 # 此页面是可用权限的完整列表。每个权限控制对数据类型或 API 的访问。
集群权限 # cluster:admin/ingest/pipeline/delete cluster:admin/ingest/pipeline/get cluster:admin/ingest/pipeline/put cluster:admin/ingest/pipeline/simulate cluster:admin/ingest/processor/grok/get cluster:admin/reindex/rethrottle cluster:admin/repository/delete cluster:admin/repository/get cluster:admin/repository/put cluster:admin/repository/verify cluster:admin/reroute cluster:admin/script/delete cluster:admin/script/get cluster:admin/script/put cluster:admin/settings/update cluster:admin/snapshot/create cluster:admin/snapshot/delete cluster:admin/snapshot/get cluster:admin/snapshot/restore cluster:admin/snapshot/status cluster:admin/snapshot/status* cluster:admin/tasks/cancel cluster:admin/tasks/test cluster:admin/tasks/testunblock cluster:monitor/allocation/explain cluster:monitor/health cluster:monitor/main cluster:monitor/nodes/hot_threads cluster:monitor/nodes/info cluster:monitor/nodes/liveness cluster:monitor/nodes/stats cluster:monitor/nodes/usage cluster:monitor/remote/info cluster:monitor/state cluster:monitor/stats cluster:monitor/task cluster:monitor/task/get cluster:monitor/tasks/list 索引权限 # indices:admin/aliases indices:admin/aliases/exists indices:admin/aliases/get indices:admin/analyze indices:admin/cache/clear indices:admin/close indices:admin/create indices:admin/delete indices:admin/exists indices:admin/flush indices:admin/flush* indices:admin/forcemerge indices:admin/get indices:admin/mapping/put indices:admin/mappings/fields/get indices:admin/mappings/fields/get* indices:admin/mappings/get indices:admin/open indices:admin/refresh indices:admin/refresh* indices:admin/resolve/index indices:admin/rollover indices:admin/seq_no/global_checkpoint_sync indices:admin/settings/update indices:admin/shards/search_shards indices:admin/shrink indices:admin/synced_flush indices:admin/template/delete indices:admin/template/get indices:admin/template/put indices:admin/types/exists indices:admin/upgrade indices:admin/validate/query indices:data/read/explain indices:data/read/field_caps indices:data/read/field_caps* indices:data/read/get indices:data/read/mget indices:data/read/mget* indices:data/read/msearch indices:data/read/msearch/template indices:data/read/mtv indices:data/read/mtv* indices:data/read/scroll indices:data/read/scroll/clear indices:data/read/search indices:data/read/search* indices:data/read/search/template indices:data/read/tv indices:data/write/bulk indices:data/write/bulk* indices:data/write/delete indices:data/write/delete/byquery indices:data/write/index indices:data/write/reindex indices:data/write/update indices:data/write/update/byquery indices:monitor/recovery indices:monitor/segments indices:monitor/settings/get indices:monitor/shard_stores indices:monitor/stats indices:monitor/upgrade 权限集合 # 为了提高权限设置的效率，我们对系统权限进行自定义管理，从而快速批量选择一组相关的权限，而不是分别选择单个权限，为了方便，系统内置了若干权限集合。</description></item><item><title>其它常用 API</title><link>/easysearch/main/docs/references/management/popular-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/popular-api/</guid><description>其它常用 API # 此页面包含 Easysearch 常用 API 的示例请求。
使用非默认设置创建索引 # PUT my-logs { &amp;#34;settings&amp;#34;: { &amp;#34;number_of_shards&amp;#34;: 4, &amp;#34;number_of_replicas&amp;#34;: 2 }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;title&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34; }, &amp;#34;year&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;integer&amp;#34; } } } } 索引单个文档并自动生成随机 ID # POST my-logs/_doc { &amp;#34;title&amp;#34;: &amp;#34;Your Name&amp;#34;, &amp;#34;year&amp;#34;: &amp;#34;2016&amp;#34; } 索引单个文档并指定 ID # PUT my-logs/_doc/1 { &amp;#34;title&amp;#34;: &amp;#34;Weathering with You&amp;#34;, &amp;#34;year&amp;#34;: &amp;#34;2019&amp;#34; } 一次索引多个文档 # 请求正文末尾的空白行是必填的。如果省略 _id 字段， Easysearch 将生成一个随机 id 。</description></item><item><title>数据流</title><link>/easysearch/main/docs/references/management/data-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/management/data-streams/</guid><description>数据流（Data streams） # 如果你正在将连续生成的时间序列数据（如日志、事件和指标）摄入 Easysearch，那么你很可能处于这样一种场景：文档数量快速增长，且你无需更新旧文档。
管理时间序列数据的典型工作流程包含多个步骤，例如创建滚动索引别名、定义写入索引，以及为底层索引定义通用的映射和设置。
数据流简化了这一过程，并强制采用最适合时间序列数据的配置方式，例如主要为仅追加（append-only）数据设计，并确保每个文档都包含一个时间戳字段。
数据流在内部由多个底层索引组成。搜索请求会被路由到所有底层索引，而写入请求则被路由到最新的写入索引。通过 索引生命周期管理（ILM） 策略，你可以自动处理索引滚动（rollover）或删除操作。
数据流使用说明 # 步骤 1：创建索引模板 # 要创建数据流，首先需要创建一个索引模板，用于将一组索引配置为数据流。data_stream 对象表明这是一个数据流，而非普通索引模板。索引模式需与数据流的名称匹配：
PUT _index_template/logs-template-nginx { &amp;quot;index_patterns&amp;quot;: &amp;quot;logs-nginx&amp;quot;, &amp;quot;data_stream&amp;quot;: { }, &amp;quot;priority&amp;quot;: 200, &amp;quot;template&amp;quot;: { &amp;quot;settings&amp;quot;: { &amp;quot;number_of_shards&amp;quot;: 1, &amp;quot;number_of_replicas&amp;quot;: 0 } } } 在此情况下，每个摄入的文档都必须包含一个 @timestamp 字段。
你也可以在 data_stream 对象中自定义时间戳字段名称。此外，你还可以在此处定义索引映射和其他设置，就像为普通索引模板所做的那样。
PUT _index_template/logs-template-nginx { &amp;quot;index_patterns&amp;quot;: &amp;quot;logs-nginx&amp;quot;, &amp;quot;data_stream&amp;quot;: { &amp;quot;timestamp_field&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;request_time&amp;quot; } }, &amp;quot;priority&amp;quot;: 200, &amp;quot;template&amp;quot;: { &amp;quot;settings&amp;quot;: { &amp;quot;number_of_shards&amp;quot;: 1, &amp;quot;number_of_replicas&amp;quot;: 0 } } } 在此示例中，logs-nginx 索引会匹配 logs-template-nginx 模板。当存在多个匹配时，Easysearch 会选择优先级更高的模板。</description></item><item><title>系统调优</title><link>/easysearch/main/docs/getting-started/settings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/settings/</guid><description>系统调优 # 芯片及操作系统兼容性 # 目前已在国产主流芯片及操作系统上进行了验证，分别为 openEuler、统信 UOS、麒麟、龙芯、申威、兆芯。同样也兼容 Windows、 MacOS、 CentOS、 Ubuntu、 RedHat 等常用操作系统。
Java 兼容性 # 默认情况下 Easysearch 并不包含 JDK, 推荐使用 Java 15.0.1+9 或 Java 17.0.6+10, 最低版本要求为 Java 11, 要使用不同的 Java 安装，请将 JAVA_HOME 环境变量设置为 Java 安装位置或将 JDK 软链接到 Easysearch 安装目录下取名为 jdk。
例如：
#设置 JAVA_HOME 环境变量，可放入 ~/.bashrc 或 /etc/profile export JAVA_HOME=/usr/local/jdk #软链接 sudo ln -s /usr/local/jdk /data/easysearch/jdk 网络要求 # Easysearch 需要打开以下端口:
端口 模块说明 9200 REST API 9300 节点间通信 系统参数 # 要保证 Easysearch 运行在最佳状态，其所在服务器的操作系统也需要进行相应的调优，以 Linux 为例。</description></item><item><title>节点扩容</title><link>/easysearch/main/docs/getting-started/install/operator/node_scale/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/operator/node_scale/</guid><description>节点扩容 # 与上述 cpu mem disk 扩容一样，只需要修改 Operator yaml 文件中的 replicas 字段值即可。 这里修改为 5 个节点，并 apply，将会并发创建新的节点：threenodes-masters-3, threenodes-masters-4
最终完成节点扩容。</description></item><item><title>密码修改</title><link>/easysearch/main/docs/getting-started/install/operator/update_password/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/operator/update_password/</guid><description>密码修改 # Operator 将 Easysearch 的密码保存在 k8s 的 Secret 中，查看已有的 Secret
现在准备修改密码，编辑 admin-credentials-secret.yaml 文件，并 apply
apiVersion: v1 kind: Secret metadata: name: threenodes-admin-password type: Opaque data: # admin username: YWRtaW4= # admin123 password: YWRtaW4xMjM= operator 感知到 threenodes-admin-password 有变化后，会检查账号密码是否有更新（通过检查账号密码生成的 hash 值是否与 job 的 annotations: &amp;ldquo;securityconfig/checksum&amp;rdquo; 值相同来判断），如果有更新则重新执行 job（集群名称-securityconfig-update）,这里的名称是 threenodes-securityconfig-update
继续查看这个 job 的 spec
可以知道，这个 job 本质上也是 Easysearch，但是它执行 shell 命令来修改集群的密码：
until curl -k -XPUT --cert admin-credentials/tls.crt --key admin-credentials/tls.key \ -H &amp;#39;Content-Type: application/json&amp;#39; &amp;#39;https://threenodes.default.svc.cluster.local:9200/_security/user/admin \ -d &amp;#39; { &amp;#34;password&amp;#34;: &amp;#34;admin&amp;#34;, &amp;#34;external_roles&amp;#34;: [&amp;#34;admin&amp;#34;] }&amp;#39;;echo &amp;#39;Waiting to connect to the cluster&amp;#39;; sleep 60; 修改密码后，再次查看 secret： 可知密码已经修改为目标密码。</description></item><item><title>常用配置</title><link>/easysearch/main/docs/getting-started/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/configuration/</guid><description>常用配置 # 大多数 Easysearch 配置都可以通过集群设置 API 进行更改，某些配置则需要修改 easysearch.yml 并重新启动集群。
easysearch.yml 对每个节点都是本地的，因此应尽可能使用集群设置 REST API, 将设置应用于集群中的所有节点，一般我们采用开发者工具来进行操作。
集群设置 API # 第一步是查看当前设置：
GET _cluster/settings?include_defaults=true 查看用户自进行的自定义设置
GET _cluster/settings 集群设置 API 中存在三类设置：持久（Persistent）、临时（Transient）和默认。持久设置在集群重新启动后仍然存在。重新启动后，Easysearch 会清临时设置。
如果在多个位置指定相同的设置，Easysearch 将使用以下优先级来读取配置：
Transient 设置 Persistent 设置 配置文件 easysearch.yml 默认设置 要更改设置，只需将新设置指定为持久或临时。采用单层 JSON 形式：
PUT /_cluster/settings { &amp;#34;persistent&amp;#34; : { &amp;#34;action.auto_create_index&amp;#34; : false } } 同样也可以使用多层 JSON 形式：
PUT /_cluster/settings { &amp;#34;persistent&amp;#34;: { &amp;#34;action&amp;#34;: { &amp;#34;auto_create_index&amp;#34;: false } } } 配置目录包括许多安全相关的设置。要了解更多信息，请参阅 安全配置。</description></item><item><title>版本升级</title><link>/easysearch/main/docs/getting-started/install/operator/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/operator/upgrade/</guid><description>版本升级 # 查看已有的版本：Easysearch:1.7.0-223
现在准备升级到 Easysearch:1.7.1-225，修改 Operator yaml 中的 version 字段，并 apply
# version: &amp;#34;1.7.0-223&amp;#34; version: &amp;#34;1.7.1-225&amp;#34; httpPort: 9200 vendor: Easysearch serviceAccount: controller-manager serviceName: threenodes 升级会比较久，因为为了保证升级过程中的服务可用性，节点升级是滚动升级的形式进行。 threenodes-masters-0 开始滚动更新，然后是 threenodes-masters-1，依次滚动更新， 直至所有节点更新完毕，大概总耗时 10 分钟
查看 Easysearch 新版本可知为 1.7.1-225
至此，版本升级完毕。</description></item><item><title>配置说明</title><link>/easysearch/main/docs/getting-started/configuration_file/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/configuration_file/</guid><description>配置文件 # 可以在每个 Easysearch 节点上找到 easysearch.yml , 通常为 Easysearch 安装目录下 config/easysearch.yml 。
警告 # 切勿将未受保护的节点暴露在公共互联网上！
常用网络设置： # Easysearch 默认只绑定到 localhost。
对于生产环境的集群，需要配置基本的网络设置。
network.host: （静态）节点绑定的主机名或 IP 地址。默认为 local，同时设置了 network.bind_host 和 network.publish_host。 discovery.seed_hosts: （静态）初始集群节点列表。默认为 [&amp;ldquo;127.0.0.1&amp;rdquo;, &amp;ldquo;[::1]&amp;quot;]。 http.port: HTTP （静态）请求的绑定端口。默认为 9200-9300。 transport.port: （静态）节点间通信的绑定端口。默认为 9300-9400。 network.bind_host: （静态）节点监听传入请求的地址。可以配置为外网地址（例如 0.0.0.0 监听所有接口）或其他特定的地址。 network.publish_host: （静态）用于节点之间的通信，在多网卡或多网络环境中，应显式设置 network.publish_host。 discovery.seed_hosts: （静态）提供集群中有资格成为主节点的节点地址列表。也可以是一个包含多个以逗号分隔的地址的单个字符串。每个地址的格式为 host:port 或 host。 discovery.type: （静态）指定 Easysearch 是否应形成一个多节点集群。如果将 discovery.type 设置为 single-node，Easysearch 将形成一个单节点集群。 cluster.initial_master_nodes: （静态）设置全新集群中的初始主节点候选节点列表。默认情况下，此列表为空，意味着该节点期望加入已经引导好的集群， 在生产环境中首次启动一个全新的 Easysearch 集群时，必须配置 cluster.initial_master_nodes，以明确哪些节点有资格参与主节点的选举。 当集群完成了首次主节点选举，集群已经正常运行时，就不再需要 cluster.</description></item><item><title>证书管理</title><link>/easysearch/main/docs/getting-started/install/operator/cert_manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/operator/cert_manager/</guid><description>证书管理 # 使用了 cert-manager 进行自动化管理证书，对于过期证书会自动重新颁发。
在这里我们根据 cert-manager 官方的配置方式配置了3套 Certificate 证书：ca-certificate、easysearch-certs 和 easysearch-admin-certs，分别用于节点间证书、http 访问证书和admin 管理员证书，具体参考下属 yaml 文件，重点需要主要证书的有效期(duration 字段)、更新时间(renewBefore 字段)和 commonName(infinilabs) 字段。
展开查看完整代码 ... apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned-issuer namespace: default spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ca-certificate namespace: default spec: secretName: ca-cert duration: 9000h # ~1year renewBefore: 360h # 15d commonName: infinilabs isCA: true privateKey: size: 2048 usages: - digital signature - key encipherment issuerRef: name: selfsigned-issuer --- apiVersion: cert-manager.</description></item><item><title>s3 备份</title><link>/easysearch/main/docs/getting-started/install/operator/s3_snapshot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/operator/s3_snapshot/</guid><description>s3 定期备份 # 与更新集群密码类似，也是根据 Operator yaml 的配置来启动一个 job, 然后请求集群 API 来配置相应的 s3 备份策略 具体参考文档： s3 定期备份</description></item><item><title>历史版本</title><link>/easysearch/main/docs/getting-started/install/operator/history_version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/operator/history_version/</guid><description/></item><item><title>Docker</title><link>/easysearch/main/docs/getting-started/install/docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/docker/</guid><description>Docker 环境下使用 Easysearch # 在使用 Docker 运行 Easysearch 之前，请确保已进行 系统调优并安装好 Docker 服务，且 Docker 服务正常运行。
最快方式：启动临时的 docker 容器，可以从前台查看到 admin 随机生成的初始密码
注： Docker 环境一般用于临时验证，如需要长期使用请务必进行数据持久化 # 直接运行镜像使用随机密码（数据及配置未持久化） docker run --name easysearch --ulimit memlock=-1:-1 -p 9200:9200 infinilabs/easysearch:1.15.0 # 使用自定义密码，可以使用环境变量配置 （需要 1.8.2 及以后的版本才支持） echo &amp;#34;EASYSEARCH_INITIAL_ADMIN_PASSWORD=$(openssl rand -hex 10)&amp;#34; | tee .env # 通过从环境变量文件设置初始密码（数据及配置未持久化） docker run --name easysearch --env-file ./.env --ulimit memlock=-1:-1 -p 9200:9200 infinilabs/easysearch:1.15.0 # 使用自定义密码及命名卷 (数据持久化到命名卷) docker run -d --name easysearch \ --ulimit memlock=-1:-1 \ --env-file .</description></item><item><title>FAQ</title><link>/easysearch/main/docs/getting-started/install/operator/FAQ/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/operator/FAQ/</guid><description>Easysearch 版本降级会报错 # cannot downgrade a node from version [1.7.0] to version [1.6.1]
[2024-01-28T09:42:34,314][ERROR][o.e.b.EasysearchUncaughtExceptionHandler] [onenode-masters-0] uncaught exception in thread [main] org.easysearch.bootstrap.StartupException: java.lang.IllegalStateException: cannot downgrade a node from version [1.7.0] to version [1.6.1] at org.easysearch.bootstrap.easysearch.init(Easysearch.java:173) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.execute(Easysearch.java:160) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:71) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.cli.Command.mainWithoutErrorHandling(Command.java:112) ~[easysearch-cli-1.6.1.jar:1.6.1] at org.easysearch.cli.Command.main(Command.java:75) ~[easysearch-cli-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.main(Easysearch.java:125) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.main(Easysearch.java:67) ~[easysearch-1.6.1.jar:1.6.1] Caused by: java.lang.IllegalStateException: cannot downgrade a node from version [1.7.0] to version [1.</description></item><item><title>Docker Compose</title><link>/easysearch/main/docs/getting-started/install/docker-compose/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/docker-compose/</guid><description>Docker Compose 环境下使用 Easysearch # 在使用 docker-compose 运行 Easysearch 集群之前，请确保已进行 系统调优并安装好 Docker 服务，且 Docker 服务正常运行。
# 安装docker-compose curl -L &amp;#34;https://github.com/docker/compose/releases/download/v2.6.1/docker-compose-$(uname -s)-$(uname -m)&amp;#34; -o /usr/local/bin/docker-compose # 增加执行权限 chmod +x /usr/local/bin/docker-compose # 检查版本信息 docker-compose -v 运行 2 节点 docker compose 项目 # 从官网下载文件并解压，然后运行初始化脚本，最后运行启动脚本。
在宿主机上创建工作目录 # 创建操作目录 sudo mkdir -p /data/docker/compose 下载文件并解压 如需测试 3 节点，只需把下面的下载文件名改为 3node.tar.gz 即可。
curl -sSL https://release.infinilabs.com/easysearch/archive/compose/2node.tar.gz | sudo tar -xzC /data/docker/compose --strip-components=1 # 调整目录权限 sudo chown -R ${USER} /data/docker/compose 注意：解压之后，请把镜像的 latest 版本手工更新成具体的版本，可参考下面的命令</description></item><item><title>离线安装</title><link>/easysearch/main/docs/getting-started/install/offline-install/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/offline-install/</guid><description>离线安装 Easysearch # 本文档介绍如何在没有网络连接的环境中安装 Easysearch。
Linux 环境离线安装 # Bundle 包内置了 JDK，是最简单的离线安装方式。
提前准备（在有网络的环境中） # 下载 bundle 包 wget https://release.infinilabs.com/easysearch/stable/bundle/easysearch-1.14.0-2228-linux-amd64-bundle.tar.gz 离线安装步骤 # 创建 easysearch 用户 groupadd -g 602 easysearch useradd -u 602 -g easysearch -m -d /home/easysearch -c &amp;#39;easysearch&amp;#39; -s /bin/bash easysearch # 创建 easysearch 安装目录 mkdir -p /data/easysearch # 解压 bundle 包到安装目录 tar -zxf easysearch-1.14.0-2228-linux-amd64-bundle.tar.gz -C /data/easysearch # 初始化 cd /data/easysearch &amp;amp;&amp;amp; bin/initialize.sh # 调整目录权限 chown -R easysearch:easysearch /data/easysearch # 运行 Easysearch su easysearch -c &amp;#34;/data/easysearch/bin/easysearch -d -p pid&amp;#34; # 停止 Easysearch kill -9 $(cat pid) Windows 环境离线安装 # 简化安装（无 HTTPS）</description></item><item><title>Helm Chart</title><link>/easysearch/main/docs/getting-started/install/helm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/helm/</guid><description>Helm Chart 部署 # INFINI Easysearch 从 1.5.0 版本开始支持 Helm Chart 方式部署。
仓库信息 # INFINI Easysearch Helm Chart 仓库地址: https://helm.infinilabs.com。
可以使用以下命令添加仓库
helm repo add infinilabs https://helm.infinilabs.com 依赖项 # StorageClass INFINI Easysearch Helm Chart 包中默认使用 local-path 进行数据持久化存储，可参考 local-path官方文档进行安装。
如果使用其他 StorageClass，请修改 Chart 包中的 storageClassName: local-path配置项。
Secret INFINI Easysearch Helm Chart 默认使用 cert-manager 进行自签 CA 证书创建及分发, 可参考 cert-manager 官方文档进行安装。
安装示例 # cat &amp;lt;&amp;lt; EOF | kubectl apply -n &amp;lt;namespace&amp;gt; -f - apiVersion: cert-manager.</description></item><item><title>Linux</title><link>/easysearch/main/docs/getting-started/install/linux/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/linux/</guid><description>Linux 环境下使用 Easysearch # 为了安全起见，Easysearch 不支持通过 root 身份来运行，需要新建普通用户，如 easysearch 用户来快速运行 Easysearch。
一键安装 # 通过我们提供的自动安装脚本可自动下载最新版本的 easysearch 进行解压安装，默认解压到 /data/easysearch
curl -sSL http://get.infini.cloud | bash -s -- -p easysearch 脚本的可选参数如下：
-v [版本号]（默认采用最新版本号）
-d [安装目录]（默认安装到/data/easysearch）
bundle 包运行 # bundle 是内置 JDK 的安装包，不需要额外下载 JDK，可直接解压运行。
# 创建 easysearch 用户 groupadd -g 602 easysearch useradd -u 602 -g easysearch -m -d /home/easysearch -c &amp;#39;easysearch&amp;#39; -s /bin/bash easysearch # 创建 easysearch 安装目录 mkdir -p /data/easysearch # 下载 bundle 包并解压到安装目录 wget -O - https://release.</description></item><item><title>Windows</title><link>/easysearch/main/docs/getting-started/install/windows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/windows/</guid><description>Windows 环境下使用 Easysearch # 目前，有多种方案可以在 Windows 下体验 Easysearch。
方案一 # 如果您的 Windows 环境上有 Docker，请查看 Docker 环境下使用 Easysearch
方案二 # 使用非 https 方式的 Easysearch
手工下载 Easysearch，并解压安装。 手工下载 JDK 将 JDK 解压到 Easysearch 安装目录下，并将目录的名称修改为 jdk。 由于 Windows 环境下默认没有 openssl，生成证书不太方便，您可以通过其他方式来生成证书，如在 Linux 环境提前生成证书。
#用记事本打开 config/easysearch.yml，并修改配置。 security.enabled: false 方案三 # 通过安装 git-for-windows 来执行 bash 操作。
注意：以下操作在 git-bash 中执行
通过在线脚本进行 Easysearch 安装 curl -sSL http://get.</description></item><item><title>IPv6 支持</title><link>/easysearch/main/docs/getting-started/install/ipv6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/getting-started/install/ipv6/</guid><description>Easysearch 的 IPv6 配置 # 配置 # Easysearch 支持运行在 IPv6 模式，以下是具体操作：
假设本机 IPv6 地址为：fe80::18df:9883:1e27:b040%en0
修改 config/easyearch.yml 将 ip 相关的参数修改为 IPv6 对应的格式:
network.host: [&amp;quot;::1&amp;quot;, &amp;quot;fe80::18df:9883:1e27:b040%en0&amp;quot;] http.port: 9200 transport.port: 9300 discovery.seed_hosts: [&amp;quot;[::1]:9300&amp;quot;, &amp;quot;[fe80::18df:9883:1e27:b040%en0]:9300&amp;quot;] cluster.initial_master_nodes: [&amp;quot;[fe80::18df:9883:1e27:b040%en0]:9300&amp;quot;] 这个配置表示节点使用 IPv6 地址加入集群的配置，监听本地回环地址和一个特定的链路本地地址。
验证 # 启动 Easysearch，即可使用 curl 工具进行测试：
% curl -6 -v -ku &amp;quot;admin:=juNrz?BY4SeSlL%Tm29OfP5&amp;quot; &amp;quot;https://[fe80::18df:9883:1e27:b040%en0]:9200&amp;quot; * Trying fe80::18df:9883:1e27:b040:9200... * Connected to fe80::18df:9883:1e27:b040 (fe80::18df:9883:1e27:b040) port 9200 (#0) * ALPN, offering h2 * ALPN, offering http/1.</description></item><item><title/><link>/easysearch/main/docs/references/aggregation/bucket-aggregations/auto-interval-date-histogram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/references/aggregation/bucket-aggregations/auto-interval-date-histogram/</guid><description>自动间隔日期直方图 # 与日期直方图聚合类似，其中你必须指定一个间隔， auto_date_histogram 是一个多分组聚合，根据你提供的分组数量和数据的时范围自动创建日期直方图分组。返回的实际分组数量总是小于或等于你指定的分组数量。当你在处理时间序列数据并希望在不同时间间隔上可视化或分析数据，而不需要手动指定间隔大小时，这种聚合特别有用。
间隔参数 # 分组间隔是根据收集的数据选择的，以确保返回的分组数量小于或等于请求的数量。
下表列出了每个时间单位可能的返回间隔。
单位 间隔参数 Seconds 1、5、10 和 30 的倍数 Minutes 1、5、10 和 30 的倍数 Hours 1、3 和 12 的倍数 Days 1 和 7 的倍数 Months 1 和 3 的倍数 Years 1、5、10、20、50 和 100 的倍数 如果一个聚合返回的分组太多（例如，每天一个分组），Easysearch 会自动减少分组的数量以确保结果可管理。它不会返回请求的确切数量的每日分组，而是会减少大约 1/7。例如，如果你请求 70 个分组，但数据中包含太多的每日间隔，Easysearch 可能只会返回 10 个分组，将数据分组到更大的间隔（如周）中，以避免结果数量过多。这有助于优化聚合，并在数据过多时防止过多细节。</description></item></channel></rss>