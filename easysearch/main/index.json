[{"category":null,"content":"Easysearch REST API #  async_search.delete #  删除指定的异步搜索请求及其结果。\nDELETE _async_search/{id} URL 参数 #     参数 类型 说明     id string 必需。异步搜索请求的 ID。    async_search.get #  获取异步搜索请求的当前状态和可用结果。\nGET _async_search/{id} URL 参数 #     参数 类型 说明     id string 必需。异步搜索请求的 ID。   wait_for_completion_timeout string 等待搜索完成的超时时间。   keep_alive string 结果保留的时间长度。    async_search.stats #  获取集群中异步搜索的统计信息。\nGET _async_search/stats URL 参数 #     参数 类型 说明     nodeId string 以逗号分隔的节点 ID 列表。仅返回指定节点的统计信息。    async_search.submit #  提交一个异步搜索请求。返回一个 ID，可用于后续获取结果。\nPOST _async_search POST {index}/_async_search URL 参数 #     参数 类型 说明     index string 以逗号分隔的索引名称。   wait_for_completion_timeout string 等待搜索完成的超时时间。   keep_alive string 结果保留的时间长度。   keep_on_completion boolean 搜索完成后是否保留结果。默认为 false。   request_cache boolean 是否使用请求缓存。   batched_reduce_size number 用于批量归约的分片数。    HTTP 请求体 #  标准搜索请求体，包含 query、aggregations 等。\nbulk #  在单个请求中执行多个索引、更新和/或删除操作。\nPOST _bulk PUT _bulk POST {index}/_bulk PUT {index}/_bulk POST {index}/{type}/_bulk PUT {index}/{type}/_bulk HTTP 请求体 #  操作定义和数据（action-data 对），以换行符分隔。\n必需：是\nURL 参数 #     参数 类型 说明     wait_for_active_shards string 在执行批量操作前，必须处于活跃状态的分片副本数。默认为 1（仅主分片）。设为 all 表示所有副本。也可设为不超过分片总副本数（副本数 + 1）的任意非负值。   refresh enum 如果为 true，刷新受影响的分片使操作对搜索可见。如果为 wait_for，等待刷新完成后再返回。如果为 false（默认），不执行刷新操作。   routing string 指定路由值。   timeout time 显式操作超时时间。   type string 未指定类型的文档的默认类型。   _source list 是否返回 _source 字段，或指定默认返回的字段列表，可在每个子请求中覆盖。   _source_excludes list 从返回的 _source 字段中排除的默认字段列表，可在每个子请求中覆盖。   _source_includes list 从 _source 字段中提取并返回的默认字段列表，可在每个子请求中覆盖。   pipeline string 用于预处理文档的 Pipeline ID。   require_alias boolean 为所有写入文档设置 require_alias，默认为 false。    cat.aliases #  显示当前已配置的索引别名信息，包括过滤器和路由信息。\nGET _cat/aliases GET _cat/aliases/{name} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   local boolean 返回本地信息，不从主节点获取状态（默认：false）   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。    cat.allocation #  显示每个数据节点的分片分配数量和磁盘使用情况快照。\nGET _cat/allocation GET _cat/allocation/{node_id} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   bytes enum 显示字节值的单位   local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题    cat.count #  快速获取整个集群或单个索引的文档数量。\nGET _cat/count GET _cat/count/{index} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题    cat.fielddata #  显示集群中每个数据节点上 fielddata 当前使用的堆内存大小。\nGET _cat/fielddata GET _cat/fielddata/{fields} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   bytes enum 显示字节值的单位   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题   fields list 要在输出中返回的字段列表，逗号分隔    cat.health #  返回集群健康状态的简要信息。\nGET _cat/health URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   time enum 显示时间值的单位   ts boolean 设为 false 可禁用时间戳   v boolean 详细模式，显示列标题    cat.help #  返回 Cat API 的帮助信息。\nGET _cat URL 参数 #     参数 类型 说明     help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔    cat.indices #  返回索引相关信息：主分片和副本数量、文档数量、磁盘大小等。\nGET _cat/indices GET _cat/indices/{index} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   bytes enum 显示字节值的单位   local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   health enum 按健康状态过滤索引（\u0026ldquo;green\u0026rdquo;、\u0026ldquo;yellow\u0026rdquo; 或 \u0026ldquo;red\u0026rdquo;）   help boolean 返回帮助信息   pri boolean 设为 true 仅返回主分片的统计信息   s list 用于排序的列名或列别名，逗号分隔   time enum 显示时间值的单位   v boolean 详细模式，显示列标题   include_unloaded_segments boolean 如果设为 true，段统计将包含未加载到内存中的段   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。    cat.master #  返回主节点的信息。\nGET _cat/master URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题    cat.nodeattrs #  返回自定义节点属性信息。\nGET _cat/nodeattrs URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题    cat.nodes #  返回集群节点的基本性能统计信息。\nGET _cat/nodes URL 参数 #     参数 类型 说明     bytes enum 显示字节值的单位   format string Accept 头的简写形式，如 json、yaml   full_id boolean 返回完整的节点 ID 而非缩写版本（默认：false）   local boolean 使用本地集群状态而非主节点状态来计算所选节点（默认：false）   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   time enum 显示时间值的单位   v boolean 详细模式，显示列标题    cat.pending_tasks #  返回集群待处理任务的简要信息。\nGET _cat/pending_tasks URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   time enum 显示时间值的单位   v boolean 详细模式，显示列标题    cat.plugins #  返回各节点已安装插件的信息。\nGET _cat/plugins URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题    cat.recovery #  返回正在进行和已完成的分片恢复信息。\nGET _cat/recovery GET _cat/recovery/{index} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   active_only boolean 如果为 true，响应仅包含正在进行的分片恢复   bytes enum 显示字节值的单位   detailed boolean 如果为 true，响应包含分片恢复的详细信息   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   index list 逗号分隔的索引名称列表或通配符表达式，用于限制返回的信息   s list 用于排序的列名或列别名，逗号分隔   time enum 显示时间值的单位   v boolean 详细模式，显示列标题    cat.repositories #  返回集群中已注册的快照仓库信息。\nGET _cat/repositories URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   local boolean 返回本地信息，不从主节点获取状态   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题    cat.segments #  提供索引分片中段（segment）的底层信息。\nGET _cat/segments GET _cat/segments/{index} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   bytes enum 显示字节值的单位   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题    cat.shards #  返回节点上分片分配的详细视图。\nGET _cat/shards GET _cat/shards/{index} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   bytes enum 显示字节值的单位   local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   time enum 显示时间值的单位   v boolean 详细模式，显示列标题    cat.snapshots #  返回特定仓库中的所有快照。\nGET _cat/snapshots GET _cat/snapshots/{repository} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   ignore_unavailable boolean 设为 true 以忽略不可用的快照   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   time enum 显示时间值的单位   v boolean 详细模式，显示列标题    cat.tasks #  返回集群中一个或多个节点上当前正在执行的任务信息。\nGET _cat/tasks URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   nodes list 用于限制返回信息的节点 ID 或名称列表（逗号分隔）；使用 _local 返回当前连接节点的信息，留空获取所有节点的信息   actions list 要返回的操作列表（逗号分隔）。留空返回全部。   detailed boolean 返回详细的任务信息（默认：false）   parent_task_id string 返回指定父任务 ID 的任务（node_id:task_number）。设为 -1 返回全部。   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   time enum 显示时间值的单位   v boolean 详细模式，显示列标题    cat.templates #  返回已存在的索引模板信息。\nGET _cat/templates GET _cat/templates/{name} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题    cat.thread_pool #  返回各节点的集群范围线程池统计信息。 默认返回所有线程池的活跃（active）、队列（queue）和拒绝（rejected）统计信息。\nGET _cat/thread_pool GET _cat/thread_pool/{thread_pool_patterns} URL 参数 #     参数 类型 说明     format string Accept 头的简写形式，如 json、yaml   size enum 显示值的倍率   local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 连接主节点的显式超时时间   h list 要显示的列名，逗号分隔   help boolean 返回帮助信息   s list 用于排序的列名或列别名，逗号分隔   v boolean 详细模式，显示列标题    clear_scroll #  显式清除搜索滚动上下文，释放相关资源。\nDELETE _search/scroll DELETE _search/scroll/{scroll_id} HTTP 请求体 #  如未通过 scroll_id 参数指定，则为要清除的滚动 ID 的逗号分隔列表\ncluster.allocation_explain #  解释集群中分片分配的原因。\nGET _cluster/allocation/explain POST _cluster/allocation/explain HTTP 请求体 #  需要解释的索引、分片和主分片标识。为空表示\u0026rsquo;解释第一个未分配的分片'\nURL 参数 #     参数 类型 说明     include_yes_decisions boolean 在解释中返回 \u0026lsquo;YES\u0026rsquo; 决策（默认：false）   include_disk_info boolean 返回磁盘使用和分片大小信息（默认：false）    cluster.delete_component_template #  删除组件模板。\nDELETE _component_template/{name} URL 参数 #     参数 类型 说明     timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间    cluster.delete_voting_config_exclusions #  清除集群投票配置排除项。\nDELETE _cluster/voting_config_exclusions URL 参数 #     参数 类型 说明     wait_for_removal boolean 指定是否等待所有被排除的节点从集群中移除后再清除投票配置排除列表。    cluster.exists_component_template #  检查指定组件模板是否存在。\nHEAD _component_template/{name} URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   local boolean 返回本地信息，不从主节点获取状态（默认：false）    cluster.get_component_template #  获取一个或多个组件模板。\nGET _component_template GET _component_template/{name} URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   local boolean 返回本地信息，不从主节点获取状态（默认：false）    cluster.get_settings #  返回集群级别的设置。\nGET _cluster/settings URL 参数 #     参数 类型 说明     flat_settings boolean 以扁平格式返回设置（默认：false）   master_timeout time 连接主节点的显式超时时间   timeout time 显式操作超时时间   include_defaults boolean 是否返回所有默认集群设置。    cluster.health #  返回集群健康状态的基本信息。\nGET _cluster/health GET _cluster/health/{index} URL 参数 #     参数 类型 说明     expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   level enum 指定返回信息的详细级别   local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 连接主节点的显式超时时间   timeout time 显式操作超时时间   wait_for_active_shards string 等待直到指定数量的分片处于活跃状态   wait_for_nodes string 等待直到指定数量的节点可用   wait_for_events enum 等待直到所有给定优先级的当前排队事件都已处理   wait_for_no_relocating_shards boolean 是否等待直到集群中没有正在迁移的分片   wait_for_no_initializing_shards boolean 是否等待直到集群中没有正在初始化的分片   wait_for_status enum 等待直到集群处于特定状态    cluster.pending_tasks #  返回所有尚未执行的集群级变更（例如创建索引、更新映射、 分配或失败分片）的列表。\nGET _cluster/pending_tasks URL 参数 #     参数 类型 说明     local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 指定连接主节点的超时时间    cluster.post_voting_config_exclusions #  通过节点 ID 或名称更新集群投票配置排除列表。\nPOST _cluster/voting_config_exclusions URL 参数 #     参数 类型 说明     node_ids string 要从投票配置中排除的节点持久化 ID 的逗号分隔列表。如果指定了此参数，则不能同时指定 ?node_names。   node_names string 要从投票配置中排除的节点名称的逗号分隔列表。如果指定了此参数，则不能同时指定 ?node_ids。   timeout time 显式操作超时时间    cluster.put_component_template #  创建或更新组件模板。\nPUT _component_template/{name} POST _component_template/{name} HTTP 请求体 #  模板定义\n必需：是\nURL 参数 #     参数 类型 说明     create boolean 索引模板是否仅在为新模板时添加，还是也可以替换现有模板   timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间    cluster.put_settings #  更新集群级别的设置。\nPUT _cluster/settings HTTP 请求体 #  要更新的设置。可以是 transient（临时）或 persistent（持久化，重启后保留）。\n必需：是\nURL 参数 #     参数 类型 说明     flat_settings boolean 以扁平格式返回设置（默认：false）   master_timeout time 连接主节点的显式超时时间   timeout time 显式操作超时时间    cluster.remote_info #  返回所有已配置的远程集群信息。\nGET _remote/info cluster.reroute #  允许手动更改集群中各分片的分配。\nPOST _cluster/reroute HTTP 请求体 #  要执行的 commands 定义（move、cancel、allocate）\nURL 参数 #     参数 类型 说明     dry_run boolean 仅模拟操作并返回结果状态   explain boolean 返回命令可以或不能执行的原因说明   retry_failed boolean 重试因过多连续分配失败而被阻止的分片分配   metric list 将返回的信息限制在指定的指标。默认返回除元数据外的全部信息   master_timeout time 连接主节点的显式超时时间   timeout time 显式操作超时时间    cluster.state #  返回集群状态的详细信息。\nGET _cluster/state GET _cluster/state/{metric} GET _cluster/state/{metric}/{index} URL 参数 #     参数 类型 说明     local boolean 返回本地信息，不从主节点获取状态（默认：false）   master_timeout time 指定连接主节点的超时时间   flat_settings boolean 以扁平格式返回设置（默认：false）   wait_for_metadata_version number 等待元数据版本等于或大于指定的元数据版本   wait_for_timeout time 等待 wait_for_metadata_version 的最大超时时间   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。    cluster.stats #  返回集群的高级统计概览。\nGET _cluster/stats GET _cluster/stats/nodes/{node_id} URL 参数 #     参数 类型 说明     flat_settings boolean 以扁平格式返回设置（默认：false）   timeout time 显式操作超时时间    count #  返回匹配查询的文档数量。\nPOST _count GET _count POST {index}/_count GET {index}/_count POST {index}/{type}/_count GET {index}/{type}/_count HTTP 请求体 #  使用 Query DSL 指定的查询条件，用于限制结果（可选）\nURL 参数 #     参数 类型 说明     ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   ignore_throttled boolean 当被节流时，是否忽略指定的具体、扩展或别名索引   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   min_score number 仅在结果中包含具有特定 _score 值的文档   preference string 指定操作应在哪个节点或分片上执行（默认：随机）   routing list 指定路由值列表，逗号分隔   q string Lucene 查询字符串语法的查询   analyzer string 用于查询字符串的分析器   analyze_wildcard boolean 是否分析通配符和前缀查询（默认：false）   default_operator enum 查询字符串查询的默认运算符（AND 或 OR）   df string 查询字符串中未指定字段前缀时使用的默认字段   lenient boolean 是否忽略基于格式的查询失败（如向数字字段提供文本）   terminate_after number 每个分片的最大计数，达到该值时查询将提前终止    create #  在索引中创建新文档。\n当索引中已存在相同 ID 的文档时返回 409 错误。\nPUT {index}/_create/{id} POST {index}/_create/{id} PUT {index}/{type}/{id}/_create POST {index}/{type}/{id}/_create HTTP 请求体 #  The document\n必需：是\nURL 参数 #     参数 类型 说明     wait_for_active_shards string 在执行索引操作前，必须处于活跃状态的分片副本数。默认为 1（仅主分片）。设为 all 表示所有副本，也可设为不超过分片总副本数的任意非负值。   refresh enum 如果为 true，刷新受影响的分片使操作对搜索可见；如果为 wait_for，等待刷新完成后再返回；如果为 false（默认），不执行刷新操作。   routing string 特定的路由值   timeout time 显式操作超时时间   version number 用于并发控制的显式版本号   version_type enum 指定版本类型   pipeline string 用于预处理传入文档的管道 ID    create_pit #  创建 Point-In-Time（PIT）上下文，为后续搜索请求提供一致的数据快照视图。\nPOST {index}/_pit URL 参数 #     参数 类型 说明     keep_alive time PIT 上下文的保持时间（必需）   allow_partial_pit_creation boolean 是否允许在部分分片不可用时仍创建 PIT（默认：true）   preference string 指定执行搜索的分片偏好   routing string 特定的路由值   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引   ignore_unavailable boolean 当指定的具体索引不可用时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略    dangling_indices.delete_dangling_index #  删除指定的悬空索引\nDELETE _dangling/{index_uuid} URL 参数 #     参数 类型 说明     accept_data_loss boolean 必须设为 true 才能删除悬空索引   timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间    dangling_indices.import_dangling_index #  导入指定的悬空索引\nPOST _dangling/{index_uuid} URL 参数 #     参数 类型 说明     accept_data_loss boolean 必须设为 true 才能导入悬空索引   timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间    dangling_indices.list_dangling_indices #  返回所有悬空索引。\nGET _dangling delete #  从索引中删除文档。\nDELETE {index}/_doc/{id} DELETE {index}/{type}/{id} URL 参数 #     参数 类型 说明     wait_for_active_shards string 设置在执行删除操作之前必须处于活跃状态的分片副本数。默认为 1，即仅主分片。设为 all 表示所有分片副本，否则设为小于或等于分片总副本数（副本数 + 1）的任意非负值   refresh enum 如果为 true，刷新受影响的分片使操作对搜索可见；如果为 wait_for，等待刷新完成后再返回；如果为 false（默认），不执行刷新操作。   routing string 特定的路由值   timeout time 显式操作超时时间   if_seq_no number 仅当最后修改文档的操作具有指定的序列号时才执行删除   if_primary_term number 仅当最后修改文档的操作具有指定的主任期时才执行删除   version number 用于并发控制的显式版本号   version_type enum 指定版本类型    delete_by_query #  删除匹配查询的文档。\nPOST {index}/_delete_by_query POST {index}/{type}/_delete_by_query HTTP 请求体 #  使用 Query DSL 的搜索定义\n必需：是\nURL 参数 #     参数 类型 说明     analyzer string 用于查询字符串的分析器   analyze_wildcard boolean 是否分析通配符和前缀查询（默认：false）   default_operator enum 查询字符串查询的默认运算符（AND 或 OR）   df string 查询字符串中未指定字段前缀时使用的默认字段   from number 起始偏移量（默认：0）   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   conflicts enum 当 delete-by-query 遇到版本冲突时的行为   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   lenient boolean 是否忽略基于格式的查询失败（如向数字字段提供文本）   preference string 指定操作应在哪个节点或分片上执行（默认：随机）   q string Lucene 查询字符串语法的查询   routing list 指定路由值列表，逗号分隔   scroll time 指定滚动搜索的索引一致性视图应维持多长时间   search_type enum 搜索操作类型   search_timeout time 每个搜索请求的显式超时时间。默认无超时。   size number 已弃用，请改用 max_docs   max_docs number 要处理的最大文档数（默认：所有文档）   sort list 逗号分隔的 : 对列表   _source list 是否返回 _source 字段，或要返回的字段列表   _source_excludes list 要从返回的 _source 字段中排除的字段列表   _source_includes list 要从 _source 字段中提取并返回的字段列表   terminate_after number 每个分片收集的最大文档数，达到此数量后查询将提前终止。   stats list 请求的特定 \u0026lsquo;tag\u0026rsquo;，用于日志和统计目的   version boolean 是否在命中结果中返回文档版本号   request_cache boolean 指定此请求是否使用请求缓存，默认使用索引级别设置   refresh boolean 受影响的索引是否应该刷新？   timeout time 每个批量请求等待不可用分片的时间。   wait_for_active_shards string 设置在执行按查询删除操作之前必须处于活跃状态的分片副本数。默认为 1，即仅主分片。设为 all 表示所有分片副本，否则设为小于或等于分片总副本数（副本数 + 1）的任意非负值   scroll_size number 驱动 delete-by-query 的滚动请求大小   wait_for_completion boolean 请求是否应阻塞直到按查询删除操作完成。   requests_per_second number 此请求的节流值（每秒子请求数）。-1 表示不节流。   slices number|string 该任务应被分成的切片数。默认为 1（不拆分子任务），可设为 auto。    delete_by_query_rethrottle #  修改 delete-by-query 操作的每秒请求数。\nPOST _delete_by_query/{task_id}/_rethrottle URL 参数 #     参数 类型 说明     requests_per_second number 此请求的节流值（每秒浮点子请求数）。-1 表示不节流。    delete_pit #  删除一个或多个 Point-In-Time（PIT）上下文。支持删除指定的 PIT（通过请求体）或一次性删除所有 PIT。\nDELETE _pit DELETE _pit/_all HTTP 请求体 #  包含要删除的 PIT ID 列表的 JSON 对象。使用 _pit/_all 路由时无需请求体。\ndelete_script #  删除存储的脚本。\nDELETE _scripts/{id} URL 参数 #     参数 类型 说明     timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间    exists #  检查指定 ID 的文档是否存在于索引中。\nHEAD {index}/_doc/{id} HEAD {index}/{type}/{id} URL 参数 #     参数 类型 说明     stored_fields list 逗号分隔的要在响应中返回的存储字段列表   preference string 指定操作应在哪个节点或分片上执行（默认：随机）   realtime boolean 指定是否以实时模式或搜索模式执行操作   refresh boolean 在执行操作之前刷新包含该文档的分片   routing string 特定的路由值   _source list 是否返回 _source 字段，或要返回的字段列表   _source_excludes list 要从返回的 _source 字段中排除的字段列表   _source_includes list 要从 _source 字段中提取并返回的字段列表   version number 用于并发控制的显式版本号   version_type enum 指定版本类型    exists_source #  检查指定 ID 文档的 _source 是否存在于索引中。\nHEAD {index}/_source/{id} HEAD {index}/{type}/{id}/_source URL 参数 #     参数 类型 说明     preference string 指定操作应在哪个节点或分片上执行（默认：随机）   realtime boolean 指定是否以实时模式或搜索模式执行操作   refresh boolean 在执行操作之前刷新包含该文档的分片   routing string 特定的路由值   _source list 是否返回 _source 字段，或要返回的字段列表   _source_excludes list 要从返回的 _source 字段中排除的字段列表   _source_includes list 要从 _source 字段中提取并返回的字段列表   version number 用于并发控制的显式版本号   version_type enum 指定版本类型    explain #  解释某个文档为什么匹配（或不匹配）某个查询。\nGET {index}/_explain/{id} POST {index}/_explain/{id} GET {index}/{type}/{id}/_explain POST {index}/{type}/{id}/_explain HTTP 请求体 #  使用 Query DSL 的查询定义\nURL 参数 #     参数 类型 说明     analyze_wildcard boolean 指定查询字符串中的通配符和前缀查询是否应被分析（默认：false）   analyzer string 查询字符串查询使用的分析器   default_operator enum 查询字符串查询的默认运算符（AND 或 OR）   df string 查询字符串查询的默认字段（默认：_all）   stored_fields list 逗号分隔的要在响应中返回的存储字段列表   lenient boolean 是否忽略基于格式的查询失败（如向数字字段提供文本）   preference string 指定操作应在哪个节点或分片上执行（默认：随机）   q string Lucene 查询字符串语法的查询   routing string 特定的路由值   _source list 是否返回 _source 字段，或要返回的字段列表   _source_excludes list 要从返回的 _source 字段中排除的字段列表   _source_includes list 要从 _source 字段中提取并返回的字段列表    field_caps #  返回多个索引中各字段的能力信息（可搜索、可聚合等）。\nGET _field_caps POST _field_caps GET {index}/_field_caps POST {index}/_field_caps HTTP 请求体 #  使用 Query DSL 指定的索引过滤器\nURL 参数 #     参数 类型 说明     fields list 逗号分隔的字段名称列表   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   include_unmapped boolean 指示是否应在响应中包含未映射的字段。    get #  获取指定文档。\nGET {index}/_doc/{id} GET {index}/{type}/{id} URL 参数 #     参数 类型 说明     stored_fields list 逗号分隔的要在响应中返回的存储字段列表   preference string 指定操作应在哪个节点或分片上执行（默认：随机）   realtime boolean 指定是否以实时模式或搜索模式执行操作   refresh boolean 在执行操作之前刷新包含该文档的分片   routing string 特定的路由值   _source list 是否返回 _source 字段，或要返回的字段列表   _source_excludes list 要从返回的 _source 字段中排除的字段列表   _source_includes list 要从 _source 字段中提取并返回的字段列表   version number 用于并发控制的显式版本号   version_type enum 指定版本类型    get_all_pits #  获取集群中所有节点上的所有活跃 Point-In-Time（PIT）上下文。\nGET _pit/_all 此 API 无需额外参数。响应包含一个 pits 数组，列出所有活跃的 PIT 及其详细信息。\nget_script #  获取存储的脚本。\nGET _scripts/{id} URL 参数 #     参数 类型 说明     master_timeout time 指定连接主节点的超时时间    get_script_context #  返回所有可用的脚本上下文。\nGET _script_context get_script_languages #  返回可用的脚本类型、语言和上下文。\nGET _script_language get_source #  获取指定文档的 _source 内容。\nGET {index}/_source/{id} GET {index}/{type}/{id}/_source URL 参数 #     参数 类型 说明     preference string 指定操作应在哪个节点或分片上执行（默认：随机）   realtime boolean 指定是否以实时模式或搜索模式执行操作   refresh boolean 在执行操作之前刷新包含该文档的分片   routing string 特定的路由值   _source list 是否返回 _source 字段，或要返回的字段列表   _source_excludes list 要从返回的 _source 字段中排除的字段列表   _source_includes list 要从 _source 字段中提取并返回的字段列表   version number 用于并发控制的显式版本号   version_type enum 指定版本类型    index #  在索引中创建或更新文档。\nPUT {index}/_doc/{id} POST {index}/_doc/{id} POST {index}/_doc POST {index}/{type} PUT {index}/{type}/{id} POST {index}/{type}/{id} HTTP 请求体 #  The document\n必需：是\nURL 参数 #     参数 类型 说明     wait_for_active_shards string 在执行索引操作前，必须处于活跃状态的分片副本数。默认为 1（仅主分片）。设为 all 表示所有副本，也可设为不超过分片总副本数的任意非负值。   op_type enum 显式操作类型。对于有显式文档 ID 的请求默认为 index，对于没有显式文档 ID 的请求默认为 create   refresh enum 如果为 true，刷新受影响的分片使操作对搜索可见；如果为 wait_for，等待刷新完成后再返回；如果为 false（默认），不执行刷新操作。   routing string 特定的路由值   timeout time 显式操作超时时间   version number 用于并发控制的显式版本号   version_type enum 指定版本类型   if_seq_no number 仅当最后修改文档的操作具有指定的序列号时才执行索引操作   if_primary_term number 仅当最后修改文档的操作具有指定的主任期时才执行索引操作   pipeline string 用于预处理传入文档的管道 ID   require_alias boolean 当为 true 时，要求目标必须是别名。默认为 false    ik.reload #  在集群的所有节点上重新加载 IK 分词器词典。可以指定从特定索引加载词典数据。\nPOST _ik/_reload HTTP 请求体 #     参数 类型 说明     dict_key string 要重新加载的词典键名。   dict_index string 词典数据所在的索引名称。默认为 .analysis_ik。    ilm.add_policy #  为一个或多个索引添加 ISM（索引状态管理）策略。\nPOST _ilm/add POST _ilm/add/{index} URL 参数 #     参数 类型 说明     index string 以逗号分隔的索引名称。    HTTP 请求体 #     参数 类型 说明     policy_id string 要添加的 ISM 策略 ID。    ilm.change_policy #  更改已应用于一个或多个索引的 ISM 策略。\nPOST _ilm/change_policy POST _ilm/change_policy/{index} URL 参数 #     参数 类型 说明     index string 以逗号分隔的索引名称。    HTTP 请求体 #  变更策略配置，包含新的 policy_id、状态过滤器等。\nilm.delete_policy #  删除指定的 ISM 策略。\nDELETE _ilm/policy/{policyID} URL 参数 #     参数 类型 说明     policyID string 必需。要删除的策略 ID。   refresh string 控制请求所做更改何时可被搜索到。有效值：true、false、wait_for。    ilm.explain #  获取一个或多个索引的 ISM 策略执行状态和详情。\nGET _ilm/explain GET _ilm/explain/{index} URL 参数 #     参数 类型 说明     index string 以逗号分隔的索引名称。   local boolean 是否仅从本地节点返回信息。默认为 false。   show_policy boolean 是否在结果中包含策略定义。   show_validate_action boolean 是否显示验证动作信息。   size number 每页返回数量（用于列表模式）。   from number 分页起始位置。   sortField string 排序字段。   sortOrder string 排序方向。有效值：asc、desc。   queryString string 过滤查询字符串。    ilm.get_policy #  获取一个或多个 ISM 策略的定义。\nGET _ilm/policy GET _ilm/policy/{policyID} HEAD _ilm/policy/{policyID} URL 参数 #     参数 类型 说明     policyID string 策略 ID。省略则列出所有策略。   size number 每页返回数量（列表模式）。   from number 分页起始位置（列表模式）。   sortField string 排序字段。   sortOrder string 排序方向。有效值：asc、desc。   queryString string 过滤查询字符串。    ilm.put_policy #  创建或更新一个 ISM 策略。\nPUT _ilm/policy/{policyID} URL 参数 #     参数 类型 说明     policyID string 必需。策略 ID。   if_seq_no number 仅在序列号匹配时执行操作，用于乐观并发控制。   if_primary_term number 仅在主要任期匹配时执行操作，用于乐观并发控制。   refresh string 控制请求所做更改何时可被搜索到。有效值：true、false、wait_for。    HTTP 请求体 #  ISM 策略定义，包含策略描述、状态列表、转换条件、操作配置等。\nilm.remove_policy #  从一个或多个索引上移除 ISM 策略。\nPOST _ilm/remove POST _ilm/remove/{index} URL 参数 #     参数 类型 说明     index string 必需。以逗号分隔的索引名称。    ilm.retry #  重试因错误而失败的 ISM 策略执行。\nPOST _ilm/retry POST _ilm/retry/{index} URL 参数 #     参数 类型 说明     index string 必需。以逗号分隔的索引名称。    HTTP 请求体 #     参数 类型 说明     state string 要重试的状态名称。    indices.add_block #  为索引添加操作限制块。\nPUT {index}/_block/{block} URL 参数 #     参数 类型 说明     timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。    indices.analyze #  对文本执行分析（分词）处理，返回分词结果。\nGET _analyze POST _analyze GET {index}/_analyze POST {index}/_analyze HTTP 请求体 #  定义分析器/分词器参数以及需要进行分析的文本\nURL 参数 #     参数 类型 说明     index string 操作范围限定的索引名称    indices.clear_cache #  清除一个或多个索引的全部或指定类型的缓存。\nPOST _cache/clear POST {index}/_cache/clear URL 参数 #     参数 类型 说明     fielddata boolean 清除字段数据   fields list 使用 fielddata 参数时要清除的字段的逗号分隔列表（默认：全部）   query boolean 清除查询缓存   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   index list 用于限制操作的逗号分隔的索引名称列表   request boolean 清除请求缓存    indices.clone #  克隆索引\nPUT {index}/_clone/{target} POST {index}/_clone/{target} HTTP 请求体 #  目标索引的配置（settings 和 aliases）\nURL 参数 #     参数 类型 说明     timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间   wait_for_active_shards string 设置操作返回前等待克隆索引上活跃分片的数量。    indices.close #  关闭索引。\nPOST {index}/_close URL 参数 #     参数 类型 说明     timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   wait_for_active_shards string 设置操作返回前等待的活跃分片数。    indices.create #  创建索引，可指定设置和映射。\nPUT {index} HTTP 请求体 #  索引的配置（settings 和 mappings）\nURL 参数 #     参数 类型 说明     include_type_name boolean 映射体中是否应包含类型。   wait_for_active_shards string 设置操作返回前等待的活跃分片数量。   timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间    indices.create_data_stream #  创建或更新数据流。\nPUT _data_stream/{name} HTTP 请求体 #  数据流定义。\n indices.data_streams_stats #  返回数据流的操作统计信息。\nGET _data_stream/_stats GET _data_stream/{name}/_stats  indices.delete_data_stream #  删除数据流。\nDELETE _data_stream/{name}  indices.get_data_stream #  获取数据流信息。\nGET _data_stream GET _data_stream/{name}  indices.delete #  删除索引。\nDELETE {index} URL 参数 #     参数 类型 说明     timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间   ignore_unavailable boolean 忽略不可用的索引（默认：false）   allow_no_indices boolean 如果通配符表达式未匹配到任何具体索引则忽略（默认：false）   expand_wildcards enum 通配符表达式是否应扩展到打开或关闭的索引（默认：open）    indices.delete_alias #  删除索引别名。\nDELETE {index}/_alias/{name} DELETE {index}/_aliases/{name} URL 参数 #     参数 类型 说明     timeout time 文档的显式时间戳   master_timeout time 指定连接主节点的超时时间    indices.delete_index_template #  删除索引模板。\nDELETE _index_template/{name} URL 参数 #     参数 类型 说明     timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间    indices.delete_template #  删除索引模板。\nDELETE _template/{name} URL 参数 #     参数 类型 说明     timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间    indices.disk_usage #  分析一个或多个索引的磁盘空间使用情况，按字段级别提供详细的空间占用报告。此操作资源开销较大，需显式设置 run_expensive_tasks=true。\nPOST {index}/_disk_usage URL 参数 #     参数 类型 说明     run_expensive_tasks boolean 必须设为 true 才能执行此操作，否则请求将被拒绝（安全保护措施）   flush boolean 分析前是否先刷新索引（默认：true）   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引   ignore_unavailable boolean 当指定的具体索引不可用时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略    indices.exists #  检查指定索引是否存在。\nHEAD {index} URL 参数 #     参数 类型 说明     local boolean 返回本地信息，不从主节点获取状态（默认：false）   ignore_unavailable boolean 忽略不可用的索引（默认：false）   allow_no_indices boolean 如果通配符表达式未匹配到任何具体索引则忽略（默认：false）   expand_wildcards enum 通配符表达式是否应扩展到打开或关闭的索引（默认：open）   flat_settings boolean 以扁平格式返回设置（默认：false）   include_defaults boolean 是否返回每个索引的所有默认设置。    indices.exists_alias #  检查指定索引别名是否存在。\nHEAD _alias/{name} HEAD {index}/_alias/{name} URL 参数 #     参数 类型 说明     ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   local boolean 返回本地信息，不从主节点获取状态（默认：false）    indices.exists_index_template #  检查指定索引模板是否存在。\nHEAD _index_template/{name} URL 参数 #     参数 类型 说明     flat_settings boolean 以扁平格式返回设置（默认：false）   master_timeout time 连接主节点的显式超时时间   local boolean 返回本地信息，不从主节点获取状态（默认：false）    indices.exists_template #  检查指定索引模板是否存在。\nHEAD _template/{name} URL 参数 #     参数 类型 说明     flat_settings boolean 以扁平格式返回设置（默认：false）   master_timeout time 连接主节点的显式超时时间   local boolean 返回本地信息，不从主节点获取状态（默认：false）    indices.exists_type #  检查指定文档类型是否存在（已废弃）。\nHEAD {index}/_mapping/{type} URL 参数 #     参数 类型 说明     ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   local boolean 返回本地信息，不从主节点获取状态（默认：false）    indices.field_usage_stats #  返回一个或多个索引的字段使用统计信息，显示各字段被查询和使用的情况。\nGET {index}/_field_usage_stats URL 参数 #     参数 类型 说明     fields list 逗号分隔的字段列表，指定要报告使用情况的字段（默认：所有字段）   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引   ignore_unavailable boolean 当指定的具体索引不可用时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略    indices.flush #  对一个或多个索引执行 flush 操作，将内存中的数据持久化到磁盘。\nPOST _flush GET _flush POST {index}/_flush GET {index}/_flush URL 参数 #     参数 类型 说明     force boolean 是否强制执行 flush，即使没有需要提交的变更。这在需要递增事务日志 ID 但没有未提交变更时很有用（此设置可视为内部设置）。   wait_if_ongoing boolean 如果设为 true，当另一个 flush 操作正在执行时，将阻塞等待。默认为 true。如果设为 false，当另一个 flush 已在运行时将跳过。   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。    indices.flush_synced #  对一个或多个索引执行同步 flush 操作。同步 flush 已废弃，请使用 flush 代替。\nPOST _flush/synced GET _flush/synced POST {index}/_flush/synced GET {index}/_flush/synced URL 参数 #     参数 类型 说明     ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。    indices.forcemerge #  对一个或多个索引执行强制合并操作。\nPOST _forcemerge POST {index}/_forcemerge URL 参数 #     参数 类型 说明     flush boolean 是否在操作后刷新索引（默认：true）   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   max_num_segments number 索引应合并成的段数（默认：动态）   only_expunge_deletes boolean 指定操作是否仅清除已删除的文档    indices.get #  返回一个或多个索引的信息。\nGET {index} URL 参数 #     参数 类型 说明     include_type_name boolean 是否在响应中添加类型名称（默认：false）   local boolean 返回本地信息，不从主节点获取状态（默认：false）   ignore_unavailable boolean 忽略不可用的索引（默认：false）   allow_no_indices boolean 如果通配符表达式未匹配到任何具体索引则忽略（默认：false）   expand_wildcards enum 通配符表达式是否应扩展到打开或关闭的索引（默认：open）   flat_settings boolean 以扁平格式返回设置（默认：false）   include_defaults boolean 是否返回每个索引的所有默认设置。   master_timeout time 指定连接主节点的超时时间    indices.get_alias #  获取索引别名信息。\nGET _alias GET _alias/{name} GET {index}/_alias/{name} GET {index}/_alias URL 参数 #     参数 类型 说明     ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   local boolean 返回本地信息，不从主节点获取状态（默认：false）    indices.get_field_mapping #  获取一个或多个字段的映射信息。\nGET _mapping/field/{fields} GET {index}/_mapping/field/{fields} GET _mapping/{type}/field/{fields} GET {index}/_mapping/{type}/field/{fields} URL 参数 #     参数 类型 说明     include_type_name boolean 映射体中是否应返回类型。   include_defaults boolean 是否也应返回默认映射值   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   local boolean 返回本地信息，不从主节点获取状态（默认：false）    indices.get_index_template #  获取索引模板。\nGET _index_template GET _index_template/{name} URL 参数 #     参数 类型 说明     flat_settings boolean 以扁平格式返回设置（默认：false）   master_timeout time 连接主节点的显式超时时间   local boolean 返回本地信息，不从主节点获取状态（默认：false）    indices.get_mapping #  获取一个或多个索引的映射信息。\nGET _mapping GET {index}/_mapping GET _mapping/{type} GET {index}/_mapping/{type} URL 参数 #     参数 类型 说明     include_type_name boolean 是否在响应中添加类型名称（默认：false）   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   master_timeout time 指定连接主节点的超时时间   local boolean 返回本地信息，不从主节点获取状态（默认：false）    indices.get_settings #  获取一个或多个索引的设置信息。\nGET _settings GET {index}/_settings GET {index}/_settings/{name} GET _settings/{name} URL 参数 #     参数 类型 说明     master_timeout time 指定连接主节点的超时时间   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   flat_settings boolean 以扁平格式返回设置（默认：false）   local boolean 返回本地信息，不从主节点获取状态（默认：false）   include_defaults boolean 是否返回每个索引的所有默认设置。    indices.get_template #  获取索引模板。\nGET _template GET _template/{name} URL 参数 #     参数 类型 说明     include_type_name boolean 映射体中是否应返回类型。   flat_settings boolean 以扁平格式返回设置（默认：false）   master_timeout time 连接主节点的显式超时时间   local boolean 返回本地信息，不从主节点获取状态（默认：false）    indices.get_upgrade #  _upgrade API 已不再有用，将被移除。\nGET _upgrade GET {index}/_upgrade URL 参数 #     参数 类型 说明     ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。    indices.open #  打开已关闭的索引。\nPOST {index}/_open URL 参数 #     参数 类型 说明     timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   wait_for_active_shards string 设置操作返回前等待的活跃分片数。    indices.put_alias #  创建或更新索引别名。\nPUT {index}/_alias/{name} POST {index}/_alias/{name} PUT {index}/_aliases/{name} POST {index}/_aliases/{name} HTTP 请求体 #  别名的设置，例如 routing 或 filter\n必需：否\nURL 参数 #     参数 类型 说明     timeout time 文档的显式时间戳   master_timeout time 指定连接主节点的超时时间    indices.put_index_template #  创建或更新索引模板。\nPUT _index_template/{name} POST _index_template/{name} HTTP 请求体 #  模板定义\n必需：是\nURL 参数 #     参数 类型 说明     create boolean 索引模板是否仅在为新模板时添加，还是也可以替换现有模板   cause string 用户定义的创建/更新索引模板原因   master_timeout time 指定连接主节点的超时时间    indices.put_mapping #  更新索引的映射。\nPUT {index}/_mapping POST {index}/_mapping PUT {index}/{type}/_mapping POST {index}/{type}/_mapping PUT {index}/_mapping/{type} POST {index}/_mapping/{type} PUT {index}/{type}/_mappings POST {index}/{type}/_mappings PUT {index}/_mappings/{type} POST {index}/_mappings/{type} PUT _mappings/{type} POST _mappings/{type} PUT {index}/_mappings POST {index}/_mappings PUT _mapping/{type} POST _mapping/{type} HTTP 请求体 #  映射定义\n必需：是\nURL 参数 #     参数 类型 说明     include_type_name boolean 映射体中是否应包含类型。   timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   write_index_only boolean 当为 true 时，仅将映射应用于别名或数据流的写入索引    indices.put_settings #  更新索引的设置。\nPUT _settings PUT {index}/_settings HTTP 请求体 #  要更新的索引设置\n必需：是\nURL 参数 #     参数 类型 说明     master_timeout time 指定连接主节点的超时时间   timeout time 显式操作超时时间   preserve_existing boolean 是否更新已有设置。如果设为 true，索引上的已有设置保持不变，默认为 false。   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引   flat_settings boolean 以扁平格式返回设置（默认：false）    indices.put_template #  创建或更新索引模板。\nPUT _template/{name} POST _template/{name} HTTP 请求体 #  模板定义\n必需：是\nURL 参数 #     参数 类型 说明     include_type_name boolean 映射体中是否应返回类型。   order number 合并多个匹配模板时此模板的顺序（数字越大越后合并，会覆盖较小数字的设置）   create boolean 索引模板是否仅在为新模板时添加，还是也可以替换现有模板   master_timeout time 指定连接主节点的超时时间    indices.recovery #  返回正在进行的分片恢复信息。\nGET _recovery GET {index}/_recovery URL 参数 #     参数 类型 说明     detailed boolean 是否显示分片恢复的详细信息   active_only boolean 仅显示当前正在进行的恢复    indices.refresh #  对一个或多个索引执行刷新操作，使最近的变更对搜索可见。\nPOST _refresh GET _refresh POST {index}/_refresh GET {index}/_refresh URL 参数 #     参数 类型 说明     ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。    indices.refresh_search_analyzers #  刷新一个或多个索引的搜索分析器，使分析器配置更改生效，无需关闭和重新打开索引。\nPOST _refresh_search_analyzers POST _refresh_search_analyzers/{index} URL 参数 #     参数 类型 说明     index string 必需。以逗号分隔的索引名称。    indices.reload #  重新加载一个或多个索引的 fast-terms 数据。\nGET {index}/_reload POST {index}/_reload URL 参数 #     参数 类型 说明     index string 必需。以逗号分隔的索引名称。    indices.resolve_index #  返回所有匹配的索引、别名和数据流的信息\nGET _resolve/index/{name} URL 参数 #     参数 类型 说明     expand_wildcards enum 通配符表达式是否应扩展到打开或关闭的索引（默认：open）    indices.rollover #  当现有索引被认为太大或太旧时，更新别名以指向新索引。\nPOST {alias}/_rollover POST {alias}/_rollover/{new_index} HTTP 请求体 #  执行滚动更新（rollover）需要满足的条件\nURL 参数 #     参数 类型 说明     include_type_name boolean 映射体中是否应包含类型。   timeout time 显式操作超时时间   dry_run boolean 如果设为 true，滚动更新操作将仅进行验证而不实际执行，即使条件匹配也是如此。默认为 false   master_timeout time 指定连接主节点的超时时间   wait_for_active_shards string 设置操作返回前在新创建的滚动更新索引上等待的活跃分片数。    indices.segments #  返回 Lucene 索引中段的底层信息。\nGET _segments GET {index}/_segments URL 参数 #     参数 类型 说明     ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   verbose boolean 包含 Lucene 的详细内存使用情况。    indices.shard_stores #  返回索引分片副本的存储信息。\nGET _shard_stores GET {index}/_shard_stores URL 参数 #     参数 类型 说明     status list 用于过滤分片以获取存储信息的逗号分隔的状态列表   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。    indices.shrink #  将现有索引收缩为具有更少主分片的新索引。\nPUT {index}/_shrink/{target} POST {index}/_shrink/{target} HTTP 请求体 #  目标索引的配置（settings 和 aliases）\nURL 参数 #     参数 类型 说明     copy_settings boolean 是否从源索引复制设置（默认：false）   timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间   wait_for_active_shards string 设置操作返回前等待收缩索引上活跃分片的数量。    indices.simulate_index_template #  模拟给定索引名称与系统中索引模板的匹配结果。\nPOST _index_template/_simulate_index/{name} HTTP 请求体 #  新的索引模板定义，将包含在模拟中，如同它已存在于系统中\n必需：否\nURL 参数 #     参数 类型 说明     create boolean 可选地在请求体中定义的索引模板是否仅在为新模板时进行模拟添加，还是也可以替换现有模板   cause string 用户定义的模拟创建新模板的原因   master_timeout time 指定连接主节点的超时时间    indices.simulate_template #  模拟解析给定的模板名称或模板体。\nPOST _index_template/_simulate POST _index_template/_simulate/{name} HTTP 请求体 #  如未指定索引模板名称，则为要模拟的新索引模板定义\n必需：否\nURL 参数 #     参数 类型 说明     create boolean 可选地在请求体中定义的索引模板是否仅在为新模板时进行模拟添加，还是也可以替换现有模板   cause string 用户定义的模拟创建新模板的原因   master_timeout time 指定连接主节点的超时时间    indices.split #  将现有索引拆分为具有更多主分片的新索引。\nPUT {index}/_split/{target} POST {index}/_split/{target} HTTP 请求体 #  目标索引的配置（settings 和 aliases）\nURL 参数 #     参数 类型 说明     copy_settings boolean 是否从源索引复制设置（默认：false）   timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间   wait_for_active_shards string 设置操作返回前等待收缩索引上活跃分片的数量。    indices.stats #  返回索引上发生的操作统计信息。\nGET _stats GET _stats/{metric} GET {index}/_stats GET {index}/_stats/{metric} URL 参数 #     参数 类型 说明     completion_fields list 用于 fielddata 和 suggest 索引指标的字段列表（支持通配符），逗号分隔   fielddata_fields list 用于 fielddata 索引指标的逗号分隔字段列表（支持通配符）   fields list 用于 fielddata 和 completion 索引指标的字段列表（支持通配符），逗号分隔   groups list 用于 search 索引指标的逗号分隔搜索组列表   level enum 返回按集群、索引或分片级别聚合的统计信息   types list 用于 indexing 索引指标的文档类型列表，逗号分隔   include_segment_file_sizes boolean 是否报告每个 Lucene 索引文件的聚合磁盘使用情况（仅在请求段统计时适用）   include_unloaded_segments boolean 如果设为 true，段统计将包含未加载到内存中的段   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   forbid_closed_indices boolean 如果设为 false，当显式指定或 expand_wildcards 扩展到关闭的索引时，也会从关闭的索引收集统计信息    indices.update_aliases #  批量更新索引别名。\nPOST _aliases HTTP 请求体 #  要执行的 actions 定义\n必需：是\nURL 参数 #     参数 类型 说明     timeout time 请求超时时间   master_timeout time 指定连接主节点的超时时间    indices.upgrade #  _upgrade API 已不再有用，将被移除。\nPOST _upgrade POST {index}/_upgrade URL 参数 #     参数 类型 说明     allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   wait_for_completion boolean 指定请求是否应阻塞直到所有段升级完成（默认：false）   only_ancient_segments boolean 如果为 true，仅升级过时（较旧的 Lucene 主版本）的段    indices.validate_query #  验证查询 DSL 的正确性，而不实际执行查询。\nGET _validate/query POST _validate/query GET {index}/_validate/query POST {index}/_validate/query GET {index}/{type}/_validate/query POST {index}/{type}/_validate/query HTTP 请求体 #  使用 Query DSL 指定的查询定义\nURL 参数 #     参数 类型 说明     explain boolean 返回错误的详细信息   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   q string Lucene 查询字符串语法的查询   analyzer string 用于查询字符串的分析器   analyze_wildcard boolean 是否分析通配符和前缀查询（默认：false）   default_operator enum 查询字符串查询的默认运算符（AND 或 OR）   df string 查询字符串中未指定字段前缀时使用的默认字段   lenient boolean 是否忽略基于格式的查询失败（如向数字字段提供文本）   rewrite boolean 提供更详细的解释，显示将执行的实际 Lucene 查询。   all_shards boolean 在所有分片上执行验证，而不是每个索引一个随机分片    info #  返回集群的基本信息。\nGET / ingest.delete_pipeline #  删除 Pipeline。\nDELETE _ingest/pipeline/{id} URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   timeout time 显式操作超时时间    ingest.get_pipeline #  获取 Pipeline 信息。\nGET _ingest/pipeline GET _ingest/pipeline/{id} URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间    ingest.processor_grok #  返回内置 Grok 模式列表。\nGET _ingest/processor/grok ingest.put_pipeline #  创建或更新 Pipeline。\nPUT _ingest/pipeline/{id} HTTP 请求体 #  摄取管道定义\n必需：是\nURL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   timeout time 显式操作超时时间    ingest.simulate #  使用示例文档模拟 Pipeline 的执行效果。\nGET _ingest/pipeline/_simulate POST _ingest/pipeline/_simulate GET _ingest/pipeline/{id}/_simulate POST _ingest/pipeline/{id}/_simulate HTTP 请求体 #  模拟定义\n必需：是\nURL 参数 #     参数 类型 说明     verbose boolean 详细模式。显示已执行管道中每个处理器的数据输出    license #  管理 Easysearch 许可证。支持查看当前许可证信息、应用新许可证和删除许可证。\nGET _license/info POST _license/apply DELETE _license HTTP 请求体（仅 POST） #  包含 license 字段的 JSON 对象，值为许可证字符串。\n必需：是（仅 POST）\nURL 参数（仅 POST 和 DELETE） #     参数 类型 说明     master_timeout time 指定连接主节点的超时时间   timeout time 显式操作超时时间    match_rules.compile #  编译指定规则仓库中的规则。编译后的规则用于 Ingest Pipeline 写入阶段的规则匹配（如 check_match_rules 处理器）。\nPOST _match_rules/{repo_id}/_compile URL 参数 #     参数 类型 说明     repo_id string 必需。规则仓库 ID。    HTTP 请求体 #     参数 类型 说明     fields array 编译时声明的字段列表（用于字段限定与范围匹配等）。   composite array 联合索引定义（用于多字段组合匹配）。   quiet boolean 是否启用静默模式。    说明：请求体是必需的，至少传空对象 {}。\nmatch_rules.delete #  删除指定的规则仓库。\nDELETE _match_rules/{repo_id} URL 参数 #     参数 类型 说明     repo_id string 必需。要删除的规则仓库 ID。    match_rules.import #  批量导入规则到规则仓库。\nPOST _match_rules/_import PUT _match_rules/_import POST _match_rules/{repo_id}/_import PUT _match_rules/{repo_id}/_import URL 参数 #     参数 类型 说明     repo_id string 规则仓库 ID。省略时自动生成 repo_{timestamp}。    HTTP 请求体 #  规则导入数据，包含规则定义列表。\n语义说明：\n POST /_match_rules/{repo_id}/_import：覆盖导入（upsert） PUT /_match_rules/{repo_id}/_import：追加导入（append） POST /_match_rules/_import：自动生成 repo_id 后覆盖导入 PUT /_match_rules/_import：自动生成 repo_id 后按追加语义导入（通常表现为新建规则库）  mget #  在单个请求中获取多个文档。\nGET _mget POST _mget GET {index}/_mget POST {index}/_mget GET {index}/{type}/_mget POST {index}/{type}/_mget HTTP 请求体 #  文档标识符；可以是 docs（包含完整文档信息）或 ids（当 URL 中提供了索引和类型时）。\n必需：是\nURL 参数 #     参数 类型 说明     stored_fields list 逗号分隔的要在响应中返回的存储字段列表   preference string 指定操作应在哪个节点或分片上执行（默认：随机）   realtime boolean 指定是否以实时模式或搜索模式执行操作   refresh boolean 在执行操作之前刷新包含该文档的分片   routing string 特定的路由值   _source list 是否返回 _source 字段，或要返回的字段列表   _source_excludes list 要从返回的 _source 字段中排除的字段列表   _source_includes list 要从 _source 字段中提取并返回的字段列表    msearch #  在单个请求中执行多个搜索操作。\nGET _msearch POST _msearch GET {index}/_msearch POST {index}/_msearch GET {index}/{type}/_msearch POST {index}/{type}/_msearch HTTP 请求体 #  请求定义（元数据-搜索请求定义对），以换行符分隔\n必需：是\nURL 参数 #     参数 类型 说明     search_type enum 搜索操作类型   max_concurrent_searches number 控制多搜索 API 同时执行的最大并发搜索数   typed_keys boolean 指定响应中聚合和建议器名称是否应以其各自类型作为前缀   pre_filter_shard_size number 预过滤阈值。当搜索请求扩展到的分片数超过此阈值时，会强制执行基于查询重写的预过滤来筛选搜索分片。例如，如果分片根据其重写方法无法匹配任何文档（如日期过滤器为必需但分片范围与查询不相交），此轮预过滤可显著减少分片数量。   max_concurrent_shard_requests number 每个子搜索在每个节点上并发执行的并发分片请求数。此值用于限制搜索对集群的影响，以限制并发分片请求的数量   rest_total_hits_as_int boolean 指示 hits.total 在搜索响应中是渲染为整数还是对象   ccs_minimize_roundtrips boolean 指示是否应在跨集群搜索请求执行过程中最小化网络往返    msearch_template #  在单个请求中使用模板执行多个搜索操作。\nGET _msearch/template POST _msearch/template GET {index}/_msearch/template POST {index}/_msearch/template GET {index}/{type}/_msearch/template POST {index}/{type}/_msearch/template HTTP 请求体 #  请求定义（元数据-搜索请求定义对），以换行符分隔\n必需：是\nURL 参数 #     参数 类型 说明     search_type enum 搜索操作类型   typed_keys boolean 指定响应中聚合和建议器名称是否应以其各自类型作为前缀   max_concurrent_searches number 控制多搜索 API 同时执行的最大并发搜索数   rest_total_hits_as_int boolean 指示 hits.total 在搜索响应中是渲染为整数还是对象   ccs_minimize_roundtrips boolean 指示是否应在跨集群搜索请求执行过程中最小化网络往返    mtermvectors #  在单个请求中获取多个文档的词项向量。\nGET _mtermvectors POST _mtermvectors GET {index}/_mtermvectors POST {index}/_mtermvectors GET {index}/{type}/_mtermvectors POST {index}/{type}/_mtermvectors HTTP 请求体 #  在此定义 ID、文档、参数或每个文档的参数列表。至少需要提供文档 ID 列表。详见文档。\n必需：否\nURL 参数 #     参数 类型 说明     ids list 逗号分隔的文档 ID 列表。必须将 ids 定义为参数，或在请求体中设置 \u0026ldquo;ids\u0026rdquo; 或 \u0026ldquo;docs\u0026rdquo;   term_statistics boolean 指定是否返回词条总频率和文档频率。适用于所有返回的文档，除非在请求体 \u0026ldquo;params\u0026rdquo; 或 \u0026ldquo;docs\u0026rdquo; 中另有指定。   field_statistics boolean 指定是否返回文档计数、文档频率总和以及词条总频率总和。适用于所有返回的文档，除非在请求体 \u0026ldquo;params\u0026rdquo; 或 \u0026ldquo;docs\u0026rdquo; 中另有指定。   fields list 逗号分隔的要返回的字段列表。适用于所有返回的文档，除非在请求体 \u0026ldquo;params\u0026rdquo; 或 \u0026ldquo;docs\u0026rdquo; 中另有指定。   offsets boolean 指定是否返回词条偏移量。适用于所有返回的文档，除非在请求体 \u0026ldquo;params\u0026rdquo; 或 \u0026ldquo;docs\u0026rdquo; 中另有指定。   positions boolean 指定是否返回词条位置。适用于所有返回的文档，除非在请求体 \u0026ldquo;params\u0026rdquo; 或 \u0026ldquo;docs\u0026rdquo; 中另有指定。   payloads boolean 指定是否返回词条负载。适用于所有返回的文档，除非在请求体 \u0026ldquo;params\u0026rdquo; 或 \u0026ldquo;docs\u0026rdquo; 中另有指定。   preference string 指定操作应在哪个节点或分片上执行（默认：随机） . Applies to all returned documents unless otherwise specified in body \u0026ldquo;params\u0026rdquo; or \u0026ldquo;docs\u0026rdquo;.   routing string 指定路由值。 Applies to all returned documents unless otherwise specified in body \u0026ldquo;params\u0026rdquo; or \u0026ldquo;docs\u0026rdquo;.   realtime boolean 指定请求是否为实时而非近实时（默认：true）。   version number 用于并发控制的显式版本号   version_type enum 指定版本类型    nodes.hot_threads #  返回集群中各节点的热线程信息。\nGET _nodes/hot_threads GET _nodes/{node_id}/hot_threads GET _cluster/nodes/hotthreads GET _cluster/nodes/{node_id}/hotthreads GET _nodes/hotthreads GET _nodes/{node_id}/hotthreads GET _cluster/nodes/hot_threads GET _cluster/nodes/{node_id}/hot_threads URL 参数 #     参数 类型 说明     interval time 线程第二次采样的时间间隔   snapshots number 线程堆栈跟踪的采样次数（默认：10）   threads number 指定要提供信息的线程数（默认：3）   ignore_idle_threads boolean 不显示处于已知空闲状态的线程，例如等待 socket select 或从空任务队列中拉取（默认：true）   type enum 采样类型（默认：cpu）   timeout time 显式操作超时时间    nodes.info #  返回集群中节点的信息。\nGET _nodes GET _nodes/{node_id} GET _nodes/{metric} GET _nodes/{node_id}/{metric} URL 参数 #     参数 类型 说明     flat_settings boolean 以扁平格式返回设置（默认：false）   timeout time 显式操作超时时间    nodes.reload_secure_settings #  重新加载安全设置（keystore）。\nPOST _nodes/reload_secure_settings POST _nodes/{node_id}/reload_secure_settings HTTP 请求体 #  包含 Easysearch 密钥库密码的对象\n必需：否\nURL 参数 #     参数 类型 说明     timeout time 显式操作超时时间    nodes.stats #  返回集群节点的统计信息。\nGET _nodes/stats GET _nodes/{node_id}/stats GET _nodes/stats/{metric} GET _nodes/{node_id}/stats/{metric} GET _nodes/stats/{metric}/{index_metric} GET _nodes/{node_id}/stats/{metric}/{index_metric} URL 参数 #     参数 类型 说明     completion_fields list 用于 fielddata 和 suggest 索引指标的字段列表（支持通配符），逗号分隔   fielddata_fields list 用于 fielddata 索引指标的逗号分隔字段列表（支持通配符）   fields list 用于 fielddata 和 completion 索引指标的字段列表（支持通配符），逗号分隔   groups boolean 用于 search 索引指标的逗号分隔搜索组列表   level enum 返回按索引、节点或分片级别聚合的索引统计信息   types list 用于 indexing 索引指标的文档类型列表，逗号分隔   timeout time 显式操作超时时间   include_segment_file_sizes boolean 是否报告每个 Lucene 索引文件的聚合磁盘使用情况（仅在请求段统计时适用）    nodes.usage #  返回节点上 REST 操作使用情况的低级信息。\nGET _nodes/usage GET _nodes/{node_id}/usage GET _nodes/usage/{metric} GET _nodes/{node_id}/usage/{metric} URL 参数 #     参数 类型 说明     timeout time 显式操作超时时间    ping #  检查集群是否正在运行。\nHEAD put_script #  创建或更新存储的脚本。\nPUT _scripts/{id} POST _scripts/{id} PUT _scripts/{id}/{context} POST _scripts/{id}/{context} HTTP 请求体 #  The document\n必需：是\nURL 参数 #     参数 类型 说明     timeout time 显式操作超时时间   master_timeout time 指定连接主节点的超时时间   context string 用于编译脚本的上下文名称    rank_eval #  允许对一组典型搜索查询评估排名搜索结果的质量\nGET _rank_eval POST _rank_eval GET {index}/_rank_eval POST {index}/_rank_eval HTTP 请求体 #  排名评估搜索定义，包括搜索请求、文档评分和排名指标定义。\n必需：是\nURL 参数 #     参数 类型 说明     ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   search_type enum 搜索操作类型    reindex #  允许将文档从一个索引复制到另一个索引，可选地过滤源文档 （通过查询过滤）、更改目标索引设置，或从远程集群 获取文档。\nPOST _reindex HTTP 请求体 #  使用 Query DSL 的搜索定义 and the prototype for the index request.\n必需：是\nURL 参数 #     参数 类型 说明     refresh boolean 受影响的索引是否应该刷新？   timeout time 每个批量请求等待不可用分片的时间。   wait_for_active_shards string 设置在执行重新索引操作之前必须处于活跃状态的分片副本数。默认为 1，即仅主分片。设为 all 表示所有分片副本，否则设为小于或等于分片总副本数（副本数 + 1）的任意非负值   wait_for_completion boolean 请求是否应阻塞直到 reindex 完成。   requests_per_second number 在此请求上设置的节流值，单位为每秒子请求数。-1 表示不节流。   scroll time 控制搜索上下文的保持时间   slices number|string 该任务应被分成的切片数。默认为 1（不拆分子任务），可设为 auto。   max_docs number 要处理的最大文档数（默认：所有文档）    reindex_rethrottle #  修改 reindex 操作的每秒请求数。\nPOST _reindex/{task_id}/_rethrottle URL 参数 #     参数 类型 说明     requests_per_second number 此请求的节流值（每秒浮点子请求数）。-1 表示不节流。    rollup.create #  创建一个 rollup 任务。Rollup 任务可以将历史数据按时间维度聚合压缩，减少存储开销。\nPUT _rollup/jobs/{rollupID} URL 参数 #     参数 类型 说明     rollupID string 必需。Rollup 任务的唯一标识符。   if_seq_no number 仅在文档序列号匹配时执行操作，用于乐观并发控制。   if_primary_term number 仅在文档主要任期匹配时执行操作，用于乐观并发控制。   replace boolean 是否替换已有的同名 rollup 任务。默认为 false。   refresh string 控制请求所做更改何时可被搜索到。有效值：true、false、wait_for。    HTTP 请求体 #  Rollup 任务配置，包含源索引、目标索引、聚合维度、调度策略等定义。\nrollup.delete #  删除指定的 rollup 任务。\nDELETE _rollup/jobs/{rollupID} URL 参数 #     参数 类型 说明     rollupID string 必需。要删除的 rollup 任务 ID。   refresh string 控制请求所做更改何时可被搜索到。有效值：true、false、wait_for。    rollup.explain #  查看一个或多个 rollup 任务的运行详情和执行状态。\nGET _rollup/jobs/{rollupID}/_explain URL 参数 #     参数 类型 说明     rollupID string 必需。Rollup 任务 ID，多个 ID 之间用逗号分隔。    rollup.get #  获取 rollup 任务的定义信息。可以获取单个任务或列出所有任务。\nGET _rollup/jobs GET _rollup/jobs/{rollupID} HEAD _rollup/jobs/{rollupID} URL 参数 #     参数 类型 说明     rollupID string Rollup 任务 ID。省略则列出所有 rollup 任务。   search string 按名称搜索 rollup 任务的过滤字符串。   from number 分页起始位置（从 0 开始）。   size number 每页返回的 rollup 任务数量。   sortField string 排序字段名称。   sortDirection string 排序方向。有效值：asc、desc。    rollup.search #  搜索包含 rollup 数据的索引。当索引配置了 rollup 任务时，搜索请求会自动合并原始数据与 rollup 聚合数据。此 API 是对标准 _search 的增强，通过 enable_rollup 参数控制是否启用 rollup 搜索。\nGET {index}/_search POST {index}/_search URL 参数 #     参数 类型 说明     enable_rollup boolean 是否启用 rollup 搜索。默认为 true。设为 false 时退化为标准搜索。   debug boolean 是否启用调试模式，输出更多日志信息。默认为 false。    HTTP 请求体 #  标准搜索请求体。当启用 rollup 搜索时，查询必须包含聚合（aggregations），且只能指定单个索引，size 必须为 0。\nrollup.start #  启动已停止的 rollup 任务。\nPOST _rollup/jobs/{rollupID}/_start URL 参数 #     参数 类型 说明     rollupID string 必需。要启动的 rollup 任务 ID。    rollup.stop #  停止正在运行的 rollup 任务。\nPOST _rollup/jobs/{rollupID}/_stop URL 参数 #     参数 类型 说明     rollupID string 必需。要停止的 rollup 任务 ID。    rollup.update #  更新已有的 rollup 任务的部分配置（如执行间隔、分页大小等）。\nPOST _rollup/jobs/{rollupID} URL 参数 #     参数 类型 说明     rollupID string 必需。要更新的 rollup 任务 ID。    HTTP 请求体 #     参数 类型 说明     interval string 新的执行间隔。   page_size number 新的分页大小。    render_search_template #  使用 Mustache 模板语言预渲染搜索定义。\nGET _render/template POST _render/template GET _render/template/{id} POST _render/template/{id} HTTP 请求体 #  搜索定义模板及其参数\nscripts_painless_execute #  执行任意 Painless 脚本并返回结果。\nGET _scripts/painless/_execute POST _scripts/painless/_execute HTTP 请求体 #  要执行的脚本\nscroll #  通过滚动方式从单次搜索请求中获取大量结果。\nGET _search/scroll POST _search/scroll GET _search/scroll/{scroll_id} POST _search/scroll/{scroll_id} HTTP 请求体 #  如未通过 URL 或查询参数传递的滚动 ID。\nURL 参数 #     参数 类型 说明     scroll time 指定滚动搜索的索引一致性视图应维持多长时间   scroll_id string 滚动搜索的滚动 ID   rest_total_hits_as_int boolean 指示 hits.total 在搜索响应中是渲染为整数还是对象    search #  执行搜索查询，返回匹配的文档。\nGET _search POST _search GET {index}/_search POST {index}/_search GET {index}/{type}/_search POST {index}/{type}/_search HTTP 请求体 #  使用 Query DSL 的搜索定义\nURL 参数 #     参数 类型 说明     analyzer string 用于查询字符串的分析器   analyze_wildcard boolean 是否分析通配符和前缀查询（默认：false）   ccs_minimize_roundtrips boolean 指示是否应在跨集群搜索请求执行过程中最小化网络往返   default_operator enum 查询字符串查询的默认运算符（AND 或 OR）   df string 查询字符串中未指定字段前缀时使用的默认字段   explain boolean 指定是否返回命中结果中分数计算的详细信息   stored_fields list 要作为命中结果的一部分返回的存储字段，逗号分隔   docvalue_fields list 要作为 docvalue 表示返回的字段列表（逗号分隔），用于每个命中结果   from number 起始偏移量（默认：0）   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   ignore_throttled boolean 当被节流时，是否忽略指定的具体、扩展或别名索引   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   lenient boolean 是否忽略基于格式的查询失败（如向数字字段提供文本）   preference string 指定操作应在哪个节点或分片上执行（默认：随机）   q string Lucene 查询字符串语法的查询   routing list 指定路由值列表，逗号分隔   scroll time 指定滚动搜索的索引一致性视图应维持多长时间   search_type enum 搜索操作类型   size number 要返回的命中数（默认：10）   sort list 逗号分隔的 : 对列表   _source list 是否返回 _source 字段，或要返回的字段列表   _source_excludes list 要从返回的 _source 字段中排除的字段列表   _source_includes list 要从 _source 字段中提取并返回的字段列表   terminate_after number 每个分片收集的最大文档数，达到此数量后查询将提前终止。   stats list 请求的特定 \u0026lsquo;tag\u0026rsquo;，用于日志和统计目的   suggest_field string 指定用于建议的字段   suggest_mode enum 指定建议模式   suggest_size number 响应中返回多少个建议   suggest_text string 要返回建议的源文本   timeout time 显式操作超时时间   track_scores boolean 是否计算并返回分数，即使它们不用于排序   track_total_hits boolean 指示是否应跟踪匹配查询的文档数量   allow_partial_search_results boolean 指示在部分搜索失败或超时时是否应返回错误   typed_keys boolean 指定响应中聚合和建议器名称是否应以其各自类型作为前缀   version boolean 是否在命中结果中返回文档版本号   seq_no_primary_term boolean 指定是否返回每个命中结果最后修改的序列号和主要任期   request_cache boolean 指定此请求是否使用请求缓存，默认使用索引级别设置   batched_reduce_size number 协调节点上一次应归约的分片结果数量。当请求涉及的潜在分片数量较大时，此值可作为保护机制减少每个搜索请求的内存开销。   max_concurrent_shard_requests number 此搜索在每个节点上并发执行的并发分片请求数。此值用于限制搜索对集群的影响，以限制并发分片请求的数量   pre_filter_shard_size number 预过滤阈值。当搜索请求扩展到的分片数超过此阈值时，会强制执行基于查询重写的预过滤来筛选搜索分片。例如，如果分片根据其重写方法无法匹配任何文档（如日期过滤器为必需但分片范围与查询不相交），此轮预过滤可显著减少分片数量。   rest_total_hits_as_int boolean 指示 hits.total 在搜索响应中是渲染为整数还是对象    search_pipeline.delete #  删除指定的搜索管道。\nDELETE _search/pipeline/{id} URL 参数 #     参数 类型 说明     master_timeout time 指定连接主节点的超时时间   timeout time 显式操作超时时间    search_pipeline.get #  获取一个或多个搜索管道的定义。不指定 ID 时返回所有管道。\nGET _search/pipeline GET _search/pipeline/{id} URL 参数 #     参数 类型 说明     master_timeout time 指定连接主节点的超时时间    search_pipeline.put #  创建或更新搜索管道。如果管道的请求处理器中包含 semantic_query_enricher，其 api_key 字段会在存储前自动加密。\nPUT _search/pipeline/{id} HTTP 请求体 #  搜索管道的配置定义（包括处理器等）。\n必需：是\nURL 参数 #     参数 类型 说明     master_timeout time 指定连接主节点的超时时间   timeout time 显式操作超时时间    search_shards #  返回搜索请求将要执行的索引和分片信息。\nGET _search_shards POST _search_shards GET {index}/_search_shards POST {index}/_search_shards URL 参数 #     参数 类型 说明     preference string 指定操作应在哪个节点或分片上执行（默认：随机）   routing string 特定的路由值   local boolean 返回本地信息，不从主节点获取状态（默认：false）   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。    search_template #  使用 Mustache 模板语言预渲染搜索定义。\nGET _search/template POST _search/template GET {index}/_search/template POST {index}/_search/template GET {index}/{type}/_search/template POST {index}/{type}/_search/template HTTP 请求体 #  搜索定义模板及其参数\n必需：是\nURL 参数 #     参数 类型 说明     ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   ignore_throttled boolean 当被节流时，是否忽略指定的具体、扩展或别名索引   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   preference string 指定操作应在哪个节点或分片上执行（默认：随机）   routing list 指定路由值列表，逗号分隔   scroll time 指定滚动搜索的索引一致性视图应维持多长时间   search_type enum 搜索操作类型   explain boolean 指定是否返回命中结果中分数计算的详细信息   profile boolean 指定是否对查询执行进行性能分析   typed_keys boolean 指定响应中聚合和建议器名称是否应以其各自类型作为前缀   rest_total_hits_as_int boolean 指示 hits.total 在搜索响应中是渲染为整数还是对象   ccs_minimize_roundtrips boolean 指示是否应在跨集群搜索请求执行过程中最小化网络往返    security.account #  获取或更新当前登录用户的账户信息，包括密码修改。\nGET _security/account PUT _security/account HTTP 请求体（PUT） #  用户账户更新信息，如新密码等。\nsecurity.authinfo #  获取当前经过身份验证的用户信息，包括角色、后端角色等。\nGET _security/authinfo POST _security/authinfo security.authtoken #  生成身份验证令牌。\nPOST _security/authtoken HTTP 请求体 #  令牌请求参数。\nsecurity.cache #  刷新安全模块缓存，使安全配置更改立即生效。\nDELETE _security/cache GET _security/cache PUT _security/cache POST _security/cache security.config #  获取或更新安全模块的全局配置。\nGET _security/securityconfig/ PUT _security/securityconfig/{name} PATCH _security/securityconfig/ URL 参数 #     参数 类型 说明     name string 配置名称（用于 PUT 操作）。    HTTP 请求体（PUT/PATCH） #  安全配置内容。\nsecurity.nodesdn #  管理可信节点的 DN（Distinguished Name）列表，用于节点间 TLS 通信认证。\nGET _security/nodesdn/ GET _security/nodesdn/{name} DELETE _security/nodesdn/{name} PUT _security/nodesdn/{name} PATCH _security/nodesdn/ PATCH _security/nodesdn/{name} URL 参数 #     参数 类型 说明     name string 节点 DN 条目名称。省略则列出所有条目。    HTTP 请求体（PUT/PATCH） #  节点 DN 配置内容。\nsecurity.permissionsinfo #  获取当前用户的权限信息。\nGET _security/permissionsinfo security.privilege #  管理操作组（权限组），用于将多个权限打包为一个命名集合。\nGET _security/privilege/ GET _security/privilege/{name} DELETE _security/privilege/{name} PUT _security/privilege/{name} PATCH _security/privilege/ PATCH _security/privilege/{name} URL 参数 #     参数 类型 说明     name string 操作组名称。省略则列出所有操作组。    HTTP 请求体（PUT/PATCH） #  操作组定义，包含允许的操作列表。\nsecurity.role #  管理安全角色。角色定义了用户可以访问的集群权限和索引权限。\nGET _security/role/ GET _security/role/{name} DELETE _security/role/{name} PUT _security/role/{name} PATCH _security/role/ PATCH _security/role/{name} URL 参数 #     参数 类型 说明     name string 角色名称。省略则列出所有角色。    HTTP 请求体（PUT/PATCH） #  角色定义，包含集群权限、索引权限、租户权限等。\nsecurity.role_mapping #  管理角色映射，将后端角色、用户等映射到安全角色。\nGET _security/role_mapping/ GET _security/role_mapping/{name} DELETE _security/role_mapping/{name} PUT _security/role_mapping/{name} PATCH _security/role_mapping/ PATCH _security/role_mapping/{name} URL 参数 #     参数 类型 说明     name string 角色映射名称。省略则列出所有映射。    HTTP 请求体（PUT/PATCH） #  角色映射定义，包含后端角色、用户、主机列表等。\nsecurity.ssl_certificates #  获取 SSL/TLS 证书信息。\nGET _security/ssl/certificates security.ssl_certs #  获取节点的 SSL/TLS 证书详细信息。\nGET _security/ssl/certs security.ssl_reload_certs #  重新加载指定类型的 SSL/TLS 证书，无需重启节点。\nPUT _security/ssl/{certType}/reloadcerts/ URL 参数 #     参数 类型 说明     certType string 必需。证书类型。有效值：http、transport。    security.sslinfo #  获取节点的 SSL/TLS 配置信息。\nGET _security/sslinfo security.user #  管理内部用户。支持创建、读取、更新和删除用户。\nGET _security/user/ GET _security/user/{name} DELETE _security/user/{name} PUT _security/user/{name} PATCH _security/user/ PATCH _security/user/{name} URL 参数 #     参数 类型 说明     name string 用户名。省略则列出所有用户。    HTTP 请求体（PUT/PATCH） #  用户定义，包含密码、后端角色、属性等。\nsecurity.validate #  验证安全模块配置的有效性。\nGET _security/validate security.whitelist #  管理 API 白名单。控制哪些 REST API 端点可以被访问。\nGET _security/whitelist PUT _security/whitelist PATCH _security/whitelist HTTP 请求体（PUT/PATCH） #  白名单配置，包含允许的 API 端点列表。\nsnapshot.cleanup_repository #  清理快照仓库中未被引用的过期数据。\nPOST _snapshot/{repository}/_cleanup URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   timeout time 显式操作超时时间    snapshot.clone #  在同一仓库内将快照中的索引克隆到另一个快照。\nPUT _snapshot/{repository}/{snapshot}/_clone/{target_snapshot} HTTP 请求体 #  快照克隆定义\n必需：是\nURL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间    snapshot.create #  在仓库中创建快照。\nPUT _snapshot/{repository}/{snapshot} POST _snapshot/{repository}/{snapshot} HTTP 请求体 #  快照定义\n必需：否\nURL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   wait_for_completion boolean 此请求是否应等待操作完成后再返回    snapshot.create_repository #  创建快照仓库。\nPUT _snapshot/{repository} POST _snapshot/{repository} HTTP 请求体 #  仓库定义\n必需：是\nURL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   timeout time 显式操作超时时间   verify boolean 创建后是否验证仓库    snapshot.delete #  删除快照。\nDELETE _snapshot/{repository}/{snapshot} URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间    snapshot.delete_repository #  删除快照仓库。\nDELETE _snapshot/{repository} URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   timeout time 显式操作超时时间    snapshot.get #  返回快照的信息。\nGET _snapshot/{repository}/{snapshot} URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   ignore_unavailable boolean 是否忽略不可用的快照，默认为 false，即会抛出 SnapshotMissingException   verbose boolean 是否显示详细快照信息，还是仅显示仓库索引 blob 中的基本信息    snapshot.get_repository #  返回快照仓库的信息。\nGET _snapshot GET _snapshot/{repository} URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   local boolean 返回本地信息，不从主节点获取状态（默认：false）    snapshot.restore #  从快照恢复数据。\nPOST _snapshot/{repository}/{snapshot}/_restore HTTP 请求体 #  要恢复的详细信息\n必需：否\nURL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   wait_for_completion boolean 此请求是否应等待操作完成后再返回    snapshot.status #  返回快照的详细状态信息。\nGET _snapshot/_status GET _snapshot/{repository}/_status GET _snapshot/{repository}/{snapshot}/_status URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   ignore_unavailable boolean 是否忽略不可用的快照，默认为 false，即会抛出 SnapshotMissingException    snapshot.verify_repository #  验证快照仓库的可用性。\nPOST _snapshot/{repository}/_verify URL 参数 #     参数 类型 说明     master_timeout time 连接主节点的显式超时时间   timeout time 显式操作超时时间    sql.close_cursor #  关闭 SQL 查询的游标，释放服务端资源。\nPOST _sql/close HTTP 请求体 #  包含要关闭的游标 ID。\nsql.explain #  对 SQL 查询语句进行解析，返回对应的 Easysearch 查询 DSL，而不实际执行查询。\nPOST _sql/_explain HTTP 请求体 #  SQL 查询语句。\nsql.query #  执行 SQL 查询并返回结果。支持 JDBC 格式输出。\nPOST _sql URL 参数 #     参数 类型 说明     format string 结果格式，如 jdbc、csv、json 等。   flat string 是否展平结果。   separator string CSV 输出分隔符。    HTTP 请求体 #  SQL 查询语句及参数。\nsql.settings #  更新 SQL 插件的查询设置。\nPUT _sql/_query/settings HTTP 请求体 #     参数 类型 说明     transient object 临时 SQL 设置，节点重启后失效。   persistent object 持久 SQL 设置，节点重启后仍保留。    sql.stats #  获取 SQL 插件的统计指标。\nPOST _sql/stats tasks.cancel #  取消正在执行的任务（如果该任务支持通过 API 取消）。\nPOST _tasks/_cancel POST _tasks/{task_id}/_cancel URL 参数 #     参数 类型 说明     nodes list 用于限制返回信息的节点 ID 或名称列表（逗号分隔）；使用 _local 返回当前连接节点的信息，留空获取所有节点的信息   actions list 要取消的操作列表（逗号分隔）。留空表示取消全部。   parent_task_id string 取消指定父任务 ID 的任务（node_id:task_number）。设为 -1 取消全部。   wait_for_completion boolean 请求是否应阻塞直到任务及其子任务的取消完成。默认为 false    tasks.get #  返回指定任务的详细信息。\nGET _tasks/{task_id} URL 参数 #     参数 类型 说明     wait_for_completion boolean 等待匹配的任务完成（默认：false）   timeout time 显式操作超时时间    tasks.list #  返回正在执行的任务列表。\nGET _tasks URL 参数 #     参数 类型 说明     nodes list 用于限制返回信息的节点 ID 或名称列表（逗号分隔）；使用 _local 返回当前连接节点的信息，留空获取所有节点的信息   actions list 要返回的操作列表（逗号分隔）。留空返回全部。   detailed boolean 返回详细的任务信息（默认：false）   parent_task_id string 返回指定父任务 ID 的任务（node_id:task_number）。设为 -1 返回全部。   wait_for_completion boolean 等待匹配的任务完成（默认：false）   group_by enum 按节点或父/子关系对任务进行分组   timeout time 显式操作超时时间    termvectors #  返回指定文档字段中词项的信息和统计数据。\nGET {index}/_termvectors/{id} POST {index}/_termvectors/{id} GET {index}/_termvectors POST {index}/_termvectors GET {index}/{type}/{id}/_termvectors POST {index}/{type}/{id}/_termvectors GET {index}/{type}/_termvectors POST {index}/{type}/_termvectors HTTP 请求体 #  定义参数和/或提供文档以获取词条向量。详见文档。\n必需：否\nURL 参数 #     参数 类型 说明     term_statistics boolean 指定是否返回词条总频率和文档频率。   field_statistics boolean 指定是否返回文档计数、文档频率总和以及词条总频率总和。   fields list 逗号分隔的要返回的字段列表。   offsets boolean 指定是否返回词条偏移量。   positions boolean 指定是否返回词条位置。   payloads boolean 指定是否返回词条负载。   preference string 指定操作应在哪个节点或分片上执行（默认：随机）.   routing string 指定路由值。   realtime boolean 指定请求是否为实时而非近实时（默认：true）。   version number 用于并发控制的显式版本号   version_type enum 指定版本类型    创建一个 transform（数据转换）任务。Transform 可以将源索引的数据通过聚合转换后写入目标索引。\nPUT _ilm/_transform/{transformID} URL 参数    参数 类型 说明     transformID string 必需。Transform 任务的唯一标识符。   if_seq_no number 仅在序列号匹配时执行操作，用于乐观并发控制。   if_primary_term number 仅在主要任期匹配时执行操作，用于乐观并发控制。   refresh string 控制请求所做更改何时可被搜索到。有效值：true、false、wait_for。    HTTP 请求体 Transform 任务配置，包含源索引、目标索引、聚合定义、分组字段等。\ntransform.delete 删除一个或多个 transform 任务。\nDELETE _ilm/_transform/{transformID} URL 参数    参数 类型 说明     transformID string 必需。以逗号分隔的 transform 任务 ID。   force boolean 是否强制删除正在运行的 transform 任务。    transform.explain 查看一个或多个 transform 任务的运行状态和执行详情。\nGET _ilm/_transform/{transformID}/_explain URL 参数    参数 类型 说明     transformID string 必需。以逗号分隔的 transform 任务 ID。    transform.get 获取 transform 任务的定义信息。可以获取单个任务或列出所有任务。\nGET _ilm/_transform GET _ilm/_transform/{transformID} HEAD _ilm/_transform/{transformID} URL 参数    参数 类型 说明     transformID string Transform 任务 ID。省略则列出所有任务。   search string 按名称搜索 transform 任务的过滤字符串。   from number 分页起始位置（从 0 开始）。   size number 每页返回数量。   sortField string 排序字段名称。   sortDirection string 排序方向。有效值：asc、desc。    transform.preview 预览 transform 任务的输出结果，不实际创建目标索引。\nPOST _ilm/_transform POST _ilm/_transform/_preview HTTP 请求体 Transform 任务配置，与 transform.create 相同。\ntransform.start 启动已停止的 transform 任务。\nPOST _ilm/_transform/{transformID}/_start URL 参数    参数 类型 说明     transformID string 必需。要启动的 transform 任务 ID。    transform.stop 停止正在运行的 transform 任务。\nPOST _ilm/_transform/{transformID}/_stop URL 参数    参数 类型 说明     transformID string 必需。要停止的 transform 任务 ID。   \u0026ndash;\u0026gt;      update #  使用脚本或部分文档更新文档。\nPOST {index}/_update/{id} POST {index}/{type}/{id}/_update HTTP 请求体 #  请求定义需要 script 或部分 doc\n必需：是\nURL 参数 #     参数 类型 说明     wait_for_active_shards string 设置在执行更新操作之前必须处于活跃状态的分片副本数。默认为 1，即仅主分片。设为 all 表示所有分片副本，否则设为小于或等于分片总副本数（副本数 + 1）的任意非负值   _source list 是否返回 _source 字段，或要返回的字段列表   _source_excludes list 要从返回的 _source 字段中排除的字段列表   _source_includes list 要从 _source 字段中提取并返回的字段列表   lang string 脚本语言（默认：painless）   refresh enum 如果为 true，刷新受影响的分片使操作对搜索可见；如果为 wait_for，等待刷新完成后再返回；如果为 false（默认），不执行刷新操作。   retry_on_conflict number 指定发生冲突时操作应重试的次数（默认：0）   routing string 特定的路由值   timeout time 显式操作超时时间   if_seq_no number 仅在更改文档的最后操作具有指定序列号时才执行更新操作   if_primary_term number 仅在更改文档的最后操作具有指定主要任期时才执行更新操作   require_alias boolean 当为 true 时，要求目标必须是别名。默认为 false    update_by_query #  对索引中的每个文档执行更新而不更改源数据， 例如以应用映射变更。\nPOST {index}/_update_by_query POST {index}/{type}/_update_by_query HTTP 请求体 #  使用 Query DSL 的搜索定义\nURL 参数 #     参数 类型 说明     analyzer string 用于查询字符串的分析器   analyze_wildcard boolean 是否分析通配符和前缀查询（默认：false）   default_operator enum 查询字符串查询的默认运算符（AND 或 OR）   df string 查询字符串中未指定字段前缀时使用的默认字段   from number 起始偏移量（默认：0）   ignore_unavailable boolean 当指定的具体索引不可用（缺失或已关闭）时是否忽略   allow_no_indices boolean 当通配符表达式不匹配任何具体索引时是否忽略（包括 _all 或未指定索引的情况）。   conflicts enum 当 update-by-query 遇到版本冲突时的行为   expand_wildcards enum 是否将通配符表达式扩展为打开的、关闭的或全部索引。   lenient boolean 是否忽略基于格式的查询失败（如向数字字段提供文本）   pipeline string 在此操作发出的索引请求上设置的摄取管道。（默认：无）   preference string 指定操作应在哪个节点或分片上执行（默认：随机）   q string Lucene 查询字符串语法的查询   routing list 指定路由值列表，逗号分隔   scroll time 指定滚动搜索的索引一致性视图应维持多长时间   search_type enum 搜索操作类型   search_timeout time 每个搜索请求的显式超时时间。默认无超时。   size number 已弃用，请改用 max_docs   max_docs number 要处理的最大文档数（默认：所有文档）   sort list 逗号分隔的 : 对列表   _source list 是否返回 _source 字段，或要返回的字段列表   _source_excludes list 要从返回的 _source 字段中排除的字段列表   _source_includes list 要从 _source 字段中提取并返回的字段列表   terminate_after number 每个分片收集的最大文档数，达到此数量后查询将提前终止。   stats list 请求的特定 \u0026lsquo;tag\u0026rsquo;，用于日志和统计目的   version boolean 是否在命中结果中返回文档版本号   version_type boolean 文档命中时是否应增加版本号（内部）（重新索引时）   request_cache boolean 指定此请求是否使用请求缓存，默认使用索引级别设置   refresh boolean 受影响的索引是否应该刷新？   timeout time 每个批量请求等待不可用分片的时间。   wait_for_active_shards string 设置在执行按查询更新操作之前必须处于活跃状态的分片副本数。默认为 1，即仅主分片。设为 all 表示所有分片副本，否则设为小于或等于分片总副本数（副本数 + 1）的任意非负值   scroll_size number 驱动 update-by-query 的滚动请求大小   wait_for_completion boolean 请求是否应阻塞直到按查询更新操作完成。   requests_per_second number 在此请求上设置的节流值，单位为每秒子请求数。-1 表示不节流。   slices number|string 此任务应被分成的切片数。默认为 1，即任务不会被分成子任务。可以设为 auto。    update_by_query_rethrottle #  修改 update-by-query 操作的每秒请求数。\nPOST _update_by_query/{task_id}/_rethrottle URL 参数 #     参数 类型 说明     requests_per_second number 此请求的节流值（每秒浮点子请求数）。-1 表示不节流。    ","subcategory":null,"summary":"","tags":null,"title":"常用 API 速查","url":"/easysearch/main/docs/api-reference/rest-api-reference/"},{"category":null,"content":"地理位置实践 #  本页以“场景实践”的形式给出几个常见地理位置需求下的解决方案，帮助你在实际应用中更好地使用地理位置功能。\n在具体 API 上，Easysearch 主要通过：\n geo_distance / _geo_distance：按距离过滤与排序 geo_shape：做复杂形状的围栏与空间关系判断 geohash_grid / geo_bounds：在地图上做网格聚合与视野控制  下面的配方会围绕这些能力展开。\n地理位置聚合 #  虽然按照地理位置对结果进行过滤或者打分很有用，但是在地图上呈现信息给用户通常更加有用。一个查询可能会返回太多结果以至于不能单独地展现每一个地理坐标点，但是地理位置聚合可以用来将地理坐标聚集到更加容易管理的 buckets 中。\n处理 geo_point 类型字段的三种聚合：\n地理距离聚合 #  geo_distance 聚合对一些搜索非常有用，例如找到所有距离我 1km 以内的披萨店。搜索结果应该也的确被限制在用户指定 1km 范围内，但是我们可以添加在 2km 范围内找到的其他结果：\nGET /attractions/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pizza\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;geo_bounding_box\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;top_left\u0026#34;: { \u0026#34;lat\u0026#34;: 40.8, \u0026#34;lon\u0026#34;: -74.1 }, \u0026#34;bottom_right\u0026#34;: { \u0026#34;lat\u0026#34;: 40.4, \u0026#34;lon\u0026#34;: -73.7 } } } } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;per_ring\u0026#34;: { \u0026#34;geo_distance\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;km\u0026#34;, \u0026#34;origin\u0026#34;: { \u0026#34;lat\u0026#34;: 40.712, \u0026#34;lon\u0026#34;: -73.988 }, \u0026#34;ranges\u0026#34;: [ { \u0026#34;from\u0026#34;: 0, \u0026#34;to\u0026#34;: 1 }, { \u0026#34;from\u0026#34;: 1, \u0026#34;to\u0026#34;: 2 } ] } } }, \u0026#34;post_filter\u0026#34;: { \u0026#34;geo_distance\u0026#34;: { \u0026#34;distance\u0026#34;: \u0026#34;1km\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.712, \u0026#34;lon\u0026#34;: -73.988 } } } } 主查询查找名称中含有 pizza 的饭店。geo_bounding_box 筛选那些只在纽约区域的结果。geo_distance 聚合统计距离用户 1km 以内，1km 到 2km 的结果的数量。最后，post_filter 将结果缩小至那些在用户 1km 范围内的饭店。\n在这个例子中，我们计算了落在每个同心环内的饭店数量。当然，我们可以在 per_rings 聚合下面嵌套子聚合来计算每个环的平均价格、最受欢迎程度，等等。\nGeohash 网格聚合 #  geohash_grid 聚合将文档按照 geohash 范围来分组，用来显示在地图上。这对于在地图上可视化大量地理坐标点非常有用。\nGET /attractions/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;world\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 2 } } } } precision 参数控制 geohash 的精度级别。精度越高，网格越细，但聚合的 buckets 也越多。\n地理边界聚合 #  geo_bounds 聚合返回一个包含所有地理位置坐标点的边界的经纬度坐标，这对显示地图时缩放比例的选择非常有用。\nGET /attractions/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;viewport\u0026#34;: { \u0026#34;geo_bounds\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34; } } } } 返回结果会包含一个边界框，可以用来设置地图的初始视图。\n常见场景实践 #  场景一：附近的人/商家 #  需求：找到距离用户当前位置一定范围内的商家或用户。\n方案：\n 使用 geo_distance 过滤器限制距离范围 使用 _geo_distance 排序按距离排序 或者使用 function_score 结合距离衰减函数来影响评分  示例：\nGET /businesses/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;geo_distance\u0026#34;: { \u0026#34;distance\u0026#34;: \u0026#34;5km\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.715, \u0026#34;lon\u0026#34;: -73.988 } } } } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;_geo_distance\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.715, \u0026#34;lon\u0026#34;: -73.988 }, \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;km\u0026#34; } } ] } 场景二：地理围栏 #  需求：判断某个位置是否在指定的地理区域内。\n方案：\n 使用 geo_shape 查询判断点是否在形状内 对于复杂形状，使用预索引形状提高性能  示例：\nGET /locations/_search { \u0026#34;query\u0026#34;: { \u0026#34;geo_shape\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;relation\u0026#34;: \u0026#34;within\u0026#34;, \u0026#34;shape\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34;: [[ [4.88330,52.38617], [4.87463,52.37254], [4.87875,52.36369], [4.88939,52.35850], [4.89840,52.35755], [4.91909,52.36217], [4.92656,52.36594], [4.93368,52.36615], [4.93342,52.37275], [4.92690,52.37632], [4.88330,52.38617] ]] } } } } } 场景三：地图可视化 #  需求：在地图上展示大量地理坐标点的分布情况。\n方案：\n 使用 geohash_grid 聚合将点聚合到网格中 使用 geo_bounds 聚合获取边界框设置地图视图 使用 geo_distance 聚合按距离范围分组  示例：\nGET /attractions/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;map_view\u0026#34;: { \u0026#34;geo_bounds\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34; } }, \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 6 }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } } } } } } 场景四：结合全文搜索和地理位置 #  需求：搜索附近的餐厅，同时考虑名称匹配度和距离。\n方案：\n 使用 bool 查询结合全文搜索和地理位置过滤 使用 function_score 结合文本相关度和距离衰减  示例：\nGET /restaurants/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pizza\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;geo_distance\u0026#34;: { \u0026#34;distance\u0026#34;: \u0026#34;10km\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.715, \u0026#34;lon\u0026#34;: -73.988 } } } } }, \u0026#34;functions\u0026#34;: [ { \u0026#34;gauss\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;origin\u0026#34;: { \u0026#34;lat\u0026#34;: 40.715, \u0026#34;lon\u0026#34;: -73.988 }, \u0026#34;offset\u0026#34;: \u0026#34;2km\u0026#34;, \u0026#34;scale\u0026#34;: \u0026#34;3km\u0026#34; } } } ], \u0026#34;score_mode\u0026#34;: \u0026#34;multiply\u0026#34; } } } 性能优化建议 #   优先使用 geo_bounding_box：如果精度要求不高，使用 geo_bounding_box 比 geo_distance 更高效 先用 bounding_box 缩小范围：对大数据集，先用矩形框过滤大幅减少候选集，再用 geo_distance 精确筛选 选择合适的距离计算方式：根据精度要求选择合适的 distance_type（arc 默认精度高，plane 更快但靠近两极精度下降） 预索引复杂形状：对于复杂的地理形状，使用预索引形状可以避免每次查询都传递大量坐标 合理设置精度：对于 geo_shape，根据实际需求设置合适的精度和距离误差，避免过度索引  小结 #   Geohashes 是一种将经纬度编码成字符串的方式，可以用于高效索引 地理位置聚合可以用于地图可视化和数据分析 常见场景包括附近的人/商家、地理围栏、地图可视化等 可以结合全文搜索和地理位置来提供更好的搜索体验 性能优化包括选择合适的过滤器、使用索引优化、预索引形状等  下一步可以继续阅读：\n  Geo Point 字段类型  Geo Shape 字段类型  参考手册（API 与参数） #    geo_bounding_box 查询  geo_distance 查询  geo_polygon 查询  geo_shape 查询  GeoPoint 字段类型  GeoShape 字段类型  ","subcategory":null,"summary":"","tags":null,"title":"地理位置实践","url":"/easysearch/main/docs/features/geo-search/geo-recipes/"},{"category":null,"content":"高级配置参数 #  本页面详细列举 Easysearch 的所有高级配置参数，包括集群、节点、索引等多层级配置。\n 概述 #  Easysearch 配置涉及多个层级：\n 节点级：影响单个节点的行为（node., path., 等） 集群级：影响整个集群的行为（cluster., discovery., 等） 索引级：影响单个索引的行为（index.*, 等） 传输层：网络通信相关（transport., http., 等）   集群协调配置 #  集群选举、节点发现和主节点选择的相关配置。\n选举配置 #     参数 默认值 动态 说明     cluster.election.initial_timeout 100ms ✗ 初始选举超时   cluster.election.back_off_time 100ms ✗ 选举退避时间   cluster.election.max_timeout 10s ✗ 最大选举超时   cluster.election.duration 30s ✗ 选举持续时间    主节点检查配置 #     参数 默认值 动态 说明     cluster.leader_check.interval 1s ✗ 主节点检查间隔   cluster.leader_check.timeout 10s ✗ 主节点检查超时   cluster.leader_check.retry_count 3 ✗ 主节点检查重试次数   cluster.follower_check.interval 1s ✗ 追随节点检查间隔   cluster.follower_check.timeout 10s ✗ 追随节点检查超时   cluster.follower_check.retry_count 3 ✗ 追随节点检查重试次数    集群形成配置 #     参数 默认值 动态 说明     cluster.initial_master_nodes - ✗ 初始主节点列表（引导集群时必需）   cluster.bootstrap_unconfigured_timeout 15s ✗ 集群初始化超时   cluster.publish_info.timeout 30s ✗ 发布信息超时   cluster.publish.timeout 30s ✗ 发布配置超时   cluster.auto_shrink_voting_configuration true ✓ 自动收缩投票配置    故障检测配置 #     参数 默认值 动态 说明     cluster.follower_lag_timeout 60s ✗ 节点滞后超时   cluster.formation_warning_timeout 10s ✗ 集群形成警告超时   discovery.cluster_formation_warning_timeout 10s ✗ 同上（别名）     路由和分片分配 #  磁盘阈值配置 #     参数 默认值 动态 说明     cluster.routing.allocation.disk.threshold_enabled true ✓ 启用磁盘阈值检查   cluster.routing.allocation.disk.watermark.low 85% ✓ 低水位线   cluster.routing.allocation.disk.watermark.high 90% ✓ 高水位线   cluster.routing.allocation.disk.watermark.flood_stage 95% ✓ 泛滥水位线   cluster.routing.allocation.disk.include_relocations true ✓ 磁盘检查是否包含重定位   cluster.routing.allocation.disk.reroute_interval 60s ✓ 磁盘检查重新路由间隔   cluster.routing.allocation.disk.enable_for_single_data_node false ✗ 单节点模式下启用磁盘检查     磁盘阈值也支持绝对字节值：1TB, 500GB 等\n 分片分配配置 #     参数 默认值 动态 说明     cluster.routing.allocation.enable all ✓ 分片分配允许的操作   cluster.routing.allocation.same_shard.host false ✓ 禁止同一主分片和副本在同一主机   cluster.routing.allocation.cluster_concurrent_recoveries -1 ✓ 并发恢复数（-1=无限制）   cluster.routing.allocation.max_retries 5 ✗ 分片分配最大重试次数   cluster.routing.allocation.awareness.attributes - ✓ 机架感知属性列表   cluster.routing.rebalance.enable all ✓ 集群重新平衡允许的操作   index.routing.allocation.enable all ✓ 索引分片分配（仅限该索引）   index.routing.allocation.initial_recovery.* - ✗ 初始恢复时的分配限制   index.total_shards_per_node -1 ✓ 单节点最大分片数（-1=无限制）   cluster.total_shards_per_node -1 ✓ 集群级别：单节点最大分片数   cluster.routing.allocation.same_shard.host false ✓ 相同分片的副本不能在同一主机    重新平衡配置 #     参数 默认值 动态 说明     cluster.routing.rebalance.enable all ✓ 允许重新平衡   cluster.routing.allocation.allow_rebalance indices_primaries_active ✓ 何时允许重新平衡     恢复配置 #  恢复失败分片数据的相关配置。\n   参数 默认值 动态 说明     indices.recovery.max_bytes_per_sec 40mb ✓ 恢复带宽限制   indices.recovery.max_concurrent_operations 1 ✓ 并发操作数   indices.recovery.max_concurrent_file_chunks 2 ✓ 并发文件块数   indices.recovery.retry_delay_state_sync 5s ✓ 状态同步重试延迟   indices.recovery.retry_delay_network 5s ✓ 网络重试延迟   indices.recovery.internal_action_timeout 15m ✓ 内部恢复操作超时   indices.recovery.internal_action_long_timeout 30m ✓ 长期恢复操作超时   indices.recovery.activity_timeout 15m ✓ 恢复活动超时   indices.recovery.max_concurrent_recoveries 2 ✓ 并发恢复数（已弃用，使用allocation配置）   index.delayed_node_left_timeout 1m ✓ 节点离线后延迟重新分配时间     索引存储配置 #  存储类型 #     参数 默认值 动态 说明     index.store.type fs ✗ 存储类型（fs=文件系统）   index.store.fs.fs_lock native ✗ 文件系统锁类型   node.store.allow_mmap true ✗ 允许使用 mmap   index.store.hybrid.chunk_size 128MB ✗ 混合存储块大小   index.optimize_auto_generated_id true ✗ 优化自动生成的文档 ID    事务日志配置 #     参数 默认值 动态 说明     index.translog.durability request ✓ 事务日志持久化策略   index.translog.sync_interval 5s ✓ 事务日志同步间隔   index.translog.flush_threshold_size 512mb ✓ 事务日志刷新阈值   index.translog.retention.size -1 ✓ 事务日志保留大小   index.translog.retention.age -1 ✓ 事务日志保留时间    Soft Delete（软删除） #     参数 默认值 动态 说明     index.soft_deletes.enabled true ✗ 启用软删除   index.soft_deletes.retention_operations -1 ✓ 软删除保留操作数   index.soft_deletes.retention.lease.period 12h ✓ 软删除保留租期     缓存配置 #  Query Cache（查询缓存） #     参数 默认值 动态 说明     indices.queries.cache.size 10% ✓ 查询缓存大小   indices.queries.cache.count 10000 ✓ 缓存条目数限制   indices.queries.cache.all_segments false ✓ 是否缓存所有 segment 过滤器   index.queries.cache.enabled true ✓ 该索引是否启用查询缓存    Request Cache（请求缓存） #     参数 默认值 动态 说明     indices.requests.cache.size 1% ✓ 请求缓存大小   indices.requests.cache.expire -1 ✓ 缓存过期时间   index.requests.cache.enable true ✓ 该索引是否启用请求缓存     断路器配置 #  断路器防止节点 OOM（内存溢出）。\n   参数 默认值 动态 说明     indices.breaker.total.limit 95% ✓ 总断路器限制   indices.breaker.fielddata.limit 40% ✓ Field Data 断路器   indices.breaker.fielddata.overhead 1.03 ✓ Field Data 开销系数   indices.breaker.request.limit 60% ✓ 请求断路器   indices.breaker.request.overhead 1.0 ✓ 请求开销系数   indices.breaker.accounting.limit 100% ✓ 会计断路器   indices.breaker.accounting.overhead 1.0 ✓ 会计开销系数   network.breaker.inflight_requests.limit 100% ✓ 传输请求断路器   network.breaker.inflight_requests.overhead 2.0 ✓ 传输请求开销系数   indices.breaker.type hierarchy ✗ 断路器类型   indices.breaker.total.use_real_memory true ✓ 使用真实内存计算     搜索相关配置 #     参数 默认值 动态 说明     index.query.default_field - ✓ 默认查询字段   indices.query.bool.max_clause_count 1024 ✓ Bool 查询最大子句数   search.max_open_pit_context 300 ✓ 最大 PIT 上下文数   search.max_keep_alive 24h ✓ 搜索/滚动最大保活时间   point_in_time.max_keep_alive 24h ✓ PIT 最大保活时间   index.search.slowlog.threshold.query.warn -1 ✓ 搜索慢日志警告阈值   index.search.slowlog.threshold.query.info -1 ✓ 搜索慢日志信息阈值   index.search.slowlog.threshold.fetch.warn -1 ✓ Fetch 慢日志警告阈值   index.search.slowlog.threshold.fetch.info -1 ✓ Fetch 慢日志信息阈值   index.search.throttled false ✓ 限制该索引的搜索速度     索引配置 #  基础设置 #     参数 默认值 动态 说明     index.number_of_shards 1 ✗ 主分片数   index.number_of_replicas 1 ✓ 副本数   index.codec best_compression ✗ 压缩编码器   index.force_memory_term_dictionary false ✗ 强制 term dictionary 使用内存    索引排序 #     参数 默认值 动态 说明     index.sort.field - ✗ 索引排序字段   index.sort.order asc ✗ 索引排序顺序   index.sort.missing _last ✗ 缺失值处理    刷新和合并 #     参数 默认值 动态 说明     index.refresh_interval 1s ✓ 刷新间隔   index.max_result_window 10000 ✓ 最大结果窗口（from+size）   index.max_inner_result_window 100000 ✓ 最大内部结果窗口   index.max_rescore_window 50000 ✓ 最大重新评分窗口   index.max_docvalue_fields_search 200 ✓ 最大 docvalue 字段数   index.max_script_fields 32 ✓ 最大脚本字段数   index.default_pipeline - ✓ 默认数据管道    合并策略 #     参数 默认值 动态 说明     index.merge.policy.type tiered ✗ 合并策略   index.merge.policy.segments_per_tier 10 ✓ 分层合并层级大小   index.merge.policy.max_merge_at_once 30 ✓ 最大合并数   index.merge.policy.max_merge_at_once_explicit 30 ✓ 明确请求时的最大合并数   index.merge.policy.floor_segment 2mb ✓ 最小合并段大小   index.merge.policy.max_merged_segment 5gb ✓ 最大合并段大小   index.merge.policy.deletes_pct_allowed_in_commits 10 ✓ 允许的删除百分比   index.merge.scheduler.max_thread_count - ✗ 合并线程数   index.merge.scheduler.auto_throttle true ✓ 自动限流     映射和分析配置 #  映射相关 #     参数 默认值 动态 说明     index.mapping.total_fields.limit 2000 ✓ 最大字段数   index.mapping.depth.limit 20 ✓ 最大嵌套深度   index.mapping.nested_fields.limit 50 ✓ 最大嵌套字段数   index.mapping.nested_objects.limit 10000 ✓ 最大嵌套对象数   index.verified_before_close false ✓ 关闭前验证索引    分析器 #     参数 默认值 动态 说明     index.analysis.hunspell.dictionary.lazy false ✗ Hunspell 字典延迟加载   index.analysis.hunspell.dictionary.ignore_case false ✗ Hunspell 忽略大小写     数据管道（Ingest）配置 #     参数 默认值 动态 说明     ingest.geoip.cache_size 1000 ✗ GeoIP 缓存大小     传输层配置 #  TCP 连接 #     参数 默认值 动态 说明     transport.tcp.connect_timeout 30s ✗ TCP 连接超时   transport.profiles.default.connect_timeout 30s ✗ 默认传输配置文件超时   transport.ping_schedule -1 ✗ 心跳 ping 间隔    远程集群配置 #     参数 默认值 动态 说明     cluster.remote.connect true ✗ 允许连接远程集群（已弃用）   cluster.remote.node.attr - ✗ 远程集群节点属性   search.remote.fetch_size 100 ✓ 远程搜索的 fetch 大小     HTTP 层配置 #     参数 默认值 动态 说明     http.host - ✗ HTTP 绑定地址   http.port 9200 ✗ HTTP 端口   http.max_content_length 100mb ✗ 最大请求体大小   http.max_header_size 16kb ✗ 最大 HTTP 头大小   http.max_initial_line_length 4kb ✗ 最大初始行长度   http.compression true ✗ 启用 gzip 压缩   http.compression_level 6 ✗ 压缩级别   http.publish_host - ✗ HTTP 发布地址   http.bind_host - ✗ HTTP 绑定地址   http.publish_port - ✗ HTTP 发布端口   http.reset_cookies false ✗ 重置 Cookie     监控配置 #  JVM 监控 #     参数 默认值 动态 说明     monitor.jvm.gc.enabled true ✗ 启用 JVM GC 监控    文件系统监控 #     参数 默认值 动态 说明     monitor.fs.health.enabled true ✓ 启用文件系统健康检查   monitor.fs.always_refresh false ✗ 总是刷新文件系统信息     网关（Gateway）配置 #  网关负责节点启动时恢复分片数据。\n   参数 默认值 动态 说明     gateway.recover_after_nodes - ✗ 启动后等待节点数达到此值后才开始恢复   gateway.recover_after_time 5m ✗ 启动后等待此时间后开始恢复   gateway.expected_nodes -1 ✗ 预期节点数（已弃用）   gateway.slow_write_logging_threshold 10s ✗ 缓慢写入日志阈值     搜索和线程池 #  线程池配置（按名称） #     线程池 线程类型 队列类型 说明     search scaling resizable 搜索线程池   index fixed resizable 索引线程池   bulk fixed resizable 批量操作   get fixed resizable 单文档获取   analyze fixed resizable 分析器   write fixed resizable 写入操作   snapshot scaling resizable 快照操作   force_merge fixed fixed 强制合并   fetch_shard_store fixed fixed 获取分片存储   management scaling resizable 管理操作    线程池参数 #     参数格式 说明     thread_pool.{name}.type 线程池类型（fixed/scaling）   thread_pool.{name}.size 线程池大小   thread_pool.{name}.queue_size 队列大小   thread_pool.{name}.keep_alive 线程保活时间     安全配置 #  参考配置文件说明中的安全配置部分\n 系统级别配置 #  进程和性能 #     参数 默认值 说明     node.max_local_storage_nodes 1 单机最大节点数   node.pidfile - PID 文件路径   node.store.allow_mmap true 允许 mmap    引导参数 #     参数 默认值 说明     bootstrap.memory_lock false 锁定 JVM 内存   bootstrap.system_call_filter.enabled true 启用系统调用过滤   bootstrap.system_call_filter.all false 过滤所有系统调用     常见优化建议 #  高吞吐量集群 #  # 禁用请求缓存（如果大量不同查询） indices.requests.cache.size: 0% # 增大查询缓存 indices.queries.cache.size: 15%\n# 调整刷新间隔 index.refresh_interval: 30s\n# 增加合并线程 index.merge.scheduler.max_thread_count: 4 内存受限环境 #\n # 降低缓存大小 indices.queries.cache.size: 5% indices.requests.cache.size: 0.5% # 启用断路器保护 indices.breaker.total.limit: 70%\n# 减少刷新间隔（更多内存使用） index.refresh_interval: 5s\n# 降低合并并发 index.merge.policy.max_merge_at_once: 10 搜索优化 #\n # 启用请求缓存 index.requests.cache.enable: true # 调整搜索超时 search.default_search_timeout: 30s\n# 禁用 PIT 自动清理（如需要） search.max_open_pit_context: 1000 \n配置更新方法 #  从文件配置（需重启） #  编辑 config/easysearch.yml 或 config/easysearch.properties，修改后重启节点。\n动态更新（推荐） #  使用集群设置 API 动态更新标记为\u0026quot;✓\u0026quot;的参数：\nPUT /_cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;indices.queries.cache.size\u0026#34;: \u0026#34;15%\u0026#34;, \u0026#34;index.refresh_interval\u0026#34;: \u0026#34;30s\u0026#34; } } 持久化更新（集群重启后仍生效）：\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;indices.queries.cache.size\u0026#34;: \u0026#34;15%\u0026#34; } }  延伸阅读 #    基础配置  集群配置  JVM 配置  内存管理  日志配置  ","subcategory":null,"summary":"","tags":null,"title":"高级配置参数","url":"/easysearch/main/docs/deployment/advanced-config/advanced-settings/"},{"category":null,"content":"Easysearch 的 IPv6 配置 #  配置 #  Easysearch 支持运行在 IPv6 模式，以下是具体操作：\n假设本机 IPv6 地址为：fe80::18df:9883:1e27:b040%en0\n修改 config/easysearch.yml 将 ip 相关的参数修改为 IPv6 对应的格式:\nnetwork.host: [\u0026#34;::1\u0026#34;, \u0026#34;fe80::18df:9883:1e27:b040%en0\u0026#34;] http.port: 9200 transport.port: 9300 discovery.seed_hosts: [\u0026#34;[::1]:9300\u0026#34;, \u0026#34;[fe80::18df:9883:1e27:b040%en0]:9300\u0026#34;] cluster.initial_master_nodes: [\u0026#34;[fe80::18df:9883:1e27:b040%en0]:9300\u0026#34;] 这个配置表示节点使用 IPv6 地址加入集群的配置，监听本地回环地址和一个特定的链路本地地址。\n验证 #  启动 Easysearch，即可使用 curl 工具进行测试：\n% curl -6 -v -ku \u0026#34;admin:=juNrz?BY4SeSlL%Tm29OfP5\u0026#34; \u0026#34;https://[fe80::18df:9883:1e27:b040%en0]:9200\u0026#34; * Trying fe80::18df:9883:1e27:b040:9200... * Connected to fe80::18df:9883:1e27:b040 (fe80::18df:9883:1e27:b040) port 9200 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/cert.pem * CApath: none * (304) (OUT), TLS handshake, Client hello (1): * (304) (IN), TLS handshake, Server hello (2): * (304) (IN), TLS handshake, Unknown (8): * (304) (IN), TLS handshake, Request CERT (13): * (304) (IN), TLS handshake, Certificate (11): * (304) (IN), TLS handshake, CERT verify (15): * (304) (IN), TLS handshake, Finished (20): * (304) (OUT), TLS handshake, Certificate (11): * (304) (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / AEAD-AES256-GCM-SHA384 * ALPN, server did not agree to a protocol * Server certificate: * subject: C=IN; ST=FI; L=NI; O=ORG; OU=UNIT; CN=infini.cloud * start date: Sep 14 01:55:46 2025 GMT * expire date: Sep 12 01:55:46 2035 GMT * issuer: C=IN; ST=FI; L=NI; O=ORG; OU=UNIT; CN=infini.cloud * SSL certificate verify result: self signed certificate (18), continuing anyway. * Server auth using Basic with user \u0026#39;admin\u0026#39; \u0026gt; GET / HTTP/1.1 \u0026gt; Host: [fe80::18df:9883:1e27:b040]:9200 \u0026gt; Authorization: Basic YWRtaW46PWp1TnJ6P0JZNFNlU2xMJVRtMjlPZlA1 \u0026gt; User-Agent: curl/7.79.1 \u0026gt; Accept: */* \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: application/json; charset=UTF-8 \u0026lt; content-length: 563 \u0026lt; { \u0026#34;name\u0026#34; : \u0026#34;zhangleideMacBook-Pro.local\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;easysearch\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;8KCQu9tjQcin1grPYhIbXw\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;distribution\u0026#34; : \u0026#34;easysearch\u0026#34;, \u0026#34;number\u0026#34; : \u0026#34;1.15.2\u0026#34;, \u0026#34;distributor\u0026#34; : \u0026#34;INFINI Labs\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;41dc40773ee4e12fe81d414f0e07cb3d3a64b7a5\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2025-09-18T01:13:46.174243Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;8.11.4\u0026#34;, \u0026#34;minimum_wire_lucene_version\u0026#34; : \u0026#34;7.7.0\u0026#34;, \u0026#34;minimum_lucene_index_compatibility_version\u0026#34; : \u0026#34;7.7.0\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, For Easy Search!\u0026#34; } * Connection #0 to host fe80::18df:9883:1e27:b040 left intact -6 参数表示使用 IPv6，输出表示连接完全成功。\n","subcategory":null,"summary":"","tags":null,"title":"IPv6 支持","url":"/easysearch/main/docs/deployment/install-guide/ipv6/"},{"category":null,"content":"腾讯云部署指南 #  本文介绍在腾讯云 CVM 上部署 Easysearch 集群的推荐配置与实践。\n推荐实例规格 #     节点角色 实例族 规格示例 说明     Master（专用） 标准型 S6 S6.LARGE16 (4C16G) 轻量计算   Data 高 IO 型 IT5 IT5.4XLARGE64 (16C64G) 本地 NVMe SSD   Data（云盘方案） 内存型 M6 M6.4XLARGE64 (16C64G) 搭配增强型 SSD   Coordinating 计算型 C6 C6.2XLARGE16 (8C16G) 按查询并发酌情添加    存储选择 #     存储类型 IOPS 适用场景     本地 NVMe SSD (IT5) 极高 高性能搜索   增强型 SSD 50,000 通用生产环境   SSD 云硬盘 26,000 标准场景   高性能云硬盘 6,000 测试环境    网络配置 #  VPC 与安全组 #  VPC CIDR: 10.0.0.0/16 子网： - 10.0.1.0/24 (可用区一) - 10.0.2.0/24 (可用区二) - 10.0.3.0/24 (可用区三) 安全组规则：\n   方向 端口 来源 说明     入站 9200 应用服务器安全组 REST API   入站 9300 Easysearch 安全组 节点间通信   入站 22 堡垒机安全组 SSH 管理    跨可用区部署 #  # node-1 (可用区一) node.attr.zone: ap-guangzhou-3 # node-2 (可用区二) node.attr.zone: ap-guangzhou-4\n# node-3 (可用区三) node.attr.zone: ap-guangzhou-6 cluster.routing.allocation.awareness.attributes: zone 安装步骤 #\n # 1. 系统调优 sysctl -w vm.max_map_count=262144 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf ulimit -n 65535 swapoff -a # 2. 格式化并挂载数据盘 mkfs.ext4 /dev/vdb mkdir -p /data mount -o noatime /dev/vdb /data echo \u0026quot;/dev/vdb /data ext4 noatime 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab\n# 3. 创建用户 groupadd -g 602 easysearch useradd -u 602 -g easysearch -m easysearch\n# 4. 安装 curl -sSL http://get.infini.cloud | bash -s \u0026ndash; -p easysearch\n# 5. 配置 JDK ln -s /usr/lib/jvm/java-17 /data/easysearch/jdk\n# 6. 修改 easysearch.yml\n# 7. 初始化并启动 cd /data/easysearch bin/initialize.sh -s chown -R easysearch:easysearch /data/easysearch su easysearch -c \u0026quot;/data/easysearch/bin/easysearch -d\u0026quot; 备份到 COS #\n 腾讯云环境推荐使用对象存储 COS 作为快照仓库。可通过 cosfs 将 bucket 挂载到本地路径：\n# 安装 cosfs yum install -y cosfs # 配置密钥 echo \u0026quot;your-bucket-name:your-secret-id:your-secret-key\u0026quot; \u0026gt; /etc/passwd-cosfs chmod 600 /etc/passwd-cosfs\n# 挂载 cosfs your-bucket-name /mnt/cos-backup -ourl=http://cos.ap-guangzhou.myqcloud.com PUT _snapshot/cos_backup { \u0026quot;type\u0026quot;: \u0026quot;fs\u0026quot;, \u0026quot;settings\u0026quot;: { \u0026quot;location\u0026quot;: \u0026quot;/mnt/cos-backup/easysearch\u0026quot; } } 延伸阅读 #\n   生产环境部署  分布式集群部署  系统调优  ","subcategory":null,"summary":"","tags":null,"title":"腾讯云部署","url":"/easysearch/main/docs/deployment/install-guide/tencent-cloud/"},{"category":null,"content":"移动云 BC-Linux 平台安装 #  移动云 BC-Linux 平台介绍 #  移动云 BC-Linux 是中国移动自主研发的企业级操作系统，基于 Linux 内核，专为云计算和大数据应用设计，具备高性能、高可靠性和强安全性，广泛应用于中国移动的云计算平台和数据中心。\n移动云 BC-Linux 平台安装参考 #  目前，Easysearch 已支持在移动云 BC-Linux 操作系统上运行，联网环境建议使用一键安装脚本进行安装，离线环境建议下载 Bundle 包进行安装，分布式集群安装请参考 分布式集群安装。\n 前提条件：已参照 系统调优进行了系统优化，同时为 Easysearch 创建了专用的用户。\n 初始化系统参数及用户命令参考 #  # 调整内核配置 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p # 增加用户组与用户 groupadd -r easysearch \u0026amp;\u0026amp; useradd -r -g easysearch -d /home/easysearch -s /sbin/nologin -c \u0026#34;Easysearch Service Account\u0026#34; easysearch 安装命令参考 #  # 创建数据目录 mkdir -p /opt/easysearch # 下载最新版本的 Easysearch 并安装 curl -sSL http://get.infini.cloud | bash -s -- -p easysearch -d /opt/easysearch # 进入 Easysearch 目录 cd /opt/easysearch # 初始化 Easysearch bin/initialize.sh -s # 调整目录权限 chown -R easysearch:easysearch /opt/easysearch # 启动 Easysearch runuser -u easysearch -- /opt/easysearch/bin/easysearch -d -p /opt/easysearch/easysearch.pid  注意：初始化过程中会生成随机密码，并不会保存到日志文件中，只会在终端显示一次，请妥善保存。如果忘记 admin 密码，可以使用 bin/reset_admin_password.sh 进行重置。\n 已验证系统环境：\n BCLinux-21.10U4  如果您在其他移动云 BC-Linux 软件平台的国产操作系统上安装遇到问题，欢迎通过 提交工单与我们联系。\n","subcategory":null,"summary":"","tags":null,"title":"移动云 BC-Linux 平台安装","url":"/easysearch/main/docs/deployment/install-guide/ciip/bclinux/"},{"category":null,"content":"AWS 部署指南 #  本文介绍在 AWS EC2 上部署 Easysearch 集群的推荐配置与实践。\n推荐实例类型 #     节点角色 实例族 规格示例 说明     Master（专用） m6i / m7i m6i.xlarge (4C16G) 轻量计算   Data i3 / i3en i3.2xlarge (8C61G) 本地 NVMe SSD   Data（EBS 方案） r6i / r7i r6i.4xlarge (16C128G) 搭配 gp3/io2   Coordinating c6i / c7i c6i.2xlarge (8C16G) 按查询并发酌情添加     生产环境至少 3 节点，跨 AZ 部署。\n 存储选择 #     存储类型 IOPS 适用场景     实例存储 NVMe (i3) 极高 高写入吞吐（注意：实例停止后数据丢失）   gp3 16,000 (可配置) 通用生产，性价比高   io2 64,000 低延迟搜索   st1 500 冷数据 / 日志归档    gp3 配置建议：\n卷大小: 按需 IOPS: 6000+（搜索场景建议 10000+） 吞吐量: 250+ MB/s 网络配置 #  VPC 与安全组 #  VPC CIDR: 10.0.0.0/16 子网： - 10.0.1.0/24 (AZ-a) - 10.0.2.0/24 (AZ-b) - 10.0.3.0/24 (AZ-c) 安全组 Inbound Rules：\n   Port Source Description     9200 App Security Group REST API   9300 ES Security Group Node Transport   22 Bastion SG SSH    跨 AZ 部署 #  # node-1 (AZ-a) node.attr.zone: us-east-1a # node-2 (AZ-b) node.attr.zone: us-east-1b\n# node-3 (AZ-c) node.attr.zone: us-east-1c cluster.routing.allocation.awareness.attributes: zone \n注意：跨 AZ 流量会产生额外费用，评估数据量与副本策略。\n 安装步骤 #  # 1. 系统调优 sudo sysctl -w vm.max_map_count=262144 echo \u0026#34;vm.max_map_count=262144\u0026#34; | sudo tee -a /etc/sysctl.conf sudo swapoff -a ulimit -n 65535 # 2. 格式化并挂载数据盘（EBS gp3 示例） sudo mkfs.xfs /dev/nvme1n1 sudo mkdir -p /data sudo mount -o noatime /dev/nvme1n1 /data echo \u0026quot;/dev/nvme1n1 /data xfs noatime 0 0\u0026quot; | sudo tee -a /etc/fstab\n# 3. 创建用户 sudo groupadd -g 602 easysearch sudo useradd -u 602 -g easysearch -m easysearch\n# 4. 安装 Easysearch curl -sSL http://get.infini.cloud | bash -s \u0026ndash; -p easysearch\n# 5. 配置 JVM sudo ln -s /usr/lib/jvm/java-17 /data/easysearch/jdk\n# 6. 修改 easysearch.yml（见上方节点配置）\n# 7. 初始化并启动 cd /data/easysearch sudo bin/initialize.sh -s sudo chown -R easysearch:easysearch /data/easysearch sudo -u easysearch bin/easysearch -d 备份到 S3 #\n PUT _snapshot/s3_backup { \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;/mnt/s3-backup/easysearch\u0026#34; } }  可通过 s3fs-fuse 挂载 S3 bucket，或使用 Easysearch 的 S3 快照仓库插件。详见 S3 备份。\n 延伸阅读 #    生产环境部署  分布式集群部署  系统调优  ","subcategory":null,"summary":"","tags":null,"title":"AWS 部署","url":"/easysearch/main/docs/deployment/install-guide/aws/"},{"category":null,"content":"阿里云部署指南 #  本文介绍在阿里云 ECS 上部署 Easysearch 集群的推荐配置与实践。\n推荐实例规格 #     节点角色 实例族 规格示例 说明     Master（专用） 通用型 g7 ecs.g7.xlarge (4C16G) 轻量计算，稳定即可   Data 存储增强型 i3 / 本地 SSD 型 ecs.i3.2xlarge (8C64G) 本地 NVMe SSD，高 IOPS   Data（云盘方案） 通用型 g7 ecs.g7.4xlarge (16C64G) 搭配 ESSD PL1/PL2   Coordinating 计算型 c7 ecs.c7.2xlarge (8C16G) 按查询并发酌情添加     生产环境至少 3 节点，跨可用区部署以实现高可用。\n 存储选择 #     存储类型 IOPS 适用场景     本地 NVMe SSD (i 系列) 极高 高写入吞吐、大数据量   ESSD PL1 50,000 通用生产环境   ESSD PL2 100,000 高性能搜索场景   高效云盘 5,000 测试环境 / 冷数据    建议：\n 数据盘单独挂载到 /data，不要与系统盘混用 使用 ext4 或 xfs 文件系统 关闭 atime：mount -o noatime /dev/vdb /data  网络配置 #  VPC 与安全组 #  VPC CIDR: 10.0.0.0/16 子网： - 10.0.1.0/24（可用区 A） - 10.0.2.0/24（可用区 B） - 10.0.3.0/24（可用区 C） 安全组规则（最小化）：\n   方向 端口 来源 说明     入方向 9200 应用服务器安全组 REST API   入方向 9300 Easysearch 安全组 节点间通信   入方向 22 运维跳板机 SSH 管理     不要将 9200 / 9300 端口暴露到公网。\n 跨可用区部署 #  将 3 个节点分别部署在不同可用区，配置 node.attr.zone 进行感知分配：\n# node-1 (可用区 A) node.attr.zone: cn-hangzhou-a # node-2 (可用区 B) node.attr.zone: cn-hangzhou-b\n# node-3 (可用区 C) node.attr.zone: cn-hangzhou-c 配合分片分配感知：\ncluster.routing.allocation.awareness.attributes: zone 安装步骤 #  # 1. 系统调优（参考系统调优文档） sysctl -w vm.max_map_count=262144 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf ulimit -n 65535 swapoff -a # 2. 创建用户 groupadd -g 602 easysearch useradd -u 602 -g easysearch -m -d /home/easysearch easysearch\n# 3. 一键安装 curl -sSL http://get.infini.cloud | bash -s \u0026ndash; -p easysearch\n# 4. 挂载数据盘 mkfs.ext4 /dev/vdb mount -o noatime /dev/vdb /data echo \u0026quot;/dev/vdb /data ext4 noatime 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab\n# 5. 配置 JDK ln -s /usr/lib/jvm/java-17 /data/easysearch/jdk\n# 6. 修改配置文件（见上方节点配置示例）\n# 7. 初始化并启动 cd /data/easysearch bin/initialize.sh -s chown -R easysearch:easysearch /data/easysearch su easysearch -c \u0026quot;/data/easysearch/bin/easysearch -d\u0026quot; 备份到 OSS #\n 阿里云环境推荐使用 OSS 作为快照仓库：\nPUT _snapshot/aliyun_oss { \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;/mnt/oss-backup/easysearch\u0026#34; } }  可通过 ossfs 将 OSS bucket 挂载到本地路径，或使用 S3 兼容接口。\n 延伸阅读 #    生产环境部署  分布式集群部署  系统调优  硬件配置  ","subcategory":null,"summary":"","tags":null,"title":"阿里云部署","url":"/easysearch/main/docs/deployment/install-guide/aliyun/"},{"category":null,"content":"统信 UOS 平台安装 #  统信 UOS 平台介绍 #  统信 UOS（Unity Operating System）是基于 Linux 内核的国产操作系统，面向党政、金融、能源等关键领域，提供安全、稳定、易用的桌面与服务器版本，深度适配主流信创CPU平台（如龙芯、鲲鹏、兆芯、海光、申威等），构建完善的国产软硬件生态体系。\n统信 UOS 平台安装参考 #  目前，Easysearch 已支持在统信 UOS 操作系统上运行，联网环境建议使用一键安装脚本进行安装，离线环境建议下载 Bundle 包进行安装，分布式集群安装请参考 分布式集群安装。\n 前提条件：已参照 系统调优进行了系统优化，同时为 Easysearch 创建了专用的用户。\n 初始化系统参数及用户命令参考 #  # 调整内核配置 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p # 增加用户组与用户 groupadd -r easysearch \u0026amp;\u0026amp; useradd -r -g easysearch -d /home/easysearch -s /sbin/nologin -c \u0026#34;Easysearch Service Account\u0026#34; easysearch 安装命令参考 #  # 创建数据目录 mkdir -p /opt/easysearch # 下载最新版本的 Easysearch 并安装 curl -sSL http://get.infini.cloud | bash -s -- -p easysearch -d /opt/easysearch # 进入 Easysearch 目录 cd /opt/easysearch # 初始化 Easysearch bin/initialize.sh -s # 调整目录权限 chown -R easysearch:easysearch /opt/easysearch # 启动 Easysearch runuser -u easysearch -- /opt/easysearch/bin/easysearch -d -p /opt/easysearch/easysearch.pid  注意：初始化过程中会生成随机密码，并不会保存到日志文件中，只会在终端显示一次，请妥善保存。如果忘记 admin 密码，可以使用 bin/reset_admin_password.sh 进行重置。\n 已验证系统环境：\n UOS 20 企业版 SP1/统信UOS服务器版20 SP1 兆芯 KH-40000/统信UOS服务器版20 SP1 Hygon C86 7185/UnionTech OS Server 20 1050d  如果您在其他统信 UOS 平台的国产操作系统上安装遇到问题，欢迎通过 提交工单与我们联系。\n","subcategory":null,"summary":"","tags":null,"title":"统信 UOS 平台安装","url":"/easysearch/main/docs/deployment/install-guide/ciip/uniontech/"},{"category":null,"content":"索引排序配置指南 #  索引排序（Index Sorting）允许在索引阶段对文档按指定字段排序存储。这可以显著加速按排序字段的查询和聚合操作，但会增加索引写入开销。\n 工作原理 #  默认情况下，Lucene 按文档写入顺序存储。启用索引排序后，每个段（Segment）内的文档将按排序字段有序存储：\n默认存储： doc1 → doc2 → doc3 → doc4 → doc5 索引排序后： doc3 → doc1 → doc5 → doc2 → doc4 (按 timestamp 降序) 优势：\n 按排序字段查询时可提前终止（Early Termination），大幅减少扫描量 提高按排序字段做聚合的效率 改善压缩率（相邻文档字段值更接近）  代价：\n 索引写入速度降低 ~40%–50%（段合并时需要排序） 索引大小可能增加 排序字段只能在索引创建时指定，无法修改   配置参数 #  index.sort.field #     项目 说明     参数 index.sort.field   默认值 无（不排序）   属性 静态（仅在索引创建时设置）   说明 排序字段列表。支持多字段排序。字段类型必须是 boolean、numeric、date 或 keyword    index.sort.order #     项目 说明     参数 index.sort.order   默认值 asc   可选值 asc、desc   属性 静态   说明 每个排序字段的排序方向。元素个数必须与 index.sort.field 一致    index.sort.mode #     项目 说明     参数 index.sort.mode   默认值 min（asc 时）/ max（desc 时）   可选值 min、max   属性 静态   说明 多值字段的取值模式。min 取最小值排序，max 取最大值排序    index.sort.missing #     项目 说明     参数 index.sort.missing   默认值 _last（asc 时）/ _first（desc 时）   可选值 _last、_first   属性 静态   说明 缺失值的排列位置。_first 排在最前，_last 排在最后     使用示例 #  按时间戳降序排序 #  最常见的场景——日志/时序数据按时间降序存储，加速\u0026quot;最新数据\u0026quot;查询：\nPUT /logs-2024 { \u0026#34;settings\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;sort.field\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;sort.order\u0026#34;: \u0026#34;desc\u0026#34;, \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;number_of_replicas\u0026#34;: 1 } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;level\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;message\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } 多字段排序 #  按租户 ID + 时间戳排序，适用于多租户场景：\nPUT /multi-tenant-index { \u0026#34;settings\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;sort.field\u0026#34;: [\u0026#34;tenant_id\u0026#34;, \u0026#34;timestamp\u0026#34;], \u0026#34;sort.order\u0026#34;: [\u0026#34;asc\u0026#34;, \u0026#34;desc\u0026#34;], \u0026#34;sort.missing\u0026#34;: [\u0026#34;_last\u0026#34;, \u0026#34;_first\u0026#34;] } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;tenant_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } } } }  提前终止（Early Termination） #  索引排序的核心收益是查询提前终止。当查询的 sort 顺序与索引排序一致时，Easysearch 可以在收集到足够文档后提前停止扫描：\n# 索引按 timestamp desc 排序 # 查询也按 timestamp desc 排序 + size=10 # → 每个段只需读取前 10 个文档 GET /logs-2024/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;sort\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: \u0026#34;desc\u0026#34; } ], \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 效果对比（示例）：\n   场景 无索引排序 有索引排序     Top 10 最新日志 扫描全部文档 每段仅读 10 条   按时间范围 + Top N 全段扫描 快速跳过无关数据   聚合（按排序字段分桶） 全量计算 可利用有序性优化     ⚠️ 提前终止仅在查询 sort 与索引 sort 完全匹配时生效。如果查询按其他字段排序，索引排序不会带来查询收益。\n  适用场景 #  ✅ 推荐使用 #     场景 排序配置 理由     日志/时序数据 timestamp desc 大多数查询是\u0026quot;最新 N 条\u0026quot;   多租户 tenant_id asc, timestamp desc 按租户隔离查询   电商订单 created_at desc 最新订单优先   传感器数据 device_id asc, timestamp desc 按设备查最新数据    ❌ 不推荐使用 #     场景 理由     高写入吞吐 索引排序增加 40–50% 写入开销   全文搜索为主 查询不按固定字段排序，无法提前终止   排序字段频繁变化 索引排序一旦设定无法修改   text 类型字段排序 不支持     性能影响 #  写入性能 #  索引排序的主要开销在段合并时的排序操作：\n   指标 无排序 有排序 影响     索引吞吐 基准 -40%~-50% 显著下降   段合并时间 基准 +50%~+100% 显著增加   索引大小 基准 +5%~-10% 取决于数据（有序数据可能压缩更好）    查询性能 #  匹配排序方向时收益显著：\n   查询类型 无排序 有排序 提升     Top N（排序匹配） 全量扫描 提前终止 10x–100x   范围查询 + 排序 全量扫描 减少扫描 2x–10x   不匹配的排序 基准 无收益 无   聚合（排序字段） 基准 有序优化 2x–5x     注意事项 #   索引排序不可修改：排序字段和方向在索引创建时确定，无法通过 _settings API 修改 Reindex 保留排序：对排序索引执行 reindex，目标索引需要重新配置排序 字段类型限制：仅支持 boolean、numeric、date、keyword 类型 与 _source 无关：索引排序影响 Lucene 段内的存储顺序，_source 中的字段顺序不变 副本同步：主分片和副本分片的段排序一致   延伸阅读 #    路径配置 - 数据存储路径配置  高级配置参数 - 完整参数参考  ","subcategory":null,"summary":"","tags":null,"title":"索引排序","url":"/easysearch/main/docs/deployment/advanced-config/index-sorting/"},{"category":null,"content":"麒麟软件平台安装 #  麒麟软件平台介绍 #  麒麟软件平台是由中国电子旗下麒麟软件有限公司研发的国产操作系统，基于 Linux 内核，提供桌面版和服务器版，广泛应用于党政、国防、金融、能源等关键领域。全面适配龙芯、鲲鹏、兆芯、海光、申威等主流信创 CPU，支持国密算法与可信计算，致力于构建安全、稳定、自主可控的国产基础软件生态。\n麒麟软件平台安装参考 #  目前，Easysearch 已支持在麒麟软件操作系统上运行，联网环境建议使用一键安装脚本进行安装，离线环境建议下载 Bundle 包进行安装，分布式集群安装请参考 分布式集群安装。\n 前提条件：已参照 系统调优进行了系统优化，同时为 Easysearch 创建了专用的用户。\n 初始化系统参数及用户命令参考 #  # 调整内核配置 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p # 增加用户组与用户 groupadd -r easysearch \u0026amp;\u0026amp; useradd -r -g easysearch -d /home/easysearch -s /sbin/nologin -c \u0026#34;Easysearch Service Account\u0026#34; easysearch 安装命令参考 #  # 创建数据目录 mkdir -p /opt/easysearch # 下载最新版本的 Easysearch 并安装 curl -sSL http://get.infini.cloud | bash -s -- -p easysearch -d /opt/easysearch # 进入 Easysearch 目录 cd /opt/easysearch # 初始化 Easysearch bin/initialize.sh -s # 调整目录权限 chown -R easysearch:easysearch /opt/easysearch # 启动 Easysearch runuser -u easysearch -- /opt/easysearch/bin/easysearch -d -p /opt/easysearch/easysearch.pid  注意：初始化过程中会生成随机密码，并不会保存到日志文件中，只会在终端显示一次，请妥善保存。如果忘记 admin 密码，可以使用 bin/reset_admin_password.sh 进行重置。\n 已验证系统环境：\n 飞腾腾云 S2500/银河麒麟高级服务器操作系统V10 SP3 华为 Kunpeng 920 Taishan 200/银河麒麟高级服务器操作系统V10 SP3  如果您在其他麒麟软件平台的国产操作系统上安装遇到问题，欢迎通过 提交工单与我们联系。\n","subcategory":null,"summary":"","tags":null,"title":"麒麟软件平台安装","url":"/easysearch/main/docs/deployment/install-guide/ciip/kylin/"},{"category":null,"content":"生产环境部署指南 #  本文提供 Easysearch 生产环境部署的完整建议，涵盖硬件选型、节点规划、高可用架构与上线前检查清单。\n硬件推荐 #     组件 CPU 内存 JVM 堆 磁盘 高可用实例数     Easysearch 16 核+ 64 GB+ 31 GB SSD ≥ 3   INFINI Console 8 核 16 GB — ≥ 50 GB 1   INFINI Gateway 8 核 16 GB — ≥ 50 GB ≥ 2     详见 硬件配置。\n 磁盘选型 #   生产环境：强烈建议使用本地 SSD（NVMe 优先）。随机 IOPS 和吞吐量直接影响索引写入与查询延迟。 避免网络存储：NFS / CIFS 不适用于 Easysearch 数据目录。云环境可使用高性能云盘（如阿里云 ESSD、AWS gp3/io2）。 容量估算：原始数据量 × (1 + 副本数) × 1.1（段合并预留），再预留 20% 空间用于日常运维操作。  内存分配 #   JVM 堆设置为物理内存的 50%，但不超过 31 GB（压缩指针阈值）。 剩余内存留给操作系统文件缓存，Lucene 段读取严重依赖页缓存。 示例：64 GB 机器 → -Xms31g -Xmx31g。  # config/jvm.options -Xms31g -Xmx31g 节点角色规划 #  Easysearch 支持以下节点角色，生产环境建议分角色部署：\n   角色 说明 建议数量     master 集群状态管理、索引元数据 3（专用）   data 索引和搜索数据 按数据量扩展   ingest 摄取管道预处理 按写入量可选   coordinating 查询路由与结果归并 高并发场景可选    小集群（3 节点起步） #  3 个节点同时承担 master + data 角色，适合数据量 \u0026lt; 1 TB 的场景：\n# easysearch.yml（每个节点） cluster.name: prod-cluster node.name: node-1 node.roles: [master, data] network.host: 0.0.0.0 discovery.seed_hosts: [\u0026#34;10.0.1.1\u0026#34;, \u0026#34;10.0.1.2\u0026#34;, \u0026#34;10.0.1.3\u0026#34;] cluster.initial_master_nodes: [\u0026#34;node-1\u0026#34;, \u0026#34;node-2\u0026#34;, \u0026#34;node-3\u0026#34;] 中大集群（6+ 节点） #  master 与 data 分离，可独立扩容数据层：\n3 × master（专用，4C16G 即可） N × data（16C64G + SSD，按需扩展） 2 × coordinating（可选，高并发查询场景） 系统调优 #  在启动 Easysearch 之前，请完成以下操作系统调优：\n# 1. 文件描述符 ulimit -n 65535 # 2. 虚拟内存 sysctl -w vm.max_map_count=262144 echo \u0026quot;vm.max_map_count=262144\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf\n# 3. 禁用 swap swapoff -a # 或在 easysearch.yml 中设置： # bootstrap.memory_lock: true \n完整参数见 系统调优。\n 安全配置 #  Easysearch 默认启用安全功能，首次部署需初始化：\n# 初始化安全模块（生成证书 + admin 密码） bin/initialize.sh -s # 密码将在终端输出中显示，请务必记住 # 也可预先设置环境变量：export EASYSEARCH_INITIAL_ADMIN_PASSWORD=\u0026#34;YourSecurePassword\u0026#34; 生产环境建议：\n 使用企业 CA 签发的证书替换自签名证书 启用节点间 TLS 传输加密 配置 LDAP / AD 对接（如有） 设置最小权限的角色与用户   详见 安全配置。\n 备份策略 #  生产环境必须配置快照备份：\n# 注册快照仓库 PUT _snapshot/my_backup { \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;/mnt/backup/easysearch\u0026#34; } } 创建快照 PUT _snapshot/my_backup/snapshot_1 建议：\n 每日自动快照，保留 7～30 天 备份存储与数据存储物理隔离 定期验证恢复流程   详见 备份与恢复。\n 滚动重启 #  当需要对集群进行维护（升级版本、更换硬件、修改配置等）时，应使用滚动重启方式逐节点操作，避免集群停机。\n操作步骤 #   暂停分片分配（避免节点下线后触发不必要的数据迁移）  PUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.enable\u0026#34;: \u0026#34;primaries\u0026#34; } } 执行 synced flush（减少恢复时间，可选）  POST /_flush/synced 停止并维护目标节点  # 停止节点 kill $(cat /data/easysearch/pid) # 执行维护操作（升级、配置修改等） # \u0026hellip;\n# 重新启动节点 su easysearch -c \u0026quot;/data/easysearch/bin/easysearch -d -p pid\u0026quot; 等待节点重新加入集群  # 确认节点已加入 curl -ku admin:YOUR_PASSWORD \u0026#34;https://localhost:9200/_cat/nodes?v\u0026#34; 恢复分片分配  PUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.enable\u0026#34;: null } } 等待集群恢复 green 状态  curl -ku admin:YOUR_PASSWORD \u0026#34;https://localhost:9200/_cluster/health?wait_for_status=green\u0026amp;timeout=120s\u0026#34; 重复步骤 1-6，直到所有节点维护完毕（每次只操作一个节点）。   注意：始终先处理数据节点，最后处理当前 master 节点。如果节点在短时间内重新加入，Easysearch 会自动使用本地数据恢复分片，避免完整的数据复制。\n 上线前检查清单 #     检查项 说明     集群状态 GET _cluster/health 返回 green   节点数量 GET _cat/nodes 确认所有节点已加入   分片分配 无 unassigned 分片   JVM 堆 不超过 31 GB，-Xms = -Xmx   文件描述符 GET _nodes/stats/process 确认 max_file_descriptors ≥ 65535   swap 已禁用或 memory_lock: true   快照 已配置并测试恢复   监控 INFINI Console 已连接，告警已配置   安全 TLS 启用，默认密码已更改    延伸阅读 #    分布式集群部署  硬件配置  系统调优  容量规划  扩缩容与分片  ","subcategory":null,"summary":"","tags":null,"title":"生产环境部署","url":"/easysearch/main/docs/deployment/install-guide/production-env/"},{"category":null,"content":"Ingest Pipeline 配置指南 #  Ingest Pipeline 允许在文档索引之前进行预处理（如字段提取、数据转换、富化等）。本文介绍 Ingest 节点的配置和 Pipeline 的高级使用技巧。\n 启用 Ingest 功能 #  默认情况下所有节点都具备 Ingest 功能。如需将 Ingest 角色分配给专用节点，请参考 集群节点配置 中的节点角色说明。\n Pipeline 管理 #  创建 Pipeline #  PUT /_ingest/pipeline/my-pipeline { \u0026#34;description\u0026#34;: \u0026#34;日志预处理流水线\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;grok\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;%{COMBINEDAPACHELOG}\u0026#34;] } }, { \u0026#34;date\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;formats\u0026#34;: [\u0026#34;dd/MMM/yyyy:HH:mm:ss Z\u0026#34;] } }, { \u0026#34;remove\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34; } } ] } 查看 Pipeline #  # 查看所有 GET /_ingest/pipeline # 查看指定 GET /_ingest/pipeline/my-pipeline\n# 查看 Pipeline 统计 GET /_nodes/stats/ingest 删除 Pipeline #\n DELETE /_ingest/pipeline/my-pipeline 模拟测试 #  在正式使用前模拟验证 Pipeline 处理效果：\nPOST /_ingest/pipeline/my-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;192.168.1.1 - - [28/Jun/2024:10:00:00 +0800] \\\u0026#34;GET /index.html HTTP/1.1\\\u0026#34; 200 1234\u0026#34; } } ] }  索引级别 Pipeline 配置 #  index.default_pipeline #     项目 说明     参数 index.default_pipeline   默认值 无   属性 动态   说明 索引的默认 Pipeline。写入请求未指定 Pipeline 时使用此配置。设为 _none 可禁用    PUT /my-index/_settings { \u0026#34;index.default_pipeline\u0026#34;: \u0026#34;my-pipeline\u0026#34; } index.final_pipeline #     项目 说明     参数 index.final_pipeline   默认值 无   属性 动态   说明 在默认/请求指定的 Pipeline 之后执行的 Pipeline。无法被请求覆盖，适用于强制审计、字段标准化等场景    PUT /my-index/_settings { \u0026#34;index.final_pipeline\u0026#34;: \u0026#34;audit-pipeline\u0026#34; } Pipeline 执行顺序 #  文档写入 → request pipeline → default_pipeline → final_pipeline → 索引 (显式指定) (未指定时使用) (始终执行)  如果请求中指定了 pipeline 参数，则使用请求中的 Pipeline 而非 default_pipeline。final_pipeline 始终执行。\n  常用 Processor 速查 #     Processor 用途 典型场景     grok 正则提取结构化字段 日志解析   date 日期解析 时间戳标准化   convert 类型转换 字符串→数字   rename 字段重命名 字段标准化   remove 删除字段 清理原始字段   set 设置/覆盖字段值 添加默认值   script 脚本处理 复杂计算   split 拆分字段 CSV→数组   json JSON 解析 嵌套 JSON 提取   lowercase / uppercase 大小写转换 字段标准化   trim 去除空白 数据清洗   gsub 正则替换 文本清洗   dissect 简单模式提取（比 grok 快） 格式固定的日志   foreach 遍历数组并处理 数组字段批量处理   pipeline 调用其他 Pipeline Pipeline 复用   drop 丢弃文档 条件过滤   fail 强制失败 数据验证     错误处理 #  on_failure 处理器 #  为 Processor 或整个 Pipeline 设置错误处理：\nPUT /_ingest/pipeline/safe-pipeline { \u0026#34;description\u0026#34;: \u0026#34;带错误处理的 Pipeline\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;grok\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;%{COMBINEDAPACHELOG}\u0026#34;], \u0026#34;on_failure\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;_tags\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;_grok_parse_failure\u0026#34;] } } ] } } ], \u0026#34;on_failure\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;error_info\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Pipeline 处理失败: {{_ingest.on_failure_message}}\u0026#34; } } ] } ignore_failure #  忽略单个 Processor 的错误：\n{ \u0026#34;convert\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;status_code\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;ignore_failure\u0026#34;: true } }  性能调优 #  专用 Ingest 节点 #  高吞吐场景建议部署专用 Ingest 节点，避免 Pipeline 处理影响搜索和索引性能。具体配置方式参考 集群节点配置。\nPipeline 性能优化建议 #     优化项 建议     Processor 数量 尽量精简，合并可合并的操作   grok vs dissect 格式固定时优先用 dissect（性能更好）   script 使用 避免复杂脚本，优先使用内置 Processor   on_failure 设置错误处理，避免写入阻塞   条件执行 使用 if 条件减少不必要的处理   批量大小 配合 _bulk API，控制每批文档数量    条件执行示例 #  仅在特定条件下执行 Processor，减少开销：\n{ \u0026#34;uppercase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;env\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;ctx.env != null \u0026amp;\u0026amp; ctx.env.length() \u0026gt; 0\u0026#34; } }  监控 #  # 查看 Ingest 节点统计 GET /_nodes/stats/ingest # 关注指标： # - ingest.total.count: 处理文档总数 # - ingest.total.failed: 失败数 # - ingest.total.time_in_millis: 总耗时 # - ingest.pipelines.\u0026lt;name\u0026gt;.processors: 各 Processor 耗时 \n延伸阅读 #    集群节点配置 - 节点角色设置  高级配置参数 - 完整参数参考  ","subcategory":null,"summary":"","tags":null,"title":"Ingest Pipeline 配置","url":"/easysearch/main/docs/deployment/advanced-config/ingest-pipeline-config/"},{"category":null,"content":"飞腾平台安装 #  飞腾平台介绍 #  飞腾平台基于 ARM 架构，由飞腾公司自主研发，提供高性能、低功耗的 CPU 产品，广泛应用于信创桌面、服务器及嵌入式领域，全面适配统信 UOS、麒麟等国产操作系统，支撑党政、金融、电信等行业国产化替代需求。\n飞腾平台安装参考 #  目前，Easysearch 已支持在飞腾芯片的国产操作系统上运行，联网环境建议使用一键安装脚本进行安装，离线环境建议下载 Bundle 包进行安装，分布式集群安装请参考 分布式集群安装。\n 前提条件：已参照 系统调优进行了系统优化，同时为 Easysearch 创建了专用的用户。\n 初始化系统参数及用户命令参考 #  # 调整内核配置 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p # 增加用户组与用户 groupadd -r easysearch \u0026amp;\u0026amp; useradd -r -g easysearch -d /home/easysearch -s /sbin/nologin -c \u0026#34;Easysearch Service Account\u0026#34; easysearch 安装命令参考 #  # 创建数据目录 mkdir -p /opt/easysearch # 下载最新版本的 Easysearch 并安装 curl -sSL http://get.infini.cloud | bash -s -- -p easysearch -d /opt/easysearch # 进入 Easysearch 目录 cd /opt/easysearch # 初始化 Easysearch bin/initialize.sh -s # 调整目录权限 chown -R easysearch:easysearch /opt/easysearch # 启动 Easysearch runuser -u easysearch -- /opt/easysearch/bin/easysearch -d -p /opt/easysearch/easysearch.pid  注意：初始化过程中会生成随机密码，并不会保存到日志文件中，只会在终端显示一次，请妥善保存。如果忘记 admin 密码，可以使用 bin/reset_admin_password.sh 进行重置。\n 已验证系统环境：\n 腾云 S2500/银河麒麟高级服务器操作系统V10 SP3  如果您在其他飞腾平台的国产操作系统上安装遇到问题，欢迎通过 提交工单与我们联系。\n","subcategory":null,"summary":"","tags":null,"title":"飞腾平台安装","url":"/easysearch/main/docs/deployment/install-guide/ciip/feiteng/"},{"category":null,"content":"集群协调调优 #  本文介绍 Easysearch 集群协调层的高级调优参数，包括选举、故障检测和集群状态发布。这些参数通常不需要修改，仅在大规模集群或网络条件不佳时才需要调整。\n 前提：请先阅读 集群发现 了解基础配置。\n  选举调优 #  选举机制控制集群在主节点失联后如何选出新的主节点。\ncluster.election.initial_timeout #     项目 说明     参数 cluster.election.initial_timeout   默认值 100ms   属性 静态   说明 节点首次发起选举前的随机等待上界。初始超时引入随机性，避免多个节点同时发起选举导致票数分裂    cluster.election.back_off_time #     项目 说明     参数 cluster.election.back_off_time   默认值 100ms   属性 静态   说明 每次选举失败后增加的退避时间上界。防止选举风暴    cluster.election.max_timeout #     项目 说明     参数 cluster.election.max_timeout   默认值 10s   属性 静态   说明 选举超时的最大值。选举超时从 initial_timeout 开始，每次失败按 back_off_time 递增，但不超过此值    cluster.election.duration #     项目 说明     参数 cluster.election.duration   默认值 500ms   属性 静态   说明 单次选举允许的最长时间。超过此时间仍未完成选举则重新开始     故障检测 #  故障检测机制用于识别已下线的节点，分为 leader 检测（非主节点检测主节点是否存活）和 follower 检测（主节点检测其他节点是否存活）。\ncluster.fault_detection.leader_check.interval #     项目 说明     参数 cluster.fault_detection.leader_check.interval   默认值 1s   属性 静态   说明 非主节点向主节点发送心跳的间隔    cluster.fault_detection.leader_check.timeout #     项目 说明     参数 cluster.fault_detection.leader_check.timeout   默认值 10s   属性 静态   说明 心跳请求超时时间。超时则视为一次检测失败    cluster.fault_detection.leader_check.retry_count #     项目 说明     参数 cluster.fault_detection.leader_check.retry_count   默认值 3   最小值 1   属性 静态   说明 连续检测失败多少次后认为主节点已下线，触发重新选举    cluster.fault_detection.follower_check.interval #     项目 说明     参数 cluster.fault_detection.follower_check.interval   默认值 1s   属性 静态   说明 主节点向其他节点发送心跳的间隔    cluster.fault_detection.follower_check.timeout #     项目 说明     参数 cluster.fault_detection.follower_check.timeout   默认值 10s   属性 静态   说明 主节点向其他节点发送心跳的超时时间    cluster.fault_detection.follower_check.retry_count #     项目 说明     参数 cluster.fault_detection.follower_check.retry_count   默认值 3   最小值 1   属性 静态   说明 主节点连续检测失败多少次后将该节点从集群中移除     集群状态发布 #  集群状态（Cluster State）包含索引映射、分片分配等元数据，主节点在每次变更时将新状态发布到所有节点。\ncluster.publish.timeout #     项目 说明     参数 cluster.publish.timeout   默认值 30s   属性 静态   说明 集群状态发布的完整超时时间（包含提交阶段）。超时则视为发布失败    cluster.publish.info_timeout #     项目 说明     参数 cluster.publish.info_timeout   默认值 10s   属性 静态   说明 发布信息阶段的超时时间。超时后会记录详细日志，但不会导致发布失败     其他协调参数 #  cluster.join.timeout #     项目 说明     参数 cluster.join.timeout   默认值 60s   属性 静态   说明 节点加入集群的超时时间。超时后节点将重试加入    cluster.follower_lag.timeout #     项目 说明     参数 cluster.follower_lag.timeout   默认值 90s   属性 静态   说明 跟随节点滞后超时。如果跟随节点在此时间内未确认集群状态更新，主节点可能将其视为滞后节点    cluster.no_master_block #     项目 说明     参数 cluster.no_master_block   默认值 write   可选值 all、write   属性 动态   说明 集群没有主节点时的行为。write = 拒绝写入但允许读取；all = 拒绝所有操作     调优建议 #  大规模集群（\u0026gt;50 节点） #  大集群中状态发布和故障检测的负担更重：\n# 增加发布超时 cluster.publish.timeout: 60s # 增加心跳容忍度，避免误判 cluster.fault_detection.leader_check.retry_count: 5 cluster.fault_detection.follower_check.retry_count: 5 网络不稳定环境 #\n 跨机房或网络延迟较高时：\n# 增加检测超时 cluster.fault_detection.leader_check.timeout: 30s cluster.fault_detection.follower_check.timeout: 30s # 增加选举超时 cluster.election.max_timeout: 30s\n# 增加加入超时 cluster.join.timeout: 120s 快速故障转移 #\n 对可用性要求极高的场景，缩短检测间隔（会增加网络开销）：\ncluster.fault_detection.leader_check.interval: 500ms cluster.fault_detection.leader_check.timeout: 5s cluster.fault_detection.leader_check.retry_count: 2 cluster.fault_detection.follower_check.interval: 500ms cluster.fault_detection.follower_check.timeout: 5s cluster.fault_detection.follower_check.retry_count: 2 \n⚠️ 注意：过于激进的故障检测设置可能导致 GC 停顿被误判为节点下线。请确保 JVM 配置合理。\n  延伸阅读 #    集群发现配置 - 基础发现参数  集群节点配置 - 节点角色配置  网络配置 - 传输层网络配置  ","subcategory":null,"summary":"","tags":null,"title":"集群协调调优","url":"/easysearch/main/docs/deployment/advanced-config/cluster-coordination/"},{"category":null,"content":"一键安装 Console 和 Easysearch #  目前，Easysearch 已提供内置的 Console UI 来管理当前集群，如果您有多个集群需要管理，建议使用 INFINI Console 来统一管理多个 Easysearch 集群。INFINI Console 也可以通过一键安装脚本进行安装，以下是安装步骤：\n# 启动默认配置 (Console + 1 个 Easysearch 节点) curl -fsSL http://get.infini.cloud/start-local | sh -s # 想要更丰富的体验？试试这个： # 启动 3 个 Easysearch 节点，设置密码，并开启 Agent 指标采集 curl -fsSL http://get.infini.cloud/start-local | sh -s \u0026ndash; up \u0026ndash;nodes 3 \u0026ndash;password \u0026quot;MyDevPass123.\u0026quot; \u0026ndash;metrics-agent \n具体参数说明请参考博客文章 一键启动：使用 start-local 脚本轻松管理 INFINI Console 与 Easysearch 本地环境。\n ","subcategory":null,"summary":"","tags":null,"title":"一键安装 Console 和 Easysearch","url":"/easysearch/main/docs/deployment/install-guide/console/"},{"category":null,"content":"评分影响方向（Positive Score Impact） #  指示排名特性对得分的影响方向。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;boost\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;rank_feature\u0026#34;, \u0026#34;positive_score_impact\u0026#34;: true } } } } 适用的字段类型 #   rank_feature（mapper-extras 模块）  默认值 #   true  说明 #   true: 特性值越高，文档得分越高（正相关） false: 特性值越高，文档得分越低（负相关）  此参数告知搜索引擎如何使用排名特性来影响文档评分。\n相关参数 #    索引参数（Index）  ","subcategory":null,"summary":"","tags":null,"title":"评分影响方向（Positive Score Impact）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/positive_score_impact/"},{"category":null,"content":"缩放因子（Scaling Factor） #  缩放比因子用于将浮点值转换为长整数以获得更高的精度。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;scaled_float\u0026#34;, \u0026#34;scaling_factor\u0026#34;: 100 } } } } 适用的字段类型 #   scaled_float（mapper-extras 模块）  说明 #  scaled_float 字段通过将浮点值乘以缩放比因子来将其存储为长整数。这可以节省磁盘空间并提高聚合性能。例如，使用 scaling_factor: 100，值 10.5 被存储为 1050。\n缩放比因子必须是正数。常见值包括：\n 10 - 一位小数 100 - 两位小数 1000 - 三位小数  示例 #  POST my-index/_doc/1 { \u0026#34;price\u0026#34;: 10.50 } 相关参数 #\n   索引参数（Index）  文档值参数（Doc Values）  ","subcategory":null,"summary":"","tags":null,"title":"缩放因子（Scaling Factor）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/scaling_factor/"},{"category":null,"content":"线程池与断路器配置 #  本页介绍 easysearch.yml 中与线程池、断路器和 API 兼容相关的配置项。\n ⚠️ 警告：Easysearch 默认的线程池配置已经过充分调优。除非有明确的性能瓶颈证据，不建议修改线程池参数。盲目增大线程数反而可能导致性能下降。\n  线程池 #  Easysearch 为不同类型的操作使用不同的线程池。\n线程池类型 #     线程池 用途 默认大小 默认队列     search 搜索请求 CPU × 3 / 2 + 1（向下取整） 1000   write 索引/删除/更新/bulk 写入 CPU 核数 10000   index 单文档索引 CPU 核数 200   get GET 请求 CPU 核数 1000   analyze 分析请求 1 16   management 集群管理 5 —   flush Flush 操作 CPU / 2（向下取整，至少 1） —   refresh 刷新操作 CPU / 2（向下取整，至少 1） —   snapshot 快照/恢复 CPU / 2（向下取整，至少 1） —   warmer 预热操作 CPU / 2（向下取整，至少 1） —   force_merge 强制合并 1 —    配置语法 #  # 线程池大小 thread_pool.\u0026lt;pool_name\u0026gt;.size: \u0026lt;number\u0026gt; # 队列大小（-1 表示无限制，不推荐） thread_pool.\u0026lt;pool_name\u0026gt;.queue_size: \u0026lt;number\u0026gt; 配置示例 #\n # 搜索线程池（仅在搜索负载极高时调整） thread_pool.search.size: 25 thread_pool.search.queue_size: 1000 # 写入线程池（仅在写入负载极高时调整） thread_pool.write.size: 16 thread_pool.write.queue_size: 10000\n# 索引线程池 thread_pool.index.size: 16 thread_pool.index.queue_size: 200 调优原则 #\n  不要随意增大线程数。线程过多会导致上下文切换开销增大、竞争加剧，反而降低性能。 关注队列拒绝率。通过 GET _cat/thread_pool?v 观察 rejected 列。如果某个线程池频繁拒绝，说明该类型操作的负载过高。 先排查根本原因。线程池拒绝通常是其他问题的表象（如慢查询、磁盘 I/O 瓶颈、GC 暂停等）。 增大队列不如增加节点。队列只是临时缓冲，长期队列积压会增加延迟和 OOM 风险。  监控线程池 #  # 查看所有线程池状态 GET _cat/thread_pool?v\u0026amp;h=name,active,queue,rejected,completed # 查看特定线程池 GET _cat/thread_pool/search?v\n# 节点级别的线程池统计 GET _nodes/stats/thread_pool \n断路器 #  断路器防止操作消耗过多内存导致 OutOfMemoryError。当请求预估将超过内存限制时，断路器会主动拒绝请求并返回错误，而不是让 JVM 崩溃。\n 断路器参数为动态设置，可通过 集群配置 API 在线修改。这里仅做参考说明。\n 断路器参数 #     参数 默认值 说明     indices.breaker.total.limit 95% 所有断路器的聚合限制（启用真实内存断路器时）。所有断路器的内存使用之和不得超过此值   indices.breaker.fielddata.limit 40% Fielddata 断路器。加载 fielddata 到内存时触发   indices.breaker.request.limit 60% 请求断路器。单次请求中聚合、排序等数据结构的内存使用   network.breaker.inflight_requests.limit 100% 传输中请求断路器。HTTP/Transport 层正在进行的请求占用的内存     百分比值相对于 JVM 堆内存（-Xmx）。\n 断路器触发时的错误 #  { \u0026#34;error\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;circuit_breaking_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;[parent] Data too large, data for [\u0026lt;operation\u0026gt;] would be [xxx/xxxmb], which is larger than the limit of [xxx/xxxmb]\u0026#34; } } 排查思路：\n 检查哪个断路器触发：GET _nodes/stats/breaker 如果是 fielddata 断路器：优化查询，避免对 text 字段做聚合，改用 keyword 如果是 request 断路器：减小聚合桶数（max_buckets）或分页聚合 如果是 total 断路器：考虑增加堆内存或增加节点   API 兼容 #     参数 默认值 属性 说明     elasticsearch.api_compatibility false 静态 是否启用 Elasticsearch API 兼容模式。启用后 Logstash、Filebeat 等 7.10.x OSS 客户端可直接连接   elasticsearch.api_compatibility_version 7.10.2 静态 兼容的 Elasticsearch 版本号。影响 _cat、_cluster/health 等 API 返回的版本信息    何时启用 #   从 Elasticsearch 迁移到 Easysearch 时，客户端代码暂时无法修改。 使用 Elastic Stack 生态工具（Logstash、Filebeat、Kibana OSS 等）直连 Easysearch。  elasticsearch.api_compatibility: true elasticsearch.api_compatibility_version: \u0026#34;7.10.2\u0026#34;  其他静态配置 #     参数 默认值 说明     script.painless.regex.enabled true 是否允许在 Painless 脚本中使用正则表达式。正则可能有性能风险，高安全场景可关闭   bulk.max_actions 无限制 单次 _bulk 请求中允许的最大操作数     延伸阅读 #    集群配置 — 断路器等动态设置的 API 操作  JVM 配置 — 堆内存设置（影响断路器计算）  内存与缓存 — Fielddata 缓存设置  ","subcategory":null,"summary":"","tags":null,"title":"线程池与断路器","url":"/easysearch/main/docs/deployment/config/node-settings/thread-pool/"},{"category":null,"content":"空白符拆分查询（Split Queries on Whitespace） #  确定查询是否在空白处拆分。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;status\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;split_queries_on_whitespace\u0026#34;: true } } } } 适用的字段类型 #   keyword wildcard  默认值 #   false  说明 #  当设置为 true 时，查询字符串在空白处拆分成多个术语。这在索引短语（如空格分隔的标签）的关键字字段中很有用。\n相关参数 #    分析器参数（Analyzer）  ","subcategory":null,"summary":"","tags":null,"title":"空白符拆分查询（Split Queries on Whitespace）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/split_queries_on_whitespace/"},{"category":null,"content":"离线安装 Easysearch #  本文档介绍如何在没有网络连接的环境中安装 Easysearch。离线安装常见于内网、政务、金融等对网络隔离有严格要求的场景。\n准备工作（在有网络的环境中） #  在可联网的机器上提前下载所有需要的安装包：\n必需文件 #     文件 用途 下载地址     Easysearch Bundle 包 包含 Easysearch + 内置 JDK  Linux AMD64   插件包（按需） IK 分词、Pinyin、KNN 等  插件列表     Bundle 包内置了 JDK，是离线安装的最简方式，无需单独准备 JDK。\n 可选文件 #     文件 用途     INFINI Console 安装包 集群管理和监控   INFINI Gateway 安装包 查询代理和网关    Linux 环境离线安装 #  步骤 1：系统调优 #  在安装 Easysearch 之前，先完成操作系统调优（此步骤不需要网络）：\n# 文件描述符 sudo tee /etc/security/limits.d/21-infini.conf \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; * soft nofile 1048576 * hard nofile 1048576 * soft memlock unlimited * hard memlock unlimited EOF # 内核参数 sudo sysctl -w vm.max_map_count=262144 echo \u0026quot;vm.max_map_count=262144\u0026quot; | sudo tee -a /etc/sysctl.conf \n完整调优参数参见 系统调优。\n 步骤 2：创建用户 #  groupadd -g 602 easysearch useradd -u 602 -g easysearch -m -d /home/easysearch -c \u0026#39;easysearch\u0026#39; -s /bin/bash easysearch 步骤 3：解压安装 #  将预先下载的 Bundle 包传输到目标机器（通过 U 盘、SCP 等方式），然后解压安装：\n# 创建安装目录 mkdir -p /data/easysearch # 解压 bundle 包到安装目录 tar -zxf easysearch-*-linux-amd64-bundle.tar.gz -C /data/easysearch\n# 初始化（生成证书和 admin 密码） cd /data/easysearch \u0026amp;\u0026amp; bin/initialize.sh -s\n# 调整目录权限 chown -R easysearch:easysearch /data/easysearch \n初始化完成后，admin 密码会在终端输出中显示，请务必记录。\n 步骤 4：安装插件（可选） #  如需使用分词等插件，将提前下载的插件包传输到目标机器后安装：\n# 切换到 easysearch 用户 su - easysearch # 离线安装插件（以 analysis-ik 为例） cd /data/easysearch bin/easysearch-plugin install file:///tmp/analysis-ik-*.zip\n# 查看已安装的插件 bin/easysearch-plugin list 步骤 5：启动并验证 #\n # 启动 Easysearch su easysearch -c \u0026#34;/data/easysearch/bin/easysearch -d -p pid\u0026#34; # 验证（使用初始化时终端输出的密码） curl -ku admin:YOUR_PASSWORD https://localhost:9200\n# 停止 Easysearch kill $(cat /data/easysearch/pid) 步骤 6：配置为系统服务（推荐） #\n # 创建 systemd 服务文件 sudo tee /usr/lib/systemd/system/easysearch.service \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; [Unit] Description=Easysearch After=network.target [Service] Type=forking User=easysearch ExecStart=/data/easysearch/bin/easysearch -d -p pid PIDFile=/data/easysearch/pid LimitNOFILE=65536 LimitMEMLOCK=infinity [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable easysearch sudo systemctl start easysearch Windows 环境离线安装 #\n 准备文件 #  在有网络的环境中下载：\n  Easysearch Windows 版  JDK 17 Windows 版  安装步骤 #   解压 Easysearch 到目标目录（如 D:\\easysearch） 解压 JDK 到 Easysearch 目录下，重命名为 jdk 修改配置文件 config/easysearch.yml：  # 如果无法在 Windows 环境生成证书，可临时禁用安全模块 security.enabled: false 运行：  bin\\easysearch.bat  如需启用安全功能，可在 Linux 环境提前生成证书，然后复制到 Windows 的 config/ 目录。\n 离线升级 #  升级步骤与安装类似，使用 滚动重启方式：\n 下载新版本 Bundle 包并传输到目标机器 按照滚动重启流程，逐节点停止、替换、启动 注意保留 config/、data/、logs/ 目录  延伸阅读 #    Linux 部署  系统调优  生产环境部署  分布式集群部署  ","subcategory":null,"summary":"","tags":null,"title":"离线安装","url":"/easysearch/main/docs/deployment/install-guide/offline-install/"},{"category":null,"content":"短语索引（Index Phrases） #  启用短语查询的优化。当启用此参数时，Easysearch 会为短语查询构建额外的索引结构。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index_phrases\u0026#34;: true } } } } 适用的字段类型 #   text match_only_text  默认值 #   false  说明 #  启用 index_phrases 后，短语查询的性能会显著提高，但会增加索引大小。\n相关参数 #    前缀索引（Index Prefixes）  索引参数（Index）  ","subcategory":null,"summary":"","tags":null,"title":"短语索引（Index Phrases）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/index_phrases/"},{"category":null,"content":"短语搜索分析器（Search Quote Analyzer） #  指定在处理带引号的短语查询时使用的分析器。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;search_quote_analyzer\u0026#34;: \u0026#34;keyword\u0026#34; } } } } 适用的字段类型 #   text  默认值 #   如果未指定，使用 search_analyzer 的值 如果 search_analyzer 也未指定，使用 analyzer 的值  说明 #  在执行带引号短语查询时，可以使用不同的分析器来改进相关性。例如，可以禁用停用词移除以获得更精确的短语匹配。\n相关参数 #    分析器参数（Analyzer）  搜索分析器参数（Search Analyzer）  ","subcategory":null,"summary":"","tags":null,"title":"短语搜索分析器（Search Quote Analyzer）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/search_quote_analyzer/"},{"category":null,"content":"海光平台安装 #  海光平台介绍 #  海光平台基于自主可控的 x86 架构（获 AMD 授权），提供高性能 CPU 和 DCU 产品，广泛应用于服务器、云计算及信创领域，兼容主流生态，支持国产操作系统与关键行业应用。\n海光平台安装参考 #  目前，Easysearch 已支持在海光芯片的国产操作系统上运行，联网环境建议使用一键安装脚本进行安装，离线环境建议下载 Bundle 包进行安装，分布式集群安装请参考 分布式集群安装。\n 前提条件：已参照 系统调优进行了系统优化，同时为 Easysearch 创建了专用的用户。\n 初始化系统参数及用户命令参考 #  # 调整内核配置 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p # 增加用户组与用户 groupadd -r easysearch \u0026amp;\u0026amp; useradd -r -g easysearch -d /home/easysearch -s /sbin/nologin -c \u0026#34;Easysearch Service Account\u0026#34; easysearch 安装命令参考 #  # 创建数据目录 mkdir -p /opt/easysearch # 下载最新版本的 Easysearch 并安装 curl -sSL http://get.infini.cloud | bash -s -- -p easysearch -d /opt/easysearch # 进入 Easysearch 目录 cd /opt/easysearch # 初始化 Easysearch bin/initialize.sh -s # 调整目录权限 chown -R easysearch:easysearch /opt/easysearch # 启动 Easysearch runuser -u easysearch -- /opt/easysearch/bin/easysearch -d -p /opt/easysearch/easysearch.pid  注意：初始化过程中会生成随机密码，并不会保存到日志文件中，只会在终端显示一次，请妥善保存。如果忘记 admin 密码，可以使用 bin/reset_admin_password.sh 进行重置。\n 已验证系统环境：\n Hygon C86 7185/UnionTech OS Server 20 1050d  如果您在其他海光平台的国产操作系统上安装遇到问题，欢迎通过 提交工单与我们联系。\n","subcategory":null,"summary":"","tags":null,"title":"海光平台安装","url":"/easysearch/main/docs/deployment/install-guide/ciip/hygon/"},{"category":null,"content":"最大输入长度（Max Input Length） #  限制自动完成建议的最大输入长度。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;suggest\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;completion\u0026#34;, \u0026#34;max_input_length\u0026#34;: 50 } } } } 适用的字段类型 #   completion  默认值 #   50  说明 #  当输入超过指定的长度时，将被截断。这用于限制自动完成索引的大小和内存使用。\n相关参数 #    完成上下文（Contexts）  ","subcategory":null,"summary":"","tags":null,"title":"最大输入长度（Max Input Length）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/max_input_length/"},{"category":null,"content":"最大短语大小（Max Shingle Size） #  搜索自动完成时使用的最大 shingle 大小。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;search_as_you_type\u0026#34;, \u0026#34;max_shingle_size\u0026#34;: 3 } } } } 适用的字段类型 #   search_as_you_type（mapper-extras 模块）  默认值 #   3  说明 #  search_as_you_type 字段类型生成额外的子字段以支持即时搜索。max_shingle_size 参数控制用于前缀查询的 shingle 的最大大小。值越大，索引越大，但查询越快。\n相关参数 #    分析器参数（Analyzer）  搜索分析器参数（Search Analyzer）  ","subcategory":null,"summary":"","tags":null,"title":"最大短语大小（Max Shingle Size）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/max_shingle_size/"},{"category":null,"content":"忽略Z坐标（Ignore Z Value） #  确定是否忽略地理空间数据中的 Z 坐标（高度）。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34;, \u0026#34;ignore_z_value\u0026#34;: true } } } } 适用的字段类型 #   geo_point geo_shape  默认值 #   true  说明 #  地理坐标通常是二维的（经度和纬度）。某些输入格式可能包括 Z 坐标（如 GeoJSON 中的高度）。启用此参数时，Z 坐标会被忽略。禁用时，包含 Z 坐标会导致错误。\n示例 #  POST my-index/_doc/1 { \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.7128, \u0026#34;lon\u0026#34;: -74.0060, \u0026#34;z\u0026#34;: 100 } } 相关参数 #    索引参数（Index）  ","subcategory":null,"summary":"","tags":null,"title":"忽略Z坐标（Ignore Z Value）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/ignore_z_value/"},{"category":null,"content":"完成上下文（Contexts） #  定义用于过滤自动完成建议的上下文。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;suggest\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;completion\u0026#34;, \u0026#34;contexts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;category\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;category\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;geo\u0026#34;, \u0026#34;precision\u0026#34;: \u0026#34;5km\u0026#34; } ] } } } } 适用的字段类型 #   completion  上下文类型 #   category: 按类别过滤建议 geo: 按地理位置过滤建议  说明 #  上下文允许根据多个维度（如类别或地理位置）对完成建议进行分类和过滤。这在需要基于上下文的自动完成的应用中很有用。\n相关参数 #    分隔符保留（Preserve Separators）  位置增量保留（Preserve Position Increments）  ","subcategory":null,"summary":"","tags":null,"title":"完成上下文（Contexts）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/contexts/"},{"category":null,"content":"字段数据频率过滤（Fielddata Frequency Filter） #  为了减少内存占用，可以配置字段数据过滤器来移除低频率的项。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;tags\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fielddata\u0026#34;: true, \u0026#34;fielddata_frequency_filter\u0026#34;: { \u0026#34;min\u0026#34;: 2, \u0026#34;min_segment_size\u0026#34;: 50 } } } } } 适用的字段类型 #   text（启用 fielddata 时）  参数选项 #   min: 项必须出现的最少次数（默认值：0） max: 项可能出现的最多次数（无上限） min_segment_size: 仅在至少有这么多文档的段中应用过滤器（默认值：0）  说明 #  当启用字段数据用于聚合或排序时，此参数可以降低内存占用。低频项会被从内存中移除。\n相关参数 #    字段数据参数（Fielddata）  ","subcategory":null,"summary":"","tags":null,"title":"字段数据频率过滤（Fielddata Frequency Filter）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/fielddata_frequency_filter/"},{"category":null,"content":"区域设置（Locale） #  用于解析日期或范围值的区域设置。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;publish_date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;dd/MM/yyyy\u0026#34;, \u0026#34;locale\u0026#34;: \u0026#34;fr_FR\u0026#34; } } } } 适用的字段类型 #   date date_nanos date_range integer_range float_range long_range double_range  说明 #  某些日期格式取决于特定的区域设置（例如月份名称）。使用此参数指定应用于解析的区域设置。\n常见区域设置值 #   en_US - 美国英语 fr_FR - 法语（法国） de_DE - 德语（德国） es_ES - 西班牙语（西班牙） zh_CN - 中文（中国）  相关参数 #    格式参数（Format）  ","subcategory":null,"summary":"","tags":null,"title":"区域设置（Locale）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/locale/"},{"category":null,"content":"前缀索引（Index Prefixes） #  启用前缀查询的优化。当启用此参数时，Easysearch 会为前缀查询构建额外的索引结构。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;username\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index_prefixes\u0026#34;: {} } } } } 适用的字段类型 #   text match_only_text  参数选项 #  { \u0026#34;min_chars\u0026#34;: 0, \u0026#34;max_chars\u0026#34;: 20 }  min_chars: 最小前缀长度（默认值：0） max_chars: 最大前缀长度（默认值：20）  说明 #  启用 index_prefixes 后，通配符查询和前缀查询的性能会显著提高。但会增加索引大小。较小的 min_chars 和 max_chars 值会使索引更大。\n相关参数 #    短语索引（Index Phrases）  索引参数（Index）  ","subcategory":null,"summary":"","tags":null,"title":"前缀索引（Index Prefixes）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/index_prefixes/"},{"category":null,"content":"分隔符保留（Preserve Separators） #  确定在自动完成建议中是否保留分隔符。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;suggest\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;completion\u0026#34;, \u0026#34;preserve_separators\u0026#34;: true } } } } 适用的字段类型 #   completion  默认值 #   true  说明 #  当启用时，分隔符（如空格、连字符和斜杠）被保留为单独的项。这有助于匹配在分隔符边界处开始的建议。\n相关参数 #    位置增量保留（Preserve Position Increments）  ","subcategory":null,"summary":"","tags":null,"title":"分隔符保留（Preserve Separators）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/preserve_separators/"},{"category":null,"content":"位置增量启用（Enable Position Increments） #  确定是否在令牌计数中包含位置增量。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;token_count\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;enable_position_increments\u0026#34;: true } } } } 适用的字段类型 #   token_count（mapper-extras 模块）  默认值 #   true  说明 #  当启用时，停用词等被移除的令牌所创建的位置间隙被计入令牌计数。当禁用时，只计算实际产生的令牌。\n相关参数 #    分析器参数（Analyzer）  ","subcategory":null,"summary":"","tags":null,"title":"位置增量启用（Enable Position Increments）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/enable_position_increments/"},{"category":null,"content":"位置增量保留（Preserve Position Increments） #  确定是否在自动完成建议中保留位置增量。\n基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;suggest\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;completion\u0026#34;, \u0026#34;preserve_position_increments\u0026#34;: true } } } } 适用的字段类型 #   completion  默认值 #   true  说明 #  当启用时，停用词等被移除的术语所创建的位置间隙被保留。这会影响自动完成建议的精确性和性能。\n相关参数 #    分隔符保留（Preserve Separators）  ","subcategory":null,"summary":"","tags":null,"title":"位置增量保留（Preserve Position Increments）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/preserve_position_increments/"},{"category":null,"content":"Log4j2 日志配置 #  config/log4j2.properties 是 Easysearch 的日志配置文件，控制日志输出格式、级别、文件轮转等。\n 文件位置与格式 #  # 默认位置 $ES_HOME/config/log4j2.properties 该文件使用 Log4j2 的 properties 格式，配置 appenders（日志目标）、loggers（日志器）和日志级别。\n修改后无需重启，Easysearch 会自动重新加载配置。\n 核心概念 #  Appenders（日志输出目标） #  定义日志输出到哪里，有以下类型：\n   Appender 说明     Console 控制台输出（测试环境使用）   RollingFile 滚动文件输出（轮转、自动删除旧文件）    Loggers（日志器） #  根据组件或模块设置不同的日志级别。根 logger (rootLogger) 作用于全局。\n日志级别 #     级别 说明     TRACE 最详细   DEBUG 调试信息   INFO 一般信息（默认）   WARN 警告   ERROR 错误   FATAL 致命错误     默认日志文件 #  Easysearch 默认配置会生成以下日志文件（存储在 ${ES_HOME}/logs/）：\n   日志文件 用途     \u0026lt;cluster-name\u0026gt;.log 主日志（纯文本，旧格式）   \u0026lt;cluster-name\u0026gt;_server.json 主日志（JSON 格式，便于采集）   \u0026lt;cluster-name\u0026gt;_deprecation.log 已废弃 API 警告   \u0026lt;cluster-name\u0026gt;_deprecation.json 已废弃 API 警告（JSON 格式）   \u0026lt;cluster-name\u0026gt;_index_search_slowlog.log 搜索慢查询日志   \u0026lt;cluster-name\u0026gt;_index_search_slowlog.json 搜索慢查询日志（JSON 格式）   \u0026lt;cluster-name\u0026gt;_index_indexing_slowlog.log 索引慢日志   \u0026lt;cluster-name\u0026gt;_index_indexing_slowlog.json 索引慢日志（JSON 格式）     常见配置修改 #  调整根日志级别 #  # 修改这一行，可选：TRACE, DEBUG, INFO, WARN, ERROR, FATAL rootLogger.level=info 示例：启用 DEBUG 日志\nrootLogger.level=debug  此方法会增加日志文件大小，建议在排查问题时临时使用。\n 调整特定模块日志级别 #  # 为特定模块设置日志级别 logger.discovery.name=org.easysearch.discovery logger.discovery.level=debug logger.cluster.name=org.easysearch.cluster logger.cluster.level=debug 禁用特定日志 #\n # 禁用某个模块的日志输出 logger.quiet_module.name=org.easysearch.some.module logger.quiet_module.level=off logger.quiet_module.appenderRef.console.ref=console logger.quiet_module.additivity=false 日志轮转设置 #  默认配置中，日志文件按大小（128 MB）和时间（每天）自动轮转：\n# 按大小轮转 appender.rolling.policies.size.size=128MB 按时间轮转（每天） appender.rolling.policies.time.type=TimeBasedTriggeringPolicy appender.rolling.policies.time.interval=1 appender.rolling.policies.time.modulate=true 修改轮转大小（例：改为 256 MB）\nappender.rolling.policies.size.size=256MB 日志自动删除策略 #  日志累积超过指定大小时自动删除最旧的文件：\n# 日志文件总大小超过 2GB 时，删除最旧的文件 appender.rolling.strategy.action.condition.nested_condition.exceeds=2GB  动态调整日志级别（推荐） #  如果不想重启或编辑文件，可以使用 集群设置 API 动态调整日志级别。详见 日志配置 - 动态调整日志级别。\n恢复为默认值\nPUT /_cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;logger.org.easysearch.discovery\u0026#34;: null } }  此方法修改仅对当前集群状态生效，重启节点后失效。对于持久修改，需编辑 log4j2.properties。\n  JSON 格式日志 #  Easysearch 的默认配置使用 JSON 格式日志，便于日志采集和分析：\nJSON 日志示例\n{ \u0026#34;@timestamp\u0026#34;: \u0026#34;2026-02-18T10:30:45.123Z\u0026#34;, \u0026#34;log.level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;started\u0026#34;, \u0026#34;node.name\u0026#34;: \u0026#34;node-1\u0026#34;, \u0026#34;cluster.name\u0026#34;: \u0026#34;my-cluster\u0026#34;, \u0026#34;elasticsearch.version\u0026#34;: \u0026#34;1.3.0\u0026#34; } JSON 日志的好处：\n 结构化便于机器解析 易于集成到 ELK/日志系统 支持强大的日志聚合和搜索   慢日志配置 #  搜索慢日志 #  搜索慢日志在 \u0026lt;cluster-name\u0026gt;_index_search_slowlog.log 中。慢查询阈值通过索引设置配置（详见 日志配置）：\nPUT /my-index/_settings { \u0026#34;index.search.slowlog.threshold.query.warn\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.info\u0026#34;: \u0026#34;5s\u0026#34; } 索引慢日志 #  索引慢日志在 \u0026lt;cluster-name\u0026gt;_index_indexing_slowlog.log 中：\nPUT /my-index/_settings { \u0026#34;index.indexing.slowlog.threshold.index.warn\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;index.indexing.slowlog.threshold.index.info\u0026#34;: \u0026#34;5s\u0026#34; }  故障排查 #  日志文件爆满 #  症状：磁盘空间不足\n原因：\n 日志级别设置过高（如 DEBUG） 轮转策略配置不当 日志没有及时删除  解决：\n 检查日志级别是否过高 确认轮转大小和删除策略 手动删除旧日志：rm logs/my-cluster-*.log.gz  找不到特定的日志信息 #  症状：想找某个操作的日志但找不到\n原因：\n 日志级别不足（如操作在 DEBUG 级别才输出） 操作来自的模块日志被禁用  解决：\n 暂时提高日志级别（使用集群设置 API） 指定模块进行调试 查看相关模块的 logger 配置  日志格式乱码 #  原因：字符编码不匹配\n解决：\n 确保日志查看工具支持 UTF-8 检查系统 locale：echo $LANG   最佳实践 #   生产环境：保持 INFO 级别，仅在排查问题时临时调整 使用 JSON 日志：便于采集和分析，不要禁用 *_server.json appender 定期清理：启用自动轮转和删除，避免日志爆满 集中管理：使用日志采集工具（Filebeat、Logstash）将日志聚合到中央日志系统 监控关键日志：对 WARN、ERROR、FATAL 级别设置告警   延伸阅读 #    日志配置 — 日志系统概览、慢日志配置、Logger 名称  集群配置 — 通过 API 动态调整日志级别  ","subcategory":null,"summary":"","tags":null,"title":"Log4j2 日志配置","url":"/easysearch/main/docs/deployment/config/node-settings/log4j2/"},{"category":null,"content":"FAQ #    Easysearch 版本降级会报错 #    cannot downgrade a node from version [1.7.0] to version [1.6.1]\n[2024-01-28T09:42:34,314][ERROR][o.e.b.EasysearchUncaughtExceptionHandler] [onenode-masters-0] uncaught exception in thread [main] org.easysearch.bootstrap.StartupException: java.lang.IllegalStateException: cannot downgrade a node from version [1.7.0] to version [1.6.1] at org.easysearch.bootstrap.easysearch.init(Easysearch.java:173) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.execute(Easysearch.java:160) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:71) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.cli.Command.mainWithoutErrorHandling(Command.java:112) ~[easysearch-cli-1.6.1.jar:1.6.1] at org.easysearch.cli.Command.main(Command.java:75) ~[easysearch-cli-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.main(Easysearch.java:125) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.main(Easysearch.java:67) ~[easysearch-1.6.1.jar:1.6.1] Caused by: java.lang.IllegalStateException: cannot downgrade a node from version [1.7.0] to version [1.6.1] at org.easysearch.env.NodeMetadata.upgradeToCurrentVersion(NodeMetadata.java:79) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.env.NodeEnvironment.loadNodeMetadata(NodeEnvironment.java:418) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.env.NodeEnvironment.\u0026lt;init\u0026gt;(NodeEnvironment.java:315) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.node.Node.\u0026lt;init\u0026gt;(Node.java:343) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.node.Node.\u0026lt;init\u0026gt;(Node.java:273) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.Bootstrap$5.\u0026lt;init\u0026gt;(Bootstrap.java:212) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.Bootstrap.setup(Bootstrap.java:212) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.Bootstrap.init(Bootstrap.java:378) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.init(Easysearch.java:169) ~[easysearch-1.6.1.jar:1.6.1]   基于已有的 PVC 构建新集群报错 #    2024-03-24T20:21:36.975095805+08:00 Caused by: org.easysearch.cluster.coordination.CoordinationStateRejectedException: join validation on cluster state with a different cluster uuid rCyF7ZGyRL275eWC0qoTDQ than local cluster uuid uSFTnF1kRU661ENZRAbq-A, rejecting 2024-03-24T20:21:36.975099446+08:00 at org.easysearch.cluster.coordination.JoinHelper.lambda$new$5(JoinHelper.java:149) ~[easysearch-1.7.1.jar:1.7.1] 2024-03-24T20:21:36.975103455+08:00 at com.infinilabs.security.ssl.transport.SecuritySSLRequestHandler.messageReceivedDecorate(SecuritySSLRequestHandler.java:201) ~[?:?] 2024-03-24T20:21:36.975107101+08:00 at com.infinilabs.security.transport.SecurityRequestHandler.messageReceivedDecorate(SecurityRequestHandler.java:330) ~[?:?] ","subcategory":null,"summary":"","tags":null,"title":"FAQ","url":"/easysearch/main/docs/deployment/install-guide/operator/FAQ/"},{"category":null,"content":"国密与国产化 #  Easysearch 围绕国产密码算法与国产化生态深度设计，为政企与关键行业提供可落地、可审计、可替代的搜索基础能力。\n 核心能力 #     能力 说明     国密算法 全量支持 SM2/SM3/SM4，替代 RSA/AES/SHA   国产 CPU 适配鲲鹏、飞腾、海光、龙芯、兆芯   国产 OS 适配银河麒麟、统信 UOS、中标麒麟   等保合规 满足等保三级及信创合规要求     国密算法 #  Easysearch 基于 铜锁（Tongsuo）实现完整的国密 TLS 能力。\nSM2 — 非对称加密 #  替代 RSA/ECDSA，用于：\n TLS 证书签名与验证 节点间身份认证 客户端证书认证  SM3 — 哈希算法 #  替代 SHA-256，用于：\n 密码存储与验证 数据完整性校验 审计日志防篡改  SM4 — 对称加密 #  替代 AES，用于：\n 节点间通信加密（Transport 层） HTTPS 传输加密（HTTP 层） 敏感数据存储  加密套件 #     套件 说明     TLS_SM4_GCM_SM3 SM4-GCM 加密 + SM3 哈希（推荐）   TLS_SM4_CCM_SM3 SM4-CCM 加密 + SM3 哈希     国产化适配 #  国产处理器 #     处理器 架构 状态     鲲鹏 (Kunpeng) ARM64 ✅ 认证通过   飞腾 (Phytium) ARM64 ✅ 认证通过   海光 (Hygon) x86 ✅ 认证通过   龙芯 (Loongson) LoongArch ✅ 认证通过   兆芯 (Zhaoxin) x86 ✅ 认证通过    国产操作系统 #     操作系统 状态     银河麒麟 (Kylin OS) ✅ 互认证   统信 UOS (UnionTech OS) ✅ 互认证   中标麒麟 (NeoKylin) ✅ 互认证    生态兼容 #  可与国产数据库、中间件、安全产品协同部署，融入整体国产化解决方案。\n 合规能力 #     维度 能力     传输加密 SM4 对称加密 + SM2 身份认证   密码存储 SM3 哈希，不可逆   访问控制 RBAC 角色权限，支持文档/字段级   审计日志 完整操作记录，SM3 防篡改   数据完整性 SM3 校验    合规标准 #     标准 说明     GM/T 0024-2014 SSL VPN 技术规范   GM/T 0028-2014 密码模块安全技术要求   JR/T 0197-2020 金融行业密码应用技术标准     应用场景 #  政务与公共服务 #  满足政务系统对国密算法与国产化环境的强制要求，支撑电子政务、公共数据开放平台。\n金融与关键基础设施 #  金融、能源、电信等行业，构建符合监管要求的搜索与分析能力。\n信创替代 #  在国产化环境中搭建搜索平台，平滑迁移既有 Elasticsearch 应用。\n行业信息化 #  作为国产化体系中的搜索引擎组件，支撑数据中台、日志分析、全文检索等场景。\n 快速开始 #  1. 生成国密证书 #  bin/generate-tlcp-certs.sh /path/to/output \u0026#34;changeit\u0026#34; 2. 配置国密 TLS #  security.enabled: true security.ssl.use_tongsuo: true # HTTP 层 security.ssl.http.enabled: true security.ssl.http.tlcp.sign_certificate: server_sign.crt security.ssl.http.tlcp.sign_key: server_sign.key security.ssl.http.tlcp.enc_certificate: server_enc.crt security.ssl.http.tlcp.enc_key: server_enc.key security.ssl.http.tlcp.trusted_ca_certificate: ca_chain.crt\n# Transport 层 security.ssl.transport.enabled: true security.ssl.transport.tlcp.sign_certificate: server_sign.crt security.ssl.transport.tlcp.sign_key: server_sign.key security.ssl.transport.tlcp.enc_certificate: server_enc.crt security.ssl.transport.tlcp.enc_key: server_enc.key security.ssl.transport.tlcp.trusted_ca_certificate: ca_chain.crt 3. 验证配置 #\n bin/tlcp-curl.sh --url https://localhost:9200/_security/sslinfo?pretty \\  -u admin:changeit \\  --cert config/client_sign.crt --key config/client_sign.key \\  --enc-cert config/client_enc.crt --enc-key config/client_enc.key \\  --ca config/ca_chain.crt  仅当服务端配置 security.ssl.http.clientauth_mode: REQUIRE 时，客户端证书访问才是必需的；默认场景可仅使用用户名密码。\n 确认 ssl_provider 为 Tongsuo_Security_Provider。\n 完整配置指南见 国密配置。\n  相关文档 #    国密配置指南：完整的 TLCP 双证书配置  TLS 安全配置：标准 TLS 配置  安全模块：认证授权与访问控制  ","subcategory":null,"summary":"","tags":null,"title":"国密与国产化","url":"/easysearch/main/docs/features/national-encryption/"},{"category":null,"content":"鲲鹏平台安装 #  鲲鹏平台介绍 #  鲲鹏平台基于 ARM 架构，由华为自主研发，提供高性能、低功耗的服务器处理器，广泛应用于信创、云计算、大数据和分布式存储等场景，全面适配国产操作系统与生态。\n鲲鹏平台安装参考 #  目前，Easysearch 已支持在鲲鹏芯片的国产操作系统上运行，联网环境建议使用一键安装脚本进行安装，离线环境建议下载 Bundle 包进行安装，分布式集群安装请参考 分布式集群安装。\n 前提条件：已参照 系统调优进行了系统优化，同时为 Easysearch 创建了专用的用户。\n 初始化系统参数及用户命令参考 #  # 调整内核配置 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p # 增加用户组与用户 groupadd -r easysearch \u0026amp;\u0026amp; useradd -r -g easysearch -d /home/easysearch -s /sbin/nologin -c \u0026#34;Easysearch Service Account\u0026#34; easysearch 安装命令参考 #  # 创建数据目录 mkdir -p /opt/easysearch # 下载最新版本的 Easysearch 并安装 curl -sSL http://get.infini.cloud | bash -s -- -p easysearch -d /opt/easysearch # 进入 Easysearch 目录 cd /opt/easysearch # 初始化 Easysearch bin/initialize.sh -s # 调整目录权限 chown -R easysearch:easysearch /opt/easysearch # 启动 Easysearch runuser -u easysearch -- /opt/easysearch/bin/easysearch -d -p /opt/easysearch/easysearch.pid  注意：初始化过程中会生成随机密码，并不会保存到日志文件中，只会在终端显示一次，请妥善保存。如果忘记 admin 密码，可以使用 bin/reset_admin_password.sh 进行重置。\n 已验证系统环境：\n Kunpeng 920 Taishan 200/银河麒麟高级服务器操作系统V10 SP3  如果您在其他鲲鹏平台的国产操作系统上安装遇到问题，欢迎通过 提交工单与我们联系。\n","subcategory":null,"summary":"","tags":null,"title":"鲲鹏平台安装","url":"/easysearch/main/docs/deployment/install-guide/ciip/kunpeng/"},{"category":null,"content":"跨集群搜索（CCS） #  Easysearch 跨集群搜索（Cross-Cluster Search, CCS）允许用户在本地集群发起一个查询请求，同时检索分布在不同地理位置、不同业务环境中的多个远程集群——无需进行数据迁移或归集，即可实现全局视角下的即时分析。\n 核心能力 #  统一视图，全局洞察 #  一个查询请求即可覆盖全公司范围内的索引数据，无需切换访问点，实现跨业务线的综合分析。\n降低成本，按需存储 #  数据保留在产生的源集群中，避免大规模数据传输带来的网络带宽损耗与存储冗余成本。\n系统解耦与灵活性 #  各个集群可以独立升级、独立维护，通过 CCS 实现逻辑上的互联互通，降低超大规模架构的运维复杂度。\n故障隔离 #  若某个远程集群暂时不可用，CCS 可配置为忽略故障集群并返回其余可用部分的搜索结果，保障服务不中断。\n 快速上手 #  1. 配置远程集群连接 #  在本地集群上注册远程集群的种子节点：\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.remote\u0026#34;: { \u0026#34;cluster-beijing\u0026#34;: { \u0026#34;seeds\u0026#34;: [\u0026#34;beijing-node:9300\u0026#34;] }, \u0026#34;cluster-shanghai\u0026#34;: { \u0026#34;seeds\u0026#34;: [\u0026#34;shanghai-node:9300\u0026#34;] } } } } 2. 跨集群搜索 #  使用 集群名:索引名 的格式指定远程索引：\nPOST /cluster-beijing:logs-*,cluster-shanghai:logs-*/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-1d/d\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;now\u0026#34; } } }, \u0026#34;size\u0026#34;: 20 } 3. 同时搜索本地和远程索引 #  POST /local-logs-*,cluster-beijing:logs-*/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34; } }, \u0026#34;size\u0026#34;: 10 } 4. 跨集群聚合 #  POST /cluster-beijing:metrics-*,cluster-shanghai:metrics-*/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;region_stats\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;region.keyword\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_latency\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;latency\u0026#34; } } } } } }  连接管理 #  查看远程集群状态 #  GET _remote/info 返回所有已配置的远程集群的连接状态、种子节点和连接模式。\n动态更新连接 #  支持实时添加或移除远程集群，无需重启：\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.remote.cluster-guangzhou.seeds\u0026#34;: [\u0026#34;guangzhou-node:9300\u0026#34;] } } 移除远程集群 #  PUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.remote.cluster-guangzhou.seeds\u0026#34;: null } }  安全与权限 #   支持基于安全凭证的远程访问 跨集群通信链路加密（TLS） 严格遵循各集群原有的数据访问权限 非管理员用户需要映射到适当的角色权限   应用场景 #     场景 说明     多地域业务统一搜索 各区域数据本地存储、本地写入，总部通过 CCS 统一查询   两地三中心容灾架构 双活或多活数据中心下，数据就近读取与异地备份查询   历史数据归档查询 历史数据保存在独立集群中，按需查询，避免热集群负担   平滑架构演进 在集群拆分、迁移、升级过程中，保持业务查询连续性     相关文档 #    跨集群复制  集群扩容与分片  ","subcategory":null,"summary":"","tags":null,"title":"跨集群搜索","url":"/easysearch/main/docs/features/cross-cluster-search/"},{"category":null,"content":"术语表 #  本页汇总文档中常用的中英文术语对照，方便写作时统一用词，也方便读者建立清晰的心智模型。\n 一、数据结构与存储 #     英文术语 中文叫法 说明     Index 索引 逻辑上的数据集合，通常一类业务一组索引，可按时间/租户再拆前缀。   Document 文档 索引中的基本数据单元，以 JSON 形式表示。   Shard 分片 水平切分单位，number_of_shards 只在建索引时生效。   Primary Shard 主分片 负责接受写入并复制到副本。   Replica Shard 副本分片 / 副本 提供高可用与读扩展，副本数可在线调整。   Segment 段 Lucene 的不可变索引文件块，刷新/合并都围绕它展开。   Inverted Index 倒排索引 从词项到文档的映射结构，全文搜索的核心数据结构。   _source _source 文档原文 建议默认保留，是检索展示与重建索引的\u0026quot;真相来源\u0026quot;。   Stored Fields 存储字段 只在少数场景单独使用，更多依赖 _source + doc_values。   doc_values doc_values 列式存储 聚合与排序的核心支撑，应在大多数可聚合/排序字段上启用。   Fielddata fielddata 仅在 text 字段聚合/排序时使用，能不用尽量不用。   Translog 事务日志 写入操作的预写日志（WAL），保证 flush 前的数据不丢失。   Routing 路由 控制文档写入和查询时定向到特定分片的机制，默认按 _id 哈希。    二、Mapping 与文本分析 #     英文术语 中文叫法 说明     Mapping 映射 / Mapping 描述字段类型与索引规则，是一切查询/聚合行为的基础。   text Field 文本字段（text） 做全文检索，用分析器拆分为词项，不适合精确过滤/聚合。   keyword Field 关键字字段（keyword） 精确匹配、过滤、聚合、排序使用，不做分词。   integer / long / float / double 数值字段 数值类型字段，支持范围查询和聚合运算。   date Field 日期字段 支持多种日期格式，底层以毫秒时间戳存储。   boolean Field 布尔字段 仅存储 true/false 值。   geo_point 地理点 存储经纬度坐标，支持地理距离和区域查询。   geo_shape 地理形状 存储多边形、线段等复杂地理形状，支持空间关系查询。   nested 嵌套类型 保持对象数组中字段关联关系的特殊映射类型。   join (Parent/Child) 父子关系 同一索引内建立文档间的层级关系。   object 对象类型 JSON 对象映射为扁平化的点分字段名，不保持数组内对象边界。   knn_dense_float_vector 向量字段类型 用于存储 dense 浮点向量，支持近似最近邻搜索。   Multi-fields 多字段 / multi-fields 一份源数据多个视图，如 title + title.keyword。   Dynamic Mapping 动态映射 自动推断新字段类型，推荐配合 dynamic_templates 使用。   Dynamic Template 动态模板 按字段名模式或数据类型自动套用指定的字段映射规则。   Analyzer 分析器 由字符过滤器、分词器和词元过滤器组成，可分索引时/查询时。   Tokenizer 分词器 决定如何切词，如 standard、语言专用分词器等。   Token Filter 词元过滤器 大小写、词干、停用词、同义词、n-gram 等都在这里实现。   Char Filter 字符过滤器 在分词前对原始文本做预处理（如 HTML 剥离、字符替换）。   Normalizer 归一化器 仅用于 keyword 字段的字符级标准化（小写、Unicode 折叠等）。   Stemming 词干提取 通过归并词形提升召回，需要结合 keyword_marker 等控制风险。   Stopwords 停用词 主要价值是性能，现代实践中应谨慎、少量地使用。   Synonyms 同义词 扩大召回，强调规则版本化与可回滚。    三、查询与相关性 #     英文术语 中文叫法 说明     Query DSL 查询 DSL Easysearch 基于 JSON 的查询语言，支持全文、精确、复合等各类查询。   Query Context 查询上下文 参与评分。   Filter Context 过滤上下文 不参与评分，易缓存。   BM25 BM25 算法 默认相关性评分算法，替代旧版 TF/IDF。参数 k1=1.2、b=0.75。   bool Query bool 查询 通过 must / should / filter / must_not 组合语义。   match Query match 查询 针对 text 字段，内部会做分析，不等同于 term。   term / terms Query term / terms 查询 精确值查询，用于 keyword/数值/日期字段。   range Query 范围查询 数值/日期字段常用，在 text 字段上属于昂贵查询。   match_phrase / slop 短语查询 / slop 用 positions 做邻近匹配，成本高于普通 match。   multi_match multi_match 多字段查询 best_fields、most_fields、cross_fields 几种模式。   dis_max + tie_breaker DisMax 查询 用于\u0026quot;字段竞争\u0026quot;型多字段搜索（如 title vs body）。   function_score function_score 加权 用于叠加业务信号（热度、时间衰减等）。   nested Query 嵌套查询 查询嵌套对象字段，保持对象边界完整性。   has_child / has_parent 父子查询 基于父子关系跨文档查询。   Span Query Span 查询 词项级精细控制查询族，可精确控制词项间距离和位置。   Fuzziness / Fuzzy Matching 模糊匹配 拼写容错，适合作兜底召回。   Rescore 查询重打分 对初始 Top-N 结果用更复杂的查询重新打分的二阶段策略。   Field Collapsing 结果折叠 按字段值对搜索结果分组去重，每组只返回 Top-N。   search_after search_after 深分页 用于替代大 from，建议在长列表场景中使用。   Scroll / PIT 滚动 / 时间点搜索 用于批量扫描/导出，不适合用户界面分页。   Async Search 异步搜索 将大查询提交到后台执行，客户端可轮询获取进度和结果。   Highlight 高亮 结果展示层的增强能力。   Suggesters / Autocomplete 建议与纠错 / 自动补全 拼写纠错和即时补全。   Search Template 搜索模板 使用 Mustache 模板参数化搜索查询，简化客户端代码。   _explain 评分解释 返回单条文档的评分计算过程，用于相关性调试。   _preference 查询偏好 控制搜索请求路由到特定分片或节点的参数。   track_total_hits 总命中数追踪 控制是否精确统计查询的总命中文档数，默认上限 10000。   Profile API 查询分析 API 分析查询各阶段耗时，用于定位性能瓶颈。   Term Vectors 词项向量 获取文档中特定字段的词频、位置、偏移量等信息。    四、聚合与分析 #     英文术语 中文叫法 说明     Aggregation (aggs) 聚合 对文档集合进行分组统计和指标计算的功能。   Bucket Aggregation 桶聚合 将文档按规则分组到不同的\u0026quot;桶\u0026quot;中（如 terms、date_histogram、range）。   Metric Aggregation 指标聚合 在桶内对数值字段做统计计算（如 avg、sum、min、max、cardinality）。   Pipeline Aggregation 管道聚合 对其他聚合的输出结果做二次计算（如 derivative、moving_avg、cumulative_sum）。   terms Aggregation terms 聚合 按字段值分桶，返回每个唯一值的文档计数。   date_histogram 日期直方图 按时间间隔（如小时/天/月）分桶，时序分析的核心聚合。   histogram 直方图聚合 按固定数值区间分桶。   range Aggregation 范围聚合 按自定义数值/日期范围分桶。   composite Aggregation 复合聚合 支持分页的多维聚合，适合高基数场景。   filter / filters Aggregation 过滤器聚合 用查询条件定义桶，按条件分组统计。   global Aggregation 全局聚合 忽略查询条件，在全量文档上做统计。   nested / reverse_nested Agg 嵌套 / 反嵌套聚合 对嵌套对象字段进行聚合或从嵌套返回父文档上下文。   sampler / diversified_sampler 采样聚合 对高频桶采样以提升聚合性能或多样性。   cardinality 基数 / 去重计数 估算字段中唯一值的数量（基于 HyperLogLog++）。   percentiles 百分位数 计算数值字段的百分位分布（P50、P95、P99 等）。   top_hits Top Hits 聚合 在每个桶内返回最相关的文档（常配合 terms 聚合使用）。   significant_terms 显著词项 发现统计上异常突出的词项，用于趋势发现和异常检测。    五、摄取与搜索管道 #  摄取管道（Ingest Pipeline） #     英文术语 中文叫法 说明     Ingest Pipeline 摄取管道 文档写入前的预处理链路，由有序处理器列表组成。   Processor 处理器 摄取管道中的单个处理单元（如 set、remove、grok、script 等）。   grok Processor Grok 处理器 使用正则模式从非结构化文本中提取结构化字段。   dissect Processor Dissect 处理器 基于分隔符从文本中提取字段，比 grok 更轻量。   script Processor 脚本处理器 使用 Painless 脚本对文档做自定义转换。   geoip Processor GeoIP 处理器 根据 IP 地址解析地理位置信息。   text_embedding Processor 文本向量化处理器 在写入时自动调用 Embedding 模型将文本转为向量。   default_pipeline 默认管道 索引设置中指定的默认摄取管道。   final_pipeline 最终管道 索引设置的强制管道，无法被客户端绕过。   on_failure 失败处理 处理器失败时执行的备用处理器列表。   _simulate 模拟管道 模拟管道执行以测试处理逻辑，不实际写入。    搜索管道（Search Pipeline） #     英文术语 中文叫法 说明     Search Pipeline 搜索管道 拦截搜索请求和响应的处理链路，支持查询重写和结果增强。   Request Processor 请求处理器 在查询执行前对原始查询进行修改和增强。   Response Processor 响应处理器 对搜索返回结果进行后处理和增强。   Search Phase Results Processor 搜索阶段结果处理器 在搜索阶段之间对合并结果进行处理（如 RRF 重排序）。    六、索引管理与生命周期 #     英文术语 中文叫法 说明     Index Alias 索引别名 指向一个或多个索引的虚拟名称，用于无感迁移和蓝绿切换。   Write Alias / is_write_index 写别名 别名标记为写入目标，配合 rollover 实现无感索引滚动。   Index Template 索引模板 新索引创建时自动套用的 settings/mappings/aliases 预配置。   Component Template 组件模板 可复用的模板构建块，被可组合索引模板引用。   Data Stream 数据流 面向时序数据的抽象，内部由多个 backing index 组成，简化 rollover 管理。   Backing Index 后备索引 数据流内部的底层索引，由数据流自动创建和管理。   ILM (Index Lifecycle Management) 索引生命周期管理 自动化管理索引从创建到删除的全生命周期策略。   ILM Phase (hot/warm/cold/delete) 生命周期阶段 ILM 策略中的生命周期阶段，按数据访问频率逐级迁移。   Hot-Warm-Cold Architecture 热温冷架构 按数据访问频率将索引分配到不同性能层级的节点。   Rollover 索引滚动 当索引达到年龄/大小/文档数阈值时，自动滚动到新索引。   Rollup 数据汇总 / 上卷 将细粒度历史数据聚合为粗粒度格式以降低存储成本。   Transform 数据转换 将源索引的数据通过聚合转换为新索引的汇总数据。   Clone / Shrink / Split 克隆 / 缩小 / 拆分 调整索引分片结构的三个 API。   Open / Close Index 打开 / 关闭索引 关闭索引不消耗资源但保留数据，可随时重新打开。   Index Block 索引限制 限制索引的读/写行为（如 index.blocks.write）。   Snapshot / Restore 快照 / 恢复 官方推荐的数据安全与恢复手段。   SLM (Snapshot Lifecycle Management) 快照生命周期管理 自动化的快照创建与清理策略。   Index Codec / ZSTD 索引编码 / ZSTD 压缩 控制索引存储字段的压缩算法，ZSTD 在压缩比和速度间取得平衡。   source_reuse Source 复用 去除 _source 中与 doc_values 重复的部分以减小索引大小。    七、常用 API #     英文术语 中文叫法 说明     Bulk API 批量 API 在单个请求中执行多个 index/create/update/delete 操作。   NDJSON 逐行 JSON Bulk API 使用的格式，每行一个 JSON 对象。   _reindex 重建索引 将文档从源索引复制到目标索引，支持查询过滤和脚本转换。   _update_by_query 按查询更新 按查询条件批量更新匹配文档，支持脚本修改。   _delete_by_query 按查询删除 按查询条件批量删除匹配文档。   CAT API CAT 接口 以易读表格格式返回集群状态信息的一组 API。   Task API 任务 API 查看和管理集群中正在运行的任务。   _analyze 分析测试 测试分析器对给定文本的分词结果。    八、分布式与集群管理 #     英文术语 中文叫法 说明     Cluster 集群 一组协同工作的节点，共享相同的 cluster.name。   Node 节点 集群中的一个 Easysearch 实例。   node.roles 节点角色 定义节点承担的职责（master/data/ingest/search 等）。   Master Node 主节点 管理集群状态、索引创建/删除、分片分配的专用节点。   Data Node 数据节点 存储和搜索数据，执行本地分片操作的工作节点。   Ingest Node 摄取节点 执行 Ingest Pipeline 预处理的节点。   Coordinating Node 协调节点 接收请求、拆分到各分片并汇总结果。每个节点都可充当协调节点。   Search Node 搜索节点 角色为 search 的专用节点。   remote_cluster_client 远程集群客户端 节点角色，允许连接和访问远程集群。   Cluster State 集群状态 集群元数据（mapping、settings、管道定义等），由主节点管理并分发。   Cluster Health 集群健康 集群的整体状态：green（全部正常）/ yellow（副本缺失）/ red（主分片缺失）。   Node Discovery 节点发现 节点加入集群的自动发现机制。   seed_hosts 种子节点 节点发现时的初始联系节点列表。   Shard Allocation 分片分配 主节点将分片分配到数据节点的决策过程。   Rebalancing 分片再平衡 集群自动在节点间移动分片以均衡负载。   Split Brain 脑裂 网络分区导致多个主节点并存的异常状态。   Refresh 刷新（refresh） 控制近实时可见性，新写入变为可搜索，不等价于持久化。   Flush 刷新到磁盘（flush） 将段持久化到磁盘并清空事务日志，主要保证数据安全。   Force Merge 强制合并 手动触发段合并，将多个小段合并为少量大段。仅用于只读索引。   Merge 段合并（merge） 合并段，带来磁盘/IO 峰值，是调优重点。    九、性能调优 #     英文术语 中文叫法 说明     Circuit Breaker 断路器 内存保护机制，在数据加载前估算内存需求，超限则中止操作。   Fielddata Circuit Breaker Fielddata 断路器 限制 fielddata 内存使用（默认不超过堆的 40%）。   Request Circuit Breaker 请求断路器 限制单个请求使用的内存（默认不超过堆的 60%）。   Query Cache 查询缓存 缓存 filter 子句结果的位集，加速重复查询。   Request Cache 请求缓存 缓存不变索引上的完整聚合/计数结果。   refresh_interval 刷新间隔 控制新写入文档变为可搜索的延迟（默认 1s）。   Slow Log 慢日志 记录超过阈值的搜索/索引操作，用于性能排查。   Write Throttling 写入限流 在节点/索引/分片三个层级控制写入速率。   Disk Watermark 磁盘水位线 磁盘使用率阈值，超过后限制分片分配或变为只读。   Thread Pool 线程池 各类操作（搜索、写入、管理等）的并发线程管理。   Indexing Buffer 索引缓冲区 新文档写入时的内存缓冲区，满后触发 refresh。   Bulk Queue 批量队列 写入请求的排队区域，队满时返回 rejected execution。    十、安全与多租户 #     英文术语 中文叫法 说明     Authentication 认证 推荐接企业 SSO/LDAP/AD/OIDC。   Authorization / Access Control 授权 / 权限控制 分集群级、索引级、文档/字段级。   Role / Role Mapping 角色 / 角色映射 安全模块内部权限载体，与外部后端角色映射。   Backend Role 后端角色 来自外部认证系统（如 LDAP 组）的角色字符串，用于角色映射。   Action Group 权限组 将多个权限打包复用（如 read、write、crud）。   Document-level Security (DLS) 文档级安全 按标签/租户过滤可见文档。   Field-level Security (FLS) 字段级安全 隐藏敏感字段（PII、密钥等）。   Field Masking 字段脱敏 对敏感字段做哈希或正则替换，用户只看到脱敏值。   Multi-tenancy 多租户 按索引隔离 vs 按字段标记 + 文档级安全两种主模式。   TLS / SSL 传输加密 Transport 层（节点间）和 HTTP 层（客户端）通信加密。   Admin Certificate 管理员证书 具有最高权限的客户端证书，用于管理安全配置。   Audit Log 审计日志 记录认证、授权、高危操作等安全事件，支持合规审计。   Run As (Impersonation) 身份模拟 允许有权限的用户以另一用户身份执行操作。   National Encryption (SM2/SM3/SM4) 国密算法 国产密码算法全量支持，满足信创与等保合规。    十一、AI 与向量搜索 #     英文术语 中文叫法 说明     Embedding 向量表示 / Embedding 由模型生成，存储在向量字段中。   kNN / ANN Search 向量 / 近似最近邻搜索 通过近似算法在高维向量空间中快速找到最相似的结果。   Hybrid Search (BM25+Vector) 混合检索 BM25 全文搜索 + 向量搜索的组合策略。   RRF (Reciprocal Rank Fusion) 倒数排名融合 融合多路检索结果排序的算法，用于混合搜索。   RAG 检索增强生成（RAG） Easysearch 作为检索层，LLM 作为生成层。   Multimodal Search 多模态搜索 跨文本/图片/音频等数据形态的统一向量检索。    十二、SQL 访问 #     英文术语 中文叫法 说明     SQL Plugin SQL 插件 允许使用标准 SQL 语法查询 Easysearch，翻译为原生 DSL 执行。   _sql Endpoint SQL 端点 执行 SQL 查询的 REST API 入口。   _sql/_explain SQL 转译 将 SQL 翻译为 Easysearch DSL 但不执行，用于调试和学习。   Cursor (SQL) 游标 SQL 分页机制，支持大结果集的逐批获取。   JDBC Driver JDBC 驱动 纯 Java 驱动程序，将 JDBC 调用转换为 Easysearch SQL REST API。    十三、跨集群 #     英文术语 中文叫法 说明     CCR (Cross-Cluster Replication) 跨集群复制 将数据从 Leader 集群实时同步到 Follower 集群，用于容灾和读写分离。   Leader Index 领导者索引 CCR 中作为数据源的可写索引。   Follower Index 跟随者索引 CCR 中的只读数据副本索引。   Auto-Follow 自动跟随 按模式自动对新建的匹配索引启动跨集群复制。   CCS (Cross-Cluster Search) 跨集群搜索 一次查询覆盖多个远程集群，无需数据迁移。   Remote Cluster 远程集群 通过种子节点连接的外部集群。    十四、脚本与扩展 #     英文术语 中文叫法 说明     Painless Painless 脚本语言 Easysearch 内置的安全脚本语言，用于自定义评分、处理和转换。   Stored Script 存储脚本 保存在集群状态中的可复用脚本，通过 ID 引用。   Plugin 插件 扩展 Easysearch 功能的可安装组件。   Module 模块 内置集成的功能组件，无需单独安装。   Rule Engine 规则引擎 高性能实时规则匹配，在写入时自动完成关键词检测和打标。    ","subcategory":null,"summary":"","tags":["术语","Glossary"],"title":"术语表","url":"/easysearch/main/docs/resources/terminology/"},{"category":null,"content":"日志配置 #  本页介绍 config/log4j2.properties 日志配置文件以及慢日志设置。\n 日志文件说明 #  Easysearch 使用 Log4j 2 作为日志框架。日志文件默认存储在 ${ES_HOME}/logs/ 目录下（可通过 path.logs 自定义）。\n   文件 说明     ${cluster.name}.log 主日志文件，记录集群运行的核心事件   ${cluster.name}_server.json JSON 格式日志，便于日志采集工具（如 Filebeat）解析   ${cluster.name}_deprecation.log API 弃用警告日志，记录使用了即将移除的 API   ${cluster.name}_slowlog.log 慢查询和慢索引日志   gc.log JVM 垃圾回收日志（由 JVM 参数控制）     log4j2.properties #  config/log4j2.properties 控制日志行为。Easysearch 的默认配置已经比较完善，通常无需修改。\n默认日志级别 #  默认日志级别为 INFO，在 log4j2.properties 中可以修改：\n# 根日志级别 rootLogger.level = info 为特定包设置不同级别 logger.discovery.name = org.easysearch.discovery logger.discovery.level = warn 日志级别 #\n    级别 说明     TRACE 最详细，会产生大量日志   DEBUG 调试信息   INFO 正常运行信息（默认）   WARN 警告信息   ERROR 错误信息   FATAL 致命错误    日志轮转 #  默认配置包含日志轮转，防止日志文件无限增长：\n# 按大小和日期轮转 appender.rolling.type = RollingFile appender.rolling.policies.size.size = 128MB  动态调整日志级别 #  推荐做法：通过集群设置 API 动态修改日志级别，无需修改文件、无需重启。\n开启 DEBUG 日志 #  PUT /_cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;logger.org.easysearch.discovery\u0026#34;: \u0026#34;DEBUG\u0026#34; } } 恢复默认级别 #  PUT /_cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;logger.org.easysearch.discovery\u0026#34;: null } } 常用 Logger 名称 #     Logger 用途     org.easysearch.discovery 集群发现与主节点选举   org.easysearch.cluster.service 集群状态更新   org.easysearch.index 索引操作   org.easysearch.gateway 分片恢复   org.easysearch.transport 节点间通信   org.easysearch.action API 请求处理   org.easysearch.indices.recovery 分片恢复详情     慢日志 #  慢日志帮助发现性能问题，记录执行时间超过阈值的搜索和索引操作。\n慢日志配置 #  慢日志是索引级别的设置，可以通过 API 动态修改：\nPUT /my-index/_settings { \u0026#34;index.search.slowlog.threshold.query.warn\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.info\u0026#34;: \u0026#34;5s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.debug\u0026#34;: \u0026#34;2s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.trace\u0026#34;: \u0026#34;500ms\u0026#34;, \u0026quot;index.search.slowlog.threshold.fetch.warn\u0026quot;: \u0026quot;1s\u0026quot;, \u0026quot;index.search.slowlog.threshold.fetch.info\u0026quot;: \u0026quot;800ms\u0026quot;, \u0026quot;index.search.slowlog.threshold.fetch.debug\u0026quot;: \u0026quot;500ms\u0026quot;, \u0026quot;index.search.slowlog.threshold.fetch.trace\u0026quot;: \u0026quot;200ms\u0026quot;,\n\u0026quot;index.indexing.slowlog.threshold.index.warn\u0026quot;: \u0026quot;10s\u0026quot;, \u0026quot;index.indexing.slowlog.threshold.index.info\u0026quot;: \u0026quot;5s\u0026quot;, \u0026quot;index.indexing.slowlog.threshold.index.debug\u0026quot;: \u0026quot;2s\u0026quot;, \u0026quot;index.indexing.slowlog.threshold.index.trace\u0026quot;: \u0026quot;500ms\u0026quot; } 慢日志参数说明 #\n    参数前缀 说明     index.search.slowlog.threshold.query.* 搜索查询阶段（query phase）的慢日志阈值   index.search.slowlog.threshold.fetch.* 搜索获取阶段（fetch phase）的慢日志阈值   index.indexing.slowlog.threshold.index.* 索引写入操作的慢日志阈值    每种类型支持 4 个级别：warn、info、debug、trace。只有超过对应阈值的操作才会以该级别记录到慢日志。\n为所有索引设置慢日志 #  使用索引模板为所有新建索引自动配置慢日志：\nPUT /_template/slowlog_template { \u0026#34;index_patterns\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;settings\u0026#34;: { \u0026#34;index.search.slowlog.threshold.query.warn\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.info\u0026#34;: \u0026#34;5s\u0026#34;, \u0026#34;index.indexing.slowlog.threshold.index.warn\u0026#34;: \u0026#34;10s\u0026#34; } } 慢日志输出位置 #  慢日志输出到 ${path.logs}/${cluster.name}_slowlog.log。\n 弃用日志 #  弃用日志记录你使用的即将被移除的 API 或功能。升级前应重点检查这些日志。\n 默认输出到 ${cluster.name}_deprecation.log。 也可通过 API 查看：  GET /_cluster/settings?flat_settings=true\u0026amp;filter_path=**.deprecation  日志排查最佳实践 #   平时保持 INFO 级别，避免日志量过大。 排查问题时通过 API 临时开启 DEBUG，排查完后立即关闭。 保持 GC 日志开启（jvm.options 中默认已配置），便于事后分析性能问题。 配置日志采集：使用 Filebeat 将日志发送到独立的监控集群。 定期清理：确保日志目录有足够空间，检查日志轮转是否正常。   延伸阅读 #    Log4j2 日志配置 — log4j2.properties 文件配置细节  JVM 配置 — GC 日志参数  路径配置 — path.logs 日志目录  集群配置 — 动态日志级别调整  ","subcategory":null,"summary":"","tags":null,"title":"日志配置","url":"/easysearch/main/docs/deployment/config/node-settings/logging/"},{"category":null,"content":"文档大小元数据字段（_size） #  _size 元数据字段记录每个文档的 _source 字段的原始未压缩大小（字节数）。启用后，可以按文档大小进行过滤、排序和聚合。\n前置条件 #  _size 字段由 mapper-size 插件提供，需要确认插件已安装：\nbin/easysearch-plugin list # 应包含 mapper-size 启用 _size 字段 #  _size 字段默认未启用，需要在索引映射中显式开启：\nPUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;_size\u0026#34;: { \u0026#34;enabled\u0026#34;: true } } } 使用示例 #  索引文档 #  启用 _size 后，索引文档时会自动计算并存储 _source 的大小：\nPUT my_index/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;示例文档\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;这是一个示例文档，用于演示 _size 字段。\u0026#34; } 查询文档大小 #  GET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;fields\u0026#34;: [\u0026#34;_size\u0026#34;], \u0026#34;_source\u0026#34;: false } 按大小过滤 #  查找大于 1KB 的文档：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;_size\u0026#34;: { \u0026#34;gt\u0026#34;: 1024 } } } } 按大小排序 #  GET my_index/_search { \u0026#34;sort\u0026#34;: [ { \u0026#34;_size\u0026#34;: \u0026#34;desc\u0026#34; } ] } 统计文档大小分布 #  GET my_index/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;size_stats\u0026#34;: { \u0026#34;stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;_size\u0026#34; } }, \u0026#34;size_distribution\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;_size\u0026#34;, \u0026#34;interval\u0026#34;: 1024 } } } } 适用场景 #   监控文档大小：识别异常大的文档，防止单个文档过大影响性能 容量规划：分析文档大小分布，预估存储需求 数据质量检查：过滤过小（可能为空或不完整）或过大（可能有问题）的文档 成本优化：按文档大小排序，找出占用存储最多的文档  注意事项 #   _size 存储的是 _source 的原始字节大小，不包括 Lucene 索引结构的大小 启用 _size 会略微增加索引体积（每个文档多存储一个整数字段） 只能在创建索引时或通过更新映射启用，不会对已有文档回填大小值  ","subcategory":null,"summary":"","tags":null,"title":"文档大小元数据字段（_size）","url":"/easysearch/main/docs/features/mapping-and-analysis/metadata-field/size/"},{"category":null,"content":"历史版本 #  Easysearch Operator 的历史版本信息请参考 GitHub Releases 页面。\n   版本 发布时间 说明     最新版本 — 查看 GitHub Releases    ","subcategory":null,"summary":"","tags":null,"title":"历史版本","url":"/easysearch/main/docs/deployment/install-guide/operator/history_version/"},{"category":null,"content":"分布式集群安装 #  本文档介绍如何使用 initialize-cluster.sh 脚本快速搭建 Easysearch 分布式集群。该脚本支持伪分布式（单机多节点）和真分布式（多服务器）两种部署模式。\n 要求 Easysearch 版本 \u0026gt;= 2.0.3。\n 前置要求 #   已完成 系统调优 JDK 21 或更高版本（脚本可自动下载） 已安装 OpenSSL 真分布式部署需要：  服务器间网络互通 已配置 SSH 互信（无密码登录） 目标路径有写入权限    快速开始 #  交互式安装（推荐） #  交互式模式会引导您输入集群配置信息：\ncd /data/easysearch bin/initialize-cluster.sh 脚本会依次提示您输入：\n 集群名称：默认为 easysearch-cluster 证书域名：默认为 infini.cloud 节点数量：默认为 3 每个节点的配置：IP 地址、HTTP 端口、节点名称、节点角色 协议选择：HTTP 或 HTTPS（默认 HTTPS） API 兼容性：是否启用 Elasticsearch API 兼容模式 证书信息：国家、省份、城市、组织、有效期等（可选）  输入示例：\n====================================== Easysearch Cluster Initialization ====================================== Enter cluster name [easysearch-cluster]: my-cluster Enter domain for certificates [infini.cloud]: infinilabs.com How many nodes in the cluster [3]: 3\nConfigure 3 cluster node(s):\nNode 1: IP address [127.0.0.1]: 192.168.1.10 HTTP port [9200]: 9200 Node name [node1]: master1 ✓ Added: master1 @ 192.168.1.10:9200\nNode 2: IP address [127.0.0.1]: 192.168.1.11 HTTP port [9201]: 9200 Node name [node2]: master2 ✓ Added: master2 @ 192.168.1.11:9200\nNode 3: IP address [127.0.0.1]: 192.168.1.12 HTTP port [9202]: 9200 Node name [node3]: data1 ✓ Added: data1 @ 192.168.1.12:9200 \n提示：所有提示项都可以直接回车使用默认值。\n 静默安装 #  本地伪分布式（3 节点） #  在同一台机器上运行 3 个完整的 Easysearch 实例，自动分配不同端口：\ncd /data/easysearch bin/initialize-cluster.sh -s 这将创建 /data/cluster/3nodes/ 目录，包含：\n easysearch1: 127.0.0.1:9200 (transport: 9300) easysearch2: 127.0.0.1:9201 (transport: 9301) easysearch3: 127.0.0.1:9202 (transport: 9302)  每个实例都是完整的 Easysearch 副本（包含 bin、lib、config、plugins 等）。\n真实分布式（3 台服务器） #  为 3 台服务器创建集群配置，并自动通过 SSH 分发：\ncd /data/easysearch bin/initialize-cluster.sh -s \\  -c prod-cluster \\  -i 192.168.64.3,192.168.64.4,192.168.64.5 脚本会自动：\n 生成每个节点的完整配置和证书 通过 SSH 将节点包复制到对应服务器的 /data/easysearch 自动解压并配置   前提：当前主机已与目标服务器建立 SSH 互信。\n 自定义节点配置 #  精确控制每个节点的配置：\ncd /data/easysearch bin/initialize-cluster.sh -s \\  -c my-cluster \\  -n 192.168.1.10:9200:master1:master,data,ingest \\  -n 192.168.1.11:9200:master2:master,data,ingest \\  -n 192.168.1.12:9200:data1:data,ingest 节点配置格式：IP:HTTP_PORT:NODE_NAME:ROLES\n自定义证书信息 #  指定证书的组织信息（示例使用通用占位符）：\ncd /data/easysearch bin/initialize-cluster.sh -s \\  -i 192.168.1.10,192.168.1.11,192.168.1.12 \\  --cert-country IN \\  --cert-state FI \\  --cert-locality NI \\  --cert-org \u0026#34;ORG\u0026#34; \\  --cert-ou \u0026#34;UNIT\u0026#34; 生成的证书 DN 将是：\nCN=node1.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN  注意：默认使用通用占位符（IN/FI/NI）以避免暴露真实地理位置信息。生产环境可根据实际需求自定义。\n 自定义证书有效期：\n# 生成有效期为 20 年的证书 bin/initialize-cluster.sh -s \\  -i 192.168.1.10,192.168.1.11,192.168.1.12 \\  --cert-days 7300 启用 Elasticsearch API 兼容性 #  cd /data/easysearch bin/initialize-cluster.sh -s \\  -i 192.168.1.10,192.168.1.11,192.168.1.12 \\  --es-compat \\  --es-version 8.9.0 将在配置中添加：\nelasticsearch.api_compatibility: true elasticsearch.api_compatibility_version: \u0026#34;8.9.0\u0026#34; 使用 HTTP 协议（不启用 SSL） #  cd /data/easysearch bin/initialize-cluster.sh -s \\  -i 192.168.1.10,192.168.1.11,192.168.1.12 \\  --protocol http TLCP 集群初始化（当前脚本） #   本节基于当前 bin/initialize-cluster.sh 的 --tlcp 实现。若与本文其他旧示例冲突，以本节为准。\n TLCP 常用参数 #   --tlcp：启用 TLCP 双证书初始化 --nodes \u0026lt;COUNT\u0026gt;：快速本地多节点（127.0.0.1） --ips、--ports、--names：显式指定节点 IP/HTTP 端口/节点名 -n, --cluster-name：集群名 -d, --domain：证书域名 --cert-days、--cert-subject：证书有效期与 Subject --enable-es-compat \u0026lt;VERSION\u0026gt;：开启 ES 兼容模式 -s, --silent、-y, --yes：静默/跳过确认  TLCP 快速示例 #  # 2 节点本地 TLCP 集群 cd /data/easysearch bin/initialize-cluster.sh --nodes 2 --tlcp -y 默认会生成目录：\n/data/easysearch/../2nodes # 3 节点本地 TLCP 集群（显式节点信息） cd /data/easysearch bin/initialize-cluster.sh --tlcp \\  --ips 127.0.0.1,127.0.0.1,127.0.0.1 \\  --ports 9200,9201,9202 \\  --names node1,node2,node3 \\  -n tlcp-local \\  -y # 2 节点真分布式 TLCP 集群（产物目录：../cluster-\u0026lt;cluster-name\u0026gt;） cd /data/easysearch bin/initialize-cluster.sh --tlcp \\  -n prod-tlcp \\  --ips 192.168.64.3,192.168.64.4 \\  --ports 9200,9200 \\  --names node1,node2 \\  -y TLCP 产物与启动验证 #  每个节点 config/ 下会包含 TLCP 相关证书：\n instance_sign.crt / instance_sign.key instance_enc.crt / instance_enc.key client_sign.crt / client_sign.key client_enc.crt / client_enc.key ca_chain.crt admin.crt / admin.key  本地伪分布式可直接启动：\ncd /data/easysearch/../2nodes ./start-all.sh 验证集群：\ncd easysearch1 bin/tlcp-curl.sh --url \u0026#39;https://127.0.0.1:9200/_cluster/health?pretty\u0026#39; -u \u0026#39;admin:YOUR_PASSWORD\u0026#39; bin/tlcp-curl.sh --url \u0026#39;https://127.0.0.1:9200/_cat/nodes?v\u0026#39; -u \u0026#39;admin:YOUR_PASSWORD\u0026#39; 默认情况下（security.ssl.http.clientauth_mode 未设置为 REQUIRE），HTTP 访问只需用户名密码，无需客户端证书。\n如需显式双证书访问：\nbin/tlcp-curl.sh --url \u0026#39;https://127.0.0.1:9200/_security/sslinfo?pretty\u0026#39; \\  -u \u0026#39;admin:YOUR_PASSWORD\u0026#39; \\  --cert config/client_sign.crt --key config/client_sign.key \\  --enc-cert config/client_enc.crt --enc-key config/client_enc.key \\  --ca config/ca_chain.crt 使用 admin.crt / admin.key 访问：\nbin/tlcp-curl.sh --url \u0026#39;https://127.0.0.1:9200/_cluster/health?pretty\u0026#39; \\  -u \u0026#39;admin:YOUR_PASSWORD\u0026#39; \\  --cert config/admin.crt \\  --key config/admin.key  仅当服务端配置 security.ssl.http.clientauth_mode: REQUIRE 时，上述证书访问方式才是必需的。\n TLCP 注意事项 #   --tlcp 与 --http 互斥，不能同时使用 Linux 下生成 TLCP 证书要求系统安装 tongsuo 命令（宿主机安装步骤见《国密配置》中的“Linux 安装 Tongsuo（宿主机）”） start-all.sh / stop-all.sh 仅在伪分布式模式生成  命令行参数 #     参数 说明 示例     -h, --help 显示帮助信息 --help   -s, --silent 静默模式，使用默认值 -s   -y, --yes 跳过确认提示 -y   -c, --cluster-name 集群名称 -c prod-cluster   -d, --domain 证书域名 -d example.com   -n, --node 添加节点（可多次使用）\n格式：IP:HTTP_PORT:NODE_NAME:ROLES -n 192.168.1.10:9200:node1:master,data   -i, --ips IP 列表（逗号分隔）\n自动生成端口和节点名 -i 192.168.1.10,192.168.1.11   --http-port-start HTTP 起始端口 --http-port-start 9200   --transport-port-start Transport 起始端口 --transport-port-start 9300   --protocol HTTP 协议（http 或 https） --protocol https   --cert-country 证书国家代码 --cert-country CN   --cert-state 证书州/省 --cert-state Hunan   --cert-locality 证书城市 --cert-locality Changsha   --cert-org 证书组织 --cert-org \u0026quot;INFINI Ltd\u0026quot;   --cert-ou 证书组织单位 --cert-ou \u0026quot;Engineering\u0026quot;   --cert-days 证书有效期（天） --cert-days 7300 (20年)   --es-compat 启用 Elasticsearch API 兼容性 --es-compat   --es-version Elasticsearch 兼容版本 --es-version 8.9.0    生成的目录结构 #  伪分布式模式 #  脚本会在当前 Easysearch 的父目录创建集群目录：\n/data/cluster/ └── 3nodes/ # 节点数量命名 ├── easysearch1/ # 完整的 Easysearch 实例 1 │ ├── bin/ │ ├── lib/ │ ├── modules/ │ ├── plugins/ # 已安装插件 │ ├── jdk/ # JDK（如果下载） │ ├── config/ │ │ ├── easysearch.yml # 节点 1 配置 │ │ ├── jvm.options │ │ ├── log4j2.properties │ │ ├── instance.crt # 节点 1 证书 │ │ ├── instance.key │ │ ├── ca.crt │ │ ├── admin.crt │ │ ├── admin.key │ │ └── security/ │ │ └── user.yml # 加密后的密码 │ ├── data/ │ └── logs/ ├── easysearch2/ # 完整的 Easysearch 实例 2 │ └── ... ├── easysearch3/ # 完整的 Easysearch 实例 3 │ └── ... ├── start-all.sh # 启动所有节点 ├── stop-all.sh # 停止所有节点 └── README.md # 集群信息（含密码） 真分布式模式 #  脚本会创建独立的节点包，并自动分发到各服务器：\n/data/cluster/ └── cluster-easysearch-cluster/ # 集群配置目录 ├── tmp/ # 临时目录（用于生成证书） ├── 192-168-64-3/ # 节点 1 包（待分发） │ ├── bin/ │ ├── lib/ │ ├── config/ │ │ ├── easysearch.yml # network.host: 192.168.64.3 │ │ └── ... │ └── ... ├── 192-168-64-4/ # 节点 2 包 │ └── ... ├── 192-168-64-5/ # 节点 3 包 │ └── ... ├── 192-168-64-3.tar.gz # 压缩包（用于手动分发） ├── 192-168-64-4.tar.gz ├── 192-168-64-5.tar.gz └── README.md # 部署说明 分发到目标服务器后的结构（每台服务器）：\n/data/easysearch/ # 统一目录名 ├── bin/ ├── lib/ ├── config/ │ ├── easysearch.yml # 包含对应 IP 的配置 │ └── ... ├── data/ ├── logs/ └── pid # 启动后生成 部署方式 #  方式一：本地伪分布式 #  适合开发测试环境，所有节点运行在同一台机器上。\n 生成集群  cd /data/easysearch bin/initialize-cluster.sh -s 启动所有节点  cd /data/cluster/3nodes ./start-all.sh 验证集群  # 获取管理员密码 cat README.md | grep \u0026#34;Admin Password\u0026#34; # 检查集群健康状态 curl -ku 'admin:YOUR_PASSWORD' \u0026quot;https://127.0.0.1:9200/_cluster/health?pretty\u0026quot;\n# 查看节点列表 curl -ku 'admin:YOUR_PASSWORD' \u0026quot;https://127.0.0.1:9200/_cat/nodes?v\u0026quot; 输出示例：\nip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name 127.0.0.1 45 68 8 0.52 0.38 0.35 dimr * node1 127.0.0.1 38 68 8 0.52 0.38 0.35 dimr - node2 127.0.0.1 42 68 8 0.52 0.38 0.35 dimr - node3 停止集群  cd /data/cluster/3nodes ./stop-all.sh  说明：stop-all.sh 脚本会读取每个节点目录下的 pid 文件来优雅地停止进程。\n 方式二：真实分布式部署 #  适合生产环境，节点分布在多台服务器上。脚本支持自动通过 SSH 分发。\n自动部署（推荐） #  前提条件：\n 当前主机已与目标服务器建立 SSH 互信 目标服务器的 /data 目录有写入权限  步骤 1：生成并自动部署\ncd /data/easysearch bin/initialize-cluster.sh -s \\  -c prod-cluster \\  -i 192.168.64.3,192.168.64.4,192.168.64.5 脚本会自动：\n 生成所有节点的配置和证书 为每个节点创建完整的 Easysearch 实例 打包并通过 SSH 复制到对应服务器 在远程服务器上自动解压到 /data/easysearch  步骤 2：在各服务器上启动节点\n在每台服务器上执行：\ncd /data/easysearch bin/easysearch -d -p pid 或者在管理节点上通过 SSH 批量启动：\nfor ip in 192.168.64.3 192.168.64.4 192.168.64.5; do ssh $ip \u0026#34;cd /data/easysearch \u0026amp;\u0026amp; bin/easysearch -d -p pid\u0026#34; done 手动部署 #  如果 SSH 自动部署失败或需要手动控制，可以使用手动方式。\n步骤 1：生成集群配置\ncd /data/easysearch bin/initialize-cluster.sh -s \\  -c prod-cluster \\  -i 192.168.64.3,192.168.64.4,192.168.64.5 当 SSH 自动部署失败时，脚本会生成压缩包供手动分发。\n步骤 2：手动分发\n将生成的压缩包复制到对应服务器：\ncd /data/cluster/cluster-prod-cluster # 复制到服务器 1 scp 192-168-64-3.tar.gz root@192.168.64.3:/tmp/\n# 复制到服务器 2 scp 192-168-64-4.tar.gz root@192.168.64.4:/tmp/\n# 复制到服务器 3 scp 192-168-64-5.tar.gz root@192.168.64.5:/tmp/ 步骤 3：在各服务器上解压\n在每台服务器上执行：\ncd /tmp tar -xzf 192-168-64-3.tar.gz mv 192-168-64-3 /data/easysearch chown -R easysearch:easysearch /data/easysearch 步骤 4：启动节点\ncd /data/easysearch su easysearch -c \u0026#34;bin/easysearch -d -p pid\u0026#34; 验证集群 #  在任意节点上验证集群状态：\n# 获取管理员密码（在生成配置的服务器上） cat /data/cluster/cluster-prod-cluster/README.md | grep \u0026#34;Admin Password\u0026#34; # 检查集群健康状态 curl -ku 'admin:YOUR_PASSWORD' \u0026quot;https://192.168.64.3:9200/_cluster/health?pretty\u0026quot;\n# 查看所有节点 curl -ku 'admin:YOUR_PASSWORD' \u0026quot;https://192.168.64.3:9200/_cat/nodes?v\u0026quot; 停止节点 #\n 在每台服务器上执行：\nkill $(cat /data/easysearch/pid) 或通过 SSH 批量停止：\nfor ip in 192.168.64.3 192.168.64.4 192.168.64.5; do ssh $ip \u0026#34;kill \\$(cat /data/easysearch/pid)\u0026#34; done 节点配置说明 #  每个节点的 config/easysearch.yml 包含以下关键配置：\n# 集群名称（所有节点相同） cluster.name: my-cluster # 节点名称（每个节点不同） node.name: node1 node.roles: [ master, data, ingest, remote_cluster_client ]\n# 数据和日志路径 path.data: data path.logs: logs\n# 网络配置（每个节点的 IP 不同） network.host: 192.168.64.3 http.port: 9200 transport.port: 9300\n# 集群发现（所有节点相同） discovery.seed_hosts: [\u0026quot;192.168.64.3:9300\u0026quot;, \u0026quot;192.168.64.4:9300\u0026quot;, \u0026quot;192.168.64.5:9300\u0026quot;] cluster.initial_master_nodes: [\u0026quot;node1\u0026quot;, \u0026quot;node2\u0026quot;, \u0026quot;node3\u0026quot;]\n# 安全配置（所有节点相同） security.enabled: true security.ssl.http.enabled: true security.ssl.http.cert_file: instance.crt security.ssl.http.key_file: instance.key security.ssl.http.ca_file: ca.crt security.ssl.transport.enabled: true security.ssl.transport.cert_file: instance.crt security.ssl.transport.key_file: instance.key security.ssl.transport.ca_file: ca.crt\n# 节点认证（所有节点相同） security.nodes_dn:\n \u0026quot;CN=node1.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026quot; \u0026quot;CN=node2.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026quot; \u0026quot;CN=node3.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026quot;  # 管理员认证（所有节点相同） security.authcz.admin_dn:\n \u0026quot;CN=admin.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026quot;   关键配置说明：\n 每个节点通过 network.host 和 node.name 进行区分 每个节点使用独特的证书 CN（如 CN=node1.infini.cloud） security.nodes_dn 列出所有允许加入集群的节点证书 DN security.authcz.admin_dn 定义管理员证书 DN path.data 和 path.logs 使用统一的相对路径 所有节点使用相同的集群发现配置   证书和节点认证 #  证书设计 #  脚本为集群中的每个节点生成独立的证书，使用不同的 CN（Common Name）：\n默认证书信息（可通过命令行参数自定义）：\n 国家 (C)：IN 州/省 (ST)：FI 城市 (L)：NI 组织 (O)：ORG 组织单位 (OU)：UNIT 域名：infini.cloud 有效期：3650 天（10 年）  生成的证书 DN：\n 节点 1 证书：CN=node1.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN 节点 2 证书：CN=node2.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN 节点 3 证书：CN=node3.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN 管理员证书：CN=admin.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN  自定义证书信息：\n使用命令行参数自定义证书的组织信息和有效期：\nbin/initialize-cluster.sh -s \\  -i 192.168.1.10,192.168.1.11 \\  --cert-country IN \\  --cert-state FI \\  --cert-locality NI \\  --cert-org \u0026#34;ORG\u0026#34; \\  --cert-ou \u0026#34;UNIT\u0026#34; \\  --cert-days 7300 \\  --domain infini.cloud 将生成有效期为 20 年的证书：\nCN=node1.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN 节点互信配置 #  通过 security.nodes_dn 配置，明确列出所有允许加入集群的节点证书 DN。这样可以：\n 提高安全性：只有证书 DN 在白名单中的节点才能加入集群 防止恶意节点：即使有人获得了 CA 签发的证书，如果 DN 不在白名单中也无法加入 灵活扩展：添加新节点时，需要在所有节点配置中添加新节点的 DN  查看证书信息 #  # 查看节点证书的 CN openssl x509 -in cluster-config/my-cluster/certs/node1.crt -noout -subject # 查看证书的有效期 openssl x509 -in cluster-config/my-cluster/certs/node1.crt -noout -dates\n# 查看证书的 SAN（Subject Alternative Names） openssl x509 -in cluster-config/my-cluster/certs/node1.crt -noout -text | grep -A1 \u0026quot;Subject Alternative Name\u0026quot;\n# 查看证书的完整信息 openssl x509 -in cluster-config/my-cluster/certs/node1.crt -noout -text 示例输出：\n# 查看主题 $ openssl x509 -in cluster-config/my-cluster/certs/node1.crt -noout -subject subject=C = IN, ST = FI, L = NI, O = ORG, OU = UNIT, CN = node1.infini.cloud # 查看有效期 $ openssl x509 -in cluster-config/my-cluster/certs/node1.crt -noout -dates notBefore=Jan 02 06:00:00 2026 GMT notAfter=Dec 30 06:00:00 2035 GMT 配置为系统服务 #\n  注意：配置系统服务主要针对生产环境的真实分布式部署。本地伪分布式测试环境直接使用 start-all.sh 脚本即可。\n 在每台生产服务器上创建 systemd 服务文件，以便系统启动时自动运行。所有服务器使用相同的服务配置，通过 config/easysearch.yml 中的 network.host 和 node.name 来区分不同节点。\n在每台服务器上创建服务文件 #  每台服务器都使用相同的服务配置（/usr/lib/systemd/system/easysearch.service）：\nsudo tee /usr/lib/systemd/system/easysearch.service \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; [Unit] Description=Easysearch Documentation=https://www.infinilabs.com After=network.target [Service] Type=forking User=easysearch WorkingDirectory=/data/easysearch Environment=\u0026#34;ES_PATH_CONF=/data/easysearch/config\u0026#34; PIDFile=/data/easysearch/pid ExecStart=/data/easysearch/bin/easysearch -d -p pid PrivateTmp=true LimitNOFILE=65536 LimitNPROC=65536 LimitAS=infinity LimitFSIZE=infinity LimitMEMLOCK=infinity TimeoutStopSec=0 KillSignal=SIGTERM KillMode=process SendSIGKILL=no SuccessExitStatus=143 [Install] WantedBy=multi-user.target EOF  说明：\n 所有服务器使用统一的服务名：easysearch.service 工作目录统一为：/data/easysearch 节点通过配置文件 /data/easysearch/config/easysearch.yml 中的 IP 和节点名区分 不需要使用 node1、node2 等子目录   启用并启动服务 #  在每台服务器上执行：\n# 重新加载服务配置 sudo systemctl daemon-reload # 启用开机自启 sudo systemctl enable easysearch\n# 启动服务 sudo systemctl start easysearch\n# 查看状态 sudo systemctl status easysearch 常见问题 #\n 1. 节点无法加入集群 #  现象：节点启动正常，但集群中只显示一个节点。\n排查步骤：\n 检查网络连通性：telnet 192.168.1.10 9300 检查防火墙：确保 HTTP 端口和 Transport 端口已开放 检查日志：查看 logs/easysearch.log 中的错误信息 检查集群名称：确保所有节点的 cluster.name 相同  2. 证书验证失败 #  现象：节点日志中出现证书错误。\n解决方案：\n 确保所有节点使用相同的 CA 证书（ca.crt） 检查证书中的 SAN 是否包含节点 IP 如果是跨网络部署，确保证书中包含所有可能的 IP 地址  3. 端口冲突 #  现象：节点启动失败，提示端口被占用。\n解决方案：\n 检查端口占用：netstat -tuln | grep 9200 使用 --http-port-start 和 --transport-port-start 指定不同的端口范围  4. 内存不足 #  现象：节点频繁重启或响应缓慢。\n解决方案：\n 调整 JVM 内存：编辑 config/jvm.options 建议设置为物理内存的 50%，但不超过 32GB  # 例如设置为 4GB -Xms4g -Xmx4g 生产环境最佳实践 #    节点规划\n 至少 3 个 master 节点（奇数个） 根据数据量规划 data 节点数量 考虑独立的 coordinating 节点处理查询    硬件配置\n CPU：8 核及以上 内存：32GB 及以上 磁盘：SSD，至少 500GB 网络：千兆网卡    安全配置\n 修改默认管理员密码 配置防火墙规则 定期更新证书 启用审计日志    监控和备份\n 使用 INFINI Console 进行集群监控 配置快照和恢复策略 定期备份配置文件和证书    性能优化\n 根据负载调整刷新间隔 合理设置分片和副本数量 使用 SSD 存储 禁用 swap    更多资源 #    系统调优  节点配置  Docker Compose 集群部署  Kubernetes 部署  INFINI Console 文档  总结 #  使用 initialize-cluster.sh 脚本可以快速搭建 Easysearch 分布式集群，支持：\n ✅ 交互式和静默两种安装模式 ✅ 伪分布式和真分布式部署 ✅ 自动生成节点配置和证书 ✅ 独立的数据和日志目录 ✅ 便捷的集群管理脚本  对于生产环境，建议使用真实分布式部署，并配置为系统服务以确保高可用性。\n","subcategory":null,"summary":"","tags":null,"title":"分布式集群","url":"/easysearch/main/docs/deployment/install-guide/cluster/"},{"category":null,"content":"哈希字段类型（Murmur3） #  murmur3 字段类型在索引时计算字段值的 MurmurHash3 128 位哈希值，并将哈希值存储为 doc values。这在 cardinality 聚合中可以提供更高的精度，因为直接使用预计算的哈希值而非运行时计算。\n前置条件 #  murmur3 字段类型由 mapper-murmur3 插件提供，需要确认插件已安装：\nbin/easysearch-plugin list # 应包含 mapper-murmur3 创建映射 #  PUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;user_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;hash\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;murmur3\u0026#34; } } } } } } 使用示例 #  Cardinality 聚合优化 #  使用 murmur3 子字段进行基数统计，可以获得更高的精度：\nGET my_index/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;unique_users\u0026#34;: { \u0026#34;cardinality\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;user_id.hash\u0026#34; } } } } 限制 #   murmur3 字段不支持搜索查询（不可检索） 只能用于聚合和排序 字段值在索引时计算并存储，会增加少量索引体积 主要用途是优化 cardinality 聚合的精度  ","subcategory":null,"summary":"","tags":null,"title":"哈希字段类型（Murmur3）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/murmur3/"},{"category":null,"content":"词元计数字段类型（Token Count） #  token_count 是一个特殊的整数字段类型，它在索引时自动计算文本经过分析器处理后产生的词元（token）数量，并将该数量作为字段值存储。这使您可以根据文本的词元数量进行过滤、排序和聚合。\n适用场景 #   按内容长度过滤：过滤词元数量在指定范围内的文档（如至少 100 个词的文章） 内容质量评估：短文本可能质量较低，可按词元数量排序 分析器效果评估：了解不同分析器对同一文本产生的词元数量差异  创建映射 #  PUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;length\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;token_count\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; } } } } } } 映射参数 #     参数 必填 说明     analyzer ✅ 是 用于分析文本的分析器名称。使用该分析器产生的词元数量作为字段值   enable_position_increments 否 是否计算位置增量。默认 true。设为 false 后，停用词等位置增量不计入总数   doc_values 否 默认 true，启用 doc values 以支持排序和聚合   index 否 默认 true，是否索引该字段   null_value 否 当原始文本为 null 时使用的替代值   store 否 默认 false，是否单独存储字段值    索引文档 #  PUT my_index/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;快速入门 Easysearch 搜索引擎\u0026#34; } PUT my_index/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Easysearch\u0026quot; }\nPUT my_index/_doc/3 { \u0026quot;title\u0026quot;: \u0026quot;深入理解 Easysearch 的分布式架构设计与高可用方案\u0026quot; } 查询示例 #\n 过滤词元数量 #  查找标题至少有 3 个词元的文档：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;title.length\u0026#34;: { \u0026#34;gte\u0026#34;: 3 } } } } 按词元数量排序 #  GET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;sort\u0026#34;: [ { \u0026#34;title.length\u0026#34;: \u0026#34;desc\u0026#34; } ] } 聚合统计 #  GET my_index/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;title_length_stats\u0026#34;: { \u0026#34;stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;title.length\u0026#34; } }, \u0026#34;title_length_distribution\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;title.length\u0026#34;, \u0026#34;interval\u0026#34;: 5 } } } } enable_position_increments 参数 #  当设置 enable_position_increments: false 时，分析器产生的位置增量（如停用词被移除留下的空位）不会计入词元总数。\nPUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;word_count\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;token_count\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;english\u0026#34;, \u0026#34;enable_position_increments\u0026#34;: false } } } } } } 对于文本 \u0026quot;The quick brown fox\u0026quot;，使用 english 分析器：\n enable_position_increments: true（默认）：计数为 4（包含停用词 \u0026ldquo;the\u0026rdquo; 的位置） enable_position_increments: false：计数为 3（不包含已移除的 \u0026ldquo;the\u0026rdquo;）  使用不同分析器 #  token_count 字段的 analyzer 可以与父字段使用不同的分析器：\nPUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;cn_words\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;token_count\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; }, \u0026#34;std_tokens\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;token_count\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; } } } } } } 这样可以同时统计不同分析器的词元数量，用于分析或对比。\n","subcategory":null,"summary":"","tags":null,"title":"词元计数字段类型（Token Count）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/token-count/"},{"category":null,"content":"标注文本字段类型（Annotated Text） #  annotated_text 字段类型是 text 字段的扩展，允许在文本中嵌入标注标记（annotation tokens）。标注标记会在索引时注入到文本的词元流（token stream）中，可以像普通词语一样被搜索。\n适用场景 #   命名实体识别（NER）：将文本中识别出的实体（人名、地名、组织名等）作为标注嵌入 文本分类标记：在文本中标记特定类别或主题 知识图谱关联：将文本中的实体链接到知识图谱的节点 自定义标签注入：在索引时为文本添加额外的可搜索标签  前置条件 #  annotated_text 字段类型由 mapper-annotated-text 插件提供，需要确认插件已安装：\nbin/easysearch-plugin list # 应包含 mapper-annotated-text 创建映射 #  PUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;annotated_text\u0026#34; } } } } annotated_text 支持与 text 字段相同的映射参数，如 analyzer、search_analyzer 等。\n标注语法 #  标注使用特殊的标记语法嵌入在文本中。标注的格式为：\n[被标注的文本](标注值) 示例 #  原始文本：\n\u0026#34;昨天在北京召开了搜索技术大会\u0026#34; 带标注的文本：\n\u0026#34;昨天在[北京](LOC=北京\u0026amp;wiki=Q956)召开了[搜索技术大会](EVENT=搜索技术大会)\u0026#34; 索引带标注的文档 #  PUT my_index/_doc/1 { \u0026#34;content\u0026#34;: \u0026#34;昨天[极限实验室](ORG=极限实验室)发布了[Easysearch](PRODUCT=Easysearch) 的新版本\u0026#34; } PUT my_index/_doc/2 { \u0026quot;content\u0026quot;: \u0026quot;张三在上海参加了技术峰会\u0026quot; } 查询 #\n 搜索标注值 #  标注值作为词元注入到了索引中，可以直接搜索：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;PRODUCT=Easysearch\u0026#34; } } } 搜索原始文本 #  普通文本搜索照常工作：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;新版本\u0026#34; } } } 组合搜索 #  可以同时搜索原始文本和标注值：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;发布\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;ORG=极限实验室\u0026#34; } } ] } } } 工作原理 #  当索引包含标注的文本时，annotated_text 字段会：\n 对方括号中的被标注文本进行正常分词和索引 将圆括号中的标注值作为额外的词元注入到相同的位置（position） 标注值与被标注文本共享相同的位置偏移，因此短语查询可以同时匹配两者  例如文本 \u0026quot;[北京](LOC=北京)天气\u0026quot; 索引后的词元流：\n   位置 词元     0 北京（原始文本分词）   0 LOC=北京（标注值，同一位置）   1 天气    限制 #   需要安装 mapper-annotated-text 插件 标注语法必须严格遵循 [文本](标注) 格式 标注值不会经过分析器处理，作为完整的词元索引 大量标注会增加索引体积和索引时间  ","subcategory":null,"summary":"","tags":null,"title":"标注文本字段类型（Annotated Text）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/annotated-text/"},{"category":null,"content":"异步搜索 #  当需要对海量数据执行复杂聚合或跨大时间范围查询时，同步搜索可能因超时而失败。Easysearch 异步搜索（Async Search）允许将这类\u0026quot;大查询\u0026quot;提交到后台执行，客户端无需保持连接等待，可以随时轮询检查进度并获取部分或最终结果。\n 适用场景 #     场景 说明     深度聚合分析 对数亿条日志做多维分组聚合，耗时可能达分钟级   大跨度时间查询 查询数月甚至数年的历史数据   快照搜索 在对象存储上搜索冷数据，I/O 延迟较高   跨集群搜索 同时检索多个远程集群，网络延迟不可控   报表与导出 后台生成大规模报表，前端异步展示进度     基本用法 #  提交异步搜索 #  使用 _async_search 端点提交查询：\nPOST /my-index/_async_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;status_over_time\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;fixed_interval\u0026#34;: \u0026#34;1h\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_response\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;response_time\u0026#34; } } } } } } 返回结果中包含一个异步搜索 ID：\n{ \u0026#34;id\u0026#34;: \u0026#34;FmRGZGFhbVF...\u0026#34;, \u0026#34;is_partial\u0026#34;: true, \u0026#34;is_running\u0026#34;: true, \u0026#34;response\u0026#34;: { ... } } 查询进度 #  使用异步搜索 ID 轮询进度：\nGET /_async_search/FmRGZGFhbVF... 当 is_running 为 false 时，搜索完成。\n获取最终结果 #  搜索完成后，返回完整结果：\n{ \u0026#34;id\u0026#34;: \u0026#34;FmRGZGFhbVF...\u0026#34;, \u0026#34;is_partial\u0026#34;: false, \u0026#34;is_running\u0026#34;: false, \u0026#34;response\u0026#34;: { \u0026#34;took\u0026#34;: 45230, \u0026#34;hits\u0026#34;: { ... }, \u0026#34;aggregations\u0026#34;: { ... } } } 删除异步搜索 #  不再需要的异步搜索任务可以手动删除以释放资源：\nDELETE /_async_search/FmRGZGFhbVF...  关键参数 #     参数 说明 默认值     wait_for_completion_timeout 初次提交时等待同步返回的时间，超时后转为异步 1s   keep_on_completion 查询完成后是否保留结果 false   keep_alive 异步搜索结果的保留时间 5d    示例——提交时最多等待 5 秒，完成后保留结果 1 天：\nPOST /my-index/_async_search?wait_for_completion_timeout=5s\u0026amp;keep_on_completion=true\u0026amp;keep_alive=1d { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } }  与快照搜索结合 #  异步搜索与 快照搜索 天然互补：\n 快照搜索面向大时间跨度和冷数据，I/O 延迟较高 通过异步搜索执行，避免长时间查询阻塞系统 特别适合审计型、合规型的历史数据分析任务  POST /my-snapshot-index/_async_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2024-12-31\u0026#34; } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;monthly_stats\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; } } } }  相关文档 #    快照搜索  跨集群搜索   API 参考 #   以下为异步搜索 REST API 的完整参数与响应字段说明。引入版本 1.11.0。\n 提交异步搜索 #  POST {index}/_async_search 请求参数\n   参数 说明 默认值 必填     wait_for_completion_timeout 等待同步返回的时间，超时后转为异步。最大 300 秒 1s 否   keep_on_completion 搜索完成后是否保留结果 false 否   keep_alive 结果保留时间（含查询执行时间），超时后自动删除 12h 否   index 搜索的索引名称，支持逗号分隔或通配符 所有索引 否    请求示例\nPOST test-index/_async_search?wait_for_completion_timeout=1ms\u0026amp;keep_on_completion=true { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34; } } } 响应示例\n{ \u0026#34;id\u0026#34;: \u0026#34;FmFqN0llTXlKVHF5...\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1740714470020, \u0026#34;expiration_time_in_millis\u0026#34;: 1740800870020, \u0026#34;response\u0026#34;: { \u0026#34;took\u0026#34;: 0, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 0, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] } } } 响应参数\n   参数 说明     id 异步搜索 ID，用于轮询进度、获取结果或删除。如果在超时内完成则不返回   state 搜索状态：RUNNING、SUCCEEDED、FAILED、PERSISTING、PERSIST_SUCCEEDED、PERSIST_FAILED、CLOSED、STORE_RESIDENT   start_time_in_millis 开始时间（毫秒）   expiration_time_in_millis 过期时间（毫秒）   took 搜索总耗时   response 搜索响应体   num_reduce_phases 协调节点聚合分片结果的次数（默认 5），数值增加表示有新结果    获取部分结果 #  GET _async_search/\u0026lt;ID\u0026gt; state 变为 STORE_RESIDENT 表示结果已成功持久化。可使用 wait_for_completion_timeout 参数轮询等待。\n删除异步搜索 #  DELETE _async_search/\u0026lt;ID\u0026gt;  搜索进行中：取消搜索 搜索已完成：删除保存的结果  监控统计信息 #  GET _async_search/stats 统计字段\n   参数 说明     submitted 已提交的请求数   initialized 已初始化的请求数   running_current 当前运行中的请求数   search_completed 成功完成的请求数   search_failed 失败的请求数   persisted 成功持久化的请求数   persist_failed 持久化失败的请求数   rejected 被拒绝的请求数   cancelled 被取消的请求数    集群设置 #  异步搜索相关设置为动态设置，无需重启集群：\nPUT _cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;async_search.max_wait_for_completion_timeout\u0026#34;: \u0026#34;5m\u0026#34; } }    设置 默认值 说明     async_search.max_search_running_time 12h 搜索最长运行时间，超时自动终止   async_search.node_concurrent_running_searches 20 每节点同时运行的异步搜索数   async_search.max_keep_alive 5d 结果最长保留时间   async_search.max_wait_for_completion_timeout 1m wait_for_completion_timeout 最大值   async_search.persist_search_failures false 是否持久化搜索失败的结果    ","subcategory":null,"summary":"","tags":null,"title":"异步搜索","url":"/easysearch/main/docs/features/async-search/"},{"category":null,"content":"Rules 规则引擎 #  Rules 插件是 EasySearch 的规则匹配引擎，可在数据写入时自动匹配规则库，为文档添加标签字段，实现高效的内容分类、审核和标注。\n功能特性 #   ✅ 实时匹配：数据写入时自动匹配规则，无需后处理 ✅ 高性能：基于优化的 C++ 规则引擎，支持复杂规则和大规则库 ✅ 灵活配置：支持自定义字段、正则表达式、数值范围匹配 ✅ 多规则匹配：一条文档可匹配多个规则，所有标签保留到数组中 ✅ 集群广播：多节点环境自动编译规则到所有 Ingest 节点   基本概念 #  规则库 (Repository) #  规则库是规则的集合，每个规则库有唯一的 repo_id，存储在 .match_rules 索引中。\n规则 (Rule) #  规则由表达式和描述组成：\nexpression\\t#offset#description  expression: 匹配表达式（支持 AND/OR/NOT、正则、范围等） offset: 规则序号（从 0 开始，系统自动生成，用于区分不同规则） description: 规则描述（匹配时作为标签返回）  重要说明：\n offset 是导入到规则索引后系统自动生成的，本身没有业务含义 通过 _import API 导入规则时不需要手动指定 offset 导入时只需提供 expression 和 description，系统会自动分配 offset  标签 (Tag) #  文档匹配规则后，规则的 #offset#description 会添加到文档的 tags 字段。\n 环境要求 #  节点配置 #  Rules 插件依赖 native 库（C++ 编译的规则引擎），需要在 config/easysearch.yml 中禁用系统调用过滤器：\nbootstrap.system_call_filter: false 为什么需要这个配置？\n Rules 插件需要调用 JNI 和 native 库（libruledb-r.so） 规则编译过程会执行系统调用（如 fork、exec 等） EasySearch 默认启用的 seccomp 过滤器会阻止这些操作  注意：禁用此过滤器会降低一定的安全性，建议只在运行 rules 插件的节点上禁用。\n 快速开始 #  步骤 1: 导入规则 #  使用 POST /_match_rules/{repo_id}/_import 导入规则到索引：\nPOST /_match_rules/security_v1/_import { \u0026#34;name\u0026#34;: \u0026#34;安全规则库\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;用于安全事件分类的规则库\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;security\u0026#34;, \u0026#34;content-filter\u0026#34;], \u0026#34;rules\u0026#34;: [ { \u0026#34;expression\u0026#34;: \u0026#34;枪 or 手枪 or 步枪 or 猎枪\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;涉枪武器关键词\u0026#34; }, { \u0026#34;expression\u0026#34;: \u0026#34;AK47 or AK-47 or M16 or M4A1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;枪支型号\u0026#34; }, { \u0026#34;expression\u0026#34;: \u0026#34;海洛因 or 冰毒 or 摇头丸 or K粉\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;毒品名称\u0026#34; } ] } 响应：\n{ \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Successfully imported 3 rules to repo_id \u0026#39;security_v1\u0026#39; (version: 1)\u0026#34;, \u0026#34;repo_id\u0026#34;: \u0026#34;security_v1\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;rule_count\u0026#34;: 3 } 步骤 2: 编译规则库 #  使用 POST /_match_rules/{repo_id}/_compile 编译规则为二进制库：\nPOST /_match_rules/security_v1/_compile { \u0026#34;fields\u0026#34;: [\u0026#34;title\u0026#34;, \u0026#34;content\u0026#34;, \u0026#34;range\u0026#34;, \u0026#34;price\u0026#34;], \u0026#34;composite\u0026#34;: [\u0026#34;(title,content)\u0026#34;, \u0026#34;(price,range)\u0026#34;], \u0026#34;quiet\u0026#34;: true } 参数说明：\n   参数 类型 必需 默认值 说明     fields array ❌ [] 编译时声明的字段列表（用于字段限定与范围匹配等）   composite array ❌ [] 联合索引定义（用于多字段组合匹配，提升性能）   quiet boolean ❌ true 是否静默模式（不输出详细日志）    联合索引说明：\ncomposite 参数用于定义多字段组合索引，提升多字段 AND 条件的匹配性能：\n{ \u0026#34;fields\u0026#34;: [\u0026#34;gender\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;income\u0026#34;], \u0026#34;composite\u0026#34;: [ \u0026#34;(gender,age,income)\u0026#34;, // 基本格式 \u0026#34;s{:}(name,city,province)\u0026#34; // 带分隔符格式 ] } 格式说明：\n (field1,field2,field3): 基本格式，多个字段组合索引 s{sep}(field1,field2): 带分隔符格式，sep 为自定义分隔符（如 :）  性能优势：\n 单字段索引需要分别扫描 + 内存交集 联合索引一次扫描组合索引，性能提升 3-10 倍  推荐使用场景：\n 规则中频繁出现 field1 and field2 and field3 的组合 字段基数较小，组合后索引膨胀可控 对匹配性能要求高的场景  响应：\n{ \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Successfully compiled 3 rules\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;duration_ms\u0026#34;: 1250, \u0026#34;total_rules\u0026#34;: 3, \u0026#34;indexed_rules\u0026#34;: 0 } } 联合索引配置指南 #  当规则中频繁出现多字段 AND 组合（如 author(...) and source(...)）时，建议配置 composite。\n步骤 1：确定规则中的高频字段组合\n规则示例：\nauthor(张三) and source(新华网) author(张三) and title(告警) 步骤 2：编译时声明 fields 与 composite\nPOST /_match_rules/news_v1/_compile { \u0026#34;fields\u0026#34;: [\u0026#34;author\u0026#34;, \u0026#34;source\u0026#34;, \u0026#34;title\u0026#34;], \u0026#34;composite\u0026#34;: [ \u0026#34;(author,source)\u0026#34;, \u0026#34;(author,title)\u0026#34;, \u0026#34;s{:}(author,source,title)\u0026#34; ], \u0026#34;quiet\u0026#34;: true } 说明：\n fields：声明编译器需要处理的字段 composite：声明联合索引定义，编译时会作为 -i 参数传给编译器 s{sep}(...)：可选分隔符格式，sep 例如 :  步骤 3：验证配置已生效\nGET .match_rules/_doc/news_v1 预期 _source 包含：\n{ \u0026#34;fields\u0026#34;: [\u0026#34;author\u0026#34;, \u0026#34;source\u0026#34;, \u0026#34;title\u0026#34;], \u0026#34;composite\u0026#34;: [\u0026#34;(author,source)\u0026#34;, \u0026#34;(author,title)\u0026#34;, \u0026#34;s{:}(author,source,title)\u0026#34;] } 步骤 4：在 Pipeline 中按需设置字段白名单\nPUT _ingest/pipeline/news-check { \u0026#34;processors\u0026#34;: [ { \u0026#34;check_match_rules\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;news_v1\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;author\u0026#34;, \u0026#34;source\u0026#34;, \u0026#34;title\u0026#34;] } } ] } 常见错误\n composite 格式错误（如 author,source 缺少括号）会导致编译失败 只配置 composite 不配置相关 fields，通常会降低可维护性（建议同时声明） 未重编译直接测试，新配置不会生效  步骤 3: 创建 Ingest Pipeline #  PUT _ingest/pipeline/security-check { \u0026#34;description\u0026#34;: \u0026#34;使用规则引擎对安全事件分类\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;check_match_rules\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;security_v1\u0026#34; } } ] } 使用自定义目标字段：\nPUT _ingest/pipeline/security-check-custom { \u0026#34;description\u0026#34;: \u0026#34;使用规则引擎对安全事件分类，结果写入 security_labels 字段\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;check_match_rules\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;security_v1\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;security_labels\u0026#34; } } ] } 配置参数：\n   参数 类型 必需 默认值 说明     id string ✅ - 规则库 ID（对应 repo_id）   target_field string ❌ \u0026ldquo;tags\u0026rdquo; 匹配结果写入的目标字段名   ignore_missing boolean ❌ false 是否忽略缺失字段错误   regex_start_at_word boolean ❌ true 正则是否从单词边界开始   fields array ❌ [] 文档字段白名单：指定后仅这些字段按原字段名参与匹配，其余字段内容汇总到 default_match_field   default_match_field string ❌ \u0026ldquo;content\u0026rdquo; 当配置了 fields 时，未包含字段会汇总到该字段名参与匹配    步骤 4: 写入文档测试 #  POST security-events/_doc?pipeline=security-check { \u0026#34;title\u0026#34;: \u0026#34;枪支交易案\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;网上出售手枪，价格5000元\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-31T10:00:00Z\u0026#34; } 查询结果：\n{ \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;枪支交易案\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;网上出售手枪，价格5000元\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-31T10:00:00Z\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;#0#涉枪武器关键词\u0026#34;] } }  规则语法 #  规则文件格式 #  规则文件是文本文件，每行一条规则，在系统中存储的格式如下：\nexpression\\t#offset#description 格式说明：\n expression: 规则表达式（匹配条件） #offset#: 规则序号（从 0 开始，由系统自动生成） description: 规则描述（匹配时作为标签返回）  导入时格式：通过 _import API 导入时，只需提供 expression 和 description，系统会自动生成 offset：\n{ \u0026#34;expression\u0026#34;: \u0026#34;枪 or 手枪 or 步枪\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;涉枪武器关键词\u0026#34; } 系统存储示例（导入后在 .match_rules 索引中的格式）：\n枪 or 手枪 or 步枪\t#0#涉枪武器关键词 AK47 or AK-47 or M16\t#1#枪支型号 海洛因 or 冰毒 or 摇头丸\t#2#毒品名称 注意：\n # 开头的行是注释 空行会被跳过 可使用 \\\\ 作为续行符将长规则分成多行 表达式和描述之间使用 Tab 键分隔（\\t）  基本语法 #  1. 逻辑操作符 #     操作符 别名 优先级 说明     and \u0026amp; 中 同时满足（逻辑与）   or \\| 低 满足任一（逻辑或）   not \u0026amp;!, - 中（二元）/最高（一元） 不包含（逻辑非）   near 无 高 距离匹配    示例：\n# AND: 同时包含两个词 中国 and 人民 OR: 包含任一词 中国 or 人民\nNOT: 包含前者但不包含后者 中国 and not 日本 中国 not 日本 中国 - 日本 2. 关键词匹配 #\n 枪 or 手枪 or 步枪 匹配包含任一关键词的文档。\n⚠️ 特殊字符警告：减号 - 的特殊含义\n减号 - 是 not 操作符的别名，会被解释为逻辑非。如果要匹配包含减号的字面文本，必须用引号括起来。\n# ❌ 错误：会被解释为\u0026#34;包含中国 AND 不包含2025年GDP\u0026#34; 中国-2025年GDP ✅ 正确：匹配字面文本\u0026quot;中国-2025年GDP\u0026quot; \u0026quot;中国-2025年GDP\u0026quot;\n❌ 错误：会被解释为\u0026quot;包含高端 AND 不包含低端\u0026quot; 高端-低端产品\n✅ 正确：匹配字面文本\u0026quot;高端-低端产品\u0026quot; \u0026quot;高端-低端产品\u0026quot; 其他需要加引号的场景：\n 包含空格：\u0026quot;紧急 通知\u0026quot; 包含特殊符号：\u0026quot;C++编程\u0026quot;, \u0026quot;@用户名\u0026quot; 包含操作符关键字：\u0026quot;and或or\u0026quot;  3. 字段限定 #  语法：\n fieldname(expression): 子串匹配，在指定字段的任意位置匹配表达式 fieldname[expression]: 全匹配，表达式必须匹配字段的全部内容  示例：\n# 在 title 字段中查找\u0026#34;紧急\u0026#34; title(紧急) 在 author 字段中查找\u0026quot;张三\u0026quot; author(张三)\n组合使用 title(紧急) and content(通知) and category(安全)\n全匹配（用于精确匹配） status[已审核] 字段处理机制：\n规则引擎对字段的处理遵循以下规则：\n  默认字段：title 和 content 是系统内置字段，无需声明\n 未指定字段的表达式默认匹配 content 字段    未声明字段的处理：文档中未在编译时声明的字段，会被当作 content 字段处理\n 例如：author(张三) 中的 author 如果未声明，会按 content 字段匹配    数值字段：用于范围匹配的数值字段（如 range(100, 500)）必须在编译时通过 fields 参数声明\n 未声明的数值字段会导致编译失败    字段声明示例：\n# 编译时声明数值字段 POST /_match_rules/repo/_compile { \u0026#34;fields\u0026#34;: [\u0026#34;range\u0026#34;, \u0026#34;price\u0026#34;, \u0026#34;ram_gb\u0026#34;] } 规则中使用 { \u0026quot;expression\u0026quot;: \u0026quot;title(手机) and price(3000, 5000)\u0026quot; } 4. 正则表达式 #\n 简单正则（用 {{...}} 括起来）：\n# 匹配北京电话号码 {{(\\(010\\)|010)-?[0-9]{8}}} 匹配邮箱地址 {{\\w+@\\w+.\\w+}} 复合正则（用 [\u0026hellip;] 括起来，支持逻辑运算）：\n# 匹配除 .com 外的所有网站 [{{https?://([\\w\\d_-]+\\.)+[\\w\\d_-]+}} and not {{.*\\.com}}] 字段中使用正则 author({{张.*}}) 重要：正则表达式必须用双引号包围。\n❌ 错误：author(/张.*/) ✅ 正确：author(\u0026quot;/张.*/\u0026quot;)\n高级功能 #  距离匹配 (near) #  near 用于匹配两个词之间的距离。\n语法：\n A near/num B: A 和 B 之间距离不超过 num（不分前后） A near/+num B: A 在 B 之前，距离不超过 num A near/-num B: A 在 B 之后，距离不超过 num  距离单位：\n 中文：每个汉字为 1 个单位 英文/数字：每个单词为 1 个单位（如 computer、10086、varname_123 均为 1 个单位）  示例：\n# \u0026#34;中国\u0026#34;和\u0026#34;人民\u0026#34;之间距离不超过 3 中国 near/3 人民 \u0026quot;中国\u0026quot;必须在\u0026quot;人民\u0026quot;之前，距离不超过 5 中国 near/+5 人民\n链式 near（等价于多个 near 的 and） 中国 near/3 人民 near/5 勤劳 near/7 勇敢 复合 near：\n(中国 or 中华 or 华夏) near/3 (人民 or 百姓) near/5 (勤劳 or 勤奋) 数值范围匹配 #  range(400, 600) price(1000, 5000) 匹配数值在指定范围内的文档（闭区间 [min, max]）。\n要求：\n 字段类型必须是数值 编译时必须声明字段名（使用 fields 参数）  示例：\nPOST /_match_rules/opinion_check/_import { \u0026#34;rules\u0026#34;: [ { \u0026#34;expression\u0026#34;: \u0026#34;title(威马M5) and content(续航) and range(0, 500)\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;威马M5续航不足\u0026#34; } ] } POST /_match_rules/opinion_check/_compile { \u0026quot;fields\u0026quot;: [\u0026quot;range\u0026quot;, \u0026quot;price\u0026quot;] } 规则编写最佳实践 #\n 1. 从简单到复杂 #  # 简单规则 枪 or 手枪 中等复杂度 title(枪支) and (买卖 or 交易 or 出售)\n复杂规则 (枪 or 手枪 or 步枪) near/5 (买 or 卖 or 交易) and not (玩具 or 模型) 2. 合理使用字段限定 #\n # 标题匹配更精准 title(紧急通知) 组合多个字段 title(安全) and content(漏洞) and author(安全团队) 3. 使用 near 提高精确度 #\n # 简单 OR 可能误匹配 枪支 or 交易 使用 near 限制距离 枪支 near/10 交易 4. 善用 NOT 排除干扰 #\n # 排除玩具枪 (枪 or 手枪) and not (玩具 or 模型 or 仿真) 5. 正则表达式用于模糊匹配 #  # 匹配所有以\u0026#34;张\u0026#34;开头的姓名 author({{张.*}}) 匹配手机号 {{1[3-9][0-9]{9}}} 常见错误 #\n ❌ 错误 1: 减号未加引号导致误解析 #  # 错误：会被解释为 NOT 操作 title(2024-2025年报) # 实际匹配：title 包含\u0026#34;2024\u0026#34;但不包含\u0026#34;2025年报\u0026#34; 正确：匹配字面文本 title(\u0026quot;2024-2025年报\u0026quot;) 常见场景：\n 日期范围：\u0026quot;2024-01-01\u0026quot;, \u0026quot;2024-2025\u0026quot; 产品型号：\u0026quot;iPhone-15\u0026quot;, \u0026quot;GTX-4090\u0026quot; 复合词：\u0026quot;高端-低端\u0026quot;, \u0026quot;线上-线下\u0026quot;  ❌ 错误 2: 正则未加引号 #  # 错误 author(/张.*/) 正确 author(\u0026quot;/张.*/\u0026quot;) ❌ 错误 3: 忘记声明自定义字段 #\n // 规则中使用了 range 字段 \u0026#34;expression\u0026#34;: \u0026#34;range(100, 500)\u0026#34; // 但编译时忘记声明 POST /_match_rules/repo/_compile { // ❌ 缺少 fields 参数 }\n// ✅ 正确做法 POST /_match_rules/repo/_compile { \u0026quot;fields\u0026quot;: [\u0026quot;range\u0026quot;] } ❌ 错误 4: 表达式和描述未用 Tab 分隔 #\n # 错误（使用空格） 枪 or 手枪 涉枪内容 正确（使用 Tab） 枪 or 手枪\t涉枪内容 特殊字符速查表 #\n    字符 在规则中的含义 如何匹配字面值 示例     - NOT 操作符 用引号括起来 \u0026quot;2024-2025\u0026quot;   \u0026amp; AND 操作符 用引号括起来 \u0026quot;Tom\u0026amp;Jerry\u0026quot;   \\| OR 操作符 用引号括起来 \u0026quot;A\\|B\u0026quot;   ( ) 分组、字段限定 用引号括起来 \u0026quot;(重要)\u0026quot;   [ ] 全匹配、复合正则 用引号括起来 \u0026quot;[TODO]\u0026quot;   { } 正则表达式 用引号括起来 \u0026quot;{变量}\u0026quot;   空格 词分隔符 用引号括起来 \u0026quot;hello world\u0026quot;    建议：当规则包含任何可能引起歧义的字符时，养成加引号的习惯。\n 高级应用示例 #  示例 1: 舆情监控 #  使用数值范围匹配来监控产品评价中的数值异常。\n导入规则：\nPOST /_match_rules/opinion_check/_import { \u0026#34;name\u0026#34;: \u0026#34;舆情监控规则\u0026#34;, \u0026#34;rules\u0026#34;: [ { \u0026#34;expression\u0026#34;: \u0026#34;title(威马M5) and content(续航) and range(0, 500)\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;威马M5续航不足\u0026#34; }, { \u0026#34;expression\u0026#34;: \u0026#34;title(威马M7) and content(续航) and range(2500, 9999)\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;威马M7续航异常高\u0026#34; } ] } 编译时声明字段：\nPOST /_match_rules/opinion_check/_compile { \u0026#34;fields\u0026#34;: [\u0026#34;range\u0026#34;, \u0026#34;price\u0026#34;, \u0026#34;energy_kwh\u0026#34;], \u0026#34;quiet\u0026#34;: true } 说明：编译时只需声明规则中实际使用的数值字段。上述规则中使用了 range(0, 500) 和 price(2500, 9999)，所以声明了这两个字段。\nPipeline 配置：\nPUT _ingest/pipeline/opinion_check { \u0026#34;description\u0026#34;: \u0026#34;使用规则引擎监测商家网络舆情\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;check_match_rules\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;opinion_check\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;range\u0026#34;, \u0026#34;price\u0026#34;, \u0026#34;energy_kwh\u0026#34;, \u0026#34;fuel_lper100km\u0026#34;, \u0026#34;ram_gb\u0026#34;, \u0026#34;storage_gb\u0026#34;, \u0026#34;volume_ml\u0026#34;, \u0026#34;concentration\u0026#34; ] } } ] } 说明：Pipeline 中的 fields 参数可以包含更多数值字段，以便支持文档中可能出现的其他数值字段匹配。这些字段不需要在当前规则库中使用，但如果文档包含这些字段，规则引擎会正确处理。\n测试匹配：\nPOST /_ingest/pipeline/opinion_check/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;reviews\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc_1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;威马M5续航测试\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;威马M5的续航表现一般，实测续航里程偏低\u0026#34;, \u0026#34;range\u0026#34;: 450 } }, { \u0026#34;_index\u0026#34;: \u0026#34;reviews\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc_2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;威马M7续航表现\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;威马M7的续航非常出色\u0026#34;, \u0026#34;range\u0026#34;: 2650 } } ] } 预期结果：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;威马M5续航测试\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;威马M5的续航表现一般，实测续航里程偏低\u0026#34;, \u0026#34;range\u0026#34;: 450, \u0026#34;tags\u0026#34;: [\u0026#34;#0#威马M5续航不足\u0026#34;] } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;威马M7续航表现\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;威马M7的续航非常出色\u0026#34;, \u0026#34;range\u0026#34;: 2650, \u0026#34;tags\u0026#34;: [\u0026#34;#1#威马M7续航异常高\u0026#34;] } } } ] }  规则索引结构 #  .match_rules 索引说明 #  .match_rules 是 Rules 插件使用的内部索引（名称以 . 开头），用于存储规则库的元数据和配置信息。每个规则库对应索引中的一条文档。\n索引特性：\n 索引名称：.match_rules（以 . 开头；当前初始化逻辑未显式设置 index.hidden） 分片配置：1 个主分片，1 个副本 自动扩展：副本数自动扩展到 0-1 文档 ID：使用 repo_id 作为文档 ID  核心字段：\n   字段 类型 必需 说明     name text ✅ 规则库名称   description text ❌ 规则库描述   tags keyword[] ❌ 规则库标签（用于分类）   rules text ✅ 规则文本内容（格式：expression\\t#offset#description）   version long ✅ 规则库版本（自动递增：1, 2, 3\u0026hellip;）   status keyword ✅ 编译状态：pending/compiled/partial/failed   total_rules integer ❌ 编译后的规则总数（导入后可能为空）   compiled_at date ❌ 编译完成时间   compiled_nodes keyword[] ❌ 已编译节点列表   fields keyword[] ❌ 编译时声明的字段列表   composite keyword[] ❌ 联合索引定义列表（2026-01-18 新增）   error_message text ❌ 错误信息（编译失败时）   created date ✅ 创建时间   updated date ✅ 更新时间    规则文本格式：\nrules 字段存储完整的规则定义，每行一条规则，包含自动生成的 offset：\nexpression\\t#offset#description 其中：\n offset 是系统自动生成的规则序号（从 0 开始） 通过 _import API 导入时不需要手动指定 offset  示例：\n枪 or 手枪 or 步枪\\t#0#涉枪武器关键词 AK47 or AK-47 or M16\\t#1#枪支型号 海洛因 or 冰毒\\t#2#毒品名称 查询示例：\n# 查询单个规则库（排除 rules 字段避免大量数据） GET .match_rules/_doc/security_v1?_source_excludes=rules 搜索所有已编译的规则库 GET .match_rules/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;status\u0026quot;: \u0026quot;compiled\u0026quot; } }, \u0026quot;_source\u0026quot;: { \u0026quot;excludes\u0026quot;: [\u0026quot;rules\u0026quot;] } }\n查询特定标签的规则库 GET .match_rules/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;tags\u0026quot;: \u0026quot;security\u0026quot; } } } \nAPI 参考 #  导入规则 API #  支持以下 4 条导入路由：\n POST /_match_rules/{repo_id}/_import：覆盖导入（upsert） PUT /_match_rules/{repo_id}/_import：追加导入（append） POST /_match_rules/_import：自动生成 repo_id 后覆盖导入 PUT /_match_rules/_import：自动生成 repo_id 后追加导入  POST /_match_rules/{repo_id}/_import #  覆盖导入（删除旧规则，创建新规则）。\n请求体：\n{ \u0026#34;name\u0026#34;: \u0026#34;规则库名称\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;规则库描述\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;tag1\u0026#34;, \u0026#34;tag2\u0026#34;], \u0026#34;rules\u0026#34;: [ { \u0026#34;expression\u0026#34;: \u0026#34;规则表达式\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;规则描述\u0026#34; } ] } PUT /_match_rules/{repo_id}/_import #  追加导入（保留旧规则，添加新规则）。\n示例：\nPUT /_match_rules/security_v1/_import { \u0026#34;rules\u0026#34;: [ { \u0026#34;expression\u0026#34;: \u0026#34;买枪 or 卖枪 or 枪支交易\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;武器交易\u0026#34; } ] } 响应：\n{ \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Successfully imported 4 rules to repo_id \u0026#39;security_v1\u0026#39; (version: 2)\u0026#34;, \u0026#34;repo_id\u0026#34;: \u0026#34;security_v1\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;rule_count\u0026#34;: 4 } POST /_match_rules/_import #  自动生成 repo_id（格式：repo_{timestamp}）。\nPUT /_match_rules/_import #  自动生成 repo_id（格式：repo_{timestamp}）并使用追加语义导入。\n说明：\n 由于每次请求都会生成新的 repo_id，该路由在多数场景下等价于创建新规则库文档 若需要对同一规则库执行真实追加，请使用 PUT /_match_rules/{repo_id}/_import  导入后的状态变化（适用于 POST/PUT）：\n 每次导入都会重置编译状态为 pending 导入后需要重新调用 POST /_match_rules/{repo_id}/_compile 才会让新规则生效  编译规则 API #  POST /_match_rules/{repo_id}/_compile #  编译规则库为二进制格式。\n请求体：\n{ \u0026#34;fields\u0026#34;: [\u0026#34;author\u0026#34;, \u0026#34;source\u0026#34;, \u0026#34;range\u0026#34;], \u0026#34;quiet\u0026#34;: true } 注意：\n 请求体是必需的，至少传空对象 {}（不带 body 会返回解析错误）  参数说明：\n   参数 类型 必需 默认值 说明     fields array ❌ [] 编译时声明的字段列表（用于字段限定与范围匹配等）   composite array ❌ [] 联合索引定义（用于多字段组合匹配，提升性能）   quiet boolean ❌ true 是否静默模式（不输出详细日志）    联合索引：\ncomposite 参数用于定义多字段组合索引，提升多字段 AND 条件的匹配性能：\n{ \u0026#34;fields\u0026#34;: [\u0026#34;gender\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;income\u0026#34;], \u0026#34;composite\u0026#34;: [ \u0026#34;(gender,age,income)\u0026#34;, // 基本格式 \u0026#34;s{:}(name,city,province)\u0026#34; // 带分隔符格式 ] } 格式说明：\n (field1,field2,field3): 基本格式，多个字段组合索引 s{sep}(field1,field2): 带分隔符格式，sep 为自定义分隔符（如 :）  性能优势：\n 单字段索引需要分别扫描 + 内存交集 联合索引一次扫描组合索引，性能提升 3-10 倍  多节点集群：编译会自动广播到所有 Ingest 节点；若个别节点未安装 rules 插件或编译失败，结果可能为部分成功（partial）。\n响应：\n{ \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Broadcast compile completed for 3 nodes:\\n ✓ node-1: 15000 rules (2500ms)\\n ✓ node-2: 15000 rules (2480ms)\\n ✓ node-3: 15000 rules (2520ms)\u0026#34;, \u0026#34;result\u0026#34;: { \u0026#34;duration_ms\u0026#34;: 2520, \u0026#34;total_rules\u0026#34;: 15000, \u0026#34;indexed_rules\u0026#34;: 0 } } 查询规则库 API #  GET .match_rules/_doc/{repo_id} #  查询规则库详情。\nGET .match_rules/_doc/security_v1 响应：\n{ \u0026#34;_index\u0026#34;: \u0026#34;.match_rules\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;security_v1\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;安全规则库\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;用于安全事件分类的规则库\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;security\u0026#34;, \u0026#34;content-filter\u0026#34;], \u0026#34;rules\u0026#34;: \u0026#34;枪 or 手枪 or 步枪 or 猎枪\\t#0#涉枪武器关键词\\nAK47 or AK-47 or M16 or M4A1\\t#1#枪支型号\\n...\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;compiled\u0026#34;, \u0026#34;compiled_at\u0026#34;: \u0026#34;2026-01-07T08:00:00Z\u0026#34;, \u0026#34;total_rules\u0026#34;: 4, \u0026#34;compiled_nodes\u0026#34;: [\u0026#34;node-1\u0026#34;, \u0026#34;node-2\u0026#34;, \u0026#34;node-3\u0026#34;], \u0026#34;created\u0026#34;: \u0026#34;2025-12-31T10:00:00Z\u0026#34;, \u0026#34;updated\u0026#34;: \u0026#34;2025-12-31T10:15:00Z\u0026#34; } } 字段说明：\n.match_rules 索引用于存储规则库的元数据和配置信息。每个规则库对应一条文档，文档 ID 即为 repo_id。\n   字段 类型 说明     name text 规则库名称（用户自定义，用于标识规则库用途）   description text 规则库描述（详细说明规则库的作用和使用场景）   tags keyword[] 规则库标签（用于分类和检索，如 [\u0026ldquo;security\u0026rdquo;, \u0026ldquo;content-filter\u0026rdquo;]）   rules text 规则文本内容（完整的规则定义，每行一条规则，格式：expression\\t#offset#description）   version long 规则库版本（每次导入递增，格式：1, 2, 3\u0026hellip;）   status keyword 编译状态：pending（待编译）、compiled（已编译）、partial（部分编译）、failed（失败）   total_rules integer 编译后的规则总数（未编译时可能为空）   compiled_at date 编译完成时间（最后一次成功编译的时间戳）   compiled_nodes keyword[] 已编译节点列表（已成功编译此规则库的节点名称）   fields keyword[] 编译时声明的字段列表（用于字段限定与范围匹配等）   composite keyword[] 联合索引定义列表（用于多字段组合匹配，如 [\u0026quot;(price,range)\u0026quot;, \u0026ldquo;(title,content)\u0026quot;]）   error_message text 错误信息（编译失败时的详细错误描述）   created date 创建时间（规则库首次导入的时间）   updated date 更新时间（规则库最后一次修改的时间）    注意：\n rules 字段可能包含大量文本（数万条规则），查询时建议使用 _source_excludes=rules 排除此字段 version 字段在每次使用 POST /_match_rules/{repo_id}/_import（覆盖模式）或 PUT /_match_rules/{repo_id}/_import（追加模式）导入时自动递增 compiled_nodes 字段在多节点集群中记录了所有成功编译的节点，用于判断集群同步状态  删除规则库 API #  DELETE /_match_rules/{repo_id} #  删除规则库（包括索引文档、物理文件和本地元数据）。\nHTTP 状态码：\n 200：删除成功（success: true） 500：业务失败（如存在 Pipeline 依赖、索引或物理文件删除失败，success: false）  功能特性：\n ✅ Pipeline 依赖检查：防止删除正在使用的规则库 ✅ 集群广播删除：多节点环境下同步删除物理文件 ✅ 物理文件清理：递归删除规则库目录 ✅ 元数据更新：自动更新本地元数据文件  依赖检查：\n删除前会检查是否有 Pipeline 正在使用该规则库，如果有依赖则删除失败：\nDELETE /_match_rules/security_v1 失败响应（有依赖）：\n{ \u0026#34;success\u0026#34;: false, \u0026#34;message\u0026#34;: \u0026#34;Cannot delete rule repository: used by pipelines: security-check, fraud-detection\u0026#34;, \u0026#34;node_name\u0026#34;: \u0026#34;node-1\u0026#34; } 成功响应：\n{ \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Deleted successfully\u0026#34;, \u0026#34;node_name\u0026#34;: \u0026#34;node-1\u0026#34; } 删除流程：\n 检查 Pipeline 依赖（递归检查嵌套 processors） 删除索引文档（从 .match_rules 索引） 广播删除到所有 Ingest 节点 每个节点删除物理目录 + 更新元数据  安全机制：\n 路径安全验证（防止 ../ 遍历攻击） 超时控制（广播操作超时 5 分钟）   节点启动与规则同步 #  自动同步机制 #  节点启动时，Rules 插件会自动检查并同步缺失的规则库：\n 检查本地规则库：对比 .match_rules 索引中的规则库 自动编译缺失库：对于本地不存在的规则库，自动执行编译 更新节点状态：同步完成后更新 compiled_nodes 列表  预期行为：\n 首次启动：自动编译所有规则库（通常 10-20 秒） 集群重启：验证本地规则库，如有缺失自动恢复 新节点加入：后台静默同步，不影响集群服务  同步期间的写入行为 #  在规则库同步期间，写入行为取决于集群拓扑与节点状态：部分场景会加同步 block 并出现重试等待，其他场景可能不加 block。\n用户体验：\n 可能出现写入等待/重试（由重试策略决定），也可能直接由其他可用 Ingest 节点处理 等待时间取决于规则库大小与节点数量（通常几秒到几分钟） 若超时或异常，写入仍可能失败，需按错误信息重试  不受影响的操作：\n ✅ 所有读操作（查询、获取文档等） ✅ 系统索引的写入（.match_rules 等）  预计等待时间：\n 通常 10-20 秒 大型规则库（数万条）可能需要更长时间   批量导入工具 #  对于大规则文件，可使用 Python 脚本批量导入：\n# 基本用法（追加模式） python import_rules.py -f rules.txt -r security_v1 -n \u0026#34;安全规则库\u0026#34; # 覆盖模式（删除旧规则） python import_rules.py -f rules.txt -r opinion_v9 -n \u0026quot;舆情规则库\u0026quot; \u0026ndash;overwrite\n# 指定名称、描述和标签 python import_rules.py -f rules.txt -r opinion_v9  -n \u0026quot;舆情监控规则\u0026quot;  -d \u0026quot;用于舆情监控的规则库\u0026quot;  -t opinion -t monitoring\n# 使用 HTTPS 和自定义认证 python import_rules.py -f rules.txt -r security_v1 -n \u0026quot;安全规则\u0026quot;  \u0026ndash;url https://localhost:9200  \u0026ndash;username admin  \u0026ndash;password admin123 规则文件格式 (rules.txt)：\n规则文件每行一条规则，使用 Tab 键分隔表达式和描述（不需要手动指定 offset）：\n枪 or 手枪 or 步枪\t涉枪武器关键词 AK47 or AK-47 or M16\t枪支型号 海洛因 or 冰毒\t毒品名称 说明：导入时系统会自动为每条规则生成 offset（从 0 开始），无需手动添加 #offset# 部分。\n工具位置：plugins/rules/src/test/resources/import_rules.py\n 使用场景 #  1. 内容安全审核 #  规则示例：\n{ \u0026#34;rules\u0026#34;: [ { \u0026#34;expression\u0026#34;: \u0026#34;枪 or 手枪 or 步枪 or 猎枪\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;涉枪内容\u0026#34; }, { \u0026#34;expression\u0026#34;: \u0026#34;海洛因 or 冰毒 or 摇头丸\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;涉毒内容\u0026#34; }, { \u0026#34;expression\u0026#34;: \u0026#34;isis or 伊斯兰国 or IS组织\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;恐怖组织\u0026#34; } ] } 查询示例：\nGET security-events/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;tags\u0026#34;: [\u0026#34;#0#涉枪内容\u0026#34;, \u0026#34;#1#涉毒内容\u0026#34;] } } } 2. 新闻分类 #  规则示例：\n{ \u0026#34;rules\u0026#34;: [ { \u0026#34;expression\u0026#34;: \u0026#34;author(张三) and source(新华网)\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;张三_新华网\u0026#34; }, { \u0026#34;expression\u0026#34;: \u0026#34;title(紧急) and content(通知) and category(安全)\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;紧急安全通知\u0026#34; } ] } 3. 商品推荐 #  规则示例：\n{ \u0026#34;rules\u0026#34;: [ { \u0026#34;expression\u0026#34;: \u0026#34;category(手机) and price(3000, 5000)\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;中端手机\u0026#34; }, { \u0026#34;expression\u0026#34;: \u0026#34;category(笔记本) and ram_gb(16, 32)\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;高性能笔记本\u0026#34; } ] } 4. 舆情监控 #  规则示例（见上文\u0026quot;数值范围匹配\u0026quot;章节）\n 性能优化 #  规则库大小建议 #   单个规则库不超过 50,000 条规则 大规则库考虑拆分为多个 repo_id  编译性能参考 #     规则数 编译耗时     1,000 ~100-200ms   10,000 ~500-1000ms   100,000 ~5-10s    字段优化 #   只声明规则中实际使用的字段 数值字段（如 range、price）必须声明，否则编译失败   限制和注意事项 #   License 要求：需要启用 rule-engine 特性的有效 License 系统配置要求：必须设置 bootstrap.system_call_filter: false（见\u0026quot;环境要求\u0026quot;章节） 目标字段可配置：默认写入 tags 字段，可通过 target_field 参数自定义 数值字段类型：数值范围匹配要求字段值为数值类型 正则语法：必须使用双引号包围正则表达式 编译依赖索引：编译前必须先导入规则到 .match_rules 索引 节点启动同步：节点启动时会自动同步规则库，写入操作会等待同步完成（通常 10-20 秒） 元数据文件：.compiled_repos.json 用于检测文件丢失，请勿手动删除   ","subcategory":null,"summary":"","tags":null,"title":"规则引擎","url":"/easysearch/main/docs/features/rule-engine/"},{"category":null,"content":"申威平台安装 #  申威平台介绍 #  申威平台基于自主 Alpha 指令集架构（后演进为 SW64 ），由申威科技研发，主打高性能计算与安全可控，广泛应用于超算、服务器及国防关键领域，全面支持国产操作系统与信创生态。\n申威平台安装参考 #  目前，Easysearch 已支持在申威芯片的国产操作系统上运行，联网环境建议使用一键安装脚本进行安装，离线环境建议下载 Bundle 包进行安装，分布式集群安装请参考 分布式集群安装。\n 前提条件：已参照 系统调优进行了系统优化，同时为 Easysearch 创建了专用的用户。\n 初始化系统参数及用户命令参考 #  # 调整内核配置 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p # 增加用户组与用户 groupadd -r easysearch \u0026amp;\u0026amp; useradd -r -g easysearch -d /home/easysearch -s /sbin/nologin -c \u0026#34;Easysearch Service Account\u0026#34; easysearch 安装命令参考 #  # 创建数据目录 mkdir -p /opt/easysearch # 下载最新版本的 Easysearch 并安装 curl -sSL http://get.infini.cloud | bash -s -- -p easysearch -d /opt/easysearch # 进入 Easysearch 目录 cd /opt/easysearch # 初始化 Easysearch bin/initialize.sh -s # 调整目录权限 chown -R easysearch:easysearch /opt/easysearch # 启动 Easysearch runuser -u easysearch -- /opt/easysearch/bin/easysearch -d -p /opt/easysearch/easysearch.pid  注意：初始化过程中会生成随机密码，并不会保存到日志文件中，只会在终端显示一次，请妥善保存。如果忘记 admin 密码，可以使用 bin/reset_admin_password.sh 进行重置。\n 已验证系统环境：\n SW3231/UnionTech OS Server 20 1050d  如果您在其他申威平台的国产操作系统上安装遇到问题，欢迎通过 提交工单与我们联系。\n","subcategory":null,"summary":"","tags":null,"title":"申威平台安装","url":"/easysearch/main/docs/deployment/install-guide/ciip/shenwei/"},{"category":null,"content":"扁平对象字段类型（Flat Object） #  flat_object（也称为 flattened 类型）将整个 JSON 对象作为单个扁平化字段存储。它会将 JSON 对象中所有叶子节点的值提取为关键字（keyword），并支持对这些值进行基本查询。\n适用场景 #  flat_object 特别适合以下场景：\n 标签/标注数据：如 Kubernetes 的 labels、annotations 等动态键值对 防止 mapping explosion：当字段名不可预知或数量极多时，避免为每个子字段创建独立的映射条目 半结构化 JSON：存储结构不固定的 JSON 数据，同时保持可搜索性 日志元数据：存储动态的日志属性或自定义元数据  与其他类型对比 #     类型 动态字段支持 查询能力 Mapping 条目 性能     object ✅ 每个子字段独立映射 完整（全文、范围等） 多（每子字段一条） 高（独立索引）   nested ✅ 每个子字段独立映射 完整 + 对象关联性 多 + 隐藏文档 中   flat_object ✅ 无需预定义 基本（term、prefix、range） 仅一条 中    创建映射 #  PUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;labels\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;flat_object\u0026#34; } } } } flat_object 使用默认配置即可，不需要额外的映射参数。\n索引文档 #  PUT my_index/_doc/1 { \u0026#34;labels\u0026#34;: { \u0026#34;env\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;team\u0026#34;: \u0026#34;backend\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2.1.0\u0026#34;, \u0026#34;priority\u0026#34;: \u0026#34;high\u0026#34; } } PUT my_index/_doc/2 { \u0026quot;labels\u0026quot;: { \u0026quot;env\u0026quot;: \u0026quot;staging\u0026quot;, \u0026quot;team\u0026quot;: \u0026quot;frontend\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.5.0\u0026quot;, \u0026quot;region\u0026quot;: \u0026quot;us-east-1\u0026quot; } } 支持嵌套结构：\nPUT my_index/_doc/3 { \u0026#34;labels\u0026#34;: { \u0026#34;deploy\u0026#34;: { \u0026#34;region\u0026#34;: \u0026#34;cn-north-1\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;a\u0026#34; }, \u0026#34;team\u0026#34;: \u0026#34;infra\u0026#34; } } 查询方式 #  term 查询 #  按值查询（不指定路径，匹配任意键的值）：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;labels\u0026#34;: \u0026#34;production\u0026#34; } } } 按路径.值查询（指定具体键）：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;labels.env\u0026#34;: \u0026#34;production\u0026#34; } } } 嵌套路径查询：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;labels.deploy.region\u0026#34;: \u0026#34;cn-north-1\u0026#34; } } } terms 查询 #  GET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;labels.env\u0026#34;: [\u0026#34;production\u0026#34;, \u0026#34;staging\u0026#34;] } } } prefix 查询 #  GET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34;: { \u0026#34;labels.version\u0026#34;: \u0026#34;2.\u0026#34; } } } range 查询 #  GET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;labels.version\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;lt\u0026#34;: \u0026#34;3.0.0\u0026#34; } } } }  注意：range 查询基于字符串比较（字典序），不是数值比较。\u0026quot;9\u0026quot; 会大于 \u0026quot;10\u0026quot;。\n exists 查询 #  检查字段是否存在：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;labels\u0026#34; } } } 检查特定键是否存在：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;labels.region\u0026#34; } } } 聚合 #  flat_object 字段支持基于 doc values 的聚合：\nGET my_index/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;env_values\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;labels.env\u0026#34; } } } } 内部原理 #  flat_object 在内部为每个字段创建三个子字段：\n   子字段 存储内容 用途     父字段 所有 JSON 键名 exists 查询   ._value 所有叶子节点的值 不带路径的 term/prefix/range 查询   ._valueAndPath 路径=值 格式的条目 带路径的 term/prefix/range 查询    例如，文档 {\u0026quot;labels\u0026quot;: {\u0026quot;env\u0026quot;: \u0026quot;prod\u0026quot;, \u0026quot;team\u0026quot;: \u0026quot;be\u0026quot;}} 会被索引为：\n ._value: \u0026quot;prod\u0026quot;, \u0026quot;be\u0026quot; ._valueAndPath: \u0026quot;env=prod\u0026quot;, \u0026quot;team=be\u0026quot;  当查询 labels.env = \u0026quot;prod\u0026quot; 时，实际搜索的是 ._valueAndPath 子字段中的 \u0026quot;env=prod\u0026quot;。\n限制 #   不支持 wildcard 查询：flat_object 字段不能使用 wildcard 查询 字符串比较：所有值按关键字（keyword）处理，range 查询基于字典序而非数值 不支持全文搜索：不能使用 match、match_phrase 等全文查询 不支持高亮：查询结果中不支持高亮显示 不支持自定义分析器：值始终按 keyword 处理，不进行分词 嵌套深度：非常深的嵌套 JSON 会增加 ._valueAndPath 条目的长度  最佳实践 #   动态标签场景优选：Kubernetes labels、自定义 tags 等字段名不固定的场景，用 flat_object 可以避免 mapping 膨胀 不要用于需要全文搜索的字段：如果需要分词搜索，应使用 text 类型 不要用于需要数值运算的字段：如果需要数值 range 或聚合运算，应使用 integer/float 等数值类型 合理控制对象大小：虽然 flat_object 没有子字段限制，但过大的 JSON 对象会影响索引性能  ","subcategory":null,"summary":"","tags":null,"title":"扁平对象字段类型（Flat Object）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/flat-object/"},{"category":null,"content":"S3 定期备份 #  Easysearch Operator 支持通过 YAML 配置来管理 S3 快照备份策略。Operator 会启动一个 Job，通过集群 API 自动配置相应的备份策略。\n工作原理 #   在 Operator YAML 中配置 S3 备份相关参数（Bucket 名称、访问密钥、备份周期等） Operator 创建一个 Job，该 Job 调用 Easysearch 的快照生命周期管理（SLM）API SLM 按照配置的策略自动执行定期快照  相关文档 #    快照生命周期管理 API  备份与恢复  ","subcategory":null,"summary":"","tags":null,"title":"S3 备份","url":"/easysearch/main/docs/deployment/install-guide/operator/s3_snapshot/"},{"category":null,"content":"JVM 配置 #  本页介绍 config/jvm.options 文件中的 JVM 启动参数。这些参数在 Easysearch 启动时由 JVM 读取，修改后需要重启节点生效。\n 堆内存设置 #  堆内存是 JVM 配置中最重要的参数。\n# 堆大小设为物理内存的 50%，但不超过 31 GB # -Xms 和 -Xmx 必须相同，避免运行时堆调整 -Xms16g -Xmx16g    参数 说明     -Xms JVM 初始堆大小   -Xmx JVM 最大堆大小    设置原则 #   -Xms 和 -Xmx 设为相同值：避免运行时堆大小调整带来的性能波动。 不超过物理内存的 50%：剩余内存留给操作系统文件缓存（Lucene 严重依赖 OS 页面缓存）。 不超过 31 GB（建议设为 31g 或 30g）：超过约 32 GB 后 JVM 无法使用压缩对象指针（Compressed OOPs），实际可用内存反而减少。 不低于 1 GB：过小的堆会导致频繁 GC。  验证压缩指针 #  GET _nodes/stats/jvm?filter_path=**.using_compressed_ordinary_object_pointers 返回 true 表示压缩指针生效。\n通过环境变量覆盖 #  如果不想修改 jvm.options 文件，可以通过环境变量覆盖：\nES_JAVA_OPTS=\u0026#34;-Xms4g -Xmx4g\u0026#34; ./bin/easysearch  环境变量的优先级高于 jvm.options 文件中的设置。\n  垃圾回收器 #  Easysearch 使用的垃圾回收器因 JDK 版本而异：\n JDK 8-10：使用 CMS（Concurrent Mark-Sweep） JDK 11+：默认使用 G1GC  # JDK 8-10: CMS 垃圾回收器 8-10:-XX:+UseConcMarkSweepGC 8-10:-XX:CMSInitiatingOccupancyFraction=75 8-10:-XX:+UseCMSInitiatingOccupancyOnly # JDK 11+: G1GC（推荐） 11-:-XX:+UseG1GC 11-:-XX:G1ReservePercent=25 11-:-XX:InitiatingHeapOccupancyPercent=30 \n  参数 说明     8-10:-XX:+UseConcMarkSweepGC JDK 8-10 使用 CMS 垃圾回收器   8-10:-XX:CMSInitiatingOccupancyFraction=75 JDK 8-10：老年代占用 75% 时触发 CMS 回收   11-:-XX:+UseG1GC JDK 11+ 使用 G1GC，支持更大的堆和更稳定的暂停时间   11-:-XX:G1ReservePercent=25 G1GC 预留 25% 堆用于浮动垃圾   11-:-XX:InitiatingHeapOccupancyPercent=30 堆占用 30% 时启动并发回收     ⚠️ 注意：版本号前缀（如 8-10:、11-）表示该参数仅在指定 JDK 版本范围内生效。除非有充分的理由和测试数据，不建议更改默认 GC 配置。\n  OOM 保护 #  # 发生 OutOfMemoryError 时生成堆转储文件 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data # 发生 OOM 时的错误日志路径 -XX:ErrorFile=logs/hs_err_pid%p.log Lucene 性能优化（JDK 9+） #\n # JDK 9+：启用 Lucene 性能优化模块 # jdk.unsupported：提供 sun.misc.Unsafe 用于低级内存操作 # jdk.management：提供扩展的管理接口 9-:--add-modules=jdk.unsupported,jdk.management Vector API 支持（JDK 20+） #  # JDK 20+：启用向量 API（孵化功能）用于 SIMD 支持 20-:--add-modules=jdk.incubator.vector  Vector API 可以加速搜索和 Lucene 索引操作，特别是在向量搜索场景中。\n  OOM 保护（旧语法） #  # 发生 OutOfMemoryError 时生成堆转储文件 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data # 发生 OOM 时立即退出 JVM（让进程管理器自动重启） -XX:+ExitOnOutOfMemoryError \n  参数 说明     -XX:+HeapDumpOnOutOfMemoryError OOM 时自动生成 heap dump 文件，用于事后分析   -XX:HeapDumpPath Heap dump 文件的存储路径（相对于 $ES_HOME）   -XX:+ExitOnOutOfMemoryError OOM 时立即退出 JVM，而不是挂起     GC 日志 #  Easysearch 默认启用 GC 日志，不同 JDK 版本的配置方式不同：\nJDK 8 GC 日志 #  8:-XX:+PrintGCDetails 8:-XX:+PrintGCDateStamps 8:-XX:+PrintTenuringDistribution 8:-XX:+PrintGCApplicationStoppedTime 8:-Xloggc:logs/gc.log 8:-XX:+UseGCLogFileRotation 8:-XX:NumberOfGCLogFiles=32 8:-XX:GCLogFileSize=64m JDK 9+ GC 日志 #  9-:-Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m    参数部分 说明     gc*,gc+age=trace,safepoint 记录 GC 事件、年龄分布、安全点   file=logs/gc.log 输出到 logs/gc.log   utctime,pid,tags 日志中包含 UTC 时间、进程 ID、标签   filecount=32,filesize=64m 最多 32 个轮转文件，每个 64 MB     GC 日志对排查性能问题非常重要，建议保持默认开启。\n  其他常用 JVM 参数 #  临时目录 #  # 临时文件目录 -Djava.io.tmpdir=${ES_TMPDIR} Java Security Manager（JDK 17+） #  # JDK 17+：允许 Java Security Manager 17-:-Djava.security.manager=allow Locale 配置 #  # JDK 9-20：使用 SPI 和系统兼容性 Provider 9-20:-Djava.locale.providers=SPI,COMPAT # JDK 21+：使用 SPI 和 CLDR Provider 21-:-Djava.locale.providers=SPI,CLDR 编译优化（JDK 23+） #\n # JDK 23：修复 MethodHandle 性能问题 23:-XX:CompileCommand=dontinline,java/lang/invoke/MethodHandle.setAsTypeCache 23:-XX:CompileCommand=dontinline,java/lang/invoke/MethodHandle.asTypeUncached Native Access（JDK 19+） #  # JDK 19+：为所有未命名模块启用原生访问 19-:--enable-native-access=ALL-UNNAMED # JDK 22+：禁用向量 API 警告 22-:-Djdk.incubator.vector.disableWarnings=true \n  参数 说明     -Djava.io.tmpdir JVM 临时文件目录   -Djava.security.manager JDK 17+ 的安全管理器（allow 表示允许使用）   -Djava.locale.providers JVM 使用的 Locale 提供者     jvm.options 文件格式 #  jvm.options 文件中每行一个 JVM 参数，支持版本条件：\n# 这是注释 # 无条件参数（所有 JVM 版本生效） -Xms16g -Xmx16g\n# 版本条件参数 8-10:-XX:+UseConcMarkSweepGC # JDK 8~10 11-:-XX:+UseG1GC # JDK 11+ 9-20:-Djava.locale.providers=SPI,COMPAT # JDK 9~20 21-:-Djava.locale.providers=SPI,CLDR # JDK 21+ 版本条件语法 #\n    格式 说明 示例     无前缀 所有版本生效 -Xms16g   N- JDK N 及以上版本 11-:-XX:+UseG1GC (JDK 11+)   N-M JDK N 到 M 版本 8-10:-XX:+UseConcMarkSweepGC (JDK 8-10)     以 # 开头的行是注释。 空行会被忽略。   jvm.options.d 目录 #  除了主 jvm.options 文件外，Easysearch 还会读取 config/jvm.options.d/ 目录下所有以 .options 结尾的文件。每个文件的格式与主 jvm.options 完全相同。\n使用场景 #   自定义参数与默认参数分离：将自定义 JVM 参数放在独立文件中，避免修改默认的 jvm.options 文件，便于升级时保留定制配置 容器化部署：通过挂载配置文件到 jvm.options.d/ 目录，在不修改镜像的情况下调整 JVM 参数 按关注点组织：将不同类型的参数（如 GC、调试、安全）放在不同文件中  示例 #  创建自定义堆内存配置：\n# config/jvm.options.d/heap.options -Xms16g -Xmx16g 创建调试参数文件：\n# config/jvm.options.d/debug.options -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/var/log/easysearch -XX:ErrorFile=/var/log/easysearch/hs_err_pid%p.log 加载顺序 #   先读取 config/jvm.options 主文件 再按文件名字母顺序读取 config/jvm.options.d/*.options 如果同一参数出现多次，后加载的值会覆盖先加载的值   注意：jvm.options.d/ 目录中的文件必须以 .options 为扩展名，其他文件会被忽略。\n  配置示例 #  生产环境（64 GB 服务器，JDK 11+） #  # 堆内存：物理内存的 50%，不超过 31 GB -Xms31g -Xmx31g # G1GC（JDK 11+） 11-:-XX:+UseG1GC 11-:-XX:G1ReservePercent=25 11-:-XX:InitiatingHeapOccupancyPercent=30\n# OOM 保护 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log\n# GC 日志 9-:-Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m\n# 其他 -Djava.io.tmpdir=${ES_TMPDIR} 开发环境（8 GB 笔记本） #\n -Xms1g -Xmx1g 传统 CMS 环境（JDK 8-10，64 GB 服务器） #  -Xms31g -Xmx31g # CMS GC（JDK 8-10） 8-10:-XX:+UseConcMarkSweepGC 8-10:-XX:CMSInitiatingOccupancyFraction=75 8-10:-XX:+UseCMSInitiatingOccupancyOnly\n# OOM 保护 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log \n常见问题 #  启动失败：堆内存分配失败 #  Initial heap size set to an invalid value 通常是因为 -Xms 或 -Xmx 超过了物理内存或操作系统限制。\n频繁 Full GC #   检查堆大小是否足够。 使用 GET _nodes/stats/jvm 查看堆使用率。 分析 GC 日志（logs/gc.log）定位原因。  压缩指针未启用 #  堆大小超过约 32 GB 时压缩指针失效。建议设为 31g 或以下。\n 延伸阅读 #    内存与缓存 — bootstrap.memory_lock 与缓存配置  硬件配置 — 内存规格选型建议  系统调优 — memlock、文件描述符等 OS 参数  ","subcategory":null,"summary":"","tags":null,"title":"JVM 配置","url":"/easysearch/main/docs/deployment/config/node-settings/jvm/"},{"category":null,"content":"Fast Terms 查询 #  Fast Terms 是 Easysearch 提供的高性能 terms 查询插件，专为大规模 terms 过滤场景优化。当需要在查询中使用大量 term 值进行过滤时（如数万甚至数十万个 ID），Fast Terms 可以提供比标准 terms 查询更好的性能。\n适用场景 #   大规模 ID 过滤：根据外部系统提供的大量 ID 列表过滤文档 权限过滤：根据用户可访问的文档 ID 集合进行过滤 黑/白名单：根据预定义的大规模列表进行包含/排除过滤 高基数字段过滤：在高基数数值字段上进行大量值的匹配  工作原理 #  Fast Terms 使用 RoaringBitmap 和零分配哈希（XXHash）等技术对大规模 terms 集合进行压缩和高效匹配，相比标准 terms 查询在以下方面有显著优势：\n   对比项 标准 terms 查询 Fast Terms     内存占用 较高（每个 term 独立存储） 低（Bitmap 压缩）   大规模 terms 性能 随 terms 数量线性下降 保持稳定   缓存效率 一般 高（LZ4 压缩 + 过期缓存）    使用方法 #  Bitmap Value Type #  在 terms 查询中使用 value_type: bitmap 来启用 Fast Terms 优化：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;user_id\u0026#34;: { \u0026#34;value\u0026#34;: [1001, 1002, 1003, 1005, 1008], \u0026#34;value_type\u0026#34;: \u0026#34;bitmap\u0026#34; } } } } 从索引加载 Terms #  可以从另一个索引中动态加载 terms 值：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;user_id\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;allowed_users\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;access_list_1\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;user_ids\u0026#34;, \u0026#34;value_type\u0026#34;: \u0026#34;bitmap\u0026#34; } } } } 重新加载数据 #  Fast Terms 支持通过 API 重新加载缓存的数据：\nPOST _fast_terms/_reload 注意事项 #   字段类型：Fast Terms 主要针对数值类型字段（integer、long 等）优化 插件安装：Fast Terms 作为插件提供，需要在所有数据节点上安装 缓存管理：Fast Terms 使用带过期时间的缓存，可自动释放不再使用的 bitmap 数据 适用规模：当 terms 数量较少（\u0026lt; 100）时，标准 terms 查询即可满足需求，无需使用 Fast Terms  ","subcategory":null,"summary":"","tags":null,"title":"Fast Terms 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/fast-terms/"},{"category":null,"content":"证书管理 #  使用了 cert-manager 进行自动化管理证书，对于过期证书会自动重新颁发。\n在这里我们根据 cert-manager 官方的配置方式配置了3套 Certificate 证书：ca-certificate、easysearch-certs 和 easysearch-admin-certs，分别用于节点间证书、http 访问证书和admin 管理员证书，具体参考下属 yaml 文件，重点需要主要证书的有效期(duration 字段)、更新时间(renewBefore 字段)和 commonName(infinilabs) 字段。\n展开查看完整代码 apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned-issuer namespace: default spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ca-certificate namespace: default spec: secretName: ca-cert duration: 9000h # ~1year renewBefore: 360h # 15d commonName: infinilabs isCA: true privateKey: size: 2048 usages: - digital signature - key encipherment issuerRef: name: selfsigned-issuer --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: ca-issuer namespace: default spec: ca: secretName: ca-cert --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: easysearch-certs namespace: default spec: secretName: easysearch-certs duration: 9000h # ~1year renewBefore: 360h # 15d isCA: false privateKey: size: 2048 algorithm: RSA encoding: PKCS8 dnsNames: - threenodes - threenodes-masters-0 - threenodes-masters-1 - threenodes-masters-2 - threenodes-masters-3 - threenodes-masters-4 - threenodes-bootstrap-0 usages: - signing - key encipherment - server auth - client auth commonName: infinilabs issuerRef: name: ca-issuer --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: easysearch-admin-certs namespace: default spec: secretName: easysearch-admin-certs duration: 9000h # ~1year renewBefore: 360h # 15d isCA: false privateKey: size: 2048 algorithm: RSA encoding: PKCS8 commonName: infinilabs usages: - signing - key encipherment - server auth - client auth issuerRef: name: ca-issuer    可以在证书所在目录查看证书的有效期\nopenssl x509 -in tls.crt -dates -noout notBefore=Feb 23 16:02:03 2024 GMT notAfter=Mar 4 16:02:03 2025 GMT \n","subcategory":null,"summary":"","tags":null,"title":"证书管理","url":"/easysearch/main/docs/deployment/install-guide/operator/cert_manager/"},{"category":null,"content":"聚合场景实践 #  本页不再解释每种聚合的语法，而是按“业务问题”给出几类常见的聚合场景实践。\n语法细节请先阅读前面的聚合基础/桶聚合/指标聚合章节。\n场景一：按状态与应用统计错误率 #  需求：做一个简单的错误率看板，按应用和日志级别统计请求量与错误量。\n思路：\n 先用 bool.filter 限定时间窗口（如最近 15 分钟/1 小时） 使用 terms 按应用分桶 在每个应用桶内，再按日志级别分桶，并计算总数  示例：\nGET /logs-*/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ { \u0026#34;range\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-15m\u0026#34; } } } ] } }, \u0026#34;aggs\u0026#34;: { \u0026#34;by_app\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;app_name.keyword\u0026#34;, \u0026#34;size\u0026#34;: 20 }, \u0026#34;aggs\u0026#34;: { \u0026#34;by_level\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;log_level.keyword\u0026#34; } } } } } } 可以在应用层根据各级别计数计算错误率，或继续在子聚合中添加 filter 聚合专门统计错误级别。\n场景二：时间序列指标折线图（分服务） #  需求：在监控看板上画出每个服务的 QPS 或平均延迟随时间变化的曲线。\n思路：\n 外层用 date_histogram 按时间切分（如 1 分钟/5 分钟） 内层用 terms 按服务名分桶 在每个子桶内计算 sum 或 avg  示例（按服务统计平均延迟）：\nGET /apm-metrics-*/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ { \u0026#34;range\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-1h\u0026#34; } } } ] } }, \u0026#34;aggs\u0026#34;: { \u0026#34;per_minute\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;fixed_interval\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;by_service\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;service.name.keyword\u0026#34;, \u0026#34;size\u0026#34;: 10 }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_latency\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;latency_ms\u0026#34; } } } } } } } } 场景三：价格分布与直方图 #  需求：电商类场景中，查看某类商品的价格分布情况（例如画价格分布条形图）。\n思路：\n 使用 histogram 或 range 聚合按价格分桶 在每个价格桶内统计文档数量  示例：\nGET /products/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;category.keyword\u0026#34;: \u0026#34;shoes\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;price_ranges\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;to\u0026#34;: 100 }, { \u0026#34;from\u0026#34;: 100, \u0026#34;to\u0026#34;: 300 }, { \u0026#34;from\u0026#34;: 300, \u0026#34;to\u0026#34;: 1000 }, { \u0026#34;from\u0026#34;: 1000 } ] } } } } 可以根据实际业务调整区间划分粒度。\n场景四：UV/去重计数（cardinality） #  需求：统计活跃用户数、访问 IP 数等，需要对某个字段做去重计数。\n思路：\n 使用 cardinality 指标聚合对用户 ID 或 IP 字段做近似去重计数 可以结合 date_histogram 做时间序列 UV 曲线  示例（最近 1 天 UV）：\nGET /web-logs-*/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-1d\u0026#34; } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;uv\u0026#34;: { \u0026#34;cardinality\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;user_id.keyword\u0026#34;, \u0026#34;precision_threshold\u0026#34;: 40000 } } } } precision_threshold 越大，精度越高但内存占用越多。\n场景五：TopN 排行榜（terms + 排序） #  需求：统计一段时间内访问量最高的页面、下单量最高的商品等。\n思路：\n 使用 terms 聚合按对象 ID 或名称分桶 通过 order 按子聚合（如 sum 或 count）排序  示例（下单量 TopN 商品）：\nGET /orders-*/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;order_time\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-7d\u0026#34; } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;top_products\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;product_id\u0026#34;, \u0026#34;size\u0026#34;: 20, \u0026#34;order\u0026#34;: { \u0026#34;total_qty\u0026#34;: \u0026#34;desc\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;total_qty\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;quantity\u0026#34; } } } } } } 场景六：结合 Geo 的热点分析与附近搜索 #  Geo 更详细的配方在 地理位置场景实践 中，这里挑几个和聚合结合较多的典型场景。\n6.1 门店热力图（geohash_grid） #  需求：在地图上展示“哪里门店/打车点最密集”，并按区域统计单量或客单价。\n思路：\n 使用 geohash_grid 或 geotile_grid 聚合，将坐标点聚合到网格； 在每个网格桶内再计算业务指标（如门店数、订单数、平均价格）。  示例：\nGET /stores/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 5 }, \u0026#34;aggs\u0026#34;: { \u0026#34;store_count\u0026#34;: { \u0026#34;value_count\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;id\u0026#34; } }, \u0026#34;avg_order_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;avg_order_price\u0026#34; } } } } } } 前端可以基于 grid 的 buckets 绘制热力图或气泡图。\n6.2 “附近 N 公里”内的指标（geo_distance + 聚合） #  需求：以用户当前位置为圆心，统计 3km / 5km / 10km 内的门店数量、平均评分等。\n思路：\n 查询阶段用 geo_distance 过滤一个最大半径（如 10km）； 聚合阶段用 geo_distance 桶聚合划分距离区间，在每个距离桶内再做指标聚合。  示例：\nGET /stores/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ { \u0026#34;geo_distance\u0026#34;: { \u0026#34;distance\u0026#34;: \u0026#34;10km\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 31.2304, \u0026#34;lon\u0026#34;: 121.4737 } } } ] } }, \u0026#34;aggs\u0026#34;: { \u0026#34;by_distance\u0026#34;: { \u0026#34;geo_distance\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;origin\u0026#34;: { \u0026#34;lat\u0026#34;: 31.2304, \u0026#34;lon\u0026#34;: 121.4737 }, \u0026#34;unit\u0026#34;: \u0026#34;km\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;to\u0026#34;: 3 }, { \u0026#34;from\u0026#34;: 3, \u0026#34;to\u0026#34;: 5 }, { \u0026#34;from\u0026#34;: 5, \u0026#34;to\u0026#34;: 10 } ] }, \u0026#34;aggs\u0026#34;: { \u0026#34;store_count\u0026#34;: { \u0026#34;value_count\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;id\u0026#34; } }, \u0026#34;avg_rating\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;rating\u0026#34; } } } } } } 6.3 按时间 + 地理维度做统计（date_histogram + geohash_grid） #  需求：查看最近 7 天内，不同区域的订单量变化趋势（例如画“时间 × 区域”的热力矩阵）。\n思路：\n 外层 date_histogram 按天切时间； 内层 geohash_grid（或按行政区域 terms）分地理区域； 在每个子桶内统计订单笔数或 GMV。  示例（时间 × geohash 网格的订单量）：\nGET /orders-*/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;order_time\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-7d\u0026#34; } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;per_day\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;order_time\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;1d\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;by_area\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;store_location\u0026#34;, \u0026#34;precision\u0026#34;: 4 }, \u0026#34;aggs\u0026#34;: { \u0026#34;order_count\u0026#34;: { \u0026#34;value_count\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;order_id\u0026#34; } } } } } } } } 这类结果可以在前端渲染成“按天 × 区域”的二维矩阵，帮助定位某些时间段、某些区域的异常波动。\n参考手册（API 与参数） #    聚合概览（功能手册）  桶聚合（功能手册）  指标聚合（功能手册）  管道聚合（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"聚合场景实践","url":"/easysearch/main/docs/features/aggregations/aggs-recipes/"},{"category":null,"content":"安全配置 #  本页介绍 easysearch.yml 中与安全模块相关的配置项，包括 TLS 加密、审计日志、节点证书、REST API 权限等。这些都是静态设置，修改后需要重启节点生效。\n 用户、角色、权限等安全管理操作请参见 安全 API。 详细的 TLS 证书管理请参见 TLS 安全配置。 国密算法配置请参见 国密配置。\n  安全模块开关 #  security.enabled #  security.enabled: true    项目 说明     参数 security.enabled   默认值 true   属性 静态   说明 是否启用安全模块。启用后所有 API 请求需要认证，节点间通信需要 TLS 加密     生产环境必须启用安全模块。关闭安全意味着任何人都可以读写集群数据。\n  审计日志 #  security.audit.type #  security.audit.type: noop    项目 说明     参数 security.audit.type   默认值 noop（发行包默认配置）   属性 静态   说明 审计日志类型。noop 表示不记录审计日志，internal 表示记录到 Easysearch 索引中       值 行为     noop 不记录审计日志（默认）   internal 将审计事件记录到 Easysearch 内部索引   log4j 将审计事件输出到 log4j 日志文件   webhook 将审计事件发送到 Webhook 端点   debug 调试模式，仅在开发环境使用     Transport 层 TLS #  Transport 层用于节点间内部通信。安全模块启用时，Transport TLS 必须配置。\n   参数 说明     security.ssl.transport.cert_file 节点证书文件（PEM 格式），相对于 config/ 目录   security.ssl.transport.key_file 节点私钥文件（PEM 格式）   security.ssl.transport.ca_file CA 根证书文件，用于验证其他节点的证书   security.ssl.transport.skip_domain_verify 是否跳过证书域名验证。true 表示只验证证书签发链，不验证 CN/SAN    security.ssl.transport.cert_file: instance.crt security.ssl.transport.key_file: instance.key security.ssl.transport.ca_file: ca.crt security.ssl.transport.skip_domain_verify: true 证书要求 #   每个节点都需要有自己的证书和私钥。 所有节点的证书必须由同一个 CA 签发（或证书链中可追溯到同一根 CA）。 证书文件路径相对于 config/ 目录。   HTTP 层 TLS #  HTTP 层用于客户端（REST API）通信。启用后客户端需要使用 https:// 连接。\n   参数 默认值 说明     security.ssl.http.enabled true 是否启用 HTTPS（初始化后默认开启）   security.ssl.http.cert_file — HTTP 层证书文件   security.ssl.http.key_file — HTTP 层私钥文件   security.ssl.http.ca_file — CA 根证书文件   security.ssl.http.clientauth_mode OPTIONAL 客户端证书认证模式    security.ssl.http.enabled: true security.ssl.http.cert_file: instance.crt security.ssl.http.key_file: instance.key security.ssl.http.ca_file: ca.crt security.ssl.http.clientauth_mode: OPTIONAL clientauth_mode 说明 #     值 行为     NONE 不要求客户端证书   OPTIONAL 客户端可以提供证书，但不强制（默认）   REQUIRE 强制要求客户端提供证书（双向 TLS / mTLS）     通常使用 OPTIONAL，仅在需要 mTLS 的高安全场景使用 REQUIRE。\n  节点 DN（Distinguished Name） #  security.nodes_dn #  security.nodes_dn: - \u0026#34;CN=node-1.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026#34; - \u0026#34;CN=node-2.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026#34; - \u0026#34;CN=node-3.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026#34;    项目 说明     参数 security.nodes_dn   默认值 空列表   属性 静态   说明 允许加入集群的节点证书 DN 列表。只有证书 DN 匹配此列表的节点才能加入集群。支持通配符    通配符示例：\nsecurity.nodes_dn: - \u0026#34;CN=*.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026#34; 查看证书 DN #  openssl x509 -in instance.crt -subject -noout  Admin DN #  security.authcz.admin_dn #  security.authcz.admin_dn: - \u0026#34;CN=admin.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026#34;    项目 说明     参数 security.authcz.admin_dn   默认值 空列表   属性 静态   说明 Admin 客户端证书 DN 列表。持有这些 DN 证书的客户端拥有完全的集群管理权限，可以执行安全配置变更等特权操作     Admin 证书用于运行 securityadmin.sh 或其他需要特权访问的管理工具。\n  REST API 配置 #  security.restapi.roles_enabled #  security.restapi.roles_enabled: [\u0026#34;superuser\u0026#34;, \u0026#34;security_rest_api_access\u0026#34;]    项目 说明     参数 security.restapi.roles_enabled   默认值 —   属性 静态   说明 允许访问安全 REST API（_security/*）的角色列表。只有拥有这些角色的用户才能通过 API 管理安全配置     安全索引初始化 #  security.allow_default_init_securityindex #  security.allow_default_init_securityindex: false    项目 说明     参数 security.allow_default_init_securityindex   默认值 false   属性 静态   说明 是否允许在首次启动时自动初始化安全索引（.security 系统索引）。设为 true 时集群启动时会自动创建安全索引并初始化默认配置     系统索引保护 #  security.system_indices #  security.system_indices.enabled: true security.system_indices.indices: [\u0026#34;.infini-*\u0026#34;]    参数 默认值 说明     security.system_indices.enabled true 是否启用系统索引保护（初始化后默认开启）   security.system_indices.indices — 受保护的系统索引模式列表。匹配的索引只能通过系统内部访问，普通用户无法直接读写     security 目录配置文件 #  除了 easysearch.yml 中的安全配置，config/security/ 目录还包含安全模块的本地配置文件：\n   文件 说明     user.yml 内置用户定义（密码哈希由初始化脚本生成）   roles.yml 角色定义   roles_mapping.yml 角色映射（用户/后端角色 → 安全角色）   action_groups.yml 权限组定义   tenants.yml 租户定义   config.yml 认证与授权后端配置（LDAP、SAML 等）     这些文件在 bin/initialize.sh 初始化时自动生成。\n  完整配置示例 #  标准生产环境 #  # ---- 安全模块 ---- security.enabled: true security.audit.type: noop # \u0026mdash;- Transport TLS \u0026mdash;- security.ssl.transport.cert_file: instance.crt security.ssl.transport.key_file: instance.key security.ssl.transport.ca_file: ca.crt security.ssl.transport.skip_domain_verify: true\n# \u0026mdash;- HTTP TLS \u0026mdash;- security.ssl.http.enabled: true security.ssl.http.cert_file: instance.crt security.ssl.http.key_file: instance.key security.ssl.http.ca_file: ca.crt security.ssl.http.clientauth_mode: OPTIONAL\n# \u0026mdash;- 访问控制 \u0026mdash;- security.allow_default_init_securityindex: true security.nodes_dn:\n \u0026quot;CN=*.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026quot; security.authcz.admin_dn: \u0026quot;CN=admin.infini.cloud,OU=UNIT,O=ORG,L=NI,ST=FI,C=IN\u0026quot; security.restapi.roles_enabled: [\u0026quot;superuser\u0026quot;, \u0026quot;security_rest_api_access\u0026quot;]  # \u0026mdash;- 系统索引保护 \u0026mdash;- security.system_indices.enabled: true security.system_indices.indices: [\u0026quot;.infini-*\u0026quot;] \n延伸阅读 #    TLS 安全配置 — 详细的证书生成与管理  国密配置 — SM2/SM3/SM4 国密 TLS  安全 API — 用户、角色、权限管理  ","subcategory":null,"summary":"","tags":null,"title":"安全配置","url":"/easysearch/main/docs/deployment/config/node-settings/security-settings/"},{"category":null,"content":"Helm Chart 部署 #  INFINI Easysearch 从 1.5.0 版本开始支持 Helm Chart 方式部署。\n仓库信息 #  INFINI Easysearch Helm Chart 仓库地址: https://helm.infinilabs.com。\n可以使用以下命令添加仓库\nhelm repo add infinilabs https://helm.infinilabs.com 依赖项 #   StorageClass  INFINI Easysearch Helm Chart 包中默认使用 local-path 进行数据持久化存储，可参考 local-path官方文档进行安装。\n如果使用其他 StorageClass，请修改 Chart 包中的 storageClassName: local-path配置项。\n Secret  INFINI Easysearch Helm Chart 默认使用 cert-manager 进行自签 CA 证书创建及分发, 可参考 cert-manager 官方文档进行安装。\n安装示例 #  cat \u0026lt;\u0026lt; EOF | kubectl apply -n \u0026lt;namespace\u0026gt; -f - apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: easysearch-ca-issuer spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: easysearch-ca-certificate spec: commonName: easysearch-ca-certificate duration: 87600h0m0s isCA: true issuerRef: kind: Issuer name: easysearch-ca-issuer privateKey: algorithm: ECDSA size: 256 renewBefore: 2160h0m0s secretName: easysearch-ca-secret EOF #安装 INFINI Easysearch, 默认单节点 helm install easysearch infinilabs/easysearch -n \u0026lt;namespace\u0026gt; \u0026ndash;create-namespace 卸载 #\n helm uninstall easysearch -n \u0026lt;namespace\u0026gt; kubectl delete pvc easysearch-data-easysearch-0 easysearch-config-easysearch-0 -n \u0026lt;namespace\u0026gt;  更多信息请参考技术博客， 通过 Helm Chart 部署 Easysearch\n ","subcategory":null,"summary":"","tags":null,"title":"Helm Chart","url":"/easysearch/main/docs/deployment/install-guide/helm/"},{"category":null,"content":"English Morphology 分词过滤器 #  english_morphology 分词过滤器是基于 Lucene Morphology 库的语言处理组件。与传统的算法分词器不同，它不依赖于简单的字符裁剪规则，而是通过语言学词典对单词进行深度的词形还原（Lemmatization）。\n相关指南（先读这些） #    文本分析：词干提取  文本分析：规范化  核心处理逻辑 #  该过滤器在处理文本时遵循“字典比对 + 语义还原”的原则，其核心逻辑包括：\n 屈折变化还原 (Inflectional)：处理动词时态（went → go）、名词单复数（children → child）等。 派生词识别 (Derivational)：识别单词间的词根关联，如将“执行者名词”关联至“动作动词”（runner → run）。 多路径索引 (Token Expansion)：当一个单词具有多重身份时，同时保留原词和还原后的原型（如 running → running, run）。  安装与使用 #  详见 英语形态分词器 部分。\n","subcategory":null,"summary":"","tags":null,"title":"英语形态分词过滤器（English Morphology）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/english-morphology/"},{"category":null,"content":"Russian Morphology 分词过滤器 #  russian_morphology 分词过滤器基于 Lucene Morphology 库，专门用于解决俄语搜索中\u0026quot;搜不全\u0026quot;和\u0026quot;语义偏差\u0026quot;的痛点。俄语由于其极其复杂的**变格（Declension）和变位（Conjugation）**系统，是形态分析最具挑战性的语言之一。\n相关指南（先读这些） #    文本分析：词干提取  文本分析：规范化  核心处理逻辑 #  该过滤器在处理俄语文本时具备以下核心能力：\n 变格还原：处理名词、形容词、代词的 6 个格位及单复数变化。  示例：автомобили (复数) 和 автомобилем (单数工具格) 都会还原为 автомоль (汽车)。   动词变位还原：处理动词的人称、时态和语气。  示例：бежал (跑/过去时) 还原为 бежать (跑/原型)。   多路径歧义处理 (Homonymy)：当一个词形对应多个原型时，同时索引它们以防漏搜。  示例：Мире 会同时产生 мир (世界/和平) 和 миро (圣油)。    俄语形态分析的必要性 #  与英语不同，俄语的一个单词根据格、性、数、时态的变化，可能会产生数十种不同的拼写形式。\n 普通分词器（如 Snowball）：只能简单地去掉词尾（如 -ом, -ами），经常导致词根被错误切分。 形态过滤器 (russian_morphology)：通过查阅俄语语言学词典，将所有变形统一回归到其 第一格（Nominative）或动词不定式（Infinitive） 原型。  安装与使用 #  详见 俄语形态分词器 部分。\n","subcategory":null,"summary":"","tags":null,"title":"俄语形态分词过滤器（Russian Morphology）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/russian-morphology/"},{"category":null,"content":"Keystore 安全设置 #  Easysearch 的 keystore 是一个加密的安全存储，用于保存敏感配置项（如密码、密钥、凭据等）。使用 keystore 可以避免将敏感信息以明文形式写在 easysearch.yml 配置文件中。\n为什么使用 Keystore #     方式 安全性 说明     easysearch.yml ⚠️ 明文 配置文件中的密码任何有文件读取权限的用户都能看到   环境变量 ⚠️ 明文 环境变量可能在进程列表、日志中暴露   Keystore ✅ 加密 AES-GCM 加密存储，可选密码保护    适合存入 keystore 的设置示例：\n S3 快照仓库的 access_key 和 secret_key LDAP 绑定密码 安全插件的管理员密码 任何包含密码或密钥的配置项  Keystore 文件 #  keystore 文件名为 easysearch.keystore，存放在配置目录（$ES_PATH_CONF）下。文件权限自动设置为 rw-rw----（660）。\n创建 Keystore #  bin/easysearch-keystore create 创建带密码保护的 keystore：\nbin/easysearch-keystore create -p 系统会提示输入密码。设置密码后，每次启动 Easysearch 时需要输入 keystore 密码。\n 注意：如果 keystore 已存在，create 命令会提示是否覆盖。\n 添加设置 #  添加字符串设置 #  交互式输入（推荐，密码不会显示在终端历史中）：\nbin/easysearch-keystore add s3.client.default.access_key 系统会提示输入值。\n从标准输入读取（适合自动化脚本）：\necho \u0026#34;AKIAIOSFODNN7EXAMPLE\u0026#34; | bin/easysearch-keystore add --stdin s3.client.default.access_key 强制覆盖已有设置（不提示确认）：\nbin/easysearch-keystore add --force s3.client.default.access_key 添加文件设置 #  将文件内容作为设置值存入 keystore：\nbin/easysearch-keystore add-file my.certificate /path/to/cert.pem 查看已存储的设置 #  列出 keystore 中所有设置的名称（不显示值）：\nbin/easysearch-keystore list 输出示例：\nkeystore.seed s3.client.default.access_key s3.client.default.secret_key  注意：keystore.seed 是系统自动生成的随机种子，用于内部安全功能，请勿删除。\n 移除设置 #  bin/easysearch-keystore remove s3.client.default.access_key 修改 Keystore 密码 #  bin/easysearch-keystore passwd 系统会提示输入当前密码和新密码。\n检查是否有密码保护 #  bin/easysearch-keystore has-passwd 如果 keystore 有密码保护，命令返回退出码 0；否则返回 1。\n升级 Keystore 格式 #  bin/easysearch-keystore upgrade 将 keystore 升级到最新格式版本。通常在 Easysearch 版本升级后执行。\n重新加载安全设置 #  修改 keystore 中的设置后，不需要重启整个 Easysearch 节点。可以使用 _nodes/reload_secure_settings API 在线重新加载：\nPOST _nodes/reload_secure_settings { \u0026#34;secure_settings_password\u0026#34;: \u0026#34;keystore密码\u0026#34; } 如果 keystore 没有设置密码，请求体可以为空：\nPOST _nodes/reload_secure_settings {} 响应会显示每个节点的重新加载状态：\n{ \u0026#34;cluster_name\u0026#34;: \u0026#34;my_cluster\u0026#34;, \u0026#34;nodes\u0026#34;: { \u0026#34;node_id_1\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;node-1\u0026#34;, \u0026#34;reload_exception\u0026#34;: null } } }  注意：并非所有设置都支持在线重新加载。不支持的设置仍需要重启节点。\n 常见用法 #  S3 快照仓库凭据 #  # 添加 S3 访问密钥 bin/easysearch-keystore add s3.client.default.access_key bin/easysearch-keystore add s3.client.default.secret_key # 如果使用临时凭据，还需添加 session token bin/easysearch-keystore add s3.client.default.session_token\n# 重新加载设置（无需重启） curl -X POST \u0026quot;https://localhost:9200/_nodes/reload_secure_settings\u0026quot;  -H \u0026quot;Content-Type: application/json\u0026quot;  -d '{}'  \u0026ndash;cacert config/certs/root-ca.pem  -u admin:密码 Docker 环境中使用 Keystore #\n 在 Docker 中，可以通过环境变量传入 keystore 密码：\nenvironment: - KEYSTORE_PASSWORD=my_keystore_password 或通过文件传入（更安全，配合 Docker Secrets）：\nenvironment: - KEYSTORE_PASSWORD_FILE=/run/secrets/keystore_password secrets: keystore_password: file: ./keystore_password.txt 技术细节 #     属性 说明     加密算法 AES/GCM/NoPadding   密钥派生 PBKDF2WithHmacSHA512（10,000 次迭代）   密钥长度 128 位   盐长度 64 字节   IV 长度 12 字节（96 位）   设置名格式 字母、数字、下划线、连字符、点号（[A-Za-z0-9_\\-.]+）   文件权限 rw-rw----（660）    ","subcategory":null,"summary":"","tags":null,"title":"Keystore 安全设置","url":"/easysearch/main/docs/deployment/config/keystore/"},{"category":null,"content":"环境变量参考 #  Easysearch 支持通过环境变量控制启动行为、JVM 设置和运行路径等。本页提供所有支持的环境变量的完整参考。\n核心环境变量 #     变量 说明 默认值     ES_HOME Easysearch 安装根目录 从启动脚本位置自动推断   ES_PATH_CONF 配置文件目录路径 $ES_HOME/config   ES_JAVA_HOME 自定义 Java 安装路径（推荐使用此变量） 未设置（优先使用内置 JDK）   JAVA_HOME Java 安装路径（后备方案，ES_JAVA_HOME 优先） 系统默认   ES_JAVA_OPTS 附加 JVM 选项，如堆大小 空   ES_TMPDIR 临时文件目录 自动创建   ES_STARTUP_SLEEP_TIME 后台启动后的等待时间（秒） 未设置    JDK 选择优先级 #  Easysearch 按以下优先级选择 Java 运行环境：\n1. 内置 JDK（$ES_HOME/jdk） ← 最优先，推荐使用 2. ES_JAVA_HOME 环境变量 ← 无内置 JDK 时使用 3. JAVA_HOME 环境变量 ← 最终后备方案  建议：使用 Easysearch 自带的内置 JDK，除非有特殊需求需要使用其他 Java 版本。\n 路径相关 #  ES_PATH_CONF #  指定配置文件目录，该目录下需包含 easysearch.yml、jvm.options、log4j2.properties 等配置文件。\n# 使用自定义配置目录 export ES_PATH_CONF=/etc/easysearch bin/easysearch 或在启动命令中内联设置：\nES_PATH_CONF=/opt/config bin/easysearch -d ES_TMPDIR #  Easysearch 启动时会在 $ES_TMPDIR（或系统临时目录）下创建一个专用的临时目录。如果需要指定临时文件位置：\nexport ES_TMPDIR=/data/easysearch/tmp JVM 相关 #  ES_JAVA_OPTS #  覆盖或追加 JVM 选项。最常见的用途是设置堆大小：\nexport ES_JAVA_OPTS=\u0026#34;-Xms8g -Xmx8g\u0026#34; bin/easysearch  注意：ES_JAVA_OPTS 中的值会追加到 jvm.options 文件中的设置之后，因此可以覆盖 jvm.options 中的同名选项。\n 被忽略的变量 #  以下 Java 相关环境变量会被 Easysearch 启动脚本主动忽略，并输出警告信息：\n   变量 处理方式 替代方案     JAVA_TOOL_OPTIONS 取消设置，输出警告 使用 ES_JAVA_OPTS   JAVA_OPTS 忽略，输出警告 使用 ES_JAVA_OPTS    在配置文件中引用环境变量 #  easysearch.yml 支持使用 ${...} 语法引用环境变量的值：\n# 使用环境变量设置集群名称 cluster.name: ${CLUSTER_NAME} # 使用环境变量设置网络地址 network.host: ${ES_NETWORK_HOST}\n# 带默认值（如果环境变量未设置，使用默认值） node.name: ${HOSTNAME:my-node} 使用示例：\nexport CLUSTER_NAME=production export ES_NETWORK_HOST=0.0.0.0 bin/easysearch  提示：任何自定义环境变量都可以在 easysearch.yml 中通过 ${VAR_NAME} 引用，不限于以 ES_ 开头的变量。\n Docker 环境特有变量 #  在 Docker 部署模式下，Easysearch 支持一些额外的环境变量和特殊行为：\n   变量 说明     EASYSEARCH_INITIAL_ADMIN_PASSWORD 初始管理员密码   ELASTIC_PASSWORD elastic 用户密码   ELASTIC_PASSWORD_FILE 从文件读取 elastic 密码（与 ELASTIC_PASSWORD 互斥）   KEYSTORE_PASSWORD Keystore 密码   KEYSTORE_PASSWORD_FILE 从文件读取 keystore 密码（与 KEYSTORE_PASSWORD 互斥）    Docker 中的自动设置转换 #  在 Docker 部署中，任何包含点号的环境变量会自动转换为 -E 命令行参数：\n# Docker 环境变量 cluster.name=my_cluster network.host=0.0.0.0 discovery.type=single-node # 自动转换为启动参数 bin/easysearch -Ecluster.name=my_cluster -Enetwork.host=0.0.0.0 -Ediscovery.type=single-node Docker Compose 示例：\nservices: easysearch: image: infinilabs/easysearch:latest environment: - cluster.name=my_cluster - node.name=node-1 - discovery.type=single-node - EASYSEARCH_INITIAL_ADMIN_PASSWORD=my_password - ES_JAVA_OPTS=-Xms4g -Xmx4g _FILE 后缀变量 #  支持 _FILE 后缀的变量用于从文件读取敏感信息（配合 Docker Secrets 使用）：\nservices: easysearch: environment: - ELASTIC_PASSWORD_FILE=/run/secrets/elastic_password secrets: - elastic_password secrets: elastic_password: file: ./elastic_password.txt  要求：_FILE 指向的文件权限必须为 400 或 600，且与非 _FILE 变体互斥。\n 系统属性 #  Easysearch 启动时会自动设置以下 Java 系统属性：\n   系统属性 来源 说明     es.path.home $ES_HOME 安装根目录   es.path.conf $ES_PATH_CONF 配置目录   es.distribution.flavor 构建时确定 发行版类型   es.distribution.type 构建时确定 安装方式（tar/docker/rpm）   es.bundled_jdk 构建时确定 是否包含内置 JDK    架构相关行为 #     架构 特殊行为     arm64 / aarch64 JVM 启动参数包含 -Xshare:off（禁用 CDS）   其他架构 JVM 使用 -Xshare:auto（自动使用 CDS，映射失败时回退）    配置方式总览 #  Easysearch 支持多种配置方式，适用于不同场景：\n   方式 适用场景 持久性 安全性     easysearch.yml 固定配置（路径、网络、发现） ✅ 持久 ⚠️ 明文   -E 命令行参数 临时覆盖、测试 ❌ 临时 ⚠️ 进程列表可见   环境变量 + ${...} 容器化部署、不同环境差异化 ❌ 临时 ⚠️ 明文    Keystore 敏感信息（密码、密钥） ✅ 持久 ✅ 加密   jvm.options / jvm.options.d/ JVM 参数 ✅ 持久 N/A   _cluster/settings API 动态集群设置 ✅/❌ N/A    ","subcategory":null,"summary":"","tags":null,"title":"环境变量参考","url":"/easysearch/main/docs/deployment/config/environment-variables/"},{"category":null,"content":"简繁转换过滤器 #  stconvert 词元过滤器在简体中文和繁体中文之间进行转换，来自 analysis-stconvert 插件。\n前提条件 #  需要安装 analysis-stconvert 插件：\nbin/easysearch-plugin install analysis-stconvert 功能说明 #  此过滤器支持：\n 简体 → 繁体（s2t） 繁体 → 简体（t2s）  可用于实现跨简繁的统一搜索。\n使用示例 #  PUT my-stconvert-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;to_simplified\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stconvert\u0026#34;, \u0026#34;convert_type\u0026#34;: \u0026#34;t2s\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;unified_chinese\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;to_simplified\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;filter\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;stconvert\u0026#34;, \u0026#34;convert_type\u0026#34;: \u0026#34;t2s\u0026#34;}], \u0026#34;text\u0026#34;: \u0026#34;計算機程式設計\u0026#34; } 响应：计算机 程式 设计\n参数 #     参数 默认值 说明     convert_type s2t 转换方向：s2t（简→繁）或 t2s（繁→简）   delimiter , 多音字拼音分隔符   keep_both false 是否同时保留简体和繁体    相关链接 #    简繁转换分析器  简繁转换分词器  ","subcategory":null,"summary":"","tags":null,"title":"简繁转换过滤器（STConvert Filter）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stconvert-filter/"},{"category":null,"content":"版本升级 #  Easysearch Operator 支持通过修改 YAML 配置实现滚动升级，升级过程中集群保持可用。\n操作步骤 #   查看当前版本  kubectl get pods -l app=easysearch -o jsonpath=\u0026#39;{.items[0].spec.containers[0].image}\u0026#39; 修改 Operator YAML 中的版本字段  # 修改前 version: \u0026#34;1.7.0-223\u0026#34; # 修改后 version: \u0026quot;1.7.1-225\u0026quot; 其他配置保持不变：\nhttpPort: 9200 vendor: Easysearch serviceAccount: controller-manager serviceName: threenodes 应用修改  kubectl apply -f easysearch-cluster.yaml 滚动升级流程 #  为保证升级过程中的服务可用性，Operator 采用滚动升级方式：\n 从 threenodes-masters-0 开始升级 等待该节点完全就绪后，升级 threenodes-masters-1 依次滚动，直到所有节点升级完毕   整个升级过程耗时约 10 分钟（3 节点集群），具体取决于数据量和分片恢复速度。\n 验证升级结果  # 进入容器验证版本 kubectl exec -it threenodes-masters-0 -- curl -ku admin:PASSWORD https://localhost:9200 确认 version.number 已更新到目标版本。\n操作演示 #    autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  ","subcategory":null,"summary":"","tags":null,"title":"版本升级","url":"/easysearch/main/docs/deployment/install-guide/operator/upgrade/"},{"category":null,"content":"拼音过滤器 #  pinyin 词元过滤器将中文词元转换为拼音表示，来自 analysis-pinyin 插件。\n前提条件 #  需要安装 analysis-pinyin 插件：\nbin/easysearch-plugin install analysis-pinyin 功能说明 #  此过滤器可以：\n 将汉字转换为拼音全拼或首字母 保留或移除原始中文词元 支持多种输出格式组合  使用示例 #  PUT my-pinyin-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_pinyin\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pinyin\u0026#34;, \u0026#34;keep_full_pinyin\u0026#34;: true, \u0026#34;keep_first_letter\u0026#34;: true, \u0026#34;keep_original\u0026#34;: true, \u0026#34;limit_first_letter_length\u0026#34;: 16 } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;pinyin_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;my_pinyin\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;filter\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;pinyin\u0026#34;, \u0026#34;keep_full_pinyin\u0026#34;: true}], \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国\u0026#34; } 参数 #     参数 默认值 说明     keep_first_letter true 保留拼音首字母（如 \u0026ldquo;中国\u0026rdquo; → zg）   keep_full_pinyin true 保留全拼（如 \u0026ldquo;中\u0026rdquo; → zhong）   keep_joined_full_pinyin false 连接全拼（如 \u0026ldquo;中国\u0026rdquo; → zhongguo）   keep_original false 保留原始中文词元   keep_none_chinese true 保留非中文字符   keep_none_chinese_in_first_letter true 首字母模式中保留非中文   keep_none_chinese_together true 非中文字符连续保留   none_chinese_pinyin_tokenize true 拼音字母也参与分词   limit_first_letter_length 16 首字母最大长度   lowercase true 拼音小写   trim_whitespace true 移除空白   remove_duplicated_term false 移除重复词项    相关链接 #    拼音分析器  拼音分词器  ","subcategory":null,"summary":"","tags":null,"title":"拼音过滤器（Pinyin Filter）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/pinyin-filter/"},{"category":null,"content":"建议与纠错 #  Easysearch 提供三种 Suggester（建议器），覆盖\u0026quot;拼写纠正→短语纠正→前缀补全\u0026quot;三个典型场景：\n   Suggester 用途 工作方式     Term Suggester 单词拼写纠正 基于编辑距离，对每个词项找索引中相似的词   Phrase Suggester 短语级纠正（\u0026ldquo;Did you mean …?\u0026quot;） 使用 N-gram 语言模型对整个短语做纠正   Completion Suggester 前缀自动补全 基于 FST 数据结构常驻内存，毫秒级响应    所有 Suggester 通过 _search 请求的 suggest 参数调用，可以与 query 同时使用。\nTerm Suggester（单词纠正） #  对单个词项，基于编辑距离在索引中查找相似候选词。\nGET /articles/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;spell-check\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;quer\u0026#34;, \u0026#34;term\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;content\u0026#34;, \u0026#34;suggest_mode\u0026#34;: \u0026#34;popular\u0026#34; } } } } 返回示例：\n{ \u0026#34;suggest\u0026#34;: { \u0026#34;spell-check\u0026#34;: [ { \u0026#34;text\u0026#34;: \u0026#34;quer\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;text\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;score\u0026#34;: 0.75, \u0026#34;freq\u0026#34;: 128 }, { \u0026#34;text\u0026#34;: \u0026#34;quer\u0026#34;, \u0026#34;score\u0026#34;: 0.5, \u0026#34;freq\u0026#34;: 2 } ] } ] } } Term Suggester 参数 #     参数 类型 默认值 说明     field String 必填 从哪个字段获取候选词   suggest_mode String missing missing：仅在原词不存在时建议；popular：建议频率更高的词；always：总是返回建议   accuracy Float 0.5 候选词的最低相似度阈值（0.0~1.0）   sort String score score：先按相似度排序；frequency：先按词频排序   max_edits Integer 2 最大编辑距离（只能是 1 或 2）   prefix_length Integer 1 前缀必须匹配的字符数，值越大候选越少但速度越快   min_word_length Integer 4 原词长度低于此值时不返回建议   min_doc_freq Float/Integer 0 候选词的最低文档频率（绝对值或比例），过滤罕见词   max_term_freq Float 0.01 原词频率高于此比例时不返回建议（高频词一般无需纠正）   string_distance String internal 相似度算法：internal、damerau_levenshtein、levenshtein、jaro_winkler、ngram   max_inspections Integer 5 检查候选词的倍数因子（越大结果越好但越慢）     建议：在搜索结果为空或很少时触发 Term Suggester，搭配 suggest_mode: \u0026quot;popular\u0026quot; 能给出更有价值的建议。\n Phrase Suggester（短语纠正） #  对整个短语做语言模型级纠正，实现\u0026quot;您是不是要找：xxx？\u0026ldquo;功能。相比 Term Suggester，Phrase Suggester 考虑词与词之间的关系。\n前提：需要用 shingle（词级 N-gram）过滤器建立索引：\nPUT /articles { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;trigram\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;shingle\u0026#34;] } }, \u0026#34;filter\u0026#34;: { \u0026#34;shingle\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;shingle\u0026#34;, \u0026#34;min_shingle_size\u0026#34;: 2, \u0026#34;max_shingle_size\u0026#34;: 3 } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;trigram\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;trigram\u0026#34; } } } } } } 查询：\nGET /articles/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;phrase-correction\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;full texr serch\u0026#34;, \u0026#34;phrase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;content.trigram\u0026#34;, \u0026#34;confidence\u0026#34;: 1.0, \u0026#34;max_errors\u0026#34;: 2, \u0026#34;direct_generator\u0026#34;: [ { \u0026#34;field\u0026#34;: \u0026#34;content.trigram\u0026#34;, \u0026#34;suggest_mode\u0026#34;: \u0026#34;popular\u0026#34; } ] } } } } Phrase Suggester 参数 #     参数 类型 默认值 说明     field String 必填 shingle 字段名   max_errors Float 1.0 允许纠正的最大词项数（可以是比例，如 0.5 表示一半的词可以被纠正）   confidence Float 1.0 置信度阈值，只返回得分高于原输入 × confidence 的建议。设为 0 返回所有候选   separator String \u0026quot; \u0026quot; 词项之间的分隔符   gram_size Integer - N-gram 大小，默认由 shingle_size 推断   real_word_error_likelihood Float 0.95 真实词被拼错的概率，降低此值会让纠正倾向于不修改真实存在的词   force_unigrams Boolean true 是否强制使用 unigram 作为候选   token_limit Integer 10 最多分析的 token 数量   smoothing Object - 平滑模型：stupid_backoff（默认）、laplace、linear_interpolation   highlight Object - 在建议结果中高亮被修改的词（设置 pre_tag 和 post_tag）   collate Object - 用模板查询校验建议是否确实能命中文档，可设置 prune: true 保留未命中的建议    direct_generator 参数 #  direct_generator 是 Phrase Suggester 生成候选纠正词的核心组件，支持以下参数：\n   参数 类型 默认值 说明     field String 必填 从哪个字段获取候选词   suggest_mode String \u0026quot;missing\u0026quot; 建议模式：missing（仅当原词不在索引中时建议）、popular（建议更常见的替代词）、always（总是建议）   size Integer 5 每个 token 返回的最大候选数   max_edits Integer 2 允许的最大编辑距离（1 或 2）   prefix_length Integer 1 前缀不参与模糊匹配的字符数   min_word_length Integer 4 候选词的最小长度   max_inspections Integer 5 每个候选词的最大检查次数，用于控制性能   min_doc_freq Float 0 候选词在索引中的最低文档频率（绝对值或比例）   max_term_freq Float 0.01 原始词的最大词频阈值，超过此值不做纠正   pre_filter String - 应用于候选词的分析过滤器   post_filter String - 应用于候选词的后置过滤器    Phrase Suggester 高亮 #  可以在纠正结果中高亮被修改的词：\nGET /articles/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;phrase-correction\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;full texr serch\u0026#34;, \u0026#34;phrase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;content.trigram\u0026#34;, \u0026#34;highlight\u0026#34;: { \u0026#34;pre_tag\u0026#34;: \u0026#34;\u0026lt;em\u0026gt;\u0026#34;, \u0026#34;post_tag\u0026#34;: \u0026#34;\u0026lt;/em\u0026gt;\u0026#34; } } } } } 返回：\u0026quot;full \u0026lt;em\u0026gt;text\u0026lt;/em\u0026gt; \u0026lt;em\u0026gt;search\u0026lt;/em\u0026gt;\u0026quot;\nCompletion Suggester（前缀补全） #  基于 FST 数据结构常驻内存，为高并发前缀补全场景做了专门优化。\n前提：使用 completion 字段类型：\nPUT /search-terms { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;suggest\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;completion\u0026#34; } } } } 写入带权重的数据：\nPUT /search-terms/_doc/1 { \u0026#34;suggest\u0026#34;: { \u0026#34;input\u0026#34;: [\u0026#34;全文搜索\u0026#34;, \u0026#34;全文检索\u0026#34;, \u0026#34;全文匹配\u0026#34;], \u0026#34;weight\u0026#34;: 10 } } PUT /search-terms/_doc/2 { \u0026quot;suggest\u0026quot;: { \u0026quot;input\u0026quot;: [\u0026quot;前缀补全\u0026quot;, \u0026quot;前缀匹配\u0026quot;], \u0026quot;weight\u0026quot;: 5 } } 查询：\nGET /search-terms/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;autocomplete\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;全文\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggest\u0026#34;, \u0026#34;size\u0026#34;: 5, \u0026#34;skip_duplicates\u0026#34;: true } } } } 模糊补全 #  允许用户输入有拼写错误：\nGET /search-terms/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;autocomplete\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;全问\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggest\u0026#34;, \u0026#34;fuzzy\u0026#34;: { \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34;, \u0026#34;transpositions\u0026#34;: true, \u0026#34;min_length\u0026#34;: 3, \u0026#34;prefix_length\u0026#34;: 1, \u0026#34;unicode_aware\u0026#34;: false } } } } } 正则补全 #  使用正则表达式匹配：\nGET /search-terms/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;autocomplete\u0026#34;: { \u0026#34;regex\u0026#34;: \u0026#34;全[文本].*\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggest\u0026#34; } } } } Completion Suggester 参数 #     参数 类型 默认值 说明     field String 必填 completion 字段名   size Integer 5 返回建议的数量   skip_duplicates Boolean false 过滤重复的建议文本   fuzzy Object - 启用模糊补全，容错输入。见下方 fuzzy 参数   regex String - 使用正则表达式匹配（替代 prefix）   contexts Object - 上下文过滤（需要字段配置 contexts），按类目或地理位置过滤建议    常见场景与选型 #     场景 推荐方案 说明     搜索框下拉补全 Completion Suggester FST 常驻内存，最快   \u0026ldquo;您是不是要找\u0026rdquo; Phrase Suggester 整句纠正，用户体验好   搜索结果为空时纠错 Term Suggester 简单场景，无需额外 mapping   前缀匹配快速原型 match_phrase_prefix 无需特殊 mapping，但性能一般   大规模前缀补全 Edge N-gram + match 索引时切分，查询时性能好    实践建议 #   从简单开始：先用 Completion Suggester 实现搜索框补全，效果明显且性能最好 纠错按需启用：Term / Phrase Suggester 适合在搜索结果为空时触发，避免过度纠错覆盖用户真实意图 权重运营：通过 weight 字段控制热门搜索词的排序，结合业务数据定期更新 候选词清洗：建议词来源（搜索日志、商品标题等）需要去重、过滤敏感词 所有建议逻辑都应遵循：宁可少给、不给，也不要给出明显错误的建议  小结 #   Easysearch 提供三种 Suggester：Term（单词纠正）、Phrase（短语纠正）、Completion（前缀补全） Completion Suggester 性能最优，适合搜索框实时补全 Phrase Suggester 适合实现\u0026quot;您是不是要找\u0026quot;功能，需要 shingle 索引 Term Suggester 最简单，适合基础拼写纠错  下一步可以继续阅读：\n  高亮  分页与排序  相关性  参考手册（API 与参数） #    自动补全与建议（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"建议与纠错","url":"/easysearch/main/docs/features/fulltext-search/suggestions/"},{"category":null,"content":"多租户建模 #  多租户（Multi-tenancy）是指多个用户或组织共享同一个 Easysearch 集群，但各自的数据需要隔离。本页介绍多租户场景下的索引设计、路由策略和容量规划思路。\n多租户的两种主要模式 #  模式一：一个用户一个索引 #  通常来说，用户使用 Easysearch 的原因是他们需要添加全文检索或者需要分析一个已经存在的应用。他们创建一个索引来存储所有文档。公司里的其他人也逐渐发现了 Easysearch 带来的好处，也想把他们的数据添加到 Easysearch 中去。\n幸运的是，Easysearch 支持多租户，所以每个用户可以在相同的集群中拥有自己的索引。有人偶尔会想要搜索所有用户的文档，这种情况可以通过搜索所有索引实现，但大多数情况下用户只关心它们自己的文档。\n一些用户有着比其他人更多的文档，一些用户可能有比其他人更多的搜索次数，所以这种对指定每个索引主分片和副本分片数量能力的需要应该很适合使用\u0026quot;一个用户一个索引\u0026quot;的模式。类似地，较为繁忙的索引可以通过分片分配过滤指定到高配的节点。\n 提示：不要为每个索引都使用默认的主分片数。想想看它需要存储多少数据。有可能你仅需要一个分片——再多的都只是浪费资源。\n 大多数 Easysearch 的用户读到这里就已经够了。简单的\u0026quot;一个用户一个索引\u0026quot;对大多数场景都可以满足了。\n模式二：共享索引 + 过滤 #  对于例外的场景，你可能会发现需要支持很大数量的用户，都是相似的需求。一个例子可能是为一个拥有几千个邮箱账户的论坛提供搜索服务。一些论坛可能有巨大的流量，但大多数都很小。将一个有着单个分片的索引用于一个小规模论坛已经是足够的了——一个分片可以承载很多个论坛的数据。\n我们需要的是一种可以在用户间共享资源的方法，给每个用户他们拥有自己的索引这种印象，而不在小用户上浪费资源。\n共享索引的实现 #  我们可以为许多的小用户使用一个大的共享的索引，将用户标识索引进一个字段并且将它用作一个过滤器：\nPUT /forums { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 10 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;forum_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } PUT /forums/_doc/1 { \u0026quot;forum_id\u0026quot;: \u0026quot;baking\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Easy recipe for ginger nuts\u0026quot;, \u0026hellip; } 我们可以把 forum_id 用作一个过滤器来针对单个用户进行搜索。这个过滤器可以排除索引中绝大部分的数据（属于其它用户的数据），缓存会保证快速的响应：\nGET /forums/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ginger nuts\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;forum_id\u0026#34;: \u0026#34;baking\u0026#34; } } } } } 使用路由优化共享索引 #  这个办法行得通，但我们可以做得更好。来自于同一个用户的文档可以简单地容纳于单个分片，但它们现在被打散到了这个索引的所有十个分片中。这意味着每个搜索请求都必须被转发至所有十个分片的一个主分片或者副本分片。\n如果能够保证所有来自于同一个用户的所有文档都被存储于同一个分片可能会是个好想法。\n在路由章节中，我们说过一个文档将通过使用如下公式来分配到一个指定分片：\nshard = hash(routing) % number_of_primary_shards routing 的值默认为文档的 _id，但我们可以覆盖它并且提供我们自己自定义的路由值，例如 user_id。所有有着相同 routing 值的文档都将被存储于相同的分片：\nPUT /forums/_doc/1?routing=baking { \u0026#34;forum_id\u0026#34;: \u0026#34;baking\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Easy recipe for ginger nuts\u0026#34;, ... } 当我们搜索一个指定用户的文档时，我们可以传递相同的 routing 值来保证搜索请求仅在存有我们文档的分片上执行：\nGET /forums/_search?routing=baking { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ginger nuts\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;forum_id\u0026#34;: \u0026#34;baking\u0026#34; } } } } } 多个用户可以通过传递一个逗号分隔的列表来指定 routing 值，然后将每个 user_id 包含于一个 terms 查询：\nGET /forums/_search?routing=baking,cooking,recipes { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ginger nuts\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;forum_id\u0026#34;: [\u0026#34;baking\u0026#34;, \u0026#34;cooking\u0026#34;, \u0026#34;recipes\u0026#34;] } } } } } 使用别名简化多租户管理 #  这种方式从技术上来说比较高效，由于要为每一个查询或者索引请求指定 routing 和 terms 的值看起来有一点的笨拙。索引别名可以帮你解决这些！\n可以为每个用户创建一个别名，并设置过滤条件和路由：\nPOST /_aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;forums\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;baking_forum\u0026#34;, \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;forum_id\u0026#34;: \u0026#34;baking\u0026#34; } }, \u0026#34;routing\u0026#34;: \u0026#34;baking\u0026#34; } } ] } 这样，用户只需要使用 baking_forum 别名进行查询，不需要关心底层的路由和过滤逻辑。\n多租户的容量规划 #  1. 评估每个租户的数据量 #   小租户：数据量小，查询频率低，适合共享索引 大租户：数据量大，查询频率高，适合独立索引  2. 评估查询模式 #   大多数查询只涉及单个租户：适合共享索引 + 路由 经常需要跨租户查询：适合独立索引或共享索引 + 过滤  3. 评估资源需求 #   小租户：共享资源，降低成本 大租户：独立资源，保证性能  安全考虑 #  在多租户场景下，安全隔离非常重要：\n 字段级别过滤：使用别名过滤确保用户只能看到自己的数据 路由隔离：使用路由确保用户数据分布在不同的分片上 权限控制：在应用层或 Easysearch 安全功能中实现权限控制  小结 #   多租户可以通过\u0026quot;一个用户一个索引\u0026quot;或\u0026quot;共享索引 + 过滤/路由\u0026quot;两种模式实现 小用户适合共享索引，大用户适合独立索引 使用路由可以优化共享索引的查询性能 使用别名可以简化多租户的管理和查询 需要根据数据量、查询模式和资源需求选择合适的模式  下一步可以继续阅读：\n  路由（Routing）  别名（Aliases）  容量规划  安全与多租户最佳实践  ","subcategory":null,"summary":"","tags":["最佳实践","数据建模"],"title":"多租户建模","url":"/easysearch/main/docs/best-practices/data-modeling/multi-tenancy/"},{"category":null,"content":"命令行工具参考 #  Easysearch 提供了一组命令行工具，位于安装目录的 bin/ 目录下，用于启动服务、管理安全设置和安装插件等。\nbin/easysearch — 启动服务 #  启动 Easysearch 节点。\n语法 #  bin/easysearch [选项] 命令行选项 #     选项 说明     -d 以守护进程（后台）模式启动。不能与 --pidfile 单独使用时不写 PID 文件   -p \u0026lt;路径\u0026gt; 启动时将进程 ID (PID) 写入指定文件，便于通过 PID 管理进程   -q 关闭标准输出和标准错误的控制台日志输出。日志仍会写入日志文件   -E \u0026lt;设置\u0026gt;=\u0026lt;值\u0026gt; 覆盖配置文件中的设置。可多次使用以覆盖多个设置   -V 显示 Easysearch 版本信息并退出   -h, --help 显示帮助信息   -v, --verbose 显示详细输出   -s, --silent 只显示错误信息    使用示例 #  前台启动（适合开发调试）：\nbin/easysearch 后台启动并记录 PID：\nbin/easysearch -d -p /var/run/easysearch.pid 覆盖配置项启动：\nbin/easysearch -E cluster.name=my_cluster -E node.name=node_1 查看版本：\nbin/easysearch -V 输出示例：\nVersion: 1.x.x, Build: xxxxxxx/..., JVM: xx.x.x 使用自定义配置目录：\nES_PATH_CONF=/opt/easysearch/config bin/easysearch 通过 PID 文件停止后台进程：\nkill $(cat /var/run/easysearch.pid) -E 设置覆盖 #  -E 选项可以覆盖 easysearch.yml 中的任何设置，优先级高于配置文件：\n# 覆盖集群名称和网络绑定地址 bin/easysearch \\  -E cluster.name=production \\  -E network.host=0.0.0.0 \\  -E discovery.seed_hosts=10.0.0.1,10.0.0.2 \\  -E node.name=node-1  注意：通过 -E 传入的设置仅在当前进程生效，不会写入配置文件。\n bin/easysearch-keystore — 安全设置管理 #  管理 Easysearch 的安全密钥库（keystore），用于存储敏感配置（如密码、API 密钥等），详见 Keystore 安全设置。\n可用命令 #     命令 说明     create 创建新的密钥库   list 列出密钥库中的所有条目   add \u0026lt;名称\u0026gt; 添加字符串类型的设置   add-file \u0026lt;名称\u0026gt; \u0026lt;文件\u0026gt; 添加文件类型的设置   remove \u0026lt;名称\u0026gt; 从密钥库中移除指定设置   upgrade 升级密钥库格式   passwd 修改密钥库密码   has-passwd 检查密钥库是否设置了密码    bin/easysearch-plugin — 插件管理 #  安装、卸载和管理 Easysearch 插件。\n可用命令 #     命令 说明 示例     install \u0026lt;插件\u0026gt; 安装插件 bin/easysearch-plugin install analysis-ik   remove \u0026lt;插件\u0026gt; 卸载插件 bin/easysearch-plugin remove analysis-ik   list 列出已安装的插件 bin/easysearch-plugin list    安装示例 #  # 从内置仓库安装 bin/easysearch-plugin install analysis-ik # 从本地文件安装 bin/easysearch-plugin install file:///path/to/plugin.zip\n# 从 URL 安装 bin/easysearch-plugin install https://example.com/plugin.zip \n注意：安装或卸载插件后需要重启 Easysearch 节点才能生效。\n 设置优先级 #  当同一个设置在多处配置时，Easysearch 按以下优先级（从高到低）应用：\n   优先级 来源 说明     1（最高） Transient 集群设置 通过 PUT _cluster/settings 设置，重启后失效   2 Persistent 集群设置 通过 PUT _cluster/settings 设置，持久保存   3 命令行 -E 选项 启动时通过 CLI 传入   4 easysearch.yml 配置文件 静态配置文件   5（最低） 默认值 代码内置的默认值     建议：对于固定不变的配置（如路径、网络绑定），写在 easysearch.yml 中；对于需要动态调整的配置（如水位线、限流），通过集群设置 API 管理。\n ","subcategory":null,"summary":"","tags":null,"title":"命令行工具参考","url":"/easysearch/main/docs/deployment/config/cli-reference/"},{"category":null,"content":"内存与缓存配置 #  本页介绍 easysearch.yml 中与内存锁定和缓存相关的配置项。\n bootstrap.memory_lock #  bootstrap.memory_lock: true    项目 说明     参数 bootstrap.memory_lock   默认值 false   属性 静态   说明 启动时锁定 JVM 堆内存，防止操作系统将堆内存交换（swap）到磁盘。生产环境强烈建议启用    为什么需要锁定内存？ #  当操作系统将 JVM 堆内存换出到磁盘时，会导致：\n GC 暂停时间显著增加：GC 需要将被换出的页面重新换入内存。 搜索和索引延迟飙升：毫秒级操作可能变成秒级。 节点被判定为不可用：主节点可能因为超时将该节点踢出集群。  配合设置 #  启用 memory_lock 需要操作系统级的配合：\n1. 设置 ulimit（systemd）\n# /etc/systemd/system/easysearch.service.d/override.conf [Service] LimitMEMLOCK=infinity 2. 设置 limits.conf\n# /etc/security/limits.conf easysearch soft memlock unlimited easysearch hard memlock unlimited 3. 验证是否生效\nGET _nodes?filter_path=**.mlockall 返回 \u0026quot;mlockall\u0026quot;: true 表示内存锁定成功。\n 如果启动日志出现 Unable to lock JVM Memory 错误，说明操作系统限制未正确配置。详见 系统调优。\n  缓存设置 #  Easysearch 使用多级缓存来加速查询。以下参数控制各类缓存的大小。\nindices.fielddata.cache.size #  indices.fielddata.cache.size: 40%    项目 说明     参数 indices.fielddata.cache.size   默认值 无限制（不设上限，按需加载）   属性 动态（可通过集群设置 API 修改）   说明 字段数据（fielddata）缓存的上限。支持百分比（堆内存占比）或绝对值（如 4gb）。当缓存达到上限时，会触发逐出（eviction），可能导致查询变慢    什么是 Fielddata？\n Fielddata 是对 text 字段进行排序、聚合时加载到内存中的倒排索引的正排形式。 它非常消耗内存，通常建议使用 keyword 字段或 doc_values 代替。 如果确实需要使用 fielddata，建议设置缓存上限防止 OOM。  indices.queries.cache.size #  indices.queries.cache.size: 10%    项目 说明     参数 indices.queries.cache.size   默认值 堆内存的 10%   属性 动态（可通过集群设置 API 修改）   说明 节点级别的查询缓存（Node Query Cache）大小。缓存 filter 上下文中的查询结果（bitset），对重复的过滤查询有显著加速效果    缓存触发条件：\n 查询必须在 filter 上下文中（如 bool.filter、constant_score）。 段（segment）中文档数超过 10,000 且占该分片总文档数的 3% 以上。  indices.requests.cache.size #  indices.requests.cache.size: 1%    项目 说明     参数 indices.requests.cache.size   默认值 堆内存的 1%   属性 动态（可通过集群设置 API 修改）   说明 分片级别的请求缓存（Shard Request Cache）大小。缓存搜索请求的最终结果（聚合结果、count、_search 的 size: 0 等），索引刷新后自动失效    适用场景：\n 对变化不频繁的索引执行相同的聚合查询。 仪表盘（dashboard）场景，相同查询反复执行。   堆内存分配概览 #  理解各组件如何瓜分堆内存，有助于合理配置缓存：\nJVM 堆内存（-Xmx） ├── 查询缓存（10%） indices.queries.cache.size ├── 请求缓存（1%） indices.requests.cache.size ├── Fielddata 缓存（无限制） indices.fielddata.cache.size ├── 索引缓冲区（10%） indices.memory.index_buffer_size ├── 断路器保护（95% 总上限） indices.breaker.total.limit └── 剩余：Lucene 段读取、聚合、排序等运行时开销  缓存参数的百分比值都是相对于 JVM 堆内存（-Xmx）的比例。 indices.breaker.total.limit 默认值为 95%（当 indices.breaker.total.use_real_memory=true 时），或 70%（当设为 false 时）。\n  配置示例 #  生产环境（通用） #  bootstrap.memory_lock: true # 缓存使用默认值即可，除非有特定性能问题 聚合密集型场景 #  bootstrap.memory_lock: true indices.fielddata.cache.size: 30% # 限制 fielddata 防止 OOM indices.requests.cache.size: 2% # 增大请求缓存 写入密集型场景 #  bootstrap.memory_lock: true indices.queries.cache.size: 5% # 减少查询缓存，留更多堆内存给写入缓冲  延伸阅读 #    JVM 配置 — 堆大小设置  系统调优 — memlock 等操作系统参数  硬件配置 — 内存规格选型  集群配置 — 断路器等动态设置  ","subcategory":null,"summary":"","tags":null,"title":"内存与缓存","url":"/easysearch/main/docs/deployment/config/node-settings/memory/"},{"category":null,"content":"兆芯平台安装 #  兆芯平台介绍 #  兆芯平台基于自主可控的 x86 架构，兼容主流 Linux 系统，广泛应用于信创桌面与服务器领域，支持统信 UOS、麒麟等国产操作系统，具备良好的软硬件生态和迁移兼容性。\n兆芯平台安装参考 #  目前，Easysearch 已支持在兆芯芯片的国产操作系统上运行，联网环境建议使用一键安装脚本进行安装，离线环境建议下载 Bundle 包进行安装，分布式集群安装请参考 分布式集群安装。\n 前提条件：已参照 系统调优进行了系统优化，同时为 Easysearch 创建了专用的用户。\n 初始化系统参数及用户命令参考 #  # 调整内核配置 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p # 增加用户组与用户 groupadd -r easysearch \u0026amp;\u0026amp; useradd -r -g easysearch -d /home/easysearch -s /sbin/nologin -c \u0026#34;Easysearch Service Account\u0026#34; easysearch 安装命令参考 #  # 创建数据目录 mkdir -p /opt/easysearch # 下载最新版本的 Easysearch 并安装 curl -sSL http://get.infini.cloud | bash -s -- -p easysearch -d /opt/easysearch # 进入 Easysearch 目录 cd /opt/easysearch # 初始化 Easysearch bin/initialize.sh -s # 调整目录权限 chown -R easysearch:easysearch /opt/easysearch # 启动 Easysearch runuser -u easysearch -- /opt/easysearch/bin/easysearch -d -p /opt/easysearch/easysearch.pid  注意：初始化过程中会生成随机密码，并不会保存到日志文件中，只会在终端显示一次，请妥善保存。如果忘记 admin 密码，可以使用 bin/reset_admin_password.sh 进行重置。\n 已验证系统环境：\n KH-40000/银河麒麟高级服务器操作系统V10 SP3  如果您在其他兆芯平台的国产操作系统上安装遇到问题，欢迎通过 提交工单与我们联系。\n","subcategory":null,"summary":"","tags":null,"title":"兆芯平台安装","url":"/easysearch/main/docs/deployment/install-guide/ciip/zhaoxin/"},{"category":null,"content":"Docker Compose 环境下使用 Easysearch #  在使用 docker-compose 运行 Easysearch 集群之前，请确保已进行 系统调优并安装好 Docker 服务，且 Docker 服务正常运行。\n# 安装docker-compose curl -L \u0026#34;https://github.com/docker/compose/releases/download/v2.6.1/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose # 增加执行权限 chmod +x /usr/local/bin/docker-compose # 检查版本信息 docker-compose -v 运行 2 节点 docker compose 项目 #  从官网下载文件并解压，然后运行初始化脚本，最后运行启动脚本。\n 在宿主机上创建工作目录  # 创建操作目录 sudo mkdir -p /data/docker/compose 下载文件并解压   如需测试 3 节点，只需把下面的下载文件名改为 3node.tar.gz 即可。\n curl -sSL https://release.infinilabs.com/easysearch/archive/compose/2node.tar.gz | sudo tar -xzC /data/docker/compose --strip-components=1 # 调整目录权限 sudo chown -R ${USER} /data/docker/compose  注意：解压之后，请把镜像的 latest 版本手工更新成具体的版本，可参考下面的命令\ncd /data/docker/compose # 替换成具体的版本, 注：如果是 MacOS 请调整为 sudo sed -i \u0026#39;\u0026#39; \u0026#34;s/easysearch:latest/easysearch:2.0.3-2534/;s/console:latest/console:1.30.2-2333/\u0026#34; docker-compose.yml sudo sed -i \u0026#34;s/easysearch:latest/easysearch:2.0.3-2534/;s/console:latest/console:1.30.2-2333/\u0026#34; docker-compose.yml #检查版本是否替换成功 grep image docker-compose.yml  运行 docker-compose 项目  cd /data/docker/compose # 调整目录权限 sudo ./init.sh # 下载镜像 docker-compose pull # 启动 docker-compose 项目 ./start.sh # 如果启动失败，提示没权限，可进行权限调整，如 MacOS Docker 需要使用当前用户权限 sudo chown -R ${USER}:staff /data/docker/compose 停止 docker-compose 项目  # 停止项目 ./stop.sh 清理 docker-compose 项目  # 清理项目，将清理 Easysearch 的 data 和 logs。 sudo ./reset.sh 检查集群节点  # 集群默认的密码为admin curl -ku admin:xxxxxxxxxxxx https://localhost:9201/_cat/nodes?v # 输出信息如下： ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name 172.24.0.3 68 31 31 1.67 0.57 0.21 dimr - easysearch-node1 172.24.0.2 55 31 31 1.67 0.57 0.21 dimr * easysearch-node2 未获取到最新镜像处理方法 #  # 通过访问 https://hub.docker.com/r/infinilabs/easysearch/tags ，然后点击 latest 找出镜像的 sha256 值。 HASH=803b8aab2ec012728901112e916f1aa0fadc85c9b6b21b887a051aa8c5e53e8a docker pull infinilabs/easysearch:latest@sha256:$HASH IMGID=$(docker image ls --format \u0026#34;table {{.ID}}\u0026#34; --digests |grep \u0026#34;$HASH\u0026#34; |awk \u0026#39;{print $1}\u0026#39;) docker tag $IMGID infinilabs/easysearch:latest docker images |grep -v none |grep $IMGID # 或者直接通过版本号进行镜像拉取 docker pull infinilabs/easysearch:2.0.3-2534 \n","subcategory":null,"summary":"","tags":null,"title":"Docker Compose","url":"/easysearch/main/docs/deployment/install-guide/docker-compose/"},{"category":null,"content":"系统日志 #  Easysearch 日志包含监控群集操作和故障排除问题的重要信息。日志的位置因安装类型而异：\n 在 Docker 上，Easysearch 将大多数日志写入控制台，并将其余日志存储在 easysearch/logs/ 中。tarball 安装也使用 easysearch/logs/。 在 RPM 和 Debian 安装上，Easysearch 将日志写入 /var/log/easysearch/。  日志可作为 .log （纯文本）和 .json 文件使用。\n应用程序日志 #  对于其应用程序日志，Easysearch 使用 Apache Log4j 2 其内置日志级别（从最低到最高）为 TRACE 、 DEBUG 、 INFO 、 WARN 、 ERROR 和 FATAL 。默认 Easysearch 日志级别为 INFO 。\n您可以更改各个 Easysearch 模块的日志级别，而不是更改默认日志级别（ logger.level ）：\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34; : { \u0026#34;logger.org.easysearch.index.reindex\u0026#34; : \u0026#34;DEBUG\u0026#34; } } 此示例更改后，Easysearch 在重新索引操作期间会发出更详细的日志：\n[2019-10-18T16:52:51,184][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: starting [2019-10-18T16:52:51,186][DEBUG][o.e.i.r.TransportReindexAction] [node1] executing initial scroll against [some-index] [2019-10-18T16:52:51,291][DEBUG][o.e.i.r.TransportReindexAction] [node1] scroll returned [3] documents with a scroll id of [DXF1Z==] [2019-10-18T16:52:51,292][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: got scroll response with [3] hits [2019-10-18T16:52:51,294][DEBUG][o.e.i.r.WorkerBulkByScrollTaskState] [node1] [1626]: preparing bulk request for [0s] [2019-10-18T16:52:51,297][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: preparing bulk request [2019-10-18T16:52:51,299][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: sending [3] entry, [222b] bulk request [2019-10-18T16:52:51,310][INFO ][o.e.c.m.MetaDataMappingService] [node1] [some-new-index/R-j3adc6QTmEAEb-eAie9g] create_mapping [_doc] [2019-10-18T16:52:51,383][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: got scroll response with [0] hits [2019-10-18T16:52:51,384][DEBUG][o.e.i.r.WorkerBulkByScrollTaskState] [node1] [1626]: preparing bulk request for [0s] [2019-10-18T16:52:51,385][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: preparing bulk request [2019-10-18T16:52:51,386][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: finishing without any catastrophic failures [2019-10-18T16:52:51,395][DEBUG][o.e.i.r.TransportReindexAction] [node1] Freed [1] contexts DEBUG 和 TRACE 级别非常冗长。如果启用其中一个来解决问题，请在完成后禁用它。\n还有其他方法可以更改日志级别：\n 向 easysearch.yml 添加行：  logger.org.easysearch.index.reindex: debug 如果您希望在多个集群中重用日志记录配置，或者使用单个节点调试启动问题，那么修改 easysearch.yml 最有意义。\n修改 log4j2.properties：  # 定义一个唯一 ID 为 reindex 的新 logger logger.reindex.name = org.easysearch.index.reindex # 设置该 logger 的日志级别 logger.reindex.level = debug 这种方法非常灵活，但需要熟悉 Log4j 2 属性文件语法. 通常，其他选项提供了更简单的配置体验。\n如果检查配置目录中的默认 log4j2.properties 文件，可以看到一些 Easysearch 特定的变量：\nappender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n appender.rolling_old.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}.log  ${sys:es.logs.base_path} 是日志目录（例如 /var/log/easysearch/）。 ${sys:es.logs.cluster_name} 是集群名称。 [%node_name] 是节点名称。  慢日志 #  Easysearch 有两个 slow logs ，帮助您识别性能问题的日志：搜索慢日志和索引慢日志。\n这些日志依赖于阈值来定义什么是“慢”搜索或索引操作。例如，如果完成一个查询需要 15 秒以上的时间，您可能会认为它很慢。与为模块配置的应用程序日志不同，为索引配置慢日志。默认情况下，两个日志都被禁用（所有阈值都设置为“-1”）：\nGET \u0026lt;some-index\u0026gt;/_settings?include_defaults=true { \u0026quot;indexing\u0026quot;: { \u0026quot;slowlog\u0026quot;: { \u0026quot;reformat\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;threshold\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;warn\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;trace\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;debug\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;info\u0026quot;: \u0026quot;-1\u0026quot; } }, \u0026quot;source\u0026quot;: \u0026quot;1000\u0026quot;, \u0026quot;level\u0026quot;: \u0026quot;TRACE\u0026quot; } }, \u0026quot;search\u0026quot;: { \u0026quot;slowlog\u0026quot;: { \u0026quot;level\u0026quot;: \u0026quot;TRACE\u0026quot;, \u0026quot;threshold\u0026quot;: { \u0026quot;fetch\u0026quot;: { \u0026quot;warn\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;trace\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;debug\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;info\u0026quot;: \u0026quot;-1\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;warn\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;trace\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;debug\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;info\u0026quot;: \u0026quot;-1\u0026quot; } } } } } 要启用这些日志，请增加一个或多个阈值：\nPUT \u0026lt;some-index\u0026gt;/_settings { \u0026#34;indexing\u0026#34;: { \u0026#34;slowlog\u0026#34;: { \u0026#34;threshold\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;warn\u0026#34;: \u0026#34;15s\u0026#34;, \u0026#34;trace\u0026#34;: \u0026#34;750ms\u0026#34;, \u0026#34;debug\u0026#34;: \u0026#34;3s\u0026#34;, \u0026#34;info\u0026#34;: \u0026#34;10s\u0026#34; } }, \u0026#34;source\u0026#34;: \u0026#34;500\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34; } } } 在此示例中，Easysearch 记录 WARN 级别需要 15 秒或更长时间的索引操作，以及 INFO 级别需要 10 到 14.x 秒的操作。如果将阈值设置为 0 秒，Easysearch 将记录所有操作，这对于测试是否确实启用了慢速日志非常有用。\n reformat 指定是将文档 _source 字段记录为单行（ true ）还是让它跨越多行（ false ）。 source 是要记录的文档 _source 字段的字符数。 level 是要包含的最低日志级别。  easysearch_index_indexing_slowlog.log 中的一行可能如下所示：\nnode1 | [2019-10-24T19:48:51,012][WARN][i.i.s.index] [node1] [some-index/i86iF5kyTyy-PS8zrdDeAA] took[3.4ms], took_millis[3], type[_doc], id[1], routing[], source[{\u0026#34;title\u0026#34;:\u0026#34;Your Name\u0026#34;, \u0026#34;Director\u0026#34;:\u0026#34;Makoto Shinkai\u0026#34;}] 如果将阈值或级别设置得太低，慢日志可能会占用大量磁盘空间。考虑暂时启用它们以进行故障排除或性能调整。要禁用慢速日志，请将所有阈值返回到 -1 。\n弃用日志 #  弃用日志记录客户端对集群进行弃用 API 调用的时间。这些日志可以帮助您在升级到新的主要版本之前识别并修复问题。默认情况下，Easysearch 在 WARN 级别记录不推荐的 API 调用，这几乎适用于所有用例。如果需要，可以使用 _cluster/settings 、 easysearch.yml 或 log4j2.properties 配置 logger.deprecation.level 。\n","subcategory":null,"summary":"","tags":null,"title":"日志管理","url":"/easysearch/main/docs/operations/logs/"},{"category":null,"content":"排名评估（Rank Eval） #  排名评估 API（_rank_eval）允许您使用一组预定义的查询和已知相关文档来评估搜索结果的排名质量。这对于调优搜索相关性、对比不同查询策略的效果非常有用。\n基本概念 #  排名评估的核心流程：\n 定义请求集：一组代表性的搜索查询 标注相关文档：为每个查询标注哪些文档是相关的（及其相关程度） 选择评估指标：如 Precision、Recall、DCG 等 执行评估：Easysearch 运行查询并根据标注计算指标得分  基本用法 #  GET my_index/_rank_eval { \u0026#34;requests\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;query_1\u0026#34;, \u0026#34;request\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;搜索引擎\u0026#34; } } }, \u0026#34;ratings\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc1\u0026#34;, \u0026#34;rating\u0026#34;: 3 }, { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc2\u0026#34;, \u0026#34;rating\u0026#34;: 2 }, { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc3\u0026#34;, \u0026#34;rating\u0026#34;: 0 } ] }, { \u0026#34;id\u0026#34;: \u0026#34;query_2\u0026#34;, \u0026#34;request\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;全文检索\u0026#34; } } }, \u0026#34;ratings\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc4\u0026#34;, \u0026#34;rating\u0026#34;: 3 }, { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc5\u0026#34;, \u0026#34;rating\u0026#34;: 1 } ] } ], \u0026#34;metric\u0026#34;: { \u0026#34;precision\u0026#34;: { \u0026#34;k\u0026#34;: 10, \u0026#34;relevant_rating_threshold\u0026#34;: 1 } } } 请求参数说明 #     字段 说明     requests 评估请求数组   requests[].id 查询的唯一标识，用于在结果中关联   requests[].request 搜索请求体（与普通 _search 相同）   requests[].ratings 该查询对应的文档相关性标注   ratings[].rating 相关性分数，通常 0=不相关，1=部分相关，2=相关，3=非常相关   metric 使用的评估指标    评估指标 #  Precision at K（精确率） #  计算前 K 个结果中相关文档的比例。\n\u0026#34;metric\u0026#34;: { \u0026#34;precision\u0026#34;: { \u0026#34;k\u0026#34;: 10, \u0026#34;relevant_rating_threshold\u0026#34;: 1, \u0026#34;ignore_unlabeled\u0026#34;: false } }    参数 默认值 说明     k 10 评估前 K 个结果   relevant_rating_threshold 1 rating 大于等于此值视为相关   ignore_unlabeled false 是否忽略未标注的文档（true=只考虑已标注的文档）    公式：$P@K = \\frac{\\text{前 K 个结果中的相关文档数}}{K}$\nRecall at K（召回率） #  计算前 K 个结果中找到了多少已知的相关文档。\n\u0026#34;metric\u0026#34;: { \u0026#34;recall\u0026#34;: { \u0026#34;k\u0026#34;: 20, \u0026#34;relevant_rating_threshold\u0026#34;: 1 } } 公式：$R@K = \\frac{\\text{前 K 个结果中的相关文档数}}{\\text{全部已知相关文档数}}$\nMean Reciprocal Rank（MRR，平均倒数排名） #  计算第一个相关文档出现位置的倒数的平均值。适合评估\u0026quot;用户只关心第一个相关结果\u0026quot;的场景。\n\u0026#34;metric\u0026#34;: { \u0026#34;mean_reciprocal_rank\u0026#34;: { \u0026#34;k\u0026#34;: 10, \u0026#34;relevant_rating_threshold\u0026#34;: 1 } } 公式：$MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{rank_i}$\n其中 $rank_i$ 是第 $i$ 个查询中第一个相关文档的排名位置。\nDCG / nDCG（折损累计增益） #  DCG 考虑了文档的相关程度和位置，越相关的文档排名越靠前得分越高。nDCG 是归一化后的 DCG（0-1 之间）。\n\u0026#34;metric\u0026#34;: { \u0026#34;dcg\u0026#34;: { \u0026#34;k\u0026#34;: 10, \u0026#34;normalize\u0026#34;: true } }    参数 默认值 说明     k 10 评估前 K 个结果   normalize false 设为 true 使用 nDCG（归一化 DCG）    公式：$DCG@K = \\sum_{i=1}^{K} \\frac{2^{rel_i} - 1}{\\log_2(i+1)}$\n$nDCG@K = \\frac{DCG@K}{IDCG@K}$\nExpected Reciprocal Rank（ERR，期望倒数排名） #  考虑了用户浏览行为的级联模型——用户在找到满意结果后会停止浏览。\n\u0026#34;metric\u0026#34;: { \u0026#34;expected_reciprocal_rank\u0026#34;: { \u0026#34;maximum_relevance\u0026#34;: 3, \u0026#34;k\u0026#34;: 10 } }    参数 说明     maximum_relevance 最大相关性分数（用于将 rating 映射到满意度概率）   k 评估前 K 个结果    响应解读 #  { \u0026#34;metric_score\u0026#34;: 0.75, \u0026#34;details\u0026#34;: { \u0026#34;query_1\u0026#34;: { \u0026#34;metric_score\u0026#34;: 0.8, \u0026#34;unrated_docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc10\u0026#34; } ], \u0026#34;hits\u0026#34;: [ { \u0026#34;hit\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc1\u0026#34; }, \u0026#34;rating\u0026#34;: 3 }, { \u0026#34;hit\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc2\u0026#34; }, \u0026#34;rating\u0026#34;: 2 }, { \u0026#34;hit\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc10\u0026#34; }, \u0026#34;rating\u0026#34;: null } ] }, \u0026#34;query_2\u0026#34;: { \u0026#34;metric_score\u0026#34;: 0.7, \u0026#34;unrated_docs\u0026#34;: [], \u0026#34;hits\u0026#34;: [...] } }, \u0026#34;failures\u0026#34;: {} }    字段 说明     metric_score 所有查询的综合得分（平均值）   details.\u0026lt;query_id\u0026gt;.metric_score 单个查询的得分   details.\u0026lt;query_id\u0026gt;.unrated_docs 出现在结果中但未标注的文档   details.\u0026lt;query_id\u0026gt;.hits 结果文档列表及其 rating   failures 执行失败的查询    使用模板简化请求 #  当多个查询结构相似时，可以使用搜索模板减少重复：\nGET my_index/_rank_eval { \u0026#34;templates\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;match_template\u0026#34;, \u0026#34;template\u0026#34;: { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;{{field}}\u0026#34;: \u0026#34;{{query}}\u0026#34; } } } } } ], \u0026#34;requests\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;query_1\u0026#34;, \u0026#34;template_id\u0026#34;: \u0026#34;match_template\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;title\u0026#34;, \u0026#34;query\u0026#34;: \u0026#34;搜索引擎\u0026#34; }, \u0026#34;ratings\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc1\u0026#34;, \u0026#34;rating\u0026#34;: 3 } ] }, { \u0026#34;id\u0026#34;: \u0026#34;query_2\u0026#34;, \u0026#34;template_id\u0026#34;: \u0026#34;match_template\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;title\u0026#34;, \u0026#34;query\u0026#34;: \u0026#34;全文检索\u0026#34; }, \u0026#34;ratings\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;doc4\u0026#34;, \u0026#34;rating\u0026#34;: 3 } ] } ], \u0026#34;metric\u0026#34;: { \u0026#34;dcg\u0026#34;: { \u0026#34;k\u0026#34;: 10, \u0026#34;normalize\u0026#34;: true } } } 最佳实践 #   选择代表性查询：选择覆盖不同查询类型（高频、长尾、精确、模糊）的查询样本 多人标注取平均：避免单一标注者的偏差，理想情况下让多人独立标注后取平均 使用 nDCG：nDCG 同时考虑了排名位置和相关程度，是最全面的评估指标 定期重新评估：索引数据变化后，定期重新运行排名评估 A/B 对比：修改查询策略前后分别运行评估，量化改进效果 关注 unrated_docs：如果很多结果文档未被标注，说明标注集覆盖不足，需要补充  ","subcategory":null,"summary":"","tags":null,"title":"排名评估（Rank Eval）","url":"/easysearch/main/docs/features/fulltext-search/rank-eval/"},{"category":null,"content":"国密 / TLCP 配置指南 #  在政务、金融等信创场景中，要求使用国密（SM）系列算法进行 TLS 加密。Easysearch 基于 铜锁（Tongsuo） 提供国密 TLCP 双证书能力，支持 SM2/SM3/SM4 加密套件。\n国密算法简介 #     算法 类型 对应国际算法 用途     SM2 非对称加密 ECDSA / RSA 数字签名、密钥交换   SM3 哈希 SHA-256 完整性校验   SM4 对称加密 AES 数据加密    国密 TLS 使用 SM2 证书 + SM4 加密 + SM3 哈希，组成完整的加密通信套件。\n 前置条件 #     条件 说明     节点运行平台 国密运行依赖 Tongsuo JNI 原生库；bin/easysearch 当前会按以下平台加载本地库：linux-x86_64 / linux-aarch64 / darwin-x86_64 / darwin-aarch64。若对应库缺失会启动失败   证书生成平台 generate-tlcp-certs.sh 在 Linux 上要求系统安装 tongsuo 命令；macOS/其他平台优先使用 tongsuo，未安装时 fallback 到 openssl（需支持 SM2/SM3）；若两者都不存在则脚本直接失败退出   安全模块 需使用包含 modules/security 的发行包   协议 推荐使用 TLSv1.3；如手工配置 TLCP 但运行环境不支持，会在启动时报错（不会自动回退）     Linux 安装 Tongsuo（宿主机） #  当你在 Linux 宿主机上执行 bin/generate-tlcp-certs.sh 时，需要系统可直接找到 tongsuo 命令。\n# 1) 下载并解压 Tongsuo 8.4.0 cd /tmp curl -fL https://github.com/Tongsuo-Project/Tongsuo/archive/refs/tags/8.4.0.tar.gz -o tongsuo-8.4.0.tar.gz tar -xzf tongsuo-8.4.0.tar.gz cd Tongsuo-8.4.0 # 2) 编译安装到 /opt/tongsuo ./config shared \u0026ndash;prefix=/opt/tongsuo \u0026ndash;openssldir=/opt/tongsuo/ssl make -j\u0026quot;$(nproc)\u0026quot; sudo make install_sw\n# 3) 配置环境变量（全局） sudo tee /etc/profile.d/tongsuo.sh \u0026gt;/dev/null \u0026lt;\u0026lt;'EOF' TONGSUO_HOME=/opt/tongsuo export PATH=$TONGSUO_HOME/bin:$PATH export LD_LIBRARY_PATH=\u0026quot;$TONGSUO_HOME/lib64:$TONGSUO_HOME/lib:${LD_LIBRARY_PATH:-}\u0026quot; EOF\n# 4) 使当前会话生效并验证 source /etc/profile.d/tongsuo.sh command -v tongsuo tongsuo version \n如不使用 root，可改为安装到用户目录并将 PATH/LD_LIBRARY_PATH 写入用户 shell 配置。\n  证书生成 #  Easysearch 自带证书生成脚本，在发行包根目录执行：\nbin/generate-tlcp-certs.sh /path/to/output \u0026#34;changeit\u0026#34;  第一个参数为输出目录，第二个参数为证书密码，第三个参数（可选）为证书 subject（默认 /C=IN/ST=FI/L=NI/O=ORG/OU=UNIT/CN=infini.cloud）。也可通过 DOMAIN 环境变量覆盖默认域名：\nDOMAIN=my.company.com bin/generate-tlcp-certs.sh /path/to/output \u0026#34;changeit\u0026#34; 生成多节点证书时，可通过 NODE_NAMES_STR 传入空格分隔的节点名：\nNODE_NAMES_STR=\u0026#34;node1 node2 node3\u0026#34; bin/generate-tlcp-certs.sh /path/to/output \u0026#34;changeit\u0026#34;  脚本输出文件 #  TLCP 要求签名证书和加密证书两套独立的密钥对（双证书体制），脚本会同时生成服务端和客户端的双证书：\n   文件 说明     node1_sign.crt / node1_sign.key（及 nodeX_*） 节点签名证书与私钥（按节点生成）   node1_enc.crt / node1_enc.key（及 nodeX_*） 节点加密证书与私钥（按节点生成）   server_sign.crt / server_sign.key 兼容文件，默认指向第一个节点签名证书副本   server_enc.crt / server_enc.key 兼容文件，默认指向第一个节点加密证书副本   admin.crt / admin.key 管理员客户端证书与私钥   client_sign.crt / client_sign.key 客户端签名证书与私钥   client_enc.crt / client_enc.key 客户端加密证书与私钥   ca.crt / ca.key 根 CA 证书与私钥   sub_ca.crt / sub_ca.key 中间 CA 证书与私钥   ca_chain.crt 中间 CA + 根 CA 信任链   http-truststore-sm2.p12 HTTP 层 PKCS12 Truststore   transport-truststore-sm2.p12 Transport 层 PKCS12 Truststore   http-keystore-sm2.p12 / transport-keystore-sm2.p12 空 PKCS12 Keystore 占位文件（兼容用途，不含 TLCP 双证书）    生成完成后，将输出目录中的证书文件复制到 Easysearch 的 config/ 目录。\n 配置方式（推荐：PEM 模式） #  TLCP 双证书配置使用 security.ssl.\u0026lt;层\u0026gt;.tlcp.* 前缀，分别指定签名（sign）和加密（enc）两套证书。\nHTTP 层 #  # ---- 启用铜锁国密引擎 ---- security.ssl.use_tongsuo: true security.ssl.http.enabled: true # \u0026mdash;- TLCP 双证书（PEM 格式） \u0026mdash;- security.ssl.http.tlcp.sign_certificate: server_sign.crt security.ssl.http.tlcp.sign_key: server_sign.key security.ssl.http.tlcp.enc_certificate: server_enc.crt security.ssl.http.tlcp.enc_key: server_enc.key\n# \u0026mdash;- 信任链 \u0026mdash;- security.ssl.http.tlcp.trusted_ca_certificate: ca_chain.crt\n# \u0026mdash;- TLCP 协议设置 \u0026mdash;- security.ssl.http.tlcp.protocol: TLSv1.3 security.ssl.http.tlcp.cipher_suites:\n \u0026quot;TLS_SM4_GCM_SM3\u0026quot; \u0026quot;TLS_SM4_CCM_SM3\u0026quot; Transport 层 #   # ---- 启用铜锁国密引擎 ---- security.ssl.use_tongsuo: true security.ssl.transport.enabled: true # \u0026mdash;- TLCP 双证书（PEM 格式） \u0026mdash;- security.ssl.transport.tlcp.sign_certificate: server_sign.crt security.ssl.transport.tlcp.sign_key: server_sign.key security.ssl.transport.tlcp.enc_certificate: server_enc.crt security.ssl.transport.tlcp.enc_key: server_enc.key\n# \u0026mdash;- 信任链 \u0026mdash;- security.ssl.transport.tlcp.trusted_ca_certificate: ca_chain.crt\n# \u0026mdash;- TLCP 协议设置 \u0026mdash;- security.ssl.transport.tlcp.protocol: TLSv1.3 security.ssl.transport.tlcp.cipher_suites:\n \u0026quot;TLS_SM4_GCM_SM3\u0026quot; \u0026quot;TLS_SM4_CCM_SM3\u0026quot;  # \u0026mdash;- 节点间通信建议 \u0026mdash;- security.ssl.transport.skip_domain_verify: true security.ssl.transport.resolve_hostname: false 参数说明 #\n    参数 说明     security.ssl.use_tongsuo 是否启用铜锁国密引擎，使用国密必须设为 true   *.tlcp.sign_certificate 签名证书文件（PEM），用于数字签名   *.tlcp.sign_key 签名私钥文件（PEM）   *.tlcp.enc_certificate 加密证书文件（PEM），用于密钥交换   *.tlcp.enc_key 加密私钥文件（PEM）   *.tlcp.trusted_ca_certificate CA 信任链文件（PEM），需使用 ca_chain.crt（包含中间 CA + 根 CA）   *.tlcp.protocol TLS 协议版本，推荐 TLSv1.3；如设为 TLCP，需确认运行环境支持，否则启动报错   *.tlcp.cipher_suites 国密密码套件列表（可选，默认 TLS_SM4_GCM_SM3）     文件路径相对于 config/ 目录。sign_alias / enc_alias 在 PEM 模式可省略（默认 sign / enc，内部仍会使用该别名装载双证书），initialize.sh --tlcp 会自动写入这两个值。\n 可用密码套件 #     套件名称 说明     TLS_SM4_GCM_SM3 SM4-GCM 加密 + SM3 哈希（推荐）   TLS_SM4_CCM_SM3 SM4-CCM 加密 + SM3 哈希     完整配置示例 #  HTTP + Transport 同时启用国密：\n# ============== 国密 TLCP 完整配置 ============== security.enabled: true\n# \u0026mdash;- 启用铜锁引擎 \u0026mdash;- security.ssl.use_tongsuo: true\n# \u0026mdash;- HTTP 层 TLCP \u0026mdash;- security.ssl.http.enabled: true security.ssl.http.tlcp.sign_certificate: server_sign.crt security.ssl.http.tlcp.sign_key: server_sign.key security.ssl.http.tlcp.enc_certificate: server_enc.crt security.ssl.http.tlcp.enc_key: server_enc.key security.ssl.http.tlcp.trusted_ca_certificate: ca_chain.crt security.ssl.http.tlcp.protocol: TLSv1.3 security.ssl.http.tlcp.cipher_suites:\n \u0026quot;TLS_SM4_GCM_SM3\u0026quot; \u0026quot;TLS_SM4_CCM_SM3\u0026quot;  # \u0026mdash;- Transport 层 TLCP \u0026mdash;- security.ssl.transport.enabled: true security.ssl.transport.tlcp.sign_certificate: server_sign.crt security.ssl.transport.tlcp.sign_key: server_sign.key security.ssl.transport.tlcp.enc_certificate: server_enc.crt security.ssl.transport.tlcp.enc_key: server_enc.key security.ssl.transport.tlcp.trusted_ca_certificate: ca_chain.crt security.ssl.transport.tlcp.protocol: TLSv1.3 security.ssl.transport.tlcp.cipher_suites:\n \u0026quot;TLS_SM4_GCM_SM3\u0026quot; \u0026quot;TLS_SM4_CCM_SM3\u0026quot; security.ssl.transport.skip_domain_verify: true security.ssl.transport.resolve_hostname: false  # \u0026mdash;- 访问控制 \u0026mdash;- security.allow_default_init_securityindex: true security.restapi.roles_enabled: [\u0026quot;superuser\u0026quot;, \u0026quot;security_rest_api_access\u0026quot;] \n备选：PKCS12 Truststore 模式 #  如果需要使用 PKCS12 格式的 Truststore（不推荐，仅用于兼容场景）：\nsecurity.ssl.http.truststore_filepath: http-truststore-sm2.p12 security.ssl.http.truststore_password: changeit security.ssl.transport.truststore_filepath: transport-truststore-sm2.p12 security.ssl.transport.truststore_password: changeit \n注意事项：\n PEM 模式为推荐路径，PKCS12 仅用于特定兼容场景。 脚本生成的 http-keystore-sm2.p12、transport-keystore-sm2.p12 为空 Keystore 占位文件，不包含 TLCP 双证书。 TLCP 双证书要求 sign/enc 两套证书；使用 PKCS12 需自行构造包含双证书与别名的 Keystore，并承担 SM2/JCA 兼容性风险。    tlcp-curl 工具 #  Easysearch 提供 bin/tlcp-curl.sh，用于在国密 TLCP 环境下发起 HTTP 请求，行为类似 curl，但底层使用铜锁 JCA Provider 建立 TLCP 连接。\nbin/tlcp-curl.sh --url https://\u0026lt;host\u0026gt;:9200/_cluster/health -u admin:changeit  tlcp-curl.sh 默认不发送客户端证书。传入 --cert/--key/--enc-cert/--enc-key 任一参数后才启用客户端证书模式；未显式指定的签名证书/密钥会回退到 config/admin.crt / config/admin.key。仅当服务端配置 security.ssl.http.clientauth_mode: REQUIRE 时，客户端证书才是必需的。\n 常用参数 #     参数 说明     --url \u0026lt;url\u0026gt; 目标 URL   -X \u0026lt;method\u0026gt; HTTP 方法，默认 GET   -u \u0026lt;user:pass\u0026gt; HTTP Basic 认证   -H \u0026lt;header\u0026gt; 自定义请求头，可多次指定   -d \u0026lt;data\u0026gt; 请求体字符串   --data-file \u0026lt;file\u0026gt; 从文件读取请求体   --cert \u0026lt;path\u0026gt; 客户端签名证书 PEM（仅在启用客户端证书模式时使用）   --key \u0026lt;path\u0026gt; 客户端签名私钥 PEM（仅在启用客户端证书模式时使用）   --enc-cert \u0026lt;path\u0026gt; 客户端加密证书 PEM（可选，支持自动推导）   --enc-key \u0026lt;path\u0026gt; 客户端加密私钥 PEM（可选，支持自动推导）   --no-cert 不发送客户端证书（当前默认行为）   --ca \u0026lt;path\u0026gt; 信任 CA PEM，默认 config/ca_chain.crt   --insecure 跳过服务端证书和主机名验证（当前默认行为，参数保留用于兼容）   --protocol \u0026lt;name\u0026gt; TLS 协议，默认 TLSv1.3   --cipher \u0026lt;name\u0026gt; 密码套件，默认 TLS_SM4_GCM_SM3   -i 输出中包含响应头   -s 静默模式，不输出诊断信息   -v 详细调试输出   -o \u0026lt;file\u0026gt; 将响应体写入文件   -w \u0026lt;format\u0026gt; 输出格式串（仅短参数），支持 %{http_code}、%{time_total}   --connect-timeout \u0026lt;s\u0026gt; 连接超时秒数，默认 10   --max-time \u0026lt;s\u0026gt; 最大等待秒数，默认 60     --enc-cert / --enc-key 自动推导规则（优先级）：显式参数 \u0026gt; 从 --cert/--key 路径中的 _sign 推导 _enc \u0026gt; config/client_enc.crt|key \u0026gt; 回退到签名证书/密钥路径。\n 示例 #  验证国密端点（默认场景，无需客户端证书）：\nbin/tlcp-curl.sh --url https://localhost:9200/_security/sslinfo?pretty \\  -u admin:changeit 服务端启用 security.ssl.http.clientauth_mode: REQUIRE 时，使用客户端双证书（双向认证）：\nbin/tlcp-curl.sh --url https://localhost:9200/_cluster/health \\  --cert config/client_sign.crt --key config/client_sign.key \\  --enc-cert config/client_enc.crt --enc-key config/client_enc.key \\  --ca config/ca_chain.crt 仅使用用户名密码（不发送客户端证书）：\nbin/tlcp-curl.sh --url https://localhost:9200/_cluster/health?pretty \\  -u admin:changeit 使用 admin.crt / admin.key 访问：\nbin/tlcp-curl.sh --url https://localhost:9200/_cluster/health?pretty \\  -u admin:changeit \\  --cert config/admin.crt \\  --key config/admin.key 输出状态码与耗时（-w）：\nbin/tlcp-curl.sh --url https://localhost:9200/_cluster/health \\  --cert config/client_sign.crt --key config/client_sign.key \\  --enc-cert config/client_enc.crt --enc-key config/client_enc.key \\  --ca config/ca_chain.crt \\  -o /tmp/tlcp-health.json \\  -w \u0026#39;http=%{http_code} time=%{time_total}s\\n\u0026#39;  工具依赖 modules/security/ 下的 jar 包，需在 Easysearch 发行包根目录下执行。\n  启动与验证 #  1. 启动节点 #  正常启动 Easysearch，日志中应能看到 Tongsuo 安全提供者加载成功。\n2. 查看 SSL 信息 #  bin/tlcp-curl.sh --url https://\u0026lt;host\u0026gt;:9200/_security/sslinfo?pretty \\  -u admin:changeit \\  --cert config/client_sign.crt --key config/client_sign.key \\  --enc-cert config/client_enc.crt --enc-key config/client_enc.key \\  --ca config/ca_chain.crt 期望返回中包含：\n{ \u0026#34;ssl_provider\u0026#34; : \u0026#34;Tongsuo_Security_Provider\u0026#34;, \u0026#34;local_certificates_list\u0026#34; : [ ... ] } 确认 ssl_provider 为 Tongsuo_Security_Provider 且 local_certificates_list 中包含签名和加密证书即表示配置成功。\n 常见问题 #  SSLContext.getInstance(\u0026quot;TLCP\u0026quot;) 报错 #  当前运行环境的 JDK 不支持 TLCP 协议名，请将 protocol 改为 TLSv1.3：\nsecurity.ssl.http.tlcp.protocol: TLSv1.3 铜锁 OpenJDK 在 TLSv1.3 下依然会使用国密套件进行协商。\nCipher Suite 不生效 #  TLS 1.3 的密码套件不可通过 setEnabledCipherSuites 禁用，这是 TLS 1.3 协议规范的行为，不影响国密功能。\n证书链验证失败 #  请确认使用 ca_chain.crt（包含中间 CA + 根 CA 的完整链）而不是单独的 ca.crt：\n# ✅ 正确 — 使用完整信任链 security.ssl.http.tlcp.trusted_ca_certificate: ca_chain.crt # ❌ 错误 — 仅包含根 CA，缺少中间 CA security.ssl.http.tlcp.trusted_ca_certificate: ca.crt 平台与原生库问题 #\n 可按“证书生成”和“节点运行”两个维度理解平台支持：\n   维度 支持平台 说明     证书生成 Linux（x86_64 / aarch64） 必须安装并使用 tongsuo 命令   证书生成 macOS（x86_64 / arm64）及其他非 Linux 平台 优先 tongsuo，未安装时 fallback 到 openssl（需支持 SM2/SM3）；若两者都不可用则脚本失败退出   节点运行（国密） linux-x86_64 bin/easysearch 可识别并加载对应 JNI 原生库   节点运行（国密） linux-aarch64 bin/easysearch 可识别并加载对应 JNI 原生库   节点运行（国密） darwin-x86_64 bin/easysearch 可识别并加载对应 JNI 原生库   节点运行（国密） darwin-aarch64 bin/easysearch 可识别并加载对应 JNI 原生库     以上“节点运行”平台为启动脚本识别的目标平台；实际可用性仍取决于发行包中是否包含对应 native 库文件。\n  合规参考 #     标准 说明     GM/T 0024-2014 SSL VPN 技术规范   GM/T 0028-2014 密码模块安全技术要求   JR/T 0197-2020 金融行业密码应用技术标准     生产环境部署前，建议与合规团队确认具体的国密要求和认证需求。\n  延伸阅读 #    TLS 安全配置 — 标准 TLS 证书管理  安全配置 — easysearch.yml 安全参数总览  安全 API — 用户、角色、权限管理  ","subcategory":null,"summary":"","tags":null,"title":"国密配置","url":"/easysearch/main/docs/deployment/advanced-config/guomi/"},{"category":null,"content":"写入限流 #  Easysearch 从 1.8.0 版本开始引入写入限流功能，支持在节点、索引和分片三个层级对写入速度进行精细化控制。在需要向正在执行查询的集群导入数据时，限流功能可以有效避免写入压力影响查询响应时间。\n 三级限流架构 #     级别 粒度 适用场景     节点级 整个节点的写入总量 保护节点整体性能，不区分索引   索引级 单个索引的写入速度 限制特定索引，不影响其他索引   分片级 每个分片的写入速度 适合高低配主机混搭的集群     节点级和分片级限流可以同时启用，互不冲突。限流功能不会限制系统索引流量，只针对业务索引。\n 限流参数参考 #  以下参数均支持动态设置，无需重启集群。\n   参数 类型 说明 默认值     cluster.throttle.node.write boolean 是否启用节点级别限流 false   cluster.throttle.node.write.max_requests int 限定时间范围内单个节点允许的最大写入请求次数 0   cluster.throttle.node.write.max_bytes 字符串 限定时间范围内单个节点允许的最大写入请求字节数（kb, mb, gb 等） 0mb   cluster.throttle.node.write.action 字符串 触发限速之后的处理动作，分为 retry 和 drop 两种，默认为 drop drop   cluster.throttle.node.write.interval int 节点级别评估限速的单位时间间隔，默认为 1s 1   cluster.throttle.shard.write boolean 是否启用分片级别限流 false   cluster.throttle.shard.write.max_requests int 限定时间范围内单个分片允许的最大写入请求次数 0   cluster.throttle.shard.write.max_bytes 字符串 限定时间范围内单个分片允许的最大写入请求字节数（kb, mb, gb 等） 0mb   cluster.throttle.shard.write.action 字符串 触发限速之后的处理动作，分为 retry 和 drop 两种，默认为 drop drop   cluster.throttle.shard.write.interval int 分片级别评估限速的单位时间间隔，默认为 1s 1   index.throttle.write.enable boolean 是否针对当前索引启用写入限流（索引 Settings 配置） false   index.throttle.write.max_requests int 限定时间范围内当前索引允许的最大写入请求次数（索引 Settings 配置） 0   index.throttle.write.max_bytes 字符串 限定时间范围内当前索引允许的最大写入请求字节数（kb, mb, gb 等）（索引 Settings 配置） 0mb   index.throttle.write.action 字符串 触发限速之后的处理动作，分为 retry 和 drop 两种，默认为 drop（索引 Settings 配置） drop   index.throttle.write.interval int 当前索引评估限速的单位时间间隔，默认为 1s（索引 Settings 配置） 1    使用示例 #  节点级别限流 #  PUT _cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;cluster.throttle.node.write\u0026#34;: true, \u0026#34;cluster.throttle.node.write.max_bytes\u0026#34;: \u0026#34;50MB\u0026#34;, \u0026#34;cluster.throttle.node.write.max_requests\u0026#34;: 1000000, \u0026#34;cluster.throttle.node.write.action\u0026#34;: \u0026#34;retry\u0026#34; } } 以上配置表示开启节点限流功能，限定时间范围内单个节点允许最大写入50MB的数据，并且写入条数限制在100万，超过设定的阈值后会持续重试1秒钟，实际流量计算会稍有偏差。\n分片级别限流 #  PUT _cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;cluster.throttle.shard.write\u0026#34;: true, \u0026#34;cluster.throttle.shard.write.max_bytes\u0026#34;: \u0026#34;50MB\u0026#34;, \u0026#34;cluster.throttle.shard.write.max_requests\u0026#34;: 1000000, \u0026#34;cluster.throttle.shard.write.action\u0026#34;: \u0026#34;drop\u0026#34; } } 以上配置表示开启分片限流功能，限定时间范围内单个分片允许最大写入50MB的数据，并且写入条数限制在100万，超过设定的阈值后会立即拒绝写入，返回 rejected execution， 实际流量计算会稍有偏差。\n索引级别限流 #  有时，我们需要只针对个别索引进行写入限流，又不想影响其他索引的写入速度，可以在创建索引时在 Settings 里指定相应的限流配置项：\nPUT test_0 { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;index.throttle.write.max_requests\u0026#34;: 6000, \u0026#34;index.throttle.write.action\u0026#34;: \u0026#34;retry\u0026#34;, \u0026#34;index.throttle.write.enable\u0026#34;: true } } action 参数说明 #     值 行为     retry 超限的请求进行短暂重试，在间隔时间后重新评估，对客户端更友好   drop 直接丢弃超限的请求（返回 rejected execution），适合允许数据丢失的场景     注意事项 #   节点级别限流针对所有 DataNode 分片级别限流只计算从协调节点分发到数据节点主分片的 bulk 请求 节点级别和分片级别限流不冲突，可以同时启用 限流功能不会限制系统索引（. 开头）的流量，只针对业务索引 所有限流参数均支持动态设置，无需重启集群   相关文档 #    文档操作  容量规划  ","subcategory":null,"summary":"","tags":null,"title":"写入限流","url":"/easysearch/main/docs/operations/cluster-admin/throttling/"},{"category":null,"content":"ICU 脚本转换过滤器 #  icu_transform 词元过滤器使用 ICU 的 Transliterator 引擎将文本从一种脚本转写为另一种脚本。\n前提条件 #  需要安装 analysis-icu 插件：\nbin/easysearch-plugin install analysis-icu 功能说明 #  此过滤器可以实现：\n 脚本转写：中文→拉丁、西里尔→拉丁、阿拉伯→拉丁等 大小写转换：Upper、Lower、Title Unicode 归一化：NFC、NFD、NFKC、NFKD 自定义转换规则  使用示例 #  西里尔字母转拉丁字母 #  PUT my-transliterate-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;cyrillic_to_latin\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;icu_transform\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;Cyrillic-Latin\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_translit\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;cyrillic_to_latin\u0026#34;] } } } } } 中文转拼音 #  PUT my-pinyin-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;han_to_latin\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;icu_transform\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;Han-Latin\u0026#34; } } } } } 链式转换 #  PUT my-chain-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;to_ascii\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;icu_transform\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;Cyrillic-Latin; NFD; [:Nonspacing Mark:] Remove; NFC\u0026#34; } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;icu_transform\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;Cyrillic-Latin\u0026#34;}], \u0026#34;text\u0026#34;: \u0026#34;Москва\u0026#34; } 响应：Moskva\n参数 #     参数 类型 必填 说明     id string ✅ ICU 转换 ID，如 Cyrillic-Latin、Han-Latin、NFD 等   dir string 否 转换方向：forward（默认）或 reverse    常用转换 ID #     ID 说明     Cyrillic-Latin 西里尔字母 → 拉丁字母   Han-Latin 汉字 → 拉丁字母（拼音）   Katakana-Latin 片假名 → 拉丁字母   Arabic-Latin 阿拉伯字母 → 拉丁字母   Any-Latin 任意脚本 → 拉丁字母   NFD / NFC Unicode 归一化   Lower / Upper / Title 大小写转换    相关链接 #    ICU 排序过滤器  ICU 分析器  文本分析  ","subcategory":null,"summary":"","tags":null,"title":"ICU 脚本转换过滤器（ICU Transform）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/icu-transform/"},{"category":null,"content":"ICU 排序过滤器 #  icu_collation_keyword 词元过滤器使用 ICU 排序规则将词元转换为排序键（collation key），实现语言感知的排序和范围查询。\n前提条件 #  需要安装 analysis-icu 插件：\nbin/easysearch-plugin install analysis-icu 功能说明 #  此过滤器将文本词元转换为 ICU CollationKey 的字节表示。转换后的词元可用于：\n 语言感知排序：按特定语言的排序规则排列结果 范围查询：在 keyword 字段上执行符合语言习惯的范围过滤 重音/大小写不敏感匹配：通过调整排序强度控制匹配精度  使用示例 #  基本用法 — 德语排序 #  PUT my-german-sort { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;german_collation\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;icu_collation_keyword\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;de\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;DE\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;german_sort\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;german_collation\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;sort\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;german_sort\u0026#34; } } } } } } 忽略重音的匹配 #  PUT my-accent-insensitive { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;collation_primary\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;icu_collation_keyword\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;en\u0026#34;, \u0026#34;strength\u0026#34;: \u0026#34;primary\u0026#34; } } } } } 参数 #     参数 类型 说明     language string ICU 语言代码（如 de, fr, zh）   country string ICU 国家代码（如 DE, FR）   strength string 排序强度：primary（忽略重音+大小写）、secondary（区分重音）、tertiary（区分大小写）、quaternary、identical   decomposition string Unicode 分解模式：no、canonical   alternate string 空白/标点处理：shifted（忽略）、non-ignorable   caseLevel boolean 是否在 primary 强度下区分大小写   caseFirst string 大小写优先：lower、upper   numeric boolean 是否按数字值排序（true 使 \u0026ldquo;2\u0026rdquo; \u0026lt; \u0026ldquo;10\u0026rdquo;）   rules string 自定义 ICU 排序规则字符串    相关链接 #    ICU 分析器  ICU Transform 过滤器  文本分析  ","subcategory":null,"summary":"","tags":null,"title":"ICU 排序过滤器（ICU Collation）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/icu-collation/"},{"category":null,"content":"任务管理 #  任务是在集群中运行的任何操作。例如，搜索图书数据集以查找标题或作者姓名是一项任务。将自动创建任务以监视集群的运行状况和性能。有关集群中当前执行的所有任务的详细信息，可以使用 tasks API 操作。\n以下请求返回有关所有任务的信息：\nGET _tasks 通过包含任务 ID，您可以获得特定任务的信息。请注意，任务 ID 由节点的标识字符串和任务的数字 ID 组成。例如，如果节点的标识串是 nodestring ，任务的数字标识是 1234 ，则任务 ID 是 nodestring:1234 。您可以通过运行 tasks 操作来查找此信息。\nGET _tasks/\u0026lt;task_id\u0026gt; 请注意，如果任务完成运行，它将不会作为请求的一部分返回。对于一个需要稍长时间才能完成的任务的示例，可以在较大的文档上运行 _reindex API 操作，然后运行 tasks 。\n响应示例\n{ \u0026#34;nodes\u0026#34;: { \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;easy-node1\u0026#34;, \u0026#34;transport_address\u0026#34;: \u0026#34;30.18.0.3:9300\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;30.18.0.3\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;30.18.0.3:9300\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;data\u0026#34;, \u0026#34;ingest\u0026#34;, \u0026#34;master\u0026#34;, \u0026#34;remote_cluster_client\u0026#34;], \u0026#34;tasks\u0026#34;: { \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:17416\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;, \u0026#34;id\u0026#34;: 17416, \u0026#34;type\u0026#34;: \u0026#34;transport\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;cluster:monitor/tasks/lists\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1613599752458, \u0026#34;running_time_in_nanos\u0026#34;: 994000, \u0026#34;cancellable\u0026#34;: false, \u0026#34;headers\u0026#34;: {} }, \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:17413\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;, \u0026#34;id\u0026#34;: 17413, \u0026#34;type\u0026#34;: \u0026#34;transport\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;indices:data/write/bulk\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1613599752286, \u0026#34;running_time_in_nanos\u0026#34;: 30846500, \u0026#34;cancellable\u0026#34;: false, \u0026#34;parent_task_id\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:17366\u0026#34;, \u0026#34;headers\u0026#34;: {} }, \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:17366\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;, \u0026#34;id\u0026#34;: 17366, \u0026#34;type\u0026#34;: \u0026#34;transport\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;indices:data/write/reindex\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1613599750929, \u0026#34;running_time_in_nanos\u0026#34;: 1529733100, \u0026#34;cancellable\u0026#34;: true, \u0026#34;headers\u0026#34;: {} } } } } } 您还可以在查询中使用以下参数。\n   参数 数据类型 描述     nodes List 以逗号分隔的节点 ID 或名称列表，用于限制返回的信息。使用 _local 从要连接的节点返回信息，指定节点名称以从特定节点获取信息，或将参数保持为空以从所有节点获取信息。   actions List 应返回的操作的逗号分隔列表。保留为空以返回全部。   detailed Boolean 返回详细的任务信息。（Default: false）   parent_task_id String 返回具有指定父任务 ID（node_id:task_number）的任务。保持为空或设置为 -1 以返回全部。   wait_for_completion Boolean 等待匹配的任务完成。（Default: false）   group_by Enum 按父/子关系或节点对任务进行分组。（Default: nodes）   timeout Time 显式操作超时。（Default: 30 seconds）   master_timeout Time 等待连接到主节点的时间。（Default: 30 seconds）    例如，此请求返回当前在名为 easy-node1 的节点上运行的任务。\n请求示例\nGET /_tasks?nodes=easy-node1 响应示例\n{ \u0026#34;nodes\u0026#34;: { \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;easy-node1\u0026#34;, \u0026#34;transport_address\u0026#34;: \u0026#34;sample_address\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;sample_host\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;sample_ip\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;data\u0026#34;, \u0026#34;ingest\u0026#34;, \u0026#34;master\u0026#34;, \u0026#34;remote_cluster_client\u0026#34;], \u0026#34;tasks\u0026#34;: { \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:24578\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;, \u0026#34;id\u0026#34;: 24578, \u0026#34;type\u0026#34;: \u0026#34;transport\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;cluster:monitor/tasks/lists\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1611612517044, \u0026#34;running_time_in_nanos\u0026#34;: 638700, \u0026#34;cancellable\u0026#34;: false, \u0026#34;headers\u0026#34;: {} }, \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:24579\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;, \u0026#34;id\u0026#34;: 24579, \u0026#34;type\u0026#34;: \u0026#34;direct\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;cluster:monitor/tasks/lists[n]\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1611612517044, \u0026#34;running_time_in_nanos\u0026#34;: 222200, \u0026#34;cancellable\u0026#34;: false, \u0026#34;parent_task_id\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:24578\u0026#34;, \u0026#34;headers\u0026#34;: {} } } } } } 放弃任务 #  获取任务列表后，您可以使用以下请求取消所有可取消的任务：\nPOST _tasks/_cancel 请注意，并非所有任务都可以取消。要查看任务是否可取消，请参阅对 tasks API 请求的响应中的 cancellable 字段。\n您还可以通过包含特定任务 ID 来取消任务。\nPOST _tasks/\u0026lt;task_id\u0026gt;/_cancel cancel 操作支持与 tasks 操作相同的参数。以下示例显示如何取消多个节点上的所有可取消任务。\nPOST _tasks/_cancel?nodes=easy-node1,easy-node2 将头信息附加到任务 #  为了将请求与任务关联起来以便更好地跟踪，可以在 curl 命令的 HTTPS 请求读取器中提供 X-Opaque-Id:\u0026lt;Id_number\u0026gt; header 。API 将在返回的结果中附加指定的头。\n用法：\ncurl -i -H \u0026#34;X-Opaque-Id: 111111\u0026#34; \u0026#34;https://localhost:9200/_tasks\u0026#34; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; --insecure _tasks 操作返回以下结果。\nHTTP/1.1 200 OK X-Opaque-Id: 111111 content-type: application/json; charset=UTF-8 content-length: 768 { \u0026quot;nodes\u0026quot;: { \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;easy-node1\u0026quot;, \u0026quot;transport_address\u0026quot;: \u0026quot;30.18.0.4:9300\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;30.18.0.4\u0026quot;, \u0026quot;ip\u0026quot;: \u0026quot;30.18.0.4:9300\u0026quot;, \u0026quot;roles\u0026quot;: [ \u0026quot;data\u0026quot;, \u0026quot;ingest\u0026quot;, \u0026quot;master\u0026quot;, \u0026quot;remote_cluster_client\u0026quot; ], \u0026quot;tasks\u0026quot;: { \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ:30072\u0026quot;: { \u0026quot;node\u0026quot;: \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ\u0026quot;, \u0026quot;id\u0026quot;: 30072, \u0026quot;type\u0026quot;: \u0026quot;direct\u0026quot;, \u0026quot;action\u0026quot;: \u0026quot;cluster:monitor/tasks/lists[n]\u0026quot;, \u0026quot;start_time_in_millis\u0026quot;: 161316670305, \u0026quot;running_time_in_nanos\u0026quot;: 245400, \u0026quot;cancellable\u0026quot;: false, \u0026quot;parent_task_id\u0026quot;: \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ:30071\u0026quot;, \u0026quot;headers\u0026quot;: { \u0026quot;X-Opaque-Id\u0026quot;: \u0026quot;111111\u0026quot; } }, \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ:30071\u0026quot;: { \u0026quot;node\u0026quot;: \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ\u0026quot;, \u0026quot;id\u0026quot;: 30071, \u0026quot;type\u0026quot;: \u0026quot;transport\u0026quot;, \u0026quot;action\u0026quot;: \u0026quot;cluster:monitor/tasks/lists\u0026quot;, \u0026quot;start_time_in_millis\u0026quot;: 161316670305, \u0026quot;running_time_in_nanos\u0026quot;: 658200, \u0026quot;cancellable\u0026quot;: false, \u0026quot;headers\u0026quot;: { \u0026quot;X-Opaque-Id\u0026quot;: \u0026quot;111111\u0026quot; } } } } } } 此操作支持与 任务 操作相同的参数。以下示例显示了如何将 X-Opaque-Id 与特定任务相关联。\ncurl -i -H \u0026#34;X-Opaque-Id: 123456\u0026#34; \u0026#34;https://localhost:9200/_tasks?nodes=easy-node1\u0026#34; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; --insecure ","subcategory":null,"summary":"","tags":null,"title":"任务管理","url":"/easysearch/main/docs/operations/cluster-admin/tasks/"},{"category":null,"content":"Geo Shape 查询 #  地理形状查询用于搜索 geo_point 或 geo_shape 字段的文档。可以使用查询中内联定义的形状，或引用预索引的形状来过滤文档。\n空间关系 #  当您向地理形状查询提供地理形状时，文档中的地理点和地理形状字段将使用以下空间关系与提供的形状进行匹配。\n   关系 描述 支持地理字段类型     INTERSECTS （默认）匹配与查询中提供的形状相交的 geopoint 或 geoshape 的文档。 geo_point, geo_shape   DISJOINT 匹配与查询中提供的形状不相交的 geoshape 的文档。 geo_shape   WITHIN 匹配完全位于查询中提供的形状内的 geoshape 的文档。 geo_shape   CONTAINS 匹配完全包含查询中提供的形状的文档。 geo_shape    在 geoshape 查询中定义形状 #  您可以在 geoshape 查询中通过在查询时提供新的形状定义或引用另一个索引中预先索引的形状名称来定义形状以过滤文档。\n使用新的形状定义 #  为了向地理形状查询提供新的形状，请在 geo_shape 字段中定义它。您必须以 GeoJSON 格式定义地理形状。\n以下示例说明了搜索包含在查询时定义的地理形状的文档。\n首先，创建一个索引并将 location 字段映射为 geo_shape ：\nPUT /testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_shape\u0026#34; } } } } 索引包含一个点和另一个包含多边形的文档：\nPUT testindex/_doc/1 { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;point\u0026#34;, \u0026#34;coordinates\u0026#34;: [ 73.0515, 41.5582 ] } } PUT testindex/_doc/2 { \u0026quot;location\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;polygon\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ [ 73.0515, 41.5582 ], [ 72.6506, 41.5623 ], [ 72.6734, 41.7658 ], [ 73.0515, 41.5582 ] ] ] } } 最后，定义一个 geoshape 以过滤文档。以下各节展示了在查询中提供各种 geoshape 的方法。有关各种 geoshape 格式的更多信息，请参阅 Geoshape 字段类型。\nEnvelope 边界矩形 #  一个 envelope 是 [[minLon, maxLat], [maxLon, minLat]] 格式中的一个边界矩形。搜索包含与提供的边界矩形相交的 geoshape 字段文档：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;geo_shape\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;shape\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;envelope\u0026#34;, \u0026#34;coordinates\u0026#34;: [ [ 71.0589, 42.3601 ], [ 74.006, 40.7128 ] ] }, \u0026#34;relation\u0026#34;: \u0026#34;WITHIN\u0026#34; } } } } 返回内容包含两个文档：\n{ \u0026#34;took\u0026#34;: 5, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0, \u0026#34;_source\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;point\u0026#34;, \u0026#34;coordinates\u0026#34;: [ 73.0515, 41.5582 ] } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0, \u0026#34;_source\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34;: [ [ [ 73.0515, 41.5582 ], [ 72.6506, 41.5623 ], [ 72.6734, 41.7658 ], [ 73.0515, 41.5582 ] ] ] } } } ] } } Point 点位 #\n 搜索包含提供的点的地理形状字段的文档：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;geo_shape\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;shape\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;point\u0026#34;, \u0026#34;coordinates\u0026#34;: [ 72.8000, 41.6300 ] }, \u0026#34;relation\u0026#34;: \u0026#34;CONTAINS\u0026#34; } } } } Linestring 线字符串 #  搜索与提供的线字符串不交叉的地理形状字段文档：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;geo_shape\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;shape\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linestring\u0026#34;, \u0026#34;coordinates\u0026#34;: [[74.0060, 40.7128], [71.0589, 42.3601]] }, \u0026#34;relation\u0026#34;: \u0026#34;DISJOINT\u0026#34; } } } }  线字符串地理形状查询不支持 WITHIN 关系。\n Multipolygon 多边形 #  搜索地理形状字段在提供的多边形内的文档：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;geo_shape\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;shape\u0026#34;: { \u0026#34;type\u0026#34; : \u0026#34;multipolygon\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [ [ [74.0060, 40.7128], [73.7562, 42.6526], [71.0589, 42.3601], [74.0060, 40.7128] ], [ [73.0515, 41.5582], [72.6506, 41.5623], [72.6734, 41.7658], [73.0515, 41.5582] ] ], [ [ [73.9146, 40.8252], [73.8871, 41.0389], [73.6853, 40.9747], [73.9146, 40.8252] ] ] ] }, \u0026#34;relation\u0026#34;: \u0026#34;WITHIN\u0026#34; } } } } Geometry collection 几何集合 #  搜索地理形状字段在提供的多边形内的文档：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;geo_shape\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;shape\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geometrycollection\u0026#34;, \u0026#34;geometries\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34;: [[ [74.0060, 40.7128], [73.7562, 42.6526], [71.0589, 42.3601], [74.0060, 40.7128] ]] }, { \u0026#34;type\u0026#34;: \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34;: [[ [73.0515, 41.5582], [72.6506, 41.5623], [72.6734, 41.7658], [73.0515, 41.5582] ]] } ] }, \u0026#34;relation\u0026#34;: \u0026#34;WITHIN\u0026#34; } } } }  Geoshape 查询中，如果几何集合包含线字符串或多线字符串，则不支持 WITHIN 关系。\n 使用预索引的形状定义 #  在构建 geoshape 查询时，您还可以引用另一个索引中预先索引的形状的名称。使用此方法，您可以在索引时定义 geoshape，并在搜索时通过名称引用它。\n您可以使用 GeoJSON 或 WKT 格式定义预索引的 geoshape。有关各种 geoshape 格式的更多信息，请参阅 Geoshape 字段类型。\nindexed_shape 对象支持以下参数。\n   参数 必填/可选 描述     id 必填 包含预索引形状的文档的文档 ID。   index 可选 包含预索引形状的索引名称。默认为 shapes 。   path 可选 包含预索引形状路径的字段名称。默认为 shape 。   routing 可选 包含预索引形状的文档的路由。    以下示例说明了如何引用另一个索引中预索引的形状的名称。在此示例中，索引 pre-indexed-shapes 包含定义边界的形状，索引 testindex 包含与这些边界进行校验的形状。\n首先，创建 pre-indexed-shapes 索引并将此索引的 boundaries 字段映射为 geo_shape :\nPUT /pre-indexed-shapes { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;boundaries\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_shape\u0026#34;, \u0026#34;orientation\u0026#34; : \u0026#34;left\u0026#34; } } } } 关于为多边形指定不同的顶点方向，请参阅《多边形》。\n将指定搜索边界的多边形索引到 pre-indexed-shapes 索引中。多边形的 ID 是 search_triangle 。在此示例中，您将以 WKT 格式索引多边形：\nPUT /pre-indexed-shapes/_doc/search_triangle { \u0026#34;boundaries\u0026#34;: \u0026#34;POLYGON ((74.0060 40.7128, 71.0589 42.3601, 73.7562 42.6526, 74.0060 40.7128))\u0026#34; } 如果您尚未这样做，请将包含一个点的一个文档和包含一个多边形的一个文档索引到 testindex 索引中：\nPUT /testindex/_doc/1 { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;point\u0026#34;, \u0026#34;coordinates\u0026#34;: [ 73.0515, 41.5582 ] } } PUT /testindex/_doc/2 { \u0026quot;location\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;polygon\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ [ 73.0515, 41.5582 ], [ 72.6506, 41.5623 ], [ 72.6734, 41.7658 ], [ 73.0515, 41.5582 ] ] ] } } 搜索地理形状在 search_triangle 内的文档：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;filter\u0026#34;: { \u0026#34;geo_shape\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;indexed_shape\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;pre-indexed-shapes\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;search_triangle\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;boundaries\u0026#34; }, \u0026#34;relation\u0026#34;: \u0026#34;WITHIN\u0026#34; } } } } } } 返回内容包含两个文档：\n{ \u0026#34;took\u0026#34;: 11, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;point\u0026#34;, \u0026#34;coordinates\u0026#34;: [ 73.0515, 41.5582 ] } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34;: [ [ [ 73.0515, 41.5582 ], [ 72.6506, 41.5623 ], [ 72.6734, 41.7658 ], [ 73.0515, 41.5582 ] ] ] } } } ] } } 查询地理点 #\n 您还可以使用地理形状查询来搜索包含地理点的文档。\n 地理点字段的地理形状查询仅支持默认的 INTERSECTS 空间关系，因此您不需要提供 relation 参数。\n  地理点字段的地理形状查询不支持以下地理形状：\n Points 点 Linestrings 线字符串 Multipoints 多点 Multilinestrings 多线字符串 Geometry collections containing one of the preceding geoshape types 包含前述地理形状类型之一的几何集合   创建一个映射，其中 location 是 geo_point ：\nPUT /testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; } } } } 将两个点索引到索引中。在这个例子中，您将提供地理坐标点作为字符串：\nPUT /testindex1/_doc/1 { \u0026#34;location\u0026#34;: \u0026#34;41.5623, 72.6506\u0026#34; } PUT /testindex1/_doc/2 { \u0026quot;location\u0026quot;: \u0026quot;76.0254, 39.2467\u0026quot; } 搜索与提供的多边形相交的地理点：\nGET /testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;geo_shape\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;shape\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34;: [ [ [74.0060, 40.7128], [73.7562, 42.6526], [71.0589, 42.3601], [74.0060, 40.7128] ] ] } } } } } 返回文档 1：\n{ \u0026#34;took\u0026#34;: 21, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0, \u0026#34;_source\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;41.5623, 72.6506\u0026#34; } } ] } } 请注意，当您索引地理点时，您指定了它们的坐标为 \u0026quot;latitude, longitude\u0026quot; 格式。当您搜索匹配的文档时，坐标数组为 [longitude, latitude] 格式。因此，文档 1 在结果中返回，但文档 2 则没有。\n参数说明 #  Geoshape 查询接受以下参数。\n   参数 数据类型 描述     relation String 空间关系。有效值为 INTERSECTS（默认）、DISJOINT、WITHIN、CONTAINS。   ignore_unmapped Boolean 指定是否忽略未映射的字段。如果设置为 true，则查询不返回包含未映射字段的任何文档。如果设置为 false，则在字段未映射时抛出异常。可选。默认为 false。   boost Float 查询的相关性得分权重。默认为 1.0。    ","subcategory":null,"summary":"","tags":null,"title":"Geo Shape 查询","url":"/easysearch/main/docs/features/geo-search/geo-shape-query/"},{"category":null,"content":"搜索分词器 #  搜索分词器是在查询时指定的，当你对text字段进行全文匹配查询时，它会被用于分析被查询字符串。\n搜索分词器的生效流程 #  在查询时确定对查询字符串使用哪种分词器，Easysearch 会按以下顺序检查以下参数：\n 查询的 analyzer 参数 字段的 search_analyzer 映射参数 索引设置 analysis.analyzer.default_search 字段的 analyzer 映射参数 standard 分词器（默认分词器）   在大多数情况下，没有必要指定与索引分词器不同的搜索分词器，而且这样做可能会对搜索结果的相关性产生负面影响，或者导致出现意想不到的搜索结果。\n 为查询内容指定搜索分词器 #  在查询时，你可以在 analyzer 字段中指定想要使用的分词器：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;speak the truth\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;english\u0026#34; } } } } 为字段指定搜索分词器 #  在创建索引映射时，你可以为每个文本字段提供 search_analyzer 参数。在提供 search_analyzer 时，你还必须提供 analyzer 参数，该参数指定在索引创建时的写入索引分词器内应用。\n例如，以下请求将 simple 分词器指定为 text_entry 字段的索引分词器，将 whitespace 分词器指定为该字段的搜索分词器：\nPUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;simple\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;whitespace\u0026#34; } } } } 为索引指定默认的搜索分词器 #  如果你希望在搜索时使用同一个分词器来解析所有的查询，你可以在 analysis.analyzer.default_search 设置中指定该搜索分词器。在提供 analysis.analyzer.default_search 时，你还必须提供 analysis.analyzer.default 参数，该参数指定了在创建索引时要使用的索引分词器。\n例如，以下请求将 simple 分词器指定为 testindex 索引的索引分词器，并将 whitespace 分词器指定为该索引的搜索分词器\nPUT testindex { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;simple\u0026#34; }, \u0026#34;default_search\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;whitespace\u0026#34; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"搜索分析器（Search Analyzer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/search-analyzers/"},{"category":null,"content":"悬挂索引（Dangling Indices） #  什么是悬挂索引 #  悬挂索引是指存在于节点本地磁盘上，但不属于当前集群状态的索引。这种情况通常发生在：\n 节点离线期间，集群中其他节点删除了某个索引 节点从一个集群迁移到另一个集群 快照恢复失败或中断 集群状态丢失后重建  当节点重新加入集群时，它本地存储的这些\u0026quot;孤立\u0026quot;索引分片就成为悬挂索引。\n自动导入 #  可以通过集群设置控制是否自动导入悬挂索引：\n# easysearch.yml gateway.auto_import_dangling_indices: false # 默认值：false  注意：自动导入默认关闭。在生产环境中，建议保持关闭，手动检查并决定是否导入，以避免意外引入过期或不需要的数据。\n API 操作 #  列出悬挂索引 #  GET /_dangling 响应示例：\n{ \u0026#34;dangling_indices\u0026#34;: [ { \u0026#34;index_name\u0026#34;: \u0026#34;my_old_index\u0026#34;, \u0026#34;index_uuid\u0026#34;: \u0026#34;r1eSJ3MoTheHQ2CvFoVrOg\u0026#34;, \u0026#34;creation_date_millis\u0026#34;: 1609459200000, \u0026#34;node_ids\u0026#34;: [\u0026#34;node-1\u0026#34;] } ] } 导入悬挂索引 #  使用索引 UUID 将悬挂索引导入到集群中：\nPOST /_dangling/r1eSJ3MoTheHQ2CvFoVrOg?accept_data_loss=true accept_data_loss=true 参数是必须的，表示您已了解导入可能存在数据不一致的风险。\n删除悬挂索引 #  不需要的悬挂索引可以直接删除：\nDELETE /_dangling/r1eSJ3MoTheHQ2CvFoVrOg?accept_data_loss=true 操作建议 #     场景 建议操作     节点短暂离线后重新加入 检查索引内容后决定是否导入   集群迁移残留数据 确认数据已在新集群中恢复后删除   集群状态丢失重建 列出所有悬挂索引，逐一导入恢复   来源不明的悬挂索引 谨慎处理，建议先备份再决定    注意事项 #   导入悬挂索引不会自动恢复副本分片，需要等待集群自行分配 如果集群中已存在同名索引（但 UUID 不同），导入会失败 悬挂索引的映射和设置保持离线前的状态，可能与当前集群配置不兼容 建议在导入前通过 _dangling API 检查索引的创建时间，确认是否是期望的数据  ","subcategory":null,"summary":"","tags":null,"title":"悬挂索引","url":"/easysearch/main/docs/operations/data-management/dangling-indices/"},{"category":null,"content":"Geo Polygon 查询 #  地理多边形查询返回 geo_point 字段值位于指定多边形内的文档。如果一个文档包含多个地理坐标点，只要至少有一个点在多边形内，该文档就匹配。\n多边形通过坐标顶点列表指定，不需要闭合（首尾相同不是必须的），但建议按顺时针或逆时针顺序列出。\n参考样例 #  创建一个映射，将 point 字段映射为 geo_point ：\nPUT /testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; } } } } 索引一个地理点，指定其纬度和经度：\nPUT testindex1/_doc/1 { \u0026#34;point\u0026#34;: { \u0026#34;lat\u0026#34;: 73.71, \u0026#34;lon\u0026#34;: 41.32 } } 搜索包含指定 geo_polygon 的 point 对象的文档：\nGET /testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;filter\u0026#34;: { \u0026#34;geo_polygon\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;points\u0026#34;: [ { \u0026#34;lat\u0026#34;: 74.5627, \u0026#34;lon\u0026#34;: 41.8645 }, { \u0026#34;lat\u0026#34;: 73.7562, \u0026#34;lon\u0026#34;: 42.6526 }, { \u0026#34;lat\u0026#34;: 73.3245, \u0026#34;lon\u0026#34;: 41.6189 }, { \u0026#34;lat\u0026#34;: 74.0060, \u0026#34;lon\u0026#34;: 40.7128 } ] } } } } } } 前一个请求中指定的地理位置形成一个四边形。匹配的文档位于此四边形内。四边形的顶点坐标以 (latitude, longitude) 格式指定。\n返回内容包含匹配的文档：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;lat\u0026#34;: 73.71, \u0026#34;lon\u0026#34;: 41.32 } } } ] } } 在先前的搜索请求中，您指定了多边形的顶点按顺时针顺序：\n\u0026#34;geo_polygon\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;points\u0026#34;: [ { \u0026#34;lat\u0026#34;: 74.5627, \u0026#34;lon\u0026#34;: 41.8645 }, { \u0026#34;lat\u0026#34;: 73.7562, \u0026#34;lon\u0026#34;: 42.6526 }, { \u0026#34;lat\u0026#34;: 73.3245, \u0026#34;lon\u0026#34;: 41.6189 }, { \u0026#34;lat\u0026#34;: 74.0060, \u0026#34;lon\u0026#34;: 40.7128 } ] } } 或者，您也可以按逆时针顺序指定顶点：\n\u0026#34;geo_polygon\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;points\u0026#34;: [ { \u0026#34;lat\u0026#34;: 74.5627, \u0026#34;lon\u0026#34;: 41.8645 }, { \u0026#34;lat\u0026#34;: 74.0060, \u0026#34;lon\u0026#34;: 40.7128 }, { \u0026#34;lat\u0026#34;: 73.3245, \u0026#34;lon\u0026#34;: 41.6189 }, { \u0026#34;lat\u0026#34;: 73.7562, \u0026#34;lon\u0026#34;: 42.6526 } ] } } 查询结果包含相同的匹配文档。\n然而，如果您按照以下顺序指定顶点：\n\u0026#34;geo_polygon\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;points\u0026#34;: [ { \u0026#34;lat\u0026#34;: 74.5627, \u0026#34;lon\u0026#34;: 41.8645 }, { \u0026#34;lat\u0026#34;: 74.0060, \u0026#34;lon\u0026#34;: 40.7128 }, { \u0026#34;lat\u0026#34;: 73.7562, \u0026#34;lon\u0026#34;: 42.6526 }, { \u0026#34;lat\u0026#34;: 73.3245, \u0026#34;lon\u0026#34;: 41.6189 } ] } } 则不返回任何结果。\n参数说明 #  Geopolygon 查询接受以下参数。\n   参数 数据类型 描述     _name String 过滤器名称。可选。   validation_method String 验证方法。有效值是 IGNORE_MALFORMED （接受坐标无效的地理点）、 COERCE （尝试将坐标转换为有效值）和 STRICT （当坐标无效时返回错误）。可选。默认是 STRICT 。   ignore_unmapped Boolean 指定是否忽略未映射的字段。如果设置为 true ，则查询不返回包含未映射字段的任何文档。如果设置为 false ，则在字段未映射时抛出异常。可选。默认为 false 。   boost Float 查询的相关性得分权重。默认为 1.0。    ","subcategory":null,"summary":"","tags":null,"title":"Geo Polygon 查询","url":"/easysearch/main/docs/features/geo-search/geo-polygon/"},{"category":null,"content":"索引分词器 #  索引分词器是在索引创建时指定的，在文档写入时对文本字段进行分词。\n写入索引分词器的生效流程 #  为了确定在对文档建立索引时对某个字段使用哪种分词器，Easysearch 会按以下顺序检查以下参数：\n 字段的分词器映射参数，即analyzer analysis.analyzer.default 索引设置 standard标准分词器（默认分词器）   在指定索引分词器时，请记住，在大多数情况下，为索引中的每个文本字段指定一个分词器效果最佳。使用相同的分词器在文本字段（在建立索引时）和查询字符串（在查询时），可确保搜索所使用的词项与存储在索引中的词项相同。\n 为字段指定索引分词器 #  在创建索引映射时，可以为每个文本字段提供 analyzer（分词器）参数。例如，以下请求为 text_entry 字段指定了 simple 简单分词器：\nPUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;simple\u0026#34; } } } } 为索引指定默认索引分词器 #  如果想对索引中的所有文本字段使用相同的分词器，则可以在索引设置中按如下方式进行指定analysis.analyzer.default：\nPUT testindex { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;simple\u0026#34; } } } } }  如果您未指定默认分词器，那么将使用standard标准分词器。\n ","subcategory":null,"summary":"","tags":null,"title":"索引分析器（Index Analyzer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/index-analyzers/"},{"category":null,"content":"Geo Distance 查询 #  地理距离查询返回 geo_point 字段值位于指定中心点给定距离范围内的文档。如果一个文档包含多个地理坐标点，只要至少有一个点在距离范围内，该文档就匹配。\n参考样例 #  创建一个映射，将 point 字段映射为 geo_point ：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; } } } } 索引一个地理点，指定其纬度和经度：\nPUT testindex1/_doc/1 { \u0026#34;point\u0026#34;: { \u0026#34;lat\u0026#34;: 74.00, \u0026#34;lon\u0026#34;: 40.71 } } 搜索距离指定的 point 内包含指定 distance 对象的文档：\nGET /testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;filter\u0026#34;: { \u0026#34;geo_distance\u0026#34;: { \u0026#34;distance\u0026#34;: \u0026#34;50mi\u0026#34;, \u0026#34;point\u0026#34;: { \u0026#34;lat\u0026#34;: 73.5, \u0026#34;lon\u0026#34;: 40.5 } } } } } } 返回内容包含匹配的文档：\n{ \u0026#34;took\u0026#34;: 5, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;lat\u0026#34;: 74, \u0026#34;lon\u0026#34;: 40.71 } } } ] } } 参数说明 #\n Geodistance 查询接受以下参数。\n   参数 数据类型 描述     _name String 过滤器名称。可选。   distance String 匹配点的距离范围。此距离是以指定点为中心的圆的半径。有关支持的距离单位，请参阅距离单位。必需。   distance_type String 指定如何计算距离。有效值是 arc 或 plane （对于长距离或接近极点的点，速度快但不够准确）。可选。默认是 arc 。   validation_method String 验证方法。有效值是 IGNORE_MALFORMED （接受坐标无效的地理点）、 COERCE （尝试将坐标转换为有效值）和 STRICT （当坐标无效时返回错误）。可选。默认是 STRICT 。   ignore_unmapped Boolean 指定是否忽略未映射的字段。如果设置为 true ，则查询不返回包含未映射字段的任何文档。如果设置为 false ，则在字段未映射时抛出异常。可选。默认为 false 。   boost Float 查询的相关性得分权重。默认为 1.0。    ","subcategory":null,"summary":"","tags":null,"title":"Geo Distance 查询","url":"/easysearch/main/docs/features/geo-search/geo-distance/"},{"category":null,"content":"龙芯平台安装 #  龙芯平台介绍 #  龙芯平台基于自主指令集架构（LoongArch），完全自主研发，不依赖国外技术，广泛应用于信创桌面、服务器及工控领域，强调安全可控与生态自主。\n龙芯平台安装参考 #  目前，Easysearch 已支持在龙芯芯片的国产操作系统上运行，联网环境建议使用一键安装脚本进行安装，离线环境建议下载 Bundle 包进行安装，分布式集群安装请参考 分布式集群安装。\n 前提条件：已参照 系统调优进行了系统优化，同时为 Easysearch 创建了专用的用户。\n 初始化系统参数及用户命令参考 #  # 调整内核配置 echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p # 增加用户组与用户 groupadd -r easysearch \u0026amp;\u0026amp; useradd -r -g easysearch -d /home/easysearch -s /sbin/nologin -c \u0026#34;Easysearch Service Account\u0026#34; easysearch 安装命令参考 #  # 创建数据目录 mkdir -p /opt/easysearch # 下载最新版本的 Easysearch 并安装 curl -sSL http://get.infini.cloud | bash -s -- -p easysearch -d /opt/easysearch # 进入 Easysearch 目录 cd /opt/easysearch # 初始化 Easysearch bin/initialize.sh -s # 调整目录权限 chown -R easysearch:easysearch /opt/easysearch # 启动 Easysearch runuser -u easysearch -- /opt/easysearch/bin/easysearch -d -p /opt/easysearch/easysearch.pid  注意：初始化过程中会生成随机密码，并不会保存到日志文件中，只会在终端显示一次，请妥善保存。如果忘记 admin 密码，可以使用 bin/reset_admin_password.sh 进行重置。\n 已验证系统环境：\n Loongson-3C5000L/Loongnix-Server Linux release 8.4.1  如果您在其他龙芯平台的国产操作系统上安装遇到问题，欢迎通过 提交工单与我们联系。\n","subcategory":null,"summary":"","tags":null,"title":"龙芯平台安装","url":"/easysearch/main/docs/deployment/install-guide/ciip/loongson/"},{"category":null,"content":"高亮 #  高亮用于在返回结果中标记命中的文本片段，提升可读性。Easysearch 在搜索阶段记录哪些文本片段匹配了查询，在返回阶段根据这些信息对原文做截取与包裹（例如 \u0026lt;em\u0026gt;...\u0026lt;/em\u0026gt; 标签）。\n基本用法 #  GET /articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;搜索 引擎\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;content\u0026#34;: {} } } } 返回结果中每条命中文档会包含 highlight 字段：\n{ \u0026#34;hits\u0026#34;: { \u0026#34;hits\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Easysearch 是一款高性能的搜索引擎...\u0026#34; }, \u0026#34;highlight\u0026#34;: { \u0026#34;content\u0026#34;: [ \u0026#34;Easysearch 是一款高性能的\u0026lt;em\u0026gt;搜索\u0026lt;/em\u0026gt;\u0026lt;em\u0026gt;引擎\u0026lt;/em\u0026gt;...\u0026#34; ] } } ] } } 三种高亮器类型 #  Easysearch 支持三种高亮器，通过 type 参数指定：\n   类型 说明 优缺点     unified（默认） 基于 Lucene Unified Highlighter，使用 BM25 对片段评分 推荐首选。支持所有字段类型，性能与准确性平衡最佳   plain 基于标准 Lucene Highlighter，逐词高亮 小文本场景尚可，大文本或复杂查询时性能较差   fvh Fast Vector Highlighter，需要 term_vector 设置为 with_positions_offsets 大文本高亮最快，但需要额外索引存储    使用 FVH 高亮器 #  PUT /articles { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;with_positions_offsets\u0026#34; } } } } GET /articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;搜索 引擎\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;fvh\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;content\u0026#34;: {} } } } 常用参数 #  GET /articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;搜索 引擎\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;pre_tags\u0026#34;: [\u0026#34;\u0026lt;strong\u0026gt;\u0026#34;], \u0026#34;post_tags\u0026#34;: [\u0026#34;\u0026lt;/strong\u0026gt;\u0026#34;], \u0026#34;fields\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;fragment_size\u0026#34;: 150, \u0026#34;number_of_fragments\u0026#34;: 3, \u0026#34;no_match_size\u0026#34;: 100 }, \u0026#34;title\u0026#34;: { \u0026#34;number_of_fragments\u0026#34;: 0 } } } } 参数速查 #     参数 类型 默认值 说明     pre_tags String[] [\u0026quot;\u0026lt;em\u0026gt;\u0026quot;] 高亮前标签   post_tags String[] [\u0026quot;\u0026lt;/em\u0026gt;\u0026quot;] 高亮后标签   fragment_size Integer 100 每个片段的字符数   number_of_fragments Integer 5 每字段返回的最大片段数。设为 0 返回整个字段   no_match_size Integer 0 无匹配时从字段开头返回的字符数   type String \u0026quot;unified\u0026quot; 高亮器类型：unified、plain、fvh   order String - 设为 \u0026quot;score\u0026quot; 按相关性排序片段   require_field_match Boolean true 只高亮查询匹配到的字段   encoder String \u0026quot;default\u0026quot; \u0026quot;default\u0026quot; 或 \u0026quot;html\u0026quot;（对 HTML 特殊字符编码）   highlight_query Object - 使用不同于搜索查询的查询来做高亮   boundary_scanner String \u0026quot;sentence\u0026quot;（unified）/ \u0026quot;chars\u0026quot;（plain/fvh） 边界扫描器：chars、word、sentence   boundary_chars String \u0026quot;.,!? \\t\\n\u0026quot; 边界字符（与 chars 扫描器配合）   boundary_max_scan Integer 20 向前扫描边界字符的最大距离   boundary_scanner_locale String Locale.ROOT 与 word/sentence 扫描器配合使用的语言区域设置   fragmenter String \u0026quot;span\u0026quot; 指定片段分割方式：span（按查询匹配位置分割）或 simple（按固定大小分割）。仅适用于 plain 高亮器   tags_schema String - 设为 \u0026quot;styled\u0026quot; 使用内置的 \u0026lt;em class=\u0026quot;hltN\u0026quot;\u0026gt; 标签方案（N=1..10），替代自定义 pre_tags/post_tags   max_analyzed_offset Integer 1000000 限制分析的最大字符偏移量。超过此值的文本不会被高亮。可在索引级通过 index.highlight.max_analyzed_offset 配置   phrase_limit Integer 256 FVH 考虑的最大短语数   force_source Boolean false 强制从 _source 读取高亮文本   matched_fields String[] - FVH 专用：合并多个字段的匹配做高亮    多字段高亮 #  不同字段可以使用不同的高亮配置：\nGET /articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;分布式 搜索\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title^3\u0026#34;, \u0026#34;content\u0026#34;] } }, \u0026#34;highlight\u0026#34;: { \u0026#34;pre_tags\u0026#34;: [\u0026#34;\u0026lt;mark\u0026gt;\u0026#34;], \u0026#34;post_tags\u0026#34;: [\u0026#34;\u0026lt;/mark\u0026gt;\u0026#34;], \u0026#34;fields\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;number_of_fragments\u0026#34;: 0 }, \u0026#34;content\u0026#34;: { \u0026#34;fragment_size\u0026#34;: 200, \u0026#34;number_of_fragments\u0026#34;: 3, \u0026#34;order\u0026#34;: \u0026#34;score\u0026#34; } } } }  title 设置 number_of_fragments: 0 表示返回整个字段内容并高亮，适合短字段。\n 使用自定义查询做高亮 #  有时你希望搜索和高亮使用不同的查询条件：\nGET /articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;搜索\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;published\u0026#34; } } } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;highlight_query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;搜索 引擎 分布式\u0026#34; } } } } } } 性能考量 #  高亮不是免费的：\n unified（默认）性能最好，适合大多数场景 fvh 对大文本最快，但需要 term_vector: with_positions_offsets，会增加索引大小约 30–50% plain 对大文本性能最差，应避免在大文本字段上使用  建议：\n 只对需要展示的字段做高亮，避免全字段高亮 控制 fragment_size 和 number_of_fragments，避免返回过大的高亮结果 对超长文本，考虑在建模时预先准备\u0026quot;摘要字段\u0026quot;，在摘要上做高亮 高并发场景优先使用 unified  常见问题 #   匹配到结果但没有高亮：检查查询字段与高亮字段是否一致（如 content vs content.keyword），检查字段 analyzer 是否匹配 高亮片段截断不理想：调整 fragment_size，或使用 boundary_scanner: \u0026quot;sentence\u0026quot; 按句子切分 中文高亮分词不对：确保高亮字段和查询使用相同的分词器（如 IK 分词器） HTML 标签被转义：使用 \u0026quot;encoder\u0026quot;: \u0026quot;html\u0026quot; 对结果中的 HTML 特殊字符编码  小结 #   高亮用于在返回结果中标记命中的文本片段，提升可读性 三种高亮器：unified（推荐）、plain、fvh（大文本最快） 可以为不同字段配置不同的高亮选项 控制高亮字段数量和片段大小，关注性能影响  下一步可以继续阅读：\n  建议与纠错  分页与排序  相关性  参考手册（API 与参数） #    高亮参数详解（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"高亮","url":"/easysearch/main/docs/features/fulltext-search/highlighting/"},{"category":null,"content":"集群配置 #  本文介绍通过集群设置 API 管理 Easysearch 集群级别配置的方式，包括动态设置的查看、修改、重置，以及可通过 API 调整的全部动态配置项参考。\n 节点级别的静态配置（需要写在 easysearch.yml 中、重启后生效的配置）请参见 节点配置。\n  集群设置 API #  大多数集群级调优参数都可以通过集群设置 API 在运行时更改，无需重启节点。\n查看设置 #  # 查看所有设置（包含默认值） GET _cluster/settings?include_defaults=true # 仅查看用户自定义设置 GET _cluster/settings 设置优先级 #\n 集群设置 API 中存在三类设置：持久（Persistent）、临时（Transient）和默认（Default）。\n优先级从高到低：\n Transient 设置 — 临时，集群重启后丢失 Persistent 设置 — 持久，跨重启保留 配置文件 easysearch.yml 默认值   推荐做法：节点相关配置放 easysearch.yml，集群范围的调优通过 API 以 persistent 方式修改，避免每个节点单独改配置文件。\n 修改设置 #  // 持久化修改（推荐用于集群调优） PUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;action.auto_create_index\u0026#34;: false } } // 临时修改（适合调试，重启后失效） PUT /_cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;logger.org.easysearch.discovery\u0026quot;: \u0026quot;DEBUG\u0026quot; } } 重置设置 #\n 将配置值设为 null 即可重置为默认值：\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;action.auto_create_index\u0026#34;: null } }  动态配置项参考 #  以下配置项均可通过 PUT /_cluster/settings 在线修改，无需重启节点。\n索引行为 #     参数 默认值 说明     action.auto_create_index true 是否允许自动创建索引。可设为 false 禁止，或指定允许的模式如 +logs-*,-*   action.destructive_requires_name false 是否要求删除索引时必须指定具体名称，禁止通配符和 _all。生产环境建议设为 true    索引默认值 #   以下为集群级别的索引默认值，作用于新创建的索引。部分参数也可以通过 PUT /\u0026lt;index\u0026gt;/_settings 在索引级别动态覆盖（标注\u0026quot;可动态修改\u0026quot;的）。\n    参数 默认值 说明     index.number_of_shards 1 新建索引的默认主分片数。注意：索引创建后主分片数不可修改   index.number_of_replicas 1 新建索引的默认副本数（可动态修改）   index.refresh_interval 1s 索引刷新间隔（文档写入后多久可被搜索到）。设为 -1 可禁用自动刷新   index.translog.durability request Translog 刷写策略。request（每次请求同步刷写，最安全）或 async（异步刷写，更快但有丢失风险）   index.translog.flush_threshold_size 512mb Translog 累积到此大小触发 flush 到 Lucene   index.max_result_window 10000 搜索结果窗口上限（from + size 的最大值）。过大会占用大量堆内存    分片分配 #     参数 默认值 说明     cluster.routing.allocation.enable all 分片分配开关。可选：all、primaries（仅主分片）、new_primaries（仅新主分片）、none（禁止）。滚动重启时设为 primaries 或 none   cluster.routing.allocation.node_concurrent_recoveries 2 单节点并发恢复分片数   cluster.routing.allocation.node_concurrent_incoming_recoveries 2 单节点并发接收恢复分片数   cluster.routing.allocation.node_concurrent_outgoing_recoveries 2 单节点并发发送恢复分片数   cluster.max_shards_per_node 1000 单节点最大分片数（所有索引分片总和）    分片分配感知 #     参数 默认值 说明     cluster.routing.allocation.awareness.attributes — 分片分配感知属性。配合 node.attr.* 使用，确保主分片和副本分布在不同区域。例：zone   cluster.routing.allocation.awareness.force.zone.values — 强制感知属性值列表。确保即使部分区域不可用也不会将所有副本分配到同一区域。例：[\u0026quot;zone-a\u0026quot;, \u0026quot;zone-b\u0026quot;]    磁盘水位线 #     参数 默认值 说明     cluster.routing.allocation.disk.threshold_enabled true 是否启用基于磁盘使用量的分片分配   cluster.routing.allocation.disk.watermark.low 85% 低水位线：磁盘使用超过此阈值后，不再向该节点分配新分片（已有分片保留不动）   cluster.routing.allocation.disk.watermark.high 90% 高水位线：超过后不再分配新分片，并开始将已有分片迁移到其他节点   cluster.routing.allocation.disk.watermark.flood_stage 95% 洪水位线：超过后相关索引变为只读   cluster.routing.allocation.disk.reroute_interval 60s 磁盘水位线检测间隔（多久检查一次并进行重新分配）   cluster.routing.allocation.disk.include_relocations true 是否在磁盘水位线计算中包含正在进行中的副本搬运（已弃用）    示例：调整磁盘水位线\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.disk.watermark.low\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;cluster.routing.allocation.disk.watermark.high\u0026#34;: \u0026#34;88%\u0026#34;, \u0026#34;cluster.routing.allocation.disk.watermark.flood_stage\u0026#34;: \u0026#34;93%\u0026#34; } } 分片分配过滤 #  用于节点退役前迁走所有分片：\n   参数 说明     cluster.routing.allocation.exclude._name 按节点名称排除   cluster.routing.allocation.exclude._ip 按 IP 地址排除   cluster.routing.allocation.exclude._host 按主机名排除   cluster.routing.allocation.include._name 仅允许分配到指定节点   cluster.routing.allocation.require._name 要求必须分配到指定节点    示例：下线节点前迁移分片\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.exclude._name\u0026#34;: \u0026#34;node-3\u0026#34; } } 分片恢复限流 #     参数 默认值 说明     indices.recovery.max_bytes_per_sec 40mb 分片恢复时每个节点的最大传输速率。SSD 可适当调高   indices.store.throttle.max_bytes_per_sec 10mb 段合并限流。SSD 环境可设为 200mb 以上    示例：提高恢复速率（SSD 环境）\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;indices.recovery.max_bytes_per_sec\u0026#34;: \u0026#34;200mb\u0026#34; } } 分片延迟分配 #     参数 默认值 说明     index.unassigned.node_left.delayed_timeout 1m 节点离开后，推迟分片重新分配的时间。避免因短暂网络抖动导致不必要的数据搬运。可通过索引设置 API 修改    内存与缓存 #     参数 默认值 说明     indices.fielddata.cache.size 无限制 字段数据缓存上限（百分比或绝对值），超过阈值时触发回收    断路器 #  断路器防止操作导致 OutOfMemoryError：\n   参数 默认值 说明     indices.breaker.total.limit 70% 总断路器上限（所有断路器的聚合限制）   indices.breaker.fielddata.limit 40% 字段数据断路器（加载 fielddata 到内存时触发）   indices.breaker.request.limit 60% 请求断路器（单次请求中聚合等数据结构）   network.breaker.inflight_requests.limit 100% 传输中请求断路器（HTTP/Transport 层正在进行的请求）    示例：调整断路器\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;indices.breaker.total.limit\u0026#34;: \u0026#34;75%\u0026#34;, \u0026#34;indices.breaker.fielddata.limit\u0026#34;: \u0026#34;45%\u0026#34; } } 跨集群搜索 #     参数 默认值 说明     cluster.remote.\u0026lt;alias\u0026gt;.seeds — 远程集群的种子节点地址列表   cluster.remote.\u0026lt;alias\u0026gt;.transport.compress false 是否压缩跨集群通信   cluster.remote.\u0026lt;alias\u0026gt;.skip_unavailable false 远程集群不可用时是否跳过（而非报错）    示例：添加远程集群\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.remote.cluster_b.seeds\u0026#34;: [\u0026#34;192.168.2.10:9300\u0026#34;, \u0026#34;192.168.2.11:9300\u0026#34;], \u0026#34;cluster.remote.cluster_b.transport.compress\u0026#34;: true, \u0026#34;cluster.remote.cluster_b.skip_unavailable\u0026#34;: true } } 日志级别 #  可在不重启节点的情况下动态调整任意 Logger 的级别：\n// 开启 DEBUG 日志 PUT /_cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;logger.org.easysearch.discovery\u0026#34;: \u0026#34;DEBUG\u0026#34; } } // 恢复默认级别 PUT /_cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;logger.org.easysearch.discovery\u0026quot;: null } } 慢日志阈值 #\n 慢日志可以按索引设置阈值，帮助发现性能问题：\nPUT /my-index/_settings { \u0026#34;index.search.slowlog.threshold.query.warn\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.info\u0026#34;: \u0026#34;5s\u0026#34;, \u0026#34;index.search.slowlog.threshold.fetch.warn\u0026#34;: \u0026#34;1s\u0026#34;, \u0026#34;index.indexing.slowlog.threshold.index.warn\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;index.indexing.slowlog.threshold.index.info\u0026#34;: \u0026#34;5s\u0026#34; }  延伸阅读 #    节点配置 — easysearch.yml 静态配置项详细参考  系统调优 — 操作系统内核参数  硬件配置 — CPU / 内存 / 磁盘 / 网络选型  安全配置 — 用户、角色、权限管理  ","subcategory":null,"summary":"","tags":null,"title":"集群配置","url":"/easysearch/main/docs/deployment/config/configuration/"},{"category":null,"content":"阿拉伯语词干过滤器 #  arabic_stemmer 词元过滤器使用 Lucene 的 ArabicStemmer 对阿拉伯语词元进行词干提取，去除常见的前缀和后缀。\n词干规则 #  此词干提取器基于 Shereen Khoja 的轻量级方法，处理以下词缀：\n   类型 示例     定冠词前缀 ال (al-)   介词前缀 و (wa-), ب (bi-), ك (ka-)   代词后缀 ها, هم, هن 等   阴性/双数/复数后缀 ة, ات, ين 等    使用示例 #  PUT my-arabic-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;arabic_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;arabic\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_arabic\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;arabic_normalization\u0026#34;, \u0026#34;arabic_stem\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;arabic_normalization\u0026#34;, \u0026#34;stemmer\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;الكتابات\u0026#34; } 参数 #  通过 stemmer 过滤器使用时：\n   参数 值 说明     type stemmer 过滤器类型   language arabic 指定阿拉伯语词干算法    在语言分析器中的位置 #   阿拉伯语分析器 内置了此过滤器，位于分析链末端。\n","subcategory":null,"summary":"","tags":null,"title":"阿拉伯语词干过滤器（Arabic Stemmer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/arabic-stem/"},{"category":null,"content":"阿拉伯语归一化过滤器 #  arabic_normalization 词元过滤器对阿拉伯语文本进行正字法归一化，将各种书写变体统一为标准形式，提高搜索的召回率。\n归一化规则 #     原始形式 归一化结果 说明     \\u0623 \\u0625 \\u0622 (带 hamza 的 alef) \\u0627 (bare alef) 统一 alef 变体   \\u0629 (taa marbuta) \\u0647 (haa) 统一词尾形式   \\u064e \\u064f \\u0650 \\u064b \\u064c \\u064d (harakat) 删除 移除变音符号   \\u0640 (tatweel/kashida) 删除 移除装饰性延长符    使用示例 #  PUT my-arabic-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;arabic_norm\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;arabic_normalization\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_arabic\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;arabic_norm\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;arabic_normalization\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;\\u0625\\u0628\\u0631\\u0627\\u0647\\u064a\\u0645\u0026#34; } 参数 #  此过滤器不接受任何参数。\n在语言分析器中的位置 #   阿拉伯语分析器 内置了此过滤器，其分析链为：\nstandard 分词器 → lowercase → arabic_normalization → decimal_digit → stop（阿拉伯语） → arabic_stemmer\n","subcategory":null,"summary":"","tags":null,"title":"阿拉伯语归一化过滤器（Arabic Normalization）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/arabic-normalization/"},{"category":null,"content":"跨集群复制（CCR） #  Easysearch 跨集群复制（Cross-Cluster Replication, CCR）支持将数据从源集群（Leader）实时同步到一个或多个目标集群（Follower）。\n 使用跨集群复制 API 管理跨集群复制。\n在跨集群复制中，可以将数据索引到一个领导者索引，然后 Easysearch 将这些数据复制到一个或多个只读的跟随者索引。所有在领导者上进行的后续操作都会在跟随者上复制，例如创建、更新或删除文档。\n先决条件 #   1.11.1 版本之前，leader 和 follower 集群都必须安装 cross-cluster-replication 插件和 index-management 插件，1.11.1 版本开始，已经内置了 cross-cluster-replication 模块。 从1.15.2版本开始，cross-cluster-replication 和 index-management 都已经内置到 modules，不再需要安装。 如果 follower 集群的 easysearch.yml 文件中覆盖了 node.roles，确保它也包括 remote_cluster_client 角色，默认启用。 node.roles: [\u0026lt;other_roles\u0026gt;, remote_cluster_client]  权限 #   确保安全功能在两个集群上都启用或都禁用。如果启用了安全功能，确保非管理员用户被映射到适当的权限，以便他们可以执行复制操作。  部署示例集群 #   在本地起 2 个单节点的 easysearch 测试集群，分别是 follower-application (9201 端口) 和 leader-application (9200 端口) 在 easysearch.yml 添加 discovery.type: single-node 如果启用 security 功能，确保 2 个集群的证书互信，测试环境可以直接合并 2 个节点的 ca 证书： 例如 cat ca.crt ../../easysearch-1.0.0_2/config/ca.crt \u0026gt; trust-chain.pem 分别设置 security.ssl.transport.ca_file: trust-chain.pem   设置跨集群复制 #  设置跨群集连接 #  跨集群复制采用“pull”模型，在 follower 集群上，添加每个种子节点的 IP 地址（端口 9300）， 为连接提供一个描述性名称，您将在请求中使用该名称来启动复制。以下是对应的 curl 命令：\ncurl -XPUT -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/_cluster/settings?pretty\u0026#39; -d \u0026#39; { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster\u0026#34;: { \u0026#34;remote\u0026#34;: { \u0026#34;my-connection-alias\u0026#34;: { \u0026#34;seeds\u0026#34;: [\u0026#34;127.0.0.1:9300\u0026#34;] } } } } }\u0026#39;  开始复制 #  首先，在 leader 集群创建 leader-01 索引, 并向索引写入数据。\ncurl -XPUT -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9200/leader-01?pretty\u0026#39; 然后设置跟随者集群开始复制。在请求体中，提供你想要复制的连接名和领导者索引，以及你想要使用的安全角色：\ncurl -XPUT -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/_replication/follower-01/_start?pretty\u0026#39; -d \u0026#39; { \u0026#34;leader_alias\u0026#34;: \u0026#34;my-connection-alias\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;leader-01\u0026#34;, \u0026#34;use_roles\u0026#34;:{ \u0026#34;leader_cluster_role\u0026#34;: \u0026#34;superuser\u0026#34;, \u0026#34;follower_cluster_role\u0026#34;: \u0026#34;superuser\u0026#34; } }\u0026#39; 因为演示用户是 admin 所以可以直接用 superuser 角色进行复制。\n确认复制状态 #  curl -XGET -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/_replication/follower-01/_status?pretty\u0026#39; 响应\n{ \u0026#34;status\u0026#34; : \u0026#34;SYNCING\u0026#34;, \u0026#34;reason\u0026#34; : \u0026#34;User initiated\u0026#34;, \u0026#34;leader_alias\u0026#34; : \u0026#34;my-connection-alias\u0026#34;, \u0026#34;leader_index\u0026#34; : \u0026#34;leader-01\u0026#34;, \u0026#34;follower_index\u0026#34; : \u0026#34;follower-01\u0026#34;, \u0026#34;syncing_details\u0026#34; : { \u0026#34;leader_checkpoint\u0026#34; : 23, \u0026#34;follower_checkpoint\u0026#34; : 23, \u0026#34;seq_no\u0026#34; : 24 } } 列出 follow 集群所有正在复制的索引 #  包括同步中、暂停、失败等状态。\nGET /_replication/all_status 响应\n下面状态为 FAILED 或 PAUSED 的 索引 是因为 leader 集群启用了 ILM，将过期的索引删除了，CCR 会将被删除的索引在 status 中标记出来。\n[ { \u0026#34;status\u0026#34;: \u0026#34;FAILED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Pause failed with \\\u0026#34;Index log-test-000006 is already paused\\\u0026#34;. Original failure for initiating pause - [[log-test-000006][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000006]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000006\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000006\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;SYNCING\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;User initiated\u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;test2\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;test2\u0026#34;, \u0026#34;syncing_details\u0026#34;: { \u0026#34;leader_checkpoint\u0026#34;: 12, \u0026#34;follower_checkpoint\u0026#34;: 12, \u0026#34;seq_no\u0026#34;: 13 } }, { \u0026#34;status\u0026#34;: \u0026#34;PAUSED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;AutoPaused: [[log-test-000001][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000001]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000001\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000001\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;PAUSED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;AutoPaused: [[log-test-000003][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000003]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000003\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000003\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;SYNCING\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;User initiated\u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;syncing_details\u0026#34;: { \u0026#34;leader_checkpoint\u0026#34;: 20, \u0026#34;follower_checkpoint\u0026#34;: 20, \u0026#34;seq_no\u0026#34;: 21 } }, { \u0026#34;status\u0026#34;: \u0026#34;PAUSED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;AutoPaused: [[log-test-000002][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000002]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000002\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000002\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;FAILED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Pause failed with \\\u0026#34;Index log-test-000005 is already paused\\\u0026#34;. Original failure for initiating pause - [[log-test-000005][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000005]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000005\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000005\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;PAUSED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;AutoPaused: [[log-test-000004][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000004]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000004\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000004\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;SYNCING\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;User initiated\u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000013\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000013\u0026#34;, \u0026#34;syncing_details\u0026#34;: { \u0026#34;leader_checkpoint\u0026#34;: -1, \u0026#34;follower_checkpoint\u0026#34;: -1, \u0026#34;seq_no\u0026#34;: 0 } }, { \u0026#34;status\u0026#34;: \u0026#34;SYNCING\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;User initiated\u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000014\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000014\u0026#34;, \u0026#34;syncing_details\u0026#34;: { \u0026#34;leader_checkpoint\u0026#34;: -1, \u0026#34;follower_checkpoint\u0026#34;: -1, \u0026#34;seq_no\u0026#34;: 0 } } ] 暂停和恢复复制 #  如果您需要修复问题或减轻主集群的负载，您可以暂时暂停索引的复制：\ncurl -XPOST -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/_replication/follower-01/_pause?pretty\u0026#39; -d \u0026#39;{}\u0026#39; 恢复\ncurl -XPOST -ku \u0026#39;admin:xxxxxxxxxxxx\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; \u0026#39;https://localhost:9201/_replication/follower-01/_resume?pretty\u0026#39; -d \u0026#39;{}\u0026#39; 获取 leader 集群状态 #  在 leader 集群执行\nGET /_replication/leader_stats 响应\n{ \u0026#34;num_replicated_indices\u0026#34;: 4, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;translog_size_bytes\u0026#34;: 0, \u0026#34;operations_read_lucene\u0026#34;: 0, \u0026#34;operations_read_translog\u0026#34;: 0, \u0026#34;total_read_time_lucene_millis\u0026#34;: 0, \u0026#34;total_read_time_translog_millis\u0026#34;: 0, \u0026#34;bytes_read\u0026#34;: 0, \u0026#34;index_stats\u0026#34;: { \u0026#34;test1\u0026#34;: { \u0026#34;operations_read\u0026#34;: 0, \u0026#34;translog_size_bytes\u0026#34;: 0, \u0026#34;operations_read_lucene\u0026#34;: 0, \u0026#34;operations_read_translog\u0026#34;: 0, \u0026#34;total_read_time_lucene_millis\u0026#34;: 0, \u0026#34;total_read_time_translog_millis\u0026#34;: 0, \u0026#34;bytes_read\u0026#34;: 0 }, \u0026#34;test2\u0026#34;: { \u0026#34;operations_read\u0026#34;: 0, \u0026#34;translog_size_bytes\u0026#34;: 0, \u0026#34;operations_read_lucene\u0026#34;: 0, \u0026#34;operations_read_translog\u0026#34;: 0, \u0026#34;total_read_time_lucene_millis\u0026#34;: 0, \u0026#34;total_read_time_translog_millis\u0026#34;: 0, \u0026#34;bytes_read\u0026#34;: 0 }, \u0026#34;log-test-000013\u0026#34;: { \u0026#34;operations_read\u0026#34;: 0, \u0026#34;translog_size_bytes\u0026#34;: 0, \u0026#34;operations_read_lucene\u0026#34;: 0, \u0026#34;operations_read_translog\u0026#34;: 0, \u0026#34;total_read_time_lucene_millis\u0026#34;: 0, \u0026#34;total_read_time_translog_millis\u0026#34;: 0, \u0026#34;bytes_read\u0026#34;: 0 }, \u0026#34;log-test-000014\u0026#34;: { \u0026#34;operations_read\u0026#34;: 0, \u0026#34;translog_size_bytes\u0026#34;: 0, \u0026#34;operations_read_lucene\u0026#34;: 0, \u0026#34;operations_read_translog\u0026#34;: 0, \u0026#34;total_read_time_lucene_millis\u0026#34;: 0, \u0026#34;total_read_time_translog_millis\u0026#34;: 0, \u0026#34;bytes_read\u0026#34;: 0 } } } 获取 follower 集群状态为同步中的索引的信息 #  curl -XGET -ku \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9200/_replication/follower_stats\u0026#39; 响应\n{ \u0026#34;num_syncing_indices\u0026#34;: 4, \u0026#34;num_bootstrapping_indices\u0026#34;: 0, \u0026#34;num_paused_indices\u0026#34;: 5, \u0026#34;num_failed_indices\u0026#34;: 2, \u0026#34;num_shard_tasks\u0026#34;: 4, \u0026#34;num_index_tasks\u0026#34;: 4, \u0026#34;operations_written\u0026#34;: 0, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;failed_read_requests\u0026#34;: 0, \u0026#34;throttled_read_requests\u0026#34;: 0, \u0026#34;failed_write_requests\u0026#34;: 0, \u0026#34;throttled_write_requests\u0026#34;: 0, \u0026#34;follower_checkpoint\u0026#34;: 30, \u0026#34;leader_checkpoint\u0026#34;: 0, \u0026#34;total_write_time_millis\u0026#34;: 0, \u0026#34;index_stats\u0026#34;: { \u0026#34;test1\u0026#34;: { \u0026#34;operations_written\u0026#34;: 0, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;failed_read_requests\u0026#34;: 0, \u0026#34;throttled_read_requests\u0026#34;: 0, \u0026#34;failed_write_requests\u0026#34;: 0, \u0026#34;throttled_write_requests\u0026#34;: 0, \u0026#34;follower_checkpoint\u0026#34;: 20, \u0026#34;leader_checkpoint\u0026#34;: 0, \u0026#34;total_write_time_millis\u0026#34;: 0 }, \u0026#34;test2\u0026#34;: { \u0026#34;operations_written\u0026#34;: 0, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;failed_read_requests\u0026#34;: 0, \u0026#34;throttled_read_requests\u0026#34;: 0, \u0026#34;failed_write_requests\u0026#34;: 0, \u0026#34;throttled_write_requests\u0026#34;: 0, \u0026#34;follower_checkpoint\u0026#34;: 12, \u0026#34;leader_checkpoint\u0026#34;: 0, \u0026#34;total_write_time_millis\u0026#34;: 0 }, \u0026#34;log-test-000014\u0026#34;: { \u0026#34;operations_written\u0026#34;: 0, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;failed_read_requests\u0026#34;: 0, \u0026#34;throttled_read_requests\u0026#34;: 0, \u0026#34;failed_write_requests\u0026#34;: 0, \u0026#34;throttled_write_requests\u0026#34;: 0, \u0026#34;follower_checkpoint\u0026#34;: -1, \u0026#34;leader_checkpoint\u0026#34;: 0, \u0026#34;total_write_time_millis\u0026#34;: 0 }, \u0026#34;log-test-000015\u0026#34;: { \u0026#34;operations_written\u0026#34;: 0, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;failed_read_requests\u0026#34;: 0, \u0026#34;throttled_read_requests\u0026#34;: 0, \u0026#34;failed_write_requests\u0026#34;: 0, \u0026#34;throttled_write_requests\u0026#34;: 0, \u0026#34;follower_checkpoint\u0026#34;: -1, \u0026#34;leader_checkpoint\u0026#34;: 0, \u0026#34;total_write_time_millis\u0026#34;: 0 } } } 停止复制 #  在 follower 集群执行\ncurl -XPOST -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/_replication/follower-01/_stop?pretty\u0026#39; -d \u0026#39;{}\u0026#39; 跨集群复制的自动跟随 #  自动开始对匹配指定模式的索引进行复制。如果领导者集群上的新索引匹配该模式，Easysearch 会自动创建一个跟随者索引。 自动跟随让你可以根据匹配的模式自动复制在领导者集群上创建的索引。当你在领导者集群上创建一个名称与指定模式匹配的索引（例如，index-01*），在跟随者集群上会自动创建一个对应的跟随者索引。\n你可以为单个集群配置多个复制规则。目前的模式只支持通配符匹配。\n在启用自动跟随之前，确认在两个集群之间设置了跨集群连接。\n如果启用了安全插件，确保非管理员用户被映射到适当的权限，以便他们可以执行复制操作。关于索引和集群级别的权限要求，请参阅跨集群复制权限。\n复制规则是你针对单个跟随者集群创建的模式集合。当你创建一个复制规则时，它首先会自动复制任何匹配模式的现有索引。然后，它将继续复制你创建的任何新索引，只要这些新索引匹配该模式。\n示例 在 follower 集群执行：\ncurl -XPOST -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/_replication/_autofollow?pretty\u0026#39; -d \u0026#39; { \u0026#34;leader_alias\u0026#34; : \u0026#34;my-connection-alias\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;my-replication-rule\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;nginx*\u0026#34;, \u0026#34;use_roles\u0026#34;:{ \u0026#34;leader_cluster_role\u0026#34;: \u0026#34;superuser\u0026#34;, \u0026#34;follower_cluster_role\u0026#34;: \u0026#34;superuser\u0026#34; } }\u0026#39; 然后 创建 nginx 开头的索引并写入数据\n查看 follower 集群的索引状态 #  curl -kuadmin:admin \u0026#39;https://localhost:9201/_cat/indices?v\u0026#39; 如果发现文档数没变化可以执行 refresh 命令后再查看\n查看已设置的自动跟随规则 #  curl -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; -k \u0026#39;https://localhost:9200/_replication/autofollow_stats\u0026#39; 删除自动跟随规则 #  可以通过在从集群上调用 API 来删除自动跟随。调用 API 仅停止任何新的自动跟随活动，并不会停止已由自动跟随启动的复制，要停止现有的复制活动并打开索引进行写入，请使用停止复制API操作。\n示例\ncurl -XDELETE -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/_replication/_autofollow?pretty\u0026#39; -d \u0026#39; { \u0026#34;leader_alias\u0026#34; : \u0026#34;my-connection-alias\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;my-replication-rule\u0026#34; }\u0026#39;  如何使用自定义用户 #  如果不使用 admin 用户，必须将你自定义的用户绑定到 replication_leader，它在领导者集群上提供复制权限， replication_follower，它在追随者集群上提供复制权限。\n示例 分别在 leader 和 follower 集群执行创建 replication_user 用户的命令：\ncurl -XPUT -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://xxxx:9xxx/_security/user/replication_user\u0026#39; -d \u0026#39; { \u0026#34;password\u0026#34;: \u0026#34;123456\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;replication_leader\u0026#34;, \u0026#34;replication_follower\u0026#34;] }\u0026#39; 用 replication_user 用户设置跟随者集群开始复制 #  curl -XPUT -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;replication_user:123456\u0026#39; \u0026#39;https://localhost:9288/_replication/follower-01/_start?pretty\u0026#39; -d \u0026#39; { \u0026#34;leader_alias\u0026#34;: \u0026#34;my-connection-alias\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;leader-01\u0026#34;, \u0026#34;use_roles\u0026#34;:{ \u0026#34;leader_cluster_role\u0026#34;: \u0026#34;replication_leader\u0026#34;, \u0026#34;follower_cluster_role\u0026#34;: \u0026#34;replication_follower\u0026#34; } }\u0026#39; 用 replication_user 用户设置跟随者集群自动复制 #  curl -XPOST -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;replication_user:123456\u0026#39; \u0026#39;https://localhost:9288/_replication/_autofollow?pretty\u0026#39; -d \u0026#39; { \u0026#34;leader_alias\u0026#34; : \u0026#34;my-connection-alias\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;my-replication-rule\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;leader*\u0026#34;, \u0026#34;use_roles\u0026#34;:{ \u0026#34;leader_cluster_role\u0026#34;: \u0026#34;replication_leader\u0026#34;, \u0026#34;follower_cluster_role\u0026#34;: \u0026#34;replication_follower\u0026#34; } }\u0026#39; 配置项 #  跨集群复制有多个配置项，这些配置项是可以动态修改的，可以将设置标记为 persistent 或 transient。\n例如，要更新跟随集群轮询主集群以获取更新的频率：\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;replication.follower.metadata_sync_interval\u0026#34;: \u0026#34;30s\u0026#34; } } 这些配置项管理远程恢复所消耗的资源。我们不建议更改这些设置；默认设置应适用于大多数使用场景。\n   设置 默认值 描述     replication.follower.index.recovery.chunk_size 10MB follower 集群为文件传输请求的块大小。指定块大小的值和单位,例如10MB、5KB。请参阅支持的单位。   replication.follower.index.recovery.max_concurrent_file_chunks 4 每个恢复过程中可以并行发送的文件块请求数。   replication.follower.index.ops_batch_size 50000 在复制的同步阶段,一次可以获取的操作数量。   replication.follower.concurrent_readers_per_shard 2 在复制的同步阶段,每个分片上追随者集群的并发请求数。   replication.autofollow.fetch_poll_interval 30s 自动跟随任务轮询leader集群以获取新的匹配索引的频率。   replication.follower.metadata_sync_interval 60s follower 集群轮询leader集群以获取更新的索引元数据的频率。   replication.translog.retention_lease.pruning.enabled true 如果启用,基于保留租约来修剪translog。   replication.translog.retention_size 512MB 控制leader索引上translog的大小。     \n","subcategory":null,"summary":"","tags":null,"title":"跨集群复制","url":"/easysearch/main/docs/operations/cluster-admin/ccr/"},{"category":null,"content":"荷兰语词干过滤器 #  dutch_stemmer 词元过滤器使用 Snowball 算法对荷兰语文本进行词干提取。\n功能说明 #  荷兰语词干提取使用 Snowball 算法，结合词干覆盖字典处理不规则变形：\n 移除常见名词/动词后缀 荷兰语分析器额外使用 stemmer_override 字典处理不规则形式 适合荷兰语和佛兰德语文本  使用示例 #  PUT my-dutch-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;dutch_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;dutch\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_dutch\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;dutch_stem\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;dutch\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;programma\u0026#39;s programmering\u0026#34; } 参数 #     参数 值 说明     type stemmer 过滤器类型   language dutch 指定荷兰语 Snowball 词干算法    可选的 dutch_kp 变体使用 Krovetz-Porter 混合算法。\n在语言分析器中的位置 #   荷兰语分析器 内置了 Snowball 词干 + stemmer_override 字典。\n","subcategory":null,"summary":"","tags":null,"title":"荷兰语词干过滤器（Dutch Stemmer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/dutch-stem/"},{"category":null,"content":"聚合与数据分析 #  聚合（Aggregations）是 Easysearch 用于做统计与分析的核心能力，可以在一次请求中同时完成\u0026quot;搜索 + 统计\u0026quot;。本页介绍聚合的概念、类型与用法。\n聚合可以解决什么问题？ #  典型场景：\n 统计：总数、平均值、最大/最小值、百分位数 分布：按字段分桶（如按状态、地区、时间段统计数量） 下钻分析：先按大维度分组，再在组内做更细粒度分析  和传统数据库的类比：\n 可以粗略类比为 GROUP BY + 聚合函数，但聚合可以与全文搜索、过滤等能力组合，且天然适应分布式环境。  聚合请求的基本结构 #  一个最小的\u0026quot;搜索 + 聚合\u0026quot;请求大致长这样：\n{ \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;by_host\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;host\u0026#34; } } } } 结构说明：\n query：决定哪些文档参与统计（可为空，表示全量） aggs：定义一个或多个聚合，每个聚合都有一个名字（如 by_host）  聚合的结果会出现在响应体的 aggregations 区域，与 hits（命中的文档）并列。\n桶（Bucket）与指标（Metric） #  大多数聚合可以拆成两类：\n 桶聚合（Bucket Aggregations）：把文档分到不同\u0026quot;桶\u0026quot;里\n例如：按字段值分桶（terms）、按数值区间分桶（range）、按时间间隔分桶（date_histogram）等。 指标聚合（Metric Aggregations）：对某个字段在桶内做统计\n例如：avg、sum、min、max、percentiles 等。  通常会把两者组合使用：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;by_status\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;status\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_latency\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;latency\u0026#34; } } } } } } 含义：\n 先按 status 分桶（每个状态一个桶） 在每个桶内，再计算 latency 的平均值  常见模式：只要聚合，不要文档 #  有时候你只需要统计结果，不关心具体命中的文档，可以把 size 设为 0：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-1d/d\u0026#34; } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;by_level\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;level\u0026#34; } } } } 这样可以减少响应体体积和网络传输开销。\n与查询结合的典型用法 #  聚合总是基于\u0026quot;参与统计的文档集合\u0026quot;来计算，因此 query 非常关键：\n 通过 query / filter 限定人群/时间范围/业务条件 通过 aggs 在这一子集上做统计和分布分析  例如：统计最近 7 天内，每个应用的错误日志数量：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;log_level\u0026#34;: \u0026#34;ERROR\u0026#34; } }, { \u0026#34;range\u0026#34;: { \u0026#34;timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-7d/d\u0026#34; } } } ] } }, \u0026#34;aggs\u0026#34;: { \u0026#34;by_app\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;app_name\u0026#34; } } } }  桶聚合（Bucket Aggregations） #  桶聚合（Bucket Aggregations）负责\u0026quot;分组\u0026quot;：把满足查询条件的文档划分到不同的桶中，后续可以在桶内再做指标聚合或进一步下钻。\n什么时候用桶聚合？ #  典型问题：\n 按状态/地区/应用等字段查看分布情况 按时间窗口（天/小时）查看趋势 按数值区间（价格段、年龄段等）汇总  桶本身不做数值计算，但提供\u0026quot;分组维度\u0026quot;；通常会和指标聚合（Metrics）组合使用。\nterms：按字段值分桶 #  terms 是最常用的桶聚合，用于按字段的不同取值聚合，例如按状态统计数量：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;by_status\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;status\u0026#34;, \u0026#34;size\u0026#34;: 10 } } } } 要点：\n 适合 keyword/数值等可精确匹配的字段 size 控制返回的桶个数（按文档数从高到低排序）  常见注意点：\n 高频值很多时（如用户 ID），terms 可能产生非常多桶 → 建议只对维度有限的字段使用（状态、地区、应用名等） 如需对极高基数字段做分析，可考虑预聚合、采样或专门的数据建模  histogram：数值直方图 #  histogram 用于把数值字段按固定间隔分桶，例如按 10 元价格区间统计订单数：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;price_histogram\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34;, \u0026#34;interval\u0026#34;: 10 } } } } 要点：\n 适合数值字段 interval 控制桶宽度，选择合适的间隔可以平衡\u0026quot;细腻度\u0026quot;和性能  date_histogram：时间序列分桶 #  date_histogram 是时间序列分析的核心，用于按固定时间间隔（天/小时/分钟等）划分桶：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;per_day\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;day\u0026#34; } } } } 常用参数：\n calendar_interval：按自然时间单位（year/month/week/day/hour 等） fixed_interval：按固定毫秒数间隔（适合更精细控制）  典型用法：\n 结合错误码或业务维度做趋势图 作为折线图/柱状图的 X 轴  range：自定义范围分桶 #  range 允许你手动定义数值或日期区间：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;price_ranges\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;to\u0026#34;: 100 }, { \u0026#34;from\u0026#34;: 100, \u0026#34;to\u0026#34;: 500 }, { \u0026#34;from\u0026#34;: 500 } ] } } } } 适合：\n 需要特定业务切分方式（如\u0026quot;低价/中价/高价\u0026quot;） 不希望桶边界由系统自动划分  桶内再嵌套桶与指标 #  桶聚合可以嵌套使用，实现多维下钻，例如：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;by_app\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;app_name\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;per_day\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;day\u0026#34; } } } } } } 结构含义：\n 第一层按应用分桶（by_app） 第二层在每个应用桶内，再按日期分桶（per_day）  在任何桶内，你都可以继续添加指标聚合（如平均值、总和等），实现\u0026quot;多维分组 + 多个指标\u0026quot;的报表。\n选型与实践建议 #   优先用 terms 处理离散维度（状态、地区、应用名等） 对数值/时间做趋势图与直方图时，使用 histogram/date_histogram 需要特定业务区间时使用 range，并确保区间覆盖与顺序正确 控制桶的数量与嵌套深度，避免一次性生成过多桶带来的内存与网络开销   指标聚合（Metric Aggregations） #  指标聚合用于在桶内（或整个结果集上）计算各种统计值，比如平均值、总和、最值、计数、百分位等。\ncount：文档数量 #  最简单的\u0026quot;指标\u0026quot;其实是每个桶里的文档数量，这在绝大多数桶聚合结果里会自动给出（doc_count 字段）。\n例如，在 terms 桶聚合结果中，每个桶都会包含：\n key：桶的键（比如状态值） doc_count：落入该桶的文档数量  通常不需要额外配置。\navg / sum / min / max：基础统计 #  这些是最常用的数值统计指标，示例：按应用统计最近一天平均延迟和总请求数：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-1d/d\u0026#34; } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;by_app\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;app_name\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_latency\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;latency\u0026#34; } }, \u0026#34;total_requests\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;requests\u0026#34; } }, \u0026#34;min_latency\u0026#34;: { \u0026#34;min\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;latency\u0026#34; } }, \u0026#34;max_latency\u0026#34;: { \u0026#34;max\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;latency\u0026#34; } } } } } } 适用场景：\n 指标监控、报表、容量评估 在业务维度（应用、地区、用户群等）下做统计  percentiles / percentile_ranks：百分位 #  百分位在性能分析中非常重要（P50/P95/P99 等），可用 percentiles 指标聚合实现：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;latency_percentiles\u0026#34;: { \u0026#34;percentiles\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;latency\u0026#34;, \u0026#34;percents\u0026#34;: [50, 95, 99] } } } } 含义：\n P50：50% 的请求延迟小于等于该值 P95：95% 的请求延迟小于等于该值  percentile_ranks 则反过来：给定一些值，计算它们分别处于全体样本的哪一百分位：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;latency_ranks\u0026#34;: { \u0026#34;percentile_ranks\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;latency\u0026#34;, \u0026#34;values\u0026#34;: [100, 300, 500] } } } } 组合多个指标 #  你可以在同一个桶下组合多个指标，例如对每个状态统计：\n 文档数（doc_count） 平均延迟（avg） 错误率（通过额外的 filter/脚本或预先建模实现）  结构类似：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;by_status\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;status\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_latency\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;latency\u0026#34; } }, \u0026#34;latency_percentiles\u0026#34;: { \u0026#34;percentiles\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;latency\u0026#34;, \u0026#34;percents\u0026#34;: [50, 95, 99] } } } } } } 实践建议 #   指标字段类型应为数值或日期类型，避免在 text/keyword 上做无意义的指标计算 对高基数/高频更新的指标字段，要注意聚合的成本（可通过采样、预聚合或索引设计优化） 在监控和报表场景下，先从少量关键指标开始，逐步增加复杂度，避免一开始就请求过多指标导致性能问题   管道聚合（Pipeline Aggregations） #  管道聚合（Pipeline Aggregations）是一类特殊的聚合，它们不直接处理文档，而是在其他聚合的结果之上进行计算。管道聚合可以用于计算衍生指标、移动平均值、百分比变化等。\n管道聚合的类型 #  管道聚合主要分为两类：\n 父级管道聚合：在父聚合的结果之上进行计算，例如计算每个桶的衍生值 兄弟管道聚合：在兄弟聚合的结果之上进行计算，例如计算两个聚合之间的差值  常见的管道聚合包括：\n derivative：计算父聚合的衍生值（一阶导数） moving_avg：计算移动平均值 bucket_script：使用脚本对多个聚合结果进行计算 bucket_selector：根据条件选择桶 cumulative_sum：计算累积和 percentile_bucket：计算百分位桶  衍生值（Derivative） #  derivative 聚合计算父聚合的衍生值。这对于时间序列数据特别有用，可以计算变化率。\n例如，计算每月销售额的月度变化：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sales\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } }, \u0026#34;sales_deriv\u0026#34;: { \u0026#34;derivative\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;sales\u0026#34; } } } } } } derivative 会计算相邻桶之间的差值。第一个桶没有前一个桶，所以不会有衍生值。\n移动平均值（Moving Average） #  moving_avg 聚合计算移动平均值，这对于平滑时间序列数据非常有用。\n例如，计算销售额的 7 天移动平均值：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_day\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;day\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sales\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } }, \u0026#34;sales_moving_avg\u0026#34;: { \u0026#34;moving_avg\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;sales\u0026#34;, \u0026#34;window\u0026#34;: 7 } } } } } } window 参数指定移动窗口的大小。移动平均值会平滑数据，减少短期波动的影响。\n桶脚本（Bucket Script） #  bucket_script 聚合允许你使用脚本对多个聚合结果进行计算。\n例如，计算每个月的销售额增长率：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sales\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } }, \u0026#34;sales_growth\u0026#34;: { \u0026#34;bucket_script\u0026#34;: { \u0026#34;buckets_path\u0026#34;: { \u0026#34;prevMonthSales\u0026#34;: \u0026#34;sales[_key - 1m]\u0026#34;, \u0026#34;currentSales\u0026#34;: \u0026#34;sales\u0026#34; }, \u0026#34;script\u0026#34;: \u0026#34;params.currentSales - params.prevMonthSales\u0026#34; } } } } } } 桶选择器（Bucket Selector） #  bucket_selector 聚合可以根据条件过滤或移除某些桶。\n例如，只保留销售额大于 1000 的月份：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sales\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } }, \u0026#34;sales_bucket_filter\u0026#34;: { \u0026#34;bucket_selector\u0026#34;: { \u0026#34;buckets_path\u0026#34;: { \u0026#34;sales\u0026#34;: \u0026#34;sales\u0026#34; }, \u0026#34;script\u0026#34;: \u0026#34;params.sales \u0026gt; 1000\u0026#34; } } } } } } 累积求和（Cumulative Sum） #  cumulative_sum 聚合计算累积和，适合展示累积增长趋势。\n例如，计算每月销售额的累计总和：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sales\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } }, \u0026#34;cumulative_sales\u0026#34;: { \u0026#34;cumulative_sum\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;sales\u0026#34; } } } } } } 同级管道聚合（Sibling Aggregations） #  同级管道聚合（如 avg_bucket、sum_bucket 等）可以在多个桶之间做计算。\n例如，计算所有销售月份的平均销售额：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sales\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } } } }, \u0026#34;total_avg_sales\u0026#34;: { \u0026#34;avg_bucket\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;sales_per_month\u0026gt;sales\u0026#34; } } } } 应用场景 #  管道聚合特别适用于：\n 时间序列分析：计算变化率、移动平均值、趋势等 业务指标计算：增长率、占比、累计值等 数据过滤：根据计算结果过滤桶 复杂计算：需要多个聚合结果参与的计算  注意事项 #   性能影响：管道聚合会增加查询的计算开销，特别是在大数据集上 内存使用：某些管道聚合（如移动平均值）需要缓存历史数据 空值处理：某些桶可能没有值，需要处理空值情况 脚本性能：使用脚本的管道聚合（如 bucket_script）性能可能不如内置聚合   小结 #   聚合由桶（Buckets）和指标（Metrics）组成 桶用于分组，指标用于统计计算 聚合可以嵌套使用，实现多维下钻 聚合总是基于查询结果进行计算 设置 size: 0 可以只返回聚合结果，不返回文档 管道聚合可以在已有聚合结果之上做二次计算   参考手册（API 与参数） #  详细的聚合类型与参数说明，请查看功能参考中的 API 文档：\n  桶聚合 API 参考  指标聚合 API 参考  管道聚合 API 参考  最佳实践 #    查询调优与慢查询排查：聚合性能优化、分片聚合策略  数据建模：为聚合而设计的字段结构、反范式权衡  ","subcategory":null,"summary":"","tags":null,"title":"聚合与数据分析","url":"/easysearch/main/docs/fundamentals/aggregations-data-analysis/"},{"category":null,"content":"罗马尼亚语词干过滤器 #  romanian_stemmer 词元过滤器使用 Snowball 算法对罗马尼亚语文本进行词干提取。\n功能说明 #  此过滤器移除罗马尼亚语名词的格变化和定冠词后缀，以及动词变位后缀。\n使用示例 #  PUT my-romanian-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;ro_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;romanian\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_romanian\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;ro_stem\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;romanian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;programarea programatori\u0026#34; } 参数 #     参数 值 说明     type stemmer 过滤器类型   language romanian 指定罗马尼亚语 Snowball 词干算法    在语言分析器中的位置 #   罗马尼亚语分析器 内置了此过滤器。\n","subcategory":null,"summary":"","tags":null,"title":"罗马尼亚语词干过滤器（Romanian Stemmer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/romanian-stem/"},{"category":null,"content":"网关与恢复配置 #  本页介绍 easysearch.yml 中与集群完全重启后分片恢复行为相关的配置项。这些都是静态设置，修改后需要重启节点生效。\n 分片恢复的限流参数（如 indices.recovery.max_bytes_per_sec）属于动态配置，通过 集群配置 API 修改。\n  为什么需要网关恢复设置？ #  当整个集群重启时，各节点可能以不同的速度启动。如果集群在只有部分节点加入时就立即开始分片恢复，会导致：\n 不必要的数据搬运：节点 A 先启动，集群把本该在节点 B 上的分片副本重新分配给 A；等 B 启动后又要搬回去。 大量网络 I/O：分片数据在节点间来回传输。 恢复时间变长：所有数据都要重新复制一遍。  网关设置让集群等到足够数量的节点加入后再开始恢复，避免上述问题。\n gateway.recover_after_nodes #  gateway.recover_after_nodes: 2    项目 说明     参数 gateway.recover_after_nodes   默认值 -1 (禁用)   属性 静态   说明 集群中至少有多少个节点加入后才开始恢复分片。已弃用：建议改用 recover_after_data_nodes 和 recover_after_master_nodes 以获得更精确的控制    推荐值 #   三节点集群：设为 2 一般规则：设为集群总节点数的大多数（$\\lceil N/2 \\rceil + 1$ 或更大）  gateway.expected_nodes #  gateway.expected_nodes: 3    项目 说明     参数 gateway.expected_nodes   默认值 -1 (禁用)   属性 静态   说明 集群预期的总节点数。当加入的节点数达到此值时，立即开始恢复（不等待 recover_after_time）。已弃用：建议改用 expected_data_nodes    gateway.recover_after_time #  # 生产环境建议显式设置 gateway.recover_after_time: 5m    项目 说明     参数 gateway.recover_after_time   默认值 0ms (无延迟)   属性 静态   说明 达到 recover_after_nodes 后，再等待多长时间才开始恢复。给剩余节点留出加入集群的时间窗口。如果在此期间达到 expected_nodes，则提前开始恢复。生产环境建议设为 5m     数据节点专用参数 #  如果集群中有角色分离（专用 master + 专用 data），可以单独设置数据节点的恢复条件：\n   参数 默认值 说明     gateway.expected_data_nodes -1 预期的数据节点数。达到后立即开始恢复   gateway.recover_after_data_nodes -1 至少有多少数据节点加入后才开始恢复   gateway.expected_master_nodes -1 预期的 master 候选节点数（已弃用）   gateway.recover_after_master_nodes 0 至少有多少 master 候选节点加入后才开始恢复（已弃用），默认跟随 discovery.zen.minimum_master_nodes     恢复流程示意 #  集群全量重启 │ ▼ 等待节点加入... │ ├── 节点数 \u0026lt; recover_after_nodes → 继续等待 │ ├── 节点数 ≥ recover_after_nodes │ │ │ ├── 节点数 = expected_nodes → 立即恢复 │ │ │ └── 等待 recover_after_time │ │ │ └── 超时 → 开始恢复 │ ▼ 开始分片恢复（先恢复 primary，再恢复 replica）  恢复条件详解 #  恢复的触发逻辑 #  集群恢复需要满足以下任一条件（按优先级）：\n  检查基本限制条件（必须先满足）：\n 如果 recover_after_nodes 设置：数据节点数 + master节点数 \u0026lt; recover_after_nodes → 等待 如果 recover_after_data_nodes 设置：数据节点数 \u0026lt; recover_after_data_nodes → 等待 如果 recover_after_master_nodes 设置：master节点数 \u0026lt; recover_after_master_nodes → 等待    检查期望节点条件（满足任一则可恢复）：\n 如果设置了 expected_* 参数且条件满足 → 立即恢复（跳过超时） 如果设置了 recover_after_time → 等待指定时间后恢复    默认行为（都未设置时）：\n 立即开始恢复    源码验证 #  根据 GatewayService.java 源码中的 clusterChanged 方法：\n// 恢复被阻止的条件（任一满足则不恢复） if (recoverAfterNodes != -1 \u0026amp;\u0026amp; (nodes.getMasterAndDataNodes().size()) \u0026lt; recoverAfterNodes) // 不恢复：总节点数不足 else if (recoverAfterDataNodes != -1 \u0026amp;\u0026amp; nodes.getDataNodes().size() \u0026lt; recoverAfterDataNodes) // 不恢复：数据节点数不足 else if (recoverAfterMasterNodes != -1 \u0026amp;\u0026amp; nodes.getMasterNodes().size() \u0026lt; recoverAfterMasterNodes) // 不恢复：主节点数不足 else // 检查期望节点：满足则立即恢复，否则等待 recover_after_time  ⚠️ 参数弃用说明 #     参数 状态 替代方案     recover_after_nodes 已弃用 使用 recover_after_data_nodes + recover_after_master_nodes   expected_nodes 已弃用 使用 expected_data_nodes   expected_master_nodes 已弃用 使用 expected_data_nodes 代替   recover_after_master_nodes 已弃用 使用 discovery.zen.minimum_master_nodes 代替     为什么弃用？ 新参数提供更精确的控制，区分数据节点和主节点，避免配置歧义。\n  配置示例 #  三节点集群（推荐配置） #  # 推荐：使用数据节点控制而非总节点数 gateway.recover_after_data_nodes: 2 gateway.expected_data_nodes: 3 gateway.recover_after_time: 5m 含义：2 个数据节点加入后开始计时；5 分钟内第 3 个数据节点也加入则立即恢复；5 分钟后仍只有 2 个数据节点也开始恢复。\n三节点集群（传统配置 - 已弃用） #  gateway.recover_after_nodes: 2 gateway.expected_nodes: 3 gateway.recover_after_time: 5m 角色分离集群（3 master + 5 data） #  # 按数据节点数控制 gateway.recover_after_data_nodes: 3 gateway.expected_data_nodes: 5 gateway.recover_after_time: 10m 大规模集群（20+ 节点） #  # 推荐：分别控制数据节点和主节点 gateway.recover_after_data_nodes: 15 gateway.expected_data_nodes: 20 gateway.recover_after_master_nodes: 2 gateway.recover_after_time: 15m  延伸阅读 #    集群配置 — 分片恢复限流（indices.recovery.max_bytes_per_sec）等动态设置  集群发现 — 节点如何发现彼此  生产环境部署 — 滚动重启流程  ","subcategory":null,"summary":"","tags":null,"title":"网关与恢复","url":"/easysearch/main/docs/deployment/config/node-settings/gateway/"},{"category":null,"content":"索拉尼语归一化过滤器 #  sorani_normalization 词元过滤器对索拉尼库尔德语（سۆرانی）文本进行字符归一化。索拉尼语使用修改后的阿拉伯字母书写。\n归一化规则 #     处理 说明     Yaa 归一化 统一 ي/ی 变体   Kaf 归一化 统一 ك/ک 变体   Haa 归一化 统一 haa 变体   变音符号移除 移除可选的变音标记    使用示例 #  PUT my-sorani-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;sorani_norm\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;sorani_normalization\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_sorani\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;sorani_norm\u0026#34;, \u0026#34;sorani_stemmer\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;sorani_normalization\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;کوردی\u0026#34; } 参数 #  此过滤器不接受任何参数。\n在语言分析器中的位置 #   索拉尼语分析器 内置了此过滤器，其分析链为：\nstandard 分词器 → lowercase → decimal_digit → sorani_normalization → stop（索拉尼语） → sorani_stemmer\n","subcategory":null,"summary":"","tags":null,"title":"索拉尼语归一化过滤器（Sorani Normalization）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/sorani-normalization/"},{"category":null,"content":"瑞典语词干过滤器 #  swedish_stemmer 词元过滤器使用 Snowball 算法对瑞典语文本进行词干提取。\n功能说明 #  Easysearch 提供两种瑞典语词干算法：\n   算法 language 值 说明     Snowball swedish 标准 Snowball 算法   轻量级 light_swedish 更保守的词干提取    使用示例 #  PUT my-swedish-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;sv_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;swedish\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_swedish\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;sv_stem\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;swedish\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;programmering programmerare\u0026#34; } 参数 #     参数 值 说明     type stemmer 过滤器类型   language swedish / light_swedish 选择词干算法    在语言分析器中的位置 #   瑞典语分析器 内置了 Snowball 词干提取器。\n","subcategory":null,"summary":"","tags":null,"title":"瑞典语词干过滤器（Swedish Stemmer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/swedish-stem/"},{"category":null,"content":"波斯语归一化过滤器 #  persian_normalization 词元过滤器对波斯语（فارسی）文本进行字符归一化，统一阿拉伯语和波斯语书写变体。\n归一化规则 #     处理 说明     阿拉伯语 Yaa → 波斯语 Yaa 统一 ي → ی   阿拉伯语 Kaf → 波斯语 Kaf 统一 ك → ک   变音符号移除 移除阿拉伯语 harakat 标记   搭配 arabic_normalization 建议与 arabic_normalization 一起使用    使用示例 #  PUT my-persian-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;persian_norm\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;persian_normalization\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_persian\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;arabic_normalization\u0026#34;, \u0026#34;persian_norm\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;arabic_normalization\u0026#34;, \u0026#34;persian_normalization\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;فارسی\u0026#34; } 参数 #  此过滤器不接受任何参数。\n在语言分析器中的位置 #   波斯语分析器 内置了此过滤器，其分析链为：\nmapping 字符过滤器（零宽非连接符）→ standard 分词器 → lowercase → decimal_digit → arabic_normalization → persian_normalization → stop（波斯语）\n","subcategory":null,"summary":"","tags":null,"title":"波斯语归一化过滤器（Persian Normalization）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/persian-normalization/"},{"category":null,"content":"法语词干过滤器 #  french_light_stem 词元过滤器使用 Lucene 的轻量级法语词干算法，移除法语常见的形态后缀。\n功能说明 #  法语分析器默认使用轻量级词干提取（light_french），而非 Snowball 算法。轻量级算法更保守：\n   算法 说明 适用场景     light_french 轻量级，只移除明显后缀 默认推荐   french Snowball 完整词干 更激进的归约   minimal_french 最小化词干 最保守    使用示例 #  PUT my-french-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;fr_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;light_french\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_french\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;elision\u0026#34;, \u0026#34;fr_stem\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;french\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;programmation programmeurs programmes\u0026#34; } 参数 #     参数 值 说明     type stemmer 过滤器类型   language light_french / french / minimal_french 选择词干算法    在语言分析器中的位置 #   法语分析器 内置了 light_french 词干提取器。\n","subcategory":null,"summary":"","tags":null,"title":"法语词干过滤器（French Stemmer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/french-stem/"},{"category":null,"content":"时间序列索引优化 #   最低版本：1.12.1\n 概述 #  在处理时序数据（如日志、监控指标、事件流）时，数据通常具有明显的时间先后顺序。Easysearch 底层的 Lucene Segment 合并是保证搜索性能和资源效率的关键操作。 然而，默认的合并策略（TieredMergePolicy）主要基于 Segment 的大小和删除文档比例来决定合并哪些 Segment，它并不感知数据的时间属性。\n对于时序场景，默认策略可能导致：\n 冷热数据混合合并：较旧的（冷）数据 Segment 可能与较新的（热）数据 Segment 合并，导致不必要的 I/O 和 CPU 开销。 查询性能下降：跨时间范围的大 Segment 可能降低按时间范围过滤的查询效率。  为此，Easysearch 引入了基于时间范围的合并策略（TimeRangeMergePolicy），专为时序索引优化 Segment 合并行为。\n核心原理 #  TimeRangeMergePolicy 在选择要合并的 Segment 时，除了考虑大小、删除比例等因素外，优先考虑 Segment 所覆盖的时间范围：\n 时间优先：倾向于合并时间上相邻的 Segment，保持数据的\u0026quot;时间局部性\u0026quot;。 保留时间分区：避免将时间跨度很大的 Segment 合并在一起。 优先合并新数据：新写入的数据变化更频繁，优先合并较新的 Segment 有助于更快回收空间和优化查询性能。  工作流程 #   每个 Segment 在创建时记录 min_timestamp 和 max_timestamp。 合并候选按最新时间倒序排列（同一时间的按大小排列）。 候选合并组的评分以时间跨度为主要因子 — 时间跨度越小评分越优。 单次合并限制在 max_merge_at_once 个 Segment 以内，合并后大小不超过 max_merged_segment。   如何启用 #  设置索引的 index.merge.policy.time_range_field 为时间字段名即可启用。\n对已有索引启用 #  PUT /my-timeseries-index/_settings { \u0026#34;index\u0026#34;: { \u0026#34;merge.policy.time_range_field\u0026#34;: \u0026#34;@timestamp\u0026#34; } } 通过索引模板默认启用 #  PUT /_index_template/timeseries_template { \u0026#34;index_patterns\u0026#34;: [\u0026#34;logs-*\u0026#34;, \u0026#34;metrics-*\u0026#34;], \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;index.merge.policy.time_range_field\u0026#34;: \u0026#34;@timestamp\u0026#34; } } }  注意：time_range_field 指定的字段必须是 date 类型，且在索引 Mapping 中已定义。\n  配置参考 #  核心设置 #     设置 描述 类型 默认值 动态     index.merge.policy.time_range_field 时间字段名。设置后自动启用 TimeRangeMergePolicy。 string \u0026quot;\u0026quot; 否    合并行为设置 #  以下设置适用于 TimeRangeMergePolicy 和默认的 TieredMergePolicy：\n   设置 描述 类型 默认值 范围 动态     index.merge.policy.max_merge_at_once 单次合并的最大 Segment 数量。 int 10 ≥ 2 是   index.merge.policy.max_merged_segment 合并产生的 Segment 最大大小。超过此大小的 Segment 不参与合并。 ByteSize 5gb ≥ 0 是   index.merge.policy.segments_per_tier 每层允许的 Segment 数量。值越小合并越频繁、Segment 越少。 double 10.0 ≥ 2.0 是   index.merge.policy.floor_segment 小于此大小的 Segment 会被\u0026quot;取整\u0026quot;到此大小参与合并评分。 ByteSize 2mb \u0026gt; 0 是   index.merge.policy.expunge_deletes_allowed 删除文档百分比超过此阈值的 Segment 才会被强制合并清理。 double 10.0 ≥ 0.0 是   index.merge.policy.deletes_pct_allowed 索引中允许的最大删除文档百分比。 double 33.0 20.0 ~ 50.0 是    高级设置 #     设置 描述 类型 默认值 状态     index.merge.policy.max_merge_at_once_explicit force merge 时的最大合并 Segment 数量。 int 30 已弃用   index.merge.policy.reclaim_deletes_weight 旧版删除回收权重。 double 2.0 已弃用   index.compound_format 复合文件系统比率。超过此比例的 Segment 不使用复合文件格式。 double 0.1 —     优势 #   降低合并开销：显著减少冷热数据混合合并，降低不必要的 I/O 和 CPU 消耗。 提高资源效率：更智能的合并可能降低存储空间占用（更快回收已删除空间）。 优化查询性能：保持 Segment 的时间局部性，提升时间范围过滤查询的速度。 对时序数据友好：特别适合日志、指标等严格按时间增长的数据模式。  注意事项 #   时间字段依赖：必须正确指定一个能代表数据时间的 date 类型字段。如果字段不存在或数据时间分布混乱，策略效果可能不佳。 适用场景：最适用于具有明显时间序列特征的数据。对于非时序数据，默认的 TieredMergePolicy 可能更合适。 不可动态切换：time_range_field 设置不支持动态修改，需要在索引创建时或关闭索引后设置。 Force Merge：执行 force merge 时，TimeRangeMergePolicy 会移除最大 Segment 大小限制，允许合并到目标段数。   完整示例 #  创建优化的时序索引 #  PUT /metrics-2024 { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;index.merge.policy.time_range_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;index.merge.policy.max_merged_segment\u0026#34;: \u0026#34;10gb\u0026#34;, \u0026#34;index.merge.policy.segments_per_tier\u0026#34;: 8 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;host\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;cpu_usage\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34; } } } } 结合生命周期管理使用 #  在索引模板中同时启用时间范围合并策略和 ILM 策略：\nPUT /_index_template/metrics_template { \u0026#34;index_patterns\u0026#34;: [\u0026#34;metrics-*\u0026#34;], \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;index.merge.policy.time_range_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;index.lifecycle.name\u0026#34;: \u0026#34;metrics_lifecycle\u0026#34;, \u0026#34;index.lifecycle.rollover_alias\u0026#34;: \u0026#34;metrics\u0026#34; }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;host\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;cpu_usage\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34; }, \u0026#34;mem_usage\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"时间序列索引优化","url":"/easysearch/main/docs/features/data-retention/time-series/"},{"category":null,"content":"时间序列建模 #  时间序列数据（如日志、指标、事件流）是 Easysearch 的常见用例。这类数据有几个特点：文档数量快速增长、基本不更新、主要查询最近的数据。本页介绍如何为时间序列数据设计索引结构。\n时间序列数据的特点 #  与传统的搜索场景不同，时间序列数据有以下特点：\n 文档数量快速增长：日志、指标等数据持续写入，不会停顿 文档基本不更新：写入后很少修改，主要是追加 查询集中在最近数据：大多数查询关注最近几小时、几天或几周的数据 旧数据逐渐失去价值：随着时间推移，旧数据的查询频率降低  按时间范围索引 #  如果我们为此种类型的文档建立一个超大索引，我们可能会很快耗尽存储空间。日志事件会不断的进来，不会停顿也不会中断。\n我们可以使用 scroll 查询和批量删除来删除旧的事件。但这种方法非常低效。当你删除一个文档，它只会被标记为被删除。在包含它的段被合并之前不会被物理删除。\n替代方案是，我们使用一个时间范围索引。你可以着手于一个按年的索引 (logs_2014) 或按月的索引 (logs_2014-10)。也许当你的数据变得十分繁忙时，你需要切换到一个按天的索引 (logs_2014-10-24)。删除旧数据十分简单：只需要删除旧的索引。\n这种方法有这样的优点，允许你在需要的时候进行扩容。你不需要预先做任何艰难的决定。每天都是一个新的机会来调整你的索引时间范围来适应当前需求。\n应用相同的逻辑到决定每个索引的大小上。起初也许你需要的仅仅是每周一个主分片。过一阵子，也许你需要每天五个主分片。这都不重要——任何时间你都可以调整到新的环境。\n使用别名管理时间序列索引 #  别名可以帮助我们更加透明地在索引间切换。当创建索引时，你可以将 logs_current 指向当前索引来接收新的日志事件，当检索时，更新 last_3_months 来指向所有最近三个月的索引：\nPOST /_aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;alias\u0026#34;: \u0026#34;logs_current\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;logs_2014-10\u0026#34; }}, { \u0026#34;remove\u0026#34;: { \u0026#34;alias\u0026#34;: \u0026#34;logs_current\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;logs_2014-09\u0026#34; }}, { \u0026#34;add\u0026#34;: { \u0026#34;alias\u0026#34;: \u0026#34;last_3_months\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;logs_2014-10\u0026#34; }}, { \u0026#34;remove\u0026#34;: { \u0026#34;alias\u0026#34;: \u0026#34;last_3_months\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;logs_2014-07\u0026#34; }} ] } 这样，写入操作始终使用 logs_current 别名，查询操作可以使用 last_3_months 别名来查询最近三个月的数据。\n时间序列索引的最佳实践 #  1. 选择合适的索引粒度 #   按年索引：适合数据量较小、查询跨度较长的场景 按月索引：适合中等数据量、查询跨度中等的场景 按天索引：适合数据量很大、查询主要集中在最近几天的场景  2. 合理设置分片数 #   根据单索引的数据量设置分片数 避免过多的小分片，也避免过少的大分片 考虑查询的并行度和写入的吞吐量  3. 使用索引模板 #  使用索引模板可以自动创建时间序列索引：\nPUT /_template/logs_template { \u0026#34;index_patterns\u0026#34;: [\u0026#34;logs_*\u0026#34;], \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;message\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } 4. 定期清理旧数据 #   根据业务需求设置数据保留策略 定期删除超过保留期的索引 可以使用索引生命周期管理（如果支持）自动化这个过程  查询时间序列数据 #  查询时间序列数据时，通常需要：\n 指定时间范围：使用 range 查询过滤时间 使用别名：通过别名查询多个时间段的索引 时间聚合：使用 date_histogram 聚合分析趋势  示例：\nGET /last_3_months/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-7d/d\u0026#34; } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;per_hour\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;hour\u0026#34; } } } } 小结 #   时间序列数据适合按时间范围组织索引，而不是使用单个大索引 使用别名可以透明地管理时间序列索引的切换 根据数据量和查询模式选择合适的索引粒度（年/月/天） 定期清理旧数据，避免存储成本过高 使用索引模板自动化索引创建过程  下一步可以继续阅读：\n  多租户建模  索引设置  别名（Aliases）  数据生命周期与保留策略  ","subcategory":null,"summary":"","tags":["最佳实践","数据建模"],"title":"时间序列建模","url":"/easysearch/main/docs/best-practices/data-modeling/time-series/"},{"category":null,"content":"斯堪的纳维亚归一化过滤器 #  scandinavian_normalization 词元过滤器将斯堪的纳维亚语言中互换使用的字符对统一为一种形式，使跨语言搜索更一致。\n归一化规则 #     原始字符 归一化为 说明     ä æ 瑞典语 ä → 丹麦语/挪威语 æ   ö ø 瑞典语 ö → 丹麦语/挪威语 ø   Ä Æ 大写同理   Ö Ø 大写同理     与 scandinavian_folding 的区别：scandinavian_normalization 只统一互换字符对（ä↔æ, ö↔ø），不会折叠到 ASCII 基础字符。而 scandinavian_folding 会进一步折叠为 a、o 等 ASCII 字符。\n 使用示例 #  PUT my-scandi-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;scandi_norm\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;scandinavian_normalization\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_scandi\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;scandi_norm\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;scandinavian_normalization\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;räksmörgås\u0026#34; } 响应中 räksmörgås → pair ræksmørgås（ä→æ, ö→ø，但 å 保持不变）。\n参数 #  此过滤器不接受任何参数。\n适用场景 #   跨斯堪的纳维亚语言搜索（瑞典语+丹麦语+挪威语混合索引） 希望统一 ä/æ 和 ö/ø，但保留 å、ø 等字符不折叠到 ASCII 如果需要更激进的折叠（å→a），请改用 scandinavian_folding  ","subcategory":null,"summary":"","tags":null,"title":"斯堪的纳维亚归一化过滤器（Scandinavian Normalization）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/scandinavian-normalization/"},{"category":null,"content":"斯堪的纳维亚字符折叠过滤器 #  scandinavian_folding 词元过滤器将斯堪的纳维亚语言中互换使用的字符统一归一化。\n折叠规则 #     原始字符 折叠为 涉及语言     å a 瑞典语、丹麦语、挪威语   ä, æ a 瑞典语(ä) / 丹麦语、挪威语(æ)   ö, ø o 瑞典语(ö) / 丹麦语、挪威语(ø)     与 scandinavian_normalization 的区别：scandinavian_normalization 只折叠互换字符对（如 ä↔æ），而 scandinavian_folding 还会进一步折叠到 ASCII 基础字符（如 å→a）。\n 使用示例 #  PUT my-scandi-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;scandi_fold\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;scandinavian_folding\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_scandi\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;scandi_fold\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;scandinavian_folding\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;räksmörgås\u0026#34; } 响应中 räksmörgås → raksmørgas 或 raksmorgas（取决于折叠层级）。\n参数 #  此过滤器不接受任何参数。\n适用场景 #   跨斯堪的纳维亚语言搜索（瑞典语+丹麦语+挪威语混合索引） 用户可能混用 ö/ø 或 ä/æ 的场景  ","subcategory":null,"summary":"","tags":null,"title":"斯堪的纳维亚字符折叠过滤器（Scandinavian Folding）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/scandinavian-folding/"},{"category":null,"content":"捷克语词干过滤器 #  czech_stemmer 词元过滤器使用 Lucene 的轻量级捷克语词干算法，移除捷克语常见的形态后缀。\n功能说明 #  此过滤器使用轻量级算法，适度移除名词格变化和动词变位后缀，在词干精度和召回率之间取得平衡。\n使用示例 #  PUT my-czech-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;czech_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;czech\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_czech\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;czech_stem\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;czech\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;programátorů programování\u0026#34; } 参数 #     参数 值 说明     type stemmer 过滤器类型   language czech 指定捷克语词干算法    在语言分析器中的位置 #   捷克语分析器 内置了此过滤器。\n","subcategory":null,"summary":"","tags":null,"title":"捷克语词干过滤器（Czech Stemmer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/czech-stem/"},{"category":null,"content":"德语词干过滤器 #  german_light_stem 词元过滤器使用轻量级算法对德语文本进行词干提取。\n功能说明 #  Easysearch 提供多种德语词干算法：\n   算法 language 值 说明     轻量级 light_german 默认，最保守   最小化 minimal_german 只处理复数   Snowball german 标准 Snowball 算法   Snowball2 german2 改进的 Snowball    使用示例 #  PUT my-german-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;de_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;light_german\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_german\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;german_normalization\u0026#34;, \u0026#34;de_stem\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;german\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Programmierung Programmierer Programme\u0026#34; } 参数 #     参数 值 说明     type stemmer 过滤器类型   language light_german / minimal_german / german / german2 选择词干算法    在语言分析器中的位置 #   德语分析器 内置了 light_german 词干提取器。\n","subcategory":null,"summary":"","tags":null,"title":"德语词干过滤器（German Stemmer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/german-stem/"},{"category":null,"content":"德语归一化过滤器 #  german_normalization 词元过滤器将德语特有的元音变音和 ß 归一化为 ASCII 等价形式。\n归一化规则 #     原始 归一化 说明     ä a 元音变音归一化   ö o 元音变音归一化   ü u 元音变音归一化   Ä A 大写变音归一化   Ö O 大写变音归一化   Ü U 大写变音归一化   ß ss 双 s 替换     注意：这与 ae → a 不同。此过滤器只处理变音符号字符本身，不处理 ae/oe/ue 的拼写变体。\n 使用示例 #  PUT my-german-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;german_norm\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;german_normalization\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_german\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;german_norm\u0026#34;, \u0026#34;german_stemmer\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;german_normalization\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;Straße Übung Ärger\u0026#34; } 响应中 Straße → strasse、Übung → ubung、Ärger → arger。\n参数 #  此过滤器不接受任何参数。\n在语言分析器中的位置 #   德语分析器 内置了此过滤器，其分析链为：\nstandard 分词器 → lowercase → stop（德语） → keyword_marker（stem_exclusion） → german_normalization → german_light_stem\n","subcategory":null,"summary":"","tags":null,"title":"德语归一化过滤器（German Normalization）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/german-normalization/"},{"category":null,"content":"巴西葡萄牙语词干过滤器 #  brazilian_stemmer 词元过滤器对巴西葡萄牙语文本进行词干提取，使用 Lucene 的 BrazilianStemmer 算法。\n功能说明 #  与通用葡萄牙语词干提取不同，此过滤器专门针对巴西葡萄牙语的特点进行优化：\n 处理巴西特有的动词变位形式 移除名词/形容词的阴阳性和单复数后缀 处理副词后缀 -mente  使用示例 #  PUT my-brazilian-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;br_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;brazilian\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_brazilian\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;br_stem\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;brazilian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;brasileiros programação\u0026#34; } 参数 #     参数 值 说明     type stemmer 过滤器类型   language brazilian 指定巴西葡萄牙语词干算法    在语言分析器中的位置 #   巴西葡萄牙语分析器 内置了此过滤器。\n","subcategory":null,"summary":"","tags":null,"title":"巴西葡萄牙语词干过滤器（Brazilian Stemmer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/brazilian-stem/"},{"category":null,"content":"密码修改 #  Easysearch Operator 将集群密码保存在 Kubernetes Secret 中。修改密码只需更新 Secret，Operator 会自动检测变更并应用。\n操作步骤 #   查看现有 Secret  kubectl get secret threenodes-admin-password -o yaml 创建或修改密码 Secret 文件  apiVersion: v1 kind: Secret metadata: name: threenodes-admin-password type: Opaque data: # admin（base64 编码） username: YWRtaW4= # admin123（base64 编码） password: YWRtaW4xMjM=  提示：使用 echo -n \u0026quot;your_password\u0026quot; | base64 生成 base64 编码值。\n 应用修改  kubectl apply -f admin-credentials-secret.yaml 自动生效流程 #  Operator 感知到 threenodes-admin-password Secret 变更后：\n 通过比较密码 hash 值与 Job annotations 中的 securityconfig/checksum 判断是否有更新 如果检测到变更，自动重新执行 threenodes-securityconfig-update Job 该 Job 通过管理员证书调用 Easysearch 安全 API 修改密码  # Job 内部执行的核心命令 curl -k -XPUT --cert admin-credentials/tls.crt --key admin-credentials/tls.key \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  \u0026#39;https://threenodes.default.svc.cluster.local:9200/_security/user/admin\u0026#39; \\  -d \u0026#39;{\u0026#34;password\u0026#34;: \u0026#34;admin123\u0026#34;, \u0026#34;external_roles\u0026#34;: [\u0026#34;admin\u0026#34;]}\u0026#39; 验证新密码  # 使用新密码测试 kubectl exec -it threenodes-masters-0 -- \\  curl -ku admin:admin123 https://localhost:9200/_cluster/health 操作演示 #    autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  ","subcategory":null,"summary":"","tags":null,"title":"密码修改","url":"/easysearch/main/docs/deployment/install-guide/operator/update_password/"},{"category":null,"content":"安全与多租户最佳实践 #  这页不讲具体配置字段，而是从“安全架构设计”的角度，回答几个问题：\n Easysearch 集群应该如何接入现有身份体系（认证）？ 权限应该怎么分层：集群、索引、文档、字段？ 多租户与业务隔离应该选什么模式？ 审计与运维流程里，Easysearch 扮演什么角色？  具体配置与 API 细节，请以参考手册中的安全模块文档为准。\n1. 认证：尽量接到现有身份系统上 #  推荐优先考虑的几种接入方式：\n 企业已有的 SSO / LDAP / AD / OIDC 等身份源 统一网关/反向代理（如 API Gateway / Ingress）前置认证，Easysearch 侧主要做鉴权  在 Easysearch 安全模块中：\n 通过配置后端（backend）连接外部身份源 使用角色映射将“后端角色”转换为 Easysearch 内部的“安全角色”  实践建议：\n 尽量避免在 Easysearch 里维护大量“本地用户”，而是以少量系统账号 + 外部身份源为主 为自动任务（备份、同步、监控等）单独准备技术账号与角色，避免用人类账号跑程序  2. 授权：从粗到细分层设计 #  可以从外到内分三层思考权限模型：\n 集群级：谁能做集群管理操作（如创建索引、调整设置、管理快照） 索引级：谁能读/写哪些索引（按业务域、租户、环境划分） 文档/字段级：在同一索引内，不同用户/角色是否看到不同的数据/字段  结合 Easysearch 的安全功能：\n 使用 角色 来封装一组权限（cluster + indices + document/field level） 对于跨多个索引的权限，可以用索引模式（如 logs-*, orders-*）来归并  实践建议：\n 先设计少量“基线角色”（如只读分析、业务写入、运维管理），再在此基础上做定制 对于高敏感字段（如 PII、密钥等），优先考虑字段级/文档级限制，而不是在应用层“忘记返回”  3. 多租户：按业务与风险选择隔离级别 #  常见多租户模式：\n 按索引隔离：每个租户一组索引（如 tenant-a-logs-*），通过角色限制可访问的索引模式 按字段标记隔离：在同一索引中用 tenant_id 字段区分租户，再用文档级安全做过滤  权衡：\n 按索引隔离：  优点：边界清晰，易于做备份/迁移/生命周期管理 成本：租户多时索引数量增长，需要小心控制 shard 数量   按字段标记隔离：  优点：索引数量可控，适合租户数量特别多且单租户数据相对较少的场景 成本：文档级权限判断增加查询开销，模型更复杂    实践建议：\n 少量大租户：更倾向于“一租户一组索引” 大量小租户：更倾向于“按字段标记 + 文档级安全”，但要严格限制哪些查询能访问这些索引  4. 审计与运维：把安全融入日常流程 #  安全相关的日常工作包括：\n 定期审查角色与用户映射，清理不再使用的账号与权限 针对高风险操作（删除索引、修改安全配置等）启用审计与告警 在运维脚本与自动化任务中，使用“最小权限”的技术账号与角色  Easysearch 侧可以提供：\n 审计日志：记录关键安全事件（登录、鉴权失败、敏感操作等） 指标与告警：如权限相关错误的异常增长、可疑的访问模式等  这些能力可以与现有的日志/监控平台配合，形成整体安全观测面板。\n5. 和其他章节的关系 #   认证与外部身份源对接的配置细节：见集成章节中的安全集成小节，以及参考手册的安全配置文档 多租户与索引划分的建模讨论：见数据建模章节中的多租户部分 索引生命周期与备份策略对安全与合规的支撑：见数据生命周期与备份/恢复相关章节  参考手册（API 与参数） #  如果你已经确定了安全与多租户的整体方案，下一步可以在这些 Reference 页面里查具体字段与 API：\n  安全模块总览  用户、角色与访问控制  字段级安全、 文档级安全  本地安全配置（YAML）  ","subcategory":null,"summary":"","tags":null,"title":"安全与多租户最佳实践","url":"/easysearch/main/docs/best-practices/security-and-multi-tenancy/"},{"category":null,"content":"孟加拉语归一化过滤器 #  bengali_normalization 词元过滤器对孟加拉语（বাংলা）文本进行 Unicode 归一化，统一字符的多种表示形式。\n归一化规则 #     处理 说明     Nukta 合成 将 base + nukta 组合转为对应的预组合字符   变体统一 统一视觉上相同但编码不同的字符   印度语系通用归一化 在 indic_normalization 基础上进一步处理    使用示例 #  PUT my-bengali-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;bengali_norm\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bengali_normalization\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_bengali\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;indic_normalization\u0026#34;, \u0026#34;bengali_norm\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;bengali_normalization\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;বাংলাদেশ\u0026#34; } 参数 #  此过滤器不接受任何参数。\n在语言分析器中的位置 #   孟加拉语分析器 内置了此过滤器，其分析链为：\nstandard 分词器 → lowercase → decimal_digit → indic_normalization → bengali_normalization → stop（孟加拉语） → bengali_stemmer\n","subcategory":null,"summary":"","tags":null,"title":"孟加拉语归一化过滤器（Bengali Normalization）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/bengali-normalization/"},{"category":null,"content":"备份与恢复 #  副本提供了高可用性，但无法防御灾难性故障（如误删除、数据损坏、机房级故障）。快照（Snapshot） 是 Easysearch 的备份机制，提供完整的数据保护能力。\n 快照机制概述 #  核心特点 #   增量备份：首次快照是全量备份，后续快照只保存变化的数据 不阻塞写入：快照过程不会阻止正常的索引和搜索操作 支持多种存储：共享文件系统、S3、HDFS、Azure Blob 等 灵活恢复：可恢复到同一集群或不同集群，支持重命名  工作流程 #  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ 创建仓库 │───▶│ 执行快照 │───▶│ 恢复数据 │ └─────────────┘ └─────────────┘ └─────────────┘ │ │ │ ▼ ▼ ▼ 配置存储位置 选择索引范围 选择目标索引 设置访问权限 等待快照完成 可重命名恢复  创建快照仓库 #  快照需要先配置一个仓库（Repository），仓库定义了快照的存储位置。\n共享文件系统仓库 #  前提条件：\n 所有节点都能访问同一个共享目录（NFS、GlusterFS 等） 在 easysearch.yml 中配置允许的路径：  path.repo: [\u0026#34;/mount/backups\u0026#34;, \u0026#34;/mount/snapshots\u0026#34;] 创建仓库：\nPUT _snapshot/my_backup { \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;/mount/backups/easysearch\u0026#34;, \u0026#34;compress\u0026#34;: true, \u0026#34;max_snapshot_bytes_per_sec\u0026#34;: \u0026#34;50mb\u0026#34;, \u0026#34;max_restore_bytes_per_sec\u0026#34;: \u0026#34;50mb\u0026#34; } } 参数说明：\n   参数 默认值 说明     location 必填 快照存储路径   compress true 是否压缩元数据文件   max_snapshot_bytes_per_sec 40mb 快照写入限速   max_restore_bytes_per_sec 40mb 恢复读取限速    查询参数：\n   参数 类型 说明     verify boolean 创建后是否自动验证仓库（默认自动验证）   master_timeout time 连接主节点的超时时间   timeout time 操作超时时间    S3 兼容存储仓库 #   将 AWS 访问密钥添加到 Easysearch 密钥库：  sudo ./bin/easysearch-keystore add s3.client.default.access_key sudo ./bin/easysearch-keystore add s3.client.default.secret_key （可选）如果使用临时凭据，添加会话令牌：  sudo ./bin/easysearch-keystore add s3.client.default.session_token （可选）如果通过代理连接，添加代理凭据：  sudo ./bin/easysearch-keystore add s3.client.default.proxy.username sudo ./bin/easysearch-keystore add s3.client.default.proxy.password （可选）将其他 S3 客户端设置添加到 easysearch.yml：  s3.client.default.endpoint: s3.amazonaws.com s3.client.default.protocol: https s3.client.default.proxy.host: my-proxy-host s3.client.default.proxy.port: 8080 s3.client.default.read_timeout: 50s s3.client.default.max_retries: 3 s3.client.default.use_throttle_retries: true s3.client.default.path_style_access: false s3.client.default.disable_chunked_encoding: false 如果修改了 easysearch.yml，需要重启节点。否则只需重新加载安全设置：  POST _nodes/reload_secure_settings 确保 S3 bucket 存在，并具备访问权限。以下 IAM 策略示例：  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [\u0026#34;s3:*\u0026#34;], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::your-bucket\u0026#34;, \u0026#34;arn:aws:s3:::your-bucket/*\u0026#34; ] } ] } 注册仓库：  PUT _snapshot/s3_backup { \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;bucket\u0026#34;: \u0026#34;my-backup-bucket\u0026#34;, \u0026#34;base_path\u0026#34;: \u0026#34;easysearch/snapshots\u0026#34;, \u0026#34;compress\u0026#34;: true } } S3 仓库参数：\n   参数 默认值 说明     bucket 必填 S3 bucket 名称   base_path 根目录 bucket 中的存储路径   client default 客户端配置名称，对应 s3.client.\u0026lt;name\u0026gt; 设置   compress false 是否压缩元数据文件   chunk_size 1gb 文件分块大小   buffer_size min(100MB, 5% 堆) 分块上传的缓冲区大小（5MB ～ 5GB）   server_side_encryption false 是否启用 S3 服务端 AES-256 加密   canned_acl private S3 预定义 ACL   storage_class standard S3 存储类型（不要使用 glacier 和 deep_archive）   max_snapshot_bytes_per_sec 40mb 快照写入限速   max_restore_bytes_per_sec 40mb 恢复读取限速   readonly false 是否只读    URL 仓库（只读） #  URL 仓库是一种只读的快照仓库类型，用于从远程 HTTP/HTTPS 地址恢复快照。它通常用于：\n 从其他集群共享的快照中恢复数据 从公开发布的快照仓库中恢复数据 将共享文件系统仓库以 HTTP 方式暴露供远程集群访问  前提条件：在 easysearch.yml 中配置允许的 URL 地址：\nrepositories.url.allowed_urls: [\u0026#34;http://backup-server.example.com/snapshots/*\u0026#34;, \u0026#34;https://cdn.example.com/es-snapshots/*\u0026#34;]  支持 *（匹配任意字符）和 **（匹配多层路径）通配符。file:// 协议也受支持，但需要在 path.repo 中注册路径。\n 注册 URL 仓库：\nPUT _snapshot/remote_backup { \u0026#34;type\u0026#34;: \u0026#34;url\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://backup-server.example.com/snapshots/easysearch/\u0026#34; } } URL 仓库参数：\n   参数 说明     url 快照数据的根 URL（必填），支持 http、https、ftp、file 和 jar 协议     使用场景：集群 A 使用共享文件系统仓库创建快照，通过 Web 服务器（如 Nginx）将快照目录发布为 HTTP 服务，集群 B 使用 URL 仓库从该地址恢复数据。\n 验证仓库配置 #  POST _snapshot/my_backup/_verify 成功响应：\n{ \u0026#34;nodes\u0026#34;: { \u0026#34;node-1\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;node-1\u0026#34; }, \u0026#34;node-2\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;node-2\u0026#34; } } } 查看所有仓库 #  GET _snapshot/_all  创建快照 #  快照所有索引 #  PUT _snapshot/my_backup/snapshot_20260211 { \u0026#34;indices\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;ignore_unavailable\u0026#34;: true, \u0026#34;include_global_state\u0026#34;: true } 参数说明：\n   参数 默认值 说明     indices 所有索引 要备份的索引（支持通配符）   ignore_unavailable false 是否忽略不存在的索引   include_global_state true 是否包含集群状态（模板、设置等）    快照指定索引 #  PUT _snapshot/my_backup/snapshot_logs_20260211 { \u0026#34;indices\u0026#34;: \u0026#34;logs-2026.02.*\u0026#34;, \u0026#34;ignore_unavailable\u0026#34;: true, \u0026#34;include_global_state\u0026#34;: false } 等待快照完成 #  默认情况下，快照请求立即返回，快照在后台执行。如需等待完成：\nPUT _snapshot/my_backup/snapshot_20260211?wait_for_completion=true  注意：大型快照可能需要较长时间，建议在脚本中使用轮询方式监控进度。\n  监控快照进度 #  查看快照状态 #  GET _snapshot/my_backup/snapshot_20260211 响应示例：\n{ \u0026#34;snapshots\u0026#34;: [ { \u0026#34;snapshot\u0026#34;: \u0026#34;snapshot_20260211\u0026#34;, \u0026#34;uuid\u0026#34;: \u0026#34;abc123...\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;SUCCESS\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2026-02-11T02:00:00.000Z\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;2026-02-11T02:15:30.000Z\u0026#34;, \u0026#34;duration_in_millis\u0026#34;: 930000, \u0026#34;indices\u0026#34;: [\u0026#34;logs-2026.02.10\u0026#34;, \u0026#34;logs-2026.02.11\u0026#34;], \u0026#34;shards\u0026#34;: { \u0026#34;total\u0026#34;: 10, \u0026#34;failed\u0026#34;: 0, \u0026#34;successful\u0026#34;: 10 } } ] } 快照状态：\n   状态 说明     IN_PROGRESS 快照正在进行   SUCCESS 快照成功完成   FAILED 快照失败   PARTIAL 部分分片失败，但快照已创建（仅在 partial: true 时出现）   INCOMPATIBLE 快照与当前 Easysearch 版本不兼容    查看进行中快照的详细进度 #  GET _snapshot/my_backup/snapshot_20260211/_status 响应示例：\n{ \u0026#34;snapshots\u0026#34;: [ { \u0026#34;snapshot\u0026#34;: \u0026#34;snapshot_20260211\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;IN_PROGRESS\u0026#34;, \u0026#34;shards_stats\u0026#34;: { \u0026#34;initializing\u0026#34;: 0, \u0026#34;started\u0026#34;: 2, \u0026#34;finalizing\u0026#34;: 1, \u0026#34;done\u0026#34;: 7, \u0026#34;failed\u0026#34;: 0, \u0026#34;total\u0026#34;: 10 }, \u0026#34;indices\u0026#34;: { \u0026#34;logs-2026.02.11\u0026#34;: { \u0026#34;shards_stats\u0026#34;: { \u0026#34;done\u0026#34;: 3, \u0026#34;total\u0026#34;: 5 }, \u0026#34;shards\u0026#34;: { \u0026#34;0\u0026#34;: { \u0026#34;stage\u0026#34;: \u0026#34;DONE\u0026#34;, \u0026#34;stats\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;file_count\u0026#34;: 15, \u0026#34;size_in_bytes\u0026#34;: 1073741824 }, \u0026#34;processed\u0026#34;: { \u0026#34;file_count\u0026#34;: 15, \u0026#34;size_in_bytes\u0026#34;: 1073741824 } } } } } } } ] } 列出所有快照 #  GET _snapshot/my_backup/_all  恢复快照 #  恢复所有索引 #  POST _snapshot/my_backup/snapshot_20260211/_restore 恢复指定索引 #  POST _snapshot/my_backup/snapshot_20260211/_restore { \u0026#34;indices\u0026#34;: \u0026#34;logs-2026.02.10\u0026#34;, \u0026#34;ignore_unavailable\u0026#34;: true } 恢复并重命名索引 #  当需要恢复到已存在的集群，或者想要对比新旧数据时：\nPOST _snapshot/my_backup/snapshot_20260211/_restore { \u0026#34;indices\u0026#34;: \u0026#34;logs-2026.02.10\u0026#34;, \u0026#34;rename_pattern\u0026#34;: \u0026#34;(.+)\u0026#34;, \u0026#34;rename_replacement\u0026#34;: \u0026#34;restored_$1\u0026#34; } 结果：logs-2026.02.10 恢复为 restored_logs-2026.02.10\n恢复到不同的索引设置 #  POST _snapshot/my_backup/snapshot_20260211/_restore { \u0026#34;indices\u0026#34;: \u0026#34;logs-2026.02.10\u0026#34;, \u0026#34;index_settings\u0026#34;: { \u0026#34;index.number_of_replicas\u0026#34;: 0 }, \u0026#34;ignore_index_settings\u0026#34;: [ \u0026#34;index.refresh_interval\u0026#34; ] }  技巧：恢复时先设置 number_of_replicas: 0，恢复完成后再调整为期望值，可以加快恢复速度。\n 恢复请求参数 #     参数 默认值 说明     indices 所有索引 要恢复的索引（支持通配符和 - 排除）   ignore_unavailable false 是否忽略不存在的索引   include_global_state false 是否恢复集群状态   include_aliases true 是否恢复别名及关联索引   partial false 是否允许恢复部分快照   rename_pattern — 匹配索引名的正则表达式，配合 rename_replacement 使用   rename_replacement — 替换模式。$0 代表完整匹配，$1 代表第一个捕获组   index_settings — 恢复时覆盖的索引设置   ignore_index_settings — 恢复时忽略的索引设置（使用集群默认值）    等待恢复完成 #  POST _snapshot/my_backup/snapshot_20260211/_restore?wait_for_completion=true  监控恢复进度 #  查看恢复状态 #  GET _cat/recovery?v\u0026amp;active_only=true 输出示例：\nindex shard time type stage source_node target_node files_percent bytes_percent logs-2026.02.10 0 2m snapshot index n/a node-1 85.0% 72.3% logs-2026.02.10 1 1m30s snapshot index n/a node-2 100.0% 100.0% 查看特定索引的恢复进度 #  GET logs-2026.02.10/_recovery  删除快照 #  使用 API 删除快照（不要手动删除文件）：\nDELETE _snapshot/my_backup/snapshot_20260210  重要：快照是增量的，手动删除文件可能损坏其他快照。始终使用 API 删除。\n 取消正在进行的快照 #  DELETE _snapshot/my_backup/snapshot_in_progress  克隆快照 #  在同一个仓库内，将一个快照中的部分索引克隆到一个新的快照中，无需重新读取原始数据：\nPUT _snapshot/my_backup/snapshot_20260211/_clone/snapshot_logs_only { \u0026#34;indices\u0026#34;: \u0026#34;logs-*\u0026#34; }  使用场景：需要保留特定索引的快照副本，或在清理旧快照前单独保存关键数据。\n  清理仓库 #  删除快照后，仓库中可能残留无用的数据文件。使用清理 API 回收这些孤立数据：\nPOST _snapshot/my_backup/_cleanup 响应示例：\n{ \u0026#34;results\u0026#34;: { \u0026#34;deleted_bytes\u0026#34;: 1073741824, \u0026#34;deleted_blobs\u0026#34;: 42 } }  建议：在批量删除快照后执行一次清理操作。\n  删除仓库 #  DELETE _snapshot/my_backup  注意：删除仓库只会取消注册，不会删除存储中的快照数据。如需清除存储空间，需要手动删除底层存储中的文件。\n  备份策略建议 #  备份频率 #     数据类型 建议频率 保留周期     业务核心数据 每日 30-90 天   日志/指标 每日或每周 7-30 天   配置/模板 变更时 永久    命名规范 #  推荐使用包含日期和类型的命名：\nsnapshot_全量_20260211 snapshot_logs_20260211 snapshot_hourly_2026021110 自动化备份脚本 #  #!/bin/bash # daily_backup.sh - 每日自动备份脚本 ES_HOST=\u0026quot;http://localhost:9200\u0026quot; REPO=\u0026quot;my_backup\u0026quot; DATE=$(date +%Y%m%d) SNAPSHOT_NAME=\u0026quot;snapshot_daily_${DATE}\u0026quot;\n# 创建快照 curl -X PUT \u0026quot;${ES_HOST}/_snapshot/${REPO}/${SNAPSHOT_NAME}?wait_for_completion=true\u0026quot;  -H 'Content-Type: application/json'  -d '{ \u0026quot;indices\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;ignore_unavailable\u0026quot;: true, \u0026quot;include_global_state\u0026quot;: true }'\n# 检查结果 STATUS=$(curl -s \u0026quot;${ES_HOST}/_snapshot/${REPO}/${SNAPSHOT_NAME}\u0026quot; | jq -r '.snapshots[0].state') if [ \u0026quot;$STATUS\u0026quot; != \u0026quot;SUCCESS\u0026quot; ]; then echo \u0026quot;Backup failed: $STATUS\u0026quot; # 发送告警\u0026hellip; exit 1 fi\necho \u0026quot;Backup completed: ${SNAPSHOT_NAME}\u0026quot;\n# 清理旧快照（保留最近 30 天） CUTOFF_DATE=$(date -d \u0026quot;30 days ago\u0026quot; +%Y%m%d) for snapshot in $(curl -s \u0026quot;${ES_HOST}/_snapshot/${REPO}/_all\u0026quot; | jq -r '.snapshots[].snapshot'); do SNAP_DATE=$(echo $snapshot | grep -oP '\\d{8}') if [[ -n \u0026quot;$SNAP_DATE\u0026quot; \u0026amp;\u0026amp; \u0026quot;$SNAP_DATE\u0026quot; \u0026lt; \u0026quot;$CUTOFF_DATE\u0026quot; ]]; then curl -X DELETE \u0026quot;${ES_HOST}/_snapshot/${REPO}/${snapshot}\u0026quot; echo \u0026quot;Deleted old snapshot: ${snapshot}\u0026quot; fi done 使用 SLM 自动化（推荐） #\n Easysearch 的快照生命周期管理（SLM）可以自动执行备份策略：\nPUT _slm/policy/daily-snapshots { \u0026#34;description\u0026#34;: \u0026#34;每日自动备份\u0026#34;, \u0026#34;creation\u0026#34;: { \u0026#34;schedule\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 30 2 * * ?\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } } }, \u0026#34;snapshot_config\u0026#34;: { \u0026#34;date_expression\u0026#34;: \u0026#34;\u0026lt;daily-snap-{now/d}\u0026gt;\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;my_backup\u0026#34;, \u0026#34;indices\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;include_global_state\u0026#34;: true }, \u0026#34;deletion\u0026#34;: { \u0026#34;condition\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;min_count\u0026#34;: 5, \u0026#34;max_count\u0026#34;: 50 } } } 参数说明：\n   参数 说明     creation.schedule.cron.expression Cron 表达式（每天凌晨 2:30）   snapshot_config.date_expression 快照名称模板（支持日期变量）   snapshot_config.repository 快照仓库名称   deletion.condition.max_age 快照过期时间   deletion.condition.min_count 最少保留快照数   deletion.condition.max_count 最多保留快照数    手动执行策略：\nPOST _slm/policy/daily-snapshots/_execute 查看策略状态：\nGET _slm/policy/daily-snapshots  安全模块注意事项 #  如果启用了安全模块，快照操作有额外限制：\n 用户必须具备内置 manage_snapshots 角色 无法恢复包含全局状态或 .security 索引的快照  如果快照包含全局状态，恢复时必须排除：\nPOST _snapshot/my_backup/snapshot_20260211/_restore { \u0026#34;indices\u0026#34;: \u0026#34;-.security\u0026#34;, \u0026#34;include_global_state\u0026#34;: false } 如果确实需要恢复 .security 索引，必须在请求中包含管理员证书：\ncurl -k --cert ./admin.pem --key ./admin-key.pem \\  -XPOST \u0026#39;https://localhost:9200/_snapshot/my_backup/snapshot_20260211/_restore?pretty\u0026#39;  版本兼容性 #  快照只能向前兼容一个主要版本。例如：\n 在 1.x 集群上创建的快照可以在 1.x 或 2.x 集群上恢复 但不能在 6.x 集群上恢复  如果有一个旧版本快照，可以尝试恢复到中间版本集群，重新索引后再创建新快照逐步迁移。\n 跨集群恢复 #  场景：恢复到新集群 #   在新集群注册相同的仓库（指向同一存储位置）：  PUT _snapshot/my_backup { \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;/mount/backups/easysearch\u0026#34;, \u0026#34;readonly\u0026#34;: true } }  注意：如果只需要恢复，建议设置 readonly: true 防止误操作。\n 查看可用快照：  GET _snapshot/my_backup/_all 执行恢复：  POST _snapshot/my_backup/snapshot_20260211/_restore { \u0026#34;indices\u0026#34;: \u0026#34;logs-*\u0026#34;, \u0026#34;ignore_unavailable\u0026#34;: true } 场景：灾难恢复演练 #  定期进行恢复演练，验证备份的有效性：\n# 1. 恢复到临时索引 POST _snapshot/my_backup/snapshot_20260211/_restore { \u0026#34;indices\u0026#34;: \u0026#34;critical-data\u0026#34;, \u0026#34;rename_pattern\u0026#34;: \u0026#34;(.+)\u0026#34;, \u0026#34;rename_replacement\u0026#34;: \u0026#34;dr_test_$1\u0026#34; } 2. 验证数据 GET dr_test_critical-data/_count GET dr_test_critical-data/_search?size=10\n3. 清理测试数据 DELETE dr_test_critical-data \n常见问题 #  快照失败：磁盘空间不足 #  症状：快照状态为 FAILED，错误信息包含 disk space\n解决方案：\n 清理旧快照释放空间 扩展仓库存储容量 分批备份大索引  恢复失败：索引已存在 #  症状：恢复时报错 index already exists\n解决方案：\n# 方案 1：先删除目标索引 DELETE logs-2026.02.10 方案 2：使用重命名恢复 POST snapshot/my_backup/snapshot20260211/restore { \u0026quot;indices\u0026quot;: \u0026quot;logs-2026.02.10\u0026quot;, \u0026quot;rename_pattern\u0026quot;: \u0026quot;(.+)\u0026quot;, \u0026quot;rename_replacement\u0026quot;: \u0026quot;restored$1\u0026quot; } 恢复很慢 #\n 优化建议：\n 恢复时设置 number_of_replicas: 0，完成后再调整 增加 max_restore_bytes_per_sec 限速值 确保网络带宽充足  POST _snapshot/my_backup/snapshot_20260211/_restore { \u0026#34;indices\u0026#34;: \u0026#34;big-index\u0026#34;, \u0026#34;index_settings\u0026#34;: { \u0026#34;index.number_of_replicas\u0026#34;: 0 } } 恢复完成后 PUT big-index/_settings { \u0026quot;index.number_of_replicas\u0026quot;: 1 } \nAPI 参考汇总 #  仓库管理 #     操作 方法 端点     创建/更新仓库 PUT / POST /_snapshot/{repository}   查看仓库 GET /_snapshot 或 /_snapshot/{repository}   删除仓库 DELETE /_snapshot/{repository}   验证仓库 POST /_snapshot/{repository}/_verify   清理仓库 POST /_snapshot/{repository}/_cleanup    快照操作 #     操作 方法 端点     创建快照 PUT / POST /_snapshot/{repository}/{snapshot}   查看快照 GET /_snapshot/{repository}/{snapshot}   快照状态 GET /_snapshot/_status 或 /_snapshot/{repository}/{snapshot}/_status   克隆快照 PUT /_snapshot/{repository}/{snapshot}/_clone/{target_snapshot}   删除快照 DELETE /_snapshot/{repository}/{snapshot}   恢复快照 POST /_snapshot/{repository}/{snapshot}/_restore    快照生命周期管理 #     操作 方法 端点     创建/更新策略 PUT /_slm/policy/{policy_id}   查看策略 GET /_slm/policy/{policy_id}   执行策略 POST /_slm/policy/{policy_id}/_execute   删除策略 DELETE /_slm/policy/{policy_id}    最佳实践：\n 定期备份，使用 SLM 自动化 定期验证，进行恢复演练 异地存储，防止机房级故障 监控告警，及时发现备份失败  下一步阅读：\n  索引生命周期管理：自动化索引生命周期  SLM 快照生命周期：自动化快照策略  可搜索快照：从快照直接搜索  数据生命周期：完整的数据保留策略  ","subcategory":null,"summary":"","tags":null,"title":"备份与恢复","url":"/easysearch/main/docs/features/data-retention/backup-restore/"},{"category":null,"content":"塞尔维亚语归一化过滤器 #  serbian_normalization 词元过滤器将塞尔维亚语西里尔字母转写为对应的拉丁字母形式，使两种书写系统的文本在搜索时可以互相匹配。\n归一化规则 #  塞尔维亚语同时使用西里尔字母和拉丁字母，此过滤器将西里尔形式统一为拉丁形式：\n   西里尔字母 拉丁字母 西里尔字母 拉丁字母     а a п p   б b р r   в v с s   г g т t   д d ћ ć   ђ đ у u   е e ф f   ж ž х h   з z ц c   и i ч č   ј j џ dž   к k ш š   л l љ lj   м m њ nj   н n     о o      使用示例 #  PUT my-serbian-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;serbian_norm\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;serbian_normalization\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_serbian\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;serbian_norm\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;serbian_normalization\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;Београд\u0026#34; } 响应中 Београд（西里尔）→ Beograd（拉丁）。\n参数 #  此过滤器不接受任何参数。\n适用场景 #   索引中混合包含塞尔维亚语西里尔和拉丁文本 用户可能用任一书写系统搜索 无需区分同一单词的西里尔与拉丁拼写  ","subcategory":null,"summary":"","tags":null,"title":"塞尔维亚语归一化过滤器（Serbian Normalization）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/serbian-normalization/"},{"category":null,"content":"土耳其语词干过滤器 #  turkish_stemmer 词元过滤器使用 Snowball 算法对土耳其语文本进行词干提取。\n功能说明 #  土耳其语是黏着语，一个词可以有多层后缀。此词干提取器移除常见的名词格后缀、所有格后缀和动词变位后缀。\n 注意：土耳其语有特殊的大小写规则（İ↔i、I↔ı），需要搭配 turkish_lowercase 过滤器。\n 使用示例 #  PUT my-turkish-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;tr_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;turkish\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_turkish\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;apostrophe\u0026#34;, \u0026#34;turkish_lowercase\u0026#34;, \u0026#34;tr_stem\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;turkish\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;programcıların programlama\u0026#34; } 参数 #     参数 值 说明     type stemmer 过滤器类型   language turkish 指定土耳其语 Snowball 词干算法    在语言分析器中的位置 #   土耳其语分析器 内置了此过滤器，搭配 apostrophe 和 turkish_lowercase。\n","subcategory":null,"summary":"","tags":null,"title":"土耳其语词干过滤器（Turkish Stemmer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/turkish-stem/"},{"category":null,"content":"印度语系归一化过滤器 #  indic_normalization 词元过滤器对印度语系（Indic）文本进行 Unicode 归一化，统一各印度语系脚本中字符的多种表示形式。它是孟加拉语、印地语等语言归一化的基础层。\n归一化规则 #     处理 说明     Unicode 分解与合成 将组合字符序列转为标准的预组合形式（NFC 归一化）   零宽字符清理 移除零宽连接符（ZWJ）和零宽非连接符（ZWNJ）   Nukta 统一 将 base + nukta 两码点序列合并为等价的单码点字符   变体编码统一 统一不同 Unicode 编码表示的相同字符    使用示例 #  PUT my-indic-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;indic_norm\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;indic_normalization\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_indic\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;indic_norm\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;indic_normalization\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;हिन्दी\u0026#34; } 参数 #  此过滤器不接受任何参数。\n使用建议 #  indic_normalization 通常作为语言专用归一化的前置步骤，与语言专用过滤器配合使用：\n   组合 分析链     孟加拉语 indic_normalization → bengali_normalization   印地语 indic_normalization → hindi_normalization    典型分析链顺序：\nstandard 分词器 → lowercase → decimal_digit → indic_normalization → 语言专用归一化 → stop → stemmer\n","subcategory":null,"summary":"","tags":null,"title":"印度语系归一化过滤器（Indic Normalization）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/indic-normalization/"},{"category":null,"content":"印地语归一化过滤器 #  hindi_normalization 词元过滤器对印地语（हिन्दी）文本进行正字法归一化，统一 Devanagari 脚本中的字符变体。\n归一化规则 #     处理 说明     Nukta 移除 移除 nukta 标记（用于外来词音译的点）   视觉归一 统一视觉上相同但编码不同的字符变体   末尾 Chandra 移除 移除词尾的 chandra 符号   搭配 indic_normalization 建议先应用 indic_normalization 过滤器    使用示例 #  PUT my-hindi-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;hindi_norm\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;hindi_normalization\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_hindi\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;indic_normalization\u0026#34;, \u0026#34;hindi_norm\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;indic_normalization\u0026#34;, \u0026#34;hindi_normalization\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;हिन्दी भाषा\u0026#34; } 参数 #  此过滤器不接受任何参数。\n在语言分析器中的位置 #   印地语分析器 内置了此过滤器，其分析链为：\nstandard 分词器 → lowercase → decimal_digit → indic_normalization → hindi_normalization → stop（印地语） → hindi_stemmer\n","subcategory":null,"summary":"","tags":null,"title":"印地语归一化过滤器（Hindi Normalization）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/hindi-normalization/"},{"category":null,"content":"别名（Aliases） #  别名（Alias）是指向一个或多个索引的\u0026quot;虚拟名称\u0026quot;，常用于实现无感迁移、蓝绿切换、按条件路由等功能。合理使用别名，可以让上游客户端几乎不感知底层索引的重建与演进。\n最常用的场景：无感重建索引 #  典型需求：你想调整 mapping/设置，只能重建一个新索引，但又不希望业务代码改来改去。\n做法示意：\n 当前索引为 logs_v1，别名为 logs，所有读写都通过 logs 创建新索引 logs_v2，并将数据迁移过去 将别名 logs 从 logs_v1 原子性切换到 logs_v2 业务只需始终访问 logs，不关心具体版本  好处：\n 切换时可以做到\u0026quot;近乎无停机\u0026quot; 可以在后台验证新索引的可用性与正确性，再切换别名  多索引聚合访问 #  别名还可以指向多个索引，例如：\n 别名 logs_all → 指向 logs_2024_01、logs_2024_02、logs_2024_03 等  查询 logs_all 即可同时访问多个时间分片索引    这种方式对时间序列/多分片索引的统一查询非常有用。\n按条件路由的读写别名 #  别名还可以带有过滤条件或路由信息，例如：\n 为某个租户创建只读别名，只能看见自己的数据 为某些写入创建\u0026quot;写别名\u0026quot;，将写入路由到特定索引/分片  这类高级用法可以在多租户、数据分层等场景中减少上游逻辑复杂度，但也需要更严格的管理与规范。\n API 操作 #  创建索引别名 #  使用 actions 方法指定要执行的操作列表：\nPOST _aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index-1\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias1\u0026#34; } } ] } 响应：\n{ \u0026#34;acknowledged\u0026#34;: true }  如果此请求失败，请确保要添加到别名的索引已存在。\n 检查别名是否引用了指定索引：\nGET alias1 添加与删除操作 #  你可以在同一 _aliases 操作中执行多个操作。例如，以下命令删除 index-1 并将 index-2 添加到 alias1：\nPOST _aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;remove\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index-1\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias1\u0026#34; } }, { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index-2\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias1\u0026#34; } } ] } add 和 remove 操作以原子方式发生，这意味着 alias1 不会同时指向 index-1 和 index-2。\n还可以基于索引模式添加索引：\nPOST _aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index*\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias1\u0026#34; } } ] } 创建索引时添加别名 #  PUT index-1 { \u0026#34;aliases\u0026#34;: { \u0026#34;alias1\u0026#34;: {} } } 管理别名 #  列出别名到索引的映射：\nGET _cat/aliases?v 检查别名指向的索引：\nGET _alias/alias1 查找指向特定索引的别名：\nGET /index-2/_alias/* 检查别名是否存在：\nHEAD /alias1/_alias/ 根据筛选条件创建别名 #  可以创建过滤的别名来访问底层索引中的文档子集：\nPOST _aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index-1\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias1\u0026#34;, \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;1574641891142\u0026#34; } } } } ] } 别名选项 #     选项 类型 描述 必填     index String 别名指向的索引名 Yes   alias String 别名名称 No   filter Object 过滤条件 No   routing String 路由到特定分片，可以分别设置 search_routing 或者 index_routing 参数 No   is_write_index String 别名是否允许接收写入操作，如果不指定则表示不允许 No     使用别名时的注意事项 #   保证别名命名规范，与真实索引名有清晰区分（如统一使用 _vN 作为真实索引后缀） 别名切换应使用原子操作，避免出现短暂的\u0026quot;指向多个不一致索引\u0026quot;的窗口 在监控与排障时，注意区分\u0026quot;别名视图\u0026quot;与\u0026quot;真实索引\u0026quot;，避免混淆  小结 #   别名是做索引演进与重建时的重要工具，可以极大降低对上游的侵入 在设计索引生命周期（建/删/迁移）时，应优先考虑通过别名来隔离\u0026quot;调用方感知的名字\u0026quot;与\u0026quot;实际索引名字\u0026quot;  相关文档 #    索引管理：创建/删除索引与零停机切换  索引设置  时间序列建模  多租户建模  ","subcategory":null,"summary":"","tags":null,"title":"别名（Aliases）","url":"/easysearch/main/docs/operations/data-management/aliases/"},{"category":null,"content":"创建一个自定义分词器 #  要创建一个自定义分词器，需要指定以下组成内容：\n 字符过滤器（零个或多个） 词元生成器（一个） 词元过滤器（零个或多个）  相关配置 #  以下参数可用于配置自定义分词器。\n   参数 必填/可选 描述     type 可选 分词器类型。默认值为custom。你也可以给这个参数指定一个预构建的分词器。   tokenizer 必填 每个分词器必须要有一个词元生成器。   char_filter 可选 要包含在分词器中的字符过滤器列表。   filter 可选 要包含在分词器中的分词过滤器列表。   position_increment_gap 可选 在对包含多个词项内容的文本字段建立索引时，应用于各词项之间的额外间隔。有关更多信息，请参阅位置间隔增量。默认值为 100。    参考样例 #  以下示例展示了各种自定义分词器的配置。\n自定义分词器用于去除 HTML 格式标签 #  以下示例中的分词器在分词之前会从文本中移除 HTML 格式标签：\nPUT simple_html_strip_analyzer_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;html_strip_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;html_strip\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;whitespace\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } } } 使用以下请求来查看使用该分词器生成的词元：\nGET simple_html_strip_analyzer_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;html_strip_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;\u0026lt;p\u0026gt;Easysearch is \u0026lt;strong\u0026gt;awesome\u0026lt;/strong\u0026gt;!\u0026lt;/p\u0026gt;\u0026#34; } 返回内容中包含生成的词元：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 3, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 16, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;awesome!\u0026#34;, \u0026#34;start_offset\u0026#34;: 25, \u0026#34;end_offset\u0026#34;: 42, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } 自定义分词器实现同义词替换和字符串映射 #  以下示例中的分词器可以在同义词过滤之前替换特定的字符：\nPUT mapping_analyzer_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;synonym_mapping_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;underscore_to_space\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;stop\u0026#34;, \u0026#34;synonym_filter\u0026#34;] } }, \u0026#34;char_filter\u0026#34;: { \u0026#34;underscore_to_space\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mapping\u0026#34;, \u0026#34;mappings\u0026#34;: [\u0026#34;_ =\u0026gt; \u0026#39; \u0026#39;\u0026#34;] } }, \u0026#34;filter\u0026#34;: { \u0026#34;synonym_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;synonym\u0026#34;, \u0026#34;synonyms\u0026#34;: [ \u0026#34;quick, fast, speedy\u0026#34;, \u0026#34;big, large, huge\u0026#34; ] } } } } } 使用以下请求来查看使用该分词器生成的词元：\nGET mapping_analyzer_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;synonym_mapping_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The slow_green_turtle is very large\u0026#34; } 返回内容中包含生成的词元：\n{ \u0026#34;tokens\u0026#34;: [ {\u0026#34;token\u0026#34;: \u0026#34;slow\u0026#34;,\u0026#34;start_offset\u0026#34;: 4,\u0026#34;end_offset\u0026#34;: 8,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 1}, {\u0026#34;token\u0026#34;: \u0026#34;green\u0026#34;,\u0026#34;start_offset\u0026#34;: 9,\u0026#34;end_offset\u0026#34;: 14,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;,\u0026#34;start_offset\u0026#34;: 15,\u0026#34;end_offset\u0026#34;: 21,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;very\u0026#34;,\u0026#34;start_offset\u0026#34;: 25,\u0026#34;end_offset\u0026#34;: 29,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 5}, {\u0026#34;token\u0026#34;: \u0026#34;large\u0026#34;,\u0026#34;start_offset\u0026#34;: 30,\u0026#34;end_offset\u0026#34;: 35,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 6}, {\u0026#34;token\u0026#34;: \u0026#34;big\u0026#34;,\u0026#34;start_offset\u0026#34;: 30,\u0026#34;end_offset\u0026#34;: 35,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 6}, {\u0026#34;token\u0026#34;: \u0026#34;huge\u0026#34;,\u0026#34;start_offset\u0026#34;: 30,\u0026#34;end_offset\u0026#34;: 35,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 6} ] } 自定义分词器实现数字规范化 #  以下示例中的分词器会通过去除破折号和空格的方式来规范化电话号码，并对规范化后的文本应用边缘 edge n-gram 以支持部分匹配：\nPUT advanced_pattern_replace_analyzer_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;phone_number_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;phone_normalization\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;edge_ngram\u0026#34;] } }, \u0026#34;char_filter\u0026#34;: { \u0026#34;phone_normalization\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern_replace\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;[-\\\\s]\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;edge_ngram\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;edge_ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 3, \u0026#34;max_gram\u0026#34;: 10 } } } } } 使用以下请求来查看使用该分词器生成的词元：\nGET advanced_pattern_replace_analyzer_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;phone_number_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;123-456 7890\u0026#34; } 返回内容中包含生成的词元：\n{ \u0026#34;tokens\u0026#34;: [ {\u0026#34;token\u0026#34;: \u0026#34;123\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 12,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;1234\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 12,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;12345\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 12,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;123456\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 12,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;1234567\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 12,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;12345678\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 12,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;123456789\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 12,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;1234567890\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 12,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0} ] } 处理正则表达式模式中的特殊字符 #  当在分词器中使用自定义正则表达式模式时，要确保能正确处理特殊字符或非英文字符。默认情况下，Java 的正则表达式仅将 [A-Za-z0-9_] 视为单词字符（\\w）。这在使用 \\w 或 \\b（用于匹配单词和非单词字符之间的边界）时可能会导致意外行为。 例如，以下分词器尝试使用模式 (\\b\\p{L}+\\b) 来匹配各语种（\\p{L}）的一个或多个字母字符：\nPUT /buggy_custom_analyzer { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;capture_words\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern_capture\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;(\\\\b\\\\p{L}+\\\\b)\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;filter_only_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;capture_words\u0026#34; ] } } } } } 然而，这个分词器会错误地将 él-empezó-a-reír 分词为 l、empez、a 和 reír，因为 \\b 无法匹配带重音符号的字符与字符串开头或结尾之间的边界。\n为了正确处理特殊字符，你需要在模式中添加 Unicode 大小写标志 (?U)：\nPUT /fixed_custom_analyzer { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;capture_words\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern_capture\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;(?U)(\\\\b\\\\p{L}+\\\\b)\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;filter_only_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;capture_words\u0026#34; ] } } } } } 位置间隔增量 #  position_increment_gap（位置间隔增量）参数用于在对多值字段（如数组）建立索引时，设置词项之间的位置间隔。除非有明确的允许，这个间隔参数可确保短语查询不会出现跨词项匹配。例如，默认间隔值为 100，表示不同数组条目中的词项之间相隔 100 个位置距离，从而在短语搜索中防止出现意外的匹配情况。你可以调整这个值，或者将其设置为 0，以便让短语能够跨越词项进行匹配。\n下面的示例通过使用 match_phrase（匹配短语）查询来演示 position_increment_gap 的作用。\n 在 test-index 索引中索引一个文档：  PUT test-index/_doc/1 { \u0026#34;names\u0026#34;: [ \u0026#34;Slow green\u0026#34;, \u0026#34;turtle swims\u0026#34;] } 使用短语匹配查询（match_phrase）来查询文档：  GET test-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;names\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;green turtle\u0026#34; } } } } 返回内容是空的，这是因为 green 和 turtle 这两个词项之间的距离是 100（即默认的 position_increment_gap 值）。\n现在，使用带有 slop 参数的 match_phrase 查询来查询文档，我们把该参数值设置成大于 position_increment_gap 的 101：  GET test-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;names\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;green turtle\u0026#34;, \u0026#34;slop\u0026#34;: 101 } } } } 返回内容包含了需要匹配的文档\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.010358453, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;test-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.010358453, \u0026#34;_source\u0026#34;: { \u0026#34;names\u0026#34;: [ \u0026#34;Slow green\u0026#34;, \u0026#34;turtle swims\u0026#34; ] } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"创建自定义分析器（Custom）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/creating-a-custom-analyzer/"},{"category":null,"content":"TLS 安全配置指南 #  Easysearch 默认启用安全功能，包括 TLS 加密。本文介绍如何配置和管理 TLS 证书，以保障集群通信安全。\nTLS 的两层保护 #     层级 配置项 保护范围     HTTP 层 security.ssl.http.* 客户端 ↔ Easysearch REST API   Transport 层 security.ssl.transport.* Easysearch 节点 ↔ 节点     生产环境两层都必须启用。\n 使用自签名证书（快速启动） #  bin/initialize.sh -s 会自动生成自签名证书，适合测试和快速验证：\nbin/initialize.sh -s # 生成的证书位于 config/ 目录 使用企业 CA 证书（生产推荐） #  1. 准备证书文件 #  需要以下文件：\n   文件 说明     ca.crt CA 根证书   node.crt 节点证书   node.key 节点私钥   admin.crt Admin 客户端证书   admin.key Admin 客户端私钥    2. 配置 easysearch.yml #  # ========== 启用安全模块 ========== security.enabled: true security.audit.type: noop # ========== Transport 层 TLS ========== security.ssl.transport.cert_file: node.crt security.ssl.transport.key_file: node.key security.ssl.transport.ca_file: ca.crt security.ssl.transport.skip_domain_verify: true\n# ========== HTTP 层 TLS ========== security.ssl.http.enabled: true security.ssl.http.cert_file: node.crt security.ssl.http.key_file: node.key security.ssl.http.ca_file: ca.crt security.ssl.http.clientauth_mode: OPTIONAL\n# ========== Admin 证书 DN ========== security.authcz.admin_dn:\n \u0026quot;CN=admin,OU=DevOps,O=MyCompany,L=Shanghai,ST=Shanghai,C=CN\u0026quot;  security.nodes_dn:\n \u0026quot;CN=node-*,OU=Infra,O=MyCompany,L=Shanghai,ST=Shanghai,C=CN\u0026quot;  # ========== 其他安全配置 ========== security.allow_default_init_securityindex: true security.restapi.roles_enabled: [\u0026quot;superuser\u0026quot;, \u0026quot;security_rest_api_access\u0026quot;] security.system_indices.enabled: true security.system_indices.indices: [\u0026quot;.infini-*\u0026quot;] 3. 使用 OpenSSL 生成证书 #\n # 生成 CA openssl genrsa -out root-ca-key.pem 2048 openssl req -new -x509 -sha256 -key root-ca-key.pem -out root-ca.pem -days 3650 \\  -subj \u0026#34;/C=CN/ST=Shanghai/L=Shanghai/O=MyCompany/OU=Infra/CN=RootCA\u0026#34; # 生成节点证书 openssl genrsa -out node-key-temp.pem 2048 openssl pkcs8 -inform PEM -outform PEM -in node-key-temp.pem -topk8 -nocrypt -out node-key.pem openssl req -new -key node-key.pem -out node.csr  -subj \u0026quot;/C=CN/ST=Shanghai/L=Shanghai/O=MyCompany/OU=Infra/CN=node-1\u0026quot; openssl x509 -req -in node.csr -CA root-ca.pem -CAkey root-ca-key.pem  -CAcreateserial -out node.pem -days 730 -sha256\n# 生成 admin 证书 openssl genrsa -out admin-key-temp.pem 2048 openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -out admin-key.pem openssl req -new -key admin-key.pem -out admin.csr  -subj \u0026quot;/C=CN/ST=Shanghai/L=Shanghai/O=MyCompany/OU=DevOps/CN=admin\u0026quot; openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem  -CAcreateserial -out admin.pem -days 730 -sha256 TLS 版本与密码套件 #\n 限制 TLS 版本 #  # 仅允许 TLS 1.2 和 1.3（禁用 1.0/1.1） security.ssl.http.enabled_protocols: - \u0026#34;TLSv1.2\u0026#34; - \u0026#34;TLSv1.3\u0026#34; security.ssl.transport.enabled_protocols:\n \u0026quot;TLSv1.2\u0026quot; \u0026quot;TLSv1.3\u0026quot; 限制密码套件 #   security.ssl.http.enabled_ciphers: - \u0026#34;TLS_AES_128_GCM_SHA256\u0026#34; - \u0026#34;TLS_AES_256_GCM_SHA384\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\u0026#34; security.ssl.transport.enabled_ciphers:\n \u0026quot;TLS_AES_128_GCM_SHA256\u0026quot; \u0026quot;TLS_AES_256_GCM_SHA384\u0026quot; \u0026quot;TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\u0026quot; \u0026quot;TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\u0026quot; \u0026quot;TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\u0026quot; \u0026quot;TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\u0026quot; 证书验证 #   # 验证证书内容 openssl x509 -in node.pem -text -noout # 验证证书链 openssl verify -CAfile root-ca.pem node.pem\n# 测试 HTTPS 连接 curl -v \u0026ndash;cacert root-ca.pem https://localhost:9200 证书续期 #\n 证书到期前需要续期，步骤：\n 使用同一 CA 签发新证书 替换各节点的 node.crt 和 node.key 逐节点滚动重启（先重启非 master 节点）   建议在监控中设置证书过期告警（提前 30 天）。\n 常见问题 #     问题 原因 解决方案     SSLHandshakeException 证书不受信任 检查 ca_file 是否包含正确的 CA   节点无法加入集群 Transport 证书 DN 不匹配 检查 nodes_dn 配置   curl 报证书错误 客户端不信任 CA 使用 -k 跳过或 --cacert 指定 CA    延伸阅读 #    国密配置  安全模块总览  YAML 安全配置  ","subcategory":null,"summary":"","tags":null,"title":"TLS 安全配置","url":"/easysearch/main/docs/deployment/advanced-config/tls-secure/"},{"category":null,"content":"SQL-JDBC 驱动 #  Easysearch SQL JDBC 驱动程序是一个独立的纯 Java 驱动，可将 JDBC 调用转换为 Easysearch SQL REST API 请求，适用于 BI 工具集成、报表生成和应用程序数据访问等场景。\n 驱动信息 #     属性 值     Driver 类名 com.easysearch.jdbc.EasysearchDriver   JDBC URL 前缀 jdbc:easysearch://   JAR 包 sql-jdbc-1.7.1.jar   最低 Java 版本 Java 8     安装 #  下载 #  从官网下载 JDBC 驱动 JAR：\nhttps://release.infinilabs.com/easysearch/archive/plugins/sql-jdbc-1.7.1.jar Maven 项目 #  将 JAR 安装到本地仓库后引用：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.easysearch\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sql-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;system\u0026lt;/scope\u0026gt; \u0026lt;systemPath\u0026gt;${project.basedir}/libs/sql-jdbc-1.7.1.jar\u0026lt;/systemPath\u0026gt; \u0026lt;/dependency\u0026gt; Gradle 项目 #  将 JAR 放入 libs/ 目录，在 build.gradle 中添加：\nplugins { id \u0026#39;java\u0026#39; } repositories { mavenCentral() flatDir { dirs 'libs' } }\ndependencies { implementation files('libs/sql-jdbc-1.7.1.jar') } \n连接 URL 格式 #  jdbc:easysearch://[scheme://]host[:port][/path][?param=value\u0026amp;...]    组成 说明 默认值     scheme 协议（http 或 https） http   host Easysearch 节点地址 localhost   port 端口号 9200   path 可选的请求路径前缀 —    示例：\njdbc:easysearch://localhost:9200 jdbc:easysearch://https://es-node.example.com:9210 jdbc:easysearch://https://localhost:9210?ssl=true\u0026amp;trustSelfSigned=true  连接属性 #  通过 Properties 对象或 URL 查询参数传递：\n基本属性 #     属性 说明 默认值     user 用户名 —   password 密码 —   fetchSize 每次获取的行数（启用游标分页） 0（不启用）    认证属性 #     属性 说明 默认值     auth 认证类型 BASIC（提供 user/password 时）    支持的 auth 值：\n   值 说明     NONE 无认证   BASIC HTTP Basic 认证（需提供 user/password）   AWS_SIGV4 AWS Signature V4 认证    SSL/TLS 属性 #     属性 说明 默认值     ssl 是否启用 SSL false   trustSelfSigned 是否信任自签名证书 false   hostnameVerification 是否验证主机名 true   trustStoreLocation 信任库文件路径（JKS/PKCS12） —   trustStorePassword 信任库密码 —   keyStoreLocation 客户端密钥库路径（双向 TLS） —   keyStorePassword 客户端密钥库密码 —     使用示例 #  基本连接与查询 #  import java.sql.*; import java.util.Properties; public class EasysearchJdbcExample { public static void main(String[] args) throws Exception { String url = \u0026quot;jdbc:easysearch://https://localhost:9210\u0026quot;;\n Properties props \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; Properties\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; props\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;put\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;user\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;admin\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; props\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;put\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;password\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;admin\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; props\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;put\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;ssl\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;true\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; props\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;put\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;trustStoreLocation\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;/path/to/truststore.jks\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; props\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;put\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;trustStorePassword\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;changeit\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; props\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;put\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;hostnameVerification\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;false\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;try\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Connection conn \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; DriverManager\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;getConnection\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;url\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; props\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; Statement stmt \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; conn\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;createStatement\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; ResultSet rs \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; stmt\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;executeQuery\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;SELECT firstname, lastname, age FROM accounts WHERE age \u0026amp;gt; 30 ORDER BY age\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;while\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;rs\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;next\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;printf\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Name: %s %s, Age: %d%n\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; rs\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;getString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;firstname\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;),\u0026lt;/span\u0026gt; rs\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;getString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;lastname\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;),\u0026lt;/span\u0026gt; rs\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;getInt\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;age\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt;  } 使用 PreparedStatement #\n String sql = \u0026#34;SELECT * FROM accounts WHERE age \u0026gt; ? AND gender = ?\u0026#34;; try (PreparedStatement pstmt = conn.prepareStatement(sql)) { pstmt.setInt(1, 30); pstmt.setString(2, \u0026#34;M\u0026#34;); try (ResultSet rs = pstmt.executeQuery()) { while (rs.next()) { System.out.println(rs.getString(\u0026#34;firstname\u0026#34;)); } } } 游标分页（大结果集） #  通过 fetchSize 启用游标分页，避免一次加载全部结果：\nProperties props = new Properties(); props.put(\u0026#34;user\u0026#34;, \u0026#34;admin\u0026#34;); props.put(\u0026#34;password\u0026#34;, \u0026#34;admin\u0026#34;); props.put(\u0026#34;fetchSize\u0026#34;, \u0026#34;500\u0026#34;); // 每批获取 500 行  try (Connection conn = DriverManager.getConnection(url, props); Statement stmt = conn.createStatement(); ResultSet rs = stmt.executeQuery(\u0026#34;SELECT * FROM large_index\u0026#34;)) { while (rs.next()) { // 自动分批获取  } } 获取结果集元数据 #  ResultSetMetaData meta = rs.getMetaData(); int columnCount = meta.getColumnCount(); for (int i = 1; i \u0026lt;= columnCount; i++) { System.out.printf(\u0026#34;Column %d: %s (%s)%n\u0026#34;, i, meta.getColumnName(i), meta.getColumnTypeName(i)); }  自签名证书配置 #  在开发环境中使用自签名证书时，有两种方式：\n方式一：信任自签名证书（简便） #  props.put(\u0026#34;ssl\u0026#34;, \u0026#34;true\u0026#34;); props.put(\u0026#34;trustSelfSigned\u0026#34;, \u0026#34;true\u0026#34;); props.put(\u0026#34;hostnameVerification\u0026#34;, \u0026#34;false\u0026#34;); 方式二：使用信任库（推荐用于生产环境） #  先将证书导入 JKS 信任库：\nkeytool -importcert -alias easysearch \\  -file /path/to/server-cert.pem \\  -keystore /path/to/truststore.jks \\  -storepass changeit -noprompt 然后配置：\nprops.put(\u0026#34;ssl\u0026#34;, \u0026#34;true\u0026#34;); props.put(\u0026#34;trustStoreLocation\u0026#34;, \u0026#34;/path/to/truststore.jks\u0026#34;); props.put(\u0026#34;trustStorePassword\u0026#34;, \u0026#34;changeit\u0026#34;);  注意事项 #   JDBC 驱动通过 HTTP/HTTPS 与 Easysearch 的 /_sql REST API 通信 默认使用 jdbc 输出格式，返回结构化的 schema + datarows 设置 fetchSize \u0026gt; 0 会启用 Scroll API 进行游标分页 驱动不完全符合 JDBC 规范，部分高级 JDBC 功能可能不可用   相关链接 #    SQL 查询总览  SQL 客户端集成  ","subcategory":null,"summary":"","tags":null,"title":"SQL-JDBC","url":"/easysearch/main/docs/features/sql/sql-jdbc/"},{"category":null,"content":"Grafana 集成 #  Grafana 是主流的开源可观测平台，通过内置的 Elasticsearch 数据源插件可以直接连接 Easysearch，构建实时监控看板和数据分析面板。\n相关指南 #    Superset 集成  监控告警  前置条件 #     条件 说明     Grafana 版本 8.0+ 推荐（内置 Elasticsearch 数据源）   网络可达 Grafana 服务器能够访问 Easysearch 的 9200 端口   认证信息 Easysearch 用户名和密码    配置步骤 #  1. 添加数据源 #   进入 Grafana → Configuration → Data Sources → Add data source 搜索并选择 Elasticsearch 填写连接信息：     配置项 值     URL https://easysearch-host:9200   Access Server（推荐）   Basic Auth 开启   User / Password Easysearch 用户名 / 密码   Skip TLS Verify 开发环境可开启；生产环境配置 CA 证书   Version 选择 7.10+   Index name 要查询的索引模式，如 logs-*   Time field name 时间字段，如 @timestamp   Max concurrent Shard Requests 建议 5    2. 测试连接 #  点击 Save \u0026amp; Test，看到绿色提示 \u0026ldquo;Index OK. Time field name OK.\u0026rdquo; 即表示连接成功。\n常用面板类型 #     面板类型 适用场景 对应查询能力     Time Series 日志趋势、请求量、延迟监控 date_histogram 聚合   Bar Chart 分类统计、Top N 分析 terms 聚合   Stat / Gauge KPI 指标、集群健康度 sum / avg / count   Table 明细数据查看、日志检索 原始文档查询   Logs 日志浏览与上下文查看 日志查询 + 高亮   Heatmap 时间×维度交叉分析 多级聚合    告警配置 #  Grafana 8+ 支持统一告警（Unified Alerting），可以基于 Easysearch 数据源的查询结果配置告警规则：\n 在面板编辑页点击 Alert 标签 设置告警条件（如：5 分钟内错误日志数 \u0026gt; 100） 配置通知渠道（邮件、钉钉、Slack、Webhook 等）  性能建议 #   时间范围：避免查询过大的时间跨度，建议默认看板时间范围不超过 24 小时 最小间隔：date_histogram 的 min_interval 根据时间跨度合理设置（如 1m、5m） 专用账户：为 Grafana 创建只读的专用账户，限制可访问的索引范围 查询缓存：对于不频繁变化的数据，适当增大面板的刷新间隔  ","subcategory":null,"summary":"","tags":null,"title":"Grafana 集成","url":"/easysearch/main/docs/integrations/third-party/grafana/"},{"category":null,"content":"Geo Bounding Box 查询 #  地理边界框查询返回 geo_point 字段值位于指定矩形边界框内的文档。如果一个文档包含多个地理坐标点，只要至少有一个点位于边界框内，该文档就匹配。\n参考样例 #  您可以使用地理边界框查询来搜索包含地理点的文档。\n创建一个映射，将 point 字段映射为 geo_point ：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; } } } } 以经纬度作为对象索引三个地理点：\nPUT testindex1/_doc/1 { \u0026#34;point\u0026#34;: { \u0026#34;lat\u0026#34;: 74.00, \u0026#34;lon\u0026#34;: 40.71 } } PUT testindex1/_doc/2 { \u0026quot;point\u0026quot;: { \u0026quot;lat\u0026quot;: 72.64, \u0026quot;lon\u0026quot;: 22.62 } }\nPUT testindex1/_doc/3 { \u0026quot;point\u0026quot;: { \u0026quot;lat\u0026quot;: 75.00, \u0026quot;lon\u0026quot;: 28.00 } } 搜索所有文档，并筛选出查询中定义的矩形内的点所在的文档：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;filter\u0026#34;: { \u0026#34;geo_bounding_box\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;top_left\u0026#34;: { \u0026#34;lat\u0026#34;: 75, \u0026#34;lon\u0026#34;: 28 }, \u0026#34;bottom_right\u0026#34;: { \u0026#34;lat\u0026#34;: 73, \u0026#34;lon\u0026#34;: 41 } } } } } } } 返回内容包含匹配的文档：\n{ \u0026#34;took\u0026#34; : 20, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;point\u0026#34; : { \u0026#34;lat\u0026#34; : 74.0, \u0026#34;lon\u0026#34; : 40.71 } } } ] } }  前一个响应未包含具有地理点 \u0026ldquo;lat\u0026rdquo;: 75.00, \u0026ldquo;lon\u0026rdquo;: 28.00 的文档，因为地理点的精度有限。\n 关于精度 #  地理点坐标在索引时总是向下取整。在查询时，边界框的上边界向下取整，下边界向上取整。因此，位于边界框下和左边缘的地理点文档可能因舍入误差而未包含在结果中。另一方面，即使它们位于边界之外，位于边界框上和右边缘的地理点也可能包含在结果中。纬度的舍入误差小于 4.20 × 10^−8 度，经度的舍入误差小于 8.39 × 10^−8 度（大约 1 厘米）。\n指定边界框 #  您可以通过提供以下任何一种顶点坐标组合来指定边界框：\n top_right 和 bottom_left top_right 和 bottom_left top , left , bottom , 和 right  以下示例展示了如何使用 top 、 left 、 bottom 和 right 坐标指定边界框：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;filter\u0026#34;: { \u0026#34;geo_bounding_box\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;top\u0026#34;: 75, \u0026#34;left\u0026#34;: 28, \u0026#34;bottom\u0026#34;: 73, \u0026#34;right\u0026#34;: 41 } } } } } } 参数说明 #  地理边界框查询接受以下参数。\n   参数 数据类型 描述     _name String 过滤器名称。可选。   validation_method String 验证方法。有效值有 IGNORE_MALFORMED （接受无效坐标的地理点）、 COERCE （尝试将坐标强制转换为有效值）和 STRICT （当坐标无效时返回错误）。默认为 STRICT 。   type String 指定如何执行过滤器。有效值有 indexed （索引过滤器）和 memory （在内存中执行过滤器）。默认为 memory 。   ignore_unmapped Boolean 指定是否忽略未映射的字段。如果设置为 true ，则查询不会返回任何包含未映射字段的文档。如果设置为 false ，则在字段未映射时抛出异常。默认为 false 。   boost Float 查询的相关性得分权重。默认为 1.0。    接受格式 #  您可以使用地理点字段类型接受的任何格式指定边界框顶点的坐标。\n使用地理哈希指定边界框 #  如果您使用地理哈希来指定边界框，地理哈希被视为一个矩形。边界框的左上顶点对应于 top_left 地理哈希的左上顶点，边界框的右下顶点对应于 bottom_right 地理哈希的右下顶点。\n以下示例展示了如何使用地理哈希来指定与前面示例相同的边界框：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;filter\u0026#34;: { \u0026#34;geo_bounding_box\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;top_left\u0026#34;: \u0026#34;ut7ftjkfxm34\u0026#34;, \u0026#34;bottom_right\u0026#34;: \u0026#34;uuvpkcprc4rc\u0026#34; } } } } } } 要指定一个覆盖整个地理哈希区域的边界框，请将该地理哈希作为边界框的 top_left 和 bottom_right 参数提供：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;filter\u0026#34;: { \u0026#34;geo_bounding_box\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;top_left\u0026#34;: \u0026#34;ut\u0026#34;, \u0026#34;bottom_right\u0026#34;: \u0026#34;ut\u0026#34; } } } } } } ","subcategory":null,"summary":"","tags":null,"title":"Geo Bounding Box 查询","url":"/easysearch/main/docs/features/geo-search/geo-bounding-box/"},{"category":null,"content":"Docker 环境下使用 Easysearch #  在使用 Docker 运行 Easysearch 之前，请确保已进行 系统调优并安装好 Docker 服务，且 Docker 服务正常运行。\n 最快方式：启动临时的 docker 容器，可以从前台查看到 admin 随机生成的初始密码\n 注： Docker 环境一般用于临时验证，如需要长期使用请务必进行数据持久化   # 直接运行镜像使用随机密码（数据及配置未持久化） docker run --name easysearch --ulimit memlock=-1:-1 -p 9200:9200 infinilabs/easysearch:2.0.3-2534 # 使用自定义密码，可以使用环境变量配置 （需要 1.8.2 及以后的版本才支持） echo \u0026quot;EASYSEARCH_INITIAL_ADMIN_PASSWORD=$(openssl rand -hex 10)\u0026quot; | tee .env\n# 通过从环境变量文件设置初始密码（数据及配置未持久化） docker run \u0026ndash;name easysearch \u0026ndash;env-file ./.env \u0026ndash;ulimit memlock=-1:-1 -p 9200:9200 infinilabs/easysearch:2.0.3-2534\n# 使用自定义密码及命名卷 (数据持久化到命名卷) docker run -d \u0026ndash;name easysearch  \u0026ndash;ulimit memlock=-1:-1  \u0026ndash;env-file ./.env -p 9200:9200  -v easysearch-data:/app/easysearch/data  -v easysearch-config:/app/easysearch/config  -v easysearch-logs:/app/easysearch/logs  infinilabs/easysearch:2.0.3-2534 数据持久化到本地（数据可长期使用） #\n 设置自定义密码，并从宿主机挂载数据目录、配置目录及日志目录，配置 jvm 内存为 512m。\n 在宿主机上创建目录  # 创建数据及日志存储目录 sudo mkdir -p /data/easysearch/{data,logs} # 根据自己的需求，设置成安全的密码 echo \u0026#34;EASYSEARCH_INITIAL_ADMIN_PASSWORD=$(openssl rand -hex 10)\u0026#34; | sudo tee /data/easysearch/.env 修改目录权限   非必须步骤，视具体操作环境而定\n # 注意：需要根据 Docker 运行环境判断是否需要调整权限，如在 MacOS 上使用 OrbStack 则不需要调整权限。 # 容器内 easysearch 用户的 uid 为 602，通过调整宿主机的目录权限，确保在容器内部 easysearch 用户有权限读写挂载的数据卷 sudo chown -R 602:602 /data/easysearch 从镜像初始化 config 目录  # 将镜像中的 config 目录复制到本地目录 docker run --rm --env-file /data/easysearch/.env -v /data/easysearch:/work infinilabs/easysearch:2.0.3-2534 cp -rf /app/easysearch/config /work 后台运行容器  #后台启动容器，并指定内存大小及挂载数据、日志目录，设定好容器名称及容器主机名称 #如需调整配置文件，可以修改以下配置文件 # 1. /data/easysearch/config/easysearch.yml # 2. /data/easysearch/config/jvm.options # 3. /data/easysearch/config/log4j2.properties docker run -d --restart always -p 9200:9200 \\  --ulimit memlock=-1:-1 \\  -e ES_JAVA_OPTS=\u0026#34;-Xms512m -Xmx512m\u0026#34; \\  -v /data/easysearch/data:/app/easysearch/data \\  -v /data/easysearch/config:/app/easysearch/config \\  -v /data/easysearch/logs:/app/easysearch/logs \\  --name easysearch --hostname easysearch \\  infinilabs/easysearch:2.0.3-2534 升级 Easysearch 版本  # 先停止并删除正在运行的容器 docker stop easysearch \u0026amp;\u0026amp; docker rm easysearch # 再修改第 4 步中镜像的版本，重新运行命令即可 验证安装 #  # 查看容器状态 docker ps --filter name=easysearch # 查看容器日志（首次运行时可从日志中找到 admin 随机密码） docker logs easysearch\n# 测试连接 curl -ku admin:YOUR_PASSWORD https://localhost:9200 健康检查 #\n Docker 运行时可以配置健康检查，自动监测 Easysearch 是否正常运行：\ndocker run -d --name easysearch \\  --ulimit memlock=-1:-1 \\  --env-file ./.env -p 9200:9200 \\  -e ES_JAVA_OPTS=\u0026#34;-Xms512m -Xmx512m\u0026#34; \\  --health-cmd=\u0026#39;curl -sku admin:${EASYSEARCH_INITIAL_ADMIN_PASSWORD} https://localhost:9200/_cluster/health || exit 1\u0026#39; \\  --health-interval=30s \\  --health-timeout=10s \\  --health-retries=3 \\  --health-start-period=60s \\  -v easysearch-data:/app/easysearch/data \\  -v easysearch-config:/app/easysearch/config \\  -v easysearch-logs:/app/easysearch/logs \\  infinilabs/easysearch:2.0.3-2534 查看健康状态：\ndocker inspect --format=\u0026#39;{{.State.Health.Status}}\u0026#39; easysearch 常见问题 #  vm.max_map_count 错误 #  # 临时生效 sudo sysctl -w vm.max_map_count=262144 # 永久生效 echo \u0026quot;vm.max_map_count=262144\u0026quot; | sudo tee -a /etc/sysctl.conf \n更多系统参数请参考 系统调优。\n 容器启动后立即退出 #  # 查看退出原因 docker logs easysearch # 常见原因： # 1. 内存不足 — 调小 ES_JAVA_OPTS # 2. 权限问题 — 调整挂载目录权限为 602:602 # 3. 端口冲突 — 更换映射端口 查看容器内文件 #\n # 进入容器 docker exec -it easysearch bash # 容器内 Easysearch 路径 ls /app/easysearch/ 延伸阅读 #\n   Docker Compose 集群  测试环境部署  系统调优  ","subcategory":null,"summary":"","tags":null,"title":"Docker","url":"/easysearch/main/docs/deployment/install-guide/docker/"},{"category":null,"content":"ST Convert 分词器 #  stconvert 分词器用于中文简繁体转换，可将简体中文转换为繁体中文，或将繁体中文转换为简体中文。\n需要安装 analysis-stconvert 插件。\n参数 #     参数 说明 默认值     convert_type 转换类型：s2t（简→繁）或 t2s（繁→简） s2t   keep_both 是否同时保留转换前后的词元 false   delimiter 用于分隔转换结果的分隔符 ,    示例 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_stconvert\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stconvert\u0026#34;, \u0026#34;convert_type\u0026#34;: \u0026#34;s2t\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;my_stconvert\u0026#34; } } } } } 相关指南 #    ST Convert 分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"简繁体转换分词器（ST Convert）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/stconvert/"},{"category":null,"content":"HanLP Speed 分词器 #  hanlp_speed 分词器使用 HanLP 极速分词模式，牺牲一定精度换取更快的分词速度，适合对延迟敏感的在线搜索场景。\n需要安装 analysis-hanlp 插件。\n示例 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_speed\u0026#34; } } } } } 相关指南 #    HanLP 分词器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"HanLP 极速分词器（HanLP Speed）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-speed/"},{"category":null,"content":"索引统计与监控 #  Easysearch 提供了一组 API，用于查看索引的运行状态、性能指标和存储详情。这些信息是性能调优和故障排查的基础。\n索引统计（Index Stats） #  获取索引级别的统计信息，包括文档数、存储大小、写入/查询/合并等操作的计数和耗时。\n// 获取特定索引的所有统计 GET /my-index/_stats // 获取所有索引的统计 GET /_stats\n// 只获取特定指标 GET /my-index/_stats/docs,store\n// 多索引 GET /index-1,index-2/_stats/indexing,search 可用指标 #\n    指标 说明     docs 文档数和已删除文档数   store 索引存储大小   indexing 写入操作统计（总数、耗时、当前进行中等）   get Get 操作统计   search 搜索操作统计（query、fetch 阶段）   merge 段合并统计（次数、耗时、大小）   refresh Refresh 统计   flush Flush 统计   query_cache 查询缓存命中率和大小   fielddata Fielddata 内存使用   completion Completion Suggester 内存使用   segments 段数量、内存占用   translog Translog 大小和操作数   request_cache 请求缓存命中率   recovery 恢复操作统计   warmer Warmer 统计   _all 所有指标（默认）    查询参数 #     参数 类型 默认值 说明     level String indices 聚合级别：cluster、indices、shards   fields String — 逗号分隔的字段名（用于 completion 和 fielddata 指标）   completion_fields String — Completion 字段名   fielddata_fields String — Fielddata 字段名   include_segment_file_sizes Boolean false 在 segments 指标中包含文件大小详情   include_unloaded_segments Boolean false 包含未加载的段信息   forbid_closed_indices Boolean true 禁止查询已关闭索引的统计   expand_wildcards String open 通配符展开策略   ignore_unavailable Boolean false 忽略不存在的索引    响应示例（节选） #  { \u0026#34;_all\u0026#34;: { \u0026#34;primaries\u0026#34;: { \u0026#34;docs\u0026#34;: { \u0026#34;count\u0026#34;: 1500000, \u0026#34;deleted\u0026#34;: 2500 }, \u0026#34;store\u0026#34;: { \u0026#34;size_in_bytes\u0026#34;: 1073741824 }, \u0026#34;indexing\u0026#34;: { \u0026#34;index_total\u0026#34;: 1500000, \u0026#34;index_time_in_millis\u0026#34;: 45000, \u0026#34;index_current\u0026#34;: 0 }, \u0026#34;search\u0026#34;: { \u0026#34;query_total\u0026#34;: 350000, \u0026#34;query_time_in_millis\u0026#34;: 12000, \u0026#34;query_current\u0026#34;: 2 }, \u0026#34;segments\u0026#34;: { \u0026#34;count\u0026#34;: 42, \u0026#34;memory_in_bytes\u0026#34;: 52428800 } } } } 常见用法 #  // 查看段数量（判断是否需要 force merge） GET /my-index/_stats/segments // 查看写入速率 GET /my-index/_stats/indexing\n// 查看查询缓存命中率 GET /my-index/_stats/query_cache,request_cache\n// 按分片级别查看统计 GET /my-index/_stats?level=shards 索引段信息（Segments） #\n 查看索引中每个分片的段（segment）详情，包括段名、文档数、大小、是否已合并等。\n// 特定索引 GET /my-index/_segments // 所有索引 GET /_segments 查询参数 #\n    参数 类型 默认值 说明     verbose Boolean false 显示详细的段信息   expand_wildcards String open 通配符展开策略   ignore_unavailable Boolean false 忽略不存在的索引   allow_no_indices Boolean true 通配符未匹配时是否报错    响应示例（节选） #  { \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;indices\u0026#34;: { \u0026#34;my-index\u0026#34;: { \u0026#34;shards\u0026#34;: { \u0026#34;0\u0026#34;: [ { \u0026#34;routing\u0026#34;: { \u0026#34;state\u0026#34;: \u0026#34;STARTED\u0026#34;, \u0026#34;primary\u0026#34;: true, \u0026#34;node\u0026#34;: \u0026#34;node-1\u0026#34; }, \u0026#34;num_committed_segments\u0026#34;: 5, \u0026#34;num_search_segments\u0026#34;: 5, \u0026#34;segments\u0026#34;: { \u0026#34;_0\u0026#34;: { \u0026#34;generation\u0026#34;: 0, \u0026#34;num_docs\u0026#34;: 100000, \u0026#34;deleted_docs\u0026#34;: 0, \u0026#34;size_in_bytes\u0026#34;: 52428800, \u0026#34;committed\u0026#34;: true, \u0026#34;search\u0026#34;: true, \u0026#34;version\u0026#34;: \u0026#34;8.11.0\u0026#34;, \u0026#34;compound\u0026#34;: true } } } ] } } } } 段信息的关键字段：\n   字段 说明     num_docs 段中的文档数（不含已删除）   deleted_docs 已删除但未合并清理的文档数   size_in_bytes 段文件大小   committed 是否已提交到磁盘   compound 是否使用复合文件格式    恢复信息（Recovery） #  查看索引分片的恢复进度，包括初始恢复、副本复制和迁移。\n// 特定索引 GET /my-index/_recovery // 所有索引 GET /_recovery\n// 仅显示进行中的恢复 GET /my-index/_recovery?active_only=true 查询参数 #\n    参数 类型 默认值 说明     detailed Boolean false 显示详细恢复信息（包含文件列表）   active_only Boolean false 仅显示正在进行中的恢复   expand_wildcards String open 通配符展开策略   ignore_unavailable Boolean false 忽略不存在的索引    恢复类型 #     类型 说明     STORE 从本地磁盘恢复（节点重启）   SNAPSHOT 从快照恢复   REPLICA 从主分片复制到副本   RELOCATING 分片迁移到另一个节点    响应示例（节选） #  { \u0026#34;my-index\u0026#34;: { \u0026#34;shards\u0026#34;: [ { \u0026#34;id\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;STORE\u0026#34;, \u0026#34;stage\u0026#34;: \u0026#34;DONE\u0026#34;, \u0026#34;primary\u0026#34;: true, \u0026#34;start_time_in_millis\u0026#34;: 1609459200000, \u0026#34;stop_time_in_millis\u0026#34;: 1609459205000, \u0026#34;total_time_in_millis\u0026#34;: 5000, \u0026#34;source\u0026#34;: {}, \u0026#34;target\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;node-1-id\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;192.168.1.1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;node-1\u0026#34; }, \u0026#34;index\u0026#34;: { \u0026#34;size\u0026#34;: { \u0026#34;total_in_bytes\u0026#34;: 1073741824, \u0026#34;recovered_in_bytes\u0026#34;: 1073741824, \u0026#34;percent\u0026#34;: \u0026#34;100.0%\u0026#34; }, \u0026#34;files\u0026#34;: { \u0026#34;total\u0026#34;: 42, \u0026#34;recovered\u0026#34;: 42, \u0026#34;percent\u0026#34;: \u0026#34;100.0%\u0026#34; } }, \u0026#34;translog\u0026#34;: { \u0026#34;recovered\u0026#34;: 0, \u0026#34;total\u0026#34;: 0, \u0026#34;percent\u0026#34;: \u0026#34;100.0%\u0026#34;, \u0026#34;total_on_start\u0026#34;: 0 } } ] } } 分片存储状态（Shard Stores） #  查看每个分片副本的存储状态和分配情况，用于诊断未分配分片的问题。\n// 特定索引 GET /my-index/_shard_stores // 所有索引 GET /_shard_stores\n// 只查看未分配的分片存储状态 GET /_shard_stores?status=red 使用 Cat API 快速查看 #\n 对于日常巡检，Cat API 提供更简洁的表格输出：\n// 索引列表与大小 GET _cat/indices?v\u0026amp;s=store.size:desc // 特定索引的分片分布 GET _cat/shards/my-index?v\n// 段信息 GET _cat/segments/my-index?v\n// 恢复进度 GET _cat/recovery/my-index?v\u0026amp;active_only=true 监控检查清单 #\n    检查项 API 关注指标     段数量是否过多 _stats/segments segments.count，只读索引建议合并到 1   已删除文档占比 _stats/docs deleted / (count + deleted)，过高应 force merge   查询缓存命中率 _stats/query_cache hit_count / (hit_count + miss_count)   写入速率 _stats/indexing index_total 增量 / 时间窗口   恢复是否完成 _recovery?active_only=true 无结果 = 恢复完成   磁盘空间 _cat/indices?v\u0026amp;h=index,store.size 定期检查增长趋势    下一步 #    Refresh、Flush 与 Force Merge：段维护操作  索引设置：合并策略、刷新间隔等调优参数  监控：集群级别的监控  故障排查：常见问题诊断  ","subcategory":null,"summary":"","tags":null,"title":"索引统计与监控","url":"/easysearch/main/docs/operations/data-management/index-stats/"},{"category":null,"content":"HanLP Dijkstra 分词器 #  hanlp_dijkstra 分词器使用 HanLP Dijkstra 最短路径分词算法，基于词典的全切分方式。\n需要安装 analysis-hanlp 插件。\n示例 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_dijkstra\u0026#34; } } } } } 相关指南 #    HanLP 分词器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"HanLP 最短路分词器（HanLP Dijkstra）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-dijkstra/"},{"category":null,"content":"重新索引数据 #  相关指南 #    索引管理：创建、删除与重建索引  别名（Aliases）  创建索引后，如果您需要进行广泛的更改，例如为每个文档添加一个新字段或合并多个索引以形成一个新的索引，而不是删除索引，使更改脱机，然后重新索引数据，则可以使用 reindex 操作。\n使用 reindex 操作，可以将通过查询选择的所有文档或文档子集复制到另一个索引。重新索引是一个 POST 操作。在最基本的形式中，指定源索引和目标索引。\n 重新编制索引可能是一项昂贵的操作，具体取决于源索引的大小。我们建议您通过将 number_of_replicas 设置为 0 来禁用目标索引中的副本，并在重新索引过程完成后重新启用它们。\n 重新索引所有文档 #  您可以将所有文档从一个索引复制到另一个索引。\n首先需要使用所需的字段映射和设置创建目标索引，或者可以从源索引中复制这些映射和设置：\nPUT destination { \u0026#34;mappings\u0026#34;:{ \u0026#34;Add in your desired mappings\u0026#34; }, \u0026#34;settings\u0026#34;:{ \u0026#34;Add in your desired settings\u0026#34; } } reindex 命令将所有文档从源索引复制到目标索引：\nPOST _reindex { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34; }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 如果尚未创建目标索引，则 reindex 操作将使用默认配置创建新的目标索引。\n从远程群集 reindex #  您可以从远程集群中的索引复制文档。使用 remote 选项指定远程主机名和所需的登录凭据。\n此命令会到达远程集群，使用用户名和密码登录，并将所有文档从该远程集群中的源索引复制到本地集群中的目标索引：\nPOST _reindex { \u0026#34;source\u0026#34;:{ \u0026#34;remote\u0026#34;:{ \u0026#34;host\u0026#34;:\u0026#34;https://\u0026lt;REST_endpoint_of_remote_cluster\u0026gt;:9200\u0026#34;, \u0026#34;username\u0026#34;:\u0026#34;YOUR_USERNAME\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;YOUR_PASSWORD\u0026#34; } }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 您可以指定以下选项：\n   选项 有效值 描述 必填     host String 远程集群的 REST 端点 Yes   username String 登录到远程集群的用户名 No   password String 登录到远程群集的密码 No   socket_timeout Time Unit 套接字读取的等待时间（默认为 30 秒） No   connect_timeout Time Unit 远程连接超时的等待时间（默认为 30 秒） No    重新索引文档子集 #  只能复制与搜索查询匹配的特定文档集。\n此命令仅将查询操作匹配的文档子集复制到目标索引：\nPOST _reindex { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;field_name\u0026#34;: \u0026#34;text\u0026#34; } } }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 有关所有查询操作的列表，请参见 全文查询。\n合并一个或多个索引 #  通过将源索引添加为列表，可以组合一个或多个索引中的文档。\n此命令将所有文档从两个源索引复制到一个目标索引：\nPOST _reindex { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:[ \u0026#34;source_1\u0026#34;, \u0026#34;source_2\u0026#34; ] }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 确保源索引和目标索引的碎片数量相同。\n仅重索引缺少的文档 #  通过将 op_type 选项设置为 create ，可以仅复制目标索引中缺少的文档。 在这种情况下，如果已经存在具有相同 ID 的文档，则操作将忽略源索引中的文档。 要忽略文档的所有版本冲突，请将 conflicts 选项设置为 proceed 。\nPOST _reindex { \u0026#34;conflicts\u0026#34;:\u0026#34;proceed\u0026#34;, \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34; }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34;, \u0026#34;op_type\u0026#34;:\u0026#34;create\u0026#34; } } 重新索引排序的文档 #  对文档中的特定字段进行排序后，可以复制某些文档。\n此命令基于 timestamp 字段复制最后 10 个文档：\nPOST _reindex { \u0026#34;size\u0026#34;:10, \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34;, \u0026#34;sort\u0026#34;:{ \u0026#34;timestamp\u0026#34;:\u0026#34;desc\u0026#34; } }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 重新索引期间转换文档 #  您可以使用 script 选项在重新索引过程中转换数据。 我们建议在 Easysearch 中编写脚本时使用 Painless。\n此命令通过 Painless 脚本运行源索引，该脚本在将 account 对象复制到目标索引之前增加 number 字段：\nPOST _reindex { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34; }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; }, \u0026#34;script\u0026#34;:{ \u0026#34;lang\u0026#34;:\u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;:\u0026#34;ctx._account.number++\u0026#34; } } 您还可以指定一个摄取管道，以在重新索引过程中转换数据。\n首先必须创建一个定义了 processors 的管道。您可以在 ingest 管道中使用许多不同的 processors。\n这是一个示例摄取管道，它定义了一个 split 处理器，该处理器基于 space 分隔符拆分 text 字段，并将其存储在新的 word 字段中。 script 处理器是一个无痛脚本，它查找 word 字段的长度并将其存储在新的 word_count 字段中。 remove 处理器删除 test 字段。\nPUT _ingest/pipeline/pipeline-test { \u0026#34;description\u0026#34;: \u0026#34;Splits the text field into a list. Computes the length of the \u0026#39;word\u0026#39; field and stores it in a new \u0026#39;word_count\u0026#39; field. Removes the \u0026#39;test\u0026#39; field.\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;split\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;separator\u0026#34;: \u0026#34;\\\\s+\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;word\u0026#34; }, } { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;ctx.word_count = ctx.word.length\u0026#34; } }, { \u0026#34;remove\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;test\u0026#34; } } ] } 创建管道后，可以使用 reindex 操作：\nPOST _reindex { \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;source\u0026#34;, }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;destination\u0026#34;, \u0026#34;pipeline\u0026#34;: \u0026#34;pipeline-test\u0026#34; } } 更新当前索引中的文档 #  要更新当前索引中的数据而不将其复制到其他索引，请使用 update_by_query 操作。\nupdate_by_query 操作是一次可以对单个索引执行的 POST 操作。\nPOST \u0026lt;index_name\u0026gt;/_update_by_query 如果在没有参数的情况下运行此命令，则会增加索引中所有文档的版本号。\n 异步执行 #  对于大量数据的重新索引，建议使用异步模式，避免长时间等待超时：\nPOST _reindex?wait_for_completion=false { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34; }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 异步模式立即返回一个 task_id，可通过 Task API 查询进度：\nGET _tasks/\u0026lt;task_id\u0026gt; 响应中包含任务状态、已处理的文档数和进度信息。\n 限流控制 #  使用 requests_per_second 参数限制重新索引的速度，避免对集群造成过大压力：\nPOST _reindex?requests_per_second=500 { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34; }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 设置为 -1 表示不限流（默认行为）。\n动态调整限流（Rethrottle） #  在重新索引任务执行过程中，可以动态调整限流速度：\nPOST _reindex/\u0026lt;task_id\u0026gt;/_rethrottle?requests_per_second=1000 设置为 -1 可取消限流。\n 切片并行 #  使用 slices 参数将重新索引操作切分为多个子任务并行执行，显著提升速度：\nPOST _reindex?slices=5 { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34; }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 也可以设置为 auto，让 Easysearch 自动根据源索引的分片数决定切片数量：\nPOST _reindex?slices=auto { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34; }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } }  建议：slices 数量不应超过源索引的分片数。auto 模式通常是最佳选择。\n  查询参数 #  _reindex API 支持以下查询参数：\n   参数 类型 描述 默认值     refresh boolean 完成后是否刷新受影响的索引 false   timeout time 每个批量请求等待不可用分片的时间 1m   wait_for_active_shards string 操作前需要活跃的分片副本数。设置为 all 表示所有副本 1   wait_for_completion boolean 是否等待操作完成再返回。false 时返回 task_id true   requests_per_second number 子请求限流速率。-1 表示不限流 -1   scroll time 搜索上下文的保持时间 5m   slices number/string 任务切片数。可设为数字或 auto 1   max_docs number 最大处理文档数 全部    源索引选项 #  可以为源索引指定以下选项：\n   选项 类型 描述 必填     index string/array 源索引名称，可提供多个作为列表 是   max_docs integer 要重新索引的最大文档数 否   query object 用于过滤文档的搜索查询 否   size integer 每批滚动的文档数 否   slice object 手动切片配置（id 和 max） 否   sort list 重新索引前的排序字段 否   remote object 远程集群连接信息 否   _source array 仅包含指定的源字段 否    目标索引选项 #  可以为目标索引指定以下选项：\n   选项 类型 描述 必填     index string 目标索引的名称 是   version_type enum 版本类型：internal、external、external_gt、external_gte 否   op_type string 设置为 create 可仅索引不存在的文档 否   pipeline string 摄取管道名称，在索引前对文档进行处理 否   routing string 路由值（keep、discard 或自定义值） 否     相关文档 #    索引管理：索引 CRUD 操作  数据生命周期：完整的数据保留策略  ","subcategory":null,"summary":"","tags":null,"title":"重建索引","url":"/easysearch/main/docs/operations/data-management/reindex/"},{"category":null,"content":"NLP 自然语言处理 #  搜索引擎的核心挑战是理解人类语言。本文介绍 NLP（Natural Language Processing）在 Easysearch 中的应用，从基础的分词到高级的向量语义搜索。\nNLP 在搜索中的角色 #  用户输入: \u0026#34;我想买一台便宜的笔记本电脑\u0026#34; ↓ NLP 处理层（分词、去停用词、同义词、向量化） ↓ Easysearch 查询执行 ↓ 返回相关结果（包括 \u0026#34;laptop\u0026#34;、\u0026#34;notebook\u0026#34;、\u0026#34;手提电脑\u0026#34; 等） Easysearch 中 NLP 的应用可以分为三个层次：\n   层次 技术 实现方式     词级处理 分词、词干提取、停用词 内置分析器   语言规则 同义词、拼音、模糊匹配 分析器插件   语义理解 向量检索、文本嵌入 AI 集成 / kNN    第一层：分词与文本分析 #  分词是 NLP 的基础——将连续文本切分为独立的词项（Token）。\n中文分词 #  中文没有空格分隔，需要专用分词器：\nPUT /my-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;ik_smart_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_smart\u0026#34; } } } } } 测试分词效果：\nPOST /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;INFINI Easysearch 是分布式搜索引擎\u0026#34; } 输出词项: [\u0026quot;infini\u0026quot;, \u0026quot;easysearch\u0026quot;, \u0026quot;是\u0026quot;, \u0026quot;分布式\u0026quot;, \u0026quot;搜索引擎\u0026quot;] Easysearch 内置的 IK 分析器支持两种粒度：\n   分词器 粒度 示例     ik_smart 粗粒度（最少切分） \u0026ldquo;中华人民共和国\u0026rdquo; → [\u0026ldquo;中华人民共和国\u0026rdquo;]   ik_max_word 细粒度（最多切分） \u0026ldquo;中华人民共和国\u0026rdquo; → [\u0026ldquo;中华人民共和国\u0026rdquo;, \u0026ldquo;中华人民\u0026rdquo;, \u0026ldquo;中华\u0026rdquo;, \u0026ldquo;人民共和国\u0026rdquo;, \u0026ldquo;人民\u0026rdquo;, \u0026ldquo;共和国\u0026rdquo;]     详见 文本分析。\n 词干提取（Stemming） #  将单词还原到词干形式，提高召回率：\nrunning → run searches → search better → better (不规则变化需要词典)  详见 词干化。\n 停用词 #  去除对搜索无意义的高频词（\u0026ldquo;的\u0026rdquo;、\u0026ldquo;是\u0026rdquo;、\u0026ldquo;the\u0026rdquo;、\u0026ldquo;a\u0026rdquo;）：\n{ \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_stop\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;stopwords\u0026#34;: [\u0026#34;的\u0026#34;, \u0026#34;了\u0026#34;, \u0026#34;在\u0026#34;, \u0026#34;是\u0026#34;, \u0026#34;我\u0026#34;] } } } }  详见 停用词。\n 第二层：语言增强 #  同义词 #  让 \u0026ldquo;手机\u0026rdquo;、\u0026ldquo;手提电话\u0026rdquo;、\u0026ldquo;mobile phone\u0026rdquo; 互相匹配：\n{ \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_synonyms\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;synonym\u0026#34;, \u0026#34;synonyms\u0026#34;: [ \u0026#34;手机,手提电话,mobile phone\u0026#34;, \u0026#34;笔记本,laptop,notebook\u0026#34; ] } } } }  详见 同义词。\n 拼音搜索 #  通过拼音插件，支持拼音首字母和全拼搜索：\n\u0026#34;刘德华\u0026#34; → 可通过 \u0026#34;ldh\u0026#34; 或 \u0026#34;liudehua\u0026#34; 搜索到 模糊匹配 #  容忍拼写错误：\n{ \u0026#34;query\u0026#34;: { \u0026#34;fuzzy\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;esysearch\u0026#34;, \u0026#34;fuzziness\u0026#34;: 1 } } } }  详见 模糊搜索。\n 第三层：语义搜索 #  传统关键词搜索依赖词项精确匹配，无法理解语义。语义搜索通过向量嵌入（Embedding）解决这个问题。\n基本流程 #  文本 → Embedding 模型 → 向量 [0.12, -0.34, 0.56, ...] ↓ 存入 Easysearch knn_dense_float_vector 字段 ↓ 使用 knn_nearest_neighbors 查询最近邻 向量字段定义 #  PUT /semantic-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;title_vector\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 768, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;L\u0026#34;: 99, \u0026#34;k\u0026#34;: 1 } } } } } kNN 搜索 #  GET /semantic-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;title_vector\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.34, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 50 } } } 混合搜索 #  将关键词搜索与向量搜索结合，兼顾精确性和语义理解：\nGET /my-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;title_vector\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 50 } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;搜索引擎\u0026#34; } } ] } } }  詳見 向量搜索 和 AI 集成。\n 选型建议 #     场景 推荐方案 复杂度     中文关键词搜索 IK 分词 + 同义词 低   多语言搜索 对应语言的分析器 低   拼写纠错 fuzzy + suggest 中   语义搜索 / 问答 Embedding + kNN 高   最佳效果 混合搜索（关键词 + 向量） 高    延伸阅读 #    文本分析  同义词  向量搜索  AI 集成  RAG 与 LLM  最佳实践 #    AI 搜索与向量检索架构：多路召回、Hybrid 搜索、LLM 集成  向量字段建模：向量维度选择、写入策略、成本控制  查询调优：NLP 查询的性能优化  ","subcategory":null,"summary":"","tags":null,"title":"NLP 自然语言处理","url":"/easysearch/main/docs/fundamentals/nlp/"},{"category":null,"content":"HanLP N-Short 分词器 #  hanlp_n_short 分词器使用 HanLP N 最短路径分词算法，能找到全局 N 条最短路径，适合需要高精度分词的场景。\n需要安装 analysis-hanlp 插件。\n示例 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_n_short\u0026#34; } } } } } 相关指南 #    HanLP 分词器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"HanLP N 最短路分词器（HanLP N-Short）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-n-short/"},{"category":null,"content":"聚合性能与内存 #  聚合的性能和内存使用是生产环境中需要重点关注的问题。本页介绍聚合背后的数据结构（doc_values 和 fielddata）、内存限制机制（断路器），以及如何优化聚合查询。\nDoc Values #  聚合使用一个叫 doc values 的数据结构。Doc values 可以使聚合更快、更高效并且内存友好。\nDoc values 的存在是因为倒排索引只对某些操作是高效的。倒排索引的优势在于查找包含某个项的文档，而对于从另外一个方向的相反操作并不高效，即：确定哪些项是否存在单个文档里，聚合需要这种次级的访问模式。\n倒排索引 vs Doc Values #  对于以下倒排索引：\nTerm Doc_1 Doc_2 Doc_3 ------------------------------------ brown | X | X | dog | X | | X dogs | | X | X fox | X | | X ... 如果我们想要获得所有包含 brown 的文档的词的完整列表，查询部分简单又高效。倒排索引是根据项来排序的，所以我们首先在词项列表中找到 brown，然后扫描所有列，找到包含 brown 的文档。\n然后，对于聚合部分，我们需要找到 Doc_1 和 Doc_2 里所有唯一的词项。用倒排索引做这件事情代价很高：我们会迭代索引里的每个词项并收集 Doc_1 和 Doc_2 列里面 token。这很慢而且难以扩展：随着词项和文档的数量增加，执行时间也会增加。\nDoc values 通过转置两者间的关系来解决这个问题。倒排索引将词项映射到包含它们的文档，doc values 将文档映射到它们包含的词项：\nDoc Terms ----------------------------------------------------------------- Doc_1 | brown, dog, fox, jumped, lazy, over, quick, the Doc_2 | brown, dogs, foxes, in, lazy, leap, over, quick, summer Doc_3 | dog, dogs, fox, jumped, over, quick, the ----------------------------------------------------------------- 当数据被转置之后，想要收集到 Doc_1 和 Doc_2 的唯一 token 会非常容易。获得每个文档行，获取所有的词项，然后求两个集合的并集。\n因此，搜索和聚合是相互紧密缠绕的。搜索使用倒排索引查找文档，聚合操作收集和聚合 doc values 里的数据。\n 注意：Doc values 不仅可以用于聚合。任何需要查找某个文档包含的值的操作都必须使用它。除了聚合，还包括排序、访问字段值的脚本、父子关系处理等。\n Fielddata #  对于 text 字段，doc values 不可用（因为文本字段需要分析）。相反，Easysearch 使用 fielddata 数据结构。Fielddata 在查询时动态构建，将分析后的文本加载到内存中。\nFielddata 的特点 #   延迟加载：如果你从来没有聚合一个分析字符串，就不会加载 fielddata 到内存中 基于字段加载：只有很活跃地使用字段才会增加 fielddata 的负担 加载所有文档：fielddata 会加载索引中（针对该特定字段的）所有的文档，而不管查询的特异性  与 doc values 不同，fielddata 结构不会在索引时创建。相反，它是在查询运行时，动态填充。这可能是一个比较复杂的操作，可能需要一些时间。将所有的信息一次加载，再将其维持在内存中的方式要比反复只加载一个 fielddata 的部分代价要低。\n限制 Fielddata 内存使用 #  JVM 堆是有限资源的，应该被合理利用。限制 fielddata 对堆使用的影响有多套机制：\nFielddata 大小限制 #  indices.fielddata.cache.size 控制为 fielddata 分配的堆空间大小。当你发起一个查询，分析字符串的聚合将会被加载到 fielddata，如果这些字符串之前没有被加载过。如果结果中 fielddata 大小超过了指定大小，其他的值将会被回收从而获得空间。\n默认情况下，设置都是 unbounded，Easysearch 永远都不会从 fielddata 中回收数据。\n这个默认设置是刻意选择的：fielddata 不是临时缓存。它是驻留内存里的数据结构，必须可以快速执行访问，而且构建它的代价十分高昂。如果每个请求都重载数据，性能会十分糟糕。\n一个有界的大小会强制数据结构回收数据。可以通过在配置文件中增加配置为 fielddata 设置一个上限：\nindices.fielddata.cache.size: 20% 可以设置堆大小的百分比，也可以是某个值，例如：5gb。\n有了这个设置，最久未使用（LRU）的 fielddata 会被回收为新数据腾出空间。\n 警告：这个设置是一个安全卫士，而非内存不足的解决方案。如果没有足够空间可以将 fielddata 保留在内存中，Easysearch 就会时刻从磁盘重载数据，并回收其他数据以获得更多空间。内存的回收机制会导致重度磁盘 I/O，并且在内存中生成很多垃圾，这些垃圾必须在晚些时候被回收掉。\n 断路器（Circuit Breaker） #  机敏的读者可能已经发现 fielddata 大小设置的一个问题。fielddata 大小是在数据加载之后检查的。如果一个查询试图加载比可用内存更多的信息到 fielddata 中会发生什么？答案很丑陋：我们会碰到 OutOfMemoryException。\nEasysearch 包括一个 fielddata 断路器，这个设计就是为了处理上述情况。断路器通过内部检查（字段的类型、基数、大小等等）来估算一个查询需要的内存。它然后检查要求加载的 fielddata 是否会导致 fielddata 的总量超过堆的配置比例。\n如果估算查询的大小超出限制，就会触发断路器，查询会被中止并返回异常。这都发生在数据加载之前，也就意味着不会引起 OutOfMemoryException。\n可用的断路器 #  Easysearch 有一系列的断路器，它们都能保证内存不会超出限制：\n indices.breaker.fielddata.limit：fielddata 断路器默认设置堆的 60% 作为 fielddata 大小的上限。 indices.breaker.request.limit：request 断路器估算需要完成其他请求部分的结构大小，例如创建一个聚合桶，默认限制是堆内存的 40%。 indices.breaker.total.limit：total 揉合 request 和 fielddata 断路器保证两者组合起来不会使用超过堆内存的 70%。  断路器的限制可以在文件 config/easysearch.yml 中指定，可以动态更新一个正在运行的集群：\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34; : { \u0026#34;indices.breaker.fielddata.limit\u0026#34; : \u0026#34;40%\u0026#34; } } 最好为断路器设置一个相对保守点的值。记住 fielddata 需要与 request 断路器共享堆内存、索引缓冲内存和过滤器缓存。Lucene 的数据被用来构造索引，以及各种其他临时的数据结构。正因如此，它默认值非常保守，只有 60%。过于乐观的设置可能会引起潜在的堆栈溢出（OOM）异常，这会使整个节点宕掉。\n 提示：indices.fielddata.cache.size 和 indices.breaker.fielddata.limit 之间的关系非常重要。如果断路器的限制低于缓存大小，没有数据会被回收。为了能正常工作，断路器的限制必须要比缓存大小要高。\n 监控 Fielddata #  无论是仔细监控 fielddata 的内存使用情况，还是看有无数据被回收都十分重要。高的回收数可以预示严重的资源问题以及性能不佳的原因。\nFielddata 的使用可以被监控：\n  按索引使用 indices-stats API：\nGET /_stats/fielddata?fields=*   按节点使用 nodes-stats API：\nGET /_nodes/stats/indices/fielddata?fields=*   使用设置 ?fields=*，可以将内存使用分配到每个字段。\n深度优先 vs 广度优先 #  terms 桶基于我们的数据动态构建桶；它并不知道到底生成了多少桶。大多数时候对单个字段的聚合查询还是非常快的，但是当需要同时聚合多个字段时，就可能会产生大量的分组，最终结果就是占用 Easysearch 大量内存，从而导致 OOM 的情况发生。\n组合爆炸问题 #  假设我们现在有一些关于电影的数据集，每条数据里面会有一个数组类型的字段存储表演该电影的所有演员的名字。如果我们想要查询出演影片最多的十个演员以及与他们合作最多的演员，使用聚合是非常简单的：\n{ \u0026#34;aggs\u0026#34; : { \u0026#34;actors\u0026#34; : { \u0026#34;terms\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;actors\u0026#34;, \u0026#34;size\u0026#34; : 10 }, \u0026#34;aggs\u0026#34; : { \u0026#34;costars\u0026#34; : { \u0026#34;terms\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;actors\u0026#34;, \u0026#34;size\u0026#34; : 5 } } } } } } 这会返回前十位出演最多的演员，以及与他们合作最多的五位演员。这看起来是一个简单的聚合查询，最终只返回 50 条数据！\n但是，这个看上去简单的查询可以轻而易举地消耗大量内存。actors 聚合会构建树的第一层，每个演员都有一个桶。然后，内套在第一层的每个节点之下，costar 聚合会构建第二层，每个联合出演一个桶。这意味着每部影片会生成 n² 个桶！\n用真实点的数据，设想平均每部影片有 10 名演员，每部影片就会生成 10² = 100 个桶。如果总共有 20,000 部影片，粗率计算就会生成 2,000,000 个桶。\n现在，记住，聚合只是简单的希望得到前十位演员和与他们联合出演者，总共 50 条数据。为了得到最终的结果，我们创建了一个有 2,000,000 桶的树，然后对其排序，取 top10。\n深度优先（默认） #  我们之前展示的策略叫做深度优先，它是默认设置，先构建完整的树，然后修剪无用节点。深度优先的方式对于大多数聚合都能正常工作，但对于如我们演员和联合演员这样例子的情形就不太适用。\n广度优先 #  为了应对这些特殊的应用场景，我们应该使用另一种集合策略叫做广度优先。这种策略的工作方式有些不同，它先执行第一层聚合，再继续下一层聚合之前会先做修剪。\n在我们的示例中，actors 聚合会首先执行，在这个时候，我们的树只有一层，但我们已经知道了前 10 位的演员！这就没有必要保留其他的演员信息，因为它们无论如何都不会出现在前十位中。\n因为我们已经知道了前十名演员，我们可以安全的修剪其他节点。修剪后，下一层是基于它的执行模式读入的，重复执行这个过程直到聚合完成。这种场景下，广度优先可以大幅度节省内存。\n要使用广度优先，只需简单的通过参数 collect_mode 开启：\n{ \u0026#34;aggs\u0026#34; : { \u0026#34;actors\u0026#34; : { \u0026#34;terms\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;actors\u0026#34;, \u0026#34;size\u0026#34; : 10, \u0026#34;collect_mode\u0026#34; : \u0026#34;breadth_first\u0026#34; }, \u0026#34;aggs\u0026#34; : { \u0026#34;costars\u0026#34; : { \u0026#34;terms\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;actors\u0026#34;, \u0026#34;size\u0026#34; : 5 } } } } } } 广度优先仅仅适用于每个组的聚合数量远远小于当前总组数的情况下，因为广度优先会在内存中缓存裁剪后的仅仅需要缓存的每个组的所有数据，以便于它的子聚合分组查询可以复用上级聚合的数据。\n广度优先的内存使用情况与裁剪后的缓存分组数据量是成线性的。对于很多聚合来说，每个桶内的文档数量是相当大的。想象一种按月分组的直方图，总组数肯定是固定的，因为每年只有 12 个月，这个时候每个月下的数据量可能非常大。这使广度优先不是一个好的选择，这也是为什么深度优先作为默认策略的原因。\n最佳实践 #   优先使用 doc_values：对于 keyword、数值、日期等字段，doc values 在索引时创建，查询时无需额外内存 避免在 text 字段上做聚合：如果需要对文本字段做聚合，考虑使用 multi-fields，在 keyword 子字段上做聚合 监控 fielddata 使用：定期检查 fielddata 的内存使用和回收情况 合理设置断路器：根据实际内存情况设置断路器限制，避免 OOM 使用广度优先模式：当嵌套聚合会产生大量桶时，考虑使用广度优先模式  小结 #   Doc values 是聚合的基础数据结构，在索引时创建，查询时高效 Fielddata 用于 text 字段的聚合，在查询时动态构建，需要消耗内存 断路器可以防止内存溢出，但应该设置保守的值 深度优先是默认策略，广度优先适合嵌套聚合产生大量桶的场景 监控 fielddata 使用情况，及时发现问题  下一步可以继续阅读：\n  桶聚合（Bucket）  指标聚合（Metrics）  管道聚合（Pipeline）  ","subcategory":null,"summary":"","tags":null,"title":"聚合性能与内存","url":"/easysearch/main/docs/features/aggregations/aggs-performance/"},{"category":null,"content":"生产环境硬件配置推荐 #  在生产环境部署 Easysearch 时，高可用性（HA）是必须满足的核心要求。为实现完整的 HA 保障，您至少需要部署 3 个节点组成 Easysearch 集群。为获得最佳运维体验，建议配合使用 INFINI Console 和 Gateway，它们提供集群监控、告警和运维管理等完整功能，可大幅提升日常运维效率。\n推荐配置 #     产品 CPU 内存 JVM 堆 磁盘 高可用实例数     Easysearch 16 核+ 64 GB+ 31 GB SSD ≥ 3   INFINI Console 8 核 16 GB — ≥ 50 GB (HDD/SSD) 1   INFINI Gateway 8 核 16 GB — ≥ 50 GB (HDD/SSD) ≥ 2     对于低负载集群或测试环境，可适当降低配置标准，但需确保满足基础性能需求。测试环境最低配置参见 测试环境部署。\n 内存 #  内存通常是 Easysearch 最重要的资源。充足的内存可以显著降低查询延迟并提高索引吞吐。\nJVM 堆内存 #   将 JVM 堆设置为物理内存的 50%，但不超过 31 GB。 超过 31 GB 后，JVM 将无法使用 压缩指针（Compressed Oops），实际可用堆反而减少。 始终保持 -Xms 和 -Xmx 相同，避免运行时堆调整带来的 GC 停顿。  # config/jvm.options 示例（64 GB 机器） -Xms31g -Xmx31g 操作系统文件缓存 #  剩余的 50% 内存留给操作系统。Lucene 的段文件依赖操作系统页缓存（Page Cache）来加速读取。如果把所有内存都给了 JVM，段读取将退化为磁盘 I/O，搜索性能会大幅下降。\n大内存机器策略 #  如果机器内存 ≥ 128 GB，可考虑在同一台机器上运行多个 Easysearch 实例（每个实例 31 GB 堆），配合 NUMA 绑定获得最佳性能。\nCPU #  Easysearch 对 CPU 的需求取决于查询复杂度和写入压力：\n 更多核心优于更高主频：Easysearch 的索引、搜索、段合并等操作高度并行化，多核心可以并行处理更多任务。 现代多核 CPU（如 16 核、32 核）完全可以满足大多数场景。 超高主频的单核处理器并不比普通主频的多核处理器更有优势。     节点角色 CPU 建议     Master（专用） 4 核即可，主要消耗在集群状态管理   Data 16 核+，索引和搜索都需要大量计算   Coordinating 8 核+，主要消耗在查询结果归并    磁盘 #  磁盘是搜索引擎性能的关键瓶颈之一。Easysearch 的索引写入、段合并、translog 刷写以及搜索读取都依赖磁盘 I/O。\nSSD vs. HDD #     指标 SATA SSD NVMe SSD HDD     随机读 IOPS ~90K ~500K+ ~200   随机写 IOPS ~50K ~200K+ ~200   适用场景 通用生产 高性能生产 仅冷数据/归档     生产环境强烈建议使用 SSD，NVMe 优先。 如果使用 SSD，应将 I/O 调度器设置为 noop 或 none。 避免使用网络存储（NFS / CIFS / SMB），这类存储不适合 Easysearch 的高频随机读写。 云环境可使用高性能云盘（如阿里云 ESSD、AWS gp3/io2、腾讯云增强型 SSD）。   详细 NVMe 配置参见 NVMe 配置指南。\n 磁盘挂载建议 #   数据盘单独挂载到 /data，不要与系统盘混用。 使用 xfs 或 ext4 文件系统。 关闭 atime：mount -o noatime /dev/vdb /data。 多块磁盘使用 JBOD 多路径，而非 RAID。  容量估算 #  所需磁盘空间 = 原始数据量 × (1 + 副本数) × 1.1（段合并预留） 实际分配空间 = 所需磁盘空间 × 1.2（日常运维预留） 示例：100 GB 原始数据，1 个副本 → 100 × 2 × 1.1 × 1.2 ≈ 264 GB。\n网络 #   集群节点间建议使用千兆以上网络，万兆网卡更佳。 低延迟网络对集群稳定性至关重要，避免跨数据中心部署单个集群。 如有跨地域需求，应使用 跨集群复制。 节点间通信延迟应 \u0026lt; 1 ms（同机房），跨可用区延迟应 \u0026lt; 5 ms。  机器数量与规格 #  推荐中等配置的机器，避免极端：\n   方案 说明     ❌ 少量超大机器 单节点故障影响太大，恢复时间长   ❌ 大量低配机器 管理复杂度高，资源利用率低   ✅ 适量中高配机器 平衡成本、性能和容错能力    建议的\u0026quot;甜点\u0026quot;配置：16C64G + NVMe SSD（500 GB ~ 2 TB）。\n延伸阅读 #    测试环境部署  生产环境部署  NUMA 配置  NVMe 配置  RAID 配置  系统调优  ","subcategory":null,"summary":"","tags":null,"title":"硬件配置","url":"/easysearch/main/docs/deployment/config/hardware/"},{"category":null,"content":"数据流（Data streams） #  如果你正在将连续生成的时间序列数据（如日志、事件和指标）摄入 Easysearch，那么你很可能处于这样一种场景：文档数量快速增长，且你无需更新旧文档。\n管理时间序列数据的典型工作流程包含多个步骤，例如创建滚动索引别名、定义写入索引，以及为底层索引定义通用的映射和设置。\n数据流简化了这一过程，并强制采用最适合时间序列数据的配置方式，例如主要为仅追加（append-only）数据设计，并确保每个文档都包含一个时间戳字段。\n数据流在内部由多个底层索引组成。搜索请求会被路由到所有底层索引，而写入请求则被路由到最新的写入索引。通过 索引生命周期管理（ILM） 策略，你可以自动处理索引滚动（rollover）或删除操作。\n数据流使用说明 #  步骤 1：创建索引模板 #  要创建数据流，首先需要创建一个索引模板，用于将一组索引配置为数据流。data_stream 对象表明这是一个数据流，而非普通索引模板。索引模式需与数据流的名称匹配：\nPUT _index_template/logs-template-nginx { \u0026#34;index_patterns\u0026#34;: \u0026#34;logs-nginx\u0026#34;, \u0026#34;data_stream\u0026#34;: { }, \u0026#34;priority\u0026#34;: 200, \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 0 } } } 在此情况下，每个摄入的文档都必须包含一个 @timestamp 字段。\n你也可以在 data_stream 对象中自定义时间戳字段名称。此外，你还可以在此处定义索引映射和其他设置，就像为普通索引模板所做的那样。\nPUT _index_template/logs-template-nginx { \u0026#34;index_patterns\u0026#34;: \u0026#34;logs-nginx\u0026#34;, \u0026#34;data_stream\u0026#34;: { \u0026#34;timestamp_field\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;request_time\u0026#34; } }, \u0026#34;priority\u0026#34;: 200, \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 0 } } } 在此示例中，logs-nginx 索引会匹配 logs-template-nginx 模板。当存在多个匹配时，Easysearch 会选择优先级更高的模板。\n步骤 2：创建数据流 #  创建索引模板后，即可创建数据流。\n你可以使用数据流 API 显式创建数据流。该 API 将初始化第一个底层索引：\nPUT _data_stream/logs-nginx 你也可以直接开始摄入数据，而无需预先创建数据流。\n由于我们已有包含 data_stream 对象的匹配索引模板，Easysearch 将自动创建数据流：\nPOST logs-nginx/_doc { \u0026#34;message\u0026#34;: \u0026#34;login attempt failed\u0026#34;, \u0026#34;@timestamp\u0026#34;: \u0026#34;2013-03-01T00:00:00\u0026#34; } 要查看特定数据流的信息：\nGET _data_stream/logs-nginx 示例响应 #  { \u0026#34;data_streams\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;logs-nginx\u0026#34;, \u0026#34;timestamp_field\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;@timestamp\u0026#34; }, \u0026#34;indices\u0026#34;: [ { \u0026#34;index_name\u0026#34;: \u0026#34;.ds-logs-nginx-000001\u0026#34;, \u0026#34;index_uuid\u0026#34;: \u0026#34;Z8RATqnzTbGxbm-khb5LKw\u0026#34; }, { \u0026#34;index_name\u0026#34;: \u0026#34;.ds-logs-nginx-000002\u0026#34;, \u0026#34;index_uuid\u0026#34;: \u0026#34;o8xka3CxSjqzNrw1BoRP6A\u0026#34; } ], \u0026#34;generation\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;GREEN\u0026#34;, \u0026#34;template\u0026#34;: \u0026#34;logs-template-nginx\u0026#34; } ] } 你可以看到时间戳字段的名称、底层索引列表、用于创建数据流的模板，以及数据流的健康状态（反映其所有底层索引中最低的状态）。\n要获取更多关于数据流的详细信息，请使用 _stats 端点：\nGET _data_stream/logs-nginx/_stats 示例响应 #  { \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 2, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;data_stream_count\u0026#34;: 1, \u0026#34;backing_indices\u0026#34;: 2, \u0026#34;total_store_size_bytes\u0026#34;: 18125, \u0026#34;data_streams\u0026#34;: [ { \u0026#34;data_stream\u0026#34;: \u0026#34;logs-nginx\u0026#34;, \u0026#34;backing_indices\u0026#34;: 2, \u0026#34;store_size_bytes\u0026#34;: 18125, \u0026#34;maximum_timestamp\u0026#34;: 1362096000000 } ] } 要查看所有数据流的信息，请使用以下请求：\nGET _data_stream 步骤 3：向数据流中摄入数据 #  你可以使用常规的索引 API 将数据摄入数据流。请确保每个索引的文档都包含时间戳字段。如果尝试摄入没有时间戳字段的文档，将会报错。\nPOST logs-nginx/_doc { \u0026#34;message\u0026#34;: \u0026#34;login attempt\u0026#34;, \u0026#34;@timestamp\u0026#34;: \u0026#34;2013-03-01T00:00:00\u0026#34; } 步骤 4：搜索数据流 #  你可以像搜索普通索引或索引别名一样搜索数据流。搜索操作将应用于所有底层索引（即数据流中的全部数据）。\nGET logs-nginx/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;login\u0026#34; } } } 示例响应 #  { \u0026#34;took\u0026#34; : 514, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 5, \u0026#34;successful\u0026#34; : 5, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 0.2876821, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;.ds-logs-redis-000001\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;-rhVmXoBL6BAVWH3mMpC\u0026#34;, \u0026#34;_score\u0026#34; : 0.2876821, \u0026#34;_source\u0026#34; : { \u0026#34;message\u0026#34; : \u0026#34;login attempt\u0026#34;, \u0026#34;@timestamp\u0026#34; : \u0026#34;2013-03-01T00:00:00\u0026#34; } } ] } } 步骤 5：滚动数据流 #  滚动操作会创建一个新的底层索引，并将其设置为数据流的新写入索引。\n要对数据流执行手动滚动操作：\nPOST logs-nginx/_rollover 示例响应 #  { \u0026#34;acknowledged\u0026#34;: true, \u0026#34;shards_acknowledged\u0026#34;: true, \u0026#34;old_index\u0026#34;: \u0026#34;.ds-logs-nginx-000001\u0026#34;, \u0026#34;new_index\u0026#34;: \u0026#34;.ds-logs-nginx-000002\u0026#34;, \u0026#34;rolled_over\u0026#34;: true, \u0026#34;dry_run\u0026#34;: false, \u0026#34;conditions\u0026#34;: {} } 现在，如果你对 logs-nginx 数据流执行 GET 操作，你会看到其生成（generation） ID 从 1 增加到了 2。\n你还可以设置一个 索引生命周期管理（ILM）策略 来自动执行数据流的滚动操作。ILM 策略在底层索引创建时即被应用。当你将策略关联到数据流时，它仅影响该数据流未来的底层索引。\n此外，你无需提供 rollover_alias 设置，因为 ILM 策略会从底层索引中自动推断此信息。\n步骤 6：删除数据流 #  删除操作会自动删除数据流的所有底层索引，然后删除数据流本身。\n要删除一个数据流及其所有隐藏的底层索引：\nDELETE _data_stream/\u0026lt;data_stream_name\u0026gt; 你可以使用通配符来删除多个数据流。\n我们建议使用 ILM 策略来删除数据流中的数据。\n API 参考汇总 #     操作 方法 端点     创建数据流 PUT /_data_stream/{name}   查看数据流 GET /_data_stream 或 /_data_stream/{name}   数据流统计 GET /_data_stream/_stats 或 /_data_stream/{name}/_stats   删除数据流 DELETE /_data_stream/{name}   滚动数据流 POST /{data_stream}/_rollover     删除操作支持逗号分隔的列表和通配符，如 DELETE /_data_stream/logs-*。\n  相关文档 #    索引模板：数据流依赖索引模板定义  索引生命周期管理：自动管理数据流的滚动与删除  数据生命周期：完整的数据保留策略体系  ","subcategory":null,"summary":"","tags":null,"title":"数据流","url":"/easysearch/main/docs/operations/data-management/data-streams/"},{"category":null,"content":"内置函数 #  Easysearch SQL 提供 80 多个内置函数，涵盖数学运算、字符串处理、日期时间、条件判断和类型转换等类别。\n 数学函数 #     函数 说明 示例     ABS(expr) 绝对值 ABS(-5) → 5   CEIL(expr) / CEILING(expr) 向上取整 CEIL(2.3) → 3   FLOOR(expr) 向下取整 FLOOR(2.7) → 2   ROUND(expr [, d]) 四舍五入到 d 位小数 ROUND(3.1415, 2) → 3.14   TRUNCATE(expr, d) 截断到 d 位小数 TRUNCATE(3.1415, 2) → 3.14   SQRT(expr) 平方根 SQRT(16) → 4.0   CBRT(expr) 立方根 CBRT(27) → 3.0   POWER(base, exp) / POW(base, exp) 幂运算 POWER(2, 10) → 1024   EXP(expr) e 的 expr 次幂 EXP(1) → 2.7183   LN(expr) 自然对数 LN(E()) → 1.0   LOG(expr) 自然对数（同 LN） LOG(10) → 2.3026   LOG2(expr) 以 2 为底的对数 LOG2(8) → 3.0   LOG10(expr) 以 10 为底的对数 LOG10(100) → 2.0   MOD(a, b) / MODULUS(a, b) 取模 MOD(10, 3) → 1   SIGN(expr) 符号（-1/0/1） SIGN(-5) → -1   PI() 圆周率 π PI() → 3.14159   E() 自然常数 e E() → 2.71828   RAND() / RANDOM() [0, 1) 随机数 RAND() → 0.7523   CONV(expr, from_base, to_base) 进制转换 CONV('a', 16, 10) → '10'   CRC32(expr) CRC32 校验和 CRC32('easysearch')    三角函数 #     函数 说明     ACOS(expr) 反余弦   ASIN(expr) 反正弦   ATAN(expr) 反正切   ATAN2(y, x) 两参数反正切   COS(expr) 余弦   COT(expr) 余切   SIN(expr) 正弦   TAN(expr) 正切   DEGREES(expr) 弧度转角度   RADIANS(expr) 角度转弧度    示例 #  SELECT ABS(balance - 10000) AS diff, ROUND(balance / 1000.0, 1) AS balance_k, SQRT(age) AS sqrt_age FROM accounts  字符串函数 #     函数 说明 示例     LENGTH(str) 字符串长度 LENGTH('hello') → 5   UPPER(str) 转大写 UPPER('hello') → 'HELLO'   LOWER(str) 转小写 LOWER('HELLO') → 'hello'   SUBSTRING(str, pos [, len]) / SUBSTR(str, pos [, len]) 截取子串（pos 从 1 开始） SUBSTRING('hello', 2, 3) → 'ell'   TRIM(str) 去除首尾空白 TRIM(' hi ') → 'hi'   LTRIM(str) 去除左侧空白 LTRIM(' hi') → 'hi'   RTRIM(str) 去除右侧空白 RTRIM('hi ') → 'hi'   CONCAT(str1, str2, ...) 连接字符串 CONCAT('a', 'b', 'c') → 'abc'   CONCAT_WS(sep, str1, str2, ...) 用分隔符连接 CONCAT_WS('-', 'a', 'b') → 'a-b'   REPLACE(str, from, to) 替换子串 REPLACE('hello', 'l', 'r') → 'herro'   LOCATE(substr, str [, pos]) 查找子串位置（从 1 开始） LOCATE('ll', 'hello') → 3   LEFT(str, len) 从左取 len 个字符 LEFT('hello', 2) → 'he'   RIGHT(str, len) 从右取 len 个字符 RIGHT('hello', 2) → 'lo'   ASCII(str) 首字符的 ASCII 码 ASCII('A') → 65   REVERSE(str) 反转字符串 REVERSE('hello') → 'olleh'    示例 #  SELECT UPPER(firstname) AS name, LENGTH(address) AS addr_len, SUBSTRING(city, 1, 3) AS city_prefix FROM accounts WHERE LENGTH(lastname) \u0026gt; 4  日期时间函数 #  构造函数 #     函数 说明 示例     DATE(expr) 提取日期部分 DATE('2025-01-15 10:30:00') → '2025-01-15'   TIME(expr) 提取时间部分 TIME('2025-01-15 10:30:00') → '10:30:00'   TIMESTAMP(expr) 转为时间戳 TIMESTAMP('2025-01-15')   MAKETIME(h, m, s) 构造时间值 MAKETIME(10, 30, 0) → '10:30:00'    提取函数 #     函数 同义函数 说明     YEAR(date) — 提取年份   MONTH(date) — 提取月份（1-12）   MONTHNAME(date) — 月份名称（英文）   DAY(date) DAYOFMONTH(date) 提取日（1-31）   DAYNAME(date) — 星期名称（英文）   DAYOFWEEK(date) — 星期几（1=周日，7=周六）   DAYOFYEAR(date) — 一年中第几天（1-366）   WEEK(date) — 一年中第几周   HOUR(time) — 提取小时   MINUTE(time) — 提取分钟   SECOND(time) — 提取秒    运算函数 #     函数 说明 示例     ADDDATE(date, INTERVAL n unit) / DATE_ADD(date, INTERVAL n unit) 日期加法 ADDDATE('2025-01-01', INTERVAL 7 DAY)   SUBDATE(date, INTERVAL n unit) / DATE_SUB(date, INTERVAL n unit) 日期减法 SUBDATE('2025-01-08', INTERVAL 7 DAY)   FROM_DAYS(n) 从天数转日期 FROM_DAYS(738886)   UNIX_TIMESTAMP(date) 转 Unix 时间戳 UNIX_TIMESTAMP('2025-01-15')    INTERVAL 支持的单位：SECOND、MINUTE、HOUR、DAY、WEEK、MONTH、QUARTER、YEAR。\n格式化函数 #     函数 说明 示例     DATE_FORMAT(date, format) 按格式字符串输出 DATE_FORMAT('2025-01-15', '%Y/%m/%d') → '2025/01/15'    常用格式占位符：\n   占位符 说明 示例值     %Y 四位年份 2025   %m 两位月份 01   %d 两位日 15   %H 24 小时制小时 10   %i 分钟 30   %s 秒 00   %W 星期名称 Wednesday   %M 月份名称 January    示例 #  SELECT YEAR(hire_date) AS yr, MONTH(hire_date) AS mon, DAYNAME(hire_date) AS dow, DATE_FORMAT(hire_date, \u0026#39;%Y-%m\u0026#39;) AS ym FROM employees WHERE hire_date \u0026gt; DATE_SUB(NOW(), INTERVAL 1 YEAR)  条件函数 #     函数 说明 示例     IF(cond, val1, val2) 条件为真返回 val1，否则 val2 IF(age \u0026gt; 30, 'senior', 'junior')   IFNULL(expr, val) expr 为 NULL 时返回 val IFNULL(employer, 'N/A')   NULLIF(expr1, expr2) 两值相等返回 NULL，否则返回 expr1 NULLIF(state, 'IL')   ISNULL(expr) 判断是否为 NULL（返回 1 或 0） ISNULL(employer)    与 CASE 表达式的对比 #  IF 函数适合简单的二选一判断，复杂的多分支条件使用 CASE 表达式更清晰（参见 复杂查询）。\n-- IF 函数 SELECT IF(balance \u0026gt; 10000, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;) AS level FROM accounts \u0026ndash; 等效 CASE 表达式 SELECT CASE WHEN balance \u0026gt; 10000 THEN 'high' ELSE 'low' END AS level FROM accounts \n类型转换 #  使用 CAST 函数在 SQL 数据类型之间进行转换。\n语法 #  CAST(expression AS target_type) 支持的目标类型 #     目标类型 说明     BOOLEAN 布尔值   BYTE 8 位有符号整数   SHORT 16 位有符号整数   INT / INTEGER 32 位有符号整数   LONG 64 位有符号整数   FLOAT 单精度浮点   DOUBLE 双精度浮点   STRING 字符串   DATE 日期   TIME 时间   DATETIME / TIMESTAMP 日期时间    示例 #  SELECT CAST(age AS DOUBLE) AS age_d, CAST(account_number AS STRING) AS num_s, CAST(\u0026#39;2025-01-15\u0026#39; AS DATE) AS dt FROM accounts  相关链接 #    SQL 查询总览  聚合查询  全文搜索  ","subcategory":null,"summary":"","tags":null,"title":"内置函数","url":"/easysearch/main/docs/features/sql/functions/"},{"category":null,"content":"HanLP CRF 分词器 #  hanlp_crf 分词器使用 HanLP CRF（条件随机场）分词模式，适合需要高精度分词的学术研究场景。\n需要安装 analysis-hanlp 插件，并确保 CRF CWS 模型可用。\n示例 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_crf\u0026#34; } } } } } 相关指南 #    HanLP 分词器  HanLP CRF 分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"HanLP CRF 分词器（HanLP CRF）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-crf/"},{"category":null,"content":"文本向量化处理器 #   需要 AI 插件和 KNN 插件\n text_embedding 处理器在文档写入时自动调用外部 Embedding 模型服务，将文本字段转换为向量并存储到指定的向量字段中，实现\u0026quot;写入即向量化\u0026quot;。该处理器是构建语义搜索和混合搜索的基础组件。\n语法 #  { \u0026#34;text_embedding\u0026#34;: { \u0026#34;text_field\u0026#34;: \u0026#34;content\u0026#34;, \u0026#34;vector_field\u0026#34;: \u0026#34;content_vector\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.openai.com/v1/embeddings\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;model_id\u0026#34;: \u0026#34;text-embedding-3-small\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;sk-xxxxxxxx\u0026#34; } } 配置参数 #     参数 是否必填 描述     text_field 必填 包含待向量化文本的源字段   vector_field 必填 存储生成的向量的目标字段。该字段应映射为 knn_vector 类型   url 必填 Embedding 模型服务的 API 端点 URL   vendor 必填 模型提供商标识。openai 表示 OpenAI 兼容接口，其他值（如 ollama）使用 Ollama 兼容接口   model_id 必填 使用的 Embedding 模型 ID   api_key 可选 API 密钥（使用 OpenAI 兼容接口时通常必填）。存储时会自动加密   dimensions 可选 期望的向量维度。未指定时使用模型默认维度   ignore_missing 可选 为 true 时，源字段缺失则跳过处理。默认为 false   batch_size 可选 批量处理时每次 API 调用包含的文档数。默认为 1   description 可选 处理器的简要描述   if 可选 处理器运行的条件   ignore_failure 可选 为 true 时，处理器出错后忽略继续执行。默认为 false   on_failure 可选 处理器失败时运行的处理器列表   tag 可选 处理器的标识标签    支持的模型服务 #     接口类型 vendor 值 认证方式 典型服务     OpenAI 兼容 openai Authorization: Bearer \u0026lt;api_key\u0026gt; 请求头 OpenAI、阿里云 DashScope、Azure OpenAI、DeepSeek 等   Ollama 兼容 其他任意值（如 ollama） 无认证 Ollama 本地部署    如何使用 #  步骤 1：创建向量索引 #  首先创建包含 KNN 向量字段的索引：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;index.knn\u0026#34;: true }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;content_vector\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_vector\u0026#34;, \u0026#34;dimension\u0026#34;: 1536 } } } } 步骤 2：创建管道 #  使用 OpenAI 兼容接口创建向量化管道：\nPUT /_ingest/pipeline/embedding_pipeline { \u0026#34;description\u0026#34;: \u0026#34;写入时自动向量化\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;text_embedding\u0026#34;: { \u0026#34;text_field\u0026#34;: \u0026#34;content\u0026#34;, \u0026#34;vector_field\u0026#34;: \u0026#34;content_vector\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;model_id\u0026#34;: \u0026#34;text-embedding-v3\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;sk-xxxxxxxx\u0026#34; } } ] } 步骤 3：写入数据 #  PUT /my_index/_doc/1?pipeline=embedding_pipeline { \u0026#34;content\u0026#34;: \u0026#34;Easysearch 是一款高性能的搜索引擎\u0026#34; } 处理器会自动将 content 字段的文本发送到 Embedding 服务，并将返回的向量存储到 content_vector 字段。\n步骤 4：验证结果 #  GET /my_index/_doc/1 { \u0026#34;_source\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Easysearch 是一款高性能的搜索引擎\u0026#34;, \u0026#34;content_vector\u0026#34;: [0.0123, -0.0456, 0.0789, ...] } } 使用 Ollama 本地模型 #  PUT /_ingest/pipeline/ollama_embedding_pipeline { \u0026#34;description\u0026#34;: \u0026#34;使用 Ollama 本地模型向量化\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;text_embedding\u0026#34;: { \u0026#34;text_field\u0026#34;: \u0026#34;content\u0026#34;, \u0026#34;vector_field\u0026#34;: \u0026#34;content_vector\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://localhost:11434/api/embed\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;ollama\u0026#34;, \u0026#34;model_id\u0026#34;: \u0026#34;nomic-embed-text\u0026#34; } } ] } 批量写入 #  使用 batch_size 提升批量写入时的向量化效率：\nPUT /_ingest/pipeline/batch_embedding_pipeline { \u0026#34;description\u0026#34;: \u0026#34;批量向量化\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;text_embedding\u0026#34;: { \u0026#34;text_field\u0026#34;: \u0026#34;content\u0026#34;, \u0026#34;vector_field\u0026#34;: \u0026#34;content_vector\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;model_id\u0026#34;: \u0026#34;text-embedding-v3\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;sk-xxxxxxxx\u0026#34;, \u0026#34;batch_size\u0026#34;: 10 } } ] } 注意事项 #   如果 vector_field 已存在于文档中，处理器会抛出异常（设置 ignore_failure: true 可跳过） API 密钥在管道配置中会被自动加密存储（形如 ENCRYPTED_VALUE...infinilabs），查询管道定义时看到的是加密后的值 批量写入时，文本为空或缺失的文档会被静默跳过，不会发送到 Embedding 服务 vector_field 的维度必须与所用模型的输出维度一致 向量化依赖外部服务，写入速度会受到网络延迟和模型服务吞吐量的影响  相关文档 #    写入数据文本向量化：完整的端到端工作流指南  向量搜索：使用向量进行近邻搜索  混合搜索：结合关键词和向量的混合搜索  ","subcategory":null,"summary":"","tags":null,"title":"文本向量化处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/text-embedding/"},{"category":null,"content":"Refresh、Flush 与 Force Merge #  这些是索引的日常维护操作，用于控制数据可见性、持久性和段文件结构。\nRefresh：让新写入的文档可搜索 #  写入的文档不会立刻出现在搜索结果中——需要经过一次 refresh 操作，在内存中创建新的 Lucene 段（segment），文档才能被 _search 检索到。\n 详见 写入与存储机制 了解 refresh 的工作原理。\n 默认情况下，Easysearch 每 1 秒 自动执行一次 refresh（由 index.refresh_interval 控制）。但你也可以手动触发：\n// 刷新特定索引 POST /my-index/_refresh // 刷新多个索引 POST /index-1,index-2/_refresh\n// 刷新所有索引 POST /_refresh 响应：\n{ \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 10, \u0026#34;successful\u0026#34;: 5, \u0026#34;failed\u0026#34;: 0 } } 查询参数 #     参数 类型 默认值 说明     expand_wildcards String open 通配符展开策略   ignore_unavailable Boolean false 忽略不存在的索引   allow_no_indices Boolean true 通配符未匹配时是否报错    常见场景 #   批量写入后立即搜索：导入数据后手动 refresh 确保数据可见 测试/CI 环境：写入后立即 refresh 以验证结果   生产环境注意：频繁手动 refresh 会增加小段数量，影响搜索性能。大批量写入时建议临时关闭自动 refresh（设为 \u0026quot;-1\u0026quot;），导入完成后再恢复。\n Flush：将数据持久化到磁盘 #  Flush 触发 Lucene 的 commit 操作，将内存中的段写入磁盘，并清空事务日志（translog）。Flush 之后，即使节点崩溃也不会丢失数据。\n Easysearch 会自动管理 flush（当 translog 达到 index.translog.flush_threshold_size 时触发），通常不需要手动 flush。\n // Flush 特定索引 POST /my-index/_flush // Flush 所有索引 POST /_flush 响应：\n{ \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 10, \u0026#34;successful\u0026#34;: 5, \u0026#34;failed\u0026#34;: 0 } } 查询参数 #     参数 类型 默认值 说明     force Boolean false 即使没有需要提交的变更也强制 flush   wait_if_ongoing Boolean true 如果另一个 flush 正在进行，是否等待而不是立即返回   expand_wildcards String open 通配符展开策略   ignore_unavailable Boolean false 忽略不存在的索引   allow_no_indices Boolean true 通配符未匹配时是否报错    何时手动 Flush #   在执行滚动重启前，手动 flush 所有索引可以加速节点恢复（减少需要重放的 translog） 在关闭索引之前  // 滚动重启前：flush 所有索引 POST /_flush Force Merge：合并段文件 #  段合并（merge）会在后台自动进行，但有时你可能需要手动触发强制合并：\n 将已删除文档从段中清除，释放磁盘空间 将小段合并为大段，提高搜索效率 将只读索引合并为一个段，获得最佳查询性能  // 将索引合并到最多 1 个段 POST /my-index/_forcemerge?max_num_segments=1 // 仅清除已删除文档 POST /my-index/_forcemerge?only_expunge_deletes=true\n// 对所有索引执行 POST /_forcemerge?max_num_segments=1 响应：\n{ \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 10, \u0026#34;successful\u0026#34;: 5, \u0026#34;failed\u0026#34;: 0 } } 查询参数 #     参数 类型 默认值 说明     max_num_segments Integer -1（无限制） 合并后的最大段数。设为 1 表示完全合并   only_expunge_deletes Boolean false 仅合并包含已删除文档的段   flush Boolean true 合并完成后是否自动 flush   expand_wildcards String open 通配符展开策略   ignore_unavailable Boolean false 忽略不存在的索引   allow_no_indices Boolean true 通配符未匹配时是否报错    使用建议 #     场景 推荐操作     只读/归档索引 max_num_segments=1，获得最佳搜索性能   大量删除后回收空间 only_expunge_deletes=true   仍在写入的索引 不要 force merge——后台自动合并足够     ⚠️ 对正在写入的索引执行 force merge 会导致很大的段产生，后续写入又会产生小段，反而加重合并压力。\n 清除缓存 #  清除索引级别的缓存（查询缓存、请求缓存、字段数据缓存），释放内存。\n// 清除特定索引的所有缓存 POST /my-index/_cache/clear // 清除所有索引的缓存 POST /_cache/clear\n// 只清除特定类型的缓存 POST /my-index/_cache/clear?query=true POST /my-index/_cache/clear?request=true POST /my-index/_cache/clear?fielddata=true\n// 清除特定字段的字段数据缓存 POST /my-index/_cache/clear?fields=field1,field2 查询参数 #\n    参数 类型 默认值 说明     query Boolean — 是否清除查询缓存   request Boolean — 是否清除请求缓存   fielddata Boolean — 是否清除字段数据缓存   fields String — 逗号分隔的字段名，指定清除哪些字段的缓存   expand_wildcards String open 通配符展开策略   ignore_unavailable Boolean false 忽略不存在的索引   allow_no_indices Boolean true 通配符未匹配时是否报错    操作对比 #     操作 作用 性能影响 需要手动触发？     Refresh 新文档变得可搜索 创建新段，增加段数 一般不需要   Flush 段持久化到磁盘，清空 translog 磁盘 IO 一般不需要   Force Merge 合并段、清除已删除文档 CPU + IO 密集 只在只读索引上   Cache Clear 释放内存中的缓存 后续查询需要重建缓存 内存压力时    下一步 #    索引设置：refresh_interval、translog.durability、合并策略等  索引统计与监控：查看段数量、合并状态等指标  写入与存储机制：Refresh、Flush 的底层原理以及为什么搜索不是实时的  ","subcategory":null,"summary":"","tags":null,"title":"Refresh、Flush 与 Force Merge","url":"/easysearch/main/docs/operations/data-management/refresh-flush-forcemerge/"},{"category":null,"content":"HanLP NLP 分词器 #  hanlp_nlp 分词器使用 HanLP NLP 分词模式，支持命名实体识别，适合需要高精度语义分析的场景。\n需要安装 analysis-hanlp 插件，并确保 perceptron CWS 模型可用。\n示例 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_nlp\u0026#34; } } } } } 相关指南 #    HanLP 分词器  HanLP NLP 分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"HanLP NLP 分词器（HanLP NLP）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-nlp/"},{"category":null,"content":"规则匹配处理器 #   需要 Rules 插件和有效许可证\n check_match_rules 处理器将 规则引擎接入 Ingest Pipeline，在文档写入阶段执行规则匹配并自动打标。\n语法 #  { \u0026#34;check_match_rules\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;my_ruleset\u0026#34; } } 配置参数 #     参数 是否必填 描述     id 必填 规则库 ID（对应已编译的 repo_id）   target_field 可选 匹配结果写入字段，默认 tags   fields 可选 文档字段白名单；指定后仅这些字段按原字段名参与匹配，其余字段内容汇总到 default_match_field   default_match_field 可选 当配置了 fields 时，未包含字段会汇总到该字段名参与匹配，默认 content   regex_start_at_word 可选 正则匹配是否从词边界开始，默认 true   ignore_missing 可选 为 true 时，处理过程中遇到错误会跳过，默认 false   description 可选 处理器描述   if 可选 处理器执行条件   ignore_failure 可选 为 true 时，处理器失败后忽略并继续执行，默认 false   on_failure 可选 处理器失败时执行的处理器列表   tag 可选 处理器标识    工作原理 #   提取文档字段：  未配置 fields：提取所有非 _ 前缀字段（含嵌套展平） 配置 fields：白名单字段保留原名，其余字段值拼接到 default_match_field   执行规则匹配：传入底层规则匹配引擎 写入结果：命中后将标签数组写入 target_field  使用步骤 #  前置条件 #  先导入并编译规则库，详见 规则引擎文档：\nPOST /_match_rules/{repo_id}/_compile {} 步骤 1：创建管道 #  PUT /_ingest/pipeline/rules_pipeline { \u0026#34;description\u0026#34;: \u0026#34;实时规则匹配\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;check_match_rules\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;my_ruleset\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;tags\u0026#34; } } ] } 步骤 2：写入测试数据 #  PUT /my_index/_doc/1?pipeline=rules_pipeline { \u0026#34;title\u0026#34;: \u0026#34;测试文档\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;这是需要进行规则检测的文本内容\u0026#34; } 步骤 3：查看结果 #  { \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;测试文档\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;这是需要进行规则检测的文本内容\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;敏感词规则A\u0026#34;, \u0026#34;内容分类规则B\u0026#34;] } } 指定匹配字段 #  { \u0026#34;check_match_rules\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;my_ruleset\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title\u0026#34;, \u0026#34;content\u0026#34;], \u0026#34;default_match_field\u0026#34;: \u0026#34;content\u0026#34; } } 注意事项 #   规则库需先通过 POST /_match_rules/{repo_id}/_compile 编译 正则类型的字段值（以 (、{ 或 [ 开头）会被识别为正则模式 建议将规则库维护、导入、编译流程统一在 rules 文档中管理  ","subcategory":null,"summary":"","tags":null,"title":"规则匹配处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/check-match-rules/"},{"category":null,"content":"索引滚动（Rollover） #  Rollover API 用于在满足特定条件时，将一个别名或数据流（Data Stream）滚动到一个新的索引。这是管理时间序列数据的核心操作——让你的索引保持合理大小，避免单个索引无限膨胀。\n基本用法 #  前置条件 #  Rollover 只能在以下两种目标上执行：\n 写入别名（Write Alias）：别名必须标记了 is_write_index: true 数据流（Data Stream）：天然支持 Rollover  对别名执行 Rollover #  先创建索引和写入别名：\nPUT /logs-000001 { \u0026#34;aliases\u0026#34;: { \u0026#34;logs-write\u0026#34;: { \u0026#34;is_write_index\u0026#34;: true } } } 当条件满足时执行 Rollover：\nPOST /logs-write/_rollover { \u0026#34;conditions\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;max_docs\u0026#34;: 10000000, \u0026#34;max_size\u0026#34;: \u0026#34;50gb\u0026#34;, \u0026#34;max_primary_shard_size\u0026#34;: \u0026#34;25gb\u0026#34; } } 响应：\n{ \u0026#34;acknowledged\u0026#34;: true, \u0026#34;shards_acknowledged\u0026#34;: true, \u0026#34;old_index\u0026#34;: \u0026#34;logs-000001\u0026#34;, \u0026#34;new_index\u0026#34;: \u0026#34;logs-000002\u0026#34;, \u0026#34;rolled_over\u0026#34;: true, \u0026#34;dry_run\u0026#34;: false, \u0026#34;conditions\u0026#34;: { \u0026#34;[max_age: 7d]\u0026#34;: false, \u0026#34;[max_docs: 10000000]\u0026#34;: true, \u0026#34;[max_size: 50gb]\u0026#34;: false, \u0026#34;[max_primary_shard_size: 25gb]\u0026#34;: false } }  任一条件满足即触发滚动。响应中会列出每个条件的评估结果。\n 对数据流执行 Rollover #  POST /logs-nginx/_rollover { \u0026#34;conditions\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;max_docs\u0026#34;: 5000000 } } 指定新索引名 #  默认情况下，新索引名会在旧索引名基础上递增数字后缀（如 logs-000001 → logs-000002）。也可以指定自定义名称：\nPOST /logs-write/_rollover/logs-2024-07-01 { \u0026#34;conditions\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;7d\u0026#34; } } 滚动条件 #     条件 类型 说明     max_age Time 索引创建后经过的时间（如 \u0026quot;7d\u0026quot;、\u0026quot;30d\u0026quot;）   max_docs Long 索引中的文档总数   max_size Size 索引的总大小（如 \u0026quot;50gb\u0026quot;）   max_primary_shard_size Size 最大主分片的大小     max_primary_shard_size 是控制分片大小最实用的条件。推荐单个分片大小维持在 10–50GB 之间。\n Dry Run：模拟执行 #  使用 dry_run 参数可以预览 Rollover 结果而不实际执行：\nPOST /logs-write/_rollover?dry_run { \u0026#34;conditions\u0026#34;: { \u0026#34;max_docs\u0026#34;: 1000000 } } 为新索引指定设置 #  可以在请求体中为新索引指定设置、映射和别名：\nPOST /logs-write/_rollover { \u0026#34;conditions\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;7d\u0026#34; }, \u0026#34;settings\u0026#34;: { \u0026#34;index.number_of_shards\u0026#34;: 3, \u0026#34;index.number_of_replicas\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } }, \u0026#34;aliases\u0026#34;: { \u0026#34;logs-read\u0026#34;: {} } }  通常建议使用 索引模板来统一管理新索引的设置和映射，而不是在每次 Rollover 时手动指定。\n 查询参数 #     参数 类型 默认值 说明     dry_run Boolean false 模拟执行，不实际创建新索引   timeout Time 30s 操作超时时间   master_timeout Time 30s 连接主节点的超时时间   wait_for_active_shards String — 等待的活跃分片数    索引命名约定 #  要让自动递增的数字后缀生效，索引名必须以 - 和数字结尾，且数字需要足够的前导零：\n   当前索引 Rollover 后     logs-000001 logs-000002   logs-2024-01-000001 logs-2024-01-000002   my-index-3 my-index-000004     建议统一使用 6 位数字后缀（如 -000001），为未来的滚动留出足够空间。\n 与 ILM 配合使用 #  在生产环境中，通常不手动调用 Rollover API，而是通过 索引生命周期管理（ILM） 策略自动触发 Rollover。ILM 会定期检查条件，满足时自动执行滚动。\n典型的 ILM + Rollover 流程：\n 定义 ILM 策略，在 hot 阶段设置 Rollover 条件 创建索引模板，关联 ILM 策略 创建初始索引和写入别名 ILM 自动管理后续的滚动、迁移和删除  下一步 #    索引模板：为 Rollover 创建的新索引预配设置  别名（Aliases）：写入别名的管理  数据流：天然支持 Rollover 的时序数据管理  索引生命周期管理：自动化 Rollover 策略  ","subcategory":null,"summary":"","tags":null,"title":"索引滚动（Rollover）","url":"/easysearch/main/docs/operations/data-management/index-rollover/"},{"category":null,"content":"HanLP Index 分词器 #  hanlp_index 分词器是 analysis-hanlp 插件 提供的索引模式分词器。它在标准分词的基础上对长词进行二次切分，生成更多子词项，适合索引时使用以提高召回率。\n前提条件 #  需要安装 analysis-hanlp 插件：\nbin/easysearch-plugin install analysis-hanlp 分词效果对比 #  以\u0026quot;中华人民共和国国歌\u0026quot;为例：\n   分词器 输出词项     hanlp_standard 中华人民共和国、国歌   hanlp_index 中华、华人、人民、共和、共和国、中华人民共和国、国歌    hanlp_index 输出更细粒度的子词，确保无论用户搜索\u0026quot;中华\u0026quot;、\u0026ldquo;人民\u0026quot;还是\u0026quot;共和国\u0026quot;都能命中。\n使用示例 #  在映射中指定 #  PUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;hanlp_index_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_index\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;hanlp_index_analyzer\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;hanlp_standard\u0026#34; } } } } 测试分词效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_index\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国国歌\u0026#34; } 最佳实践 #     场景 推荐     索引时 使用 hanlp_index（最大化召回）   搜索时 使用 hanlp_standard 或 hanlp_nlp（精确匹配）    相关链接 #    HanLP Standard 分词器 — 标准分词模式  HanLP NLP 分词器 — 命名实体识别模式  HanLP 通用 — HanLP 分词器概述  文本分析  ","subcategory":null,"summary":"","tags":null,"title":"HanLP Index 分词器","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-index/"},{"category":null,"content":"cat API #  您可以使用紧凑且对齐的文本 （CAT） API 以易于理解的表格格式获取有关集群的基本统计信息。cat API 是一个人类可读的接口，它返回纯文本而不是传统的 JSON。\n使用 cat API，您可以回答诸如哪个节点是选定的主节点、集群处于什么状态、每个索引中有多少文档等问题。\n要查看 cat API 中的可用操作，请使用以下命令：\nGET _cat 还可以在查询中使用以下字符串参数。\n   参数 描述     ?v 通过向列添加标题使输出更详细。它还添加了一些格式，以帮助将每列对齐在一起。此页面上的所有示例都包含 v 参数。   ?help 列出给定操作的默认标头和其他可用标头。   ?h 将输出限制为特定标头。   ?format 以 JSON、YAML 或 CBOR 格式输出结果。   ?sort 按指定列对输出进行排序。    要查看每列表示的内容，请使用 ?v 参数：\nGET _cat/\u0026lt;operation_name\u0026gt;?v 要查看所有可用的标头，请使用 ?help 参数：\nGET _cat/\u0026lt;operation_name\u0026gt;?help 要将输出限制为标头的子集，请使用 ?h 参数：\nGET _cat/\u0026lt;operation_name\u0026gt;?h=\u0026lt;header_name_1\u0026gt;,\u0026lt;header_name_2\u0026gt;\u0026amp;v 通常，对于任何操作，您可以使用 ?help 参数找出可用的标头，然后使用 ?h 参数将输出限制为您关注的标头。 通常，您可以使用 ?help 参数查看任何操作可用的表头，然后使用 ?h 参数将输出限制为您关心的表头。\n别名 Aliases #  列出别名到索引的映射，以及路由和筛选信息。\nGET _cat/aliases?v 若要将信息限制为特定别名，请在查询后添加别名。\nGET _cat/aliases/\u0026lt;alias\u0026gt;?v 分配 Allocation #  列出索引的磁盘空间分配以及每个节点上的分片数。 Default request:\nGET _cat/allocation?v 文档数 Count #  列出群集中的文档数。\nGET _cat/count?v 若要查看特定索引中的文档数，请在查询后添加索引名称。\nGET _cat/count/\u0026lt;index\u0026gt;?v 字段数据 Field data #  列出每个节点的每个字段使用的内存大小。\nGET _cat/fielddata?v 若要将信息限制为特定字段，请在查询后添加字段名称。\nGET _cat/fielddata/\u0026lt;fields\u0026gt;?v 集群健康状态 Health #  列出群集的状态、群集已启动的时间、节点数以及有助于分析群集运行状况的其他有用信息。\nGET _cat/health?v 索引信息 Indices #  列出与索引相关的信息 - 索引使用的磁盘空间量、分片数、运行状况等。\nGET _cat/indices?v 若要将信息限制为特定索引，请在查询后添加索引名称。\nGET _cat/indices/\u0026lt;index\u0026gt;?v 主节点 Master #  列出有助于识别选定主节点的信息。\nGET _cat/master?v 节点属性 Node attributes #  列出自定义节点的属性。\nGET _cat/nodeattrs?v 节点信息 Nodes #  列出节点级别信息，包括节点角色和负载指标。\n一些重要的节点指标是 pid ， name ， master ， ip ， port ， version ， build ， jdk ，以及 disk ， heap ， ram 和 file_desc 。\nGET _cat/nodes?v 待处理任务 Pending tasks #  列出所有待处理任务的进度，包括任务优先级和队列中的时间。\nGET _cat/pending_tasks?v 已安装插件 Plugins #  列出已安装插件的名称、组件和版本。\nGET _cat/plugins?v 恢复 Recovery #  列出所有已完成和正在进行的索引和分片的恢复。\nGET _cat/recovery?v 若要仅查看特定索引的恢复情况，请在查询后添加索引名称。\nGET _cat/recovery/\u0026lt;index\u0026gt;?v 存储库 Repositories #  列出所有快照存储库及其类型。\nGET _cat/repositories?v 段信息 Segments #  列出每个索引的 Lucene 段级别信息。\nGET _cat/segments?v 若要仅查看有关特定索引段的信息，请在查询后添加索引名称。\nGET _cat/segments/\u0026lt;index\u0026gt;?v 分片 Shards #  列出所有主分片和副本分片的状态及其分布方式。\nGET _cat/shards?v 若要仅查看特定索引的分片信息，请在查询后添加索引名称。\nGET _cat/shards/\u0026lt;index\u0026gt;?v 快照 Snapshots #  列出存储库的所有快照。\nGET _cat/snapshots/\u0026lt;repository\u0026gt;?v 任务 Tasks #  列出群集上当前运行的所有任务的进度。\nGET _cat/tasks?v 索引模板 Templates #  列出索引模板的名称、模式、订单号和版本号。\nGET _cat/templates?v 线程池状态 Thread pool #  列出每个节点上不同线程池的活动线程、排队线程和拒绝线程。\nGET _cat/thread_pool?v 若要将信息限制为特定线程池，请在查询后添加线程池名称。\nGET _cat/thread_pool/\u0026lt;thread_pool\u0026gt;?v ","subcategory":null,"summary":"","tags":null,"title":"CAT 接口","url":"/easysearch/main/docs/operations/cluster-admin/cat/"},{"category":null,"content":"相关性与排序 #  在 Easysearch 中，搜索结果默认按**相关性（relevance）**降序排列。理解相关性如何计算、如何自定义排序，是用好全文搜索的关键。\n什么是相关性？ #  每个文档都有一个 _score 字段，表示它与查询的匹配程度。评分越高，相关性越高。\n_score 的计算基于 BM25 算法（Okapi BM25），它是经典 TF/IDF 的改进版本。BM25 使用三个核心因素来衡量相关性：\n词频（Term Frequency, TF） #  检索词在文档字段中出现的频率。出现 5 次比出现 1 次的权重更高。BM25 对词频做了饱和处理——频率增长到一定程度后增益递减，不像经典 TF/IDF 会无限增长：\n$$tf_{BM25} = \\frac{freq \\cdot (k_1 + 1)}{freq + k_1 \\cdot (1 - b + b \\cdot \\frac{dl}{avgdl})}$$\n逆向文档频率（Inverse Document Frequency, IDF） #  检索词在所有文档中出现的频率。越常见的词（如\u0026quot;的\u0026quot;\u0026ldquo;和\u0026rdquo;），权重越低；越罕见的词权重越高。\n$$idf(t) = \\ln\\left(1 + \\frac{N - df + 0.5}{df + 0.5}\\right)$$\n其中 $N$ 是文档总数，$df$ 是包含该词的文档数。\n字段长度归一化（Field-length Norm） #  字段越短，权重越高。检索词出现在 title（短字段）比出现在 content（长字段）更有意义。在 BM25 中通过参数 $b$ 控制长度归一化的影响程度。\nBM25 默认参数 #     参数 默认值 作用     k1 1.2 控制词频饱和速度。值越大，高频词获得的额外分数越多   b 0.75 控制字段长度归一化的强度。0 = 不归一化，1 = 完全按比例归一化   discount_overlaps true 位置重叠的词条（如同义词）不计入长度    可以在 mapping 中为特定字段自定义 BM25 参数：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;similarity\u0026#34;: { \u0026#34;custom_bm25\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;BM25\u0026#34;, \u0026#34;k1\u0026#34;: 1.5, \u0026#34;b\u0026#34;: 0.3 } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;custom_bm25\u0026#34; } } } } 这三个因素结合起来，决定了一个词在特定文档中的权重，最终汇总为 _score。\n理解评分：explain #  使用 explain 参数可以查看评分的详细计算过程：\nGET /my_index/_search?explain=true { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;search\u0026#34; } } } 返回中的 _explanation 会详细列出 TF、IDF、field norm 等各因子的值。\n也可以查看某个文档为什么不匹配：\nGET /my_index/_explain/123 { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;search\u0026#34; } } }  ⚠️ explain 开销很大，仅用于调试，不要在生产环境使用。\n 按字段排序 #  不是所有场景都需要相关性排序。可以使用 sort 参数按任意字段排序：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;published\u0026#34; } } } }, \u0026#34;sort\u0026#34;: { \u0026#34;date\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } } 使用 sort 后：\n _score 不会被计算（显示为 null），节省性能 结果中会有 sort 字段显示排序值 如果还需要 _score，设置 \u0026quot;track_scores\u0026quot;: true  多级排序 #  可以按多个字段依次排序：\nGET /_search { \u0026#34;sort\u0026#34;: [ { \u0026#34;date\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } }, { \u0026#34;_score\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } 先按日期降序，日期相同时按相关性降序。\n多值字段排序 #  对于多值字段（如数组），可以用 mode 指定取哪个值排序：\n\u0026#34;sort\u0026#34;: { \u0026#34;prices\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;min\u0026#34; } } 可选模式：min、max、avg、sum。\n字符串排序与多字段 #  text 类型字段会被分析成多个词条，直接用于排序可能得到意外结果。推荐做法是使用 multi-fields：\nPUT /my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } 搜索用 title（全文匹配），排序用 title.keyword（精确值）：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;search engine\u0026#34; } }, \u0026#34;sort\u0026#34;: \u0026#34;title.keyword\u0026#34; } 小结 #     概念 要点     _score 相关性评分，基于 BM25（k1=1.2, b=0.75）   TF 词频越高，权重越高   IDF 文档频率越高，权重越低（常见词权重低）   Field norm 字段越短，权重越高   sort 按字段排序，跳过评分计算   Multi-fields text + keyword 子字段，分别用于搜索和排序   explain 调试评分的利器，仅用于开发环境    下一步可以继续阅读：\n  全文搜索：搜索 DSL 的完整使用  映射与分析入门：分析器与字段类型  分页与排序：深分页优化  最佳实践 #    查询调优与慢查询排查：相关性调优、评分影响因素  数据建模：多字段策略对排序的影响  ","subcategory":null,"summary":"","tags":null,"title":"相关性与排序","url":"/easysearch/main/docs/fundamentals/relevance-and-sorting/"},{"category":null,"content":"其它常用 API #  此页面包含 Easysearch 常用 API 的示例请求。\n使用非默认设置创建索引 #  PUT my-logs { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 4, \u0026#34;number_of_replicas\u0026#34;: 2 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;year\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } 索引单个文档并自动生成随机 ID #  POST my-logs/_doc { \u0026#34;title\u0026#34;: \u0026#34;Your Name\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;2016\u0026#34; } 索引单个文档并指定 ID #  PUT my-logs/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;Weathering with You\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;2019\u0026#34; } 一次索引多个文档 #  请求正文末尾的空白行是必填的。如果省略 _id 字段， Easysearch 将生成一个随机 id 。\nPOST _bulk { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my-logs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34; } } { \u0026#34;title\u0026#34;: \u0026#34;The Garden of Words\u0026#34;, \u0026#34;year\u0026#34;: 2013 } { \u0026#34;index\u0026#34; : { \u0026#34;_index\u0026#34;: \u0026#34;my-logs\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;3\u0026#34; } } { \u0026#34;title\u0026#34;: \u0026#34;5 Centimeters Per Second\u0026#34;, \u0026#34;year\u0026#34;: 2007 } 列出所有索引 #\n GET _cat/indices?v 打开或关闭与模式匹配的所有索引 #  POST my-logs*/_open POST my-logs*/_close 删除与模式匹配的所有索引 #  DELETE my-logs* 创建索引别名 #  此请求为索引 my-logs-2019-11-13 创建别名 my-logs-today 。\nPUT my-logs-2019-11-13/_alias/my-logs-today 列出所有别名 #  GET _cat/aliases?v 搜索单个索引或与模式匹配的所有索引 #  GET my-logs/_search?q=test GET my-logs*/_search?q=test 获取群集设置，包括默认值 #  GET _cluster/settings?include_defaults=true 更改磁盘使用限制 #  PUT _cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;cluster.routing.allocation.disk.watermark.low\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;cluster.routing.allocation.disk.watermark.high\u0026#34;: \u0026#34;85%\u0026#34; } } 获取群集运行状况 #  GET _cluster/health 列出群集中的节点 #  GET _cat/nodes?v 获取节点统计信息 #  GET _nodes/stats 在存储库中获取快照 #  GET _snapshot/my-repository/_all 生成快照 #  PUT _snapshot/my-repository/my-snapshot 从快照还原 #  POST _snapshot/my-repository/my-snapshot/_restore { \u0026#34;indices\u0026#34;: \u0026#34;-.security\u0026#34;, \u0026#34;include_global_state\u0026#34;: false } 统计索引每个字段的访问次数 #  GET metrics/_field_usage_stats 分析指定索引每个字段的磁盘占用大小 #  POST metrics/_disk_usage?run_expensive_tasks=true ","subcategory":null,"summary":"","tags":null,"title":"常用操作","url":"/easysearch/main/docs/operations/cluster-admin/common/"},{"category":null,"content":"克隆、缩小与拆分索引 #  这三个操作都是\u0026quot;把一个现有索引复制到一个新索引\u0026quot;，区别在于目标索引的主分片数：\n   操作 API 目标主分片数 典型场景     Clone _clone 与源索引相同 复制索引用于测试/实验   Shrink _shrink 源分片数的因子（如 6→3、6→2、6→1） 合并小分片、降低开销   Split _split 源分片数的倍数（如 2→4、2→6） 扩展分片以提高写入并发     前置条件：源索引必须是只读状态。所有操作都会创建一个全新的索引，源索引保持不变。\n 公共前置步骤 #  在执行 Clone / Shrink / Split 之前，源索引必须标记为只读：\nPUT /source-index/_settings { \u0026#34;index.blocks.write\u0026#34;: true } 对于 Shrink 操作，还需要将所有分片的副本迁移到同一个节点：\nPUT /source-index/_settings { \u0026#34;index.routing.allocation.require._name\u0026#34;: \u0026#34;node-1\u0026#34;, \u0026#34;index.blocks.write\u0026#34;: true } 等待所有分片完成迁移：\nGET _cat/shards/source-index?v 确认所有分片都在目标节点上后再执行 Shrink。\nClone：克隆索引 #  将索引完整克隆为一个新索引（相同的分片数、设置和数据）。\nPOST /source-index/_clone/target-index 可以在请求体中为目标索引指定不同的设置：\nPOST /source-index/_clone/target-index { \u0026#34;settings\u0026#34;: { \u0026#34;index.number_of_replicas\u0026#34;: 2 }, \u0026#34;aliases\u0026#34;: { \u0026#34;my-alias\u0026#34;: {} } } 克隆完成后，记得移除源索引的写入块：\nPUT /source-index/_settings { \u0026#34;index.blocks.write\u0026#34;: false } Shrink：缩小索引 #  将索引缩小为更少的主分片。目标分片数必须是源分片数的因子。\n例如，一个 12 分片的索引可以缩小为 6、4、3、2 或 1 个分片。\nPOST /logs-000001/_shrink/logs-000001-shrunk { \u0026#34;settings\u0026#34;: { \u0026#34;index.number_of_shards\u0026#34;: 1, \u0026#34;index.number_of_replicas\u0026#34;: 1, \u0026#34;index.routing.allocation.require._name\u0026#34;: null, \u0026#34;index.blocks.write\u0026#34;: null } }  注意：目标索引的设置中通常需要清除 routing.allocation.require 和 blocks.write，否则新索引会继承这些限制。\n Shrink 的工作原理 #  Shrink 使用硬链接（hard link）而不是复制数据，所以速度很快且几乎不消耗额外磁盘空间。具体过程：\n 创建目标索引，主分片数为指定值 将源索引的段（segment）通过硬链接映射到目标索引 对目标索引进行恢复（如同刚打开的索引）  Split：拆分索引 #  将索引拆分为更多的主分片。目标分片数必须是源分片数的倍数。\n例如，一个 2 分片的索引可以拆分为 4、6、8 等。\nPOST /my-index/_split/my-index-split { \u0026#34;settings\u0026#34;: { \u0026#34;index.number_of_shards\u0026#34;: 4 } } Split 的前置条件 #  源索引创建时，index.number_of_routing_shards 决定了拆分的上限。例如：\nPUT /my-index { \u0026#34;settings\u0026#34;: { \u0026#34;index.number_of_shards\u0026#34;: 2, \u0026#34;index.number_of_routing_shards\u0026#34;: 8 } } 此索引最多可以拆分到 8 个分片（2→4→8）。\n公共查询参数 #  三个 API 共享相同的查询参数：\n   参数 类型 默认值 说明     timeout Time 30s 操作超时时间   master_timeout Time 30s 连接主节点的超时时间   wait_for_active_shards String — 等待的活跃分片数（数字、all 或 index-setting）    请求体 #  三个 API 的请求体结构相同：\n   字段 类型 说明     settings Object 目标索引的设置（会覆盖从源索引继承的值）   aliases Object 目标索引的别名定义    操作完成后 #  完成后通常需要：\n 移除源索引的写入块（如果还需要使用源索引） 验证目标索引的数据完整性 如果不再需要源索引，可以删除或关闭它 如果使用了别名，更新别名指向  // 验证文档数 GET /target-index/_count // 通过别名切换 POST _aliases { \u0026quot;actions\u0026quot;: [ { \u0026quot;remove\u0026quot;: { \u0026quot;index\u0026quot;: \u0026quot;source-index\u0026quot;, \u0026quot;alias\u0026quot;: \u0026quot;my-alias\u0026quot; } }, { \u0026quot;add\u0026quot;: { \u0026quot;index\u0026quot;: \u0026quot;target-index\u0026quot;, \u0026quot;alias\u0026quot;: \u0026quot;my-alias\u0026quot; } } ] } 下一步 #\n   索引管理：创建与删除索引  开关索引与索引限制：设置只读限制  别名（Aliases）：零停机切换索引  索引设置：分片与副本配置  ","subcategory":null,"summary":"","tags":null,"title":"克隆、缩小与拆分索引","url":"/easysearch/main/docs/operations/data-management/clone-shrink-split/"},{"category":null,"content":"HanLP Standard 分词器 #  hanlp_standard 分词器使用 HanLP 标准分词模式对中文文本进行分词，适合大多数中文搜索场景。\n需要安装 analysis-hanlp 插件。\n示例 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_standard\u0026#34; } } } } } 相关指南 #    HanLP 分词器  HanLP 标准分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"HanLP 标准分词器（HanLP Standard）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-standard/"},{"category":null,"content":"GeoIP 处理器 #   需要 ingest-geoip 插件\n geoip 处理器根据 IP 地址查询 MaxMind GeoLite2 数据库，为文档添加地理位置信息，包括国家、城市、经纬度、时区和 ASN 等。该处理器非常适合日志分析、流量审计和用户来源可视化等场景。\n语法 #  { \u0026#34;geoip\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;ip\u0026#34; } } 配置参数 #     参数 是否必填 描述     field 必填 包含 IP 地址的源字段名。支持 IPv4 和 IPv6   target_field 可选 存储地理位置信息的目标字段。默认为 geoip   database_file 可选 使用的 MaxMind 数据库文件名。默认为 GeoLite2-City.mmdb   properties 可选 要提取的属性列表。未指定时根据数据库类型使用默认属性集（见下表）   ignore_missing 可选 为 true 时，如果源字段缺失或为 null，则跳过处理不报错。默认为 false   first_only 可选 当源字段为数组时，为 true 仅返回第一个匹配结果；为 false 返回所有匹配结果的列表。默认为 true   description 可选 处理器的简要描述   if 可选 处理器运行的条件   ignore_failure 可选 为 true 时，处理器出错后忽略继续执行。默认为 false   on_failure 可选 处理器失败时运行的处理器列表   tag 可选 处理器的标识标签    支持的数据库 #  处理器根据数据库类型后缀自动识别数据库格式：\n   数据库 文件名 说明     GeoLite2-City GeoLite2-City.mmdb 城市级定位，含经纬度（默认）   GeoLite2-Country GeoLite2-Country.mmdb 国家级定位   GeoLite2-ASN GeoLite2-ASN.mmdb 自治系统编号（ASN）信息    也可以将自定义的 .mmdb 数据库文件放入 plugins/ingest-geoip/ 目录使用。\n可提取属性 #  City 数据库（默认） #     属性 说明 默认提取     continent_name 所在大洲 ✅   country_iso_code ISO 国家代码 ✅   country_name 国家名称 ✅   region_iso_code ISO 3166-2 地区代码（如 CN-BJ） ✅   region_name 地区/省份名称 ✅   city_name 城市名称 ✅   location 经纬度坐标（{\u0026quot;lat\u0026quot;: ..., \u0026quot;lon\u0026quot;: ...}） ✅   timezone 时区 ❌   ip 格式化的 IP 地址 ❌    Country 数据库 #     属性 说明 默认提取     continent_name 所在大洲 ✅   country_iso_code ISO 国家代码 ✅   country_name 国家名称 ✅   ip 格式化的 IP 地址 ❌    ASN 数据库 #     属性 说明 默认提取     ip 格式化的 IP 地址 ✅   asn 自治系统编号 ✅   organization_name 组织名称 ✅   network 网段 CIDR ✅    如何使用 #  步骤 1：安装插件 #  bin/easysearch-plugin install ingest-geoip 安装后重启节点。\n步骤 2：创建管道 #  以下管道使用 City 数据库为包含 IP 地址的文档添加地理位置信息：\nPUT /_ingest/pipeline/geoip_pipeline { \u0026#34;description\u0026#34;: \u0026#34;根据 IP 地址添加地理位置信息\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;geoip\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;client_ip\u0026#34; } } ] } 步骤 3：测试管道 #  POST /_ingest/pipeline/geoip_pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;client_ip\u0026#34;: \u0026#34;8.8.8.8\u0026#34; } } ] } 响应示例：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;client_ip\u0026#34;: \u0026#34;8.8.8.8\u0026#34;, \u0026#34;geoip\u0026#34;: { \u0026#34;continent_name\u0026#34;: \u0026#34;North America\u0026#34;, \u0026#34;country_iso_code\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;country_name\u0026#34;: \u0026#34;United States\u0026#34;, \u0026#34;region_iso_code\u0026#34;: \u0026#34;US-CA\u0026#34;, \u0026#34;region_name\u0026#34;: \u0026#34;California\u0026#34;, \u0026#34;city_name\u0026#34;: \u0026#34;Mountain View\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 37.386, \u0026#34;lon\u0026#34;: -122.0838 } } } } } ] } 使用 ASN 数据库 #  PUT /_ingest/pipeline/geoip_asn_pipeline { \u0026#34;description\u0026#34;: \u0026#34;根据 IP 地址添加 ASN 信息\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;geoip\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;client_ip\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;asn_info\u0026#34;, \u0026#34;database_file\u0026#34;: \u0026#34;GeoLite2-ASN.mmdb\u0026#34; } } ] } 选择特定属性 #  只提取国家代码和城市名称：\n{ \u0026#34;geoip\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;client_ip\u0026#34;, \u0026#34;properties\u0026#34;: [\u0026#34;country_iso_code\u0026#34;, \u0026#34;city_name\u0026#34;] } } 结合多个数据库 #  同时获取城市定位和 ASN 信息：\nPUT /_ingest/pipeline/geoip_full_pipeline { \u0026#34;description\u0026#34;: \u0026#34;获取完整的地理位置和 ASN 信息\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;geoip\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;client_ip\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;geo\u0026#34; } }, { \u0026#34;geoip\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;client_ip\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;asn\u0026#34;, \u0026#34;database_file\u0026#34;: \u0026#34;GeoLite2-ASN.mmdb\u0026#34; } } ] } 注意事项 #   如果 IP 地址在数据库中未找到，处理器不会添加 target_field（不会报错） 查询结果会缓存在内存中（默认缓存 1000 条），可通过节点设置 ingest.geoip.cache_size 调整缓存大小 数据库默认通过内存映射文件（mmap）加载；设置系统属性 es.geoip.load_db_on_heap=true 可将数据库完整加载到堆内存  ","subcategory":null,"summary":"","tags":null,"title":"GeoIP 处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/geoip/"},{"category":null,"content":"搭建集群 #  在深入研究 Easysearch 以及搜索和聚合数据之前，你首先需要创建一个 Easysearch 集群。\nEasysearch 可以作为一个单节点或多节点集群运行。一般来说，配置两者的步骤是非常相似的。本页演示了如何创建和配置一个多节点集群，但只需做一些小的调整，你可以按照同样的步骤创建一个单节点集群。\n要根据你的要求创建和部署一个 Easysearch 集群，重要的是要了解节点发现和集群形成是如何工作的，以及哪些设置对它们有影响。\n有许多方法可以设计一个集群。下面的插图显示了一个基本架构。\n这是一个四节点的集群，有一个专用的主节点，一个专用的协调节点，还有两个数据节点，这两个节点是主节点，也是用来摄取数据的。\n下表提供了节点类型的简要描述。\n   节点类型 描述 最佳实践     Master 管理集群的整体运作并跟踪集群的状态。这包括创建和删除索引，跟踪加入和离开集群的节点，检查集群中每个节点的健康状况（通过运行 ping 请求），并将分片分配给节点。 在三个不同区域的三个专用主节点是几乎所有生产用例的正确方法。这可以确保你的集群永远不会失去法定人数。两个节点在大部分时间都是空闲的，除非一个节点宕机或需要一些维护。   Data 存储和搜索数据。在本地分片上执行所有与数据有关的操作（索引、搜索、聚合）。这些是你的集群的工作节点，需要比其他任何节点类型更多的磁盘空间。 当你添加数据节点时，保持它们在各区之间的平衡。例如，如果你有三个区，以三的倍数添加数据节点，每个区一个。我们建议使用存储和内存重的节点。    默认情况下，每个节点是一个主节点和数据节点。决定节点的数量，分配节点类型，并为每个节点类型选择硬件，取决于你的使用情况。你必须考虑到一些因素，如你想保留数据的时间，你的文件的平均大小，你的典型工作负载（索引、搜索、聚合），你的预期性价比，你的风险容忍度，等等。\n在你评估所有这些要求之后，我们建议你使用一个管理工具。要开始使用 INFINI Console，请参阅 INFINI Console 文档。\n本页演示了如何处理不同的节点类型。它假设你有一个类似于前面插图的四节点集群。\n前提条件 #  在你开始之前，你必须在你的所有节点上安装和配置 Easysearch。有关可用选项的信息，请参见 安装和配置。\n完成后，使用 SSH 连接到每个节点，然后打开 config/easysearch.yml 文件。\n你可以在这个文件中为你的集群设置所有的配置。\nStep 1: 命名集群 #  为集群指定一个唯一的名字。如果你不指定集群名称，它将被默认设置为 easysearch。设置一个描述性的集群名称很重要，特别是如果你想在一个网络内运行多个集群。\n要指定集群名称，请修改下面一行。\n#cluster.name: my-application to\ncluster.name: easy-cluster 在所有的节点上做同样的修改，以确保它们会加入形成一个集群。\nStep 2: 为集群中的每个节点设置节点属性 #  在你命名集群后，为集群中的每个节点设置节点属性。\nMaster node #  给你的主节点一个名字。如果你不指定一个名字，Easysearch 会分配一个机器生成的名字，这使得节点难以监控和排除故障。\nnode.name: easy-master 你也可以明确地指定这个节点是一个主节点。使用 node.roles 进行配置，推荐方式如下：\nnode.roles: [master] 这样就将该节点配置为专用的主节点，不会作为数据节点执行双重任务。\nData nodes #  将两个节点的名称分别改为 easy-d1 和 easy-d2 。\nnode.name: easy-d1 node.name: easy-d2 你可以让它们既成为主节点，也作为数据节点。使用 node.roles 配置：\nnode.roles: [master, data] Step 3: 将集群与特定的 IP 地址绑定 #  network_host 定义了用于绑定节点的 IP 地址。默认情况下，Easysearch 在本地主机上监听，这将集群限制在一个节点上。你也可以使用 _local_ 和 _site_ 来绑定任何环回或站点本地地址，无论是 IPv4 还是 IPv6。\nnetwork.host: [_local_, _site_] 要形成一个多节点的集群，指定节点的 IP 地址。\nnetwork.host: \u0026lt;IP address of the node\u0026gt; 请确保在你的所有节点上配置这些设置。\nStep 4: 为集群配置发现主机 #  现在你已经配置了网络主机，你需要配置发现主机。\nZen Discovery 是内置的、默认的机制，使用 单播来寻找集群中的其他节点。\n一般来说，你可以直接将所有符合主控条件的节点添加到 discovery.seed_hosts 数组中。当一个节点启动时，它会找到其他符合主控条件的节点，确定哪一个是主控，并要求加入集群。\n例如，对于 easy-master ，这一行看起来是这样的。\ndiscovery.seed_hosts: [\u0026#34;\u0026lt;private IP of easy-d1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;private IP of easy-d2\u0026gt;\u0026#34;, \u0026#34;\u0026lt;private IP of easy-c1\u0026gt;\u0026#34;] Step 5: 启动集群 #  设置好配置后，在所有节点上启动 Easysearch。\nsudo systemctl start easysearch.service 请参阅 以服务的形式运行 Easysearch ，了解如何创建和启动服务。\n然后去看日志文件，看看集群的形成情况。\nless /var/log/easysearch/easy-cluster.log 在任何节点上执行以下 _cat 查询，以查看作为集群的所有节点。\ncurl -XGET https://\u0026lt;private-ip\u0026gt;:9200/_cat/nodes?v -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; --insecure ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name x.x.x.x 13 61 0 0.02 0.04 0.05 mi * easy-master x.x.x.x 16 60 0 0.06 0.05 0.05 md - easy-d1 x.x.x.x 34 38 0 0.12 0.07 0.06 md - easy-d2 x.x.x.x 23 38 0 0.12 0.07 0.06 md - easy-c1 为了更好地了解和监控你的集群，使用 cat API。\n(高级) 第 6 步：配置分片分配意识或强制意识 #  如果你的节点分布在几个地理区域，你可以配置分片分配意识，将所有的复制分片分配到一个与主分片不同的区域。\n有了分片分配意识，如果你的一个区域的节点发生故障，你可以保证你的复制分片分布在你的其他区域。它增加了一个容错层，以确保你的数据在区域故障中幸存下来，而不仅仅是单个节点的故障。\n要配置碎片分配意识，请分别向 easy-d1 和 easy-d2 添加区域属性。\nnode.attr.zone: zoneA node.attr.zone: zoneB 更新集群设置。\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.awareness.attributes\u0026#34;: \u0026#34;zone\u0026#34; } } 你可以使用 persistent 或 transient 设置。我们推荐使用 persistent 设置，因为它在集群重启后仍然有效。瞬时设置不会在集群重启时持续存在。\n碎片分配意识试图在多个区中分离主碎片和复制碎片。但是，如果只有一个区是可用的（比如在一个区发生故障后），Easysearch 会将复制分片分配到唯一剩下的区。\n另一个选择是要求主副 shards 永远不被分配到同一个区。这被称为强制意识。\n要配置强制意识，为你的区属性指定所有可能的值。\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.awareness.attributes\u0026#34;: \u0026#34;zone\u0026#34;, \u0026#34;cluster.routing.allocation.awareness.force.zone.values\u0026#34;:[\u0026#34;zoneA\u0026#34;, \u0026#34;zoneB\u0026#34;] } } 现在，如果一个数据节点发生故障，强制意识不会将副本分配给同一区域的节点。相反，集群进入黄色状态，只有当另一个区的节点上线时才会分配副本。\n在我们的两区架构中，如果 easy-d1 和 easy-d2 的利用率低于 50% ，我们就可以使用分配意识，这样他们每个人都有存储容量来分配同一区的复制。 如果不是这样，并且 easy-d1 和 easy-d2 没有容量来容纳所有的主分片和复制分片，我们可以使用强制意识。这种方法有助于确保在发生故障时，Easysearch 不会因为缺乏存储而使你最后剩下的区域超载并锁定你的集群。\n选择分配意识或强制意识取决于你在每个区可能需要多少空间来平衡你的主副分片。\n(高级）第 7 步：建立一个 hot-warm 架构 #  你可以设计一个 hot-warm 架构，首先将你的数据索引到热节点上\u0026ndash;快速而昂贵\u0026ndash;然后在一段时间后将它们转移到暖节点上\u0026ndash;慢速而廉价。\n如果你分析的是很少更新的时间序列数据，并希望将较旧的数据转移到较便宜的存储上，这种架构就很适合。\n这种架构有助于节省存储成本。与其增加热节点的数量并使用快速、昂贵的存储，你可以为你不经常访问的数据增加暖节点。\n要配置一个 hot-warm 的存储架构，分别给 easy-d1 和 easy-d2 添加 temp 属性。\nnode.attr.temp: hot node.attr.temp: warm 你可以将属性名称和值设置为任何你想要的东西，只要它对你所有的热节点和温节点是一致的。\n要给热节点添加一个索引 newindex 。\nPUT newindex { \u0026#34;settings\u0026#34;: { \u0026#34;index.routing.allocation.require.temp\u0026#34;: \u0026#34;hot\u0026#34; } } 请看下面的 newindex 的分片分配。\nGET _cat/shards/newindex?v index shard prirep state docs store ip node new_index 2 p STARTED 0 230b 10.0.0.225 easy-d1 new_index 2 r UNASSIGNED new_index 3 p STARTED 0 230b 10.0.0.225 easy-d1 new_index 3 r UNASSIGNED new_index 4 p STARTED 0 230b 10.0.0.225 easy-d1 new_index 4 r UNASSIGNED new_index 1 p STARTED 0 230b 10.0.0.225 easy-d1 new_index 1 r UNASSIGNED new_index 0 p STARTED 0 230b 10.0.0.225 easy-d1 new_index 0 r UNASSIGNED 在这个例子中，所有的主碎片都被分配到 easy-d1 ，这是我们的热节点。所有的副本碎片都没有分配，因为我们强迫这个索引只分配给热节点。\n要将索引 oldindex 添加到热节点上。\nPUT oldindex { \u0026#34;settings\u0026#34;: { \u0026#34;index.routing.allocation.require.temp\u0026#34;: \u0026#34;warm\u0026#34; } } oldindex 的分片分配：\nGET _cat/shards/oldindex?v index shard prirep state docs store ip node old_index 2 p STARTED 0 230b 10.0.0.74 easy-d2 old_index 2 r UNASSIGNED old_index 3 p STARTED 0 230b 10.0.0.74 easy-d2 old_index 3 r UNASSIGNED old_index 4 p STARTED 0 230b 10.0.0.74 easy-d2 old_index 4 r UNASSIGNED old_index 1 p STARTED 0 230b 10.0.0.74 easy-d2 old_index 1 r UNASSIGNED old_index 0 p STARTED 0 230b 10.0.0.74 easy-d2 old_index 0 r UNASSIGNED 在这种情况下，所有的主分片都被分配到 easy-d2 。同样地，所有的复制分片都没有被分配，因为我们只有一个温暖的节点。\n一个流行的方法是配置你的 index templates，将 index.routing.allocation.require.temp 值设置为 hot 。这样，Easysearch 将你的最新数据存储在热节点上。\n","subcategory":null,"summary":"","tags":null,"title":"集群管理","url":"/easysearch/main/docs/operations/cluster-admin/cluster/"},{"category":null,"content":"用户代理处理器 #  user_agent 处理器用于从用户代理字符串中提取信息，例如客户端使用的浏览器、设备和操作系统。user_agent 处理器特别适用于分析用户行为，并根据用户设备、操作系统和浏览器识别趋势。它还可以帮助解决特定用户代理配置的问题。\n以下是为 user_agent 处理器提供的语法：\n{ \u0026#34;processor\u0026#34;: { \u0026#34;user_agent\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;user_agent\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;user_agent_info\u0026#34; } } } 配置参数 #  下表列出了 user_agent 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含用户代理字符串的字段。   target_field 可选 存储提取的用户代理信息的字段。如果未指定，则信息存储在 user_agent 字段中。   ignore_missing 可选 指定处理器是否应忽略不包含指定 field 的文档。如果设置为 true ，则处理器在 field 不存在时不会修改文档。默认为 false 。   regex_file 可选 包含用于解析用户代理字符串的正则表达式模式的文件。此文件应位于 Easysearch 包中的 config/ingest-user-agent 目录下。如果未指定，则使用默认文件 regexes.yaml。   properties 可选 要从用户代理字符串中提取并添加到 target_field 的属性列表。如果未指定，则使用默认属性 name 、 major 、 minor 、 patch 、 build 、 os 、 os_name 、 os_major 、 os_minor 和 device 。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 user_agent_pipeline 的管道，该管道使用 user_agent 处理器提取用户代理信息：\nPUT _ingest/pipeline/user_agent_pipeline { \u0026#34;description\u0026#34;: \u0026#34;User agent pipeline\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;user_agent\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;user_agent\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;user_agent_info\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/user_agent_pipeline/_simulate { \u0026#34;pipeline\u0026#34;: \u0026#34;user_agent_pipeline\u0026#34;, \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\u0026#34;, \u0026#34;user_agent_info\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Chrome\u0026#34;, \u0026#34;original\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\u0026#34;, \u0026#34;os\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Windows\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;full\u0026#34;: \u0026#34;Windows 10\u0026#34; }, \u0026#34;device\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Other\u0026#34; }, \u0026#34;version\u0026#34;: \u0026#34;58.0.3029.110\u0026#34; } }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-04-25T21:41:28.744407425Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=user_agent_pipeline { \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\u0026#34; } 前一个请求将 user_agent 字符串解析为其组件，并将文档以及包含这些组件的所有文档索引到 testindex1 索引中，如下面的响应所示：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 66, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 65, \u0026#34;_primary_term\u0026#34;: 47 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 响应包括原始的 user_agent 字段和包含设备、操作系统和浏览器信息的解析后的 user_agent_info 字段：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 66, \u0026#34;_seq_no\u0026#34;: 65, \u0026#34;_primary_term\u0026#34;: 47, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\u0026#34;, \u0026#34;user_agent_info\u0026#34;: { \u0026#34;original\u0026#34;: \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\u0026#34;, \u0026#34;os\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Mac OS X\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;10.15.7\u0026#34;, \u0026#34;full\u0026#34;: \u0026#34;Mac OS X 10.15.7\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;Chrome\u0026#34;, \u0026#34;device\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Mac\u0026#34; }, \u0026#34;version\u0026#34;: \u0026#34;90.0.4430.212\u0026#34; } } } ","subcategory":null,"summary":"","tags":null,"title":"用户代理处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/user-agent/"},{"category":null,"content":"开关索引与索引限制 #  关闭索引 #  关闭的索引不消耗集群资源（CPU、内存、文件句柄），但索引数据仍保留在磁盘上。关闭后索引不能读写，但可以随时重新打开。\n典型场景：\n 历史索引暂时不需要查询，但不想删除 修改 Static 类型的索引设置（必须先关闭索引） 降低集群负载  POST /my-index/_close 响应：\n{ \u0026#34;acknowledged\u0026#34;: true, \u0026#34;shards_acknowledged\u0026#34;: true, \u0026#34;indices\u0026#34;: { \u0026#34;my-index\u0026#34;: { \u0026#34;closed\u0026#34;: true } } } 批量关闭：\nPOST /logs-2024-01,logs-2024-02/_close POST /logs-2024-*/_close 查询参数 #     参数 类型 默认值 说明     timeout Time 30s 操作超时时间   master_timeout Time 30s 连接主节点的超时时间   wait_for_active_shards String — 等待的活跃分片数（数字、all 或 index-setting）   expand_wildcards String open 通配符展开策略：open、closed、hidden、none、all   ignore_unavailable Boolean false 是否忽略不存在的索引   allow_no_indices Boolean true 通配符未匹配到索引时是否报错    打开索引 #  POST /my-index/_open 打开后索引会进入恢复流程（重建分片、分配副本），需要等待分片就绪后才能正常读写。\n响应：\n{ \u0026#34;acknowledged\u0026#34;: true, \u0026#34;shards_acknowledged\u0026#34;: true } 查询参数 #     参数 类型 默认值 说明     timeout Time 30s 操作超时时间   master_timeout Time 30s 连接主节点的超时时间   wait_for_active_shards String — 等待的活跃分片数   expand_wildcards String closed 通配符展开策略   ignore_unavailable Boolean false 是否忽略不存在的索引   allow_no_indices Boolean true 通配符未匹配到索引时是否报错    修改 Static 设置的典型流程 #  有些索引设置（如 index.number_of_shards、index.codec）是 Static 类型，只能在创建时或关闭状态下修改：\n// 1. 关闭索引 POST /my-index/_close // 2. 修改 Static 设置 PUT /my-index/_settings { \u0026quot;index.codec\u0026quot;: \u0026quot;best_compression\u0026quot; }\n// 3. 重新打开 POST /my-index/_open 索引限制（Index Block） #\n 索引限制可以精细控制索引的读写行为，比\u0026quot;关闭索引\u0026quot;更灵活。\n添加索引限制 #  PUT /my-index/_block/write 响应：\n{ \u0026#34;acknowledged\u0026#34;: true, \u0026#34;shards_acknowledged\u0026#34;: true, \u0026#34;indices\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;blocked\u0026#34;: true } ] } 可用的限制类型 #     限制类型 对应设置 说明     read_only index.blocks.read_only 完全只读：禁止写入和元数据变更   read index.blocks.read 禁止读操作   write index.blocks.write 禁止写操作（但允许元数据变更）   metadata index.blocks.metadata 禁止元数据修改（settings、mappings 变更等）   read_only_allow_delete index.blocks.read_only_allow_delete 只读但允许删除索引（磁盘空间不足时系统自动设置）    查询参数 #     参数 类型 默认值 说明     timeout Time 30s 操作超时时间   master_timeout Time 30s 连接主节点的超时时间   expand_wildcards String open 通配符展开策略   ignore_unavailable Boolean false 是否忽略不存在的索引   allow_no_indices Boolean true 通配符未匹配到索引时是否报错    通过 Settings API 设置/移除限制 #  也可以直接通过 _settings API 管理限制：\n// 设为只读 PUT /my-index/_settings { \u0026#34;index.blocks.write\u0026#34;: true } // 移除只读 PUT /my-index/_settings { \u0026quot;index.blocks.write\u0026quot;: false } 使用建议 #\n    场景 推荐方式     索引归档，长期不查询 _close 关闭索引   索引仍需查询但禁止写入 _block/write 或 index.blocks.write: true   修改 Static 设置 先 _close，改完再 _open   磁盘空间紧张时自动保护 系统自动设置 read_only_allow_delete，释放空间后手动移除    下一步 #    索引设置：了解 Static 与 Dynamic 设置的完整列表  克隆/缩小/拆分：调整索引分片结构  索引管理：索引的创建与删除  ","subcategory":null,"summary":"","tags":null,"title":"开关索引与索引限制","url":"/easysearch/main/docs/operations/data-management/open-close-index/"},{"category":null,"content":"HanLP 分词器 #  HanLP 是一个功能强大的中文自然语言处理库，通过 analysis-hanlp 插件 集成到 Easysearch 中。该插件提供了 7 种分词模式，覆盖从高速到高精度的各种需求。\n前提条件 #  bin/easysearch-plugin install analysis-hanlp 分词模式一览 #     分词器名称 模式 速度 精度 适用场景      hanlp_standard 标准分词 ★★★ ★★★★ 通用中文分词    hanlp_index 索引分词 ★★★ ★★★ 索引时最大化召回    hanlp_nlp NLP 分词 ★★ ★★★★★ 命名实体识别    hanlp_crf CRF 分词 ★★ ★★★★★ 新词发现    hanlp_n_short N-最短路径 ★★ ★★★★ 歧义消解    hanlp_dijkstra 最短路径 ★★★ ★★★ 快速精确分词    hanlp_speed 极速分词 ★★★★★ ★★ 大数据量高吞吐    索引/搜索推荐搭配 #  PUT my-hanlp-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;hanlp_index_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_index\u0026#34; }, \u0026#34;hanlp_search_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_standard\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;hanlp_index_analyzer\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;hanlp_search_analyzer\u0026#34; } } } } 测试分词 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;hanlp_standard\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国国歌\u0026#34; } 模式选择建议 #     需求 推荐模式     通用场景 hanlp_standard   索引时最大召回 hanlp_index   人名/地名/机构名识别 hanlp_nlp   识别新词（训练语料外的词） hanlp_crf   追求最大吞吐量 hanlp_speed    相关链接 #    文本分析  ","subcategory":null,"summary":"","tags":null,"title":"HanLP 分词器概述","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp/"},{"category":null,"content":"高亮 #  高亮用于在返回结果中突出显示命中的查询词，方便用户快速定位关键信息。\n相关指南 #    高亮   基础用法 #  在查询体中添加 highlight 段即可启用：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;life\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;text_entry\u0026#34;: {} } } } 响应中每条命中文档会带一个 highlight 对象，默认使用 \u0026lt;em\u0026gt; 标签包裹：\n\u0026#34;highlight\u0026#34;: { \u0026#34;text_entry\u0026#34;: [ \u0026#34;my \u0026lt;em\u0026gt;life\u0026lt;/em\u0026gt;, except my \u0026lt;em\u0026gt;life\u0026lt;/em\u0026gt;.\u0026#34; ] }  自定义标签 #  通过 pre_tags / post_tags 自定义包裹标签：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;pre_tags\u0026#34;: [\u0026#34;\u0026lt;strong\u0026gt;\u0026#34;], \u0026#34;post_tags\u0026#34;: [\u0026#34;\u0026lt;/strong\u0026gt;\u0026#34;], \u0026#34;fields\u0026#34;: { \u0026#34;play_name\u0026#34;: {} } } } 也可以使用 \u0026quot;tags_schema\u0026quot;: \u0026quot;styled\u0026quot; 启用内置的多级标签样式（\u0026lt;em class=\u0026quot;hlt1\u0026quot;\u0026gt;、\u0026lt;em class=\u0026quot;hlt2\u0026quot;\u0026gt; 等）。\n 高亮器类型（type） #  Easysearch 支持三种高亮器，通过 type 参数指定：\n   类型 说明 特点     unified 默认，基于 Lucene Unified Highlighter 性能好，支持所有字段类型   plain 标准高亮器 精确度高，适合小字段   fvh Fast Vector Highlighter 需要 term_vector 设置，适合大文本字段    GET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;life\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;unified\u0026#34; } } } } 选择建议 #     场景 推荐类型     通用场景 unified（默认即可）   需要精确片段控制 plain   大文本字段（如文章正文） fvh（需在 mapping 中设置 \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot;）     片段控制参数 #     参数 说明 默认值     fragment_size 每个高亮片段的最大字符数 100   number_of_fragments 返回的最大片段数量 5   no_match_size 无匹配时返回的字段内容长度（0 表示不返回） 0   fragment_offset 片段的起始偏移量（仅 fvh） -   order 片段排序方式，score 表示按相关性排序 无特定排序    GET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;life\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;fragment_size\u0026#34;: 150, \u0026#34;number_of_fragments\u0026#34;: 3, \u0026#34;no_match_size\u0026#34;: 50 } } } }  高级参数 #     参数 说明 默认值     require_field_match 是否只高亮查询中指定的字段 true   highlight_query 使用不同于主查询的查询来生成高亮 主查询   matched_fields 组合多个字段的匹配来生成高亮（仅 fvh） -   phrase_limit 短语匹配的最大数量（仅 fvh） 256   encoder 输出编码方式 default（原文）或 html（HTML 转义）    边界扫描参数（boundary_scanner） #  控制高亮片段如何在句子/段落边界处截断：\n   参数 说明 默认值     boundary_scanner 边界扫描模式 chars（字符级）、sentence（句子级）、word（词级）   boundary_chars 作为边界的字符集 .,!? \\t\\n   boundary_max_scan 扫描边界字符的最大范围 20   boundary_scanner_locale 句子/词级分割的语言区域 系统默认    GET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;question\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;boundary_scanner\u0026#34;: \u0026#34;sentence\u0026#34;, \u0026#34;fragment_size\u0026#34;: 200 } } } }  多字段高亮 #  可以同时高亮多个字段，每个字段可以有不同的配置：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;king\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;play_name\u0026#34;, \u0026#34;text_entry\u0026#34;] } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;play_name\u0026#34;: { \u0026#34;number_of_fragments\u0026#34;: 0 }, \u0026#34;text_entry\u0026#34;: { \u0026#34;fragment_size\u0026#34;: 150, \u0026#34;number_of_fragments\u0026#34;: 3 } } } }  提示：number_of_fragments 设为 0 时返回整个字段内容而非片段，适合短字段（如标题）。\n  同义词/词干：即便查询时启用了同义词或词干还原，高亮依然会基于原始文本中的命中词来生成片段，保证展示对用户友好。\n ","subcategory":null,"summary":"","tags":null,"title":"高亮","url":"/easysearch/main/docs/features/query-dsl/highlight/"},{"category":null,"content":"集群发现配置 #  本页介绍 easysearch.yml 中与节点发现、主节点选举和故障检测相关的配置项。这些都是静态设置，修改后需要重启节点生效。\n discovery.seed_hosts #  discovery.seed_hosts: - 192.168.1.10:9300 - 192.168.1.11:9300 - 192.168.1.12:9300    项目 说明     参数 discovery.seed_hosts   默认值 未设置（自动使用 [\u0026quot;127.0.0.1\u0026quot;, \u0026quot;[::1]\u0026quot;]）   属性 静态   说明 提供集群中候选主节点的地址列表，用于新节点加入时的发现。这是组建多节点集群最关键的配置。未配置时默认连接本地 IPv4 和 IPv6 环回地址    格式说明 #   每个地址格式为 host:port 或 host（省略端口时使用 transport.port 的值，默认 9300） 支持 IP 地址、主机名或域名 只需列出具有 master 角色的节点地址  多种写法：\n# YAML 列表 discovery.seed_hosts: - 192.168.1.10:9300 - 192.168.1.11:9300 - 192.168.1.12:9300 # 单行数组 discovery.seed_hosts: [\u0026quot;192.168.1.10:9300\u0026quot;, \u0026quot;192.168.1.11:9300\u0026quot;]\n# 使用主机名 discovery.seed_hosts:\n master-1.internal:9300 master-2.internal:9300 master-3.internal:9300  # 省略端口（使用默认 9300） discovery.seed_hosts:\n 192.168.1.10 192.168.1.11 注意事项 #    列表中不需要包含本机地址，但包含也无妨。 不需要列出所有节点，只需要列出 master 候选节点即可。 生产环境中始终使用单播（seed hosts），不要依赖组播。   cluster.initial_master_nodes #  cluster.initial_master_nodes: - node-1 - node-2 - node-3    项目 说明     参数 cluster.initial_master_nodes   默认值 []（空列表）   属性 静态   说明 新集群首次启动时，参与初始主节点选举的节点列表。这个设置仅在集群引导（bootstrap）阶段有效    ⚠️ 重要注意事项 #   仅用于首次启动：集群完成首次主节点选举后，应从所有节点的配置中移除此设置。 值是节点名称（node.name），不是 IP 地址。 不要在已运行的集群中添加此设置，否则可能导致脑裂。 列表中的节点必须全部都具有 master 角色。  正确做法：\n# 首次启动时配置（使用 node.name 值） cluster.initial_master_nodes: - master-1 - master-2 - master-3 也可以使用地址格式：\n# 使用 host:transport_port 格式 cluster.initial_master_nodes: - 192.168.1.10:9300 - 192.168.1.11:9300 - 192.168.1.12:9300  discovery.type #  discovery.type: single-node    项目 说明     参数 discovery.type   默认值 zen（多节点集群）   属性 静态   说明 发现类型。设为 single-node 时 Easysearch 形成单节点集群，跳过发现和引导过程       值 行为     zen 默认值，形成多节点集群，通过 discovery.seed_hosts 发现其他节点   single-node 单节点模式，该节点自动成为 master，不会与其他节点组成集群    单节点模式适用场景：\n 本地开发和测试 CI/CD 环境 不需要高可用的小型应用  # 单节点模式完整配置 cluster.name: dev-cluster node.name: dev-node network.host: 0.0.0.0 discovery.type: single-node  故障检测 #  Easysearch 通过故障检测（Fault Detection）机制监控集群中其他节点的健康状态。\ndiscovery.fd.ping_interval #  discovery.fd.ping_interval: 1s    项目 说明     参数 discovery.fd.ping_interval   默认值 1s   属性 静态   说明 节点间心跳检测的频率。缩短间隔可以更快发现节点故障，但会增加网络开销    discovery.fd.ping_timeout #  discovery.fd.ping_timeout: 30s    项目 说明     参数 discovery.fd.ping_timeout   默认值 30s   属性 静态   说明 故障检测的超时时间。超过此时间未收到响应则标记为一次失败    discovery.fd.ping_retries #  discovery.fd.ping_retries: 3    项目 说明     参数 discovery.fd.ping_retries   默认值 3   属性 静态   说明 故障检测的重试次数。连续失败超过此次数后，该节点被判定为离线    故障检测调优建议 #  默认行为：每 1 秒 ping 一次，超时 30 秒，连续 3 次失败后判定离线。即最快在约 3 秒后检测到节点故障。\n 稳定网络环境：默认值即可。 不稳定网络：适当增大 ping_timeout 和 ping_retries，避免频繁误判。 低延迟场景（如同机房）：可缩短 ping_interval 到 500ms，加快故障发现。  # 不稳定网络环境 discovery.fd.ping_interval: 2s discovery.fd.ping_timeout: 60s discovery.fd.ping_retries: 5 # 低延迟高可用场景 discovery.fd.ping_interval: 500ms discovery.fd.ping_timeout: 10s discovery.fd.ping_retries: 3 \n其他发现参数 #     参数 默认值 说明     discovery.seed_providers settings 种子节点提供者类型。settings 表示从 discovery.seed_hosts 读取；file 表示从 $ES_PATH_CONF/unicast_hosts.txt 动态加载（无需重启即可更新节点列表）   discovery.cluster_formation_warning_timeout 10s 集群形成超时后输出警告日志的等待时间     配置示例 #  分布式集群模式 #  3 个独立 Master 节点 + N 个 Data 节点的标准生产架构：\nMaster 节点配置：\ncluster.name: prod-cluster node.name: master-1 node.roles: [master] network.host: 192.168.1.10 http.port: 9200 transport.port: 9300 discovery.seed_hosts: - 192.168.1.10:9300 - 192.168.1.11:9300 - 192.168.1.12:9300 # 仅首次启动时配置，集群建立后移除 cluster.initial_master_nodes: - master-1 - master-2 - master-3 Data 节点配置：\ncluster.name: prod-cluster node.name: data-1 node.roles: [data, ingest] network.host: 192.168.1.20 http.port: 9200 transport.port: 9300 # Data 节点只需指向 Master 节点 discovery.seed_hosts: - 192.168.1.10:9300 - 192.168.1.11:9300 - 192.168.1.12:9300 # Data 节点不需要配置 cluster.initial_master_nodes  Master 使用 3 个就够了，数据节点可以根据需要动态增加。\n 单节点开发模式 #  cluster.name: dev-cluster node.name: dev-node network.host: 0.0.0.0 http.port: 9200 transport.port: 9300 discovery.type: single-node  延伸阅读 #    集群与节点 — 节点角色与命名  网络配置 — 绑定地址与端口  网关与恢复 — 集群重启后的分片恢复行为  集群管理 — 集群管理操作指南  ","subcategory":null,"summary":"","tags":null,"title":"集群发现","url":"/easysearch/main/docs/deployment/config/node-settings/discovery/"},{"category":null,"content":"English Morphology 分析器 #  english_morphology 分析器专为处理复杂的英语文本而设计。与仅执行简单算法剪裁的常规分析器不同，它基于词形还原（Lemmatization）技术，能够精准识别英语词汇的形态变化，并将其还原为词典中的标准原型。\n这确保了用户在搜索单词的不同形态（如动词时态 ran/running、名词单复数 foxes/fox、或不规则变化 feet/foot）时，能够实现精准的跨形态匹配。\n该分析器由以下分词器和分词过滤器组成：\n standard 分词器：去除大部分标点符号，并依据空格和其他常见分隔符对文本进行分割。 lowercase 分词过滤器：将所有词元转换为小写，以确保匹配时不区分大小写。 english_morphology 分词过滤器：执行英语词汇的形态分析，将动词的时态、形容词的比较级以及名词的复数形式映射到其唯一的语义原型。  相关指南（先读这些） #    文本分析：词干提取  文本分析基础  安装 #  英语形态分词器包含在Morphological Analysis插件中。此插件已包含在Easysearch的bundle包中。\nanalysis-morphology插件安装命令如下：\nbin/easysearch-plugin install analysis-morphology 参考样例 #  以下命令创建一个名为 my_morphology_index 的索引，并为 my_field 字段配置俄语形态分词器的索引：\nPUT /my_morphology_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;english_morphology\u0026#34; } } } } 配置自定义分词器 #  在生产环境中，为了兼顾性能和准确度，建议定义一个包含小写化、形态还原和停用词过滤的自定义分析器：\nPUT /my_custom_morphology_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;english_morphology\u0026#34;, \u0026#34;stop\u0026#34; ] } } } } } 产生的词元 #  通过形态分析，不同的词形会被索引为相同的词元。\n以下请求用来检查分词器生成的词元：\nPOST /my_morphology_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;english_morphology\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;A runner was running.\u0026#34; } 返回内容中包含了产生的词元：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;a\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;start_offset\u0026#34;: 2, \u0026#34;end_offset\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;runner\u0026#34;, \u0026#34;start_offset\u0026#34;: 2, \u0026#34;end_offset\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;be\u0026#34;, \u0026#34;start_offset\u0026#34;: 9, \u0026#34;end_offset\u0026#34;: 12, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;start_offset\u0026#34;: 13, \u0026#34;end_offset\u0026#34;: 20, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;start_offset\u0026#34;: 13, \u0026#34;end_offset\u0026#34;: 20, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 } ] } 返回结果分析 #  英语形态分词器能够识别单词的派生关系及动词时态，并同时索引多个相关的原型词元：\n   原始单词 产生的词元(原型) 说明     runner runner, run 派生名词还原：既保留独立名词，又关联到动词词根 run。   was be 不规则动词还原：将助动词 was 还原为原形 be。   running running, run 多重形态识别：索引独立名词形式及动词 run 的分词形式。    与常规分词器的区别 #  在 Elasticsearch 中，处理英语通常有两种主流方式：一种是基于 词干提取（Stemming） 的常规分词器（如 english analyzer）， 另一种是基于 词形还原（Lemmatization） 的形态分词器（如 analysis-morphology 插件）。\n   特性 常规分词器 (基于Stemming) 形态分词器 (基于Lemmatization)     处理方式 启发式剪裁：根据算法(如Porter Stemmer) 剥离后缀(如-ing,-ed,-s) 词典比对：通过内置词典查找单词的真实词源 (Lemma)   结果准确性 较低。容易产生非单词的词根（如 universal 变为 univers） 极高。始终返回具有语义的真实单词 (如saw还原为see或saw)   过度匹配 常见。可能会将 organization 和 organ 还原为同一词根造成误报 极少。基于语法和词典，严格区分语义   性能消耗 极低。算法简单，计算速度极快 较高。需要加载和检索词典，内存和 CPU 消耗略大    演示 #  具体地展示了英语形态分词器的工作原理和使用方法的脚本，详见 俄语形态分词器。\n","subcategory":null,"summary":"","tags":null,"title":"英语形态分析器（English Morphology）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/english-morphology-analyzer/"},{"category":null,"content":"节点扩容 #  Easysearch Operator 支持通过修改 YAML 配置实现快速水平扩容。\n操作步骤 #  修改 Operator YAML 文件中的 replicas 字段值。例如，将集群从 3 节点扩容到 5 节点：\n# 修改前 replicas: 3 # 修改后 replicas: 5 应用修改：\nkubectl apply -f easysearch-cluster.yaml Operator 会并发创建新的节点（如 threenodes-masters-3、threenodes-masters-4），新节点启动后自动加入集群并参与分片分配。\n 注意：扩容前请确保 Kubernetes 集群有足够的计算和存储资源。缩容操作需要谨慎，建议先手动迁移分片后再减少副本数。\n 操作演示 #    autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  ","subcategory":null,"summary":"","tags":null,"title":"节点扩容","url":"/easysearch/main/docs/deployment/install-guide/operator/node_scale/"},{"category":null,"content":"聚合查询 #  聚合函数对一组文档进行计算并返回单个值。通常与 GROUP BY 子句组合使用，对数据进行分组后计算汇总统计量。\n 聚合函数 #  基本聚合函数 #     函数 说明 示例     COUNT(*) 计算所有行数 SELECT COUNT(*) FROM accounts   COUNT(field) 计算非 NULL 值行数 SELECT COUNT(age) FROM accounts   COUNT(DISTINCT field) 计算字段不同值数量 SELECT COUNT(DISTINCT gender) FROM accounts   SUM(expr) 求和 SELECT SUM(balance) FROM accounts   AVG(expr) 算术平均值 SELECT AVG(age) FROM accounts   MAX(expr) 最大值 SELECT MAX(balance) FROM accounts   MIN(expr) 最小值 SELECT MIN(balance) FROM accounts    统计聚合函数 #     函数 同义函数 说明     VAR_POP(expr) VARIANCE(expr) 总体方差   VAR_SAMP(expr) — 样本方差   STDDEV_POP(expr) STD(expr)、STDDEV(expr) 总体标准差   STDDEV_SAMP(expr) — 样本标准差    使用示例 #  -- 基本聚合 SELECT COUNT(*), SUM(balance), AVG(age), MAX(balance), MIN(balance) FROM accounts \u0026ndash; 分组聚合 SELECT gender, COUNT(*) AS cnt, AVG(age) AS avg_age FROM accounts GROUP BY gender\n\u0026ndash; 聚合表达式 SELECT gender, SUM(age) * 2 AS double_sum FROM accounts GROUP BY gender\n\u0026ndash; 表达式作为聚合参数 SELECT gender, SUM(age * 2) AS sum_double FROM accounts GROUP BY gender COUNT 的特殊形式 #\n    形式 行为     COUNT(field) 仅计数字段值非 NULL 的行   COUNT(*) 计数所有输入行（包含 NULL）   COUNT(1) 等同于 COUNT(*)，任何非 NULL 字面量均可   COUNT(DISTINCT field) 计数字段的不重复值    SELECT COUNT(DISTINCT gender), COUNT(gender) FROM accounts    COUNT(DISTINCT gender) COUNT(gender)     2 4     GROUP BY #  GROUP BY 按表达式对结果分组。支持三种写法：\n-- 按字段名 SELECT gender, SUM(age) FROM accounts GROUP BY gender \u0026ndash; 按 SELECT 列表中的序号 SELECT gender, SUM(age) FROM accounts GROUP BY 1\n\u0026ndash; 按表达式 SELECT ABS(account_number), SUM(age) FROM accounts GROUP BY ABS(account_number) \nHAVING #  HAVING 在分组之后对聚合结果进行过滤（WHERE 是在分组之前对行进行过滤）。\n带 GROUP BY 的 HAVING #  SELECT gender, SUM(age) FROM accounts GROUP BY gender HAVING SUM(age) \u0026gt; 100 HAVING 中可以使用 SELECT 别名：\nSELECT gender, SUM(age) AS s FROM accounts GROUP BY gender HAVING s \u0026gt; 100  HAVING 中的聚合函数不要求与 SELECT 中相同。例如 SELECT 使用 SUM()，HAVING 可以用 MIN() 过滤。\n 不带 GROUP BY 的 HAVING #  HAVING 也可以不配合 GROUP BY 使用，适用于对全表聚合结果做条件判断：\nSELECT \u0026#39;Total of age \u0026gt; 100\u0026#39; FROM accounts HAVING SUM(age) \u0026gt; 100  FILTER 子句 #  FILTER 子句为聚合函数设置独立的过滤条件，只有满足条件的行才参与该聚合计算。\n语法 #  aggregation_function(expr) FILTER(WHERE condition) 带 GROUP BY 的 FILTER #  每个聚合可以有不同的过滤条件：\nSELECT AVG(age) FILTER(WHERE balance \u0026gt; 10000) AS high_bal_avg, AVG(age) FILTER(WHERE balance \u0026lt;= 10000) AS low_bal_avg, gender FROM accounts GROUP BY gender 不带 GROUP BY 的 FILTER #  SELECT COUNT(*) AS total, COUNT(*) FILTER(WHERE age \u0026gt; 34) AS over_34 FROM accounts    total over_34     4 1    FILTER 也可以与 DISTINCT COUNT 结合：\nSELECT COUNT(DISTINCT firstname) FILTER(WHERE age \u0026gt; 30) AS cnt FROM accounts  窗口函数（Window Functions） #  窗口函数在一组相关行（\u0026ldquo;窗口\u0026rdquo;）上执行计算，每行保留独立结果，不折叠为单行。\n语法 #  function_name(...) OVER ( [PARTITION BY expr [, ...]] [ORDER BY expr [ASC | DESC] [, ...]] ) 排名函数 #     函数 说明     ROW_NUMBER() 窗口内连续行号，从 1 开始   RANK() 排名，相同值并列，后续排名跳过   DENSE_RANK() 排名，相同值并列，后续排名不跳过    示例 #  -- 按 gender 分区，按 age 降序排名 SELECT firstname, gender, age, ROW_NUMBER() OVER (PARTITION BY gender ORDER BY age DESC) AS row_num, RANK() OVER (PARTITION BY gender ORDER BY age DESC) AS rnk, DENSE_RANK() OVER (PARTITION BY gender ORDER BY age DESC) AS dense_rnk FROM accounts \u0026ndash; 聚合函数也可以作为窗口函数使用 SELECT firstname, age, SUM(age) OVER (ORDER BY age) AS running_sum, AVG(age) OVER () AS global_avg FROM accounts \n相关链接 #    SQL 查询总览  聚合分析基础教程  内置函数参考  ","subcategory":null,"summary":"","tags":null,"title":"聚合查询","url":"/easysearch/main/docs/features/sql/aggregations/"},{"category":null,"content":"索引设置（Index Settings） #  索引级别的设置直接影响写入、查询性能以及资源利用率。本页按功能分类列出关键设置及其默认值，帮助你在建索引或调优时有一个清单。\n查看与修改索引设置 #  // 查看索引的所有设置 GET /my-index/_settings // 查看特定设置 GET /my-index/_settings/index.refresh_interval\n// 动态修改设置（仅限 Dynamic 类型的设置） PUT /my-index/_settings { \u0026quot;index.refresh_interval\u0026quot;: \u0026quot;30s\u0026quot; } \nStatic vs Dynamic：Static 设置只能在索引创建时指定或在关闭索引后修改；Dynamic 设置可以在运行时通过 _settings API 随时修改。\n 分片与副本 #     设置 默认值 类型 说明     index.number_of_shards 1 Static 主分片数量。创建后不可更改（上限由 es.index.max_number_of_shards 控制，默认 1024）   index.number_of_replicas 1 Dynamic 每个主分片的副本数量。最小值 0   index.number_of_routing_shards 等于主分片数 Static 用于 _split 操作的路由分片数，必须 ≥ number_of_shards   index.auto_expand_replicas false Dynamic 根据集群节点数自动扩展副本，例如 \u0026quot;0-5\u0026quot; 或 \u0026quot;0-all\u0026quot;   index.routing_partition_size 1 Dynamic 自定义路由时可以路由到的分片子集大小    设计建议 #   主分片数 创建后无法修改（取模路由会变），需要扩容时通过 新索引 + 重建索引 + 别名切换 实现 对中小数据量索引，避免设置过多主分片；推荐按时间/业务拆索引 副本越多读吞吐越高，但写入与存储成本也越大；写多读少的索引可适当降低副本数  创建索引时指定分片与副本：\nPUT /my-index { \u0026#34;settings\u0026#34;: { \u0026#34;index.number_of_shards\u0026#34;: 3, \u0026#34;index.number_of_replicas\u0026#34;: 2 } } 刷新与近实时 #     设置 默认值 类型 说明     index.refresh_interval 1s Dynamic 刷新间隔。设为 \u0026quot;-1\u0026quot; 可关闭自动刷新    刷新控制\u0026quot;近实时\u0026quot;行为——新写入的文档需要经过一次 refresh 才能被 _search 检索到（详见 写入与存储机制）。\n常见调优场景：\n 高写入吞吐（如批量导入）：临时设为 \u0026quot;-1\u0026quot; 关闭自动刷新，导入完成后手动刷新 对实时性要求不高的索引：设为 \u0026quot;30s\u0026quot; 或 \u0026quot;60s\u0026quot; 以减少刷新开销  // 批量导入时关闭自动刷新 PUT /my-index/_settings { \u0026#34;index.refresh_interval\u0026#34;: \u0026#34;-1\u0026#34; } // 导入完成后恢复并手动刷新 PUT /my-index/_settings { \u0026quot;index.refresh_interval\u0026quot;: \u0026quot;1s\u0026quot; }\nPOST /my-index/_refresh 事务日志（Translog） #\n 事务日志用于保证数据持久性——在 Lucene 提交（flush）之前，所有写入都记录在 translog 中，以便节点崩溃后恢复。\n   设置 默认值 类型 说明     index.translog.durability REQUEST Dynamic REQUEST：每次写入后 fsync（安全但较慢）；ASYNC：异步 fsync（更快但有丢失风险）   index.translog.sync_interval 5s Dynamic ASYNC 模式下的 fsync 间隔   index.translog.flush_threshold_size 512mb Dynamic 当 translog 达到此大小时触发 flush（Lucene 提交）    调优建议：\n 默认 REQUEST 模式最安全，每次写入返回成功即表示数据已持久化 如果可以接受极端情况（节点突然断电）下丢失最近几秒的写入，可改为 ASYNC 以提升写入性能 flush_threshold_size 越大，flush 频率越低，但恢复时间也越长  段合并（Merge） #  写入产生的小段（segment）会被后台合并为更大的段，以维持查询效率。合并策略影响 IO 与 CPU 开销。\n合并策略（Merge Policy） #     设置 默认值 类型 说明     index.merge.policy.floor_segment 2mb Dynamic 小于此值的段会被优先合并   index.merge.policy.max_merge_at_once 10 Dynamic 一次合并最多合并的段数（最小 2）   index.merge.policy.max_merged_segment 5gb Dynamic 合并后单个段的最大大小；超过此值的段不再参与合并   index.merge.policy.expunge_deletes_allowed 10.0 Dynamic 强制合并时允许的已删除文档占比   index.merge.policy.segments_per_tier 10.0 Dynamic 每层允许的段数（最小 2.0）   index.merge.policy.deletes_pct_allowed 33.0 Dynamic 段中已删除文档的最大占比（20.0–50.0），超过则强制合并    合并调度器（Merge Scheduler） #     设置 默认值 类型 说明     index.merge.scheduler.max_thread_count max(1, min(4, CPU/2)) Dynamic 并发合并线程数，基于节点 CPU 核数自动计算   index.merge.scheduler.max_merge_count max_thread_count + 5 Dynamic 最大待合并数量   index.merge.scheduler.auto_throttle true Dynamic 是否自动调节合并 IO 速率     一般情况下，默认策略已经足够。应通过监控观察是否存在长期高合并压力或段数量异常；只有在确有问题时才针对性调整。\n 编解码器与压缩（Codec） #     设置 默认值 类型 说明     index.codec \u0026quot;default\u0026quot; Static 数据压缩编解码器。可选值：\u0026quot;default\u0026quot;（LZ4）、\u0026quot;best_compression\u0026quot;（DEFLATE，压缩比更高但速度稍慢）、\u0026quot;lucene_default\u0026quot;   index.codec.compression_level （默认未设置） Static 压缩级别，仅部分 codec 支持    选择建议：\n 写入密集型 / 对查询延迟敏感：使用默认的 \u0026quot;default\u0026quot;（LZ4） 存储成本敏感 / 冷数据归档：使用 \u0026quot;best_compression\u0026quot;（DEFLATE）  更多关于索引压缩的讨论，参见 索引压缩。\n查询与结果限制 #     设置 默认值 类型 说明     index.max_result_window 10000 Dynamic from + size 的最大值（深分页上限）   index.max_inner_result_window 100 Dynamic 内部命中（inner_hits / top_hits 聚合）的 from + size 上限   index.max_rescore_window 10000 Dynamic rescore 请求的 window_size 上限，默认与 max_result_window 一致   index.max_docvalue_fields_search 100 Dynamic 查询中允许的 docvalue_fields 最大数量   index.max_script_fields 32 Dynamic 查询中允许的 script_fields 最大数量   index.max_terms_count 65536 Dynamic Terms 查询中允许的最大 term 数量   index.max_regex_length 1000 Dynamic 正则表达式查询的最大长度    其他重要设置 #     设置 默认值 类型 说明     index.blocks.read_only false Dynamic 设为 true 则索引变为只读   index.blocks.read_only_allow_delete false Dynamic 只读但允许删除（磁盘空间不足时系统自动设置）   index.blocks.read false Dynamic 禁止读操作   index.blocks.write false Dynamic 禁止写操作   index.blocks.metadata false Dynamic 禁止元数据修改   index.hidden false Dynamic 隐藏索引，不会被通配符匹配到   index.default_pipeline \u0026quot;_none\u0026quot; Dynamic 默认的 摄取管道，所有写入自动经过该管道   index.final_pipeline \u0026quot;_none\u0026quot; Dynamic 最终管道，在 default_pipeline 之后执行，不可被请求级别覆盖   index.search.idle.after 30s Dynamic 搜索空闲超时时间，超过后跳过后台刷新以节省资源   index.highlight.max_analyzed_offset 1000000 Dynamic 高亮分析的最大字符偏移量    模板与默认设置 #  为了保持索引配置的一致性，通常使用 索引模板来：\n 为某类索引（按名称模式匹配）统一设置分片数、副本数、刷新策略等 统一分析器、动态映射策略  在多租户或多环境（开发/测试/生产）场景下，建议通过模板与自动化来管理索引设置，避免手工创建索引时遗漏关键参数。\n小结 #   index.number_of_shards 和 index.number_of_replicas 是最基础的设置，直接影响数据分布与可用性 index.refresh_interval 控制近实时可见性，是写入性能调优的首要旋钮 index.translog.durability 在性能与持久性之间做权衡 段合并设置一般保持默认，除非监控显示合并压力异常 index.codec 在存储成本与查询速度之间选择 用 索引模板统一管理设置，避免逐个索引手动配置  下一步可以继续阅读：\n  别名（Aliases）  容量规划  映射基础  ","subcategory":null,"summary":"","tags":null,"title":"索引设置（Index Settings）","url":"/easysearch/main/docs/operations/data-management/index-settings/"},{"category":null,"content":"系统调优 #  芯片及操作系统兼容性 #  目前已在国产主流芯片及操作系统上进行了验证，分别为 openEuler、统信 UOS、麒麟、龙芯、申威、兆芯。同样也兼容 Windows、 MacOS、 CentOS、 Ubuntu、 RedHat 等常用操作系统。\nJava 兼容性 #  默认情况下 Easysearch 并不包含 JDK, 推荐使用 Java 15.0.1+9 或 Java 17.0.6+10, 最低版本要求为 Java 11, 要使用不同的 Java 安装，请将 JAVA_HOME 环境变量设置为 Java 安装位置或将 JDK 软链接到 Easysearch 安装目录下取名为 jdk。\n例如：\n#设置 JAVA_HOME 环境变量，可放入 ~/.bashrc 或 /etc/profile export JAVA_HOME=/usr/local/jdk #软链接 sudo ln -s /usr/local/jdk /data/easysearch/jdk 网络要求 #  Easysearch 需要打开以下端口:\n   端口 模块说明     9200 REST API   9300 节点间通信    系统参数 #  要保证 Easysearch 运行在最佳状态，其所在服务器的操作系统也需要进行相应的调优，以 Linux 为例。\nsudo tee /etc/security/limits.d/21-infini.conf \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; * soft nofile 1048576 * hard nofile 1048576 * soft memlock unlimited * hard memlock unlimited root soft nofile 1048576 root hard nofile 1048576 root soft memlock unlimited root hard memlock unlimited EOF 内核调优 #  cat \u0026lt;\u0026lt; SETTINGS | sudo tee /etc/sysctl.d/70-infini.conf fs.file-max = 10485760 fs.nr_open = 10485760 vm.max_map_count = 262145 net.core.somaxconn = 65535 net.core.netdev_max_backlog = 65535 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.core.rmem_max = 4194304 net.core.wmem_max = 4194304 net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1 net.ipv4.ip_local_port_range = 1024 65535 net.ipv4.conf.default.accept_redirects = 0 net.ipv4.conf.default.rp_filter = 1 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.send_redirects = 0 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_max_tw_buckets = 300000 net.ipv4.tcp_timestamps = 1 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn_backlog = 65535 net.ipv4.tcp_synack_retries = 0 net.ipv4.tcp_keepalive_intvl = 30 net.ipv4.tcp_keepalive_time = 900 net.ipv4.tcp_keepalive_probes = 3 net.ipv4.tcp_fin_timeout = 10 net.ipv4.tcp_max_orphans = 131072 net.ipv4.tcp_rmem = 4096 4096 16777216 net.ipv4.tcp_wmem = 4096 4096 16777216 net.ipv4.tcp_mem = 786432 3145728 4194304 SETTINGS 执行下面的命令验证配置参数是否合法。\nsysctl -p /etc/sysctl.d/70-infini.conf  您也可以重启操作系统，然后检查配置是否生效。\n ","subcategory":null,"summary":"","tags":null,"title":"系统调优","url":"/easysearch/main/docs/deployment/config/settings/"},{"category":null,"content":"磁盘加密配置指南 #  在合规要求较高的场景（金融、政务、医疗），可能需要对 Easysearch 数据所在磁盘进行静态加密（Encryption at Rest）。\n加密方案对比 #     方案 透明性 性能影响 适用场景     LUKS (dm-crypt) 完全透明 5~15% Linux 原生，推荐   eCryptfs 文件级 15~30% 不推荐（性能差）   硬件自加密 (SED) 完全透明 ~0% 硬件支持时首选   云盘加密 完全透明 ~0% 云环境推荐    LUKS 全盘加密（Linux） #  配置步骤 #  # 1. 安装 cryptsetup yum install -y cryptsetup # CentOS/RHEL apt install -y cryptsetup # Ubuntu/Debian # 2. 加密磁盘（会清除数据！） cryptsetup luksFormat /dev/nvme0n1 # 输入加密密码\n# 3. 打开加密卷 cryptsetup open /dev/nvme0n1 es-data # 创建 /dev/mapper/es-data\n# 4. 格式化并挂载 mkfs.xfs -f /dev/mapper/es-data mkdir -p /data/easysearch mount -o noatime /dev/mapper/es-data /data/easysearch\n# 5. 配置开机自动解锁 # 方式一：密钥文件（适合服务器） dd if=/dev/urandom of=/root/.luks-key bs=256 count=1 chmod 400 /root/.luks-key cryptsetup luksAddKey /dev/nvme0n1 /root/.luks-key\n# /etc/crypttab echo \u0026quot;es-data /dev/nvme0n1 /root/.luks-key luks\u0026quot; \u0026gt;\u0026gt; /etc/crypttab\n# /etc/fstab echo \u0026quot;/dev/mapper/es-data /data/easysearch xfs noatime 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab 性能优化 #\n # 使用 AES-NI 硬件加速（现代 CPU 均支持） cryptsetup open --type luks --allow-discards /dev/nvme0n1 es-data # 验证 AES-NI 是否启用 grep -o aes /proc/cpuinfo | head -1 # 输出 \u0026quot;aes\u0026quot; 表示支持\n# 选择高效的加密参数 cryptsetup luksFormat \u0026ndash;cipher aes-xts-plain64 \u0026ndash;key-size 256 \u0026ndash;hash sha256 /dev/nvme0n1 云环境加密 #\n 阿里云 #  创建云盘时勾选「加密」，选择 KMS 密钥 # 或通过 API： aliyun ecs CreateDisk --Encrypted true --KMSKeyId \u0026lt;key-id\u0026gt; AWS #  创建 EBS 卷时启用加密： aws ec2 create-volume --encrypted --kms-key-id \u0026lt;key-id\u0026gt; ... 腾讯云 #  创建云硬盘时勾选「加密」  云盘加密对 Easysearch 完全透明，无需额外配置，也无明显性能损耗。\n 密钥管理建议 #     建议 说明     使用 KMS 云环境使用云厂商的 KMS 管理密钥   密钥轮换 定期轮换加密密钥   备份密钥 LUKS header 备份，防止密钥丢失导致数据不可恢复   分离存放 密钥不要存放在加密磁盘本身    # 备份 LUKS header（重要！） cryptsetup luksHeaderBackup /dev/nvme0n1 --header-backup-file /backup/luks-header.bak 延伸阅读 #    TLS 安全配置  国密配置  安全模块总览  ","subcategory":null,"summary":"","tags":null,"title":"磁盘加密","url":"/easysearch/main/docs/deployment/advanced-config/disk-encryption/"},{"category":null,"content":"相关性常用策略 #  本页以\u0026quot;配方\u0026quot;的形式给出几个常见场景下的相关性策略，帮助你在默认评分的基础上做业务级调优。\n通用配方：标题 + 正文 + 标签 #  适用：大多数文档检索（文章、博客、知识库等）。\n思路：\n 使用 multi_match 将多个字段一起搜索 提高标题权重，其次是标签，再是正文  示意：\n{ \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;搜索 引擎\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title^3\u0026#34;, \u0026#34;tags^2\u0026#34;, \u0026#34;content\u0026#34; ] } } } 要点：\n 确保字段类型合理（title/content 为 text，tags 可为 keyword 或 text+keyword） 对排序特别重要的字段，优先在 Mapping 里设计好 multi-fields  电商配方：匹配 + 业务信号 #  适用：商品搜索。\n核心元素：\n 文本相关性：标题、品牌、类目、卖点等字段 业务信号：销量、点击率、转化率、上下架状态、库存  常见做法：\n 先通过 bool + multi_match 保证文本相关性 再通过字段 boost 或 function_score 将业务信号以\u0026quot;加分项\u0026quot;的形式叠加进来  注意：\n 下架/无库存商品要在 filter 层过滤掉，而不是靠\u0026quot;权重降低\u0026quot; 对业务信号做归一化（例如映射到 0–1 或有限区间）以避免极端值影响整体排序  示例：按受欢迎度提升权重\n设想有个网站供用户发布博客并且可以让他们为自己喜欢的博客点赞，我们希望将更受欢迎的博客放在搜索结果列表中相对较上的位置，同时全文搜索的评分仍然作为相关度的主要排序依据：\nGET /blogposts/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ] } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;votes\u0026#34;, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34;, \u0026#34;factor\u0026#34;: 0.1 }, \u0026#34;boost_mode\u0026#34;: \u0026#34;sum\u0026#34; } } } 日志配方：时间权重 + 精确过滤 #  适用：日志/监控类场景。\n思路：\n 结构化过滤先行：时间范围、服务名、环境、主机等 全文仅对确实有助于定位问题的字段（如 message）做匹配 对时间施加一定权重（越近的日志权重更高）  实践建议：\n 避免在大范围时间上直接做全文搜素，优先限定最近一段时间 按相关性和时间做综合排序：先看\u0026quot;相关且最近\u0026quot;的日志  地理位置配方：距离衰减 #  适用：需要根据地理位置排序的场景（如度假屋、餐厅、商店等）。\n思路：\n 使用 function_score 查询结合衰减函数（gauss、exp、linear） 根据距离原点的远近调整评分  示例：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;functions\u0026#34;: [ { \u0026#34;gauss\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;origin\u0026#34;: { \u0026#34;lat\u0026#34;: 51.5, \u0026#34;lon\u0026#34;: 0.12 }, \u0026#34;offset\u0026#34;: \u0026#34;2km\u0026#34;, \u0026#34;scale\u0026#34;: \u0026#34;3km\u0026#34; } } }, { \u0026#34;gauss\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;origin\u0026#34;: \u0026#34;50\u0026#34;, \u0026#34;offset\u0026#34;: \u0026#34;50\u0026#34;, \u0026#34;scale\u0026#34;: \u0026#34;20\u0026#34; } }, \u0026#34;weight\u0026#34;: 2 } ] } } } location 语句可以简单理解为：\n 以伦敦市中作为原点 origin 所有距原点 origin 2km 范围内的位置的评分是 1.0 距中心 5km（offset + scale）的位置的评分是 0.5  过滤集提升权重 #  回到忽略 TF/IDF 里处理过的问题，我们希望根据每个度假屋的特性数量来评分，当时我们希望能用缓存的过滤器来影响评分，现在 function_score 查询正好可以完成这件事情。\n到目前为止，我们展现的都是为所有文档应用单个函数的使用方式，现在会用过滤器将结果划分为多个子集（每个特性一个过滤器），并为每个子集使用不同的函数。\n在下面例子中，我们会使用 weight 函数，它与 boost 参数类似可以用于任何查询。有一点区别是 weight 没有被 Lucene 归一化成难以理解的浮点数，而是直接被应用。\n查询的结构需要做相应变更以整合多个函数：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;Barcelona\u0026#34; } }, \u0026#34;functions\u0026#34;: [ { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;features\u0026#34;: \u0026#34;wifi\u0026#34; }}, \u0026#34;weight\u0026#34;: 1 }, { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;features\u0026#34;: \u0026#34;garden\u0026#34; }}, \u0026#34;weight\u0026#34;: 1 }, { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;features\u0026#34;: \u0026#34;pool\u0026#34; }}, \u0026#34;weight\u0026#34;: 2 } ], \u0026#34;score_mode\u0026#34;: \u0026#34;sum\u0026#34; } } } function_score 查询有个 filter 过滤器而不是 query 查询。functions 关键字存储着一个将被应用的函数列表。函数会被应用于和 filter 过滤器（可选的）匹配的文档。pool 比其他特性更重要，所以它有更高 weight。score_mode 指定各个函数的值进行组合运算的方式。\n评分模式 score_mode #  每个函数返回一个结果，所以需要一种将多个结果缩减到单个值的方式，然后才能将其与原始评分 _score 合并。评分模式 score_mode 参数正好扮演这样的角色，它接受以下值：\n multiply：函数结果求积（默认） sum：函数结果求和 avg：函数结果的平均值 max：函数结果的最大值 min：函数结果的最小值 first：使用首个函数（可以有过滤器，也可能没有）的结果作为最终结果  在本例中，我们将每个过滤器匹配结果的权重 weight 求和，并将其作为最终评分结果，所以会使用 sum 评分模式。\n不与任何过滤器匹配的文档会保有其原始评分，_score 值的为 1。\n站内搜索配方：引入用户反馈 #  适用：内容站点、文档中心、社区问答。\n思路：\n 基础检索同\u0026quot;通用配方\u0026quot; 持续采集用户行为信号：  某次查询下被点击的结果 停留时长、跳出情况   将这些信号累积成\u0026quot;内容质量分数\u0026quot;，在排序中作为一项独立的加权因子  要点：\n 行为信号需要做时间衰减（近期行为权重更高） 适合与 AB 实验结合评估效果  调整与验证 #  无论哪种配方，都建议：\n 先在小范围或灰度环境中验证改动 通过 AB 实验、标注样本或业务方评审，观察结果质量是否有明显提升或退化 将相关性策略版本化管理（记录参数与规则集），便于回滚与持续迭代  这些配方并非\u0026quot;银弹\u0026quot;，但可以作为你为不同业务场景设计相关性策略的起点。\n小结 #   不同场景需要不同的相关性策略 通用配方适合大多数文档检索场景 电商场景需要结合业务信号 日志场景需要时间权重和精确过滤 地理位置场景可以使用距离衰减函数 过滤集提升权重适合多条件评分场景 监控用户行为是评价搜索结果质量的关键  下一步可以继续阅读：\n  评分基础  加权与调参  调试与 Explain  ","subcategory":null,"summary":"","tags":null,"title":"相关性常用策略","url":"/easysearch/main/docs/features/fulltext-search/relevance/relevance-recipes/"},{"category":null,"content":"Pinyin First Letter 分词器 #  pinyin_first_letter 分词器将中文字符转换为拼音首字母缩写，例如 \u0026ldquo;中华人民共和国\u0026rdquo; → \u0026ldquo;zhrmghg\u0026rdquo;。适用于拼音首字母搜索场景。\n需要安装 analysis-pinyin 插件。\n示例 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;pinyin_first_letter\u0026#34; } } } } } 相关指南 #    Pinyin 分词器  Pinyin 分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"拼音首字母分词器（Pinyin First Letter）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/pinyin-first-letter/"},{"category":null,"content":"向量搜索与语义搜索 #  向量搜索和语义搜索经常被混用，但它们面向的层次不同。理解它们的区别有助于选择正确的方案。\n概念区分 #     概念 本质 Easysearch 中的对应     向量搜索 在高维空间中查找最近邻 knn_nearest_neighbors 查询   语义搜索 按\u0026quot;含义\u0026quot;而非\u0026quot;关键词\u0026quot;检索 文本 → Embedding → 向量搜索   Hybrid 检索 关键词 + 语义多路召回融合 bool（knn + match）   全文搜索 基于 BM25 的词频匹配 match、multi_match 等    关系：语义搜索 = Embedding 模型 + 向量搜索。向量搜索是底层能力，语义搜索是应用层方案。\n什么时候用什么 #     场景 推荐方案 理由     用户查询关键词明确 全文搜索 BM25 对精确关键词匹配效果最好   用户用自然语言提问 语义搜索 Embedding 能捕获同义词和意图   既要关键词又要语义 Hybrid 检索 两路召回互补，综合效果最好   以图搜图、跨模态 向量搜索 不同模态映射到同一向量空间   RAG / 知识库问答 语义搜索 或 Hybrid 为 LLM 提供高质量上下文   推荐系统 向量搜索 用户/商品特征向量的近邻查找    端到端语义搜索流程 #  ┌──────────────┐ 用户查询 ──────▶ │ Embedding 模型│ ──────▶ 查询向量 └──────────────┘ │ ▼ ┌─────────────────┐ │ knn_nearest_ │ │ neighbors 查询 │ └─────────────────┘ │ ▼ 按语义相似度排序的结果  写入阶段：将文本通过 Embedding 模型转为向量，连同原始字段一起写入索引 查询阶段：将用户查询通过同一模型转为向量，使用 knn_nearest_neighbors 检索 融合阶段（可选）：将向量相似度得分与 BM25 得分加权融合  Embedding 模型的接入方式参见 Embedding 服务集成。\nHybrid 检索示例 #  POST /my-index/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.03, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 100 } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;分布式搜索引擎\u0026#34;, \u0026#34;boost\u0026#34;: 2.0 } } } ] } } } 通过 boost 参数调节关键词得分与向量得分的权重比例。\n相关资源 #    向量搜索指南：完整的向量索引与查询操作  k-NN 查询 API：查询参数完整参考  Embedding 服务集成：接入外部 Embedding 模型  向量工作流：Embedding 生成与索引的工作流配置  AI API 集成：Hybrid Search API 等高级功能  ","subcategory":null,"summary":"","tags":null,"title":"向量搜索与语义搜索","url":"/easysearch/main/docs/features/vector-search/vector-and-semantic-search/"},{"category":null,"content":"分页与排序 #  分页与排序看似简单，但在分布式搜索里细节很多。本页聚焦几种常见分页方式的适用场景，以及排序字段选择上的坑。\nfrom/size：小页场景的首选 #  from + size 适合：\n 页码较小（例如前几十页以内）的分页 结果集总量不特别大  特点：\n 使用简单，语义直观（类似 SQL 的 OFFSET/LIMIT） 页码越大，开销越大（需要跳过前面的结果）  参数说明：\n size：显示应该返回的结果数量，默认是 10 from：显示应该跳过的初始结果数量，默认是 0  如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果：\nGET /_search?size=5 GET /_search?size=5\u0026amp;from=5 GET /_search?size=5\u0026amp;from=10 深度分页的问题 #  理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。\n当我们请求结果的第一页（结果从 1 到 10），每一个分片产生前 10 的结果，并且返回给协调节点，协调节点对 50 个结果排序得到全部结果的前 10 个。\n现在假设我们请求第 1000 页——结果从 10001 到 10010。所有都以相同的方式工作，除了每个分片不得不产生前 10010 个结果以外。然后协调节点对全部 50050 个结果排序，最后丢弃掉这些结果中的 50040 个结果。\n可以看到，在分布式系统中，对结果排序的成本随分页的深度成线性上升。这就是为什么 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因。\n建议：\n 用于\u0026quot;前台列表\u0026quot;的普通翻页（如前几页搜索结果） 避免深度分页（例如翻到几百页之后）  深度分页：search_after #  当需要做“滚动式”分页，且页数很深时，推荐使用 search_after：\n 不再用页码，而是“记住上一页最后一条记录的排序键” 下一页从这个排序键之后继续取  适用场景：\n 后台批量浏览或处理大量结果 需要相对稳定的排序顺序  基本用法 #  第一步，执行初始搜索，指定排序字段（必须包含唯一性排序键，如 _id）：\nGET /my_index/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;搜索\u0026#34; } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;date\u0026#34;: \u0026#34;desc\u0026#34; }, { \u0026#34;_id\u0026#34;: \u0026#34;asc\u0026#34; } ] } 获取结果中最后一条的 sort 值，作为下一页的 search_after 参数：\nGET /my_index/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;搜索\u0026#34; } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;date\u0026#34;: \u0026#34;desc\u0026#34; }, { \u0026#34;_id\u0026#34;: \u0026#34;asc\u0026#34; } ], \u0026#34;search_after\u0026#34;: [1609459200000, \u0026#34;doc_42\u0026#34;] } 配合 Point In Time (PIT) 使用 #  在生产环境中，建议将 search_after 与 PIT 配合使用，以保证分页过程中索引视图一致：\nPOST /my_index/_search/point_in_time?keep_alive=1m 返回一个 pit_id，后续分页时携带：\nGET /_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;搜索\u0026#34; } }, \u0026#34;pit\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;pit_id\u0026gt;\u0026#34;, \u0026#34;keep_alive\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;sort\u0026#34;: [ { \u0026#34;date\u0026#34;: \u0026#34;desc\u0026#34; }, { \u0026#34;_id\u0026#34;: \u0026#34;asc\u0026#34; } ], \u0026#34;search_after\u0026#34;: [1609459200000, \u0026#34;doc_42\u0026#34;] }  使用 PIT 时不需要指定索引名称，因为 PIT 已绑定到特定索引。\n 使用要点：\n 必须有一个稳定且唯一的排序组合键（常用：业务字段 + _id） 不能直接跳到任意页，只能一页一页向后推进  大规模扫描：scroll #  在需要对大量数据进行离线处理或导出时，可考虑使用 scroll API：\n 在一段时间内\u0026quot;固定\u0026quot;快照视图 逐批拉取数据进行处理 禁用排序使取回行为更有效率  scroll 查询的典型用法：\nGET /_search?scroll=1m { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {}} } 这个查询的返回包括一个 _scroll_id，它是一个 base64 编码的长字符串。可以用这个 _scroll_id 来获取下一批结果：\nGET /_search/scroll { \u0026#34;scroll\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;scroll_id\u0026#34;: \u0026#34;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==\u0026#34; } scroll 参数告诉 Easysearch 保持搜索的上下文等待另一个 1 分钟。scroll_id 可以在 body 或 URL 里传递，或者作为查询参数传递。\n 注意：scroll API 创建了一个快照，它不会看到在初始搜索请求后对索引所做的更改。它通过保留旧的数据文件来实现这一点，这样即使索引正在被更新，文档看起来仍然像它们在进行初始搜索时一样。\n 不适合用 scroll 的场景：\n 在线用户翻页（会占用资源，不利于扩展） 对实时性要求高的前台查询 需要看到最新数据的场景  排序 #  默认情况下，返回的结果是按照相关性进行排序的——最相关的文档排在最前。相关性得分由一个浮点数进行表示，并在搜索结果中通过 _score 参数返回，默认排序是 _score 降序。\n按照字段的值排序 #  有时，相关性评分对你来说并没有意义。例如，下面的查询返回所有 user_id 字段包含 1 的结果：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;user_id\u0026#34;: 1 } } } } } 这里没有一个有意义的分数：因为我们使用的是 filter（过滤），这表明我们只希望获取匹配 user_id: 1 的文档，并没有试图确定这些文档的相关性。实际上文档将按照随机顺序返回，并且每个文档都会评为零分。\n如果评分为零对你造成了困扰，你可以使用 constant_score 查询进行替代：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;constant_score\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;user_id\u0026#34;: 1 } } } } } 这将让所有文档应用一个恒定分数（默认为 1）。\n按字段值排序 #  通过时间来对文档进行排序是有意义的，最新的文档排在最前。我们可以使用 sort 参数进行实现：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;user_id\u0026#34;: 1 }} } }, \u0026#34;sort\u0026#34;: { \u0026#34;date\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; }} } 你会注意到结果中的两个不同点：\n _score 和 max_score 字段都是 null。计算 _score 的花销巨大，通常仅用于排序；我们并不根据相关性排序，所以记录 _score 是没有意义的。如果无论如何你都要计算 _score，你可以将 track_scores 参数设置为 true。 每个结果中有一个新的名为 sort 的元素，它包含了我们用于排序的值。   提示： 当结果集很大时，可以设置 \u0026quot;track_total_hits\u0026quot;: false 或指定一个上限值（如 \u0026quot;track_total_hits\u0026quot;: 1000）来避免精确计数全部命中文档的开销，提升查询性能。\n 一个简便方法是，你可以指定一个字段用来排序：\n\u0026#34;sort\u0026#34;: \u0026#34;number_of_children\u0026#34; 字段将会默认升序排序，而按照 _score 的值进行降序排序。\n多级排序 #  假定我们想要结合使用 date 和 _score 进行查询，并且匹配的结果首先按照日期排序，然后按照相关性排序：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;tweet\u0026#34;: \u0026#34;manage text search\u0026#34; }}, \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;user_id\u0026#34;: 2 }} } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;date\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; }}, { \u0026#34;_score\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; }} ] } 排序条件的顺序是很重要的。结果首先按第一个条件排序，仅当结果集的第一个 sort 值完全相同时才会按照第二个条件进行排序，以此类推。\n多级排序并不一定包含 _score。你可以根据一些不同的字段进行排序，如地理距离或是脚本计算的特定值。\n多值字段的排序 #  一种情形是字段有多个值的排序，需要记住这些值并没有固有的顺序；一个多值的字段仅仅是多个值的包装，这时应该选择哪个进行排序呢？\n对于数字或日期，你可以将多值字段减为单值，这可以通过使用 min、max、avg 或是 sum 排序模式。例如你可以按照每个 date 字段中的最早日期进行排序：\n\u0026#34;sort\u0026#34;: { \u0026#34;dates\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;min\u0026#34; } } 字符串排序与多字段 #  被分析的字符串字段也是多值字段，但是很少会按照你想要的方式进行排序。如果你想分析一个字符串，如 fine old art，这包含 3 项。我们很可能想要按第一项的字母排序，然后按第二项的字母排序，诸如此类，但是 Easysearch 在排序过程中没有这样的信息。\n你可以使用 min 和 max 排序模式（默认是 min），但是这会导致排序以 art 或是 old，任何一个都不是所希望的。\n为了以字符串字段进行排序，这个字段应该是 keyword 类型（不分词）。但是我们仍需要 text 字段，这样才能以全文进行查询。\n一个简单的方法是用两种方式对同一个字符串进行索引，这将在文档中包括两个字段：text 用于搜索，keyword 用于排序。这就是 multi-fields 的典型用法（详见「Mapping 模式与最佳实践」章节）。\n 警告：以 text 字段排序会消耗大量的内存。获取更多信息请参考相关文档。\n 排序字段选择 #  排序的性能和正确性高度依赖字段设计：\n 对高基数字段排序（如随机字符串）通常成本更高 对文本字段排序（如 text 类型）需要额外存储支持，且排序规则复杂  建议：\n 优先选择 keyword 或数值/日期字段作为排序字段 对需要排序的字段，在 Mapping 设计阶段就考虑好类型与是否开启 doc_values 使用 multi-fields 模式：text 字段用于搜索，keyword 字段用于排序  常见用法：\n 时间倒序：按 @timestamp 或类似字段排序 相关性排序 + 二次排序：先按 _score，再按时间或其他业务字段排序  常见陷阱与实践建议 #   避免在前台暴露“跳到第 N 页”的行为（特别是 N 很大时）；改用“加载更多/滚动加载”等方式 对需要长期浏览大量数据的内部工具，优先用 search_after 或专用扫描接口，而不是盲目用 from/size 排序字段必须在 Mapping 层面设计好，临时在 text 字段上排序往往会导致性能和行为问题  下一步可以继续阅读：\n  高亮  建议与纠错  相关性  参考手册（API 与参数） #    搜索操作概览（功能手册）  分页与深度滚动（异步搜索）（功能手册）  Point In Time / PIT（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"分页与排序","url":"/easysearch/main/docs/features/fulltext-search/pagination-and-sorting/"},{"category":null,"content":"Delimited Payload 分词过滤器 #  delimited_payload 分词过滤器用于在分词过程中解析包含负载的词元。例如，字符串 red|1.5 fast|2.0 car|1.0 会被解析为词元 red（负载为 1.5）、fast（负载为 2.0）和 car（负载为 1.0）。当你的词元包含额外的关联数据（如权重、分数或其他数值），且你打算将这些数据用于评分或自定义查询逻辑时，这个过滤器就特别有用。该过滤器可以处理不同类型的负载，包括整数、浮点数和字符串，并将负载（额外的元数据）附加到词元上。\n在文本分词时，delimited_payload 分词过滤器会解析每个词元，提取负载并将其附加到词元上。后续在查询中可以使用这个负载来影响评分、提升权重或实现其他自定义行为。\n负载以 Base64 编码的字符串形式存储。默认情况下，负载不会随词元一起在查询响应中返回。若要返回负载，你必须配置额外的参数。\n相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  参数说明 #  分隔式负载分词过滤器有两个参数：\n   参数 必需/可选 数据类型 描述     encoding 可选 字符串 指定附加到词元的负载的数据类型。这决定了在分析和查询期间如何解释负载数据。\n有效值为：\n- float：使用 IEEE 754 格式将负载解释为 32 位浮点数（例如，car\\|2.5 中的 2.5）。\n- identity：将负载解释为字符序列（例如，在 user\\|admin 中，admin 被解释为字符串）。\n- int：将负载解释为 32 位整数（例如，priority \\| 1中的1）。\n默认值为 float。   delimiter 可选 字符串 指定在输入文本中分隔词元及其负载的字符。默认值为竖线字符（\\|）。    不带负载存储的分词示例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有分隔式负载过滤器的分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_payload_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;delimited_payload\u0026#34;, \u0026#34;delimiter\u0026#34;: \u0026#34;|\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;float\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;whitespace\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;my_payload_filter\u0026#34;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;red|1.5 fast|2.0 car|1.0\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;fast\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 16, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;, \u0026#34;start_offset\u0026#34;: 17, \u0026#34;end_offset\u0026#34;: 24, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } 带有负载存储的分词示例 #  若要在返回内容中带有负载，需创建一个存储词项向量的索引，并在索引映射中将 term_vector 设置为 with_positions_payloads 或 with_positions_offsets_payloads。例如，以下索引被配置为将负载存储到词项向量内容：\nPUT /visible_payloads { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;with_positions_payloads\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;custom_analyzer\u0026#34; } } }, \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_payload_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;delimited_payload\u0026#34;, \u0026#34;delimiter\u0026#34;: \u0026#34;|\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;float\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;whitespace\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;my_payload_filter\u0026#34; ] } } } } } 你可以使用以下请求将一个文档索引到这个索引中：\nPUT /visible_payloads/_doc/1 { \u0026#34;text\u0026#34;: \u0026#34;red|1.5 fast|2.0 car|1.0\u0026#34; } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /visible_payloads/_termvectors/1 { \u0026#34;fields\u0026#34;: [\u0026#34;text\u0026#34;] } 返回内容中包含生成的词元，其中包括负载信息：\n{ \u0026#34;_index\u0026#34;: \u0026#34;visible_payloads\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;took\u0026#34;: 5, \u0026#34;term_vectors\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;field_statistics\u0026#34;: { \u0026#34;sum_doc_freq\u0026#34;: 3, \u0026#34;doc_count\u0026#34;: 1, \u0026#34;sum_ttf\u0026#34;: 3 }, \u0026#34;terms\u0026#34;: { \u0026#34;car\u0026#34;: { \u0026#34;term_freq\u0026#34;: 1, \u0026#34;tokens\u0026#34;: [ { \u0026#34;position\u0026#34;: 2, \u0026#34;payload\u0026#34;: \u0026#34;P4AAAA==\u0026#34; } ] }, \u0026#34;fast\u0026#34;: { \u0026#34;term_freq\u0026#34;: 1, \u0026#34;tokens\u0026#34;: [ { \u0026#34;position\u0026#34;: 1, \u0026#34;payload\u0026#34;: \u0026#34;QAAAAA==\u0026#34; } ] }, \u0026#34;red\u0026#34;: { \u0026#34;term_freq\u0026#34;: 1, \u0026#34;tokens\u0026#34;: [ { \u0026#34;position\u0026#34;: 0, \u0026#34;payload\u0026#34;: \u0026#34;P8AAAA==\u0026#34; } ] } } } } } ","subcategory":null,"summary":"","tags":null,"title":"分隔符负载分词过滤器（Delimited Payload）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/delimited-payload/"},{"category":null,"content":"Russian Morphology 分析器 #  russian_morphology 分析器专为处理俄语文本而设计。与 standard 分析器不同，它能够识别俄语词汇的形态变化，将单词还原为其词干或原型（Lemmatization）。这使得用户在搜索某个单词的特定形式（如单复数、格的变化）时，能够匹配到该单词的其他形态。\n该分析器由以下分词器和分词过滤器组成：\n standard 分词器：去除大部分标点符号，并依据空格和其他常见分隔符对文本进行分割。 lowercase 分词过滤器：将所有词元转换为小写，以确保匹配时不区分大小写。 russian_morphology 分词过滤器：执行俄语词汇的形态分析，将不同格、性、数的单词映射到统一原型。  相关指南（先读这些） #    文本分析：词干提取  文本分析基础  安装 #  俄语形态分词器包含在Morphological Analysis插件中。此插件已包含在Easysearch的bundle包中。\nanalysis-morphology插件安装命令如下：\nbin/easysearch-plugin install analysis-morphology 参考样例 #  以下命令创建一个名为 my_morphology_index 的索引，并为 my_field 字段配置俄语形态分词器的索引：\nPUT /my_morphology_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;russian_morphology\u0026#34; } } } } 配置自定义分词器 #  在生产环境中，为了兼顾性能和准确度，建议定义一个包含小写化、形态还原和停用词过滤的自定义分析器：\nPUT /my_custom_morphology_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;russian_morphology\u0026#34;, \u0026#34;english_morphology\u0026#34;, \u0026#34;my_stopwords\u0026#34; ] } }, \u0026#34;filter\u0026#34;: { \u0026#34;my_stopwords\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;stopwords\u0026#34;: \u0026#34;_russian_\u0026#34; } } } } } 产生的词元 #  通过形态分析，不同的词形会被索引为相同的词元。\n以下请求用来检查分词器生成的词元：\nPOST /my_morphology_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;russian_morphology\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Китайские автомобили\u0026#34; } 返回内容中包含了产生的词元：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;китайский\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 9, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;автомобиль\u0026#34;, \u0026#34;start_offset\u0026#34;: 10, \u0026#34;end_offset\u0026#34;: 20, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } 返回结果分析 #  分词器会识别出“中国的（复数）”和“汽车（复数）”，并同时索引它们的原型：\n   原始单词 产生的词元(原型) 说明     Китайские китайский 形容词原型   автомобили автомобиль 名词单数原型    由于形态还原的存在，当用户搜索单数 автомобиль 时，能够精准匹配到包含复数 автомобили 的文档，这极大地提升了俄语环境下搜索的召回率。\n更多示例 #  词义多义性 #  在处理某些俄语单词时，分词器会识别出该词形可能对应的多个不同词源（Lemmas）。 当一个单词在语法变形后与其他词的原型或变形发生重合时， 插件会同时索引所有可能的原型，以确保搜索的完整性。\n   原始单词 产生的词元(原型) 说明     Мире мир, миро 词形重合：既可以是“世界/和平”（мир）的单数前置格，也可能是“圣油”（миро）的格位变形。   из из, иза 跨词性重合：既是常用介词“从\u0026hellip;”（из），也可能是专有名词/人名“Иза”的单数第二格。    演示 #  以下脚本具体地展示了俄语形态分词器的工作原理和使用方法。\n#!/bin/sh curl --header \u0026#34;Content-Type:application/json\u0026#34; -XDELETE \u0026#39;http://localhost:9200/rustest\u0026#39; \u0026amp;\u0026amp; echo curl --header \u0026#34;Content-Type:application/json\u0026#34; -XPUT \u0026#39;http://localhost:9200/rustest\u0026#39; -d \u0026#39;{ \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;russian_morphology\u0026#34;, \u0026#34;english_morphology\u0026#34;, \u0026#34;my_stopwords\u0026#34;] } }, \u0026#34;filter\u0026#34;: { \u0026#34;my_stopwords\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;stopwords\u0026#34;: \u0026#34;а,без,более,бы,был,была,были,было,быть,в,вам,вас,весь,во,вот,все,всего,всех,вы,где,да,даже,для,до,его,ее,если,есть,еще,же,за,здесь,и,из,или,им,их,к,как,ко,когда,кто,ли,либо,мне,может,мы,на,надо,наш,не,него,нее,нет,ни,них,но,ну,о,об,однако,он,она,они,оно,от,очень,по,под,при,с,со,так,также,такой,там,те,тем,то,того,тоже,той,только,том,ты,у,уже,хотя,чего,чей,чем,что,чтобы,чье,чья,эта,эти,это,я,a,an,and,are,as,at,be,but,by,for,if,in,into,is,it,no,not,of,on,or,such,that,the,their,then,there,these,they,this,to,was,will,with\u0026#34; } } } } }\u0026#39; \u0026amp;\u0026amp; echo curl --header \u0026#34;Content-Type:application/json\u0026#34; -XPUT \u0026#39;http://localhost:9200/rustest/_mapping\u0026#39; -d \u0026#39;{ \u0026#34;properties\u0026#34; : { \u0026#34;body\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34; : \u0026#34;russian_morphology\u0026#34; }, \u0026#34;text\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34; : \u0026#34;my_analyzer\u0026#34; } } }\u0026#39; \u0026amp;\u0026amp; echo curl --header \u0026#34;Content-Type:application/json\u0026#34; -XPUT \u0026#39;http://localhost:9200/rustest/_doc/1\u0026#39; -d \u0026#39;{\u0026#34;body\u0026#34;: \u0026#34;У московского бизнесмена из автомобиля украли шесть миллионов рублей \u0026#34;}\u0026#39; \u0026amp;\u0026amp; echo curl --header \u0026#34;Content-Type:application/json\u0026#34; -XPUT \u0026#39;http://localhost:9200/rustest/_doc/1\u0026#39; -d \u0026#39;{\u0026#34;body\u0026#34;: \u0026#34;У московского бизнесмена из автомобиля украли шесть миллионов рублей \u0026#34;}\u0026#39; \u0026amp;\u0026amp; echo curl --header \u0026#34;Content-Type:application/json\u0026#34; -XPUT \u0026#39;http://localhost:9200/rustest/_doc/2\u0026#39; -d \u0026#39;{\u0026#34;body\u0026#34;: \u0026#34;Креативное агентство Jvision, запустило сервис, способствующий развитию автомобильного туризма в России.\u0026#34;}\u0026#39; \u0026amp;\u0026amp; echo curl --header \u0026#34;Content-Type:application/json\u0026#34; -XPUT \u0026#39;http://localhost:9200/rustest/_doc/3\u0026#39; -d \u0026#39;{\u0026#34;body\u0026#34;: \u0026#34;Просто авто\u0026#34;}\u0026#39; \u0026amp;\u0026amp; echo curl --header \u0026#34;Content-Type:application/json\u0026#34; -XPUT \u0026#39;http://localhost:9200/rustest/_doc/4\u0026#39; -d \u0026#39;{\u0026#34;body\u0026#34;: \u0026#34;Китайские автомобили вновь заняли в мире первые места в рейтингах\u0026#34;}\u0026#39; \u0026amp;\u0026amp; echo curl --header \u0026#34;Content-Type:application/json\u0026#34; -XPUT \u0026#39;http://localhost:9200/rustest/_doc/5\u0026#39; -d \u0026#39;{\u0026#34;body\u0026#34;: \u0026#34;Январский профицит платежного баланса Китая превысил $100 млрд.\u0026#34;}\u0026#39; \u0026amp;\u0026amp; echo curl --header \u0026#34;Content-Type:application/json\u0026#34; -XPUT \u0026#39;http://localhost:9200/rustest/_doc/6\u0026#39; -d \u0026#39;{\u0026#34;body\u0026#34;: \u0026#34;Китайская корпорация Xinmi представила новый смартфон под названием Astra Pro.\u0026#34;}\u0026#39; \u0026amp;\u0026amp; echo curl --header \u0026#34;Content-Type:application/json\u0026#34; -XPOST \u0026#39;http://localhost:9200/rustest/_refresh\u0026#39; \u0026amp;\u0026amp; echo echo \u0026#34;Should return 5\u0026#34; curl --header \u0026#34;Content-Type:application/json\u0026#34; -s \u0026#39;http://localhost:9200/rustest/_search?pretty=true\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;: {\u0026#34;query_string\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;body:Китай\u0026#34;}}}\u0026#39; | grep \u0026#34;_id\u0026#34; echo \u0026#34;Should return 4, 6\u0026#34; curl --header \u0026#34;Content-Type:application/json\u0026#34; -s \u0026#39;http://localhost:9200/rustest/_search?pretty=true\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;: {\u0026#34;query_string\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;body:Китайский\u0026#34;}}}\u0026#39; | grep \u0026#34;_id\u0026#34; echo \u0026#34;Should return 4\u0026#34; curl --header \u0026#34;Content-Type:application/json\u0026#34; -s \u0026#39;http://localhost:9200/rustest/_search?pretty=true\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;: {\u0026#34;query_string\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;body:первый\u0026#34;}}}\u0026#39; | grep \u0026#34;_id\u0026#34; echo \u0026#34;Should return 1, 4\u0026#34; curl --header \u0026#34;Content-Type:application/json\u0026#34; -s \u0026#39;http://localhost:9200/rustest/_search?pretty=true\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;: {\u0026#34;query_string\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;body:автомобиль\u0026#34;}}}\u0026#39; | grep \u0026#34;_id\u0026#34; echo \u0026#34;Should return 2\u0026#34; curl --header \u0026#34;Content-Type:application/json\u0026#34; -s \u0026#39;http://localhost:9200/rustest/_search?pretty=true\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;: {\u0026#34;query_string\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;body:автомобильный\u0026#34;}}}\u0026#39; | grep \u0026#34;_id\u0026#34; echo \u0026#34;Should return 3\u0026#34; curl --header \u0026#34;Content-Type:application/json\u0026#34; -s \u0026#39;http://localhost:9200/rustest/_search?pretty=true\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;: {\u0026#34;query_string\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;body:авто\u0026#34;}}}\u0026#39; | grep \u0026#34;_id\u0026#34; echo \u0026#34;Should return 1,2,3,4\u0026#34; curl --header \u0026#34;Content-Type:application/json\u0026#34; -s \u0026#39;http://localhost:9200/rustest/_search?pretty=true\u0026#39; -d \u0026#39;{\u0026#34;query\u0026#34;: {\u0026#34;query_string\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;body:авто*\u0026#34;, \u0026#34;analyze_wildcard\u0026#34;: true}}}\u0026#39; | grep \u0026#34;_id\u0026#34; curl \u0026ndash;header \u0026quot;Content-Type:application/json\u0026quot; -XPUT 'http://localhost:9200/rustest/_doc/1' -d '{\u0026quot;text\u0026quot;: \u0026quot;Curiously enough, the only thing that went through the mind of the bowl of petunias as it fell was Oh no, not again.\u0026quot;}' \u0026amp;\u0026amp; echo curl \u0026ndash;header \u0026quot;Content-Type:application/json\u0026quot; -XPUT 'http://localhost:9200/rustest/_doc/2' -d '{\u0026quot;text\u0026quot;: \u0026quot;Many people have speculated that if we knew exactly why the bowl of petunias had thought that we would know a lot more about the nature of the Universe than we do now.\u0026quot;}' \u0026amp;\u0026amp; echo curl \u0026ndash;header \u0026quot;Content-Type:application/json\u0026quot; -XPUT 'http://localhost:9200/rustest/_doc/3' -d '{\u0026quot;text\u0026quot;: \u0026quot;Не повезло только кашалоту, который внезапно возник из небытия в нескольких милях над поверхностью планеты.\u0026quot;}' \u0026amp;\u0026amp; echo curl \u0026ndash;header \u0026quot;Content-Type:application/json\u0026quot; -XPOST 'http://localhost:9200/rustest/_refresh' \u0026amp;\u0026amp; echo echo \u0026quot;Should return 3\u0026quot; curl \u0026ndash;header \u0026quot;Content-Type:application/json\u0026quot; -s 'http://localhost:9200/rustest/_search?pretty=true' -d '{\u0026quot;query\u0026quot;: {\u0026quot;query_string\u0026quot;: {\u0026quot;query\u0026quot;: \u0026quot;text:\u0026amp;#34;миль по поверхности\u0026amp;#34;\u0026quot;, \u0026quot;analyze_wildcard\u0026quot;: true}}}' | grep \u0026quot;_id\u0026quot; echo \u0026quot;Should return 1\u0026quot; curl \u0026ndash;header \u0026quot;Content-Type:application/json\u0026quot; -s 'http://localhost:9200/rustest/_search?pretty=true' -d '{\u0026quot;query\u0026quot;: {\u0026quot;query_string\u0026quot;: {\u0026quot;query\u0026quot;: \u0026quot;text:go\u0026quot;, \u0026quot;analyze_wildcard\u0026quot;: true}}}' | grep \u0026quot;_id\u0026quot; echo \u0026quot;Should return 2\u0026quot; curl \u0026ndash;header \u0026quot;Content-Type:application/json\u0026quot; -s 'http://localhost:9200/rustest/_search?pretty=true' -d '{\u0026quot;query\u0026quot;: {\u0026quot;query_string\u0026quot;: {\u0026quot;query\u0026quot;: \u0026quot;text:thinking\u0026quot;, \u0026quot;analyze_wildcard\u0026quot;: true}}}' | grep \u0026quot;_id\u0026quot; echo \u0026quot;Searching _all field\u0026quot; curl \u0026ndash;header \u0026quot;Content-Type:application/json\u0026quot; -XPOST 'localhost:9200/rustest/_search?pretty' -d '{ \u0026quot;query\u0026quot;: { \u0026quot;query_string\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;Китай\u0026quot;, \u0026quot;analyze_wildcard\u0026quot;: true } } }' \n","subcategory":null,"summary":"","tags":null,"title":"俄语形态分析器（Russian Morphology）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/russian-morphology-analyzer/"},{"category":null,"content":"Python 客户端快速入门 #  本页面帮助你快速跑通 Python 客户端连接 Easysearch 的完整流程。更深入的生产配置请参阅 Python 客户端详细指南。\n推荐：Easysearch 官方 Python 客户端 #  Easysearch 提供了官方 Python 客户端 easysearch-py，基于 Apache 2.0 协议开源。\n安装 #  # 从 GitHub Releases 安装（推荐） pip install https://github.com/infinilabs/easysearch-py/releases/download/v0.1.0/easysearch-0.1.0-py2.py3-none-any.whl # 如需 async/await 支持 pip install \u0026quot;easysearch[async] @ https://github.com/infinilabs/easysearch-py/releases/download/v0.1.0/easysearch-0.1.0-py2.py3-none-any.whl\u0026amp;#34; 建立连接 #\n from easysearch import Easysearch es = Easysearch( [\u0026quot;https://localhost:9200\u0026quot;], http_auth=(\u0026quot;admin\u0026quot;, \u0026quot;your_password\u0026quot;), verify_certs=False, # 开发环境；生产环境请配置 CA 证书 timeout=30, )\n# 验证连接 print(es.info()) 生产环境（使用 CA 证书） #\n from ssl import create_default_context context = create_default_context(cafile=\u0026quot;/path/to/root-ca.pem\u0026quot;) es = Easysearch( [\u0026quot;https://easysearch-host:9200\u0026quot;], ssl_context=context, http_auth=(\u0026quot;admin\u0026quot;, \u0026quot;your_password\u0026quot;), ) 索引文档 #\n doc = { \u0026#34;title\u0026#34;: \u0026#34;Easysearch 入门\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;分布式搜索引擎快速上手\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;搜索\u0026#34;, \u0026#34;教程\u0026#34;], \u0026#34;views\u0026#34;: 100, } resp = es.index(index=\u0026quot;articles\u0026quot;, id=1, body=doc) print(resp[\u0026quot;result\u0026quot;]) # created 搜索 #\n resp = es.search( index=\u0026#34;articles\u0026#34;, body={ \u0026#34;query\u0026#34;: {\u0026#34;match\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;Easysearch\u0026#34;}}, }, ) for hit in resp[\u0026quot;hits\u0026quot;][\u0026quot;hits\u0026quot;]: print(f'{hit[\u0026quot;_score\u0026quot;]:.2f} {hit[\u0026quot;_source\u0026quot;][\u0026quot;title\u0026quot;]}') 批量写入 #\n from easysearch.helpers import bulk actions = [ {\u0026quot;_index\u0026quot;: \u0026quot;articles\u0026quot;, \u0026quot;_id\u0026quot;: i, \u0026quot;_source\u0026quot;: {\u0026quot;title\u0026quot;: f\u0026quot;文章 {i}\u0026quot;, \u0026quot;views\u0026quot;: i * 10}} for i in range(2, 102) ]\nsuccess, errors = bulk(es, actions) print(f\u0026quot;成功写入 {success} 条\u0026quot;) \n备选：使用 elasticsearch-py 兼容连接 #  Easysearch 兼容 Elasticsearch 7.10 API，也可以使用 elasticsearch-py 7.10.x 客户端。\npip install elasticsearch==7.10.1 from elasticsearch import Elasticsearch es = Elasticsearch( [\u0026quot;https://localhost:9200\u0026quot;], http_auth=(\u0026quot;admin\u0026quot;, \u0026quot;your_password\u0026quot;), verify_certs=False, timeout=30, ) \n使用兼容客户端时，API 调用方式相同，但导入路径为 from elasticsearch import Elasticsearch，helpers 为 from elasticsearch.helpers import bulk。\n 注意事项 #     事项 说明     推荐客户端 easysearch-py（官方客户端，包名 easysearch）   兼容客户端 elasticsearch==7.10.1 也可正常使用   证书 开发环境可 verify_certs=False；生产环境应配置 CA 证书   多节点 传入列表 [\u0026quot;https://node1:9200\u0026quot;, \u0026quot;https://node2:9200\u0026quot;]    相关文档 #    easysearch-py GitHub  Python 客户端完整指南：聚合、更新删除、连接池等完整用法  使用 Curl 访问 Easysearch  入门教程  ","subcategory":null,"summary":"","tags":null,"title":"使用 Python Client 连接 Easysearch","url":"/easysearch/main/docs/quick-start/connect/python-client/"},{"category":null,"content":"Windows 环境下使用 Easysearch #  目前，有多种方案可以在 Windows 下体验 Easysearch。\n前置要求 #   Windows 10 / Windows Server 2016 或更高版本 至少 4 GB 可用内存 JDK 11+（推荐 JDK 17+，2.0.3 及以上版本要求 JDK 21+）。Bundle 包已内置 JDK，无需单独安装。  方案一：Docker 安装（推荐） #  如果您的 Windows 环境上有 Docker Desktop，可以用最简单的方式启动：\ndocker run -d --name easysearch ` -p 9200:9200 ` -e \u0026#34;EASYSEARCH_INITIAL_ADMIN_PASSWORD=MyTest@2024\u0026#34; ` -e \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; ` infinilabs/easysearch:latest  详细 Docker 配置请参考 Docker 环境下使用 Easysearch。\n 方案二：手工安装（无 HTTPS） #   由于 Windows 环境下默认没有 OpenSSL，生成证书不太方便。如果仅用于开发测试，可以先关闭安全模块快速体验。生产环境请务必启用安全功能（参见方案三）。\n  手工下载 Easysearch，解压到目标目录（如 D:\\easysearch）。 手工下载 JDK，解压到 Easysearch 安装目录下，并将目录名称重命名为 jdk。   也可以下载 Bundle 包（内置 JDK），省去手动配置 JDK 的步骤。Bundle 包 下载地址。\n 用记事本打开 config\\easysearch.yml，修改配置：  # 关闭安全模块（仅开发测试，生产环境不建议） security.enabled: false 双击运行 bin\\easysearch.bat 或在命令行执行：  bin\\easysearch.bat 验证安装：  # 在 PowerShell 中 Invoke-RestMethod -Uri \u0026#34;http://localhost:9200\u0026#34; # 或在浏览器中打开 http://localhost:9200 方案三：通过 Git Bash 安装（支持 HTTPS） #\n 安装 Git for Windows，使用其内置的 Bash 环境来执行初始化脚本，可正常生成证书并启用 HTTPS。\n 注意：以下操作在 Git Bash 终端中执行。\n  通过在线脚本安装 Easysearch  curl -sSL http://get.infini.cloud | bash -s -- -p easysearch -d /d/data/easysearch 下载并配置 JDK  # 下载 JDK curl -# https://cdn.azul.com/zulu/bin/zulu17.54.21-ca-jre17.0.13-win_x64.zip -o /d/opt/jdk.zip # 解压并重命名 cd /d/data/easysearch \u0026amp;\u0026amp; unzip -q /d/opt/jdk.zip mv zulu* jdk\n# 设置环境变量 export JAVA_HOME=/d/data/easysearch/jdk 初始化证书、密码及插件  bin/initialize.sh  初始化过程中会生成随机密码，只会在终端显示一次，请妥善保存。\n 运行 Easysearch  bin\\easysearch.bat 验证安装  # 在 Git Bash 中（使用初始化时输出的密码） curl -ku admin:YOUR_PASSWORD https://localhost:9200 常见问题 #  端口被占用 #  如果 9200 端口被占用，可修改 config\\easysearch.yml：\nhttp.port: 9201 忘记 admin 密码 #  在 Git Bash 中运行：\ncd /d/data/easysearch bin/reset_admin_password.sh 内存不足 #  编辑 config\\jvm.options，调小堆内存：\n-Xms512m -Xmx512m 延伸阅读 #    Docker 部署  测试环境部署  系统调优  ","subcategory":null,"summary":"","tags":null,"title":"Windows","url":"/easysearch/main/docs/deployment/install-guide/windows/"},{"category":null,"content":"URL 解码处理器 #  urldecode 处理器在解码日志数据或其他文本字段中的 URL 编码字符串时非常有用。这可以使数据更易于阅读和分析，尤其是在处理包含特殊字符或空格的 URL 或查询参数时。\n以下是为 urldecode 处理器提供的语法：\n{ \u0026#34;urldecode\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_to_decode\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;decoded_field\u0026#34; } } 配置参数 #  下表列出了 urldecode 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要解码的 URL 编码字符串的字段。   target_field 可选 存储解码字符串的字段。如果未指定，则解码字符串将存储在原始编码字符串相同的字段中。   ignore_missing 可选 指定处理器是否应忽略不包含指定 field 的文档。如果设置为 true ，则处理器忽略 field 中的缺失值并保持 target_field 不变。默认为 false 。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 urldecode_pipeline 的管道，该管道使用 urldecode 处理器解码 encoded_url 字段中的 URL 编码字符串，并将解码后的字符串存储在 decoded_url 字段中：\nPUT _ingest/pipeline/urldecode_pipeline { \u0026#34;description\u0026#34;: \u0026#34;Decode URL-encoded strings\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;urldecode\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;encoded_url\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;decoded_url\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/urldecode_pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;encoded_url\u0026#34;: \u0026#34;https://example.com/search?q=hello%20world\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;decoded_url\u0026#34;: \u0026#34;https://example.com/search?q=hello world\u0026#34;, \u0026#34;encoded_url\u0026#34;: \u0026#34;https://example.com/search?q=hello%20world\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-04-25T23:16:44.886165001Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=urldecode_pipeline { \u0026#34;encoded_url\u0026#34;: \u0026#34;https://example.com/search?q=url%20decode%20test\u0026#34; } 前一个请求将文档索引到索引 testindex1 ，并将包含 encoded_url 字段的全部文档索引，该字段由 urldecode_pipeline 处理以填充 decoded_url 字段，如下面的响应所示：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 67, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 68, \u0026#34;_primary_term\u0026#34;: 47 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 响应包括原始的 encoded_url 字段和 decoded_url 字段：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 67, \u0026#34;_seq_no\u0026#34;: 68, \u0026#34;_primary_term\u0026#34;: 47, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;decoded_url\u0026#34;: \u0026#34;https://example.com/search?q=url decode test\u0026#34;, \u0026#34;encoded_url\u0026#34;: \u0026#34;https://example.com/search?q=url%20decode%20test\u0026#34; } } ","subcategory":null,"summary":"","tags":null,"title":"URL 解码处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/urldecode/"},{"category":null,"content":"SeaTunnel 集成 #   Apache SeaTunnel 是一款高性能分布式数据集成平台，支持海量数据的实时和离线同步。\nSeaTunnel 从 2.3.4 版本开始内置了原生的 INFINI Easysearch Connector，同时支持 Source（读取）和 Sink（写入），使用 easysearch-client 专用客户端，无需依赖 Elasticsearch 兼容层。\n Sink 关键特性：Exactly-Once 语义、CDC（INSERT / UPDATE / DELETE）、HTTPS/TLS。 Source 关键特性：Batch / Stream 模式、Exactly-Once、列投影、并行读取、自定义分片。\n 典型场景 #     场景 Source → Sink     MySQL 全量/增量同步 MySQL → Easysearch   日志接入 Kafka → Easysearch   数据仓库导出 Hive / ClickHouse → Easysearch   跨集群迁移 Elasticsearch → Easysearch   Easysearch 数据导出 Easysearch → ClickHouse / Kafka / 文件   CDC 实时同步 MySQL CDC → Easysearch    安装 SeaTunnel #  # 下载（建议使用 2.3.12） wget https://archive.apache.org/dist/seatunnel/2.3.12/apache-seatunnel-2.3.12-bin.tar.gz tar -xzf apache-seatunnel-2.3.12-bin.tar.gz cd apache-seatunnel-2.3.12 数据类型映射 #     Easysearch 类型 SeaTunnel 类型     STRING / KEYWORD / TEXT STRING   BOOLEAN BOOLEAN   BYTE BYTE   SHORT SHORT   INTEGER INT   LONG LONG   FLOAT / HALF_FLOAT FLOAT   DOUBLE DOUBLE   Date LOCAL_DATE_TIME_TYPE    配置示例：MySQL → Easysearch #  创建任务配置文件 mysql-to-easysearch.conf：\nenv { parallelism = 2 job.mode = \u0026#34;BATCH\u0026#34; } source { Jdbc { url = \u0026quot;jdbc:mysql://localhost:3306/mydb\u0026quot; driver = \u0026quot;com.mysql.cj.jdbc.Driver\u0026quot; user = \u0026quot;root\u0026quot; password = \u0026quot;password\u0026quot; query = \u0026quot;SELECT id, title, content, created_at FROM articles\u0026quot; } }\ntransform { }\nsink { Easysearch { hosts = [\u0026quot;https://localhost:9200\u0026quot;] username = \u0026quot;admin\u0026quot; password = \u0026quot;your-password\u0026quot; index = \u0026quot;articles\u0026quot; tls_verify_certificate = false tls_verify_hostname = false } } 运行：\nbin/seatunnel.sh --config mysql-to-easysearch.conf 配置示例：Kafka → Easysearch（实时） #  env { parallelism = 4 job.mode = \u0026#34;STREAMING\u0026#34; checkpoint.interval = 5000 } source { Kafka { bootstrap.servers = \u0026quot;kafka:9092\u0026quot; topic = \u0026quot;app-logs\u0026quot; format = \u0026quot;json\u0026quot; consumer.group = \u0026quot;seatunnel-es\u0026quot; start_mode = \u0026quot;latest\u0026quot; } }\nsink { Easysearch { hosts = [\u0026quot;https://localhost:9200\u0026quot;] username = \u0026quot;admin\u0026quot; password = \u0026quot;your-password\u0026quot; index = \u0026quot;app-logs-${now('yyyy-MM-dd')}\u0026quot; tls_verify_certificate = false } } 配置示例：ES → Easysearch 迁移 #\n env { parallelism = 4 job.mode = \u0026#34;BATCH\u0026#34; } source { Elasticsearch { hosts = [\u0026quot;http://old-es:9200\u0026quot;] index = \u0026quot;old-index\u0026quot; query = {\u0026quot;match_all\u0026quot;: {}} scroll_time = \u0026quot;5m\u0026quot; scroll_size = 1000 } }\nsink { Easysearch { hosts = [\u0026quot;https://localhost:9200\u0026quot;] username = \u0026quot;admin\u0026quot; password = \u0026quot;your-password\u0026quot; index = \u0026quot;new-index\u0026quot; tls_verify_certificate = false } } 配置示例：Easysearch 作为 Source 读取数据 #\n Easysearch Connector 同时提供 Source 能力，可以从 Easysearch 读取数据同步到其他系统：\nenv { parallelism = 4 job.mode = \u0026#34;BATCH\u0026#34; } source { Easysearch { hosts = [\u0026quot;https://localhost:9200\u0026quot;] username = \u0026quot;admin\u0026quot; password = \u0026quot;your-password\u0026quot; index = \u0026quot;articles\u0026quot; source = [\u0026quot;_id\u0026quot;, \u0026quot;title\u0026quot;, \u0026quot;content\u0026quot;, \u0026quot;views\u0026quot;] query = {\u0026quot;match_all\u0026quot;: {}} scroll_time = \u0026quot;5m\u0026quot; scroll_size = 1000 tls_verify_certificate = false } }\nsink { Console {} } \nSource 名称同样是 Easysearch，支持通配符索引匹配（如 seatunnel-*）和 DSL 查询过滤。\n 配置示例：CDC 实时同步 #  env { parallelism = 2 job.mode = \u0026#34;STREAMING\u0026#34; checkpoint.interval = 5000 } source { MySQL-CDC { hostname = \u0026quot;localhost\u0026quot; port = 3306 username = \u0026quot;root\u0026quot; password = \u0026quot;password\u0026quot; database-name = \u0026quot;mydb\u0026quot; table-name = \u0026quot;orders\u0026quot; } }\nsink { Easysearch { hosts = [\u0026quot;https://localhost:9200\u0026quot;] username = \u0026quot;admin\u0026quot; password = \u0026quot;your-password\u0026quot; index = \u0026quot;orders\u0026quot; primary_keys = [\u0026quot;order_id\u0026quot;] tls_verify_certificate = false } } \nCDC 模式下必须指定 primary_keys，Easysearch Connector 会自动将 INSERT / UPDATE / DELETE 事件映射为对应的文档操作。\n 配置示例：TLS 证书验证 #  sink { Easysearch { hosts = [\u0026#34;https://localhost:9200\u0026#34;] username = \u0026#34;admin\u0026#34; password = \u0026#34;your-password\u0026#34; index = \u0026#34;my-index\u0026#34; tls_keystore_path = \u0026#34;/path/to/easysearch/config/certs/http.p12\u0026#34; tls_keystore_password = \u0026#34;your-keystore-password\u0026#34; } } Sink 参数说明 #     参数 类型 必填 默认值 说明     hosts list ✅ — Easysearch 节点地址，格式 host:port   index string ✅ — 目标索引名，支持字段变量如 seatunnel_${age}   primary_keys list — — 用于生成 _id 的主键字段（CDC 模式必填）   key_delimiter string — _ 复合主键的分隔符   username string — — 认证用户名   password string — — 认证密码   max_batch_size int — 10 批量写入的文档数   max_retry_count int — 3 失败重试次数   tls_verify_certificate bool — true 是否验证 TLS 证书   tls_verify_hostname bool — true 是否验证主机名   tls_keystore_path string — — PEM 或 JKS 密钥库路径   tls_keystore_password string — — 密钥库密码   tls_truststore_path string — — PEM 或 JKS 信任库路径   tls_truststore_password string — — 信任库密码   schema_save_mode enum — CREATE_SCHEMA_WHEN_NOT_EXIST 目标索引处理策略：RECREATE_SCHEMA / CREATE_SCHEMA_WHEN_NOT_EXIST / ERROR_WHEN_SCHEMA_NOT_EXIST / IGNORE   data_save_mode enum — APPEND_DATA 目标数据处理策略：DROP_DATA / APPEND_DATA / ERROR_WHEN_DATA_EXISTS    Source 参数说明 #     参数 类型 必填 默认值 说明     hosts list ✅ — Easysearch 节点地址，格式 host:port   index string ✅ — 源索引名，支持 * 通配符匹配   username string — — 认证用户名   password string — — 认证密码   source list — — 要读取的字段列表，可含 _id；不配置则需配置 schema   query json — — Easysearch DSL 查询条件，控制读取范围   scroll_time string — — Scroll 上下文保持时间   scroll_size int — — 每次 Scroll 请求返回的最大文档数   schema object — — 数据结构定义；不配置则需配置 source   tls_verify_certificate bool — true 是否验证 TLS 证书   tls_verify_hostname bool — true 是否验证主机名   tls_keystore_path string — — PEM 或 JKS 密钥库路径   tls_keystore_password string — — 密钥库密码   tls_truststore_path string — — PEM 或 JKS 信任库路径   tls_truststore_password string — — 信任库密码    注意事项 #     注意项 说明     Connector 名称 Source 和 Sink 名称均为 Easysearch（不是 Elasticsearch），这是 SeaTunnel 的原生连接器   版本要求 SeaTunnel ≥ 2.3.4（建议 2.3.12）   HTTPS Easysearch 默认启用 HTTPS，需配置 tls_verify_certificate = false 或提供证书   Mapping 建议提前创建目标索引的 Mapping，避免自动映射不符合预期   性能 调整 parallelism 和 max_batch_size 以优化吞吐    延伸阅读 #    SeaTunnel Easysearch Sink 官方文档  SeaTunnel Easysearch Source 官方文档  GitHub: connector-easysearch  数据接入  Bulk API  ","subcategory":null,"summary":"","tags":null,"title":"SeaTunnel 集成","url":"/easysearch/main/docs/integrations/third-party/seatunnel/"},{"category":null,"content":"Parent-Child 建模 #  Parent-Child 用来表达\u0026quot;两个文档属于不同类型/生命周期，但又需要建立关联\u0026quot;的场景。相比 nested，它更适合父文档频繁变化 / 子文档数量较多 / 生命周期不同步的情况。\n什么时候考虑 Parent-Child？ #  典型场景：\n 主资源 + 活动记录：例如用户（父）+ 多条行为日志/评论（子） 订单 + 物流/状态变更记录：订单比较稳定，状态记录会持续追加 文档 + 标签/评分：标签或评分变化频率远高于主体文档  这类关系有几个共同特点：\n 父/子文档生命周期不同步（子可以频繁新增/删除，父相对稳定） 子文档数量可能很多，如果全部嵌入父文档会让父文档变得非常庞大 查询时既可能只查子文档，也可能需要\u0026quot;从父找子\u0026quot;或\u0026quot;从子找父\u0026quot;  Parent-Child 与 Nested 的对比 #  可以用下面的方式做一个快速选择：\n  更适合 Nested 的情况：\n 子元素数量有限，整体更新成本可接受 查询几乎总是\u0026quot;连带父文档一起看\u0026quot; 不需要单独对\u0026quot;子\u0026quot;做大规模搜索或独立生命周期管理    更适合 Parent-Child 的情况：\n 子元素数量较多，且经常新增/删除 子文档需要独立参与搜索与统计 父/子有不同的更新/存储策略    Nested 更像\u0026quot;文档内部的结构\u0026quot;， Parent-Child 更像\u0026quot;两个文档集合之间的引用关系\u0026quot;。\n建模要点（概念级） #  底层上，父文档与子文档都存储在同一个索引中，通过一个\u0026quot;连接字段\u0026quot;来描述父/子的关系：\n 父文档：在连接字段中声明自己是某个\u0026quot;父类型\u0026quot; 子文档：在连接字段中声明自己是某个\u0026quot;子类型\u0026quot;，并携带父文档 ID 父/子关系只在这个索引内部生效，不能跨索引  实务上需要注意：\n 父/子文档通常需要落在同一个分片上，以保证查询效率与一致性\n→ 路由策略（routing）必须以父文档为基准，写子文档时复用父文档的 routing。 一旦索引创建好，父/子结构（连接字段定义）很难修改，通常需要通过\u0026quot;新索引 + 重建数据\u0026quot;来演进。   具体的 mapping 与连接字段配置、查询语法，请以参考手册中的\u0026quot;关联查询\u0026quot;与字段类型文档为准，这里聚焦建模思路与取舍。\n 查询与聚合上的典型用法（进阶视角） #  有了父子关系之后，你可以表达例如：\n \u0026ldquo;找到包含满足某条件子文档的所有父文档\u0026rdquo; \u0026ldquo;找到拥有指定父文档的子文档子集\u0026rdquo;  在查询层面，通常会用到几类\u0026quot;关联查询\u0026quot;：\n has_child：从父的角度出发，筛选\u0026quot;拥有满足子查询条件的父文档\u0026quot; has_parent：从子的角度出发，筛选\u0026quot;其父文档满足某条件的子文档\u0026quot; parent_id：根据父 ID 定位子文档  配合聚合，可以：\n 按父文档分组统计子文档属性（例如每个用户的评论数、最近一条评论时间） 在子文档条件过滤后，对父文档层面的属性做分析（例如\u0026quot;最近 7 天内有投诉记录的用户，按地区分布\u0026quot;）  这些需求如果用纯扁平索引或 nested 来表达，要么需要在应用层做二次 join，要么需要索引大量冗余数据。\n性能与复杂度权衡 #  优势：\n 相比把所有信息塞进一个大文档，父/子分开可以：  降低单个文档的更新成本 对子文档的增删改更轻量   更适合大量子文档的场景（日志/评论/记录类）  成本与限制：\n 建模和查询语义更复杂，开发/调试成本更高 需要更谨慎地规划索引与路由策略，否则可能出现父/子落在不同分片的问题 某些查询与聚合在父/子混合场景下会显著更重，属于\u0026quot;昂贵查询\u0026quot;范畴  在部分配置下，如果禁止昂贵查询，对应的关联查询（如 has_child/has_parent）会被直接拒绝    经验建议：\n 如果 nested 能满足需求，优先尝试 nested（语义简单、查询更直观） 只有在 nested 明显不适合（子文档太多 / 生命周期差异很大）时，才考虑 Parent-Child 对使用了父子关系的索引：  控制索引规模与分片数，避免在\u0026quot;超大索引上跑海量父子关联查询\u0026quot; 为关键父子查询单独设计 API 与限流、超时策略，而不是开放为任意 DSL    小结 #   Parent-Child 用来表达\u0026quot;两个文档属于不同类型/生命周期，但又需要建立关联\u0026quot;的场景 相比 nested，它更适合父文档频繁变化、子文档数量较多、生命周期不同步的情况 父/子文档通常需要落在同一个分片上，以保证查询效率与一致性 建模和查询语义更复杂，需要更谨慎地规划索引与路由策略  下一步可以继续阅读：\n  Nested 建模  反范式与权衡  路由（Routing）  参考手册（API 与参数） #    关联查询（功能手册）  Join 字段类型  ","subcategory":null,"summary":"","tags":["最佳实践","数据建模"],"title":"Parent-Child 建模","url":"/easysearch/main/docs/best-practices/data-modeling/parent-child/"},{"category":null,"content":"AI 搜索与向量检索架构实践 #  本页从整体架构的角度，讨论 Easysearch 在 AI/语义搜索系统中的位置：\n 如何规划“全文 + 向量”的多路召回 如何利用 Easysearch 做向量检索与文档存储 如何与上游模型服务、下游应用/LLM 集成  详细的向量字段与 kNN API 参数，请参考 Mapping 与 Reference 中的相关章节，本页重点放在“怎么搭这条链路”。\n1. 一条典型的 AI 搜索链路长什么样？ #  可以把 AI 搜索拆成几段：\n 向量生产：由模型服务（自研或第三方）把文本/图片等转换成向量 向量与文档存储：在 Easysearch 里存放文档及其 Embedding 多路召回：  全文召回：BM25 等传统检索 向量召回：基于 kNN 的相似度检索   重排与融合：  按业务需要对多路召回结果做打分融合或模型重排   应用消费：  直接给用户展示 作为上下文提供给问答系统或大模型    Easysearch 重点负责第 2～4 段中的“存储 + 检索”能力。\n2. 全文 + 向量：多路召回的常见拆法 #  结合 Easysearch，比较常见的做法有：\n 主路向量 + 辅助全文：  用向量检索作为主召回通道，保证语义相似 用全文搜索做补充或过滤（例如强约束、精确匹配字段）   主路全文 + 向量重排：  第一阶段用全文搜索做大规模粗召回 第二阶段对 TopN 文档用向量相似度做重排或融合评分   并行召回 + 融合：  一路全文、一向向量，并行检索 合并结果时按权重或业务策略做打分组合    你可以根据场景（关键词检索 vs 自然语言问答）为不同入口选择不同策略。\n3. Easysearch 在架构中的角色拆分 #  在一个典型部署中，可以把职责划分为：\n 模型服务：负责生成与维护 Embedding（在线/离线） 数据接入层：Logstash、自研服务等，负责把文档和向量写进 Easysearch Easysearch 集群：  存储原始文档、结构化字段 存储向量字段，并对外提供 kNN / Hybrid 搜索接口   应用层 / API 网关：  根据请求类型路由到不同检索策略 负责把搜索结果转成前端/下游系统易消费的形态   （可选）LLM/问答服务：  调用 Easysearch 完成检索，再基于结果生成答案    Easysearch 不负责模型训练，但负责把“检索”这一环做到稳定可控。\n4. 实战：Hybrid Search 配置示例 #   前置条件：需要安装 knn 插件，参考 插件安装。\n 索引设计 #  Easysearch 使用 knn_dense_float_vector 类型存储密集型浮点向量，支持 LSH（近似搜索）和 Exact（精确搜索）两种模型。\nPUT /ai-knowledge-base { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;number_of_replicas\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34; }, \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;embedding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 768, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;L\u0026#34;: 99, \u0026#34;k\u0026#34;: 1 } }, \u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } } } } 参数说明：\n dims：向量维度 model：索引模型，lsh 为近似搜索，exact 为精确搜索 similarity：相似度类型，支持 cosine、l1、l2 L：哈希表数量，增大可提高召回率 k：哈希函数数量，增大可提高精度  Hybrid Search 查询 #  Easysearch 使用 knn_nearest_neighbors 查询进行向量检索，可与 bool 查询组合实现混合检索：\nPOST /ai-knowledge-base/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.34, 0.56, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 100 } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;如何优化搜索性能\u0026#34;, \u0026#34;boost\u0026#34;: 0.3 } } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;技术文档\u0026#34; } } ] } }, \u0026#34;_source\u0026#34;: [\u0026#34;title\u0026#34;, \u0026#34;content\u0026#34;, \u0026#34;category\u0026#34;] } 查询参数说明：\n field：向量字段名 vec.values：查询向量 model：必须与索引时指定的模型一致 similarity：相似度函数 candidates：候选数量，增大可提高召回率   权重调参建议：通过调整 should 子句的 boost 值控制全文与向量的相对权重。关键词检索场景可提高 BM25 权重，自然语言问答场景可减小或移除 should 中的全文查询。\n 5. 性能与成本权衡 #     维度 建议     向量维度 50~768 维兼顾效果与性能；维度越高内存开销越大   LSH 参数 L=99, k=1 为常用起点，追求召回率可增大 L，追求精度可增大 k   分片策略 向量索引建议单分片 5~10GB，避免过多小分片   精确搜索 小数据量（\u0026lt;10 万文档）可用 exact 模型，无需配置额外参数   过滤优化 先用 filter 缩小范围再做向量检索，避免全量扫描    6. 和现有文档的关系 #   字段设计与向量存储：见 向量字段 向量工作流与写入方式：见 向量工作流 RAG 与 LLM 集成细节：见 RAG 与 LLM 集成 Embedding 服务对接：见 Embedding 服务接入  ","subcategory":null,"summary":"","tags":null,"title":"AI 搜索与向量检索架构实践","url":"/easysearch/main/docs/best-practices/ai-search-architecture/"},{"category":null,"content":"Pinyin 分词器 #  pinyin 分词器来自 analysis-pinyin 插件，将中文文本直接转换为拼音词元。与 pinyin 词元过滤器不同，分词器在分词阶段就完成拼音转换。\n前提条件 #  bin/easysearch-plugin install analysis-pinyin 分词器 vs 过滤器 #     组件 说明 适用场景     pinyin 分词器 直接将整段文本转拼音并分词 纯拼音索引   pinyin 词元过滤器 在已有分词结果上转拼音 需要保留原始中文词元    使用示例 #  基本用法 #  PUT my-pinyin-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;pinyin_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;pinyin\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;pinyin_analyzer\u0026#34; } } } } 测试分词 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;pinyin\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国\u0026#34; } 参数 #  分词器支持与 pinyin 词元过滤器相同的参数：\n   参数 默认值 说明     keep_first_letter true 保留拼音首字母   keep_full_pinyin true 保留全拼   keep_joined_full_pinyin false 连接全拼输出   keep_original false 保留原始中文   keep_none_chinese true 保留非中文字符   limit_first_letter_length 16 首字母最大长度   lowercase true 拼音小写   remove_duplicated_term false 移除重复词项    相关链接 #    拼音过滤器 — 作为词元过滤器使用  拼音分析器 — 预配置的拼音分析器  拼音首字母分词器 — 仅首字母模式  文本分析  ","subcategory":null,"summary":"","tags":null,"title":"拼音分词器（Pinyin）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/pinyin/"},{"category":null,"content":"大写处理器 #  uppercase 处理器将特定字段中的所有文本转换为大写字母。\n语法 #  以下是为 uppercase 处理器提供的语法：\n{ \u0026#34;uppercase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_name\u0026#34; } } 配置参数 #  下表列出了 uppercase 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要附加数据的字段名称。支持模板使用。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。如果设置为 true ，则处理器在字段不存在或为 null 时不会修改文档。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。   target_field 可选 要存储解析数据的字段名称。默认为 field 。默认情况下， field 将就地更新。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 uppercase 的管道，该管道将 field 字段中的文本转换为大写：\nPUT _ingest/pipeline/uppercase { \u0026#34;processors\u0026#34;: [ { \u0026#34;uppercase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;name\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/uppercase/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;JOHN\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-08-28T19:54:42.289624792Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=uppercase { \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"大写处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/uppercase/"},{"category":null,"content":"Predicate Token Filter 分词过滤器 #  predicate_token_filter 分词过滤器会根据自定义脚本中定义的条件来评估词元是应该保留还是丢弃。词元的评估是在分析谓词上下文中进行的。此过滤器仅支持内联 Painless 脚本。\n相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  参数说明 #  谓词分词过滤器有一个必需参数：script。该参数提供一个条件，用于评估词元是否应被保留。\n参考样例 #  以下示例请求创建了一个名为 predicate_index 的新索引，并配置了一个带有谓词分词过滤器的分词器。该过滤器指定仅输出长度超过 7 个字符的词元。\nPUT /predicate_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_predicate_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;predicate_token_filter\u0026#34;, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;token.term.length() \u0026gt; 7\u0026#34; } } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;predicate_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_predicate_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /predicate_index/_analyze { \u0026#34;text\u0026#34;: \u0026#34;The Easysearch community is growing rapidly\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;predicate_analyzer\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 14, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;community\u0026#34;, \u0026#34;start_offset\u0026#34;: 15, \u0026#34;end_offset\u0026#34;: 24, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"谓词分词过滤器（Predicate Token Filter）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/predicate-token-filter/"},{"category":null,"content":"裁剪处理器 #  trim 处理器用于从指定的字段中删除前导和尾随空白字符。\n以下是为 trim 处理器提供的语法：\n{ \u0026#34;trim\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_to_trim\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;trimmed_field\u0026#34; } } 配置参数 #  下表列出了 trim 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要修剪文本的字段。   target_field 必填 存储被截断文本的字段。如果未指定，则字段将就地更新。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。如果设置为 true ，则处理器忽略字段中的缺失值，并保持 target_field 不变。默认为 false 。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 trim_pipeline 的管道，该管道使用 trim 处理器从 raw_text 字段中删除前导和尾随空白，并将截断后的文本存储在 trimmed_text 字段中：\nPUT _ingest/pipeline/trim_pipeline { \u0026#34;description\u0026#34;: \u0026#34;Trim leading and trailing white space\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;trim\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;raw_text\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;trimmed_text\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/trim_pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;raw_text\u0026#34;: \u0026#34; Hello, world! \u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;raw_text\u0026#34;: \u0026#34; Hello, world! \u0026#34;, \u0026#34;trimmed_text\u0026#34;: \u0026#34;Hello, world!\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-04-26T20:58:17.418006805Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=trim_pipeline { \u0026#34;raw_text\u0026#34;: \u0026#34; This is a test document. \u0026#34; } 请求将文档索引到索引 testindex1 ，并将所有具有 raw_text 字段的文档索引，由 trim_pipeline 处理，以填充 trimmed_text 字段，如下面的响应所示：\n\u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 68, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 70, \u0026#34;_primary_term\u0026#34;: 47 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 响应包括已去除前后空白的 trimmed_text 字段：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 69, \u0026#34;_seq_no\u0026#34;: 71, \u0026#34;_primary_term\u0026#34;: 47, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;raw_text\u0026#34;: \u0026#34; This is a test document. \u0026#34;, \u0026#34;trimmed_text\u0026#34;: \u0026#34;This is a test document.\u0026#34; } } ","subcategory":null,"summary":"","tags":null,"title":"裁剪处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/trim/"},{"category":null,"content":"Classic 分词过滤器 #  classic 分词过滤器的主要功能是与 classic 分词器配合使用。它通过应用以下常见的转换操作来处理词元，这些操作有助于文本分析和搜索：\n 移除所有格词尾，例如 \u0026ldquo;\u0026rsquo;s\u0026rdquo;。比如，\u0026ldquo;John\u0026rsquo;s\u0026rdquo; 会变为 \u0026ldquo;John\u0026rdquo;。 从首字母缩略词中移除句点。例如，\u0026ldquo;D.A.R.P.A.\u0026rdquo; 会变为 \u0026ldquo;DARPA\u0026rdquo;。  相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参考样例 #  以下示例请求创建了一个名为 custom_classic_filter 的新索引，并配置了一个使用经典分词过滤器的分词器。\nPUT /custom_classic_filter { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_classic\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;classic\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;classic\u0026#34;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /custom_classic_filter/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_classic\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;John\u0026#39;s co-operate was excellent.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;APOSTROPHE\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;co\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 9, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;operate\u0026#34;, \u0026#34;start_offset\u0026#34;: 10, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;was\u0026#34;, \u0026#34;start_offset\u0026#34;: 18, \u0026#34;end_offset\u0026#34;: 21, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;excellent\u0026#34;, \u0026#34;start_offset\u0026#34;: 22, \u0026#34;end_offset\u0026#34;: 31, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"经典分词过滤器（Classic）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/classic/"},{"category":null,"content":"索引的增删改查 #  索引（Index）是 Easysearch 的\u0026quot;逻辑命名空间\u0026quot;，但它背后是不可变段（segment）与固定主分片数等一系列硬约束。结果就是：很多你以为\u0026quot;改一下就行\u0026quot;的变更，最后都会走到\u0026quot;建新索引 + 迁移数据\u0026quot;的路上。\n本页涵盖索引的完整生命周期操作：创建、查看、修改设置/映射、删除，以及重建索引的常见套路。\n创建索引：把关键设置提前定好 #  你当然可以“先写一条文档让索引自动出现”，但在生产环境，更推荐显式创建索引，把关键设置、映射、分析器在写入前一次性确定：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;number_of_replicas\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;tags\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } 你需要提前考虑的常见项：\n 分片与副本：见 索引设置 映射与字段类型：见 Mapping 基础 分析器：见 Mapping 与文本分析  索引命名限制 #  Easysearch 索引命名规则：\n 所有字母必须小写 索引名称不能以 _（下划线）或 -（连字符）开头 索引名称不能包含空格、逗号或以下字符：: \u0026quot; * + / \\ | ? # \u0026gt; \u0026lt;  是否允许“自动创建索引”？ #  对日志类场景，“按天/按月写到新索引”很常见，此时自动创建索引会很省事；但对强管控业务，自动创建索引可能变成“拼写错误导致创建一堆垃圾索引”的事故源头。\n你可以在集群配置里关闭自动创建索引（具体配置项以你部署的 Easysearch 为准），或用更精细的白名单策略只允许某些模式的索引名自动创建。\n删除索引：最需要“安全栅栏”的操作 #  删除索引是不可逆操作（除非你有快照）。基本用法：\nDELETE /my_index 也可以批量删除：\nDELETE /index_one,index_two DELETE /index_* 强烈建议：要求“删除必须写全名” #  为了避免误操作，一般建议开启“删除必须指定明确名称”的保护，禁止 /_all 和通配符这种一把梭：\n 好处：减少“手滑删库”的概率 代价：你需要在批量删除时显式列出目标索引名   具体配置项以你部署的 Easysearch 为准。生产环境建议默认开启这类破坏性操作保护。\n 查看索引信息 #  获取索引的设置、映射和别名 #  GET /my_index 响应中包含索引的完整信息——设置（settings）、映射（mappings）和别名（aliases）：\n{ \u0026#34;my_index\u0026#34;: { \u0026#34;aliases\u0026#34;: {}, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;tags\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } }, \u0026#34;settings\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;number_of_replicas\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;creation_date\u0026#34;: \u0026#34;...\u0026#34; } } } } 查询参数：\n   参数 类型 默认值 说明     flat_settings Boolean false 以扁平格式返回 settings   include_defaults Boolean false 包含默认设置值   local Boolean false 从本地节点返回信息，不查询主节点   master_timeout Time 30s 连接主节点的超时时间    检查索引是否存在 #  使用 HEAD 请求检查索引是否存在，不返回响应体：\nHEAD /my_index  返回 200 表示索引存在 返回 404 表示索引不存在  列出索引 #  使用 Cat API 以表格方式查看索引列表：\nGET _cat/indices?v GET _cat/indices/logs-*?v\u0026amp;s=store.size:desc\u0026amp;h=index,health,status,pri,rep,docs.count,store.size 更新映射（Mapping） #\n 在已有索引上添加新字段：\nPUT /my_index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;new_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } }  注意：已有字段的类型不能修改。如果需要更改字段类型，只能 重建索引。\n 查看索引的当前映射：\nGET /my_index/_mapping 查看特定字段的映射：\nGET /my_index/_mapping/field/title 更多映射概念详见 Mapping 基础。\n什么时候必须“重建索引”？ #  有些变更不能在原索引上“就地修补”，典型例子：\n 修改某个字段的类型（例如 text 改 keyword） 修改分析器/分词链（尤其是影响 token 的变更） 变更会导致“老数据的索引方式”与“新数据的索引方式”不一致  这类变更如果强行在原索引上继续写，会出现非常真实的“新旧数据搜索体验不一致”：同样的查询，新写入的数据命中了，老数据没命中（或反过来）。\n因此正确姿势通常是：\n 建一个新索引（新 settings/mappings/analysis） 把旧索引的数据迁过去（重建索引） 切流量（通常用别名做零停机切换）  重建索引的两种常见方式 #  方式 A：scroll + bulk（通用、可控） #  思路是从旧索引分批读出文档（scroll），再用 bulk 写入新索引：\n 优点：控制力强，方便做节流、过滤、字段变换 缺点：需要你自己写搬运逻辑（或用现成工具）   scroll 与 bulk 的细节分别见： 分页与排序 与 Bulk API。\n 方式 B：_reindex API（如果你的 Easysearch 支持） #  很多部署会提供 _reindex 这类“服务端搬运”的能力：你给出源索引与目标索引，服务端完成复制。\n 优点：简单，少写工具代码 缺点：可控性相对弱；大规模数据仍要注意限速与资源占用  如果你准备在生产使用，建议先在小数据量上压测与演练，再上真实业务索引。\n零停机切换：用别名做“换索引不换应用” #  重建索引的最大痛点通常不是“搬数据”，而是“切索引名”：\n 应用写死了索引名：切换就要发版 多服务、多语言、多团队：协调成本爆炸  解决办法是：应用永远只访问别名，真实索引带版本号或时间戳。\n一个典型套路：\n 创建 my_index_v1，把别名 my_index 指向它 需要变更时创建 my_index_v2 并迁移数据 用一次原子操作把别名从 v1 切到 v2  完整示例与注意事项见： 别名（Aliases）。\n小结 #   生产环境优先显式创建索引，把 settings/mappings/analyzers 提前确定 删除索引务必加“安全栅栏”，尽量避免通配符误删 映射/分析器等关键变更通常需要重建索引 重建索引常用 scroll + bulk 或（若支持）_reindex 零停机切换的核心是：应用只认别名，不认真实索引名  参考手册（API 与参数） #    索引设置  索引模板  索引别名  开关索引与索引限制  克隆/缩小/拆分  Rollover  重建数据（Reindex）  备份还原（快照/恢复）  ","subcategory":null,"summary":"","tags":null,"title":"索引的增删改查","url":"/easysearch/main/docs/operations/data-management/index-management/"},{"category":null,"content":"Condition 分词过滤器 #  condition 分词过滤器是一种特殊类型的过滤器，它允许你根据特定标准有条件地应用其他分词过滤器。这使得在文本分析过程中，你能更精准地控制何时应用特定的分词过滤器。你可以配置多个过滤器，并且只有当满足你定义的条件时，这些过滤器才会被应用。该分词过滤器在进行特定语言处理和特殊字符处理时非常有用。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参数说明 #  要使用条件分词过滤器，必须配置两个参数，具体如下：\n   参数 必需/可选 数据类型 描述     filter 必需 数组 当满足由 script 参数定义的指定条件时，指定应对词元应用哪个分词过滤器。   script 必需 对象 配置一个内联脚本，该脚本定义了要应用 filter 参数中指定的过滤器所需满足的条件（仅接受内联脚本）。    参考样例 #  以下示例请求创建了一个名为 my_conditional_index 的新索引，并配置了一个带有条件过滤器的分词器。该过滤器会对任何包含字符序列 “um” 的词元应用小写（lowercase）字母过滤器。\nPUT /my_conditional_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_conditional_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;condition\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;], \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;token.getTerm().toString().contains(\u0026#39;um\u0026#39;)\u0026#34; } } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;my_conditional_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_conditional_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;THE BLACK CAT JUMPS OVER A LAZY DOG\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;THE\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;BLACK\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 9, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;CAT\u0026#34;, \u0026#34;start_offset\u0026#34;: 10, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;jumps\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;OVER\u0026#34;, \u0026#34;start_offset\u0026#34;: 20, \u0026#34;end_offset\u0026#34;: 24, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;start_offset\u0026#34;: 25, \u0026#34;end_offset\u0026#34;: 26, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 }, { \u0026#34;token\u0026#34;: \u0026#34;LAZY\u0026#34;, \u0026#34;start_offset\u0026#34;: 27, \u0026#34;end_offset\u0026#34;: 31, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 6 }, { \u0026#34;token\u0026#34;: \u0026#34;DOG\u0026#34;, \u0026#34;start_offset\u0026#34;: 32, \u0026#34;end_offset\u0026#34;: 35, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 7 } ] } ","subcategory":null,"summary":"","tags":null,"title":"条件分词过滤器（Condition）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/condition/"},{"category":null,"content":"Fingerprint 分词过滤器 #  fingerprint 分词过滤器用于对文本进行标准化和去重处理。当文本处理的唯一性或者一致性至关重要时，这个过滤器特别有用。fingerprint 分词过滤器通过以下步骤处理文本以实现这一目的：\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元   小写转换：将所有文本转换为小写。 分词：把文本拆分成词元。 排序：按字母顺序排列这些词元。 去重：消除重复的词元。 合并词元：将这些词元合并成一个字符串，通常使用空格或其他指定的分隔符进行连接。  参数说明 #  指纹分词过滤器可以使用以下两个参数进行配置。\n   参数 必需/可选 数据类型 描述     max_output_size 可选 整数 限制生成的指纹字符串的长度。如果拼接后的字符串超过了 max_output_size，过滤器将不会产生任何输出，从而得到一个空词元。默认值是 255   separator 可选 字符串 定义在词元排序和去重后，用于将词元连接成单个字符串。默认是空格（\u0026quot; \u0026quot;）。    参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有指纹分词过滤器的分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_fingerprint\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;fingerprint\u0026#34;, \u0026#34;max_output_size\u0026#34;: 200, \u0026#34;separator\u0026#34;: \u0026#34;-\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_fingerprint\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch is a powerful search engine that scales easily\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;a-easily-engine-is-easysearch-powerful-scales-search-that\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 57, \u0026#34;type\u0026#34;: \u0026#34;fingerprint\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"指纹分词过滤器（Fingerprint）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/fingerprint/"},{"category":null,"content":"Common Grams 分词过滤器 #  common_grams 分词过滤器通过保留文本中常见的短语（常用词组）来提高搜索的相关性。当处理某些语言或数据集时，特定的单词组合经常作为一个整体出现，如果将它们当作单独的词元处理，可能会影响搜索的相关性，此时该过滤器就非常有用。如果输入字符串中常用词，这个分词过滤器会同时生成它们的一元词组（单个词）和二元词组（两个词的组合）。\n使用这个分词过滤器可以保持常用短语的完整性，从而提高搜索的相关性。这有助于更准确地匹配查询内容，尤其是对于频繁出现的单词组合。它还能通过减少不相关的匹配结果来提高搜索的精度。\n相关指南（先读这些） #    文本分析：停用词  邻近匹配  文本分析：规范化   使用此过滤器时，你必须谨慎选择并维护常用词(common_words)列表。\n 参数说明 #  常用词组分词过滤器可通过以下参数进行配置：\n   参数 必需/可选 数据类型 描述     common_words 必需 字符串列表 一个应被视为常用词词组的单词列表。如果 common_words 参数是一个空列表，common_grams 分词过滤器将成为一个无操作过滤器，即它根本不会修改输入的词元。   ignore_case 可选 布尔值 指示过滤器在匹配常用词时是否应忽略大小写差异。默认值为 false。   query_mode 可选 布尔值 当设置为 true 时，应用以下规则：\n- 从 common_words 生成的一元词组（单个词）不包含在输出中。\n- 非常用词后跟常用词形成的二元词组会保留在输出中。\n- 如果非常用词的一元词组后面紧接着一个常用词，则该一元词组会被排除。\n- 如果非常用词出现在文本末尾且前面是一个常用词，其一元词组不包含在输出中。    参考样例 #  以下示例请求创建了一个名为 my_common_grams_index 的新索引，并配置了一个使用常用词组（common_grams）过滤器的分析器。\nPUT /my_common_grams_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_common_grams_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;common_grams\u0026#34;, \u0026#34;common_words\u0026#34;: [\u0026#34;a\u0026#34;, \u0026#34;in\u0026#34;, \u0026#34;for\u0026#34;], \u0026#34;ignore_case\u0026#34;: true, \u0026#34;query_mode\u0026#34;: true } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_common_grams_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_common_grams_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;A quick black cat jumps over the lazy dog in the park\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ {\u0026#34;token\u0026#34;: \u0026#34;a_quick\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 7,\u0026#34;type\u0026#34;: \u0026#34;gram\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;quick\u0026#34;,\u0026#34;start_offset\u0026#34;: 2,\u0026#34;end_offset\u0026#34;: 7,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 1}, {\u0026#34;token\u0026#34;: \u0026#34;black\u0026#34;,\u0026#34;start_offset\u0026#34;: 8,\u0026#34;end_offset\u0026#34;: 13,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;cat\u0026#34;,\u0026#34;start_offset\u0026#34;: 14,\u0026#34;end_offset\u0026#34;: 17,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;jumps\u0026#34;,\u0026#34;start_offset\u0026#34;: 18,\u0026#34;end_offset\u0026#34;: 23,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;over\u0026#34;,\u0026#34;start_offset\u0026#34;: 24,\u0026#34;end_offset\u0026#34;: 28,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 5}, {\u0026#34;token\u0026#34;: \u0026#34;the\u0026#34;,\u0026#34;start_offset\u0026#34;: 29,\u0026#34;end_offset\u0026#34;: 32,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 6}, {\u0026#34;token\u0026#34;: \u0026#34;lazy\u0026#34;,\u0026#34;start_offset\u0026#34;: 33,\u0026#34;end_offset\u0026#34;: 37,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 7}, {\u0026#34;token\u0026#34;: \u0026#34;dog_in\u0026#34;,\u0026#34;start_offset\u0026#34;: 38,\u0026#34;end_offset\u0026#34;: 44,\u0026#34;type\u0026#34;: \u0026#34;gram\u0026#34;,\u0026#34;position\u0026#34;: 8}, {\u0026#34;token\u0026#34;: \u0026#34;in_the\u0026#34;,\u0026#34;start_offset\u0026#34;: 42,\u0026#34;end_offset\u0026#34;: 48,\u0026#34;type\u0026#34;: \u0026#34;gram\u0026#34;,\u0026#34;position\u0026#34;: 9}, {\u0026#34;token\u0026#34;: \u0026#34;the\u0026#34;,\u0026#34;start_offset\u0026#34;: 45,\u0026#34;end_offset\u0026#34;: 48,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 10}, {\u0026#34;token\u0026#34;: \u0026#34;park\u0026#34;,\u0026#34;start_offset\u0026#34;: 49,\u0026#34;end_offset\u0026#34;: 53,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 11} ] } ","subcategory":null,"summary":"","tags":null,"title":"常见词组分词过滤器（Common Grams）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/common-grams/"},{"category":null,"content":"Multiplexer 分词过滤器 #  multiplexer 分词过滤器允许你通过应用不同的过滤器来创建同一词元的多个版本。当你想要以多种方式分析同一个词元时，这非常有用。例如，你可能希望使用不同的词干提取、同义词或 n-gram 过滤器来分析一个词元，并将所有生成的词元一起使用。这个分词过滤器的工作方式是复制分词流，并对每个副本应用不同的过滤器。\n 注意：multiplexer 分词过滤器会从分词流中移除重复的词元。\n  限制：multiplexer 分词过滤器不支持 synonym 过滤器、synonym_graph 分词过滤器或 shingle 分词过滤器，因为这些过滤器不仅需要分析当前词元，还需要分析后续的词元，以便正确确定如何转换输入内容。\n 相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  参数说明 #  多路复用分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     filters 可选 字符串列表 要应用于分词流每个副本的分词过滤器的逗号分隔列表。默认值为空列表。   preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 true。    参考样例 #  以下示例请求创建了一个名为 multiplexer_index 的新索引，并配置了一个带有多路复用过滤器的分词器：\nPUT /multiplexer_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;english_stemmer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;english\u0026#34; }, \u0026#34;synonym_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;synonym\u0026#34;, \u0026#34;synonyms\u0026#34;: [ \u0026#34;quick,fast\u0026#34; ] }, \u0026#34;multiplexer_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;multiplexer\u0026#34;, \u0026#34;filters\u0026#34;: [\u0026#34;english_stemmer\u0026#34;, \u0026#34;synonym_filter\u0026#34;], \u0026#34;preserve_original\u0026#34;: true } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;multiplexer_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;multiplexer_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /multiplexer_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;multiplexer_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The slow turtle hides from the quick dog\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;The\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;slow\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;, \u0026#34;start_offset\u0026#34;: 9, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;turtl\u0026#34;, \u0026#34;start_offset\u0026#34;: 9, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;hides\u0026#34;, \u0026#34;start_offset\u0026#34;: 16, \u0026#34;end_offset\u0026#34;: 21, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;hide\u0026#34;, \u0026#34;start_offset\u0026#34;: 16, \u0026#34;end_offset\u0026#34;: 21, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;from\u0026#34;, \u0026#34;start_offset\u0026#34;: 22, \u0026#34;end_offset\u0026#34;: 26, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;the\u0026#34;, \u0026#34;start_offset\u0026#34;: 27, \u0026#34;end_offset\u0026#34;: 30, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 }, { \u0026#34;token\u0026#34;: \u0026#34;quick\u0026#34;, \u0026#34;start_offset\u0026#34;: 31, \u0026#34;end_offset\u0026#34;: 36, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 6 }, { \u0026#34;token\u0026#34;: \u0026#34;fast\u0026#34;, \u0026#34;start_offset\u0026#34;: 31, \u0026#34;end_offset\u0026#34;: 36, \u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;, \u0026#34;position\u0026#34;: 6 }, { \u0026#34;token\u0026#34;: \u0026#34;dog\u0026#34;, \u0026#34;start_offset\u0026#34;: 37, \u0026#34;end_offset\u0026#34;: 40, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 7 } ] } ","subcategory":null,"summary":"","tags":null,"title":"多路复用分词过滤器（Multiplexer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/multiplexer/"},{"category":null,"content":"向量字段建模指南 #  概述 #  向量字段是实现语义搜索（向量/kNN 搜索）的基础设施。本指南覆盖 Easysearch 中向量字段的设计原则、存储优化、性能权衡，帮助你做出合理的架构决策。\n核心出发点： 向量字段的设计直接影响三个维度——存储成本、索引性能、查询效率。在业务约束下找到最优平衡是关键。\n 1. 文档模型设计 #  1.1 基本模式：混合字段设计 #  在支持语义搜索的系统中，典型文档包含三类字段：\n{ \u0026#34;_id\u0026#34;: \u0026#34;doc-001\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Easysearch 向量搜索最佳实践\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2026-02-13\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;AI搜索\u0026#34; }, \u0026#34;content\u0026#34;: \u0026#34;完整的正文内容……\u0026#34;, \u0026#34;embedding\u0026#34;: [0.124, -0.031, 0.092, ...], \u0026#34;snippet\u0026#34;: \u0026#34;用于快速展示的摘要\u0026#34; } 字段分工：\n   字段类型 典型字段名 数据类型 用途 存储开销     文本字段 title, content text BM25 全文搜索、分面过滤 低   元数据字段 created_at, category keyword, date 精确过滤、排序、聚合 低   向量字段 embedding, vector dense_vector kNN 语义相似度查询 高   展示字段 snippet, summary text（不分词） 快速返回、避免重查询 中    设计要点：\n 文本字段和向量字段并存，支持混合查询（BM25 + kNN） 向量字段不需要分词、不需要倒排索引，但需要特殊的向量索引（如 HNSW） 避免冗余存储：已有 embedding 时，一般不需要额外的 content_vector 除非有多种语义模式  1.2 映射定义示例 #  { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; }, \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;embedding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 384, \u0026#34;index\u0026#34;: true, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; }, \u0026#34;snippet\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: false, \u0026#34;enabled\u0026#34;: true } } } }  2. 维度选择与成本控制 #  2.1 维度对成本的影响 #  向量维度（dims）是影响系统资源消耗的最重要因素：\n单条文档开销 ≈ dims × 4 字节（float32） 示例：\n 维度 128 → 512 字节/文档 维度 384 → 1.5 KB/文档 维度 768 → 3 KB/文档 维度 1536 → 6 KB/文档（如 OpenAI 的 embedding） 100 万文档的存储对比：\n     维度 向量存储大小 索引开销 总体影响     128 ~512 MB 低 ✅ 极优   384 ~1.5 GB 中 ✅ 推荐   768 ~3 GB 高 ⚠️ 中等   1536 ~6 GB 很高 ⚠️ 需评估    2.2 维度选择策略 #  优先级原则（从高到低）：\n 业务精度要求 → 必须满足搜索效果 成本-效果平衡 → 在保证效果的前提下最小化维度 基础设施约束 → 内存、存储、网络带宽限制  实践建议：\n场景 1：通用语义搜索（推荐） └─ 使用 384 维模型（如 sentence-transformers 系列） └─ 成本合理 + 效果充分 场景 2：精细领域应用（金融、医疗） └─ 使用 768 维模型 └─ 更强的表达能力，但需评估存储成本\n场景 3：大规模系统（超 5000 万文档） └─ 使用 128～256 维压缩模型 └─ 或使用量化（binary/int8）模式（如果引擎支持）\n场景 4：超大维度（\u0026gt;1000） └─ 评估是否真的必要 └─ 考虑降维：PCA、LSH 等预处理 └─ 或采用多阶段检索：粗排用低维，精排用高维 2.3 动态维度策略 #\n 对于演进中的系统，可采用多字段策略：\n{ \u0026#34;properties\u0026#34;: { \u0026#34;embedding_v1\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 384, \u0026#34;index\u0026#34;: true, // 线上用 \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; }, \u0026#34;embedding_v2\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 768, \u0026#34;index\u0026#34;: false, // 过渡阶段，暂不用于查询 \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; } } } 好处： 逐步升级模型，减少迁移成本。\n成本： 额外的存储开销（短期）。\n 3. 多向量模式 #  3.1 何时需要多向量 #  场景判断：\n   场景 是否需要多向量 说明     单一搜索场景（通用搜索） ❌ 否 一个 embedding 足够   多语言文档库 ✅ 是 不同语言用不同向量模型   标题和正文语义不同 ⚠️ 可选 取决于是否分别查询   不同时间段数据用不同模型 ⚠️ 可选 如无必要，应该做离线重算   A/B 测试新模型 ✅ 是（临时） 双向量并行验证，验证后清理    3.2 多向量设计示例 #  { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;title_embedding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 384, \u0026#34;index\u0026#34;: true, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; }, \u0026#34;content_embedding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 384, \u0026#34;index\u0026#34;: true, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; } } } } 查询时：\n# 基于标题的语义查询 response = es.search( index=\u0026#34;docs\u0026#34;, body={ \u0026#34;query\u0026#34;: { \u0026#34;knn\u0026#34;: { \u0026#34;title_embedding\u0026#34;: { \u0026#34;vector\u0026#34;: [0.12, -0.03, ...], \u0026#34;k\u0026#34;: 10 } } } } ) # 或使用 bool 组合（标题权重更高） # 详见搜索章节的 Hybrid 检索指南 3.3 多向量的成本警告 #\n ⚠️ 每增加一个向量字段，成本翻倍：\n2 个向量字段 = 2× 存储 + 2× 索引构建时间 + 2× 内存占用 决策框架：\n是否真的必须分别查询这两个向量？ ↓ 是 → 需要多向量 ↓ 否 → 考虑： a) 用单向量拼接/融合（如 concat([title_embedding, content_embedding])) b) 用 boosting 给标题更高权重（在查询侧解决） c) 在应用层做分步检索（先查标题，再查内容）  4. 写入与数据一致性 #  4.1 同步 vs 异步策略 #  策略对比：\n   方案 延迟 一致性 模型服务负载 适用场景     同步生成 高（秒级） ✅ 强一致 高 对搜索精度敏感的应用   异步补齐 低（毫秒） ⚠️ 最终一致 低 实时性要求不高，数据量大   混合策略 中等 ✅ 可控 中等 推荐    4.2 同步写入实现 #  from elasticsearch import Elasticsearch import requests es = Elasticsearch([\u0026quot;localhost:9200\u0026quot;]) embedding_service = \u0026quot;http://localhost:8001\u0026quot; # 向量服务\ndef index_document(doc): # 1. 调用模型生成向量 embedding_response = requests.post( f\u0026quot;{embedding_service}/embed\u0026quot;, json={\u0026quot;text\u0026quot;: doc[\u0026quot;content\u0026quot;]}, timeout=5 )\n\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; embedding_response\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;status_code \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;!=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#bd93f9\u0026quot;\u0026gt;200\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 降级策略：生成失败后如何处理？\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 选项 A：中断写入，返回错误\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 选项 B：仅写文本，标记为\u0026amp;#34;缺向量\u0026amp;#34;，后续异步补齐\u0026lt;/span\u0026gt; logger\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;warning(f\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Embedding failed for doc {doc[\u0026amp;#39;_id\u0026amp;#39;]}, marking for async\u0026amp;#34;\u0026lt;/span\u0026gt;) doc[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;_embedding_status\u0026amp;#34;\u0026lt;/span\u0026gt;] \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;pending\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;else\u0026lt;/span\u0026gt;: doc[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;embedding\u0026amp;#34;\u0026lt;/span\u0026gt;] \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; embedding_response\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;json()[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;vector\u0026amp;#34;\u0026lt;/span\u0026gt;] \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 2. 写入 Easysearch\u0026lt;/span\u0026gt; es\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;index( index\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;documents\u0026amp;#34;\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;doc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;get(\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;_id\u0026amp;#34;\u0026lt;/span\u0026gt;), body\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;doc )  注意事项：\n 设置合理的超时时间（建议 5～10 秒） 实现失败重试机制和降级策略 监控向量服务的延迟和错误率  4.3 异步补齐实现 #  # 方案：使用 Kafka/消息队列解耦 def index_document_async(doc): # 1. 立即写入文档（不含向量） es.index( index=\u0026quot;documents\u0026quot;, id=doc.get(\u0026quot;_id\u0026quot;), body={ **doc, \u0026quot;_embedding_status\u0026quot;: \u0026quot;pending\u0026quot; } )\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 2. 发送到异步任务队列\u0026lt;/span\u0026gt; kafka_producer\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;send(\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;embedding_tasks\u0026amp;#34;\u0026lt;/span\u0026gt;, { \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;doc_id\u0026amp;#34;\u0026lt;/span\u0026gt;: doc[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;_id\u0026amp;#34;\u0026lt;/span\u0026gt;], \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;text\u0026amp;#34;\u0026lt;/span\u0026gt;: doc[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;content\u0026amp;#34;\u0026lt;/span\u0026gt;] })  # 异步 Worker 处理： def embedding_worker(): for message in kafka_consumer.consume(\u0026quot;embedding_tasks\u0026quot;): task = json.loads(message.value)\n \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 批量处理，提升效率\u0026lt;/span\u0026gt; embeddings \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; model\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;encode([task[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;text\u0026amp;#34;\u0026lt;/span\u0026gt;]]) \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 更新文档的向量字段\u0026lt;/span\u0026gt; es\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;update( index\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;documents\u0026amp;#34;\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;task[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;doc_id\u0026amp;#34;\u0026lt;/span\u0026gt;], body\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;{ \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;doc\u0026amp;#34;\u0026lt;/span\u0026gt;: { \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;embedding\u0026amp;#34;\u0026lt;/span\u0026gt;: embeddings[\u0026lt;span style=\u0026quot;color:#bd93f9\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;], \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;_embedding_status\u0026amp;#34;\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;completed\u0026amp;#34;\u0026lt;/span\u0026gt; } } )  4.4 混合策略（推荐） #\n 实时性高的业务逻辑 → 同步生成 + 写入 ↓ 后台数据导入 → 异步补齐 ↓ 批量更新 → 定时离线处理 好处： 在用户体验和系统负载间找到平衡。\n 5. 存储与索引优化 #  5.1 禁用不必要的索引 #  如果不在线上查询某个向量，应禁用其索引以节省资源：\n{ \u0026#34;properties\u0026#34;: { \u0026#34;embedding_online\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 384, \u0026#34;index\u0026#34;: true, // ✅ 用于 kNN 查询 \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; }, \u0026#34;embedding_offline\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 768, \u0026#34;index\u0026#34;: false, // ❌ 仅存储，不索引 \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; } } } 开销对比（100 万文档）：\n index: true → 存储 + 索引，共约 2.5～3 GB（384 维） index: false → 仅存储，约 1.5 GB  5.2 向量量化（如果支持） #  某些搜索引擎支持 int8/binary 量化，可进一步节省空间：\n原始 float32 向量: 4 字节 × 384 = 1.5 KB 量化为 int8: 1 字节 × 384 = 384 字节 (节省 75%) 量化为 binary: ~48 字节 × 1 = 48 字节 (节省 97%) 权衡： 精度下降 vs 资源节省。需实际评估对搜索效果的影响。\n5.3 分片与副本策略 #  向量索引通常比文本索引更消耗内存，需要特别关注集群资源：\n{ \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 5, // 根据数据量调整 \u0026#34;number_of_replicas\u0026#34;: 1, // 高可用性 \u0026#34;index.store.type\u0026#34;: \u0026#34;niofs\u0026#34; // 使用内存映射，加速 kNN } } 建议：\n 向量索引的分片大小控制在 30～50 GB 如果单个向量字段超过 100 GB，考虑独立分片或多索引   6. 多向量融合与降维 #  6.1 向量拼接融合 #  场景： 需要同时考虑标题和内容的语义\n方案 1：应用层融合（推荐）\ndef fuse_embeddings(title_emb, content_emb, weights=(0.3, 0.7)): \u0026#34;\u0026#34;\u0026#34; 线性加权融合两个向量 标题权重 30%，内容权重 70% \u0026#34;\u0026#34;\u0026#34; fused = ( np.array(title_emb) * weights[0] + np.array(content_emb) * weights[1] ) # 归一化 return fused / np.linalg.norm(fused) # 写入 doc[\u0026quot;embedding\u0026quot;] = fuse_embeddings( model.encode(doc[\u0026quot;title\u0026quot;]), model.encode(doc[\u0026quot;content\u0026quot;]) ) 优点： 单向量，成本最低；权重可灵活调整。\n方案 2：多向量并行查询（见搜索章节）\n允许用户同时对标题和内容向量查询，系统端融合排序。\n6.2 PCA 降维 #  对于维度过高的向量（如 1536），可以用 PCA 预处理降至 384 维，保留 95%+ 的信息：\nfrom sklearn.decomposition import PCA # 离线：对样本向量做降维 pca = PCA(n_components=384) original_vectors = [\u0026hellip;] # shape: (N, 1536) reduced_vectors = pca.fit_transform(original_vectors)\n# 保存 PCA 模型 joblib.dump(pca, \u0026quot;pca_model.pkl\u0026quot;)\n# 在线：新来的向量也要用同一个 PCA 模型降维 new_vector = model.encode(\u0026quot;some text\u0026quot;) # shape: (1536,) new_reduced = pca.transform([new_vector])[0] # shape: (384,) 成本变化：\n 从 1536 维 → 384 维 ≈ 节省 75% 存储 查询速度提升 10～15 倍 精度损失 通常在可接受范围（1～3%）   7. 监控与运维 #  7.1 关键指标 #  监控以下指标确保系统健康：\n存储相关： ├─ 单个文档平均大小（向量占比） ├─ 索引总大小 vs 原始数据大小（压缩率） └─ 磁盘使用趋势 性能相关： ├─ kNN 查询延迟（P50, P99） ├─ 向量索引构建时间 ├─ 内存使用率 └─ 向量生成服务的吞吐量和错误率\n数据相关： ├─ 缺向量文档数量（_embedding_status: pending） ├─ 向量更新延迟 └─ 多向量同步率（如有多个向量字段） 7.2 告警规则示例 #\n 告警: 向量生成服务超时率 \u0026gt; 5% └─ 行动：扩容服务 / 优化模型推理 告警: 缺向量文档数量 \u0026gt; 文档总数的 1% └─ 行动：检查异步补齐任务 / 查看错误日志\n告警: kNN 查询 P99 延迟 \u0026gt; 1 秒 └─ 行动：检查节点负载 / 评估是否需要优化维度 \n8. 总结与决策树 #  快速决策流程 #  Q1: 需要做向量/语义搜索吗？ ├─ 否 → 使用纯文本索引 └─ 是 ↓ Q2: 数据量大小？ ├─ \u0026lt; 100 万 → 维度可用 768 ├─ 100 万～1000 万 → 推荐 384 └─ \u0026gt; 1000 万 → 谨慎选择，推荐 256～384 + 量化 ↓\nQ3: 有多种语义视角吗（标题 vs 内容等）？ ├─ 否 → 单向量 + 应用层融合 └─ 是 ↓\nQ4: 必须分别查询吗？ ├─ 否 → 融合为单向量 └─ 是 → 多向量（但评估成本） ↓\nQ5: 向量从何而来？ ├─ 实时生成 → 同步写入 + 降级策略 ├─ 离线预算 → 异步补齐 └─ 混合 → 混合策略 核心最佳实践 #\n    原则 说明     够用最优 在满足效果的前提下最小化维度和字段数   明确用途 每个向量字段都要清楚其查询需求   异步解耦 向量生成不应阻塞主业务流程   可观测性 监控向量生成、存储、查询的全链路   灰度迁移 更换模型时用多字段并行验证，而非全量切换     相关章节 #    向量查询与语义搜索 - 如何查询向量字段  Hybrid 检索 - 混合文本和向量搜索  字段类型参考 - dense_vector 字段详细参数  Mapping 与文本分析 - 基础概念  ","subcategory":null,"summary":"","tags":null,"title":"向量字段建模指南","url":"/easysearch/main/docs/best-practices/vector-fields/"},{"category":null,"content":"向量字段建模 #  概述 #  向量字段是实现语义搜索（向量/kNN 搜索）的基础设施。本指南覆盖 Easysearch 中向量字段的设计原则、存储优化、性能权衡，帮助你做出合理的架构决策。\n核心出发点： 向量字段的设计直接影响三个维度——存储成本、索引性能、查询效率。在业务约束下找到最优平衡是关键。\n 1. 文档模型设计 #  1.1 基本模式：混合字段设计 #  在支持语义搜索的系统中，典型文档包含三类字段：\n{ \u0026#34;_id\u0026#34;: \u0026#34;doc-001\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Easysearch 向量搜索最佳实践\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2026-02-13\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;AI搜索\u0026#34; }, \u0026#34;content\u0026#34;: \u0026#34;完整的正文内容……\u0026#34;, \u0026#34;embedding\u0026#34;: [0.124, -0.031, 0.092, ...], \u0026#34;snippet\u0026#34;: \u0026#34;用于快速展示的摘要\u0026#34; } 字段分工：\n   字段类型 典型字段名 数据类型 用途 存储开销     文本字段 title, content text BM25 全文搜索、分面过滤 低   元数据字段 created_at, category keyword, date 精确过滤、排序、聚合 低   向量字段 embedding, vector dense_vector kNN 语义相似度查询 高   展示字段 snippet, summary text（不分词） 快速返回、避免重查询 中    设计要点：\n 文本字段和向量字段并存，支持混合查询（BM25 + kNN） 向量字段不需要分词、不需要倒排索引，但需要特殊的向量索引（如 HNSW） 避免冗余存储：已有 embedding 时，一般不需要额外的 content_vector 除非有多种语义模式  1.2 映射定义示例 #  { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; }, \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;embedding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 384, \u0026#34;index\u0026#34;: true, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; }, \u0026#34;snippet\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: false, \u0026#34;enabled\u0026#34;: true } } } }  2. 维度选择与成本控制 #  2.1 维度对成本的影响 #  向量维度（dims）是影响系统资源消耗的最重要因素：\n单条文档开销 ≈ dims × 4 字节（float32） 示例：\n 维度 128 → 512 字节/文档 维度 384 → 1.5 KB/文档 维度 768 → 3 KB/文档 维度 1536 → 6 KB/文档（如 OpenAI 的 embedding） 100 万文档的存储对比：\n     维度 向量存储大小 索引开销 总体影响     128 ~512 MB 低 ✅ 极优   384 ~1.5 GB 中 ✅ 推荐   768 ~3 GB 高 ⚠️ 中等   1536 ~6 GB 很高 ⚠️ 需评估    2.2 维度选择策略 #  优先级原则（从高到低）：\n 业务精度要求 → 必须满足搜索效果 成本-效果平衡 → 在保证效果的前提下最小化维度 基础设施约束 → 内存、存储、网络带宽限制  实践建议：\n场景 1：通用语义搜索（推荐） └─ 使用 384 维模型（如 sentence-transformers 系列） └─ 成本合理 + 效果充分 场景 2：精细领域应用（金融、医疗） └─ 使用 768 维模型 └─ 更强的表达能力，但需评估存储成本\n场景 3：大规模系统（超 5000 万文档） └─ 使用 128～256 维压缩模型 └─ 或使用量化（binary/int8）模式（如果引擎支持）\n场景 4：超大维度（\u0026gt;1000） └─ 评估是否真的必要 └─ 考虑降维：PCA、LSH 等预处理 └─ 或采用多阶段检索：粗排用低维，精排用高维 2.3 动态维度策略 #\n 对于演进中的系统，可采用多字段策略：\n{ \u0026#34;properties\u0026#34;: { \u0026#34;embedding_v1\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 384, \u0026#34;index\u0026#34;: true, // 线上用 \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; }, \u0026#34;embedding_v2\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 768, \u0026#34;index\u0026#34;: false, // 过渡阶段，暂不用于查询 \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; } } } 好处： 逐步升级模型，减少迁移成本。\n成本： 额外的存储开销（短期）。\n 3. 多向量模式 #  3.1 何时需要多向量 #  场景判断：\n   场景 是否需要多向量 说明     单一搜索场景（通用搜索） ❌ 否 一个 embedding 足够   多语言文档库 ✅ 是 不同语言用不同向量模型   标题和正文语义不同 ⚠️ 可选 取决于是否分别查询   不同时间段数据用不同模型 ⚠️ 可选 如无必要，应该做离线重算   A/B 测试新模型 ✅ 是（临时） 双向量并行验证，验证后清理    3.2 多向量设计示例 #  { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;title_embedding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 384, \u0026#34;index\u0026#34;: true, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; }, \u0026#34;content_embedding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 384, \u0026#34;index\u0026#34;: true, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; } } } } 查询时：\n# 基于标题的语义查询 response = es.search( index=\u0026#34;docs\u0026#34;, body={ \u0026#34;query\u0026#34;: { \u0026#34;knn\u0026#34;: { \u0026#34;title_embedding\u0026#34;: { \u0026#34;vector\u0026#34;: [0.12, -0.03, ...], \u0026#34;k\u0026#34;: 10 } } } } ) # 或使用 bool 组合（标题权重更高） # 详见搜索章节的 Hybrid 检索指南 3.3 多向量的成本警告 #\n ⚠️ 每增加一个向量字段，成本翻倍：\n2 个向量字段 = 2× 存储 + 2× 索引构建时间 + 2× 内存占用 决策框架：\n是否真的必须分别查询这两个向量？ ↓ 是 → 需要多向量 ↓ 否 → 考虑： a) 用单向量拼接/融合（如 concat([title_embedding, content_embedding])) b) 用 boosting 给标题更高权重（在查询侧解决） c) 在应用层做分步检索（先查标题，再查内容）  4. 写入与数据一致性 #  4.1 同步 vs 异步策略 #  策略对比：\n   方案 延迟 一致性 模型服务负载 适用场景     同步生成 高（秒级） ✅ 强一致 高 对搜索精度敏感的应用   异步补齐 低（毫秒） ⚠️ 最终一致 低 实时性要求不高，数据量大   混合策略 中等 ✅ 可控 中等 推荐    4.2 同步写入实现 #  from elasticsearch import Elasticsearch import requests es = Elasticsearch([\u0026quot;localhost:9200\u0026quot;]) embedding_service = \u0026quot;http://localhost:8001\u0026quot; # 向量服务\ndef index_document(doc): # 1. 调用模型生成向量 embedding_response = requests.post( f\u0026quot;{embedding_service}/embed\u0026quot;, json={\u0026quot;text\u0026quot;: doc[\u0026quot;content\u0026quot;]}, timeout=5 )\n\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; embedding_response\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;status_code \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;!=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#bd93f9\u0026quot;\u0026gt;200\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 降级策略：生成失败后如何处理？\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 选项 A：中断写入，返回错误\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 选项 B：仅写文本，标记为\u0026amp;#34;缺向量\u0026amp;#34;，后续异步补齐\u0026lt;/span\u0026gt; logger\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;warning(f\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Embedding failed for doc {doc[\u0026amp;#39;_id\u0026amp;#39;]}, marking for async\u0026amp;#34;\u0026lt;/span\u0026gt;) doc[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;_embedding_status\u0026amp;#34;\u0026lt;/span\u0026gt;] \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;pending\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;else\u0026lt;/span\u0026gt;: doc[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;embedding\u0026amp;#34;\u0026lt;/span\u0026gt;] \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; embedding_response\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;json()[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;vector\u0026amp;#34;\u0026lt;/span\u0026gt;] \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 2. 写入 Easysearch\u0026lt;/span\u0026gt; es\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;index( index\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;documents\u0026amp;#34;\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;doc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;get(\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;_id\u0026amp;#34;\u0026lt;/span\u0026gt;), body\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;doc )  注意事项：\n 设置合理的超时时间（建议 5～10 秒） 实现失败重试机制和降级策略 监控向量服务的延迟和错误率  4.3 异步补齐实现 #  # 方案：使用 Kafka/消息队列解耦 def index_document_async(doc): # 1. 立即写入文档（不含向量） es.index( index=\u0026quot;documents\u0026quot;, id=doc.get(\u0026quot;_id\u0026quot;), body={ **doc, \u0026quot;_embedding_status\u0026quot;: \u0026quot;pending\u0026quot; } )\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 2. 发送到异步任务队列\u0026lt;/span\u0026gt; kafka_producer\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;send(\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;embedding_tasks\u0026amp;#34;\u0026lt;/span\u0026gt;, { \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;doc_id\u0026amp;#34;\u0026lt;/span\u0026gt;: doc[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;_id\u0026amp;#34;\u0026lt;/span\u0026gt;], \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;text\u0026amp;#34;\u0026lt;/span\u0026gt;: doc[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;content\u0026amp;#34;\u0026lt;/span\u0026gt;] })  # 异步 Worker 处理： def embedding_worker(): for message in kafka_consumer.consume(\u0026quot;embedding_tasks\u0026quot;): task = json.loads(message.value)\n \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 批量处理，提升效率\u0026lt;/span\u0026gt; embeddings \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; model\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;encode([task[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;text\u0026amp;#34;\u0026lt;/span\u0026gt;]]) \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 更新文档的向量字段\u0026lt;/span\u0026gt; es\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;update( index\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;documents\u0026amp;#34;\u0026lt;/span\u0026gt;, \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;task[\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;doc_id\u0026amp;#34;\u0026lt;/span\u0026gt;], body\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;{ \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;doc\u0026amp;#34;\u0026lt;/span\u0026gt;: { \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;embedding\u0026amp;#34;\u0026lt;/span\u0026gt;: embeddings[\u0026lt;span style=\u0026quot;color:#bd93f9\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;], \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;_embedding_status\u0026amp;#34;\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;completed\u0026amp;#34;\u0026lt;/span\u0026gt; } } )  4.4 混合策略（推荐） #\n 实时性高的业务逻辑 → 同步生成 + 写入 ↓ 后台数据导入 → 异步补齐 ↓ 批量更新 → 定时离线处理 好处： 在用户体验和系统负载间找到平衡。\n 5. 存储与索引优化 #  5.1 禁用不必要的索引 #  如果不在线上查询某个向量，应禁用其索引以节省资源：\n{ \u0026#34;properties\u0026#34;: { \u0026#34;embedding_online\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 384, \u0026#34;index\u0026#34;: true, // ✅ 用于 kNN 查询 \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; }, \u0026#34;embedding_offline\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dense_vector\u0026#34;, \u0026#34;dims\u0026#34;: 768, \u0026#34;index\u0026#34;: false, // ❌ 仅存储，不索引 \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; } } } 开销对比（100 万文档）：\n index: true → 存储 + 索引，共约 2.5～3 GB（384 维） index: false → 仅存储，约 1.5 GB  5.2 向量量化（如果支持） #  某些搜索引擎支持 int8/binary 量化，可进一步节省空间：\n原始 float32 向量: 4 字节 × 384 = 1.5 KB 量化为 int8: 1 字节 × 384 = 384 字节 (节省 75%) 量化为 binary: ~48 字节 × 1 = 48 字节 (节省 97%) 权衡： 精度下降 vs 资源节省。需实际评估对搜索效果的影响。\n5.3 分片与副本策略 #  向量索引通常比文本索引更消耗内存，需要特别关注集群资源：\n{ \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 5, // 根据数据量调整 \u0026#34;number_of_replicas\u0026#34;: 1, // 高可用性 \u0026#34;index.store.type\u0026#34;: \u0026#34;niofs\u0026#34; // 使用内存映射，加速 kNN } } 建议：\n 向量索引的分片大小控制在 30～50 GB 如果单个向量字段超过 100 GB，考虑独立分片或多索引   6. 多向量融合与降维 #  6.1 向量拼接融合 #  场景： 需要同时考虑标题和内容的语义\n方案 1：应用层融合（推荐）\ndef fuse_embeddings(title_emb, content_emb, weights=(0.3, 0.7)): \u0026#34;\u0026#34;\u0026#34; 线性加权融合两个向量 标题权重 30%，内容权重 70% \u0026#34;\u0026#34;\u0026#34; fused = ( np.array(title_emb) * weights[0] + np.array(content_emb) * weights[1] ) # 归一化 return fused / np.linalg.norm(fused) # 写入 doc[\u0026quot;embedding\u0026quot;] = fuse_embeddings( model.encode(doc[\u0026quot;title\u0026quot;]), model.encode(doc[\u0026quot;content\u0026quot;]) ) 优点： 单向量，成本最低；权重可灵活调整。\n方案 2：多向量并行查询（见搜索章节）\n允许用户同时对标题和内容向量查询，系统端融合排序。\n6.2 PCA 降维 #  对于维度过高的向量（如 1536），可以用 PCA 预处理降至 384 维，保留 95%+ 的信息：\nfrom sklearn.decomposition import PCA # 离线：对样本向量做降维 pca = PCA(n_components=384) original_vectors = [\u0026hellip;] # shape: (N, 1536) reduced_vectors = pca.fit_transform(original_vectors)\n# 保存 PCA 模型 joblib.dump(pca, \u0026quot;pca_model.pkl\u0026quot;)\n# 在线：新来的向量也要用同一个 PCA 模型降维 new_vector = model.encode(\u0026quot;some text\u0026quot;) # shape: (1536,) new_reduced = pca.transform([new_vector])[0] # shape: (384,) 成本变化：\n 从 1536 维 → 384 维 ≈ 节省 75% 存储 查询速度提升 10～15 倍 精度损失 通常在可接受范围（1～3%）   7. 监控与运维 #  7.1 关键指标 #  监控以下指标确保系统健康：\n存储相关： ├─ 单个文档平均大小（向量占比） ├─ 索引总大小 vs 原始数据大小（压缩率） └─ 磁盘使用趋势 性能相关： ├─ kNN 查询延迟（P50, P99） ├─ 向量索引构建时间 ├─ 内存使用率 └─ 向量生成服务的吞吐量和错误率\n数据相关： ├─ 缺向量文档数量（_embedding_status: pending） ├─ 向量更新延迟 └─ 多向量同步率（如有多个向量字段） 7.2 告警规则示例 #\n 告警: 向量生成服务超时率 \u0026gt; 5% └─ 行动：扩容服务 / 优化模型推理 告警: 缺向量文档数量 \u0026gt; 文档总数的 1% └─ 行动：检查异步补齐任务 / 查看错误日志\n告警: kNN 查询 P99 延迟 \u0026gt; 1 秒 └─ 行动：检查节点负载 / 评估是否需要优化维度 \n8. 总结与决策树 #  快速决策流程 #  Q1: 需要做向量/语义搜索吗？ ├─ 否 → 使用纯文本索引 └─ 是 ↓ Q2: 数据量大小？ ├─ \u0026lt; 100 万 → 维度可用 768 ├─ 100 万～1000 万 → 推荐 384 └─ \u0026gt; 1000 万 → 谨慎选择，推荐 256～384 + 量化 ↓\nQ3: 有多种语义视角吗（标题 vs 内容等）？ ├─ 否 → 单向量 + 应用层融合 └─ 是 ↓\nQ4: 必须分别查询吗？ ├─ 否 → 融合为单向量 └─ 是 → 多向量（但评估成本） ↓\nQ5: 向量从何而来？ ├─ 实时生成 → 同步写入 + 降级策略 ├─ 离线预算 → 异步补齐 └─ 混合 → 混合策略 核心最佳实践 #\n    原则 说明     够用最优 在满足效果的前提下最小化维度和字段数   明确用途 每个向量字段都要清楚其查询需求   异步解耦 向量生成不应阻塞主业务流程   可观测性 监控向量生成、存储、查询的全链路   灰度迁移 更换模型时用多字段并行验证，而非全量切换     相关章节 #    向量查询与语义搜索 - 如何查询向量字段  Hybrid 检索 - 混合文本和向量搜索  字段类型参考 - dense_vector 字段详细参数  Mapping 与文本分析 - 基础概念  ","subcategory":null,"summary":"","tags":null,"title":"向量字段建模","url":"/easysearch/main/docs/best-practices/data-modeling/vector-fields/"},{"category":null,"content":"RESTful 与 Query DSL #  Easysearch 通过 RESTful API 提供所有功能，使用 JSON 格式的 Query DSL 描述查询逻辑。理解这两个基础概念，是使用 Easysearch 的第一步。\nRESTful API 约定 #  Easysearch 的所有操作都通过 HTTP 请求完成，遵循 RESTful 风格：\n   HTTP 方法 含义 示例     GET 读取 GET /my-index/_search   PUT 创建或全量更新 PUT /my-index   POST 创建或部分更新 POST /my-index/_doc   DELETE 删除 DELETE /my-index    请求结构 #  一个典型的 Easysearch 请求由三部分组成：\n\u0026lt;HTTP方法\u0026gt; /\u0026lt;索引\u0026gt;/\u0026lt;操作\u0026gt; { \u0026lt;JSON请求体\u0026gt; } 例如，在 products 索引中搜索 \u0026ldquo;手机\u0026rdquo;：\nGET /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;手机\u0026#34; } } } 常用端点 #     端点 说明     /_cat/health 集群健康状态（文本格式）   /_cluster/health 集群健康状态（JSON）   /_cat/indices 索引列表   /\u0026lt;index\u0026gt;/_search 搜索   /\u0026lt;index\u0026gt;/_doc/\u0026lt;id\u0026gt; 读写单条文档   /_bulk 批量操作   /_analyze 分词测试    响应结构 #  每个搜索响应都包含以下关键字段：\n{ \u0026#34;took\u0026#34;: 5, // 耗时（毫秒） \u0026#34;timed_out\u0026#34;: false, // 是否超时 \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 42 }, // 命中总数 \u0026#34;max_score\u0026#34;: 1.5, // 最高评分 \u0026#34;hits\u0026#34;: [ // 命中的文档列表 { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.5, \u0026#34;_source\u0026#34;: { ... } // 原始文档 } ] } } Query DSL 概览 #  Query DSL（Domain Specific Language）是 Easysearch 的查询语言，使用 JSON 结构描述查询条件。所有查询都放在 query 字段中。\n两类查询上下文 #     上下文 关键字 作用 是否计算评分     查询上下文 query 文档与查询的匹配程度 是   过滤上下文 filter 文档是否匹配（是/否） 否（可缓存）     能用 filter 的场景尽量用 filter，不计算评分 + 可缓存 = 更快。\n 基础查询类型 #  match — 全文搜索 #  对输入文本分词后查询，是最常用的全文查询：\n{ \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;分布式搜索引擎\u0026#34; } } } term — 精确匹配 #  不分词，精确匹配 keyword 字段：\n{ \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;published\u0026#34; } } } range — 范围查询 #  { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;gte\u0026#34;: 100, \u0026#34;lte\u0026#34;: 500 } } } } bool — 组合查询 #  通过 must / should / must_not / filter 组合多个条件：\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;手机\u0026#34; } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;range\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;lte\u0026#34;: 3000 } } }, { \u0026#34;term\u0026#34;: { \u0026#34;brand\u0026#34;: \u0026#34;xiaomi\u0026#34; } } ] } } } match 与 term 的区别 #      match term     分词 会对输入分词 不分词   适用字段 text keyword、数值、日期   典型场景 全文搜索 过滤、精确查找    分页与排序 #  { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;from\u0026#34;: 0, \u0026#34;size\u0026#34;: 10, \u0026#34;sort\u0026#34;: [ { \u0026#34;created_at\u0026#34;: \u0026#34;desc\u0026#34; }, \u0026#34;_score\u0026#34; ] }  from + size：适合浅分页（前几百页） search_after：适合深分页 scroll：适合批量导出  高亮 #  { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;搜索引擎\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;content\u0026#34;: {} } } } 响应中的高亮片段：\n\u0026#34;highlight\u0026#34;: { \u0026#34;content\u0026#34;: [\u0026#34;分布式\u0026lt;em\u0026gt;搜索引擎\u0026lt;/em\u0026gt;的核心原理\u0026#34;] } 聚合（简介） #  聚合用于统计分析，可与查询组合：\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;brand_count\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;brand.keyword\u0026#34; } } } }  详见 聚合分析。\n 延伸阅读 #    入门教程：完整的动手练习  全文搜索：Query DSL 完整指南  结构化搜索：过滤与精确查询  常用 REST 参数：pretty、human 等通用参数  ","subcategory":null,"summary":"","tags":null,"title":"RESTful 与 Query DSL","url":"/easysearch/main/docs/fundamentals/restful-query-dsl/"},{"category":null,"content":"Jieba Index 分词器 #  jieba_index 分词器是 analysis-jieba 插件 提供的索引模式分词器。它在精确分词的基础上对长词进行二次切分，生成更多子词项，适合索引时使用以最大化召回率。\n前提条件 #  bin/easysearch-plugin install analysis-jieba 分词效果 #  以\u0026quot;中华人民共和国国歌\u0026quot;为例：\n   分词器 输出     jieba_search 中华人民共和国、国歌   jieba_index 中华、华人、人民、共和、共和国、中华人民共和国、国歌    使用示例 #  基本用法 #  PUT my-jieba-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;jieba_idx\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;jieba_index\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;jieba_idx\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;jieba_srch\u0026#34; } } } } 测试分词 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;jieba_index\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国国歌\u0026#34; } 最佳实践 #     场景 分词器     索引时 jieba_index（最大化召回）   搜索时 jieba_search（精确匹配）    相关链接 #    Jieba Search 分词器 — 搜索模式  文本分析  ","subcategory":null,"summary":"","tags":null,"title":"Jieba Index 分词器","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/jieba-index/"},{"category":null,"content":"结果折叠 #  结果折叠（Field Collapsing）允许你按某个字段的值对搜索结果进行分组去重，每组只返回最相关的一条（或通过 inner_hits 展开多条）。常用于：\n 商品搜索：同一品牌/店铺只展示最相关的一个商品 新闻搜索：同一来源的新闻只展示一条 日志分析：按主机名折叠，每台机器只显示最新的一条  基础用法 #  GET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;手机\u0026#34; } }, \u0026#34;collapse\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;brand.keyword\u0026#34; }, \u0026#34;sort\u0026#34;: [{ \u0026#34;_score\u0026#34;: \u0026#34;desc\u0026#34; }] } 每个 brand.keyword 值只返回分数最高的一条文档。\n 字段要求：collapse 字段必须是 keyword 或数值类型，且必须启用 doc_values。\n 参数说明 #     参数 类型 默认值 说明     field String 必填 用于折叠的字段名（keyword 或数值类型，需有 doc_values）   inner_hits Object 或 Array 空 展开折叠组，返回组内的多条文档   max_concurrent_group_searches Integer 0（不限制） 内部 inner_hits 请求的最大并发数    使用 inner_hits 展开 #  默认每组只返回一条文档。通过 inner_hits 可以展开每组，返回组内的 Top-N 文档：\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;手机\u0026#34; } }, \u0026#34;collapse\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;brand.keyword\u0026#34;, \u0026#34;inner_hits\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;brand_top3\u0026#34;, \u0026#34;size\u0026#34;: 3, \u0026#34;sort\u0026#34;: [{ \u0026#34;price\u0026#34;: \u0026#34;asc\u0026#34; }] } }, \u0026#34;sort\u0026#34;: [{ \u0026#34;_score\u0026#34;: \u0026#34;desc\u0026#34; }] } inner_hits 参数 #     参数 类型 默认值 说明     name String - inner_hits 的名称标识   size Integer 3 每组返回的文档数   from Integer 0 组内偏移量   sort Array - 组内排序（可以与外层排序不同）   _source Boolean/Object - 控制返回的 _source 字段   highlight Object - 组内文档的高亮配置   explain Boolean false 是否返回评分解释   stored_fields Array - 返回的存储字段列表   docvalue_fields Array - 返回的 doc_value 字段列表   script_fields Object - 脚本字段   track_scores Boolean false 是否计算分数   version Boolean false 是否返回文档版本   seq_no_primary_term Boolean false 是否返回 seq_no 和 primary_term    多个 inner_hits #  可以为同一个折叠字段配置多组 inner_hits，各自独立排序：\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;手机\u0026#34; } }, \u0026#34;collapse\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;brand.keyword\u0026#34;, \u0026#34;inner_hits\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;most_relevant\u0026#34;, \u0026#34;size\u0026#34;: 3, \u0026#34;sort\u0026#34;: [{ \u0026#34;_score\u0026#34;: \u0026#34;desc\u0026#34; }] }, { \u0026#34;name\u0026#34;: \u0026#34;cheapest\u0026#34;, \u0026#34;size\u0026#34;: 3, \u0026#34;sort\u0026#34;: [{ \u0026#34;price\u0026#34;: \u0026#34;asc\u0026#34; }] } ], \u0026#34;max_concurrent_group_searches\u0026#34;: 4 } } 二级折叠 #  inner_hits 内部还可以再嵌套一层 collapse，实现二级折叠：\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;手机\u0026#34; } }, \u0026#34;collapse\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;brand.keyword\u0026#34;, \u0026#34;inner_hits\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;by_model\u0026#34;, \u0026#34;size\u0026#34;: 3, \u0026#34;collapse\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;model.keyword\u0026#34; } } } }  限制：二级折叠只支持 field 参数，不支持进一步嵌套 inner_hits。\n 注意事项 #   折叠在 _score 排序和自定义排序下都有效 折叠不影响聚合（aggregations），聚合仍然基于全部匹配文档计算 响应中的 hits.total 是折叠前的匹配文档总数，而非折叠后的组数 折叠后不支持 scroll 和 rescore 如果折叠字段的值为 null 或缺失，该文档会被分到一个\u0026quot;空值组\u0026quot;  ","subcategory":null,"summary":"","tags":null,"title":"结果折叠","url":"/easysearch/main/docs/features/query-dsl/collapse/"},{"category":null,"content":"权限列表 #  此页面是可用权限的完整列表。每个权限控制对数据类型或 API 的访问。\n集群权限 #  核心集群权限 #   cluster:admin/component_template/delete cluster:admin/component_template/get cluster:admin/component_template/put cluster:admin/indices/dangling/delete cluster:admin/indices/dangling/find cluster:admin/indices/dangling/import cluster:admin/indices/dangling/list cluster:admin/ingest/pipeline/delete cluster:admin/ingest/pipeline/get cluster:admin/ingest/pipeline/put cluster:admin/ingest/pipeline/simulate cluster:admin/ingest/processor/grok/get cluster:admin/nodes/reload_secure_settings cluster:admin/reindex/rethrottle cluster:admin/repository/_cleanup cluster:admin/repository/delete cluster:admin/repository/get cluster:admin/repository/put cluster:admin/repository/verify cluster:admin/reroute cluster:admin/script/delete cluster:admin/script/get cluster:admin/script/put cluster:admin/script_context/get cluster:admin/script_language/get cluster:admin/scripts/painless/context cluster:admin/scripts/painless/execute cluster:admin/search/pipeline/delete cluster:admin/search/pipeline/get cluster:admin/search/pipeline/put cluster:admin/settings/update cluster:admin/snapshot/clone cluster:admin/snapshot/create cluster:admin/snapshot/delete cluster:admin/snapshot/get cluster:admin/snapshot/restore cluster:admin/snapshot/status cluster:admin/snapshot/status* cluster:admin/tasks/cancel cluster:admin/tasks/test cluster:admin/tasks/testunblock cluster:admin/voting_config/add_exclusions cluster:admin/voting_config/clear_exclusions cluster:monitor/allocation/explain cluster:monitor/health cluster:monitor/main cluster:monitor/nodes/hot_threads cluster:monitor/nodes/info cluster:monitor/nodes/liveness cluster:monitor/nodes/stats cluster:monitor/nodes/usage cluster:monitor/remote/info cluster:monitor/state cluster:monitor/stats cluster:monitor/task cluster:monitor/task/get cluster:monitor/tasks/lists  异步搜索权限 #   cluster:admin/async_search/delete cluster:admin/async_search/get cluster:admin/async_search/stats cluster:admin/async_search/submit  安全管理权限 #   cluster:admin/security/config/update cluster:admin/security/whoami  ILM 索引生命周期管理权限 #   cluster:admin/ilm/managedindex/add cluster:admin/ilm/managedindex/change cluster:admin/ilm/managedindex/explain cluster:admin/ilm/managedindex/remove cluster:admin/ilm/managedindex/retry cluster:admin/ilm/policy/delete cluster:admin/ilm/policy/get cluster:admin/ilm/policy/search cluster:admin/ilm/policy/write cluster:admin/ilm/update/managedindexmetadata  Rollup 权限 #   cluster:admin/rollup/delete cluster:admin/rollup/explain cluster:admin/rollup/get cluster:admin/rollup/index cluster:admin/rollup/mapping/update cluster:admin/rollup/search cluster:admin/rollup/start cluster:admin/rollup/stop cluster:admin/rollup/update  Transform 权限 #   cluster:admin/transform/delete cluster:admin/transform/explain cluster:admin/transform/get cluster:admin/transform/get_transforms cluster:admin/transform/index cluster:admin/transform/preview cluster:admin/transform/start cluster:admin/transform/stop  快照管理权限 #   cluster:admin/snapshot_management/policy/delete cluster:admin/snapshot_management/policy/explain cluster:admin/snapshot_management/policy/get cluster:admin/snapshot_management/policy/search cluster:admin/snapshot_management/policy/start cluster:admin/snapshot_management/policy/stop cluster:admin/snapshot_management/policy/write  通知权限 #   cluster:admin/easysearch/notifications/channels/get cluster:admin/easysearch/notifications/configs/create cluster:admin/easysearch/notifications/configs/delete cluster:admin/easysearch/notifications/configs/get cluster:admin/easysearch/notifications/configs/update cluster:admin/easysearch/notifications/feature/publish cluster:admin/easysearch/notifications/feature/send cluster:admin/easysearch/notifications/features  跨集群复制权限 #   cluster:admin/plugins/replication/autofollow cluster:admin/plugins/replication/autofollow/update cluster:indices/admin/replication cluster:indices/shards/replication  插件权限 #   cluster:admin/ai/embed cluster:admin/ik/dictionary/reload cluster:admin/rules/compile/internal cluster:admin/rules/delete/internal cluster:admin/rules/sync/block  索引权限 #  核心索引管理权限 #   indices:admin/aliases indices:admin/aliases/exists indices:admin/aliases/get indices:admin/analyze indices:admin/analyze_space_usage indices:admin/auto_create indices:admin/block/add indices:admin/cache/clear indices:admin/close indices:admin/create indices:admin/data_stream/create indices:admin/data_stream/delete indices:admin/data_stream/get indices:admin/delete indices:admin/exists indices:admin/flush indices:admin/flush* indices:admin/forcemerge indices:admin/get indices:admin/index_template/delete indices:admin/index_template/get indices:admin/index_template/put indices:admin/index_template/simulate indices:admin/index_template/simulate_index indices:admin/mapping/auto_put indices:admin/mapping/put indices:admin/mappings/fields/get indices:admin/mappings/fields/get* indices:admin/mappings/get indices:admin/open indices:admin/refresh indices:admin/refresh* indices:admin/refresh_search_analyzers indices:admin/reload indices:admin/resize indices:admin/resolve/index indices:admin/rollover indices:admin/seq_no/global_checkpoint_sync indices:admin/settings/update indices:admin/shards/search_shards indices:admin/shrink indices:admin/synced_flush indices:admin/template/delete indices:admin/template/get indices:admin/template/put indices:admin/types/exists indices:admin/upgrade indices:admin/validate/query  索引数据读取权限 #   indices:data/read/async_search indices:data/read/explain indices:data/read/field_caps indices:data/read/field_caps* indices:data/read/get indices:data/read/mget indices:data/read/mget* indices:data/read/msearch indices:data/read/msearch/template indices:data/read/mtv indices:data/read/mtv* indices:data/read/point_in_time/create indices:data/read/point_in_time/delete indices:data/read/point_in_time/readall indices:data/read/rank_eval indices:data/read/rollup/search indices:data/read/scroll indices:data/read/scroll/clear indices:data/read/search indices:data/read/search* indices:data/read/search/template indices:data/read/tv  索引数据写入权限 #   indices:data/write/bulk indices:data/write/bulk* indices:data/write/delete indices:data/write/delete/byquery indices:data/write/index indices:data/write/reindex indices:data/write/update indices:data/write/update/byquery  索引监控权限 #   indices:monitor/data_stream/stats indices:monitor/field_usage_stats indices:monitor/recovery indices:monitor/segments indices:monitor/settings/get indices:monitor/shard_stores indices:monitor/stats indices:monitor/upgrade  ILM 索引级别权限 #   indices:admin/ilm/managedindex  跨集群复制索引权限 #   indices:admin/plugins/replication/autofollow/stats indices:admin/plugins/replication/follower/stats indices:admin/plugins/replication/index/all-shards indices:admin/plugins/replication/index/pause indices:admin/plugins/replication/index/resume indices:admin/plugins/replication/index/setup/validate indices:admin/plugins/replication/index/start indices:admin/plugins/replication/index/stats indices:admin/plugins/replication/index/status_check indices:admin/plugins/replication/index/stop indices:admin/plugins/replication/index/update indices:admin/plugins/replication/index/update_metadata indices:admin/plugins/replication/resources/release indices:admin/replication/index/all_status indices:data/read/plugins/replication/changes indices:data/read/plugins/replication/file_chunk indices:data/read/plugins/replication/file_metadata indices:data/write/plugins/replication/changes  权限集合 #  为了提高权限设置的效率，我们对系统权限进行自定义管理，从而快速批量选择一组相关的权限，而不是分别选择单个权限，为了方便，系统内置了若干权限集合。\n超级权限 #     名称 描述     unlimited 超级权限，集群和索引级别，等同于 \u0026quot;*\u0026quot; 。    集群级别 #     名称 描述     cluster_all 授予全部集群级别的权限，等同于 cluster:*。   cluster_monitor 授予全部集群的监控相关的权限，等同于 cluster:monitor/*。   cluster_composite_ops_ro 授予请求的只读权限，如 mget、msearch 或 mtv，再加上别名和 scroll 的访问。   cluster_composite_ops 与 cluster_composite_ops_ro 相同，但是额外包括 bulk、reindex 和所有的别名权限。   manage_snapshots 授予所有和快照、仓库管理相关的权限，等同于 cluster:admin/snapshot/* 和 cluster:admin/repository/*。   cluster_manage_index_templates 授予管理索引模板的权限，等同于 indices:admin/template/*。   cluster_manage_pipelines 授予管理 Ingest Pipeline 的权限，等同于 cluster:admin/ingest/pipeline/*。   rollup 授予所有 Rollup 相关的操作权限，包括创建、删除、启动、停止和搜索等。   manage_data_streams 授予管理数据流（Data Stream）的权限，等同于 indices:admin/data_stream/* 和 indices:monitor/data_stream/stats。    索引级别 #     名称 描述     indices_all 授予全部索引级别的权限，等同于 indices:*。   get 授予 get 和 mget 操作的权限。   read 授予只读相关的权限，如搜索、获取字段 Mapping、get 和 mget 操作。   write 授予在已有索引里面写入数据的权限，包括 index、update、bulk 和 mapping/put。   index 授予索引文档的权限，包括 index、update、bulk 和 mapping/put 操作。   delete 授予删除文档的权限。   crud 增删改查的组合，等同于 read + write。   search 授予文档搜索的权限，包括 search、msearch、rollup/search 和 suggest 权限。   suggest 授予使用搜索提示 API 的权限。   data_access 授予所有数据读写权限，等同于 indices:data/* 加上 crud。   create_index 授予创建索引和 Mapping 的权限。   indices_monitor 授予访问索引监控相关接口的权限（如：recovery、segments、index stats 和 status 等）。   manage_aliases 授予管理别名的权限，等同于 indices:admin/aliases*。   manage 授予所有索引监控和管理相关的权限，等同于 indices:monitor/* 加 indices:admin/*。    内置角色 #  系统内置了以下角色，可以直接分配给用户：\n   角色名 描述     superuser 超级用户角色，授予对所有集群 API 和所有索引的无限制访问权限。   monitor 监控角色，授予集群健康和性能的监控权限，适用于运维和仪表盘用户。   readonly 只读角色，授予对所有索引的只读访问权限，适用于数据消费者、分析人员和只读应用客户端。    ","subcategory":null,"summary":"","tags":null,"title":"权限列表","url":"/easysearch/main/docs/operations/security/access-control/permissions/"},{"category":null,"content":"数据汇总 #   先读概述：如果您尚未了解 Rollup 的优势与应用场景，建议先阅读 数据生命周期 中的 Rollup 部分。\n 数据汇总或上卷（Rollup），对于时序场景类的数据，往往会有大量的非常详细的聚合指标，随着时间的图推移，存储将持续增长。汇总功能可以将旧的、细粒度的数据汇总为粗粒度格式以进行长期存储。通过将数据汇总到一个单一的文档中，可以大大降低历史数据的存储成本。 Easysearch 的 rollup 具备一些独特的优势，可以自动对 rollup 索引进行滚动而不用依赖其他 API 去单独设置，并且在进行聚合查询时支持直接搜索原始索引，做到了对业务端的搜索代码完全兼容，从而对用户无感知。\n支持的聚合类型 #  对数值类型字段支持的聚合\n avg sum max min value_count percentiles  对 keyword 类型字段提供 terms 聚合。\n对 date 类型字段 除了 date_histogram 聚合，还支持 date_range 聚合。(v1.10.0)\n查询 rollup 数据时，增加支持 Filter aggregation，某些场景可以用来替代 query 过滤数据。(v1.10.1)\n增加针对个别字段自定义 special_metrics 指标的配置项。 (v1.10.1)\n增加支持 Bucket sort aggregation。 (v1.10.1)\n混合查询原始索引和 rollup 索引时，返回的 response 里增加了 origin 参数，表示包含 rollup 数据。(v1.10.1)\nRollup 查询 API 提供了 debug 参数，显示 Easysearch 内部执行的查询语句。(v1.10.1)\n使用汇总（rollup）的先决条件 #  必须安装索引生命周期管理插件，rollup 属于该插件功能的一部分。\n要汇总的指标索引必须具备 date 类型的字段。\n全局启用或禁用 rollup 搜索的设置 #   rollup 功能在 index-management 模块里。\n从 1.15.2 版本开始，index-management 已经成为 modules 的一部分，不需要单独安装插件。\n 从 1.10.0 版本开始，索引生命周期管理插件不再默认启用 rollup 搜索功能，如果想启用搜索 rollup 搜索功能，需要设置\nPUT /_cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;rollup.search.enabled\u0026#34;: true } } 通配符方式批量启动停止 rollup job #\n 从 1.10.0 版本开始，创建 rollup job 之后不再自动启动，需要手动启动，但是可以通过通配符的方式批量启动、停止 job\nPOST _rollup/jobs/rollup*/_start POST _rollup/jobs/rollup*/_stop\n设置 rollup 索引自动滚动的条数 #\n 达到此条数就会自动滚动新的索引\nPUT /_cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;rollup.max_docs\u0026#34;: 10000000 } } 新增 ROLLUP_SEARCH_MAX_COUNT 配置 #\n 从 1.10.0 版本开始，新增了 ROLLUP_SEARCH_MAX_COUNT 配置项，用于控制 Rollup 在运行 Job 时收集历史数据的最大并发分片请求数。这个配置项可以帮助你优化 Rollup 任务的性能，并避免集群资源过载。\n设置 Rollup 查询的时间范围上限 #  从 1.10.1 版本开始，可以通过 rollup.hours_before 集群配置项，设置 rollup 数据的查询时间范围上限，默认值是 1，表示 针对 rollup 数据的查询不会超过当前时间的 1 小时之前， 此参数的使用场景：\n 在混合查询时将 rollup 数据和原始数据的查询分割，防止查询的时间范围和数据不匹配。 限制 rollup 数据的查询时间上限。  使用示例：\nPUT /_cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;rollup.hours_before\u0026#34;: 24 } } 设置后，对 rollup 数据的查询范围上限不会超过 24 小时之前。\n功能： #   控制并发请求数：限制 Rollup 任务在执行搜索请求时的最大并发分片请求数。 动态调整：支持在集群运行时动态调整，无需重启集群。 默认值：2，即默认情况下，Rollup 任务最多会同时发送 2 个并发分片请求。  示例： #  PUT /_cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;rollup.search.max_count\u0026#34;: 2 } } 在这个例子中，ROLLUP_SEARCH_MAX_COUNT 被设置为 2，表示 Rollup 任务在执行搜索请求时，最多会同时发送 2 个并发分片请求。\n配置建议： #   小规模集群：建议设置为较小的值（如 2），以避免资源竞争。 大规模集群：可以适当增加该值（如 4），以提高并发性能。 动态调整：根据集群负载情况动态调整该值，以优化性能和资源利用率。  rollup 自动滚动后的索引列表 #  health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open rollup1_.infini_metrics-000001 aNET5la8QBa8AtCAtplPAw 1 0 179783 0 1gb 1gb green open rollup1_.infini_metrics-000002 7TDSIvWvSvKuKbB0DH-J0w 1 0 179170 0 1gb 1gb green open rollup1_.infini_metrics-000003 MhtBEj_-RgCg29gAbkbg6g 1 0 178297 0 1gb 1gb green open rollup1_.infini_metrics-000004 -Gfp80nUR5Cz-XnFy3F0rw 1 0 178297 0 1gb 1gb green open rollup1_.infini_metrics-000005 CuhhEf9SQ--yzMkG0QgLQw 1 0 177665 0 1gb 1gb green open rollup1_.infini_metrics-000006 9LqFJ28XRKexgwlMgIy3Bg 1 0 178624 0 1gb 1gb green open rollup1_.infini_metrics-000007 XkrPz8DDSRSq8xhnWo3nbA 1 0 180581 0 1gb 1gb green open rollup1_.infini_metrics-000008 xrfETulmT7OZnfcfVrIJTw 1 0 180050 0 1gb 1gb 汇总（rollup） API： #\n  创建或替换索引汇总任务 获取索引汇总任务 删除索引汇总任务 启动或停止索引汇总任务 解释索引汇总任务  创建或更新索引汇总任务 #  创建或自动替换一个索引汇总任务\nPUT _rollup/jobs/\u0026lt;rollup_id\u0026gt;?replace\n\u0026lt;rollup_id\u0026gt;: 为您的 Rollup 任务指定一个唯一标识符。\n?replace: 如果携带此参数，将替换已存在的 Rollup 所有配置，请谨慎使用。\n请求示例 #  下面的例子会创建一个自动滚动的 rollup 索引\nPUT _rollup/jobs/rollup_node_stats { \u0026#34;rollup\u0026#34;: { \u0026#34;source_index\u0026#34;: \u0026#34;.infini_metrics\u0026#34;, \u0026#34;target_index\u0026#34;: \u0026#34;rollup_node_stats_{{ctx.source_index}}\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;continuous\u0026#34;: true, \u0026#34;page_size\u0026#34;: 200, \u0026#34;cron\u0026#34;: \u0026#34;*/5 1-23 * * *\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;UTC+8\u0026#34;, \u0026#34;stats\u0026#34;: [ { \u0026#34;max\u0026#34;: {} }, { \u0026#34;min\u0026#34;: {} }, { \u0026#34;value_count\u0026#34;: {} } ], \u0026#34;interval\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;identity\u0026#34;: [ \u0026#34;metadata.labels.cluster_id\u0026#34;, \u0026#34;metadata.labels.cluster_uuid\u0026#34;, \u0026#34;metadata.category\u0026#34;, \u0026#34;metadata.labels.node_id\u0026#34;, \u0026#34;metadata.labels.transport_address\u0026#34; ], \u0026#34;attributes\u0026#34;: [ \u0026#34;agent.*\u0026#34;, \u0026#34;metadata.*\u0026#34; ], \u0026#34;filter\u0026#34;: { \u0026#34;metadata.name\u0026#34;: \u0026#34;node_stats\u0026#34; }, \u0026#34;metrics\u0026#34;: [ \u0026#34;payload.elasticsearch.node_stats.*\u0026#34; ], \u0026#34;special_metrics\u0026#34;: [ { \u0026#34;source_field\u0026#34;: \u0026#34;payload.elasticsearch.node_stats.jvm.mem.heap_used_in_bytes\u0026#34;, \u0026#34;metrics\u0026#34;: [ { \u0026#34;avg\u0026#34;: {} }, { \u0026#34;max\u0026#34;: {} }, { \u0026#34;min\u0026#34;: {} }, { \u0026#34;percentiles\u0026#34;: {} } ] } ], \u0026#34;write_optimization\u0026#34;: true, \u0026#34;field_abbr\u0026#34;: true } } 参数详细说明 #     参数 解释 示例值说明     source_index 源索引，数据将从此索引中收集 .infini_metrics: 表示从名为 .infini_metrics 的索引中收集数据   target_index 目标索引，汇总数据将存储在此索引中 rollup1_{{ctx.source_index}}: 动态生成目标索引名，使用源索引名作为一部分   timestamp 用于时间聚合的时间戳字段 timestamp: 指定用于时间聚合的字段名   continuous 设置为 true，表示这是一个连续的 rollup 作业，默认为false true: 任务将持续运行，而不是一次性执行   page_size 每次处理的文档数量 1000: 每批处理1000个文档，可根据系统性能调整   cron 作业运行的调度表达式 */10 1-23 * * *: 每10分钟执行一次，在1点到23点之间   timezone 用于 cron 表达式的时区 UTC+8: 使用UTC+8时区（如北京时间）   stats 要计算的统计信息 示例中计算最大值和计数，可根据需求添加其他统计如avg, min等   interval 时间聚合的间隔 1m: 每分钟聚合一次数据   identity 标识字段，用于分组聚合 列出的字段将用于创建唯一的聚合组   attributes 要包含在 rollup 中的属性字段 agent.*, metadata.*: 包含所有以agent和metadata开头的字段   metrics 要汇总的指标字段 payload.elasticsearch.index_stats.*: 汇总所有index_stats下的指标   special_metrics 对特别关注的指标配置更丰富的统计方式 专门对 JVM 堆内存使用量这个字段计算 avg max min percentiles 4 个指标，不再计算 stats 里的默认指标   exclude 要排除的指标字段 payload.elasticsearch.index_stats.routing.*: metrics 会排除符合模式的字段   filter 用于过滤源文档的条件 {\u0026quot;metadata.name\u0026quot;: \u0026quot;index_stats\u0026quot;}: 只处理metadata.name为index_stats的文档   write_optimization 启用后采用自动生成文档 ID 的策略，提升写入速度 true: 启用写入优化（1.12.0 版本增加）   field_abbr 启用字段名缩写，可降低内存消耗 true: 启用字段名缩写（1.12.0 版本增加）   is_continue 当创建的 Rollup 已经有历史数据并要断点续跑时配置 true: 启用断点续跑，默认不配置为 false（1.12.3 版本增加）    注意事项 #   cron 表达式需要谨慎设置，确保不会对系统造成过大负担。 page_size 应根据您的系统性能和数据量来调整。 identity 字段的选择会影响聚合的粒度，请根据您的分析需求仔细选择。 使用通配符（如 agent.*）时要注意，这可能会包含大量字段，影响性能。 filter 可以有效减少处理的数据量，提高效率。 write_optimization 和 field_abbr 是从 1.12.0 新增的参数。  获取索引汇总任务 #  请求\nGET _rollup/jobs/\u0026lt;rollup_id\u0026gt; 响应\n{ \u0026#34;_id\u0026#34;: \u0026#34;rollup4\u0026#34;, \u0026#34;_version\u0026#34;: 3, \u0026#34;_seq_no\u0026#34;: 5, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;rollup\u0026#34;: { \u0026#34;rollup_id\u0026#34;: \u0026#34;rollup4\u0026#34;, \u0026#34;enabled\u0026#34;: false, \u0026#34;schedule\u0026#34;: { \u0026#34;interval\u0026#34;: { \u0026#34;start_time\u0026#34;: 1718105163766, \u0026#34;period\u0026#34;: 1, \u0026#34;unit\u0026#34;: \u0026#34;Minutes\u0026#34;, \u0026#34;schedule_delay\u0026#34;: 0 } }, \u0026#34;last_updated_time\u0026#34;: 1718105163767, \u0026#34;enabled_time\u0026#34;: null, \u0026#34;description\u0026#34;: \u0026#34;Example rollup job\u0026#34;, \u0026#34;schema_version\u0026#34;: 17, \u0026#34;source_index\u0026#34;: \u0026#34;test-data\u0026#34;, \u0026#34;target_index\u0026#34;: \u0026#34;rollup4-test\u0026#34;, \u0026#34;metadata_id\u0026#34;: \u0026#34;nuMNB5ABnvQbLaVPM5qO\u0026#34;, \u0026#34;page_size\u0026#34;: 200, \u0026#34;delay\u0026#34;: 0, \u0026#34;continuous\u0026#34;: false, \u0026#34;dimensions\u0026#34;: [ { \u0026#34;date_histogram\u0026#34;: { \u0026#34;fixed_interval\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;source_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;UTC\u0026#34; } } ], \u0026#34;metrics\u0026#34;: [ { \u0026#34;source_field\u0026#34;: \u0026#34;passenger_count\u0026#34;, \u0026#34;metrics\u0026#34;: [ { \u0026#34;avg\u0026#34;: {} }, { \u0026#34;sum\u0026#34;: {} }, { \u0026#34;max\u0026#34;: {} }, { \u0026#34;min\u0026#34;: {} }, { \u0026#34;value_count\u0026#34;: {} } ] } ] } } 删除索引汇总任务 #  请求\nDELETE _rollup/jobs/\u0026lt;rollup_id\u0026gt; 响应\n{ \u0026#34;_index\u0026#34;: \u0026#34;.easysearch-ilm-config\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;rollup4\u0026#34;, \u0026#34;_version\u0026#34;: 7, \u0026#34;result\u0026#34;: \u0026#34;deleted\u0026#34;, \u0026#34;forced_refresh\u0026#34;: true, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 23, \u0026#34;_primary_term\u0026#34;: 1 } 启动或停止索引汇总任务 #  请求\nPOST _rollup/jobs/\u0026lt;rollup_id\u0026gt;/_start POST _rollup/jobs/\u0026lt;rollup_id\u0026gt;/_stop 响应\n{ \u0026#34;acknowledged\u0026#34;: true } 解释索引汇总任务 #  请求\nGET _rollup/jobs/\u0026lt;rollup_id\u0026gt;/_explain 响应\n{ \u0026#34;rollup1\u0026#34;: { \u0026#34;metadata_id\u0026#34;: \u0026#34;VCAnj5IBQpC3NDmZwgL7\u0026#34;, \u0026#34;rollup_metadata\u0026#34;: { \u0026#34;rollup_id\u0026#34;: \u0026#34;rollup1\u0026#34;, \u0026#34;last_updated_time\u0026#34;: 1729151524871, \u0026#34;continuous\u0026#34;: { \u0026#34;next_window_start_time\u0026#34;: 1728196860000, \u0026#34;next_window_end_time\u0026#34;: 1728196920000 }, \u0026#34;status\u0026#34;: \u0026#34;started\u0026#34;, \u0026#34;failure_reason\u0026#34;: null, \u0026#34;stats\u0026#34;: { \u0026#34;pages_processed\u0026#34;: 77014, \u0026#34;documents_processed\u0026#34;: 135026197, \u0026#34;rollups_indexed\u0026#34;: 26812185, \u0026#34;index_time_in_millis\u0026#34;: 38094616, \u0026#34;search_time_in_millis\u0026#34;: 88930100 } } } }    字段名 类型 描述     metadata_id string rollup 元数据的唯一标识符   rollup_metadata.rollup_id string rollup 作业的 ID   rollup_metadata.last_updated_time long 上次更新时间的 Unix 时间戳（毫秒）   rollup_metadata.continuous.next_window_start_time long 下一个 rollup 窗口的开始时间（Unix 时间戳，毫秒）   rollup_metadata.continuous.next_window_end_time long 下一个 rollup 窗口的结束时间（Unix 时间戳，毫秒）   rollup_metadata.status string rollup 作业当前的状态   rollup_metadata.failure_reason string/null 如果作业失败，显示失败原因；否则为 null   rollup_metadata.stats.pages_processed long 已处理的页面数   rollup_metadata.stats.documents_processed long 已处理的文档数   rollup_metadata.stats.rollups_indexed long 已索引的 rollup 文档数   rollup_metadata.stats.index_time_in_millis long 索引所用的总时间（毫秒）   rollup_metadata.stats.search_time_in_millis long 搜索所用的总时间（毫秒）    修改已经运行 Job 的 interval 和 page_size 参数 (1.13.0) #  在线上环境运行 Rollup 时会遇到这种情况：由于机器硬件资源或其他原因发现某个 Rollup 压力较大，需要调整最小时间区间或单次请求条数。\n从 1.13.0 版本开始，可以使用 POST _rollup/jobs/\u0026lt;rollup_id\u0026gt; API 进行更新，此操作内部会自动执行\n 停止 Job 更新 参数 重启 Job 运行 Job 时 索引数据会增加 unique 字段标识当前 Job 批次的数据，方便数据排重  请求\nPOST _rollup/jobs/rollup_node_stats { \u0026#34;page_size\u0026#34;: 100, \u0026#34;interval\u0026#34;: \u0026#34;5m\u0026#34; } 响应\n{ \u0026#34;acknowledged\u0026#34;: true } 如何使用 rollup 索引进行搜索 #  无需特意搜索 rollup 索引，只需使用标准的 search API 对原始目标索引进行搜索。查询时，请确保查询符合目标索引的约束条件。 例如，如果你没有在某个字段上设置 terms 聚合，那么你不会收到该字段的 terms 聚合结果。如果你没有设置 maximum 聚合，也不会收到 maximum 聚合的结果。\n无法访问目标索引中数据的内部结构，因为插件会在后台自动重写查询以适应目标索引。这是为了确保对源索引和目标索引使用相同的查询。\n请求示例\nGET target-test/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;a\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;fixed_interval\u0026#34;: \u0026#34;1h\u0026#34; } }, \u0026#34;total_passenger_count\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;passenger_count\u0026#34; } } } } 响应\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;a\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2023-08-01T10:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1690884000000, \u0026#34;doc_count\u0026#34;: 10 } ] }, \u0026#34;total_passenger_count\u0026#34;: { \u0026#34;value\u0026#34;: 38 } } } 注意 #  不能在同一个搜索请求中同时搜索汇总索引和非汇总索引。\n为自定义用户赋予rollup 权限 #  以下示例表示创建了一个自定义用户，这个用户只对 test 开头的索引具备所有权限，并赋予他 rollup 的权限，授权后，test_user将可以创建 rollup job，并对rollup 开头的索引有查询权限。\nPUT /_security/role/test_role { \u0026#34;indices\u0026#34;: [ { \u0026#34;names\u0026#34;: [ \u0026#34;test*\u0026#34;], \u0026#34;privileges\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } PUT /_security/user/test_user { \u0026quot;password\u0026quot; : \u0026quot;123456\u0026quot;, \u0026quot;roles\u0026quot; : [ \u0026quot;test_role\u0026quot;,\u0026quot;rollup_all\u0026quot; ] } 断点续跑 (1.13.0) #\n 创建Rollup 时如果检测到历史索引及元数据，会自动从中断的状态点继续处理，避免重复计算。\nRollup 配置增加了 window_start_time 字段，当重建 Rollup 时 会把历史 metadata 的最新时间戳自动写到 window_start_time 里， 通过观察 window_start_time 字段的值可以判断当前 Job 开始的 ’断点时间‘。\n","subcategory":null,"summary":"","tags":null,"title":"数据汇总","url":"/easysearch/main/docs/features/data-retention/rollup/"},{"category":null,"content":"Dictionary Decompounder 分词过滤器 #  dictionary_decompounder 分词过滤器用于根据预定义的词典将复合词拆分为其组成部分。该过滤器对于德语、荷兰语或芬兰语等复合词较为常见的语言特别有用，因为将复合词拆分可以提高搜索的相关性。dictionary_decompounder 分词过滤器会根据已知词列表判断每个词元（单词）是否可以拆分为更小的词元。如果该词元可以拆分为已知单词，过滤器就会为该词元生成子词元。\n相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  参数说明 #  词典复合词分词过滤器具有以下参数：\n   参数 必需/可选 数据类型 描述     word_list 除非配置了 word_list_path，否则为必需 字符串数组 过滤器用于拆分复合词的单词词典。   word_list_path 除非配置了 word_list，否则为必需 字符串 包含词典单词的文本文件的文件路径。可以接受绝对路径或相对于配置(config)目录的相对路径。词典文件必须采用 UTF-8 编码，且每个单词必须单独占一行。   min_word_size 可选 整数 会被考虑进行拆分的整个复合词的最小长度。如果复合词短于该值，则不会进行拆分。默认值为 5。   min_subword_size 可选 整数 任何子词的最小长度。如果子词短于该值，则不会包含在输出中。默认值为 2。   max_subword_size 可选 整数 任何子词的最大长度。如果子词长于该值，则不会包含在输出中。默认值为 15。   only_longest_match 可选 布尔值 如果设置为 true，则仅返回最长匹配的子词。默认值为 false。    参考样例 #  以下示例请求创建了一个名为 decompound_example 的新索引，并配置了一个带有词典复合词过滤器的分词器：\nPUT /decompound_example { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_dictionary_decompounder\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;dictionary_decompounder\u0026#34;, \u0026#34;word_list\u0026#34;: [\u0026#34;slow\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;turtle\u0026#34;] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;my_dictionary_decompounder\u0026#34;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /decompound_example/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;slowgreenturtleswim\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;slowgreenturtleswim\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;slow\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"字典复合词分词过滤器（Dictionary Decompounder）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/dictionary-decompounder/"},{"category":null,"content":"多字段搜索 #  真实的搜索很少是单个字段的 match。更常见的是：\n 同一条查询要在多个字段上执行（标题/正文/标签） 不同的查询片段要映射到不同字段（标题 vs 作者） 多个字段共同组成一个“实体”（姓名、地址），每个词可能落在不同字段里  难点在于：匹配结果怎么合并、相关性怎么计算。这一页会按权威指南的三种典型场景，给出可直接落地的查询结构。\n场景 1：多字符串、多字段（你知道每个词该搜哪个字段） #  例如你要找作者 Leo Tolstoy 写的《War and Peace》：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;War and Peace\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;author\u0026#34;: \u0026#34;Leo Tolstoy\u0026#34; } } ] } } } bool 查询采用“匹配越多越好”的策略：匹配到更多 should 子句的文档会得到更高 _score。\n语句优先级：用 boost 做权重分配 #  当有些子句更重要（例如标题、作者比译者更重要），可以给关键子句加 boost：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;War and Peace\u0026#34;, \u0026#34;boost\u0026#34;: 2 } } }, { \u0026#34;match\u0026#34;: { \u0026#34;author\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;Leo Tolstoy\u0026#34;, \u0026#34;boost\u0026#34;: 2 } } }, { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;translator\u0026#34;: \u0026#34;Constance Garnett\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;translator\u0026#34;: \u0026#34;Louise Maude\u0026#34; } } ] } } ] } } }  经验：boost 往往不需要非常大。通常从 1~10 试起，配合真实样本不断调试更可靠。\n 场景 2：单字符串、多字段（字段彼此“竞争”）：best_fields #  当用户输入的是一个“概念/短语”，并且你希望它尽量在同一个字段里被完整命中（例如：标题或正文之一），这就是 best_fields 场景。\n一个典型反例：用 bool/should 直接把 title 与 body 的 match 叠加，可能会把“两个字段都命中一个词”的文档排到“单个字段命中两个词”的文档前面。\n解决办法是用 dis_max：返回所有匹配，但把最佳匹配字段的得分作为主得分。\ndis_max 与 tie_breaker #  纯 dis_max 只吃“最佳字段”的分数，可能忽略“另一个字段也命中”的价值。可以用 tie_breaker 做折中：\n 主分数 = 最佳匹配字段的 _score 附加分 = 其他匹配字段 _score * tie_breaker  multi_match：best_fields 的更简洁写法 #  multi_match 会为每个字段生成 match，并按 type 选择合并策略。默认类型就是 best_fields（本质上就是 dis_max）。\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;quick brown fox\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;best_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;body\u0026#34; ], \u0026#34;tie_breaker\u0026#34;: 0.3, \u0026#34;minimum_should_match\u0026#34;: \u0026#34;30%\u0026#34; } } } 字段名通配与单字段 boost #  fields 支持通配写法与 ^ 权重语法：\n{ \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;quick brown fox\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;*_title\u0026#34;, \u0026#34;chapter_title^2\u0026#34; ] } } } 场景 3：同一段文本用多种方式索引（“信号叠加”）：most_fields #  为了提高召回与排序质量，一个常见策略是：把同一段文本索引到多个字段，每个字段使用不同分析链：\n 一个字段更“宽松”（词干、同义词、去音标等）→ 扩大召回 另一些字段更“精确”（原词、shingles 等）→ 提供额外相关性信号  这就是 most_fields：匹配字段越多越好，把各字段的得分合并起来。\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;jumping rabbits\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;most_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;title.std\u0026#34; ] } } } 你可以为“主字段”加大权重，让宽松字段主导召回，而精确字段更多起到“加分信号”的作用：\n{ \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;jumping rabbits\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;most_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title^10\u0026#34;, \u0026#34;title.std\u0026#34; ] } } }  most_fields 适合“同一内容、多种分析链”的场景；它并不适合“实体由多个字段拼出来”的场景（见下一节）。\n 跨字段实体搜索：cross_fields（词中心式） #  对于 person、address 这类实体，用户输入 Peter Smith，词可能分别落在 first_name 与 last_name 字段中。此时用 best_fields（找一个最佳字段）显然不对；用 most_fields（字段中心式）也会带来三个问题（权威指南的总结）：\n 在多个字段重复命中同一个词，可能比在不同字段命中更多不同词更“高分” operator / minimum_should_match 很难按预期工作（会要求同一个字段里包含所有词） TF/IDF 在不同字段上独立统计，可能导致“名/姓”的常见度被错误放大  cross_fields 使用词中心式逻辑：把多个字段视为一个大字段，对每个词在所有字段里找匹配，并对 IDF 做“混合/融合”（blended）处理。\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;peter smith\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cross_fields\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name\u0026#34;, \u0026#34;last_name\u0026#34; ] } } } cross_fields 的关键前提：分析器要一致 #  为了让 cross_fields 最优工作，被合并的字段应使用相同的分析器。分析器不同的字段会被分组，并以类似 best_fields 的方式加入结果中，这会影响 operator/minimum_should_match 的语义。\n跨字段也可以按字段 boost #  { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;peter smith\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cross_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title^2\u0026#34;, \u0026#34;description\u0026#34; ] } } } 索引时方案：copy_to（自定义“全字段”） #  如果你希望在索引时就把多个字段拼成一个“可搜索的组合字段”，可以用 copy_to（权威指南称为自定义 _all）：\nPUT /my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;first_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;copy_to\u0026#34;: \u0026#34;full_name\u0026#34; }, \u0026#34;last_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;copy_to\u0026#34;: \u0026#34;full_name\u0026#34; }, \u0026#34;full_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } 这样你既可以分别搜 first_name / last_name，也可以用 full_name 一把梭做实体搜索。\n不要把“精确值字段”混进 multi_match 的全文字段里 #  权威指南强调：把“全文字段”和“精确值字段”混在同一个 multi_match 里通常没意义。\n原因是：精确值字段（例如 keyword）不会分词，查询 peter smith 会被当作一个整体去匹配 title:\u0026quot;peter smith\u0026quot;，几乎必然匹配不到。\n实践建议：\n 全文相关：用 text 字段（match / multi_match） 精确过滤/聚合/排序：用 keyword 字段（term/terms、聚合、排序） 若必须混用，通常应拆成 bool：全文部分做 must/should，精确条件做 filter  小结：怎么选？ #   已知“词 → 字段”映射：用 bool 组织多条 match，用 boost 控权重 字段竞争（title vs body）：用 best_fields（dis_max / tie_breaker） 同一内容多分析链（信号叠加）：用 most_fields（multi-fields） 实体跨字段（名/姓、地址分段）：优先 cross_fields 或索引时用 copy_to 组合字段  下一步可以继续阅读：\n  全文检索  邻近匹配  Mapping 模式与最佳实践  参考手册（API 与参数） #    multi_match 查询（功能手册）  DisMax / dis_max 查询（功能手册）  bool 查询（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"多字段搜索","url":"/easysearch/main/docs/features/fulltext-search/multi-field-search/"},{"category":null,"content":"分割处理器 #  split 处理器用于根据指定的分隔符将字符串字段拆分为一个子字符串数组。\n以下是为 split 处理器提供的语法：\n{ \u0026#34;split\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_to_split\u0026#34;, \u0026#34;separator\u0026#34;: \u0026#34;\u0026lt;delimiter\u0026gt;\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;split_field\u0026#34; } } 配置参数 #  下表列出了 split 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要拆分的字符串的字段。   separator 必填 字符串分割时使用的分隔符。这可以是一个正则表达式模式。   preserve_trailing 可选 如果设置为 true ，则保留结果数组中的空尾字段（例如， '' ）。如果设置为 false ，则从结果数组中删除空尾字段。默认为 false 。   target_field 可选 存储子字符串数组的字段。如果没有指定，则字段将就地更新。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。如果设置为 true ，则处理器忽略字段中的缺失值，并保持 target_field 不变。默认为 false 。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 split_pipeline 的管道，该管道使用 split 处理器在 log_message 字段上按逗号字符分割，并将结果数组存储在 log_parts 字段中：\nPUT _ingest/pipeline/split_pipeline { \u0026#34;description\u0026#34;: \u0026#34;Split log messages by comma\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;split\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;log_message\u0026#34;, \u0026#34;separator\u0026#34;: \u0026#34;,\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;log_parts\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/split_pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;log_message\u0026#34;: \u0026#34;error,warning,info\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;log_message\u0026#34;: \u0026#34;error,warning,info\u0026#34;, \u0026#34;log_parts\u0026#34;: [ \u0026#34;error\u0026#34;, \u0026#34;warning\u0026#34;, \u0026#34;info\u0026#34; ] }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-04-26T22:29:23.207849376Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=split_pipeline { \u0026#34;log_message\u0026#34;: \u0026#34;error,warning,info\u0026#34; } 请求将文档索引到索引 testindex1 ，并在索引前将 log_message 字段以逗号分隔符拆分，如下面的响应所示：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 70, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 72, \u0026#34;_primary_term\u0026#34;: 47 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 响应显示 log_message 字段为以逗号分隔符分割的值数组：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 70, \u0026#34;_seq_no\u0026#34;: 72, \u0026#34;_primary_term\u0026#34;: 47, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;log_message\u0026#34;: \u0026#34;error,warning,info\u0026#34;, \u0026#34;log_parts\u0026#34;: [ \u0026#34;error\u0026#34;, \u0026#34;warning\u0026#34;, \u0026#34;info\u0026#34; ] } } ","subcategory":null,"summary":"","tags":null,"title":"分割处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/split/"},{"category":null,"content":"Jieba Search 分词器 #  jieba_search 分词器是 analysis-jieba 插件 提供的搜索模式分词器。它在精确分词的基础上对长词不做额外切分，适合搜索时使用。\n前提条件 #  bin/easysearch-plugin install analysis-jieba 与 jieba_index 的对比 #     分词器 模式 以\u0026quot;中华人民共和国\u0026quot;为例 适用场景     jieba_search 精确模式 中华人民共和国 搜索时   jieba_index 索引模式 中华、华人、人民、共和、共和国、中华人民共和国 索引时    使用示例 #  索引/搜索搭配 #  PUT my-jieba-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;jieba_idx\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;jieba_index\u0026#34; }, \u0026#34;jieba_srch\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;jieba_search\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;jieba_idx\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;jieba_srch\u0026#34; } } } } 测试分词 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;jieba_search\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国国歌\u0026#34; } 相关链接 #    Jieba Index 分词器 — 索引模式  文本分析  ","subcategory":null,"summary":"","tags":null,"title":"Jieba Search 分词器","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/jieba-search/"},{"category":null,"content":"邻近匹配 #  标准全文检索可以把字段视为“一袋词”（bag of words）：match 能告诉你这些词是否存在，但无法表达词与词之间的顺序与距离。这会导致一些明显不合理的匹配：\n Sue ate the alligator. The alligator ate Sue. Sue never goes anywhere without her alligator-skin purse.  搜索 sue alligator 时，上面三句都会被 match 命中，但它们的语义完全不同。邻近匹配（proximity matching）并不能“理解语义”，但它能利用位置信息来判断词项是否相邻、是否接近，从而让结果更符合直觉。\nmatch_phrase：短语匹配 #  match_phrase 是最常用的位置敏感查询：要求词项按顺序出现，并且位置相邻。\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;quick brown fox\u0026#34; } } } 它的核心逻辑是：查询字符串先被分析为词项列表，然后只保留那些同时包含全部词项，且词项位置关系一致的文档。\n词项位置（position）从哪来？ #  分词不仅会产出 tokens，还会产出 position。你可以用 _analyze 观察：\nGET /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Quick brown fox\u0026#34; } 典型输出会包含 position（示意）：\n quick：position 1 brown：position 2 fox：position 3  短语 quick brown fox 能匹配的条件就是：\n 三个词都出现 brown.position = quick.position + 1 fox.position = quick.position + 2   说明：从底层看，短语匹配本质上会落到更底层的 span 家族能力上，但绝大多数场景使用 match_phrase 即可。\n slop：让短语匹配“没那么严格” #  精确短语过于严格时，可以用 slop 允许“插词/换位”等轻微偏离：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;quick fox\u0026#34;, \u0026#34;slop\u0026#34;: 1 } } } } slop 可以理解为：为了让查询词序与文档对齐，允许“移动词项”多少步。slop 越大，越宽松，也越容易引入噪声与性能开销。\n多值字段与 position_increment_gap #  权威指南里有一个非常经典的坑：数组字段的每个元素在分析后会被“连续拼接”，导致跨元素短语被意外命中。\n示例文档：\nPUT /my_index/_doc/1 { \u0026#34;names\u0026#34;: [ \u0026#34;John Abraham\u0026#34;, \u0026#34;Lincoln Smith\u0026#34; ] } 短语查询：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;names\u0026#34;: \u0026#34;Abraham Lincoln\u0026#34; } } } 这可能会命中，因为 abraham 与 lincoln 在 position 上是相邻的（一个来自第一个数组元素，一个来自第二个）。\n解决方案是给数组元素之间留“足够大的空隙”：position_increment_gap：\nPUT /my_index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;names\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;position_increment_gap\u0026#34;: 100 } } } 这样第二个元素的词项会被整体平移 100 个 position；要跨元素做短语匹配就必须设置一个极大的 slop，从而默认情况下避免误命中。\n性能：短语/邻近查询为什么更贵？ #  match 只需要判断词项是否存在于倒排索引；match_phrase 还需要读取 positions 并做位置组合计算。权威指南给出的直觉是：\n term 查询远快于短语查询 有 slop 的邻近查询更贵  不过在常见文本场景里，短语查询通常仍然是可用的（毫秒级别很常见）。真正麻烦的是某些“病理数据”（例如大量重复词项的序列）叠加很大的 slop 值，会导致组合爆炸。\n只对 top-N 结果做邻近增强：rescore #  一个很实用的性能策略是：用便宜的 match 先找候选并排序，再用更贵的短语/邻近查询只对每分片 top-K 做重新评分（rescore）。\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;quick brown fox\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: \u0026#34;30%\u0026#34; } } }, \u0026#34;rescore\u0026#34;: { \u0026#34;window_size\u0026#34;: 50, \u0026#34;query\u0026#34;: { \u0026#34;rescore_query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;quick brown fox\u0026#34;, \u0026#34;slop\u0026#34;: 50 } } } } } } 直觉上：match 决定“进候选集的人”，rescore 决定“候选集里谁排得更前”。\nshingles：索引时的“词对信号”，更灵活也更快 #  短语/邻近查询严格要求词项存在并且靠近。若你希望捕捉“关联词对”并提升排序，而不是严格过滤，可以在索引时生成 shingles（常见是 bigram）。\n对句子 Sue ate the alligator：\n unigrams：sue、ate、the、alligator bigrams：sue ate、ate the、the alligator  这样你可以：\n 用 unigram 字段作为基础召回（必须） 用 bigram 字段作为加分信号（should）  生成 shingles（示意） #  PUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_shingle_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;shingle\u0026#34;, \u0026#34;min_shingle_size\u0026#34;: 2, \u0026#34;max_shingle_size\u0026#34;: 2, \u0026#34;output_unigrams\u0026#34;: false } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_shingle_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_shingle_filter\u0026#34; ] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;shingles\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_shingle_analyzer\u0026#34; } } } } } } 查询：unigram 负责召回，shingles 负责加分 #  GET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;the hungry alligator ate sue\u0026#34; } }, \u0026#34;should\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title.shingles\u0026#34;: \u0026#34;the hungry alligator ate sue\u0026#34; } } } } } 权威指南的经验是：shingles 查询像普通 match 一样高效（查询时），而短语/邻近查询需要 positions 计算。代价转移到了索引端（更多词项、更多磁盘）。\n小结 #   match_phrase 使用 positions 做严格短语匹配；slop 提供可控的宽松度 数组字段短语匹配要注意跨元素误命中，用 position_increment_gap 做隔离 性能上可采用“先 match、后 rescore”的两阶段思路，把昂贵计算只用于 top-N shingles 是索引时策略：用 bigram 作为相关性信号，通常更灵活也更快  下一步可以继续阅读：\n  部分匹配  多字段搜索  文本分析：同义词  参考手册（API 与参数） #    短语匹配（match_phrase）（功能手册）  短语前缀匹配（match_phrase_prefix）（功能手册）  Intervals 查询（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"邻近匹配","url":"/easysearch/main/docs/features/fulltext-search/proximity-matching/"},{"category":null,"content":"Pattern Replace 分词过滤器 #  pattern_replace 分词过滤器允许您使用正则表达式来修改词元。此过滤器会将词元中的模式替换为指定的值，这使您在对词元进行索引之前，能够灵活地转换或规范化词元。当您在分析过程中需要清理或规范文本时，该过滤器尤其有用。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参数说明 #  匹配替换分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     pattern 必需 字符串 一个正则表达式模式，用于匹配需要被替换的文本。   all 可选 布尔值 是否替换所有匹配的模式。如果为 false，则仅替换第一个匹配项。默认值为 true。   replacement 可选 字符串 用于替换匹配模式的字符串。默认值为空字符串。    参考样例 #  以下示例请求创建了一个名为 text_index 的新索引，并配置了一个带有匹配替换过滤器的分词器，用于将包含数字的词元替换为字符串 [NUM]：\nPUT /text_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;number_replace_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern_replace\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;\\\\d+\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;[NUM]\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;number_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;number_replace_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /text_index/_analyze { \u0026#34;text\u0026#34;: \u0026#34;Visit us at 98765 Example St.\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;number_analyzer\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;visit\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;us\u0026#34;, \u0026#34;start_offset\u0026#34;: 6, \u0026#34;end_offset\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;at\u0026#34;, \u0026#34;start_offset\u0026#34;: 9, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;[NUM]\u0026#34;, \u0026#34;start_offset\u0026#34;: 12, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;start_offset\u0026#34;: 18, \u0026#34;end_offset\u0026#34;: 25, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;st\u0026#34;, \u0026#34;start_offset\u0026#34;: 26, \u0026#34;end_offset\u0026#34;: 28, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 } ] } ","subcategory":null,"summary":"","tags":null,"title":"正则替换分词过滤器（Pattern Replace）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/pattern-replace/"},{"category":null,"content":"Pattern Capture 分词过滤器 #  pattern_capture 分词过滤器是一种功能强大的过滤器，它使用正则表达式根据特定模式来捕获和提取文本的部分内容。当你想要提取词元的特定部分，例如电子邮件域名、话题标签或数字，并将其重新用于进一步的分析或索引编制时，这个过滤器会非常有用。\n相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  参数说明 #  捕获匹配分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     patterns 必需 字符串数组 用于捕获文本部分的正则表达式数组。   preserve_original 必需 布尔值 是否在输出中保留原始词元。默认值为 true。    参考样例 #  以下示例请求创建了一个名为 email_index 的新索引，并配置了一个带有捕获匹配过滤器的分词器，以便从电子邮件地址中提取本地部分和域名。\nPUT /email_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;email_pattern_capture\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern_capture\u0026#34;, \u0026#34;preserve_original\u0026#34;: true, \u0026#34;patterns\u0026#34;: [ \u0026#34;^([^@]+)\u0026#34;, \u0026#34;@(.+)$\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;email_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;uax_url_email\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;email_pattern_capture\u0026#34;, \u0026#34;lowercase\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /email_index/_analyze { \u0026#34;text\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;email_analyzer\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 20, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;EMAIL\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;john.doe\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 20, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;EMAIL\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;example.com\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 20, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;EMAIL\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"正则捕获分词过滤器（Pattern Capture）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/pattern-capture/"},{"category":null,"content":"模糊匹配 #  结构化数据通常追求精确匹配，但好的全文检索往往需要一定的“容错”：用户可能会拼错名字、记不清准确写法，或者输入了常见变体。\n模糊匹配（Fuzzy matching）允许你在查询时匹配拼写接近的词项，并把“更接近”的结果排在前面或作为兜底补充。\n编辑距离与 fuzziness #  模糊匹配常以“编辑距离”定义两个词的相似度：把一个词变成另一个词需要的最少单字符编辑次数。\n一次编辑可能是：\n 替换：fox → box 插入：sic → sick 删除：black → back 相邻换位：star → tsar  fuzziness 参数用来限制允许的最大编辑距离：\n 0：不允许编辑（精确） 1：允许一次编辑（多数拼写错误属于这一档） 2：更宽松，但更容易引入噪声、性能也更差 AUTO：根据词长自动选择（短词更严格，长词更宽松）  实践上，若你发现 AUTO 返回的结果“太松”，把 fuzziness 设为 1 往往能得到更好的质量与性能平衡。\nfuzzy 查询：term 级别的模糊等价 #  fuzzy 查询可以看作是 term 查询的模糊版本。它不会对输入做分析（不分词、不归一化），而是直接在词典上找“编辑距离足够近”的候选词项：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;fuzzy\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;surprize\u0026#34;, \u0026#34;fuzziness\u0026#34;: 1 } } } } 性能保护：prefix_length 与 max_expansions #  模糊查询本质上要在词典里做扩展，候选词项越多，越可能拖垮性能。权威指南给出的两个重要“刹车”：\n prefix_length：前 N 个字符不参与模糊化（必须精确匹配）\n大部分拼写错误发生在词尾，提高前缀长度通常能显著减少扩展量。 max_expansions：限制最多扩展多少个候选词项\n防止查询扩展出成百上千个候选词导致“有匹配但没意义、还很慢”。  GET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;fuzzy\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;surprize\u0026#34;, \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34;, \u0026#34;prefix_length\u0026#34;: 3, \u0026#34;max_expansions\u0026#34;: 50 } } } } match/multi_match 的 fuzziness：更常见的用法 #  多数时候你不会直接使用 fuzzy 查询，而是在 match 查询中开启 fuzziness：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;SURPRIZE ME!\u0026#34;, \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34; } } } } 流程是：\n 先按字段分析链分词/归一化（例如小写、去音标） 对每个词项应用 fuzziness 做模糊扩展  multi_match 在 best_fields 或 most_fields 类型下也支持 fuzziness（权威指南的提示）：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;text\u0026#34;, \u0026#34;title\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;SURPRIZE ME!\u0026#34;, \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34; } } }  说明：某些类型的查询（例如短语类、cross_fields 等）可能不支持 fuzziness。遇到限制时，需要用“词项级查询 + 结构化组合”的方式重新设计查询。\n 模糊匹配与相关性：为什么不要让它主导排序 #  权威指南指出了一个典型陷阱：如果只有一篇文档拼错了名字，它在全局上更“稀有”，TF/IDF 可能会让这个错误拼写在相关性上更高，从而把错误结果排到正确结果前面。\n因此建议把模糊匹配作为：\n 兜底扩大召回：在精确/正常匹配结果不足时再启用 或在查询结构中赋予更低权重（较低 boost），避免破坏主排序  语音匹配（Phonetic matching） #  当编辑距离无法覆盖“读音相近但拼写差异很大”的情况时，可以考虑语音匹配（例如 Soundex、Metaphone、Double Metaphone 等算法），把词映射为语音编码来匹配。\n注意点：\n 语音算法通常针对特定语言设计（常见为英语/德语），通用性有限 语音匹配的目的通常是提高召回，不适合直接参与评分 建议与常规字段分开建多字段（例如 name.phonetic），在查询时作为补充信号  实务建议 #   默认优先使用“正常分析 + 精确匹配”，模糊匹配作为补充策略，而不是全局默认 对短关键词更谨慎：短词允许 1～2 次编辑会引入大量噪声 使用 prefix_length 与 max_expansions 保护集群，避免一次请求扩展出海量候选词项 在搜索体验上，常见策略是“先精确、后模糊”，并给模糊结果较低权重  小结 #   fuzziness 基于编辑距离提供拼写容错；AUTO 会按词长调整容错 fuzzy 查询是 term 级模糊；match/multi_match 的 fuzziness 更常用 用 prefix_length、max_expansions 控制扩展量与性能 模糊匹配更适合做兜底召回，不建议让其主导相关性排序  下一步可以继续阅读：\n  建议与纠错  相关性基础  加权与调参  参考手册（API 与参数） #    模糊查询（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"模糊匹配","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis-fuzzy/"},{"category":null,"content":"查询重打分 #  查询重打分（Rescore）允许你在初始查询返回 Top-N 结果后，用更复杂、更精细的查询对这些结果重新打分。这是一种二阶段打分策略：第一阶段用快速查询（如 match）粗筛，第二阶段用代价更高的查询（如 match_phrase、function_score）精排。\n使用场景 #   短语精排：先用 match 召回，再用 match_phrase 提升短语完全匹配的文档 复杂评分：初始召回后，对 Top 结果做 function_score 或 script_score 精细计算 向量混排：用 BM25 召回后，对 Top-N 用 kNN 重打分  基础用法 #  GET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;to be or not to be\u0026#34; } }, \u0026#34;rescore\u0026#34;: { \u0026#34;window_size\u0026#34;: 100, \u0026#34;query\u0026#34;: { \u0026#34;rescore_query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;to be or not to be\u0026#34;, \u0026#34;slop\u0026#34;: 2 } } }, \u0026#34;query_weight\u0026#34;: 0.7, \u0026#34;rescore_query_weight\u0026#34;: 1.2 } } } 参数说明 #  顶层参数 #     参数 类型 默认值 说明     window_size Integer 10 每个分片上参与重打分的 Top-N 文档数   query Object 必填 重打分查询配置    query 对象参数 #     参数 类型 默认值 说明     rescore_query Query DSL 必填 用于重打分的查询（任意 Query DSL）   query_weight Float 1.0 原始查询分数的权重   rescore_query_weight Float 1.0 重打分查询分数的权重   score_mode String total 原始分数与重打分分数的合并方式    score_mode 可选值 #     值 公式 说明     total（或 sum） $\\text{original} \\times w_1 + \\text{rescore} \\times w_2$ 加权求和（默认）   avg $\\frac{\\text{original} \\times w_1 + \\text{rescore} \\times w_2}{2}$ 加权平均   max $\\max(\\text{original} \\times w_1,\\ \\text{rescore} \\times w_2)$ 取最大值   min $\\min(\\text{original} \\times w_1,\\ \\text{rescore} \\times w_2)$ 取最小值   multiply（或 product） $\\text{original} \\times w_1 \\times \\text{rescore} \\times w_2$ 加权乘积    多级重打分 #  可以指定多个 rescore 对象组成数组，按顺序依次执行：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;love\u0026#34; } }, \u0026#34;rescore\u0026#34;: [ { \u0026#34;window_size\u0026#34;: 200, \u0026#34;query\u0026#34;: { \u0026#34;rescore_query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;love\u0026#34;, \u0026#34;slop\u0026#34;: 1 } } }, \u0026#34;query_weight\u0026#34;: 0.7, \u0026#34;rescore_query_weight\u0026#34;: 1.5 } }, { \u0026#34;window_size\u0026#34;: 50, \u0026#34;query\u0026#34;: { \u0026#34;rescore_query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: \u0026#34;_score * doc[\u0026#39;popularity\u0026#39;].value\u0026#34; } } }, \u0026#34;score_mode\u0026#34;: \u0026#34;multiply\u0026#34; } } ] }  每一级重打分都在上一级的结果基础上执行。\n 性能建议 #   window_size 越大，精度越高但开销越大。建议从 100~500 开始调试 重打分查询不影响召回率（只影响已召回文档的排序） 重打分不支持与 sort 一起使用（需要按 _score 排序才有意义） 每个分片独立执行重打分，因此实际重打分文档数 = window_size × 分片数  ","subcategory":null,"summary":"","tags":null,"title":"查询重打分","url":"/easysearch/main/docs/features/query-dsl/rescore/"},{"category":null,"content":"可搜索快照（Searchable Snapshots） #  Easysearch 快照搜索允许系统直接在对象存储（如 S3、MinIO、OSS）上挂载快照索引，让归档数据瞬间具备检索能力——无需将快照恢复为完整索引，即使是 PB 级快照数据也能在数分钟内完成挂载并开放搜索。\n 核心优势 #  分钟级上线，零恢复等待 #  传统快照恢复需要将全量数据拷贝到本地磁盘，耗时可能达数小时。快照搜索消除了这一步骤，仅挂载元数据，数据按需从对象存储拉取。\n极低的存储成本 #  核心数据保留在低成本的对象存储中，仅在查询时按需调用，大幅节省高性能磁盘（SSD）的占用空间。\n统一的搜索体验 #  快照搜索与在线集群共享统一的 API 和查询语法，业务人员无需学习新接口即可调取历史记录。\n 前置条件 #  配置 search 角色节点 #  只有角色为 search 的节点才能执行快照搜索。在 easysearch.yml 中配置：\nnode.name: search-node node.roles: [search] Docker 环境：\n- node.roles: [search]  集群中至少需要一个 search 角色节点才能使用可搜索快照功能。\n 注册快照仓库 #  以 MinIO（S3 兼容）为例：\nPUT _snapshot/my-repo { \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;access_key\u0026#34;: \u0026#34;minioadmin\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;minioadmin\u0026#34;, \u0026#34;bucket\u0026#34;: \u0026#34;es-bucket\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://127.0.0.1:9000\u0026#34;, \u0026#34;compress\u0026#34;: true } }  使用方法 #  1. 创建快照（如果尚未创建） #  PUT _snapshot/my-repo/snapshot-2024 { \u0026#34;indices\u0026#34;: \u0026#34;logs-2024-*\u0026#34;, \u0026#34;include_global_state\u0026#34;: false } 2. 挂载为可搜索快照索引 #  使用 _restore API 并指定 remote_snapshot 存储类型：\nPOST _snapshot/my-repo/snapshot-2024/_restore { \u0026#34;indices\u0026#34;: \u0026#34;logs-2024-01\u0026#34;, \u0026#34;rename_pattern\u0026#34;: \u0026#34;(.+)\u0026#34;, \u0026#34;rename_replacement\u0026#34;: \u0026#34;searchable-$1\u0026#34;, \u0026#34;storage_type\u0026#34;: \u0026#34;remote_snapshot\u0026#34; } 存储类型说明 #     类型 说明     local 所有快照数据下载到本地存储（传统恢复）   remote_snapshot 仅下载元数据，数据按需从远程仓库拉取    恢复参数 #     参数 类型 说明     indices string 要恢复的索引名称（支持通配符）   rename_pattern string 索引名称重命名的正则匹配模式   rename_replacement string 索引名称重命名的替换字符串   storage_type string 存储类型：local 或 remote_snapshot   ignore_unavailable boolean 是否忽略不存在的索引   include_global_state boolean 是否包含集群全局状态   include_aliases boolean 是否恢复别名   partial boolean 是否允许部分恢复    3. 搜索快照数据 #  像查询普通索引一样搜索：\nPOST /searchable-logs-2024-01/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2024-01-31\u0026#34; } } } } 支持完整的查询语法：过滤、排序、聚合等。\n 与异步搜索结合 #  快照搜索通常面向大时间跨度与大数据量场景，I/O 延迟可能较高。推荐与 异步搜索 结合使用：\nPOST /searchable-logs-2024-*/_async_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;OutOfMemoryError\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;monthly_errors\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; } } } }  冷热数据分层架构 #  快照搜索是实现数据分层存储的关键组件：\n┌─────────────┐ ┌─────────────┐ ┌──────────────────┐ │ 热数据 Hot │ ──→ │ 温数据 Warm │ ──→ │ 冷数据 Cold │ │ SSD 高性能 │ │ HDD 大容量 │ │ 对象存储（S3） │ │ 实时读写 │ │ 只读/低频 │ │ 快照搜索按需访问 │ └─────────────┘ └─────────────┘ └──────────────────┘  热数据：高性能集群实时访问 温数据：降低副本数、只读，仍在集群中 冷数据：低成本对象存储 + 快照搜索按需查询   配合 索引生命周期管理（ILM），可以自动化实现数据在各层之间的流转。\n  应用场景 #     场景 说明     历史日志与审计分析 安全审计、合规检查、问题回溯，无需恢复历史索引   长周期数据留存 满足监管或业务对数据长期保存的要求，同时控制成本   冷热数据分层 在线集群只保留热数据，历史数据迁移至快照存储   低频但高价值查询 大跨度、低频次、分析型查询，不影响在线系统性能     缓存机制 #  可搜索快照索引采用智能缓存策略：\n 频繁访问的段会被缓存到 search 节点的本地磁盘 最近最少使用（LRU） 策略自动淘汰不常访问的段 缓存数据与节点的通用索引共享磁盘空间   建议：为 search 角色节点配置充足的本地磁盘空间，以提升缓存命中率和查询性能。\n  注意事项 #   快照搜索索引是只读的，任何写入操作都会报错 查询性能取决于对象存储的 I/O 性能和网络带宽 首次查询可能较慢（冷启动），后续查询随缓存预热会加速 集群中至少需要一个 search 角色节点 对象存储的访问成本（API 调用次数）需要纳入运营预算考量   相关文档 #    备份与恢复  异步搜索  数据生命周期  ","subcategory":null,"summary":"","tags":null,"title":"可搜索快照","url":"/easysearch/main/docs/features/data-retention/searchable-snapshot/"},{"category":null,"content":"Sort 处理器 #  sort 处理器可以按升序或降序对项目数组进行排序。数值数组按数值排序，而字符串或混合数组（字符串和数字）按字典顺序排序。如果输入不是数组，处理器将抛出错误。\n以下是为 sort 处理器提供的语法：\n{ \u0026#34;description\u0026#34;: \u0026#34;Sort an array of items\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;sort\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;my_array_field\u0026#34;, \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } 配置参数 #  下表列出了 sort 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 要排序的字段。必须是数组。   order 可选 应用排序顺序。接受 asc 用于升序或 desc 用于降序。默认是 asc 。   target_field 可选 存储排序数组字段的名称。如果未指定，则排序数组将存储在原始数组（ field 变量）相同的字段中。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 sort-pipeline 的管道，使用 sort 处理器按降序对 my_field 进行排序，并将排序后的值存储在 sorted_field 中：\nPUT _ingest/pipeline/sort-pipeline { \u0026#34;description\u0026#34;: \u0026#34;Sort an array of items in descending order\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;sort\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;my_array_field\u0026#34;, \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;sorted_array\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/sort-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;my_array_field\u0026#34;: [3, 1, 4, 1, 5, 9, 2, 6, 5] } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;sorted_array\u0026#34;: [ 9, 6, 5, 5, 4, 3, 2, 1, 1 ], \u0026#34;my_array_field\u0026#34;: [ 3, 1, 4, 1, 5, 9, 2, 6, 5 ] }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-05-30T22:10:13.405692128Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPOST testindex1/_doc/1?pipeline=sort-pipeline { \u0026#34;my_array_field\u0026#34;: [3, 1, 4, 1, 5, 9, 2, 6, 5] } 请求将文档索引到索引 testindex1 中，然后按降序索引所有 my_array_field 文档，如下面的响应所示：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 9, \u0026#34;_primary_term\u0026#34;: 2 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"Sort 处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/sort/"},{"category":null,"content":"Jieba Index 分析器 #  jieba_index 分析器是为中文索引分词的分析器，使用 Jieba 分词器的索引模式。\n需要安装 analysis-jieba 插件。  分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n jieba_index 分词器：使用 Jieba 索引模式，适合索引时的细粒度分词 lowercase 分词过滤器：转换为小写  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;jieba_index\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;小明硕士毕业于中国科学院计算所\u0026#34; } 相关指南 #    文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"Jieba 索引分析器（Jieba Index）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/jieba-index-analyzer/"},{"category":null,"content":"Jieba Search 分析器 #  jieba_search 分析器是为中文搜索分词的分析器，使用 Jieba 分词器的搜索模式，在精确模式的基础上对长词再次切分。\n需要安装 analysis-jieba 插件。  分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n jieba_search 分词器：使用 Jieba 搜索模式，对长词进行二次切分以提高召回率 lowercase 分词过滤器：转换为小写  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;jieba_search\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;小明硕士毕业于中国科学院计算所\u0026#34; } 相关指南 #    文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"Jieba 搜索分析器（Jieba Search）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/jieba-search-analyzer/"},{"category":null,"content":"IK Max Word 分词器 #  ik_max_word 是 IK 分词器插件提供的细粒度分词模式。\n分词方式 #  最大词模式（Max Word）倾向于将文本分成最细粒度的词项，适合对召回率要求高的场景。\n示例 #  POST _analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;北京市朝阳区建国路1号\u0026#34; } 分析结果 #  [ \u0026#34;北京市\u0026#34;, \u0026#34;北京\u0026#34;, \u0026#34;市\u0026#34;, \u0026#34;朝阳区\u0026#34;, \u0026#34;朝阳\u0026#34;, \u0026#34;区\u0026#34;, \u0026#34;建国路\u0026#34;, \u0026#34;建国\u0026#34;, \u0026#34;路\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;号\u0026#34; ] 相关指南 #    IK分词器文档  文本分析基础  依赖插件 #   analysis-ik 插件  配置 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_ik_max_word\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ik_max_word\u0026#34; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"IK最大词分词器（IK Max Word）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/ik-max-word/"},{"category":null,"content":"Length 分词过滤器 #  length 分词过滤器用于从词元流中移除那些不符合指定长度标准（最小和最大长度值）的词元。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参数说明 #  长度分词过滤器可以使用以下参数进行配置。\n   参数 必填/可选 数据类型 描述     min 可选 整数 词元的最小长度。默认值为 0。   max 可选 整数 词元的最大长度。默认值为整数类型的最大值（2147483647）。    参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有长度过滤器的分词器：\nPUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;only_keep_4_to_10_characters\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;whitespace\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;length_4_to_10\u0026#34; ] } }, \u0026#34;filter\u0026#34;: { \u0026#34;length_4_to_10\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;length\u0026#34;, \u0026#34;min\u0026#34;: 4, \u0026#34;max\u0026#34;: 10 } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;only_keep_4_to_10_characters\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch is a great tool!\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;great\u0026#34;, \u0026#34;start_offset\u0026#34;: 16, \u0026#34;end_offset\u0026#34;: 21, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;tool!\u0026#34;, \u0026#34;start_offset\u0026#34;: 22, \u0026#34;end_offset\u0026#34;: 27, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"长度分词过滤器（Length）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/length/"},{"category":null,"content":"部分匹配 #  部分匹配允许用户输入“词的一部分”，并找出包含该片段的词项。全文检索很多时候并不需要它（分析器已经做了词干/同义词等处理），但在权威指南总结的几类场景中，部分匹配非常有价值：\n 精确值字段：邮编、产品序列号等以固定前缀/模式出现的值 输入即搜索（search-as-you-type）：用户还没输完就给出候选结果 复合词语言：德语/荷兰语等将多个词组合成长词  本页按“查询时方案 → 索引时优化”的顺序展开，并重点强调性能与可控性。\n查询时方案 1：prefix 前缀查询 #  查找所有以 W1 开头的邮编：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34;: { \u0026#34;postcode\u0026#34;: \u0026#34;W1\u0026#34; } } } prefix 是词项级查询，不会分析输入，它做的事情近似于：\n 扫描倒排词典（有序词列表），找到第一个以 W1 开头的词 收集该词对应的文档 ID 向后移动，继续收集所有以 W1 开头的词，直到遇到不再匹配的词  重要的性能结论（权威指南强调）：前缀越短，扫描的词越多；当唯一词很多时，前缀查询的伸缩性并不好。尽量使用更长的前缀，或者转向索引时优化方案（见下文）。\n查询时方案 2：wildcard / regexp（更灵活，也更危险） #  通配符查询（? 匹配一个字符，* 匹配 0 或多个字符）：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;postcode\u0026#34;: \u0026#34;W?F*HW\u0026#34; } } } 正则表达式查询：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;postcode\u0026#34;: \u0026#34;W[0-9].+\u0026#34; } } } 它们与 prefix 的本质一致：仍然需要扫描词典找出“匹配的词”，再收集对应文档。也因此有相同的风险：\n 避免左通配（如 *foo 或正则 .*foo）：会导致海量扩展 对唯一词非常多的字段慎用：会消耗大量资源  在 text 字段上使用时的一个常见坑 #  prefix/wildcard/regexp 都是词项级查询：如果你对 text 字段使用它们，它们匹配的是字段里被分析出的单个词项，不是“整段文本”。\n例如文本 Quick brown fox 通常会被分析为 quick、brown、fox：\n 会匹配：{ \u0026quot;regexp\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;br.*\u0026quot; } } 不会匹配：{ \u0026quot;regexp\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Qu.*\u0026quot; } }（因为索引里是 quick） 不会匹配：{ \u0026quot;regexp\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;quick br*\u0026quot; } }（因为 quick 与 brown 是两个词项）  查询时输入即搜索：match_phrase_prefix #  权威指南给出的“最简单的输入即搜索”方式是在查询时用 match_phrase_prefix：它和 match_phrase 一样做短语约束，但把最后一个词当作前缀：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;brand\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;johnnie walker bl\u0026#34;, \u0026#34;max_expansions\u0026#34;: 50 } } } } 要点：\n 只有最后一个词做前缀 max_expansions 很关键：限制前缀能扩展成多少个候选词项 用户每输入一个字符就会再触发一次查询，所以必须控制成本  索引时优化：把查询成本“搬到写入端” #  查询时方案的共同特点是：灵活，但可能慢。权威指南的建议是：在需要低延迟体验（例如 Web 实时搜索）时，把部分匹配的成本转移到索引阶段——只付一次写入代价，换取每次查询更快。\n1）edge n-grams：索引时输入即搜索 #  边界 n-gram（edge n-gram）会为词生成前缀序列，例如 quick：\n q、qu、qui、quic、quick  配置一个用于索引的 edge_ngram 过滤器 + 分析器（示意）：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;autocomplete_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;edge_ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 1, \u0026#34;max_gram\u0026#34;: 20 } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;autocomplete\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;autocomplete_filter\u0026#34; ] } } } } } 关键点（来自权威指南）：不要让搜索端也用 autocomplete 分析器，否则查询会被扩展成大量前缀词项（例如 b br bro ...），会把不相关结果拉进来。\n更合理的做法是：\n 索引时：用 autocomplete 生成前缀 搜索时：用 standard（只保留用户输入的完整词项）  可以在映射里为字段设置 search_analyzer（示意）：\nPUT /my_index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;autocomplete\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;standard\u0026#34; } } } 2）Completion suggester：更快的前缀建议 #  权威指南指出：对“名字/品牌”这种词序相对固定的补全场景，completion suggester 会把所有可能完成项编码进内存结构（FST），前缀查找速度非常快。\n这类能力在本书对应到“建议（Suggester）”体系，详见：\n  建议  3）用 edge n-grams 做结构化值（邮编） #  邮编这类值通常不需要分词，但仍可用 keyword tokenizer 把它作为一个整体 token，再用 edge n-grams 生成前缀（权威指南的技巧）：\n 索引时：keyword tokenizer + edge_ngram 搜索时：keyword tokenizer（保持精确）  复合词：用 n-grams 做“词内匹配” #  对德语等复合词语言，权威指南给出另一种更通用但更“霰弹枪”的做法：对词做 n-gram（常从 trigram 开始），把词内部片段也索引出来。\n示意（trigram）：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;trigrams_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 3, \u0026#34;max_gram\u0026#34;: 3 } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;trigrams\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;trigrams_filter\u0026#34; ] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;trigrams\u0026#34; } } } } 这种方法能显著提升召回，但可能引入奇怪匹配（例如只命中一个 trigram 的噪声）。权威指南建议用 minimum_should_match 把噪声剪掉。\n小结 #   prefix/wildcard/regexp 都会扫描词典扩展候选词项：越短越危险，左通配最危险 match_phrase_prefix 可快速实现“查询时输入即搜索”，务必设置 max_expansions 低延迟场景建议做索引时优化：edge n-grams + search_analyzer 分离 复合词可用 n-grams 做词内匹配，但要配合 minimum_should_match 控噪  下一步可以继续阅读：\n  邻近匹配  多字段搜索  建议  参考手册（API 与参数） #    前缀查询与 Prefix 相关选项（功能手册）  通配符查询（功能手册）  正则查询与语法（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"部分匹配","url":"/easysearch/main/docs/features/fulltext-search/partial-matching/"},{"category":null,"content":"设置处理器 #  set 处理器向文档中添加或更新字段。它设置一个字段并将其与指定的值关联。如果该字段已存在，则其值将被提供的值替换，除非设置了 override 参数为 false。当 override 为 false 且指定的字段存在时，该字段的值保持不变。\n以下是为 set 处理器提供的语法：\n{ \u0026#34;description\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;new_field\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;some_value\u0026#34; } } ] } 配置参数 #  下表列出了 set 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 要设置或更新的字段的名称。支持模板使用。   value 必填 字段分配的值。支持模板使用。   override 可选 一个布尔标志，用于确定处理器是否应覆盖字段的现有值。   ignore_empty_value 可选 一个布尔标志，用于确定处理器是否应忽略 null 值或空字符串。默认为 false 。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 set-pipeline 的管道，该管道使用 set 处理器向文档添加一个新字段 new_field ，其值为 some_value ：\nPUT _ingest/pipeline/set-pipeline { \u0026#34;description\u0026#34;: \u0026#34;Adds a new field \u0026#39;new_field\u0026#39; with the value \u0026#39;some_value\u0026#39;\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;new_field\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;some_value\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/set-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;existing_field\u0026#34;: \u0026#34;value\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;existing_field\u0026#34;: \u0026#34;value\u0026#34;, \u0026#34;new_field\u0026#34;: \u0026#34;some_value\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-05-30T21:56:15.066180712Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPOST testindex1/_doc/1?pipeline=set-pipeline { \u0026#34;existing_field\u0026#34;: \u0026#34;value\u0026#34; } 请求将文档索引到索引 testindex1 中，然后索引所有将 new_field 设置为 some_value 的文档，如下面的响应所示：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"设置处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/set/"},{"category":null,"content":"索引模板 #  日志、指标、审计事件这类数据，常见模式是\u0026quot;按天/按月一个新索引\u0026quot;。如果每次新索引都要手动 PUT /index 配 settings + mappings，很快就会变成运维噩梦。\n索引模板（Index Template） 就是为了解决这个问题：当新索引（手动或自动）被创建时，只要名字命中规则，就自动套用一组预配置——索引设置、映射和别名。\n 先了解： 索引管理：创建、删除与重建索引 | 时间序列建模\n Easysearch 支持三类模板：\n   类型 API 路径 说明     可组合索引模板 _index_template 推荐使用，支持组件模板组合   组件模板 _component_template 可复用的模板构建块，被可组合模板引用   遗留索引模板 _template 旧版 API，建议迁移到可组合模板     注意：可组合索引模板（_index_template）优先级高于遗留模板（_template）。如果同时存在匹配的可组合模板和遗留模板，将使用可组合模板。\n  可组合索引模板 #  可组合索引模板是推荐的模板方式，支持通过 composed_of 引用组件模板，实现模板的模块化组合。\n创建模板 #  PUT _index_template/daily_logs { \u0026#34;index_patterns\u0026#34;: [\u0026#34;logs-2024-01-*\u0026#34;], \u0026#34;priority\u0026#34;: 100, \u0026#34;template\u0026#34;: { \u0026#34;aliases\u0026#34;: { \u0026#34;my_logs\u0026#34;: {} }, \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 2, \u0026#34;number_of_replicas\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; }, \u0026#34;value\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } } 请求参数 #     参数 类型 描述 默认值     create boolean 为 true 时，仅在模板不存在时创建，不会覆盖已有模板 false   cause string 创建/更新模板的原因（记录到日志） —   master_timeout time 连接主节点的超时时间 30s    请求体参数 #     参数 类型 描述 必填     index_patterns array 索引名称匹配模式列表 是   template object 包含 aliases、settings、mappings 的模板定义 否   composed_of array 引用的组件模板名称列表，按顺序合并 否   priority number 模板优先级，值越大优先级越高 0   version number 模板版本号（用户自定义，仅供管理使用） —   _meta object 模板的元数据信息 —   data_stream object 设置后匹配的索引将作为数据流创建 —    查看模板 #  # 获取指定模板 GET _index_template/daily_logs 获取所有可组合模板 GET _index_template\n通配符匹配 GET _index_template/daily* 查询参数 #\n    参数 类型 描述 默认值     flat_settings boolean 以扁平格式返回 Settings false   local boolean 从本地节点返回信息，不查询主节点 false   master_timeout time 连接主节点的超时时间 30s    检查模板是否存在 #  HEAD _index_template/daily_logs 返回 200 表示存在，404 表示不存在。\n删除模板 #  DELETE _index_template/daily_logs 查询参数 #     参数 类型 描述     timeout time 操作超时时间   master_timeout time 连接主节点的超时时间    模拟模板 #  在正式创建模板前，可以使用模拟 API 预览模板的合并效果。\n模拟已有模板 #  查看一个已有模板（或请求体中提供的模板定义）的最终合并结果：\nPOST _index_template/_simulate/daily_logs 模拟索引匹配 #  查看对于一个具体的索引名称，最终会应用哪些模板设置：\nPOST _index_template/_simulate_index/logs-2024-01-15 也可以在请求体中提供一个新的模板定义，模拟它加入系统后的效果：\nPOST _index_template/_simulate_index/logs-2024-01-15 { \u0026#34;index_patterns\u0026#34;: [\u0026#34;logs-*\u0026#34;], \u0026#34;priority\u0026#34;: 200, \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 5 } } }  组件模板 #  组件模板是可复用的模板构建块，不能直接应用于索引，只能被可组合索引模板通过 composed_of 引用。\n适用场景：多个索引模板需要共享相同的 Settings 或 Mappings 片段。\n创建组件模板 #  PUT _component_template/base_settings { \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;index.refresh_interval\u0026#34;: \u0026#34;5s\u0026#34; } } } PUT _component_template/log_mappings { \u0026#34;template\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;message\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;level\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;host\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } 查询参数 #     参数 类型 描述     create boolean 仅在不存在时创建   timeout time 操作超时时间   master_timeout time 连接主节点的超时时间    在索引模板中引用组件模板 #  PUT _index_template/logs_template { \u0026#34;index_patterns\u0026#34;: [\u0026#34;logs-*\u0026#34;], \u0026#34;priority\u0026#34;: 100, \u0026#34;composed_of\u0026#34;: [\u0026#34;base_settings\u0026#34;, \u0026#34;log_mappings\u0026#34;], \u0026#34;template\u0026#34;: { \u0026#34;aliases\u0026#34;: { \u0026#34;logs\u0026#34;: {} } } } composed_of 中的组件模板按顺序合并，后面的覆盖前面的同名设置。索引模板自身的 template 设置优先级最高，会覆盖所有组件模板的设置。\n查看组件模板 #  # 获取指定组件模板 GET _component_template/base_settings 获取所有组件模板 GET _component_template 检查组件模板是否存在 #\n HEAD _component_template/base_settings 删除组件模板 #  DELETE _component_template/base_settings  注意：删除正在被索引模板引用的组件模板会导致错误。请先解除引用关系。\n  模板优先级与合并规则 #  当一个索引名称匹配多个模板时，按以下规则处理：\n 可组合模板优先：如果同时匹配可组合模板和遗留模板，使用可组合模板 priority 决定优先级：多个可组合模板匹配时，选择 priority 最高的 组件模板按顺序合并：composed_of 列表中的组件模板从左到右合并，后面覆盖前面 索引模板自身设置最高优先：索引模板的 template 字段会覆盖组件模板的同名设置  工程建议：\n 让全局模板只做\u0026quot;底座\u0026quot;（通用的 refresh 策略、通用动态模板约束） 让业务模板决定分片、副本、映射与别名 任何\u0026quot;例外\u0026quot;用更窄的 index_patterns 覆盖，而不是在通用模板里堆 if/else  示例：\n# 组件模板：基础设置 PUT _component_template/base { \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1 } } } 组件模板：高性能设置 PUT _component_template/performance { \u0026quot;template\u0026quot;: { \u0026quot;settings\u0026quot;: { \u0026quot;number_of_shards\u0026quot;: 3, \u0026quot;refresh_interval\u0026quot;: \u0026quot;10s\u0026quot; } } }\n索引模板：组合以上两个组件 PUT _index_template/my_template { \u0026quot;index_patterns\u0026quot;: [\u0026quot;myindex-*\u0026quot;], \u0026quot;composed_of\u0026quot;: [\u0026quot;base\u0026quot;, \u0026quot;performance\u0026quot;], \u0026quot;priority\u0026quot;: 100, \u0026quot;template\u0026quot;: { \u0026quot;settings\u0026quot;: { \u0026quot;number_of_replicas\u0026quot;: 2 } } } 最终效果：number_of_shards = 3（performance 覆盖 base），refresh_interval = 10s，number_of_replicas = 2（索引模板自身设置）。\n 把别名写进模板 #  模板里配置别名非常实用，常见用法：\n logs_write：写入别名（通常只指向当前写索引） logs_read：读取别名（指向多个历史索引）  但注意：模板只能\u0026quot;给新索引加别名\u0026quot;，不会自动把老索引从别名里移走。\n如果你想维持\u0026quot;只查最近 90 天\u0026quot;的 logs_recent：\n 新索引会自动加入 logs_recent 但 90 天前的老索引仍需要你定期从别名里移除，或直接删除索引  这属于\u0026quot;数据退役/保留策略\u0026quot;的范筹，见： 数据生命周期管理。\n 遗留索引模板 #  遗留模板使用 _template API，功能较为有限，不支持组件模板组合。建议新项目使用可组合索引模板。\n创建遗留模板 #  PUT _template/legacy_logs { \u0026#34;index_patterns\u0026#34;: [\u0026#34;legacy-logs-*\u0026#34;], \u0026#34;order\u0026#34;: 0, \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } } } }  注意：遗留模板使用 order 控制优先级（数值越大优先级越高），而可组合模板使用 priority。\n 查看遗留模板 #  # 获取指定模板 GET _template/legacy_logs 获取所有遗留模板 GET _template\n查看所有模板列表 GET _cat/templates 删除遗留模板 #\n DELETE _template/legacy_logs  常见坑 #   自动创建索引的风险：拼错索引名也会自动创建新索引，导致数据散落。生产里建议至少做命名白名单或关闭自动创建。 动态映射的意外字段：日志字段飘忽不定时，动态映射可能推断出错误类型（比如把数字当字符串），建议用 dynamic_templates 做兜底约束。 分片数拍脑袋：模板里把 number_of_shards 设很大，后果通常是集群碎成渣。分片策略见： 扩缩容与分片。   API 参考汇总 #  可组合索引模板 #     操作 方法 端点     创建/更新 PUT / POST /_index_template/{name}   查看 GET /_index_template 或 /_index_template/{name}   检查存在 HEAD /_index_template/{name}   删除 DELETE /_index_template/{name}   模拟模板 POST /_index_template/_simulate/{name}   模拟索引 POST /_index_template/_simulate_index/{name}    组件模板 #     操作 方法 端点     创建/更新 PUT / POST /_component_template/{name}   查看 GET /_component_template 或 /_component_template/{name}   检查存在 HEAD /_component_template/{name}   删除 DELETE /_component_template/{name}    遗留索引模板 #     操作 方法 端点     创建/更新 PUT / POST /_template/{name}   查看 GET /_template 或 /_template/{name}   检查存在 HEAD /_template/{name}   删除 DELETE /_template/{name}     相关文档 #    数据流：使用索引模板创建数据流  索引生命周期管理：在模板中关联 ILM 策略  数据生命周期：完整的数据保留策略体系  ","subcategory":null,"summary":"","tags":null,"title":"索引模板","url":"/easysearch/main/docs/operations/data-management/index-templates/"},{"category":null,"content":"Elision 分词过滤器 #  elision 分词过滤器用于从某些语言的单词中去除省略的字符。省略现象通常出现在像法语这样的语言中，在这些语言里，单词常常会发生缩合，并与后面的单词结合，常见的方式是省略一个元音字母，并用一个撇号来替代。\n 注意：elision 分词过滤器已经在以下语言分词器中预先配置好了：加泰罗尼亚语（catalan）、法语（french）、爱尔兰语（irish）和意大利语（italian）。\n 相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参数说明 #  自定义省略词分词过滤器可使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     articles 若未配置 articles_path 则为必需 字符串数组 定义当某些冠词或短词作为省略形式的一部分出现时，哪些应该被移除。   articles_path 若未配置 articles 则为必需 字符串 指定在分析过程中应被移除的自定义冠词列表的路径。   articles_case 可选 布尔值 指定在匹配省略形式时，该过滤器是否区分大小写。默认值为 false。    参考样例 #  法语中默认的省略形式集合包括 l'、m'、t'、qu'、n'、s'、j'、d'、c'、jusqu'、quoiqu'、lorsqu' 和 puisqu'。你可以通过配置 french_elision 分词过滤器来更新这个集合。以下示例请求创建了一个名为 french_texts 的新索引，并配置了一个带有 french_elision 过滤器的分词器：\nPUT /french_texts { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;french_elision\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;elision\u0026#34;, \u0026#34;articles\u0026#34;: [ \u0026#34;l\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;n\u0026#34;, \u0026#34;s\u0026#34;, \u0026#34;j\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;french_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;french_elision\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;french_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /french_texts/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;french_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;L\u0026#39;étudiant aime l\u0026#39;école et le travail.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;étudiant\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;aime\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;école\u0026#34;, \u0026#34;start_offset\u0026#34;: 16, \u0026#34;end_offset\u0026#34;: 23, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;et\u0026#34;, \u0026#34;start_offset\u0026#34;: 24, \u0026#34;end_offset\u0026#34;: 26, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;le\u0026#34;, \u0026#34;start_offset\u0026#34;: 27, \u0026#34;end_offset\u0026#34;: 29, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;travail\u0026#34;, \u0026#34;start_offset\u0026#34;: 30, \u0026#34;end_offset\u0026#34;: 37, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 } ] } ","subcategory":null,"summary":"","tags":null,"title":"省略词分词过滤器（Elision）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/elision/"},{"category":null,"content":"Apostrophe 分词过滤器 #  apostrophe 分词过滤器的主要功能是移除所有格形式的撇号以及跟在撇号后面的所有内容。这在分析那些大量使用撇号的语言的文本时非常有用，比如土耳其语。在土耳其语中，撇号用于将词根与后缀分隔开来，这些后缀包括所有格后缀、格标记以及其他语法词尾。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参考样例 #  以下示例请求创建了一个名为 custom_text_index 的新索引，在索引设置中配置了一个自定义分词器，并在映射中使用该分词器：\nPUT /custom_text_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;apostrophe\u0026#34; ] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;custom_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /custom_text_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;John\u0026#39;s car is faster than Peter\u0026#39;s bike\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;john\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;faster\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 20, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;than\u0026#34;, \u0026#34;start_offset\u0026#34;: 21, \u0026#34;end_offset\u0026#34;: 25, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;peter\u0026#34;, \u0026#34;start_offset\u0026#34;: 26, \u0026#34;end_offset\u0026#34;: 33, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 }, { \u0026#34;token\u0026#34;: \u0026#34;bike\u0026#34;, \u0026#34;start_offset\u0026#34;: 34, \u0026#34;end_offset\u0026#34;: 38, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 6 } ] }  内置的省略符号分词过滤器并不适用于像法语这样的语言，在法语中撇号会出现在单词的开头。例如，C'est l'amour de l'école 这句话使用该过滤器分词后将会得到四个词元：“C”、“l”、“de” 和 “l”。\n ","subcategory":null,"summary":"","tags":null,"title":"撇号分词过滤器（Apostrophe）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/apostrophe/"},{"category":null,"content":"Truncate 分词过滤器 #  truncate 分词过滤器用于缩短超过指定长度的词元。它会将词元修剪至最大字符数，确保超过该限制的词元被截断。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参数说明 #  截断分词过滤器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     length 可选 整数 指定生成的词元的最大长度。默认值为 10。    参考样例 #  以下示例请求创建了一个名为 truncate_example 的新索引，并配置了一个带有截断过滤器的分词器。\nPUT /truncate_example { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;truncate_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;truncate\u0026#34;, \u0026#34;length\u0026#34;: 5 } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;truncate_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;truncate_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /truncate_example/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;truncate_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch is powerful and scalable\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;easys\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;power\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 22, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;and\u0026#34;, \u0026#34;start_offset\u0026#34;: 23, \u0026#34;end_offset\u0026#34;: 26, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;scala\u0026#34;, \u0026#34;start_offset\u0026#34;: 27, \u0026#34;end_offset\u0026#34;: 35, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"截断分词过滤器（Truncate）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/truncate/"},{"category":null,"content":"快照生命周期管理 #  快照生命周期管理（Snapshot Lifecycle Management, SLM）提供自动化的快照创建与清理能力。 通过配置 SLM 策略，您可以按照预定计划自动创建快照，并根据保留条件自动删除过期快照。\n 策略配置参考 #  策略结构 #  { \u0026#34;description\u0026#34;: \u0026#34;策略描述\u0026#34;, \u0026#34;creation\u0026#34;: { ... }, \u0026#34;deletion\u0026#34;: { ... }, \u0026#34;snapshot_config\u0026#34;: { ... }, \u0026#34;notification\u0026#34;: { ... } } creation — 快照创建配置 #     参数 描述 类型 是否必需     schedule 创建快照的时间计划。 object 是   time_limit 创建快照的最大等待时间。 string 否    schedule — 时间计划 #  支持 cron 表达式或固定间隔两种格式：\nCron 格式：\n{ \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 8 * * *\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } } 固定间隔格式：\n{ \u0026#34;interval\u0026#34;: { \u0026#34;start_time\u0026#34;: 1685348095913, \u0026#34;period\u0026#34;: 1, \u0026#34;unit\u0026#34;: \u0026#34;Hours\u0026#34; } } deletion — 快照删除配置 #     参数 描述 类型 是否必需     schedule 执行删除检查的时间计划。省略时使用创建计划的时间。 object 否   condition 触发删除的条件。 object 是   time_limit 删除快照的最大等待时间。 string 否    condition — 删除条件 #     参数 描述 类型 是否必需 验证     max_age 删除超过此年龄的快照。 string 条件必需 ¹ —   max_count 当快照总数超过此值时删除最旧的快照。 int 条件必需 ¹ 必须 \u0026gt; min_count   min_count 始终保留的最小快照数量。 int 否 必须 \u0026gt; 0，默认 1     ¹ max_age 和 max_count 中至少需要指定一个。\n snapshot_config — 快照配置 #     参数 描述 类型 是否必需     repository 快照仓库名称。 string 是   date_format 快照名称中的日期格式。 string 否   date_format_timezone 日期格式使用的时区。 string 否   indices 要包含在快照中的索引（支持通配符）。 string 否   ignore_unavailable 是否忽略不可用的索引。 string 否   include_global_state 是否包含集群全局状态。 string 否   partial 是否允许部分快照。 string 否   metadata 快照元数据（任意键值对）。 object 否    notification — 通知配置（可选） #     参数 描述 类型 是否必需     channel 通知渠道配置。 object 是   conditions 触发通知的事件。 object 否    通知触发条件 #     参数 描述 类型 默认值     creation 创建快照时通知。 boolean true   deletion 删除快照时通知。 boolean false   failure 操作失败时通知。 boolean false   time_limit_exceeded 超过时间限制时通知。 boolean false     创建策略 #  创建一个新的快照管理策略。\n请求 #  POST /_slm/policies/{policyName} 请求示例 #  以下策略配置：\n 每天上午 8 点自动创建快照，名称格式为 yyyy-MM-dd-HH:mm，存储在 my_backup 仓库 每天凌晨 1 点检查并删除超过 7 天的快照，保留至少 7 个，最多 21 个 创建和删除的超时限制均为 1 小时  POST /_slm/policies/daily-policy { \u0026#34;description\u0026#34;: \u0026#34;每日快照策略\u0026#34;, \u0026#34;creation\u0026#34;: { \u0026#34;schedule\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 8 * * *\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } }, \u0026#34;time_limit\u0026#34;: \u0026#34;1h\u0026#34; }, \u0026#34;deletion\u0026#34;: { \u0026#34;schedule\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 1 * * *\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } }, \u0026#34;condition\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;max_count\u0026#34;: 21, \u0026#34;min_count\u0026#34;: 7 }, \u0026#34;time_limit\u0026#34;: \u0026#34;1h\u0026#34; }, \u0026#34;snapshot_config\u0026#34;: { \u0026#34;date_format\u0026#34;: \u0026#34;yyyy-MM-dd-HH:mm\u0026#34;, \u0026#34;date_format_timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34;, \u0026#34;indices\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;my_backup\u0026#34;, \u0026#34;ignore_unavailable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;include_global_state\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;partial\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;any_key\u0026#34;: \u0026#34;any_value\u0026#34; } } } 响应示例 #  { \u0026#34;_id\u0026#34;: \u0026#34;daily-policy-sm-policy\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;sm_policy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;daily-policy\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;每日快照策略\u0026#34;, \u0026#34;schema_version\u0026#34;: 17, \u0026#34;creation\u0026#34;: { \u0026#34;schedule\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 8 * * *\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } }, \u0026#34;time_limit\u0026#34;: \u0026#34;1h\u0026#34; }, \u0026#34;deletion\u0026#34;: { \u0026#34;schedule\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 1 * * *\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } }, \u0026#34;condition\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;min_count\u0026#34;: 7, \u0026#34;max_count\u0026#34;: 21 }, \u0026#34;time_limit\u0026#34;: \u0026#34;1h\u0026#34; }, \u0026#34;snapshot_config\u0026#34;: { \u0026#34;indices\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;any_key\u0026#34;: \u0026#34;any_value\u0026#34; }, \u0026#34;ignore_unavailable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;date_format_timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34;, \u0026#34;include_global_state\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;date_format\u0026#34;: \u0026#34;yyyy-MM-dd-HH:mm\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;my_backup\u0026#34;, \u0026#34;partial\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;schedule\u0026#34;: { \u0026#34;interval\u0026#34;: { \u0026#34;start_time\u0026#34;: 1685348095913, \u0026#34;period\u0026#34;: 1, \u0026#34;unit\u0026#34;: \u0026#34;Minutes\u0026#34; } }, \u0026#34;enabled\u0026#34;: true, \u0026#34;last_updated_time\u0026#34;: 1685348095938, \u0026#34;enabled_time\u0026#34;: 1685348095909 } }  获取策略 #  获取单个策略 #  GET /_slm/policies/{policyName} 响应示例 #  { \u0026#34;_id\u0026#34;: \u0026#34;daily-policy-sm-policy\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;sm_policy\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;daily-policy\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;每日快照策略\u0026#34;, \u0026#34;schema_version\u0026#34;: 17, \u0026#34;creation\u0026#34;: { \u0026#34;...\u0026#34; : \u0026#34;...\u0026#34; }, \u0026#34;deletion\u0026#34;: { \u0026#34;...\u0026#34; : \u0026#34;...\u0026#34; }, \u0026#34;snapshot_config\u0026#34;: { \u0026#34;...\u0026#34; : \u0026#34;...\u0026#34; }, \u0026#34;schedule\u0026#34;: { \u0026#34;...\u0026#34; : \u0026#34;...\u0026#34; }, \u0026#34;enabled\u0026#34;: true, \u0026#34;last_updated_time\u0026#34;: 1685348095938, \u0026#34;enabled_time\u0026#34;: 1685348095909 } } 获取所有策略 #  GET /_slm/policies    查询参数 描述 类型 默认值     size 每页返回数量。 int 20   from 分页起始偏移量。 int 0   sortField 排序字段。 string —   sortOrder 排序方向。 string asc   queryString 搜索过滤条件。 string *    响应示例 #  { \u0026#34;policies\u0026#34;: [ { \u0026#34;_id\u0026#34;: \u0026#34;daily-policy-sm-policy\u0026#34;, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;sm_policy\u0026#34;: { \u0026#34;...\u0026#34; : \u0026#34;...\u0026#34; } } ], \u0026#34;total_policies\u0026#34;: 1 }  更新策略 #  更新已有策略。必须指定 if_seq_no 和 if_primary_term 进行乐观并发控制。\n请求 #  PUT /_slm/policies/{policyName}?if_seq_no=0\u0026amp;if_primary_term=1 请求体格式与创建策略相同。\n请求示例 #  PUT /_slm/policies/daily-policy?if_seq_no=0\u0026amp;if_primary_term=1 { \u0026#34;description\u0026#34;: \u0026#34;每日快照策略（更新）\u0026#34;, \u0026#34;creation\u0026#34;: { \u0026#34;schedule\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 9 * * *\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } }, \u0026#34;time_limit\u0026#34;: \u0026#34;1h\u0026#34; }, \u0026#34;deletion\u0026#34;: { \u0026#34;schedule\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 1 * * *\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } }, \u0026#34;condition\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;max_count\u0026#34;: 21, \u0026#34;min_count\u0026#34;: 7 }, \u0026#34;time_limit\u0026#34;: \u0026#34;1h\u0026#34; }, \u0026#34;snapshot_config\u0026#34;: { \u0026#34;date_format\u0026#34;: \u0026#34;yyyy-MM-dd-HH:mm\u0026#34;, \u0026#34;date_format_timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34;, \u0026#34;indices\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;my_backup\u0026#34;, \u0026#34;ignore_unavailable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;include_global_state\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;partial\u0026#34;: \u0026#34;true\u0026#34; } }  查看策略状态 #  使用 _explain API 查看策略的当前执行状态，包括下次创建/删除快照的时间，以及最近一次执行的结果。\n支持通配符匹配多个策略。\n请求 #  GET /_slm/policies/{policyName}/_explain 请求示例 #  GET /_slm/policies/daily*/_explain 响应示例 #  { \u0026#34;policies\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;daily-policy\u0026#34;, \u0026#34;creation\u0026#34;: { \u0026#34;current_state\u0026#34;: \u0026#34;CREATION_START\u0026#34;, \u0026#34;trigger\u0026#34;: { \u0026#34;time\u0026#34;: 1685404800000 } }, \u0026#34;deletion\u0026#34;: { \u0026#34;current_state\u0026#34;: \u0026#34;DELETION_START\u0026#34;, \u0026#34;trigger\u0026#34;: { \u0026#34;time\u0026#34;: 1685379600000 } }, \u0026#34;policy_seq_no\u0026#34;: 0, \u0026#34;policy_primary_term\u0026#34;: 1, \u0026#34;enabled\u0026#34;: true } ] } 状态说明 #  创建工作流状态：\n   状态 描述     CREATION_START 等待触发条件。   CREATION_CONDITION_MET 条件已满足，准备创建。   CREATING 正在创建快照。   CREATION_FINISHED 创建完成。    删除工作流状态：\n   状态 描述     DELETION_START 等待触发条件。   DELETION_CONDITION_MET 条件已满足，准备删除。   DELETING 正在删除快照。   DELETION_FINISHED 删除完成。    执行状态：\n   状态 描述     IN_PROGRESS 执行中。   RETRYING 正在重试。   SUCCESS 执行成功。   FAILED 执行失败。   TIME_LIMIT_EXCEEDED 超过时间限制。     删除策略 #  删除指定的 SLM 策略。\n请求 #  DELETE /_slm/policies/{policyName} 请求示例 #  DELETE /_slm/policies/daily-policy 响应示例 #  { \u0026#34;_index\u0026#34;: \u0026#34;.easysearch-ilm-config\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;daily-policy-sm-policy\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;result\u0026#34;: \u0026#34;deleted\u0026#34;, \u0026#34;forced_refresh\u0026#34;: true, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 8, \u0026#34;_primary_term\u0026#34;: 1 }  启动策略 #  启动（启用）一个已停止的策略。\n请求 #  POST /_slm/policies/{policyName}/_start 响应示例 #  { \u0026#34;acknowledged\u0026#34;: true }  停止策略 #  停止（禁用）一个正在运行的策略。策略停止后不会自动创建或删除快照。\n请求 #  POST /_slm/policies/{policyName}/_stop 响应示例 #  { \u0026#34;acknowledged\u0026#34;: true }  完整示例 #  场景：每小时快照 + 7 天保留 #  POST /_slm/policies/hourly-backup { \u0026#34;description\u0026#34;: \u0026#34;每小时快照，保留 7 天\u0026#34;, \u0026#34;creation\u0026#34;: { \u0026#34;schedule\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 * * * *\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } }, \u0026#34;time_limit\u0026#34;: \u0026#34;1h\u0026#34; }, \u0026#34;deletion\u0026#34;: { \u0026#34;schedule\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 2 * * *\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } }, \u0026#34;condition\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;max_count\u0026#34;: 200, \u0026#34;min_count\u0026#34;: 24 }, \u0026#34;time_limit\u0026#34;: \u0026#34;1h\u0026#34; }, \u0026#34;snapshot_config\u0026#34;: { \u0026#34;date_format\u0026#34;: \u0026#34;yyyy-MM-dd-HH:mm\u0026#34;, \u0026#34;date_format_timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34;, \u0026#34;indices\u0026#34;: \u0026#34;logs-*,metrics-*\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;my_backup\u0026#34;, \u0026#34;ignore_unavailable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;include_global_state\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;partial\u0026#34;: \u0026#34;true\u0026#34; } } 检查策略状态 #  GET /_slm/policies/hourly-backup/_explain 停止策略（维护期间） #  POST /_slm/policies/hourly-backup/_stop 恢复策略 #  POST /_slm/policies/hourly-backup/_start ","subcategory":null,"summary":"","tags":null,"title":"快照生命周期","url":"/easysearch/main/docs/features/data-retention/slm/"},{"category":null,"content":"同义词 #  词干提取通过归并词形变化扩大搜索范围；同义词（Synonyms） 则通过归并“意义接近的不同表达”扩大搜索范围。\n例如，查询“英国女王”时，你可能也希望命中包含“英国君主”的文档；用户搜索“美国”时，可能也期望命中包含“美利坚合众国”“USA”等表达的文档。\n同义词看起来很简单，但“用对”并不容易：如果把语义跨度过大的词强行绑定，会让结果变得像“随机返回”。经验上，同义词应当 少而精，服务于明确的高价值场景。\n基本用法：synonym 过滤器 #  同义词通过 synonym 词元过滤器实现，它会在分析过程中把词项替换或扩展为多个词项：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_synonym_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;synonym\u0026#34;, \u0026#34;synonyms\u0026#34;: [ \u0026#34;british,english\u0026#34;, \u0026#34;queen,monarch\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_synonyms\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_synonym_filter\u0026#34; ] } } } } } 同义词通常与原词项处于同一 position，因此短语查询仍能成立（前提是规则与分析链设置得当）。\n同义词规则格式 #  权威指南里给出了两种常用格式：\n1）等价同义词（逗号分隔） #  jump,leap,hop 含义：遇到其中任意一个词项时，扩展为这一组同义词（等价看待）。\n2）定向映射（=\u0026gt;） #  u s a,united states,united states of america =\u0026gt; usa g b,gb,great britain =\u0026gt; britain,england,scotland,wales 含义：把左侧的一组表达统一映射为右侧一个或多个词项。\n当多条规则存在重叠时，通常会倾向于最长匹配，避免短规则截断长短语导致的意外扩展。\n索引时扩展 vs 查询时扩展 #  同义词既可以在索引时应用，也可以在查询时应用。权威指南给出的权衡非常实用：\n 索引时扩展  优点：查询更快/更简单（索引里已经包含扩展后的词项） 缺点：索引会变大；同义词策略变更往往需要重建索引才能完全生效 相关性：不同同义词可能被迫共享 IDF 权重（通用词可能被“抬高”）   查询时扩展  优点：规则可快速迭代，通常无需重建索引 缺点：查询会被重写成更复杂的形式，可能影响性能 相关性：各词项的 IDF 更贴近原始统计    实践建议：\n 稳定、变化极少的词汇（例如确定的缩写/标准化写法），可考虑索引时扩展 业务策略驱动、经常调整的词汇（品牌/类目/运营口径），更适合查询时扩展  同义词与分析链：顺序决定“能否匹配” #  同义词过滤器只能看到它前面的 tokenizer/filters 输出，而看不到原始字符串。权威指南用 U.S.A. 举了一个典型例子：\n原始文本: \u0026#34;U.S.A.\u0026#34; standard tokenizer → (U),(S),(A) lowercase filter → (u),(s),(a) synonym filter → (usa) 因此，如果你在同义词表里写 U.S.A.，它永远匹配不到，因为在到达 synonym 过滤器时，点号已经不见了、大小写也被归一化了。\n同样地，若你同时使用词干提取：\n 把 synonym 放在词干提取 之前：你可能需要枚举各种词形变化（规则更长更难维护） 把 synonym 放在词干提取 之后：你可以只针对“词干形态”写规则（通常更简洁）  大小写敏感的同义词 #  同义词通常放在 lowercase 之后，规则也就默认全是小写。但某些缩写/专名需要区分大小写语义（如 CAT 扫描 vs cat 动物）。\n此时可以考虑：\n 把“大小写敏感的同义词过滤器”放在 lowercase 之前（代价是规则需要枚举不同大小写） 或者拆分为两套过滤器：一套大小写敏感，一套大小写不敏感（配置会变复杂，需要用 _analyze 反复验证）  多词同义词：短语查询的经典坑 #  多词同义词（例如 united states of america ⇔ usa）会显著影响 position，从而让短语查询出现“诡异匹配”或“本该匹配却不匹配”。\n权威指南的结论非常明确：想让短语查询更可靠，倾向于用 简单收缩（把多词短语映射为单个词项）：\nunited states,u s a,united states of america =\u0026gt; usa 代价是：你无法再用 united 或 states 去匹配到被收缩后的表达（通常需要额外字段/额外分析链来兼顾）。\n符号同义词：先做字符映射，再分词 #  符号/表情往往会在分词阶段被当作标点丢弃，但在某些业务里它们很关键（例如情绪分析）。权威指南建议用 mapping 字符过滤器在分词前把符号替换成“可索引的词”：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;emoticons\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mapping\u0026#34;, \u0026#34;mappings\u0026#34;: [ \u0026#34;:)=\u0026gt;emoticon_happy\u0026#34;, \u0026#34;:(=\u0026gt;emoticon_sad\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_emoticons\u0026#34;: { \u0026#34;char_filter\u0026#34;: [ \u0026#34;emoticons\u0026#34; ], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34; ] } } } } } 实务建议 #   同义词要可控：从少量高价值规则开始，配合真实查询日志回放评估噪声 规则版本化：支持回滚，避免一次改动把线上搜索体验“打碎” 关注分析链顺序：同义词要匹配的是“分词/归一化后的词项”，不是原始字符串 多词同义词优先用“收缩”保证短语查询可靠；需要兼顾精确语义时用多字段/多分析链  小结 #   同义词能扩大召回，但用不好会降低搜索可解释性 两种核心规则：等价扩展（,）与定向映射（=\u0026gt;） 索引时/查询时各有取舍：性能、灵活性、相关性都会受影响 分析链顺序决定规则是否能命中；多词同义词要特别小心短语查询与高亮  下一步可以继续阅读：\n  模糊匹配  停用词  相关性基础  参考手册（API 与参数） #    同义词过滤器（功能手册）  同义词图过滤器（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"同义词","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis-synonyms/"},{"category":null,"content":"Reverse 分词过滤器 #  reverse 分词过滤器会反转每个词元中字符的顺序，这样在分析过程中，后缀信息就会位于反转后词元的开头。\n相关指南（先读这些） #    文本分析：识别词元  部分匹配  文本分析基础  这对于基于后缀的搜索很有用：\n反转分词过滤器在你需要进行基于后缀的搜索时很有帮助，例如以下场景：\n 后缀匹配：根据单词的后缀来搜索单词，比如识别以特定后缀结尾的单词（例如 -tion 或 -ing）。 文件扩展名搜索：通过文件的扩展名来搜索文件，例如 .txt 或 .jpg。 自定义排序或排名：通过反转词元，你可以基于后缀实现独特的排序或排名逻辑。 后缀自动补全：实现基于后缀而非前缀的自动补全建议。  参考说明 #  以下示例请求创建了一个名为 my-reverse-index 的新索引，并配置了一个带有反转过滤器的分词器。\nPUT /my-reverse-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;reverse_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;reverse\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_reverse_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;reverse_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-reverse-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_reverse_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;hello world\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;olleh\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;dlrow\u0026#34;, \u0026#34;start_offset\u0026#34;: 6, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"反转分词过滤器（Reverse）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/reverse/"},{"category":null,"content":"Trim 分词过滤器 #  trim 分词过滤器会从词元中去除前导和尾随的空白字符。\n 注意：许多常用的分词器，例如 standard 分词器、keyword 分词器和 whitespace 分词器，在分词过程中会自动去除前导和尾随的空白字符。当使用这些分词器时，无需额外配置 trim 分词过滤器。\n 相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参考样例 #  以下示例请求创建了一个名为 my_pattern_trim_index 的新索引，并配置了一个带有修剪过滤器和匹配词元生成器的分词器。其中，匹配词元生成器不会去除词元的前导和尾随空白字符。\nPUT /my_pattern_trim_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_trim_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;trim\u0026#34; } }, \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_pattern_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_pattern_trim_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_pattern_tokenizer\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_trim_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_pattern_trim_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_pattern_trim_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34; Easysearch , is , powerful \u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 12, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34;: 13, \u0026#34;end_offset\u0026#34;: 18, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;powerful\u0026#34;, \u0026#34;start_offset\u0026#34;: 19, \u0026#34;end_offset\u0026#34;: 32, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"修剪分词过滤器（Trim）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/trim/"},{"category":null,"content":"Mapping 与文本分析 #  Easysearch 中不同字段类型的索引和搜索行为截然不同。理解映射（Mapping）和分析（Analysis）是用好全文搜索的关键。\n精确值 VS 全文 #  Easysearch 中的数据可以概括为两类：\n精确值（Exact Values）\n 例如日期 2024-09-15、用户 ID user_123、状态码 published Foo 和 foo 是不同的，2024 和 2024-09-15 也是不同的 查询是二进制的：要么匹配，要么不匹配  全文（Full Text）\n 例如文章内容、商品描述、邮件正文 查询不是\u0026quot;是否匹配\u0026quot;，而是\u0026quot;匹配程度有多大\u0026quot; 期望搜索 jump 能匹配 jumped、jumps、jumping 期望搜索 UK 能返回包含 United Kingdom 的文档  为了支持全文搜索，Easysearch 在索引之前会先对文本进行分析，生成标准化的词条后写入倒排索引。\n分析器：三步处理 #  分析（Analysis） 是将文本转换为适合倒排索引的词条的过程。一个分析器由三个部分组成：\n   组件 作用 示例     字符过滤器 在分词前整理字符串 去掉 HTML 标签、\u0026amp; → and   分词器 将字符串拆分为单独的词条 按空格/标点拆分   Token 过滤器 对词条做变换 小写化、词干提取、同义词扩展、停用词删除    内置分析器对比 #  以文本 \u0026quot;Set the shape to semi-transparent by calling set_trans(5)\u0026quot; 为例：\n   分析器 产生的词条     标准分析器（默认） set, the, shape, to, semi, transparent, by, calling, set_trans, 5   简单分析器 set, the, shape, to, semi, transparent, by, calling, set, trans   空格分析器 Set, the, shape, to, semi-transparent, by, calling, set_trans(5)   英语分析器 set, shape, semi, transpar, call, set_tran, 5    测试分析器 #  使用 _analyze API 查看文本被分析后的结果：\nGET /_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Quick brown foxes\u0026#34; } 返回：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;quick\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;brown\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;foxes\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } 也可以针对已有索引的某个字段测试：\nGET /my_index/_analyze { \u0026#34;field\u0026#34;: \u0026#34;content\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Quick brown foxes\u0026#34; } 语言分析器选择 #  Easysearch 为很多流行语言提供了开箱即用的语言分析器。每个语言分析器都针对该语言的特点进行了优化：\n 英语分析器：移除了所有格 's（John's → john）、词干提取 法语分析器：移除了元音省略（如 l' 和 qu'）和变音符号 德语分析器：规范化了切词，将 ä 和 ae 替换为 a，或将 ß 替换为 ss 中文分析器（IK、Pinyin）：分词与拼音支持  映射：字段的类型定义 #  映射（Mapping）定义了索引中每个字段的数据类型和处理方式。\n核心字段类型 #     类型 说明 用途     text 全文字段，会被分析后建立倒排索引 标题、正文、描述   keyword 精确值字段，不分析，用于过滤、排序、聚合 状态、分类、ID、邮箱   long, integer, short, byte 整数类型 计数、编号   double, float, half_float 浮点数类型 价格、评分   boolean 布尔类型 是否标志   date 日期类型 时间戳   object JSON 对象 嵌套数据    动态映射 #  当你索引一个包含新字段的文档时，Easysearch 会自动猜测字段类型：\n   JSON 值 推断的类型     true / false boolean   123 long   123.45 double   \u0026quot;2024-09-15\u0026quot; date   \u0026quot;hello world\u0026quot; text + keyword 子字段     ⚠️ 如果通过引号索引一个数字（如 \u0026quot;123\u0026quot;），它会被映射为 text。类型一旦映射后不能修改，只能增加新字段。\n 创建与修改映射 #  创建索引时指定映射：\nPUT /my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;views\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } 增加新字段（不能修改已有字段）：\nPUT /my_index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;tag\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } 查看映射 #  GET /my_index/_mapping 复杂数据类型 #  数组 #  任何字段都可以包含多个值（数组），不需要特殊映射：\n{ \u0026#34;tag\u0026#34;: [\u0026#34;search\u0026#34;, \u0026#34;nosql\u0026#34;] }  ⚠️ 数组中所有值必须是相同数据类型。数组被索引为多值字段——可搜索，但无序。\n 内部对象 #  内部对象会被 Lucene\u0026quot;扁平化\u0026quot;索引。例如：\n{ \u0026#34;user\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;first\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;Smith\u0026#34; }, \u0026#34;age\u0026#34;: 26 } } 实际被索引为：\n{ \u0026#34;user.name.first\u0026#34;: [\u0026#34;john\u0026#34;], \u0026#34;user.name.last\u0026#34;: [\u0026#34;smith\u0026#34;], \u0026#34;user.age\u0026#34;: [26] } 内部对象可以通过点号路径引用（如 user.name.first）。\n ⚠️ 如果内部对象是数组，扁平化会导致对象间的关联性丢失。如果需要保持对象间关联，请使用 nested 类型。\n 关键原则 #   索引和查询用相同的分析器：确保词条格式一致才能匹配 精确值字段不需要分析：用 keyword 类型 映射一旦创建不能修改：只能增加新字段 用 _analyze API 验证分析效果：调试分析器的最佳工具  小结 #     概念 要点     精确值 不分析，用 keyword/数值/日期，二进制匹配   全文 通过分析器生成词条，倒排索引，相关性匹配   分析器 字符过滤 → 分词 → Token 过滤，三步流水线   映射 定义字段类型和分析方式，创建后不可改只可加    下一步 #    Mapping 基础深入：text/keyword 策略、动态映射陷阱  Mapping 模式与最佳实践：多字段设计、嵌套数据  文本分析主题:词汇识别、词干提取、同义词等  文本分析 API：分析器、分词器、过滤器参考  Lucene 底层原理：倒排索引与段结构  最佳实践 #    数据建模：文档设计、Nested、反范式权衡  索引设计：分片规划、Mapping 模板优化  ","subcategory":null,"summary":"","tags":null,"title":"Mapping 与文本分析","url":"/easysearch/main/docs/fundamentals/mapping-analysis-intro/"},{"category":null,"content":"IK Smart 分词器 #  ik_smart 是 IK 分词器插件提供的智能分词模式，是最常用的中文分词方式。\n##分词方式\n智能分词（Smart）模式倾向于将文本分成\u0026quot;人类可读\u0026quot;的粗粒度词项，适合大多数应用场景。\n示例 #  POST _analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;北京市朝阳区建国路1号\u0026#34; } 分析结果 #  [ \u0026#34;北京市\u0026#34;, \u0026#34;朝阳区\u0026#34;, \u0026#34;建国路\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;号\u0026#34; ] 相关指南 #    IK分词器文档  文本分析基础  依赖插件 #   analysis-ik 插件  配置 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_ik_smart\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ik_smart\u0026#34; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"IK智能分词器（IK Smart）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/ik-smart/"},{"category":null,"content":"ICU 分析器 #  icu 分析器是为多语言文本分析的分析器，基于 ICU（International Components for Unicode）实现，对亚洲语言混合文本提供比标准分析器更好的分词效果。\n需要安装 analysis-icu 插件。  分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n icu_tokenizer 分词器：使用 ICU Unicode 文本分割算法 icu_normalizer 分词过滤器：Unicode 归一化（NFC 模式）  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;icu\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Elasticsearch の全文検索エンジン\u0026#34; } 相关指南 #    文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"ICU 分析器（ICU）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/icu-analyzer/"},{"category":null,"content":"Geohash 编码 #   Geohash 是一种将经纬度坐标（lat/lon）编码成字符串的方式。这么做的初衷只是为了让地理位置在 URL 上呈现的形式更加友好，但现在 Geohash 已经变成一种在数据库中高效索引地理坐标点和地理形状的方式。\n 工作原理 #  Geohash 把整个世界分为 32 个单元的格子——4 行 8 列——每一个格子都用一个字母或者数字标识。比如 g 这个单元覆盖了半个格林兰、冰岛的全部和大不列颠的大部分。\n每一个单元还可以进一步被分解成新的 32 个单元，这些单元又可以继续被分解成 32 个更小的单元，不断重复下去：\n gc 覆盖了爱尔兰和英格兰 gcp 覆盖了伦敦的大部分和部分南英格兰 gcpuuz94k 是白金汉宫的入口，精确到约 5 米  核心特性：Geohash 的长度越长，精度就越高。如果两个 Geohash 有一个共同的前缀（如 gcpuuz），就表示它们挨得很近。共同的前缀越长，距离就越近。\n 精度级别 #  下表展示了不同长度 Geohash 对应的近似尺寸：\n   Geohash 级别 近似尺寸     g 1 ~ 5,004km × 5,004km   gc 2 ~ 1,251km × 625km   gcp 3 ~ 156km × 156km   gcpu 4 ~ 39km × 19.5km   gcpuu 5 ~ 4.9km × 4.9km   gcpuuz 6 ~ 1.2km × 610m   gcpuuz9 7 ~ 152.8m × 152.8m   gcpuuz94 8 ~ 38.2m × 19.1m   gcpuuz94k 9 ~ 4.78m × 4.78m   gcpuuz94kk 10 ~ 1.19m × 0.60m   gcpuuz94kkp 11 ~ 14.9cm × 14.9cm   gcpuuz94kkp5 12 ~ 3.7cm × 1.8cm     选择精度：大多数应用场景中，精度 5~7（约 5km 到 150m）足以满足需求。过高的精度会增加索引体积和查询开销。\n  边界问题 #  Geohash 有一个重要的边界特性需要注意：两个刚好相邻的位置，可能会有完全不同的 Geohash。\n例如，伦敦千禧穹顶（Millennium Dome）的 Geohash 是 u10hbp，因为它落在了 u 这个单元里，而紧挨着它东边的最大的单元是 g。这意味着：\n 共同前缀 ≠ 唯一的邻近判断方式 在网格边界处，相邻位置可能属于完全不同的 Geohash 前缀   在 Easysearch 中的应用 #  自动索引 Geohash 前缀 #  当你索引一个 geo_point 字段时，Easysearch 会自动索引该坐标点的 Geohash 以及所有的 Geohash 前缀。\n例如，索引白金汉宫入口位置（纬度 51.501568，经度 -0.141257）时，会同时索引：g、gc、gcp、gcpu、gcpuu、gcpuuz、gcpuuz9、gcpuuz94、gcpuuz94k 等所有前缀。\n这使得基于 Geohash 前缀的聚合和过滤非常高效。\nGeohash 网格聚合 #  geohash_grid 聚合利用 Geohash 将地理坐标点聚合到网格中，非常适合地图可视化：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 5 } } } } 返回结果中每个 bucket 的 key 就是 Geohash 值，可以用来在地图上绘制网格热力图。\n配合边界框优化性能 #  由于 Geohash 网格聚合会为所有匹配文档生成 bucket，在大数据集上可能产生大量 bucket。推荐配合 geo_bounding_box 过滤器限制范围：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;geo_bounding_box\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;top_left\u0026#34;: { \u0026#34;lat\u0026#34;: 40.8, \u0026#34;lon\u0026#34;: -74.1 }, \u0026#34;bottom_right\u0026#34;: { \u0026#34;lat\u0026#34;: 40.4, \u0026#34;lon\u0026#34;: -73.7 } } } } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 6 } } } }  Geohash vs Geotile #  Easysearch 同时支持 geohash_grid 和 geotile_grid 两种地理网格聚合，它们使用不同的网格划分方式：\n   特性 geohash_grid geotile_grid     网格类型 Geohash 编码（32 进制） 地图瓦片坐标（x/y/zoom）   适用场景 通用地理聚合 Web 地图瓦片渲染   精度参数 1-12（字符串长度） 0-29（zoom 级别）   与地图库集成 需要转换 直接对应瓦片坐标    如果你的应用需要与 Web 地图（如 Google Maps、Leaflet、Mapbox）集成，geotile_grid 可能是更好的选择。\n 实用技巧 #  1. 根据地图缩放级别选择精度 #  缩放级别 3-5 → precision 2-3（国家/省级） 缩放级别 6-8 → precision 4-5（城市级） 缩放级别 9-12 → precision 6-7（街区级） 缩放级别 13+ → precision 8+（街道/建筑级） 2. 获取 Geohash 对应的边界框 #  前端可以使用 JavaScript 库（如 ngeohash）将 Geohash 转换为边界框坐标，用于地图绘制：\nconst geohash = require(\u0026#39;ngeohash\u0026#39;); const bbox = geohash.decode_bbox(\u0026#39;gcpuu\u0026#39;); // 返回 [lat_min, lon_min, lat_max, lon_max] 3. 结合 geo_bounds 聚合 #  使用 geo_bounds 子聚合获取每个 Geohash 网格内的精确边界：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 5 }, \u0026#34;aggs\u0026#34;: { \u0026#34;cell_bounds\u0026#34;: { \u0026#34;geo_bounds\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34; } } } } } }  相关文档 #    Geo Point 字段类型  Geo Shape 字段类型  地理位置搜索  地理位置实践  Geohash 网格聚合  Geotile 网格聚合  ","subcategory":null,"summary":"","tags":null,"title":"Geohash 编码","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/geo-field-type/geohash/"},{"category":null,"content":"脚本处理器 #  script 处理器执行内联和存储的脚本，可以在数据导入过程中修改或转换 Easysearch 文档中的数据。由于脚本可能按文档重新编译，处理器使用脚本缓存以提高性能。有关在 Easysearch 中使用脚本的信息，请参阅脚本 API。\n以下是为 script 处理器提供的语法：\n{ \u0026#34;processor\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026lt;script_source\u0026gt;\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;\u0026lt;script_language\u0026gt;\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;\u0026lt;param_name\u0026gt;\u0026#34;: \u0026#34;\u0026lt;param_value\u0026gt;\u0026#34; } } } } 配置参数 #  下表列出了 script 处理器所需的和可选参数。\n   参数 是否必填 描述     source 可选 要执行的 Painless 脚本。必须指定 id 或 source ，但不能同时指定两者。如果指定了 source ，则使用提供的源代码执行脚本。   id 可选 存储脚本的 ID，之前使用 Create Stored Script API 创建的。必须指定 id 或 source ，但不能同时指定两者。如果指定了 id ，则从具有指定 ID 的存储脚本中检索脚本源。   lang 可选 脚本的编程语言。默认为 painless 。   params 可选 可以传递给脚本的参数。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 my-script-pipeline 的管道，该管道使用 script 处理器将 message 字段转换为大写：\nPUT _ingest/pipeline/my-script-pipeline { \u0026#34;description\u0026#34;: \u0026#34;Example pipeline using the ScriptProcessor\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx.message = ctx.message.toUpperCase()\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Convert message field to uppercase\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/my-script-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;hello, world!\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;HELLO, WORLD!\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-05-30T16:24:23.30265405Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPOST testindex1/_doc/1?pipeline=my-script-pipeline { \u0026#34;message\u0026#34;: \u0026#34;hello, world!\u0026#34; } 响应确认该文档已索引到 testindex1 ，并且已将所有具有 message 字段的文档转换为大写：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 6, \u0026#34;_primary_term\u0026#34;: 2 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"脚本处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/script/"},{"category":null,"content":"Remove Duplicates 分词过滤器 #  remove_duplicates 分词过滤器用于去除在分词过程中在相同位置生成的重复词元。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参考样例 #  以下示例请求创建了一个带有 keyword_repeat（关键词重复）分词过滤器的索引。该过滤器会在每个词元自身所在的相同位置添加该词元的关键词版本，然后使用 kstem（K 词干提取）过滤器来创建该词元的词干形式。\nPUT /example-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;keyword_repeat\u0026#34;, \u0026#34;kstem\u0026#34; ] } } } } } 使用以下请求来分析字符串 “Slower turtle”。\nGET /example-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Slower turtle\u0026#34; } 返回内容中在同一位置包含了两次词元 “turtle”。\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;slower\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;slow\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } 可以通过在索引设置中添加一个去重分词过滤器来移除重复的词元。\nPUT /index-remove-duplicate { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;keyword_repeat\u0026#34;, \u0026#34;kstem\u0026#34;, \u0026#34;remove_duplicates\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /index-remove-duplicate/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Slower turtle\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;slower\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;slow\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"去重分词过滤器（Remove Duplicates）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/remove-duplicates/"},{"category":null,"content":"停用词 #  停用词（Stopwords）是指在大多数文本中非常常见、对区分文档贡献很小的词，例如 the、and、is。早期信息检索系统会大量使用停用词来减少索引体积；在今天，更需要从 性能与表达力 两方面权衡。\n高频词与低频词 #  从词项频率的角度，我们可以把词项粗略分为两类：\n 低频词（更重要）：在文档集合中出现较少，通常信息量更大，区分能力更强 高频词（次重要）：在大量文档中都出现，对区分能力弱，但会显著影响性能  哪些词是“高频”，强依赖你的语料：在英文语料里 the 极常见；在中文语料里它反而不会出现。\n停用词的优缺点 #  优点：性能 #  假设一个索引有 100 万文档，词 fox 只出现在 20 个文档中：\n 查询 fox：只需要对很少的候选文档计算 _score 查询 the OR fox：由于 the 几乎在所有文档里都出现，系统可能需要对大量文档计算 _score，代价骤增  因此，减少极高频词对候选集合的“污染”，有助于控制查询成本。\n缺点：表达力下降 #  把常见词一律移除会让一些真实需求变得难以表达：\n 区分 happy 与 not happy 搜索乐队名 The The 搜索名句 “to be, or not to be” 某些短词在特定语料里可能很关键（例如国家代码、产品型号）  在现代硬件条件下，“省空间”往往不再是使用停用词的主理由；更重要的是避免让搜索变得“说不清、搜不准”。\n如何配置停用词 #  停用词由 stop 过滤器处理，也可以通过分析器参数直接配置。\n为 standard 分析器指定停用词 #  PUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;stopwords\u0026#34;: [ \u0026#34;and\u0026#34;, \u0026#34;the\u0026#34; ] } } } } } 使用语言分析器的默认停用词表 #  语言分析器通常会内置适合该语言的停用词列表，例如：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_english\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;english\u0026#34;, \u0026#34;stopwords\u0026#34;: \u0026#34;_english_\u0026#34; } } } } } 如需禁用停用词，可使用 _none_（示意）：\n{ \u0026#34;type\u0026#34;: \u0026#34;english\u0026#34;, \u0026#34;stopwords\u0026#34;: \u0026#34;_none_\u0026#34; } 从文件加载停用词 #  { \u0026#34;type\u0026#34;: \u0026#34;english\u0026#34;, \u0026#34;stopwords_path\u0026#34;: \u0026#34;stopwords/english.txt\u0026#34; } 要点：\n 文件需要存在于每个节点上（路径相对于配置目录） 更新文件后，需要通过“关闭并重新打开索引”或“滚动重启节点”等方式让分析器重新加载 更新停用词只影响后续分析/查询；已索引的数据若要生效通常需要重建索引  为什么短语查询特别怕高频词 #  短语查询需要依赖**位置（positions）**数据：词在文档中的顺序与位置会被存储起来。高频词在大量文档里出现，会产生巨量的位置信息：\n 一个包含常用词（如 the）的短语查询，可能需要读取大量 positions 数据 这些数据会强烈影响磁盘读取与系统缓存，进而拖慢其他查询  这也是为什么在慢查询中，短语类查询（尤其包含高频词）常常占比很高。\n不用“删词”也能控制性能 #  现代实践中，很多系统倾向于少用或不用停用词，而是通过查询策略减少“只命中常用词”的候选集合，例如：\n1）operator: and #  { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the quick brown fox\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34; } } } 2）minimum_should_match #  { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the quick brown fox\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: \u0026#34;75%\u0026#34; } } } 这类做法的核心是：让“更重要的词”主导候选集合，把高频词更多用于评分区分，而不是让它们把候选集合撑爆。\n常用词项分治：cutoff_frequency（来自权威指南的思路） #  权威指南提供了一个实用思路：把查询词拆成“低频（更重要）”与“高频（次重要）”两组。match 查询的 cutoff_frequency 可以用于这个分治：\n{ \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;quick and the dead\u0026#34;, \u0026#34;cutoff_frequency\u0026#34;: 0.01 } } } 直觉效果：\n 低频词参与匹配（决定候选集） 高频词更多参与评分（在候选集内部区分更相关的结果）   说明：cutoff_frequency 与部分“常用词项查询”能力在不同产品/版本中的支持情况可能不同，你可以把它视为一种“分治策略”的参考：核心目标是避免高频词让候选集合与评分成本失控。\n 小结 #   停用词的主要价值是性能，但代价是表达力下降 高并发与短语查询场景里，高频词的 positions 数据可能成为主要性能瓶颈 现代系统中，常见做法是少用停用词，转而用 operator: and、minimum_should_match、分治策略来控制成本 如果要用停用词，建议从“领域高频词/噪声词”入手，谨慎、可回滚地迭代  下一步可以继续阅读：\n  同义词  模糊匹配  相关性基础  参考手册（API 与参数） #    停用词过滤器（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"停用词","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis-stopwords/"},{"category":null,"content":"Point In Time (PIT) API #  Point In Time（PIT）搜索具有与常规搜索相同的功能，不同之处在于 PIT 搜索作用于较旧的数据集，而常规搜索作用于实时数据集。PIT 搜索不绑定于特定查询，因此您可以在同一个冻结在时间点上的数据集上运行不同的查询。\n您可以使用创建 PIT API 来创建 PIT。当您为一组索引创建 PIT 时，Easysearch 会锁定这些索引的一组段，使它们在时间上冻结。在底层，此 PIT 所需的资源不会被修改或删除。如果作为 PIT 一部分的段被合并，Easysearch 会在 PIT 创建时通过 keep_alive 参数指定的时间段内保留这些段的副本。\n创建 PIT 操作会返回一个 PIT ID，您可以使用该 ID 在冻结的数据集上运行多个查询。即使索引继续摄取数据并修改或删除文档，PIT 引用的数据自 PIT 创建以来不会发生变化。当您的查询包含 PIT ID 时，您不需要将索引传递给搜索，因为它将使用该 PIT。使用 PIT ID 的搜索在多次运行时将产生完全相同的结果。\n相关指南（先读这些） #    分页与排序  查询 DSL 基础  创建 PIT #  创建一个 PIT。查询参数 keep_alive 是必需的；它指定了保持 PIT 的时间长度。\n端点 #  POST /\u0026lt;target_indexes\u0026gt;/_pit?keep_alive=1h\u0026amp;routing=\u0026amp;expand_wildcards=\u0026amp;preference= 路径参数 #     参数 数据类型 描述     target_indexes 字符串 PIT 的目标索引名称。可以包含以逗号分隔的列表或通配符索引模式。    查询参数 #     参数 数据类型 描述     keep_alive 时间 保持 PIT 的时间长度。每次使用搜索 API 访问 PIT 时，PIT 的生命周期都会延长一段等于 keep_alive 参数的时间。必需。   preference 字符串 用于执行搜索的节点或分片。可选。默认为随机。   routing 字符串 指定将搜索请求路由到特定分片。可选。默认为文档的 _id。   expand_wildcards 字符串 可匹配通配符模式的索引类型。支持逗号分隔的值。有效值如下：\n- all：匹配任何索引或数据流，包括隐藏的。\n- open：匹配开放的、非隐藏的索引或非隐藏的数据流。\n- closed：匹配关闭的、非隐藏的索引或非隐藏的数据流。\n- hidden：匹配隐藏的索引或数据流。必须与 open、closed 或同时与 open 和 closed 组合使用。\n- none：不接受通配符模式。\n可选。默认为 open。   allow_partial_pit_creation 布尔值 指定是否在部分失败的情况下创建 PIT。可选。默认为 true。    请求示例 #  POST /test-index/_pit?keep_alive=100m 示例响应 #  { \u0026#34;pit_id\u0026#34;: \u0026#34;o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAIWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;creation_time\u0026#34;: 1658146050064 } 响应体字段 #     字段 数据类型 描述     pit_id Base64 编码 PIT ID。   creation_time 长整型 PIT 创建的时间，以毫秒为单位（从纪元开始）。    延长 PIT 时间 #  您可以在执行搜索时，通过在 pit 对象中提供 keep_alive 参数来延长 PIT 时间：\nGET /_search { \u0026#34;size\u0026#34;: 10000, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;user.id\u0026#34; : \u0026#34;elkbee\u0026#34; } }, \u0026#34;pit\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\u0026#34;, \u0026#34;keep_alive\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;sort\u0026#34;: [ {\u0026#34;@timestamp\u0026#34;: {\u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34;}} ], \u0026#34;search_after\u0026#34;: [ \u0026#34;2021-05-20T05:30:04.832Z\u0026#34; ] } 搜索请求中的 keep_alive 参数是可选的。它指定了延长保持 PIT 时间的时长。\n列出所有 PIT #  返回 Easysearch 集群中的所有 PIT。\n请求示例 #  GET /_pit/_all 示例响应 #  { \u0026#34;pits\u0026#34;: [ { \u0026#34;pit_id\u0026#34;: \u0026#34;o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAEWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\u0026#34;, \u0026#34;creation_time\u0026#34;: 1658146048666, \u0026#34;keep_alive\u0026#34;: 6000000 }, { \u0026#34;pit_id\u0026#34;: \u0026#34;o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAIWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\u0026#34;, \u0026#34;creation_time\u0026#34;: 1658146050064, \u0026#34;keep_alive\u0026#34;: 6000000 } ] } 响应体字段 #     字段 数据类型 描述     pits JSON 对象数组 所有 PIT 的列表。    每个 PIT 对象包含以下字段。\n   字段 数据类型 描述     pit_id Base64 编码 PIT ID。   creation_time 长整型 PIT 创建的时间，以毫秒为单位（从纪元开始）。   keep_alive 长整型 保持 PIT 的时间长度，以毫秒为单位。    删除 PIT #  删除一个、多个或所有 PIT。当 keep_alive 时间段过后，PIT 会自动删除。但是，为了释放资源，您可以使用删除 PIT API 来删除 PIT。删除 PIT API 支持通过 ID 删除 PIT 列表或一次性删除所有 PIT。\n请求示例: 删除所有 PIT #  DELETE /_pit/_all 如果您想删除一个或多个PIT，请在请求体中指定 PIT ID。\n请求体字段 #     字段 数据类型 描述     pit_id Base64 编码 或二进制数组 要删除的 PIT 的 PIT ID。必需。    示例请求：通过 ID 删除 PIT #  DELETE /_pit { \u0026quot;pit_id\u0026quot;: [ \u0026quot;o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAEWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\u0026quot;, \u0026quot;o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAIWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\u0026quot; ] } 示例响应 #\n 对于每个 PIT，响应包含一个带有 PIT ID 和 successful 字段的 JSON 对象，该字段指定删除是否成功。部分失败被视为失败。\n{ \u0026#34;pits\u0026#34;: [ { \u0026#34;successful\u0026#34;: true, \u0026#34;pit_id\u0026#34;: \u0026#34;o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAEWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\u0026#34; }, { \u0026#34;successful\u0026#34;: false, \u0026#34;pit_id\u0026#34;: \u0026#34;o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAIWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\u0026#34; } ] } 响应体字段 #     字段 数据类型 描述     successful 布尔值 删除操作是否成功。   pit_id Base64 编码 要删除的 PIT 的 PIT ID。    PIT 设置 #  您可以通过设置 _cluaster/settings 的方式为 PIT 指定以下设置。\n   设置 描述 默认值     point_in_time.max_keep_alive 一个集群级别的设置，指定 keep_alive 参数的最大值。 24h   search.max_open_pit_context 一个节点级别的设置，指定节点的最大开放 PIT 上下文数量。 300    ","subcategory":null,"summary":"","tags":null,"title":"Point In Time (PIT) API","url":"/easysearch/main/docs/features/query-dsl/pit-api/"},{"category":null,"content":"HanLP Index 分析器 #  hanlp_index 分析器是为中文索引分词的分析器，使用 HanLP 索引分词模式，会对文本进行更细粒度的切分。\n需要安装 analysis-hanlp 插件。  分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n hanlp_index 分词器：使用 HanLP 索引分词模式，对文本进行细粒度切分 lowercase 分词过滤器：转换为小写  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;hanlp_index\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国国歌\u0026#34; } 相关指南 #    文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"HanLP 索引分析器（HanLP Index）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-index-analyzer/"},{"category":null,"content":"HanLP Standard 分析器 #  hanlp_standard 分析器是为中文标准分词的分析器，使用 HanLP 标准分词模式，适合大多数通用中文搜索场景。\n需要安装 analysis-hanlp 插件。  分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n hanlp_standard 分词器：使用 HanLP 标准分词模式 lowercase 分词过滤器：转换为小写  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;hanlp_standard\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国国歌\u0026#34; } 相关指南 #    文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"HanLP 标准分析器（HanLP Standard）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-standard-analyzer/"},{"category":null,"content":"HanLP NLP 分析器 #  hanlp_nlp 分析器是为中文 NLP 分词的分析器，使用 HanLP NLP 模式，支持命名实体识别，适合需要高精度语义分析的场景。\n需要安装 analysis-hanlp 插件。  分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n hanlp_nlp 分词器：使用 HanLP NLP 模式，支持命名实体识别 lowercase 分词过滤器：转换为小写  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;hanlp_nlp\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;刘德华和张学友是好朋友\u0026#34; } 相关指南 #    文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"HanLP NLP 分析器（HanLP NLP）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-nlp-analyzer/"},{"category":null,"content":"HanLP CRF 分析器 #  hanlp_crf 分析器是为中文 CRF 分词的分析器，使用 HanLP CRF（条件随机场）模型进行分词，适合学术研究场景。\n需要安装 analysis-hanlp 插件。  分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n hanlp_crf 分词器：使用 HanLP CRF 条件随机场模型进行分词 lowercase 分词过滤器：转换为小写  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;hanlp_crf\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中国科学院计算技术研究所\u0026#34; } 相关指南 #    文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"HanLP CRF 分析器（HanLP CRF）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-crf-analyzer/"},{"category":null,"content":"重命名处理器 #  rename 处理器用于重命名现有字段，也可以用来将字段从一个对象移动到另一个对象或根级别。\n语法 #  以下是为 rename 处理器提供的语法：\n{ \u0026#34;rename\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_name\u0026#34;, \u0026#34;target_field\u0026#34; : \u0026#34;target_field_name\u0026#34; } } 配置参数 #  下表列出了 rename 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要重命名的数据的字段名称。支持模板使用。   target_field 必填 字段的新名称。支持模板使用。   ignore_missing 可选 指定处理器是否应忽略不包含指定 field 的文档。如果设置为 true ，则处理器在 field 不存在时不会修改文档。默认为 false 。   override_target 可选 确定当 target_field 存在于文档中时会发生什么。如果设置为 true ，则处理器将现有的 target_field 值覆盖为新值。如果设置为 false ，则现有值保持不变，处理器不会覆盖它。默认为 false 。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 rename_field 的管道，该管道将对象中的一个字段移动到根级别：\nPUT /_ingest/pipeline/rename_field { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that moves a field to the root level.\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;rename\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message.content\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;content\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/rename_field/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;:{ \u0026#34;message\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;192.168.1.10 - - [03/Nov/2023:15:20:45 +0000] \\\u0026#34;POST /login HTTP/1.1\\\u0026#34; 200 3456\u0026#34; } } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nginx\u0026#34;, }, \u0026#34;content\u0026#34;: \u0026#34;\u0026#34;\u0026#34;192.168.1.10 - - [03/Nov/2023:15:20:45 +0000] \u0026#34;POST /login HTTP/1.1\u0026#34; 200 3456\u0026#34;\u0026#34;\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-04-15T07:54:16.010447Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=rename_field { \u0026#34;message\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;192.168.1.10 - - [03/Nov/2023:15:20:45 +0000] \\\u0026#34;POST /login HTTP/1.1\\\u0026#34; 200 3456\u0026#34; } } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"重命名处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/rename/"},{"category":null,"content":"词干提取 #  很多语言的单词都存在“词形变化”（inflection）：同一个概念会换着马甲出现：\n 单复数：fox / foxes 时态：pay / paid / paying 性别：waiter / waitress 人称变化：hear / hears  如果只做精确词项匹配，用户搜索 fox 时不会命中只包含 foxes 的文档。词干提取（Stemming） 的目标就是把这些“形式差异”抹平：让相关词项尽量归并到同一个词根上，从而提升召回率。\n词干提取会遇到的两类问题 #  词干提取不是“精确科学”，它更像“有原则的近似”。常见两类风险：\n 弱提取（Understemming）：相关词没有收敛到同一词干\n例如：jumped、jumps → jump，但 jumping → jumpi，导致漏召回。 过度提取（Overstemming）：本应区分的词被合并到同一词干\n例如：general 与 generate 都被提取为 gener，导致误召回、降低精度。  这也是为什么“哪个词干器最好”没有统一答案：你的语料、你的用户、你的容错边界都不一样。\n词干提取 vs 词形还原（Lemmatization） #   词干提取：规则化“裁剪”词形，得到的词根未必是真实单词（如 jumpi）。\n重点是：索引与查询两端一致即可。 词形还原：尝试按“词义/词典形态”归类（如 is/was/am/being → be），通常更复杂、成本更高，需要更多语言知识与上下文。  在多数搜索系统里，词干提取更常作为默认选择：成本低、收益稳。词形还原则更像“精装修”，做得好很香，做不好也可能很贵。\n算法词干器（Algorithmic stemmers） #  Easysearch 常用的是算法型词干器：通过一系列规则把词映射到词根，不依赖大词典。你可以把它理解为“规则派”：不查字典，按套路办事。\n优点：\n 速度快、内存占用小 对“规律变化”的词效果稳定  缺点：\n 对不规则词形支持有限（如 mice / mouse） 需要控制过度提取带来的噪声  示例：在分析链中加入词干器（示意） #  PUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;english_stemmer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;english\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_english\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;english_stemmer\u0026#34; ] } } } } } 如果你发现默认提干“太激进”，可以选择更轻量的变体（例如 light_english），减少过度提取的风险（具体可选项以你部署的 Easysearch 为准）。\n控制词干提取：排除与覆写 #  开箱即用的词干提取不可能在所有业务里都完美。常见的两种“纠偏”方式：\n1）阻止某些词被提干（keyword_marker） #  如果你希望某些词保持原样（例如业务术语、人名、品牌），可以先用 keyword_marker 标记它们，后续词干器会跳过这些词：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;no_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword_marker\u0026#34;, \u0026#34;keywords\u0026#34;: [ \u0026#34;skies\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_english\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;no_stem\u0026#34;, \u0026#34;english_stemmer\u0026#34; ] } } } } } 实践中也可以把排除列表放到文件（keywords_path）中统一维护，便于运维更新（不然每次改规则都要改配置，挺累的）。\n2）自定义词干结果（stemmer_override） #  对于不规则变化或特定业务词形，你可以显式指定“原词 =\u0026gt; 词根”的覆写规则：\n\u0026#34;custom_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer_override\u0026#34;, \u0026#34;rules\u0026#34;: [ \u0026#34;skies=\u0026gt;sky\u0026#34;, \u0026#34;mice=\u0026gt;mouse\u0026#34;, \u0026#34;feet=\u0026gt;foot\u0026#34; ] } 关键点：stemmer_override 必须放在通用词干器 之前，先让“特例”落地，再让“通用规则”接管。\n不建议：把“原词 + 词干”塞进同一个字段 #  有一种做法会把“原词”和“提干后的词”放在同一字段同一位置（例如 foxes,fox），看起来很省事（一个字段搞定），但通常会带来两个问题：\n 无法区分精确匹配与宽松匹配：很难在查询时用 boost 清晰地区分“原词命中”和“提干命中”的贡献 相关性统计被扭曲：同一文档里词根出现次数被放大，会影响 TF/IDF 等统计，导致排序不稳定  更推荐的方式是使用 multi-fields（一份内容，多条索引路线）：\n title：保留更“原始/精确”的分析链 title.stemmed：启用更宽松的词干提取  查询时组合两个字段：原始字段主导排序，提干字段扩大召回并作为补充信号。这样“既要又要”，还更可控。\n如何选择词干策略（实务） #   面向终端用户检索：优先选择更温和/轻量的词干器，避免误召回导致“看起来随机” 文档规模较小：可以稍微更激进一点以提升可搜到的概率 文档规模很大：更温和通常更稳，噪声会被放大得更明显 关键术语：用 keyword_marker/自定义规则保护  小结 #   词干提取通过归并词形变化提升召回率，但需要警惕弱提取与过度提取 算法词干器成本低、速度快，是多数场景的首选 通过 keyword_marker 与 stemmer_override 可以对词干提取做“可控的例外” 推荐使用 multi-fields 分离“精确字段”和“宽松字段”，查询时组合使用  下一步可以继续阅读：\n  停用词  同义词  模糊匹配  参考手册（API 与参数） #    词干过滤器（功能手册）  Snowball 词干过滤器（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"词干提取","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis-stemming/"},{"category":null,"content":"STConvert 分析器 #  stconvert 分析器可在索引与查询阶段将简体中文与繁体中文之间进行双向转换，解决两种文字体系混合检索的问题。\n相关指南（先读这些） #    文本分析基础  文本分析：规范化  参数说明 #     参数 说明 默认值     convert_type 转换方向，可选：s2t（简 → 繁），t2s（繁 → 简） s2t   keep_both 是否同时保留转换前后两种 token false   delimiter 当 keep_both=true 时，两种 token 之间的分隔符 ,    使用介绍 #  映射创建\nPUT /stconvert/ { \u0026#34;settings\u0026#34; : { \u0026#34;analysis\u0026#34; : { \u0026#34;analyzer\u0026#34; : { \u0026#34;tsconvert\u0026#34; : { \u0026#34;tokenizer\u0026#34; : \u0026#34;tsconvert\u0026#34; } }, \u0026#34;tokenizer\u0026#34; : { \u0026#34;tsconvert\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;stconvert\u0026#34;, \u0026#34;delimiter\u0026#34; : \u0026#34;#\u0026#34;, \u0026#34;keep_both\u0026#34; : false, \u0026#34;convert_type\u0026#34; : \u0026#34;t2s\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;tsconvert\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;stconvert\u0026#34;, \u0026#34;delimiter\u0026#34; : \u0026#34;#\u0026#34;, \u0026#34;keep_both\u0026#34; : false, \u0026#34;convert_type\u0026#34; : \u0026#34;t2s\u0026#34; } }, \u0026#34;char_filter\u0026#34; : { \u0026#34;tsconvert\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;stconvert\u0026#34;, \u0026#34;convert_type\u0026#34; : \u0026#34;t2s\u0026#34; } } } } } 分词测试\nGET stconvert/_analyze { \u0026#34;tokenizer\u0026#34; : \u0026#34;keyword\u0026#34;, \u0026#34;filter\u0026#34; : [\u0026#34;lowercase\u0026#34;], \u0026#34;char_filter\u0026#34; : [\u0026#34;tsconvert\u0026#34;], \u0026#34;text\u0026#34; : \u0026#34;国际國際\u0026#34; } 返回 { \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;国际国际\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } 归一化方法\nPUT index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;tsconvert\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stconvert\u0026#34;, \u0026#34;convert_type\u0026#34;: \u0026#34;t2s\u0026#34; } }, \u0026#34;normalizer\u0026#34;: { \u0026#34;my_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [ \u0026#34;tsconvert\u0026#34; ], \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34; ] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;foo\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;my_normalizer\u0026#34; } } } } PUT index/_doc/1 { \u0026quot;foo\u0026quot;: \u0026quot;國際\u0026quot; }\nPUT index/_doc/2 { \u0026quot;foo\u0026quot;: \u0026quot;国际\u0026quot; }\nGET index/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;foo\u0026quot;: \u0026quot;国际\u0026quot; } } }\nGET index/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;foo\u0026quot;: \u0026quot;國際\u0026quot; } } } \n","subcategory":null,"summary":"","tags":null,"title":"简繁转换分析器（STConvert）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/stconvert-analyzer/"},{"category":null,"content":"Unique 分词过滤器 #  unique 分词过滤器可确保在分词过程中仅保留唯一的词元，它会去除在单个字段或文本块中出现的重复词元。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参数说明 #  唯一分词过滤器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     only_on_same_position 可选 布尔值 如果设置为 true，该分词过滤器将充当去重分词过滤器，仅移除位于相同位置的词元。默认值为 false。    参考样例 #  以下示例请求创建了一个名为 unique_example 的新索引，并配置了一个带有唯一过滤器的分词器。\nPUT /unique_example { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;unique_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;unique\u0026#34;, \u0026#34;only_on_same_position\u0026#34;: false } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;unique_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;unique_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /unique_example/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;unique_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch Easysearch is powerful powerful and scalable\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34;: 22, \u0026#34;end_offset\u0026#34;: 24, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;powerful\u0026#34;, \u0026#34;start_offset\u0026#34;: 25, \u0026#34;end_offset\u0026#34;: 33, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;and\u0026#34;, \u0026#34;start_offset\u0026#34;: 43, \u0026#34;end_offset\u0026#34;: 46, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;scalable\u0026#34;, \u0026#34;start_offset\u0026#34;: 47, \u0026#34;end_offset\u0026#34;: 55, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"唯一分词过滤器（Unique）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/unique/"},{"category":null,"content":"移除处理器 #  remove 处理器用于从文档中移除一个字段。\n语法 #  以下是为 remove 处理器提供的语法：\n{ \u0026#34;remove\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_name\u0026#34; } } 配置参数 #  下表列出了 remove 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要移除的数据的字段名称。支持模板片段。 _index 、 _version 、 _version_type 和 _id 元数据字段不能被移除。如果指定了 version ，则不能从摄取文档中移除 _id 。   exclude_field 必填 要保留的字段名称。除元数据字段外，所有其他字段都将被删除。 exclude_field 和 field 选项互斥。支持模板片段。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 remove_ip 的管道，用于从文档中删除 ip_address 字段：\nPUT /_ingest/pipeline/remove_ip { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that excludes the ip_address field.\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;remove\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;ip_address\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/remove_ip/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;:{ \u0026#34;ip_address\u0026#34;: \u0026#34;203.0.113.1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-08-24T18:02:13.218986756Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=remove_ip { \u0026#34;ip_address\u0026#34;: \u0026#34;203.0.113.1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"移除处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/remove/"},{"category":null,"content":"Pinyin 分析器 #  pinyin 分析器能够在索引阶段将中文字符实时转写为拼音，并在检索阶段对拼音、汉字及混输查询进行统一匹配。借助该插件，你可以轻松实现：\n相关指南（先读这些） #     文本分析基础\n   文本分析：识别词元\n  支持 全拼、首字母、全拼拼接 等多种检索方式；\n  保留非中文字符，实现「中英混输」搜索；\n  借助 token filter 在分词链中灵活组合不同策略；\n  在联想输入、排序、聚合等场景下提升中文用户体验。\n  适用于人名、地名、品牌、歌曲、商品等多种中文文本检索场景。\n参数说明 #  下表整理了 pinyin 分词器 / 过滤器支持的全部可选参数及默认值。\n   参数 说明 默认值     keep_first_letter 仅保留每个汉字的拼音首字母。例如 刘德华 → ldh true   keep_separate_first_letter 将首字母分开存储，提高命中率。例如 刘德华 → l, d, h，注意：这可能会由于词频增加而增加查询模糊性。 false   limit_first_letter_length 首字母结果最长长度 16   keep_full_pinyin 保留每个汉字的全拼。如 刘德华 → liu, de, hua true   keep_joined_full_pinyin 将全拼连接在一起。如 刘德华 → liudehua false   keep_none_chinese 保留非中文字符（数字/字母等） true   keep_none_chinese_together 将连续非中文字符作为整体保留。例如， DJ 音乐家 变成 DJ 、 yin 、 yue 、 jia 。当设置为 false 时， DJ 音乐家 变成 D 、 J 、 yin 、 yue 、 jia 。注意： keep_none_chinese 应该先启用。 true   keep_none_chinese_in_first_letter 在首字母结果中保留非中文字。例如， 刘德华 AT2016 变成 ldhat2016 。符 true   keep_none_chinese_in_joined_full_pinyin 在拼接全部拼音时保留非中文字。例如， 刘德华 2016 变成 liudehua2016 。符 false   none_chinese_pinyin_tokenize 将非中文字符中可能的拼音继续拆分例如， liudehuaalibaba13zhuanghan 变成 liu 、 de 、 hua 、 a 、 li 、 ba 、 ba 、 13 、 zhuang 、 han 。注意： keep_none_chinese 和 keep_none_chinese_together 应该先启用。 true   keep_original 同时保留原始文本 false   lowercase 对非中文字符强制小写 true   trim_whitespace 去除首尾空格 true   remove_duplicated_term 开启时，移除重复术语以节省索引空间。例如， de 的 变为 de 。注意：位置相关的查询可能会受影响。积 false   ignore_pinyin_offset 忽略 offset 以允许重叠 token。在 6.0 版本之后，偏移量受到严格限制，不允许重叠的标记。使用此参数，通过忽略偏移量将允许重叠的标记。请注意，所有与位置相关的查询或高亮将变得不正确。您应该使用多字段，并为不同的查询目的指定不同的设置。如果您需要偏移量，请将其设置为 false。 true     备注：以上参数可按需自由组合。若使用 token filter，请将其放置于自定义分词链中。\n  常见用例 #  创建索引并配置自定义 Pinyin Analyzer #  PUT /medcl { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_pinyin\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pinyin\u0026#34;, \u0026#34;keep_separate_first_letter\u0026#34;: false, \u0026#34;keep_full_pinyin\u0026#34;: true, \u0026#34;keep_original\u0026#34;: true, \u0026#34;limit_first_letter_length\u0026#34;: 16, \u0026#34;lowercase\u0026#34;: true, \u0026#34;remove_duplicated_term\u0026#34;: true } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;pinyin_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;my_pinyin\u0026#34; } } } } } 测试分词效果\nGET /medcl/_analyze { \u0026#34;text\u0026#34;: [\u0026#34;刘德华\u0026#34;], \u0026#34;analyzer\u0026#34;: \u0026#34;pinyin_analyzer\u0026#34; } 预期返回：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;liu\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;de\u0026#34;, \u0026#34;start_offset\u0026#34;: 1, \u0026#34;end_offset\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;hua\u0026#34;, \u0026#34;start_offset\u0026#34;: 2, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;刘德华\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;ldh\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } 创建字段映射并建立文档 #  直接使用拼音分词器\nPOST /medcl/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;pinyin\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;store\u0026#34;: false, \u0026#34;term_vector\u0026#34;: \u0026#34;with_offsets\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;pinyin_analyzer\u0026#34;, \u0026#34;boost\u0026#34;: 10 } } } } }\n索引一个文档 POST /medcl/_create/andy {\u0026quot;name\u0026quot;:\u0026quot;刘德华\u0026quot;}\n查询测试 curl http://localhost:9200/medcl/_search?q=name:%E5%88%98%E5%BE%B7%E5%8D%8E curl http://localhost:9200/medcl/_search?q=name.pinyin:%e5%88%98%e5%be%b7 curl http://localhost:9200/medcl/_search?q=name.pinyin:liu curl http://localhost:9200/medcl/_search?q=name.pinyin:ldh curl http://localhost:9200/medcl/_search?q=name.pinyin:de+hua\n使用 Pinyin-TokenFilter #\n PUT /medcl1/ { \u0026#34;settings\u0026#34; : { \u0026#34;analysis\u0026#34; : { \u0026#34;analyzer\u0026#34; : { \u0026#34;user_name_analyzer\u0026#34; : { \u0026#34;tokenizer\u0026#34; : \u0026#34;whitespace\u0026#34;, \u0026#34;filter\u0026#34; : \u0026#34;pinyin_first_letter_and_full_pinyin_filter\u0026#34; } }, \u0026#34;filter\u0026#34; : { \u0026#34;pinyin_first_letter_and_full_pinyin_filter\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;pinyin\u0026#34;, \u0026#34;keep_first_letter\u0026#34; : true, \u0026#34;keep_full_pinyin\u0026#34; : false, \u0026#34;keep_none_chinese\u0026#34; : true, \u0026#34;keep_original\u0026#34; : false, \u0026#34;limit_first_letter_length\u0026#34; : 16, \u0026#34;lowercase\u0026#34; : true, \u0026#34;trim_whitespace\u0026#34; : true, \u0026#34;keep_none_chinese_in_first_letter\u0026#34; : true } } } } } Token 测试:刘德华 张学友 郭富城 黎明 四大天王\nGET /medcl1/_analyze { \u0026#34;text\u0026#34;: [\u0026#34;刘德华 张学友 郭富城 黎明 四大天王\u0026#34;], \u0026#34;analyzer\u0026#34;: \u0026#34;user_name_analyzer\u0026#34; } { \u0026#34;tokens\u0026#34; : [ { \u0026#34;token\u0026#34; : \u0026#34;ldh\u0026#34;, \u0026#34;start_offset\u0026#34; : 0, \u0026#34;end_offset\u0026#34; : 3, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 0 }, { \u0026#34;token\u0026#34; : \u0026#34;zxy\u0026#34;, \u0026#34;start_offset\u0026#34; : 4, \u0026#34;end_offset\u0026#34; : 7, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 1 }, { \u0026#34;token\u0026#34; : \u0026#34;gfc\u0026#34;, \u0026#34;start_offset\u0026#34; : 8, \u0026#34;end_offset\u0026#34; : 11, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 2 }, { \u0026#34;token\u0026#34; : \u0026#34;lm\u0026#34;, \u0026#34;start_offset\u0026#34; : 12, \u0026#34;end_offset\u0026#34; : 14, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 3 }, { \u0026#34;token\u0026#34; : \u0026#34;sdtw\u0026#34;, \u0026#34;start_offset\u0026#34; : 15, \u0026#34;end_offset\u0026#34; : 19, \u0026#34;type\u0026#34; : \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34; : 4 } ] } 短语查询 #  场景 1:全拼音查询\nPUT /medcl2/ { \u0026#34;settings\u0026#34; : { \u0026#34;analysis\u0026#34; : { \u0026#34;analyzer\u0026#34; : { \u0026#34;pinyin_analyzer\u0026#34; : { \u0026#34;tokenizer\u0026#34; : \u0026#34;my_pinyin\u0026#34; } }, \u0026#34;tokenizer\u0026#34; : { \u0026#34;my_pinyin\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;pinyin\u0026#34;, \u0026#34;keep_first_letter\u0026#34;:false, \u0026#34;keep_separate_first_letter\u0026#34; : false, \u0026#34;keep_full_pinyin\u0026#34; : true, \u0026#34;keep_original\u0026#34; : false, \u0026#34;limit_first_letter_length\u0026#34; : 16, \u0026#34;lowercase\u0026#34; : true } } } } } POST /medcl2/_mapping { \u0026quot;properties\u0026quot;: { \u0026quot;name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;fields\u0026quot;: { \u0026quot;pinyin\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;store\u0026quot;: false, \u0026quot;term_vector\u0026quot;: \u0026quot;with_offsets\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;pinyin_analyzer\u0026quot;, \u0026quot;boost\u0026quot;: 10 } } } } }\nPOST /medcl2/_doc?refresh=true {\u0026quot;name\u0026quot;:\u0026quot;liudehua\u0026quot;}\nGET /medcl2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;刘德华\u0026quot; } } } 场景 2:全拼/缩写/混写等多场景匹配\nPUT /medcl3/ { \u0026#34;settings\u0026#34; : { \u0026#34;analysis\u0026#34; : { \u0026#34;analyzer\u0026#34; : { \u0026#34;pinyin_analyzer\u0026#34; : { \u0026#34;tokenizer\u0026#34; : \u0026#34;my_pinyin\u0026#34; } }, \u0026#34;tokenizer\u0026#34; : { \u0026#34;my_pinyin\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;pinyin\u0026#34;, \u0026#34;keep_first_letter\u0026#34;:true, \u0026#34;keep_separate_first_letter\u0026#34; : true, \u0026#34;keep_full_pinyin\u0026#34; : true, \u0026#34;keep_original\u0026#34; : false, \u0026#34;limit_first_letter_length\u0026#34; : 16, \u0026#34;lowercase\u0026#34; : true } } } } } POST /medcl3/_mapping { \u0026quot;properties\u0026quot;: { \u0026quot;name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;fields\u0026quot;: { \u0026quot;pinyin\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;store\u0026quot;: false, \u0026quot;term_vector\u0026quot;: \u0026quot;with_offsets\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;pinyin_analyzer\u0026quot;, \u0026quot;boost\u0026quot;: 10 } } } } }\nGET /medcl3/_analyze { \u0026quot;text\u0026quot;: [\u0026quot;刘德华\u0026quot;], \u0026quot;analyzer\u0026quot;: \u0026quot;pinyin_analyzer\u0026quot; }\nPOST /medcl3/_create/andy {\u0026quot;name\u0026quot;:\u0026quot;刘德华\u0026quot;}\nGET /medcl3/_search { \u0026quot;query\u0026quot;: {\u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;刘德h\u0026quot; }} }\nGET /medcl3/_search { \u0026quot;query\u0026quot;: {\u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;刘dh\u0026quot; }} }\nGET /medcl3/_search { \u0026quot;query\u0026quot;: {\u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;liudh\u0026quot; }} }\nGET /medcl3/_search { \u0026quot;query\u0026quot;: {\u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;liudeh\u0026quot; }} }\nGET /medcl3/_search { \u0026quot;query\u0026quot;: {\u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;liude华\u0026quot; }} }\n\n","subcategory":null,"summary":"","tags":null,"title":"拼音分析器（Pinyin）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/pinyin-analyzer/"},{"category":null,"content":"归一化与规范化器 #  词元归一化是将不同形式的词项统一为标准形式的过程，这对于提高搜索的召回率非常重要。例如，搜索 rôle 时也应该能匹配到 role。\n为什么需要归一化？ #  在搜索中，我们经常遇到这样的情况：\n 同一个词的不同大小写形式：The、the、THE 带变音符号和不带变音符号的词：rôle、role Unicode 的不同表示形式：é 可能以不同方式编码  如果不进行归一化，这些不同的形式会被视为不同的词项，导致搜索时无法匹配。\n小写转换（Lowercasing） #  小写转换是最常见的归一化操作。大多数分析器默认都会进行小写转换。\n使用 lowercase 过滤器 #  PUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_lowercase_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } } } 测试效果：\nGET /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_lowercase_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The Quick Brown Fox\u0026#34; } 输出：[the, quick, brown, fox]\n何时需要保留大小写？ #  某些场景下可能需要保留大小写：\n 专有名词：iPhone、C++ 缩写：USA、NASA 代码标识符：userId、getUserInfo  对于这些场景，可以使用 keyword 类型或自定义分析器。\n移除变音符号（Removing Diacritics） #  变音符号（如 é、ö、ñ）在某些语言中很重要，但在搜索时可能需要忽略它们。\n使用 asciifolding 过滤器 #  asciifolding 过滤器可以将带变音符号的字符转换为对应的 ASCII 字符：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_asciifolding_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;asciifolding\u0026#34;] } } } } } 测试效果：\nGET /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_asciifolding_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;rôle café naïve\u0026#34; } 输出：[role, cafe, naive]\n保留变音符号的场景 #  对于某些语言，变音符号是区分词义的关键：\n 西班牙语：año（年）vs ano（肛门） 德语：schön（美丽）vs schon（已经）  在这些场景下，应该保留变音符号，或者使用多字段同时索引两种形式。\nUnicode 规范化 #  Unicode 中，同一个字符可能有多种表示方式：\n 组合形式：é = e + ´ 预组合形式：é = 单个字符 é  使用 icu_normalizer #  ICU 插件提供了更强大的 Unicode 规范化功能：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_icu_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;icu_normalizer\u0026#34;, \u0026#34;lowercase\u0026#34;] } } } } } 字符折叠（Character Folding） #  字符折叠是将相似字符统一的过程，例如：\n ™ → tm © → c ® → r  使用 mapping 字符过滤器 #  可以创建自定义的字符映射：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;my_char_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mapping\u0026#34;, \u0026#34;mappings\u0026#34;: [ \u0026#34;™ =\u0026gt; tm\u0026#34;, \u0026#34;© =\u0026gt; c\u0026#34;, \u0026#34;® =\u0026gt; r\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_char_folding_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;my_char_filter\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } } } 排序和校对（Sorting and Collations） #  对于排序操作，可能需要考虑语言的特定规则：\n 德语：ä 应该排在 a 和 b 之间 瑞典语：ö 应该排在 z 之后  这些规则通常在应用层处理，而不是在索引时处理。\n最佳实践 #   默认使用小写转换：大多数场景下都应该进行小写转换 谨慎使用 asciifolding：只在确实需要忽略变音符号时使用 使用多字段：如果需要同时支持精确匹配和归一化匹配，使用多字段 测试效果：使用 _analyze API 测试归一化效果  示例：同时支持精确匹配和归一化匹配\nPUT /my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;normalized\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_asciifolding_analyzer\u0026#34; } } } } } }  规范化器（Normalizer） #  对于 keyword 字段类型，可以使用 Normalizer（规范化器）在索引和查询之前对值进行归一化处理。Normalizer 类似于分析器，但只输出单个词元，不做分词。\n详细配置请参阅 规范化器参考文档，包括：\n  Lowercase 规范化器 — 内置，直接使用  自定义规范化器 — 组合字符过滤器和词元过滤器   小结 #   词元归一化是将不同形式的词项统一为标准形式的过程 小写转换是最常见的归一化操作 asciifolding 可以移除变音符号，但需要谨慎使用 Unicode 规范化可以处理字符的不同表示形式 Normalizer 专用于 keyword 字段，只做字符级变换不分词 使用多字段可以同时支持精确匹配和归一化匹配  相关参考 #    规范化器配置参考  词干提取  停用词  词汇识别  字符过滤器  词元过滤器：normalization  ","subcategory":null,"summary":"","tags":null,"title":"归一化与规范化器","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis-normalization/"},{"category":null,"content":"Flatten Graph 分词过滤器 #  flatten_graph 分词过滤器用于处理在图结构中同一位置生成多个词元时出现的复杂词元关系。一些分词过滤器，比如 synonym_graph 和 word_delimiter_graph，会生成多位置词元——即相互重叠或跨越多个位置的词元。这些词元图对于搜索查询很有用，但在索引过程中却不被直接支持。flatten_graph 分词过滤器会将多位置词元解析为一个线性的词元序列。对图进行扁平化处理可确保与索引过程兼容。\n 注意：词元图的扁平化是一个有损耗的过程。只要有可能，就应避免使用 flatten_graph 过滤器。相反，应仅在搜索分词器中应用图分词过滤器，这样就无需使用 flatten_graph 分词过滤器了。\n 相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  参考样例 #  以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有平图化分词过滤器的分词器：\nPUT /test_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_index_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;my_custom_filter\u0026#34;, \u0026#34;flatten_graph\u0026#34; ] } }, \u0026#34;filter\u0026#34;: { \u0026#34;my_custom_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;word_delimiter_graph\u0026#34;, \u0026#34;catenate_all\u0026#34;: true } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /test_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_index_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch helped many employers\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;helped\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;many\u0026#34;, \u0026#34;start_offset\u0026#34;: 18, \u0026#34;end_offset\u0026#34;: 22, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;employers\u0026#34;, \u0026#34;start_offset\u0026#34;: 23, \u0026#34;end_offset\u0026#34;: 32, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 } ] } ","subcategory":null,"summary":"","tags":null,"title":"展平图分词过滤器（Flatten Graph）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/flatten-graph/"},{"category":null,"content":"_source 与字段存储 #  _source 是文档写入时的原始 JSON，而字段的存储和检索有多种方式。本页讨论如何在\u0026quot;读取灵活性、存储成本与性能\u0026quot;之间做权衡。\n_source：默认的真相来源 #  _source 保存了写入时的完整 JSON 文档。在 _source 开启的情况下：\n GET 请求可以直接返回原始文档 update 操作可以在服务端基于 _source 进行部分更新 需要重建索引时，可以直接从 _source 读取数据（参见 重建索引）  大多数场景下，建议保持 _source 开启，除非有非常严格的存储或合规要求。\n_source 字段过滤 #  在查询时，可以通过 _source_includes 和 _source_excludes 只返回需要的字段，减少网络传输：\n// GET 请求中过滤 _source GET /my-index/_doc/1?_source_includes=title,date // Search 请求中过滤 _source POST /my-index/_search { \u0026quot;_source\u0026quot;: { \u0026quot;includes\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;date\u0026quot;], \u0026quot;excludes\u0026quot;: [\u0026quot;description\u0026quot;] }, \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} } }\n// 完全不返回 _source GET /my-index/_doc/1?_source=false \n这是接口级别的\u0026quot;投影\u0026quot;，不影响存储——_source 中仍然保存完整文档。\n Doc Values：列式存储 #  Easysearch 中，大多数字段类型默认使用 doc_values 进行列式存储，这使得：\n 聚合、排序、脚本等操作可以直接从 doc_values 读取，无需加载 _source 以列式存储，对聚合和排序操作非常高效 默认情况下，除了 text 和 annotated_text 字段外，所有字段都启用 doc_values  在查询中使用 docvalue_fields 可以直接从列式存储读取字段值：\nPOST /my-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;docvalue_fields\u0026#34;: [\u0026#34;timestamp\u0026#34;, \u0026#34;status_code\u0026#34;], \u0026#34;_source\u0026#34;: false } 如果某个字段永远不需要聚合或排序，可以在映射中关闭 doc_values 以节省磁盘空间：\nPUT /my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;doc_values\u0026#34;: false } } } } Stored Fields：按字段单独存储 #   注意：stored fields 已经基本被 doc_values 替代。只有在极少数特殊场景下才需要使用。\n stored fields 可以让你只从某些字段读取值，而不必返回整个 _source。但在现代用法中：\n 大多数场景下，直接依赖 _source + 客户端投影已经足够 对于聚合、排序等操作，doc_values 已经提供了更好的性能 只有在以下极端场景才考虑使用 stored fields：  _source 极大且不方便拆分 读取路径非常敏感，对响应大小有严格要求 需要返回的字段没有启用 doc_values（如 text 字段）    读取策略：按场景选择 #     场景 推荐策略 说明     后台任务 / 管理工具 直接拉 _source 处理灵活度最高   在线接口（延迟/带宽敏感） _source_includes 限制字段 减少网络传输   聚合和排序 doc_values（默认） 无需加载 _source，列式读取更高效   text 字段聚合 启用 fielddata 注意内存开销，通常建议用 keyword 子字段替代   不需要 _source 的大批量扫描 _source: false + docvalue_fields 适合数据导出、ETL 场景    在设计接口与文档结构时，可以先规划好\u0026quot;接口级别的字段投影\u0026quot;，而不是在存储层做过早优化。\n关闭 _source 的风险 #  在某些极端场景下，有人会考虑关闭 _source 以节省存储，但这会带来明显风险：\n 无法直接 GET 出完整文档 无法在服务端进行基于 _source 的 update 操作 无法仅依靠索引本身完成 重建索引（需要从外部数据源重拉） 无法使用 _update_by_query 批量更新  因此，除非你有非常清晰的外部数据主源与恢复策略，否则不建议关闭 _source。\n小结 #   _source 通常应保留——它是重建索引、调试与 update 操作的\u0026quot;最后后盾\u0026quot; doc_values 是聚合、排序、脚本操作的首选，默认已启用（text 字段除外） stored fields 在现代用法中已基本过时，仅在极少数特殊场景下使用 真正需要优化响应大小时，优先用 _source_includes / _source_excludes 做接口级投影 关闭 _source 会让 reindex、update 等操作无法在服务端完成，需谨慎评估  下一步可以继续阅读：\n  索引设置  别名（Aliases）  映射基础  ","subcategory":null,"summary":"","tags":null,"title":"_source 与字段存储","url":"/easysearch/main/docs/fundamentals/source-and-stored-fields/"},{"category":null,"content":"Wildcard 字段类型 #  wildcard 字段是 keyword 字段的一种变体，专为任意子字符串和正则表达式匹配而设计。\n当您的内容由\u0026quot;字符串\u0026quot;而非\u0026quot;文本\u0026quot;组成时，应使用 wildcard 字段。示例包括非结构化日志行和计算机代码。\nwildcard 字段类型的索引方式与 keyword 字段类型不同。keyword 字段将原始字段值写入索引，而 wildcard 字段类型则将字段值拆分为长度小于或等于 3 的子字符串，并将这些子字符串写入索引。例如，字符串 test 被拆分为 t、te、tes、e、es 和 est 这些子字符串。\n在搜索时，将查询模式中所需的子字符串与索引进行匹配以生成候选文档，然后根据查询中的模式对这些文档进行过滤。例如，对于搜索词 test，Easysearch 执行索引搜索 tes AND est。如果搜索词包含少于三个字符，Easysearch 会使用长度为一或二的字符子字符串。对于每个匹配的文档，如果源值为 test，则该文档将出现在结果中。这样可以排除误报值，如 nikola tesla felt alternating current was best。\n通常，精确匹配查询（如 term 或 terms 查询）在 wildcard 字段上的表现不如在 keyword 字段上有效，而 wildcard、 prefix 和 regexp 查询在 wildcard 字段上表现更好。\n相关指南（先读这些） #    映射基础  部分匹配  结构化搜索  示例 #  创建带有 wildcard 字段的映射：\nPUT logs { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;log_line\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;wildcard\u0026#34; } } } } 参数 #  以下表格列出了 wildcard 字段可用的所有参数。 `\n   参数 描述     doc_values 布尔值，指定该字段是否应存储在磁盘上，以便用于聚合、排序或脚本操作。默认值为 false。   ignore_above 长度超过此整数值的任何字符串都不会被索引。默认值为 2147483647。   normalizer 用于预处理索引和搜索值的标准化器。默认情况下，不进行标准化，使用原始值。您可以使用 lowercase 标准化器在该字段上执行不区分大小写的匹配。   null_value 用于替代 null 的值。必须与字段类型相同。如果未指定此参数，则当字段值为 null 时，该字段将被视为缺失。默认值为 null。    ","subcategory":null,"summary":"","tags":null,"title":"通配符字段类型（Wildcard）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/wildcard/"},{"category":null,"content":"词汇识别 #  词汇识别是文本分析的第一步：将文本拆分为可搜索的词项。不同语言的词汇识别方式差异很大，需要选择合适的分析器和分词器。\n不同语言的挑战 #  英语 #  英语单词相对容易识别：单词之间通常以空格或标点分隔。但也有一些边界情况：\n you're 是一个单词还是两个？ o'clock、cooperate、half-baked、eyewitness 等复合词如何处理？  德语和荷兰语 #  这些语言会将独立的单词合并成长复合词，例如：\n Weißkopfseeadler（white-headed sea eagle）  为了在查询 Adler（eagle）时也能匹配到 Weißkopfseeadler，需要将复合词拆分成词组。\n亚洲语言 #  亚洲语言更复杂：\n 很多语言在单词、句子甚至段落之间没有空格 有些词可以用一个字表达，但同样的字在另一个字旁边时就是不同意思的长词的一部分  标准分析器 #  任何全文检索的 text 字段默认使用 standard 分析器。标准分析器包括：\n 分词器：standard 分词器 词元过滤器：lowercase（小写转换）和 stop（停用词，默认关闭）  标准分析器可以这样重新实现：\n{ \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;stop\u0026#34;] } 标准分词器 #  standard 分词器基于 Unicode 文本分割算法，能够：\n 识别单词边界（空格、标点） 处理大多数欧洲语言 处理亚洲语言（虽然可能不够精确）  示例：\nGET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The 2 QUICK Brown-Foxes jumped over the lazy dog\u0026#39;s bone.\u0026#34; } 输出：\n[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog\u0026#39;s, bone ] 可以看到：\n Brown-Foxes 被拆分为 Brown 和 Foxes dog's 被保留为一个词项（不会拆分）  语言特定的分析器 #  对于特定语言，Easysearch 提供了专门的分析器：\n 中文：需要中文分词器（如 IK 分词器） 日语：需要日语分词器（如 Kuromoji） 韩语：需要韩语分词器 德语：可以使用德语分析器处理复合词  ICU 分词器 #  对于需要更精确 Unicode 支持的语言，可以使用 ICU 分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_icu_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;icu_tokenizer\u0026#34; } } } } } ICU 分词器能够：\n 更好地处理 Unicode 文本 支持更多语言的文本分割规则 处理变音符号和特殊字符  选择合适的分词器 #  选择分词器时需要考虑：\n  语言类型：\n 欧洲语言：standard 分词器通常足够 亚洲语言：需要语言特定的分词器 混合语言：可能需要自定义分析器    文本特点：\n 是否包含复合词 是否需要保留特殊字符 是否需要处理变音符号    查询需求：\n 是否需要精确匹配 是否需要模糊匹配 是否需要处理同义词    测试分词器 #  使用 _analyze API 测试分词器的效果：\nGET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The quick brown fox\u0026#34; } 或者测试整个分析器：\nGET /my_index/_analyze { \u0026#34;field\u0026#34;: \u0026#34;title\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The quick brown fox\u0026#34; } 小结 #   词汇识别是将文本拆分为词项的第一步 不同语言的词汇识别方式差异很大 standard 分词器适合大多数欧洲语言 亚洲语言需要语言特定的分词器 可以使用 _analyze API 测试分词器的效果  下一步可以继续阅读：\n  词元归一化  词干提取  文本分析  参考手册（API 与参数） #    分词器（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"词汇识别","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis-identifying-words/"},{"category":null,"content":"混合评分解释处理器（Hybrid Score Explanation Processor） #   需要 AI 插件\n hybrid_score_explanation 响应处理器用于为混合搜索结果添加评分解释信息。当与 hybrid_ranker_processor 配合使用时，该处理器将归一化和得分融合的详细过程附加到每个搜索命中结果中，帮助用户理解最终得分的来源。\n工作原理 #  在混合搜索管道中，hybrid_ranker_processor 在得分融合过程中会将详细的计算信息（各子查询的原始得分、归一化方式、融合方式）记录到管道处理上下文中。hybrid_score_explanation 处理器在响应阶段读取这些信息，并将其合并到每个搜索命中结果的 _explanation 字段中。\n解释信息包括：\n 归一化详情：使用的归一化技术及每个子查询得分的归一化结果 得分融合详情：使用的融合算法（如 RRF）及各路得分的融合过程  请求体字段 #     字段 类型 是否必填 说明     tag String 否 处理器标识标签   description String 否 处理器描述   ignore_failure Boolean 否 处理器失败时是否继续执行。默认 false    示例 #  基本用法 #  PUT /_search/pipeline/my_explain_pipeline { \u0026#34;phase_results_processors\u0026#34;: [ { \u0026#34;hybrid_ranker_processor\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;ranker\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;RRF 混合排序\u0026#34; } } ], \u0026#34;response_processors\u0026#34;: [ { \u0026#34;hybrid_score_explanation\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;explanation\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;添加混合评分解释\u0026#34; } } ] } 返回结果示例 #  使用上述管道执行混合搜索后，每个命中结果的 _explanation 中会包含类似以下的评分解释：\n{ \u0026#34;hits\u0026#34;: { \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.016393442, \u0026#34;_explanation\u0026#34;: { \u0026#34;value\u0026#34;: 0.016393442, \u0026#34;description\u0026#34;: \u0026#34;combined score of:\u0026#34;, \u0026#34;details\u0026#34;: [ { \u0026#34;value\u0026#34;: 0.016393442, \u0026#34;description\u0026#34;: \u0026#34;rrf combination of:\u0026#34;, \u0026#34;details\u0026#34;: [ { \u0026#34;value\u0026#34;: 0.016129032, \u0026#34;description\u0026#34;: \u0026#34;rrf normalization of:\u0026#34;, \u0026#34;details\u0026#34;: [ { \u0026#34;value\u0026#34;: 1.0, \u0026#34;description\u0026#34;: \u0026#34;keyword query score\u0026#34; } ] }, { \u0026#34;value\u0026#34;: 0.016393442, \u0026#34;description\u0026#34;: \u0026#34;rrf normalization of:\u0026#34;, \u0026#34;details\u0026#34;: [ { \u0026#34;value\u0026#34;: 0.95, \u0026#34;description\u0026#34;: \u0026#34;vector query score\u0026#34; } ] } ] } ] } } ] } } 注意事项 #   该处理器通常与 hybrid_ranker_processor 配合使用，单独使用时无评分解释信息可供附加 使用 explain: true 参数可以同时获取搜索引擎自身的评分解释和混合搜索的评分解释  相关文档 #    混合搜索排序处理器：执行得分归一化和融合  混合搜索：完整的混合搜索工作流和示例  ","subcategory":null,"summary":"","tags":null,"title":"混合评分解释处理器","url":"/easysearch/main/docs/features/query-dsl/search-pipelines/hybrid-score-explanation-processor/"},{"category":null,"content":"Word Delimiter Graph 分词过滤器 #  word_delimiter_graph 分词过滤器用于依据预定义的字符对词元进行拆分，还能基于可定制的规则对词元进行可选的规范化处理。\n 提示：word_delimiter_graph 分词过滤器可用于去除零件编号或产品 ID 这类复杂标识符中的标点符号。在这种情况下，它最好与 keyword 分词器搭配使用。对于带有连字符的单词，建议使用 synonym_graph 分词过滤器而非 word_delimiter_graph 分词过滤器，因为用户在搜索这些词时，往往既会搜索带连字符的形式，也会搜索不带连字符的形式。\n 相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  默认情况下，该过滤器会应用以下规则：\n   描述 输入 输出     将非字母数字字符视为分隔符 ultra-fast ultra, fast   去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder   当字母大小写发生转换时拆分词元 Easysearch Easy, search   当字母和数字之间发生转换时拆分词元 T1000 T, 1000   去除词元末尾的所有格形式（\u0026lsquo;s） John's John     重要的是，不要将此过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并且会影响诸如 catenate_all 或 preserve_original 之类的选项。我们建议将此过滤器与关键字(keyword)分词器或空白(whitespace)分词器搭配使用。\n 参数说明 #  你可以使用以下参数来配置单词分隔符图分词过滤器。\n   参数 必需/可选 数据类型 描述     adjust_offsets 可选 布尔值 决定是否要为拆分或连接后的词元重新计算词元偏移量。若为 true，过滤器会调整词元偏移量，以准确呈现词元在词元流中的位置。这种调整能确保词元在文本中的位置与处理后的修改形式相匹配，这对高亮显示或短语查询等应用特别有用。若为 false，偏移量保持不变，这可能会导致处理后的词元映射回原始文本位置时出现错位。如果你的分词器使用了像 trim 这类会改变词元长度但不改变偏移量的过滤器，建议将此参数设为 false。默认值为 true。   catenate_all 可选 布尔值 从一系列字母数字部分生成连接后的词元。例如，quick-fast-200 会变成 [ quickfast200, quick, fast, 200 ]。默认值为 false。   catenate_numbers 可选 布尔值 连接数字序列。例如，10-20-30 会变成 [ 102030, 10, 20, 30 ]。默认值为 false。   catenate_words 可选 布尔值 连接字母单词。例如，high-speed-level 会变成 [ highspeedlevel, high, speed, level ]。默认值为 false。   generate_number_parts 可选 布尔值 若为 true，输出中会包含纯数字词元（仅由数字组成的词元）。默认值为 true。   generate_word_parts 可选 布尔值 若为 true，输出中会包含纯字母词元（仅由字母字符组成的词元）。默认值为 true。   ignore_keywords 可选 布尔值 是否处理标记为关键字的词元。默认值为 false。   preserve_original 可选 布尔值 在输出中，除了生成的词元外，还保留原始词元（可能包含非字母数字分隔符）。例如，auto-drive-300 会变成 [ auto - drive - 300, auto, drive, 300 ]。如果为 true，该过滤器会生成索引不支持的多位置词元，因此请勿在索引分词器中使用此过滤器，或者在该过滤器之后使用 flatten_graph 过滤器。默认值为 false。   protected_words 可选 字符串数组 指定不应被拆分的词元。   protected_words_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含不应被分隔的词元，词元需分行列出。   split_on_case_change 可选 布尔值 在连续字母大小写不同（一个为小写，另一个为大写）的位置拆分词元。例如，“EasySearch” 会变成 [ Easy, Search ]。默认值为 true。   split_on_numerics 可选 布尔值 在连续字母和数字的位置拆分词元。例如，v8engine 会变成 [ v, 8, engine ]。默认值为 true。   stem_english_possessive 可选 布尔值 去除英语所有格结尾，如 \u0026lsquo;s。默认值为 true。   type_table 可选 字符串数组 一个自定义映射，用于指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。例如，要将连字符（-）视为字母数字字符，可指定 [\u0026quot;- =\u0026gt; ALPHA\u0026quot;]，这样单词就不会在连字符处拆分。有效类型有：\n- ALPHA：字母\n- ALPHANUM：字母数字\n- DIGIT：数字\n- LOWER：小写字母\n- SUBWORD_DELIM：非字母数字分隔符\n- UPPER：大写字母   type_table_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含自定义字符映射。该映射指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。有关有效类型，请参阅 type_table。    参考样例 #  以下示例请求创建了一个名为 my-custom-index 的新索引，并配置了一个带有单词分隔符图过滤器（word_delimiter_graph）的分词器。\nPUT /my-custom-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;custom_word_delimiter_filter\u0026#34; ] } }, \u0026#34;filter\u0026#34;: { \u0026#34;custom_word_delimiter_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;word_delimiter_graph\u0026#34;, \u0026#34;split_on_case_change\u0026#34;: true, \u0026#34;split_on_numerics\u0026#34;: true, \u0026#34;stem_english_possessive\u0026#34;: true } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-custom-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;FastCar\u0026#39;s Model2023\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Fast\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;Car\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;Model\u0026#34;, \u0026#34;start_offset\u0026#34;: 10, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;2023\u0026#34;, \u0026#34;start_offset\u0026#34;: 15, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 } ] } 单词分隔符图过滤器（word_delimiter_graph）和单词分隔符过滤器（word_delimiter）的区别 #  当以下任何参数设置为 true 时，单词分隔符图过滤器和单词分隔符分词过滤器都会生成跨越多个位置的词元：\n catenate_all catenate_numbers catenate_words preserve_original  为了说明这两种过滤器的区别，我们以输入文本 Pro-XT500 为例。\n单词分隔符图过滤器（word_delimiter_graph） #  单词分隔符图过滤器会为多位置词元分配一个 positionLength 属性，该属性表明一个词元跨越了多少个位置。这确保了该过滤器始终能生成有效的词元图，使其适用于高级词元图场景。虽然带有多位置词元的词元图不支持用于索引，但它们在搜索场景中仍然很有用。例如，像 match_phrase 这样的查询可以使用这些图从单个输入字符串生成多个子查询。对于示例输入文本，单词分隔符图过滤器会生成以下词元：\n Pro（位置 1） XT500（位置 2） ProXT500（位置 1，positionLength：2）  positionLength 属性使得生成的图可用于高级查询。\n单词分隔符过滤器（word_delimiter） #  相比之下，单词分隔符过滤器不会为多位置词元分配 positionLength 属性，当存在这些词元时，会导致生成无效的图。对于示例输入文本，单词分隔符过滤器会生成以下词元：\n Pro（位置 1） XT500（位置 2） ProXT500（位置 1，无 positionLength）  缺少 positionLength 属性会导致包含多位置词元的词元流所生成的词元图无效。\n","subcategory":null,"summary":"","tags":null,"title":"单词分隔图分词过滤器（Word Delimiter Graph）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/word-delimiter-graph/"},{"category":null,"content":"Search Template API #  您可以将全文查询转换为查询模板，以接受用户输入并将其动态插入到查询中。\n例如，如果您使用 Easysearch 作为应用程序或网站的后端搜索引擎，则可以从搜索栏或表单字段接收用户查询，并将其作为参数传递到查询模板中。这样，创建 Easysearch 查询的语法就从最终用户那里抽象出来了。\n当您编写代码将用户输入转换为 Easysearch 查询时，可以使用查询模板简化代码。如果需要将字段添加到搜索查询中，只需修改模板即可，而无需更改代码。\n查询模板使用 Mustache 语言。有关所有语法选项的列表，请参阅 Mustache 手册。\n相关指南（先读这些） #    查询 DSL 基础  全文搜索  创建查询模版 #  查询模版有两个组件：查询和参数。参数是放置在变量中的用户输入值。在 Mustache 符号中，变量用双括号表示。当在查询中遇到类似 {% raw %}{{var}}{% endraw %} 的变量时，Easysearch 会转到 params 部分，查找名为 var 的参数，并用指定的值替换它。\n您可以编写应用程序代码，询问用户要搜索什么，然后在运行时将该值插入 params 对象中。\n此命令定义了一个查询模版，用于按名称查找播放。查询中的 {% raw %}{{play_name}}{% endraw %} 被值 Henry IV 替换：\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;{% raw %}{{play_name}}{% endraw %}\u0026#34; } } }, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } } 此模板在整个集群上运行搜索。\n要在特定索引上运行此搜索，请将索引名称添加到请求中：\nGET shakespeare/_search/template 指定 from 和 size 参数：\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;{% raw %}{{from}}{% endraw %}\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;{% raw %}{{size}}{% endraw %}\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;{% raw %}{{play_name}}{% endraw %}\u0026#34; } } }, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34;, \u0026#34;from\u0026#34;: 10, \u0026#34;size\u0026#34;: 10 } } 为了改善搜索体验，您可以定义默认值，这样用户就不必指定每个可能的参数。如果参数未在 params 部分中定义，Easysearch 将使用默认值。\n定义变量 var 默认值的语法如下：\n{% raw %}{{var}}{{^var}}default value{{/var}}{% endraw %} 此命令将 from 的默认值设置为 10，将 size 设置为 10：\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;{% raw %}{{from}}{{^from}}10{{/from}}{% endraw %}\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;{% raw %}{{size}}{{^size}}10{{/size}}{% endraw %}\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;{% raw %}{{play_name}}{% endraw %}\u0026#34; } } }, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } } 保存并执行查询模版 #  查询模版按您希望的方式工作后，可以将该模板的源保存为脚本，使其可用于不同的输入参数。\n将查询模版另存为脚本时，需要将 lang 参数指定为 muscle ：\nPOST _scripts/play_search_template { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;mustache\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;{% raw %}{{from}}{{^from}}0{{/from}}{% endraw %}\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;{% raw %}{{size}}{{^size}}10{{/size}}{% endraw %}\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;{{play_name}}\u0026#34; } } }, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } } } 现在，您可以通过引用模板的 id 参数来重用模板。\n您可以将此源模板用于不同的输入值。\nGET _search/template { \u0026#34;id\u0026#34;: \u0026#34;play_search_template\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34;, \u0026#34;from\u0026#34;: 0, \u0026#34;size\u0026#34;: 1 } } 输出示例 #  { \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 6, \u0026#34;successful\u0026#34;: 6, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3205, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 3.641852, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 3.641852, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 5, \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34;, \u0026#34;speech_number\u0026#34;: 1, \u0026#34;line_number\u0026#34;: \u0026#34;1.1.2\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;KING HENRY IV\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;Find we a time for frighted peace to pant,\u0026#34; } } ] } } 如果您有一个存储的模板并希望对其进行验证，请使用 render 操作：\nPOST _render/template { \u0026#34;id\u0026#34;: \u0026#34;play_search_template\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } } 输出示例 #  { \u0026#34;template_output\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } } } } 使用查询模版进行高级参数转换 #  Mustache 中有很多不同的语法选项，可以将输入参数转换为查询。\n您可以指定条件、运行循环、连接数组、将数组转换为 JSON 等。\n条件 #  使用 Mustache 中的条件表达方式：\n{% raw %}{{#var}}var{{/var}}{% endraw %} 当 var 是布尔值时，此语法充当 if 条件。只有当 var 的值为 true 时， {#var}} 和 {/var}}} 标记之间才会插入参数值。\n使用 section 标记会使 JSON 无效，因此必须改用字符串格式编写查询。\n只有当 limit 参数设置为 true 时，此命令才会在查询中包含 size 参数。\n在以下示例中， limit 参数为 true ，因此 size 参数被激活。因此，您只会得到两个文档。\nGET _search/template { \u0026#34;source\u0026#34;: \u0026#34;{% raw %}{ {{#limit}} \\\u0026#34;size\\\u0026#34;: \\\u0026#34;{{size}}\\\u0026#34;, {{/limit}} \\\u0026#34;query\\\u0026#34;:{\\\u0026#34;match\\\u0026#34;:{\\\u0026#34;play_name\\\u0026#34;: \\\u0026#34;{{play_name}}\\\u0026#34;}}}{% endraw %}\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34;, \u0026#34;limit\u0026#34;: true, \u0026#34;size\u0026#34;: 2 } } 您还可以设计 if-else 条件。\n如果 limit 为 true ，则此命令将 size 设置为 2 。否则，它将 大小 设置为 10 。\nGET _search/template { \u0026#34;source\u0026#34;: \u0026#34;{% raw %}{ {{#limit}} \\\u0026#34;size\\\u0026#34;: \\\u0026#34;2\\\u0026#34;, {{/limit}} {{^limit}} \\\u0026#34;size\\\u0026#34;: \\\u0026#34;10\\\u0026#34;, {{/limit}} \\\u0026#34;query\\\u0026#34;:{\\\u0026#34;match\\\u0026#34;:{\\\u0026#34;play_name\\\u0026#34;: \\\u0026#34;{{play_name}}\\\u0026#34;}}}{% endraw %}\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34;, \u0026#34;limit\u0026#34;: true } } 循环 #  您还可以使用 section 标记来实现 foreach 循环：\n{% raw %}{{#var}}{{.}}{{/var}}{% endraw %} 当 var 是一个数组时，查询模版遍历它并创建一个 terms 查询。\nGET _search/template { \u0026#34;source\u0026#34;: \u0026#34;{% raw %}{\\\u0026#34;query\\\u0026#34;:{\\\u0026#34;terms\\\u0026#34;:{\\\u0026#34;play_name\\\u0026#34;:[\\\u0026#34;{{#play_name}}\\\u0026#34;,\\\u0026#34;{{.}}\\\u0026#34;,\\\u0026#34;{{/play_name}}\\\u0026#34;]}}}{% endraw %}\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: [ \u0026#34;Henry IV\u0026#34;, \u0026#34;Othello\u0026#34; ] } } 此模板呈现为：\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;play_name\u0026#34;: [ \u0026#34;Henry IV\u0026#34;, \u0026#34;Othello\u0026#34; ] } } } } 关联 #  可以使用 join 标记连接数组的值（用逗号分隔）：\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;{% raw %}{{#join}}{{text_entry}}{{/join}}{% endraw %}\u0026#34; } } }, \u0026#34;params\u0026#34;: { \u0026#34;text_entry\u0026#34;: [ \u0026#34;To be\u0026#34;, \u0026#34;or not to be\u0026#34; ] } } 此模板呈现为:\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;{0=To be, 1=or not to be}\u0026#34; } } } } 转换为 JSON #  您可以使用 toJson 标记将参数转换为 JSON 表示形式：\nGET _search/template { \u0026#34;source\u0026#34;: \u0026#34;{\\\u0026#34;query\\\u0026#34;:{\\\u0026#34;bool\\\u0026#34;:{\\\u0026#34;must\\\u0026#34;:[{\\\u0026#34;terms\\\u0026#34;: {\\\u0026#34;text_entries\\\u0026#34;: {% raw %}{{#toJson}}text_entries{{/toJson}}{% endraw %} }}] }}}\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;text_entries\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text_entry\u0026#34; : \u0026#34;love\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;text_entry\u0026#34; : \u0026#34;soldier\u0026#34; } } ] } } 渲染结果:\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;terms\u0026#34;: { \u0026#34;text_entries\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;love\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;soldier\u0026#34; } } ] } } ] } } } } 多个查询模版 #  您可以捆绑多个查询模版，并使用 msearch 操作在单个请求中将它们发送到 Easysearch 集群。\n这节省了网络往返时间，因此与独立请求相比，您可以更快地返回响应。\nGET _msearch/template {\u0026#34;index\u0026#34;:\u0026#34;shakespeare\u0026#34;} {\u0026#34;id\u0026#34;:\u0026#34;if_search_template\u0026#34;,\u0026#34;params\u0026#34;:{\u0026#34;play_name\u0026#34;:\u0026#34;Henry IV\u0026#34;,\u0026#34;limit\u0026#34;:false,\u0026#34;size\u0026#34;:2}} {\u0026#34;index\u0026#34;:\u0026#34;shakespeare\u0026#34;} {\u0026#34;id\u0026#34;:\u0026#34;play_search_template\u0026#34;,\u0026#34;params\u0026#34;:{\u0026#34;play_name\u0026#34;:\u0026#34;Henry IV\u0026#34;}} 管理查询模版 #  要列出所有脚本，请运行以下命令：\nGET _cluster/state/metadata?pretty\u0026amp;filter_path=**.stored_scripts 要检索特定查询模版，请运行以下命令：\nGET _scripts/\u0026lt;name_of_search_template\u0026gt; 要删除查询模版，请运行以下命令：\nDELETE _scripts/\u0026lt;name_of_search_template\u0026gt; ","subcategory":null,"summary":"","tags":null,"title":"Search Template API","url":"/easysearch/main/docs/features/query-dsl/search-template/"},{"category":null,"content":"IK Smart 分析器 #  ik_smart 分析器是为中文智能分词的分析器，使用 IK 分词器的智能模式，会尽量将文本切分为最少的词语。\n需要安装 analysis-ik 插件。  分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n ik_smart 分词器：使用 IK 智能分词模式，倾向于切分出较长的词语 lowercase 分词过滤器：转换为小写  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国国歌\u0026#34; } 相关指南 #    文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"IK 智能分析器（IK Smart）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/ik-smart-analyzer/"},{"category":null,"content":"ICU Tokenizer #  icu_tokenizer 分词器使用 ICU 的 Unicode 文本分割算法，对多语言文本（尤其是亚洲语言混合文本）提供比 standard 分词器更好的分词效果。\n需要安装 analysis-icu 插件。\n示例 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_icu\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;icu_tokenizer\u0026#34; } } } } } 参数 #     参数 说明 默认值     rule_files 自定义 ICU 分词规则文件路径 无    相关指南 #    ICU 分析器  Standard 分词器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"ICU 分词器（ICU Tokenizer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/icu-tokenizer/"},{"category":null,"content":"Eager Global Ordinals 参数 #  eager_global_ordinals 参数控制是否在索引刷新时提前构建全局序数（Global Ordinals）。全局序数是 keyword、ip 等字段执行聚合和排序时使用的内部数据结构。\n相关指南 #    doc_values 参数  fielddata 参数  参数选项 #     值 说明     false 默认值。全局序数在首次查询（聚合/排序）时惰性构建。   true 在索引刷新（refresh）时立即构建全局序数。    默认行为 #  默认情况下，全局序数采用惰性加载策略：第一次聚合或排序请求会触发构建，后续请求直接使用缓存。当索引发生变更并刷新后，缓存失效，下次查询时重新构建。\n示例 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;eager_global_ordinals\u0026#34;: true } } } } 何时启用 #     场景 建议     高频聚合字段，要求低延迟 设为 true，将构建开销转移到索引阶段   普通聚合字段 保持默认 false，惰性构建已足够   很少聚合的字段 保持默认 false，避免浪费资源   写入量远大于查询量 保持默认 false，频繁刷新会导致频繁重建    注意事项 #   启用后会增加索引刷新时间，因为每次 refresh 都需要重新构建全局序数 全局序数存储在堆内存中，高基数字段会占用较多内存 可通过 _stats API 查看全局序数占用的内存大小 对于时序数据等写入密集场景，建议保持默认值以避免刷新延迟增加  ","subcategory":null,"summary":"","tags":null,"title":"预加载全局序数参数（Eager Global Ordinals）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/eager_global_ordinals/"},{"category":null,"content":"Search-as-you-type 字段类型 #  search-as-you-type 字段类型通过前缀和中缀补全提供边输入边搜索的功能。\n代码样例 #  将字段映射为 search-as-you-type 类型时，会为该字段创建 n-gram 子字段，其中 n 的范围为 [2, max_shingle_size]。此外，还会创建一个索引前缀子字段。\n创建一个 search-as-you-type 的映射字段\nPUT books { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;suggestions\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;search_as_you_type\u0026#34; } } } } 除了创建 suggestions 字段外，还会生成 suggestions._2gram、suggestions._3gram 和 suggestions._index_prefix 字段。\n以下是使用 search-as-you-type 字段索引文档的示例：\nPUT books/_doc/1 { \u0026#34;suggestions\u0026#34;: \u0026#34;one two three four\u0026#34; } 要匹配任意顺序的词项，可以使用 bool_prefix 或 multi-match 查询。\n这些查询会将搜索词项按顺序匹配的文档排名提高，而将词项顺序不一致的文档排名降低。\nGET books/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;tw one\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bool_prefix\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;suggestions\u0026#34;, \u0026#34;suggestions._2gram\u0026#34;, \u0026#34;suggestions._3gram\u0026#34; ] } } } 返回内容包含匹配的文档：\n{ \u0026#34;took\u0026#34; : 13, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;books\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : \u0026#34;one two three four\u0026#34; } } ] } } 要按顺序匹配词项，可以使用 match_phrase_prefix 查询：\nGET books/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;suggestions\u0026#34;: \u0026#34;two th\u0026#34; } } } 返回内容包含匹配到的文档：\n{ \u0026#34;took\u0026#34; : 23, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 0.4793051, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;books\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 0.4793051, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : \u0026#34;one two three four\u0026#34; } } ] } } 要精确匹配最后的词项，可以使用 match_phrase 查询：\nGET books/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;suggestions\u0026#34;: \u0026#34;four\u0026#34; } } } 返回内容：\n{ \u0026#34;took\u0026#34; : 2, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 0.2876821, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;books\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 0.2876821, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : \u0026#34;one two three four\u0026#34; } } ] } } 参数说明 #  下表列出了 search-as-you-type 字段类型支持的参数，所有参数均为可选项。    参数 描述 默认值     analyzer 指定该字段使用的分析器，默认情况下用于索引和搜索阶段。如果需要在搜索时覆盖分析器，请设置 search_analyzer 参数。默认使用 standard analyzer，基于 Unicode 文本分割算法和语法规则进行分词。此设置适用于根字段和子字段。 standard analyzer   index 布尔值，指定字段是否可被搜索。此设置适用于根字段和子字段。 true   index_options 指定索引中存储的信息，以支持搜索和高亮显示。可选值：docs (仅文档编号)、freqs (文档编号和词频)、positions (文档编号、词频和位置)、offsets (文档编号、词频、位置及字符偏移)。此设置适用于根字段和子字段。 positions   max_shingle_size 整数值，指定最大 n-gram 大小，有效范围为 [2, 4]。创建的 n-gram 范围为 [2, max_shingle_size]。默认值为 3，会生成 2-gram 和 3-gram。较大的值更适合具体查询，但会导致索引大小增加。 3   norms 布尔值，指定在计算相关性评分时是否使用字段长度。适用于根字段和 n-gram 子字段（默认为 false）。不适用于前缀子字段（在前缀子字段中默认为 false）。 false   search_analyzer 指定搜索时使用的分析器。默认值为 analyzer 参数中指定的分析器。适用于根字段和子字段。 analyzer 参数值   search_quote_analyzer 指定搜索短语时使用的分析器。默认值为 analyzer 参数中指定的分析器。适用于根字段和子字段。 analyzer 参数值   similarity 用于计算相关性评分的排序算法。默认值为 BM25。适用于根字段和子字段。 BM25   store 布尔值，指定字段值是否单独存储并可从 _source 外检索。仅适用于根字段。 false   term_vector 布尔值，指定是否为该字段存储词向量。适用于根字段和 n-gram 子字段（默认为 no）。不适用于前缀子字段。 no    ","subcategory":null,"summary":"","tags":null,"title":"边输入边搜索字段类型（Search-as-you-type）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/autocomplete-field-type/search-as-you-type/"},{"category":null,"content":"_routing 元数据字段 #  Easysearch 使用哈希算法将文档路由到索引中的特定分片。默认情况下，文档的 _id 字段用作路由值，但您也可以为每个文档指定自定义路由值。\n默认路由 #  以下是 Easysearch 的默认路由公式。_routing 值是文档的 _id。\n相关指南（先读这些） #    映射基础  元数据字段  shard_num = hash(_routing) % num_primary_shards 自定义路由 #  您可以在索引文档时指定自定义路由值，如以下示例所示：\nPUT sample-index1/_doc/1?routing=JohnDoe1 { \u0026#34;title\u0026#34;: \u0026#34;This is a document\u0026#34; } 在此示例中，文档使用的路由值是 JohnDoe1 而不是默认的 _id 。\n在检索、删除或更新文档时，您必须提供相同的路由值，如以下示例所示：\nGET sample-index1/_doc/1?routing=JohnDoe1 通过路由查询 #  您可以使用 _routing 字段根据文档的路由值进行查询，如以下示例所示。此查询仅搜索与 JohnDoe1 路由值关联的分片：\nGET sample-index1/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;_routing\u0026#34;: [ \u0026#34;JohnDoe1\u0026#34; ] } } } 设置路由为必需项 #  您可以使索引上的所有 CRUD 操作都必需提供路由值，如以下示例。如果您尝试在不提供路由值的情况下索引文档，Easysearch 将抛出异常。\nPUT sample-index2 { \u0026#34;mappings\u0026#34;: { \u0026#34;_routing\u0026#34;: { \u0026#34;required\u0026#34;: true } } } 路由到特定分片组 #  您可以配置索引将自定义值路由到分片的子集，而不是单个分片。这是通过在创建索引时设置 index.routing_partition_size 来实现的。计算分片的公式是 shard_num = (hash(_routing) + hash(_id)) % routing_partition_size) % num_primary_shards。\n以下示例请求将文档路由到索引中的四个分片之一：\nPUT sample-index3 { \u0026#34;settings\u0026#34;: { \u0026#34;index.routing_partition_size\u0026#34;: 4 }, \u0026#34;mappings\u0026#34;: { \u0026#34;_routing\u0026#34;: { \u0026#34;required\u0026#34;: true } } } ","subcategory":null,"summary":"","tags":null,"title":"路由元数据字段（_routing）","url":"/easysearch/main/docs/features/mapping-and-analysis/metadata-field/routing/"},{"category":null,"content":"资源扩容 #  Easysearch Operator 支持对集群的 CPU、内存和磁盘资源进行在线扩容，扩容过程采用滚动更新方式，不影响集群可用性。\n查看当前资源 #  kubectl get sts/threenodes-masters -o yaml 示例输出（关键部分）：\nresources: requests: cpu: \u0026#34;1\u0026#34; memory: 3Gi limits: cpu: \u0026#34;1\u0026#34; memory: 5Gi # 存储 resources: requests: storage: 30Gi 修改资源配置 #\n 编辑 Operator YAML 文件，调整资源限制：\nresources: requests: cpu: \u0026#34;1\u0026#34; memory: 4Gi limits: cpu: \u0026#34;2\u0026#34; memory: 6Gi # 磁盘扩容 resources: requests: storage: 50Gi 应用修改：\nkubectl apply -f easysearch-cluster.yaml  注意：磁盘扩容依赖于 StorageClass 是否支持在线扩容（allowVolumeExpansion: true）。\n 滚动更新流程 #  Operator 会按顺序逐个更新节点：\n 从 threenodes-masters-0 开始更新 等待该节点完全就绪后，更新 threenodes-masters-1 依次滚动，直到所有节点更新完毕  更新完成后可验证资源是否生效：\nkubectl get sts/threenodes-masters -o yaml | grep -A5 resources 操作演示 #    autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  ","subcategory":null,"summary":"","tags":null,"title":"资源扩容","url":"/easysearch/main/docs/deployment/install-guide/operator/resource_manager/"},{"category":null,"content":"调试与 Explain #  当你觉得\u0026quot;这条结果不该排这么前/这么后\u0026quot;时，就需要用调试工具把 _score 拆开来看。本页给出一个通用的排查流程。\n典型症状 #   明显相关的文档排在很后面 噪声文档排在前几条 修改 Mapping 或查询结构后，排序结果变得难以解释  这些问题往往源于：字段类型/分析器不匹配、查询结构不合理、boost 失衡等。\n理解评分标准 #  当调试一条复杂的查询语句时，想要理解 _score 究竟是如何计算是比较困难的。Easysearch 在每个查询语句中都有一个 explain 参数，将 explain 设为 true 就可以得到更详细的信息。\nGET /_search?explain { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;tweet\u0026#34; : \u0026#34;honeymoon\u0026#34; }} } explain 参数可以让返回结果添加一个 _score 评分的得来依据。\n 注意：增加一个 explain 参数会为每个匹配到的文档产生一大堆额外内容，但是花时间去理解它是很有意义的。如果现在看不明白也没关系——等你需要的时候再来回顾这一节就行。\n 首先，我们看一下普通查询返回的元数据：\n{ \u0026#34;_index\u0026#34; : \u0026#34;us\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;12\u0026#34;, \u0026#34;_score\u0026#34; : 0.076713204, \u0026#34;_source\u0026#34; : { ... trimmed ... }, \u0026#34;_node\u0026#34; : \u0026#34;mzIVYCsqSWCG_M_ZffSs9Q\u0026#34;, 这里加入了该文档来自于哪个节点哪个分片上的信息，这对我们是比较有帮助的，因为词频率和文档频率是在每个分片中计算出来的，而不是每个索引中。\n然后它提供了 _explanation。每个入口都包含一个 description、value、details 字段，它分别告诉你计算的类型、计算结果和任何我们需要的计算细节。\n\u0026#34;_explanation\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;weight(tweet:honeymoon in 0) [PerFieldSimilarity], result of:\u0026#34;, \u0026#34;value\u0026#34;: 0.076713204, \u0026#34;details\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;fieldWeight in 0, product of:\u0026#34;, \u0026#34;value\u0026#34;: 0.076713204, \u0026#34;details\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;tf(freq=1.0), with freq of:\u0026#34;, \u0026#34;value\u0026#34;: 1, \u0026#34;details\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;termFreq=1.0\u0026#34;, \u0026#34;value\u0026#34;: 1 } ] }, { \u0026#34;description\u0026#34;: \u0026#34;idf(docFreq=1, maxDocs=1)\u0026#34;, \u0026#34;value\u0026#34;: 0.30685282 }, { \u0026#34;description\u0026#34;: \u0026#34;fieldNorm(doc=0)\u0026#34;, \u0026#34;value\u0026#34;: 0.25 } ] } ] }  honeymoon 相关性评分计算的总结 检索词频率：检索词 honeymoon 在这个文档的 tweet 字段中的出现次数 反向文档频率：检索词 honeymoon 在索引上所有文档的 tweet 字段中出现的次数 字段长度准则：在这个文档中，tweet 字段内容的长度——内容越长，值越小  复杂的查询语句解释也非常复杂，但是包含的内容与上面例子大致相同。通过这段信息我们可以了解搜索结果是如何产生的。\n 提示：JSON 形式的 explain 描述是难以阅读的，但是转成 YAML 会好很多，只需要在参数中加上 format=yaml。\n  警告：输出 explain 结果代价是十分昂贵的，它只能用作调试工具。千万不要用于生产环境。\n 理解文档是如何被匹配到的 #  当 explain 选项加到某一文档上时，explain API 会帮助你理解为何这个文档会被匹配，更重要的是，一个文档为何没有被匹配。\n请求路径为 /index/_doc/id/_explain，如下所示：\nGET /us/_doc/12/_explain { \u0026#34;query\u0026#34; : { \u0026#34;bool\u0026#34; : { \u0026#34;filter\u0026#34; : { \u0026#34;term\u0026#34; : { \u0026#34;user_id\u0026#34; : 2 }}, \u0026#34;must\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;tweet\u0026#34; : \u0026#34;honeymoon\u0026#34; }} } } } 这个 API 会返回一个 explanation，说明为什么这个文档匹配或不匹配。如果文档不匹配，explanation 会说明原因。\n理解查询语句 #  对于合法查询，使用 explain 参数将返回可读的描述，这对准确理解 Easysearch 是如何解析你的 query 是非常有用的：\nGET /_validate/query?explain { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;tweet\u0026#34; : \u0026#34;really powerful\u0026#34; } } } 我们查询的每一个 index 都会返回对应的 explanation，因为每一个 index 都有自己的映射和分析器：\n{ \u0026#34;valid\u0026#34; : true, \u0026#34;_shards\u0026#34; : { ... }, \u0026#34;explanations\u0026#34; : [ { \u0026#34;index\u0026#34; : \u0026#34;us\u0026#34;, \u0026#34;valid\u0026#34; : true, \u0026#34;explanation\u0026#34; : \u0026#34;tweet:really tweet:powerful\u0026#34; }, { \u0026#34;index\u0026#34; : \u0026#34;gb\u0026#34;, \u0026#34;valid\u0026#34; : true, \u0026#34;explanation\u0026#34; : \u0026#34;tweet:realli tweet:power\u0026#34; } ] } 从 explanation 中可以看出，匹配 really powerful 的 match 查询被重写为两个针对 tweet 字段的 single-term 查询，一个 single-term 查询对应查询字符串分出来的一个 term。\n当然，对于索引 us，这两个 term 分别是 really 和 powerful，而对于索引 gb，term 则分别是 realli 和 power。之所以出现这个情况，是由于我们将索引 gb 中 tweet 字段的分析器修改为 english 分析器。\n用 Explain 看清得分构成 #  Explain 能展示某个文档的 _score 由哪些部分组成，例如：\n 哪些子查询命中了 每个子查询分别贡献了多少分 字段长度、词频、逆文档频率等因素的影响  排查思路：\n 选出\u0026quot;排序异常\u0026quot;的代表文档（一个排得太低，一个排得太高） 对它们运行 explain，查看每个命中子句的得分构成 对比两个文档之间的差异：是某个字段权重过高？某个词的影响被放大或忽略？  这能帮助你回答：是评分机制本身的问题，还是 Mapping/查询结构的问题？\n用 Profile 看执行结构与耗时 #  Profile 工具偏向\u0026quot;执行计划与性能\u0026quot;：它会告诉你：\n 每个子查询执行了多久 哪些部分最耗时  GET /my_index/_search { \u0026#34;profile\u0026#34;: true, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;搜索引擎\u0026#34; } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;published\u0026#34; } } ] } } } 返回结果中的 profile 字段将包含每个分片上各查询组件的耗时明细（单位：纳秒），帮助你定位性能瓶颈。 在相关性调试中，它可以帮助你：\n 发现那些几乎不贡献结果却非常耗时的子句 确认是否有不必要的嵌套、重复计算  通常流程是：先用慢查询日志找到\u0026quot;慢\u0026quot;，再用 Profile 找出\u0026quot;为什么慢\u0026quot;，然后再结合 Explain 调整结构。\n慢查询日志（Slowlog） #  慢查询日志是发现问题的入口之一：\n 记录超过阈值的查询请求 帮你锁定最值得优化的那一小部分查询  通过索引设置配置慢查询日志阈值：\nPUT /my_index/_settings { \u0026#34;index.search.slowlog.threshold.query.warn\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.info\u0026#34;: \u0026#34;5s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.debug\u0026#34;: \u0026#34;2s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.trace\u0026#34;: \u0026#34;500ms\u0026#34;, \u0026#34;index.search.slowlog.threshold.fetch.warn\u0026#34;: \u0026#34;1s\u0026#34;, \u0026#34;index.search.slowlog.threshold.fetch.info\u0026#34;: \u0026#34;800ms\u0026#34;, \u0026#34;index.search.slowlog.threshold.fetch.debug\u0026#34;: \u0026#34;500ms\u0026#34;, \u0026#34;index.search.slowlog.threshold.fetch.trace\u0026#34;: \u0026#34;200ms\u0026#34; }  慢查询日志会记录到 Easysearch 的日志文件中，可通过日志配置文件控制输出位置和格式。\n 建议：\n 为重要索引配置合理的慢查询阈值（读写分别设置） 定期回顾慢查询日志，筛选出\u0026quot;又慢又频繁\u0026quot;的查询模式，优先优化  推荐调试流程 #  一个可重复使用的调试流程：\n 发现问题：通过用户反馈、监控或慢查询日志，锁定有问题的查询和索引 构造样本：收集少量具有代表性的请求和预期结果 Explain：对正常/异常结果分别执行 explain，比对 _score 构成 Profile：在性能相关问题时，用 profile 分析执行时间和结构 调整：根据分析结果修改：  Mapping（字段类型、分析器、multi-fields 设计） 查询结构（must/should/filter、子句拆分/合并） Boost 策略（字段/子句权重、function_score 等）   验证与回归：在测试环境或小流量下验证改动效果，避免引入新的问题  通过这样的过程，可以把\u0026quot;感觉不对\u0026quot;的相关性问题，转化为可解释、可复现、可回滚的技术改动。\n调试相关度是最后 10% 要做的事情 #  理解评分过程是非常重要的，这样就可以根据具体的业务对评分结果进行调试、调节、减弱和定制。\n实践中，简单的查询组合就能提供很好的搜索结果，但是为了获得具有成效的搜索结果，就必须反复推敲修改前面介绍的这些调试方法。\n通常，经过对策略字段应用权重提升，或通过对查询语句结构的调整来强调某个句子的重要性这些方法，就足以获得良好的结果。有时，如果 Easysearch 基于词的 TF/IDF 模型不再满足评分需求（例如希望基于时间或距离来评分），则需要更具侵略性的调整。\n除此之外，相关度的调试就有如兔子洞，一旦跳进去就很难再出来。最相关这个概念是一个难以触及的模糊目标，通常不同人对文档排序又有着不同的想法，这很容易使人陷入持续反复调整而没有明显进展的怪圈。\n我们强烈建议不要陷入这种怪圈，而要监控测量搜索结果。监控用户点击最顶端结果的频次，这可以是前 10 个文档，也可以是第一页的；用户不查看首次搜索的结果而直接执行第二次查询的频次；用户来回点击并查看搜索结果的频次，等等诸如此类的信息。\n这些都是用来评价搜索结果与用户之间相关程度的指标。如果查询能返回高相关的文档，用户会选择前五中的一个，得到想要的结果，然后离开。不相关的结果会让用户来回点击并尝试新的搜索条件。\n一旦有了这些监控手段，想要调试查询就并不复杂，稍作调整，监控用户的行为改变并做适当反复尝试。本章介绍的一些工具就只是工具而已，要想物尽其用并将搜索结果提高到极高的水平，唯一途径就是需要具备能评价度量用户行为的强大能力。\n小结 #   explain API 可以帮助理解文档的评分构成和匹配原因 validate API 可以帮助理解查询是如何被解析的 Profile 工具可以帮助分析查询的性能问题 慢查询日志是发现问题的入口 监控用户行为是评价搜索结果质量的关键  下一步可以继续阅读：\n  相关性常用策略  评分基础  加权与调参  ","subcategory":null,"summary":"","tags":null,"title":"调试与 Explain","url":"/easysearch/main/docs/features/fulltext-search/relevance/debug-and-explain/"},{"category":null,"content":"词干提取配置参考 #  本页提供词干提取（Stemming）相关组件的技术配置参考。关于词干提取的概念、策略选择和最佳实践，请先阅读指南。\n相关指南（概念与策略） #    文本分析：词干提取 - 词干提取的收益、风险与控制方式  文本分析基础 - 分析器原理   概述 #  词干提取是将单词还原为其词根或基本形式（即词干）的过程。这项技术可确保在搜索操作中，单词的不同变体都能匹配到相应结果。例如，单词 \u0026ldquo;running\u0026rdquo;（跑步，现在分词形式）、\u0026ldquo;runner\u0026rdquo;（跑步者，名词形式）和 \u0026ldquo;ran\u0026rdquo;（跑步，过去式）都可以还原为词干 \u0026ldquo;run\u0026rdquo;（跑步，原形），这样一来，搜索这些词中的任何一个都能返回相关结果。\n在自然语言中，由于动词变位、名词复数变化或词的派生等原因，单词常常以各种形式出现。词干提取在以下方面提升了搜索操作的效果：\n 提高搜索召回率：通过将不同的单词形式匹配到同一个词干，词干提取增加了检索到的相关文档的数量 减小索引大小：仅存储单词的词干形式可以减少搜索索引的总体大小  词干提取是通过在分析器中使用词元过滤器来配置的。一个分析器包含以下组件：\n 字符过滤器：在分词之前修改字符流 分词器：将文本拆分为词元（通常是单词） 词元过滤器：在分词之后修改词元，例如，应用词干提取操作  使用内置词元过滤器进行词干提取的示例 #  要实现词干提取，你可以配置一个内置的词元过滤器，比如 porter_stem 或 kstem 过滤器。\n波特词干提取算法（ Porter stemming algorithm）是一种常用于英语的词干提取算法。\n创建带有自定义分词器的索引 #  以下示例请求创建了一个名为 my_stemming_index 的新索引，并配置了一个使用 porter_stem 词元过滤器的分词器：\nPUT /my_stemming_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_stemmer_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;porter_stem\u0026#34; ] } } } } } 此配置包含以下内容：\n 标准分词器：根据单词边界将文本拆分为词项。 小写字母过滤器：将所有词元转换为小写形式。 波特词干过滤器（porter_stem 过滤器）：将单词还原为它们的词根形式。  测试分词器 #  为了检验词干提取的效果，使用之前配置好的自定义分词器来分析一段示例文本：\nPOST /my_stemming_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_stemmer_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The runners are running swiftly.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;the\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;runner\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;ar\u0026#34;, \u0026#34;start_offset\u0026#34;: 12, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;start_offset\u0026#34;: 16, \u0026#34;end_offset\u0026#34;: 23, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;swiftli\u0026#34;, \u0026#34;start_offset\u0026#34;: 24, \u0026#34;end_offset\u0026#34;: 31, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } 词干提取器的类别 #  你可以配置属于以下两类的词干提取器：\n 算法词干提取器 字典词干提取器  算法词干提取器 #  算法词干提取器运用预先设定的规则，系统地去除单词的词缀（前缀和后缀），将单词还原为词干。以下这些词元过滤器使用了算法词干提取器：\n porter_stem：应用波特词干提取算法，去除常见的后缀，把单词还原为词干。例如，“running”（跑步，现在分词）会变成“run”（跑步，原形）。 kstem：这是一款专为英语设计的轻量级词干提取器，它将算法词干提取与内置字典相结合。它能把复数形式还原为单数形式，将动词的各种时态转换为原形，并去除常见的派生词尾。 stemmer：为包括英语在内的多种语言提供算法词干提取功能，有像 light_english（轻型英语词干提取算法）、minimal_english（最简英语词干提取算法）和 porter2 等不同词干提取算法可供选择。 snowball：应用雪球算法，为包括英语、法语、德语等在内的多种语言提供高效且准确的词干提取服务。  字典词干提取器 #  字典词干提取器依赖于庞大的字典，将单词映射到它们的词根形式，有效地对不规则单词进行词干提取。它们会在预先编译好的列表中查找每个单词，以找到其对应的词干。这种操作对资源的消耗更大，但对于不规则单词以及那些看似词干相似但含义差异很大的单词，往往能得出更好的词干提取结果。\n字典词干提取器中最具代表性的例子是 hunspell 词元过滤器，它使用 Hunspell——一个在许多开源应用程序中都有使用的拼写检查引擎。\n注意事项 #  在选择词干提取器时，请注意以下几点：\n 当处理速度和内存效率是优先考虑的因素，并且所处理的语言具有相对规则的词形变化模式时，算法词干提取器是合适的选择。 当处理不规则单词形式时的准确性至关重要，并且有足够的资源来支持增加的内存使用和处理时间时，字典词干提取器是理想的选择。  额外的词干提取配置 #  尽管 “organize”（组织）和 “organic”（有机的）有共同的语言词根，这会导致词干提取器将它们都处理成 “organ”（器官；机构），但它们在概念上的差异是很大的。在实际的搜索场景中，这种共同的词根可能会导致在搜索结果中返回不相关的匹配项。\n你可以通过以下方法来应对这些挑战：\n 显式的词干提取覆盖：不必仅仅依赖算法词干提取，你可以定义特定的词干提取规则。使用 stemmer_override（词干提取覆盖），你可以确保 \u0026ldquo;organize\u0026rdquo; 保持不变，而 \u0026ldquo;organic\u0026rdquo; 被还原为 \u0026ldquo;organ\u0026rdquo;。这提供了对词项最终形式的精细控制。 关键词保留：为了保持重要词项的完整性，你可以使用 keyword_marker（关键词标记）词元过滤器。这个过滤器将特定的单词指定为关键词，防止后续的词干提取器过滤器对它们进行修改。在这个例子中，你可以将 \u0026ldquo;organize\u0026rdquo; 标记为关键词，确保它按照其原本的形式进行索引。 条件词干提取控制： condition（条件）词元过滤器使你能够建立规则来确定一个词项是否应该进行词干提取。这些规则可以基于各种标准，例如词项是否出现在预定义的列表中。 特定语言的词项排除：对于内置的语言分词器，stem_exclusion（词干提取排除）参数提供了一种指定应免于词干提取的单词的方法。例如，你可以将 \u0026ldquo;organize\u0026rdquo; 添加到 stem_exclusion 列表中，防止分词器对其进行词干提取。这对于在特定语言中保留特定词项的独特含义可能很有用。   词干提取相关词元过滤器 #  算法词干提取器 #     过滤器 说明      stemmer 通用词干提取器，支持多种语言和算法    snowball 基于 Snowball 算法的词干提取器    porter_stem Porter 英语词干提取算法    kstem 英语轻量级词干提取器（算法+字典）    语言专用词干提取器 #     过滤器 语言      arabic_stem 阿拉伯语    brazilian_stem 巴西葡萄牙语    czech_stem 捷克语    dutch_stem 荷兰语    french_stem 法语    german_stem 德语    romanian_stem 罗马尼亚语    swedish_stem 瑞典语    turkish_stem 土耳其语    词干提取控制 #     过滤器 说明      stemmer_override 自定义词干提取规则，覆盖默认行为    keyword_marker 将指定词标记为关键词，阻止词干提取    keyword_repeat 同时保留原词和词干形式    condition 按条件决定是否应用词干提取    ","subcategory":null,"summary":"","tags":null,"title":"词干提取（Stemming）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/stemming/"},{"category":null,"content":"Term Vector 参数 #  term_vector 参数控制是否为字段存储词条向量（Term Vector）信息。词条向量包含词条及其位置、偏移量等信息，供高亮和 More Like This 查询使用。\n相关指南 #    全文搜索  analyzer 参数  可选值 #     值 说明     no 默认值。不存储词条向量。   yes 仅存储词条，不含位置和偏移量。   with_positions 存储词条和位置信息。   with_offsets 存储词条和字符偏移量。   with_positions_offsets 存储词条、位置和偏移量。推荐用于快速高亮。   with_positions_payloads 存储词条、位置和有效载荷。   with_positions_offsets_payloads 存储所有信息。    示例 #  为高亮优化 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;with_positions_offsets\u0026#34; } } } } 使用 with_positions_offsets 可以让 fast vector highlighter（FVH）高亮器直接从词条向量中提取数据，而无需重新分析文档，显著提高高亮性能。\n何时启用 #     场景 建议     需要快速高亮大文本字段 使用 with_positions_offsets   使用 More Like This 查询 使用 yes 或更高级别   不需要高亮 保持默认 no    注意事项 #   存储词条向量会显著增加索引大小，通常增加 50%–100% 仅对 text 类型字段有意义 如果只需要普通高亮（unified 或 plain 高亮器），不必启用词条向量 对于 More Like This 查询，如果不存储词条向量，系统会实时分析文本，速度较慢但节省空间  ","subcategory":null,"summary":"","tags":null,"title":"词向量参数（Term Vector）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/term_vector/"},{"category":null,"content":"Norms 参数 #  norms 参数控制是否存储字段长度归一化因子，用于相关性评分计算。\n在 BM25 评分算法中，字段长度是一个重要因素：短字段中的匹配通常比长字段中的匹配更相关。norms 存储的就是这个字段长度信息。\n相关指南（先读这些） #    映射基础  评分基础  参数选项 #     字段类型 默认值 说明     text true 默认启用，用于全文搜索评分   keyword false 默认禁用，keyword 通常用于过滤/聚合    示例 #  禁用 text 字段的 norms：\nPUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;tags\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;norms\u0026#34;: false } } } } 何时禁用 norms #     场景 建议     字段用于全文搜索，需要相关性排序 保持启用   字段仅用于过滤（filter context） 可禁用，节省空间   字段是日志/标签等，不关心评分 可禁用   多个字段的评分权重由 boost 控制 保持启用    注意事项 #   禁用 norms 可以节省磁盘空间（每个文档每个字段约 1 byte） norms 一旦禁用后无法重新启用，需要重建索引 对于不需要评分的字段（如纯过滤用途），禁用 norms 是安全的优化  ","subcategory":null,"summary":"","tags":null,"title":"评分规范参数（Norms）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/norms/"},{"category":null,"content":"Normalizer 参数 #  normalizer 参数用于 keyword 字段，在索引和查询之前对值进行标准化处理（如转换为小写）。与 analyzer 不同，normalizer 不会对字符串进行分词，只做字符级别的变换。\n 完整指南 → 归一化与规范化器，包含概念介绍、自定义配置、兼容过滤器列表和最佳实践。\n 参数选项 #     值 说明     null 不使用 normalizer，值按原样索引。默认值。   自定义名称 使用在索引 settings 中定义的 normalizer。    使用示例 #  keyword 字段默认区分大小写。使用内置的 lowercase normalizer 可忽略大小写：\n\u0026#34;status\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;lowercase\u0026#34; } 或自定义 normalizer：\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;normalizer\u0026#34;: { \u0026#34;lowercase_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;status\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;lowercase_normalizer\u0026#34; } } } } 写入 \u0026quot;OK\u0026quot; 后，使用 \u0026quot;ok\u0026quot; 可以查到，因为两者在索引时都被标准化为 \u0026quot;ok\u0026quot;。\n注意事项 #   normalizer 同时影响索引时和查询时，确保两端一致 聚合和排序使用的也是标准化后的值 如果只需要查询时忽略大小写而保留原始值用于展示，可以使用多字段：一个带 normalizer，一个不带  相关参考 #    归一化与规范化器（概念指南）  规范化器配置参考  Keyword 字段类型  ","subcategory":null,"summary":"","tags":null,"title":"规范化器参数（Normalizer）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/normalizer/"},{"category":null,"content":"Range 字段类型 #  以下表格列出了 Easysearch 支持的所有范围字段类型。\n   字段数据类型 描述     integer_range 整数值范围。   long_range 长整型值范围。   double_range 双精度浮点值范围。   float_range 浮点值范围。   ip_range IPv4 或 IPv6 地址范围，起始和结束地址可使用不同格式。   date_range 日期值范围，起始和结束日期可采用不同格式。内部以 64 位无符号整数存储，自纪元以来的毫秒数表示。    相关指南（先读这些） #    映射基础  结构化搜索  参考代码 #  创建一个有双精度浮点数范围字段和日期范围字段的映射\nPUT testindex { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;gpa\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;double_range\u0026#34; }, \u0026#34;graduation_date\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;date_range\u0026#34;, \u0026#34;format\u0026#34; : \u0026#34;strict_year_month||strict_year_month_day\u0026#34; } } } } 索引一个包含这两个字段的文档\nPUT testindex/_doc/1 { \u0026#34;gpa\u0026#34; : { \u0026#34;gte\u0026#34; : 1.0, \u0026#34;lte\u0026#34; : 4.0 }, \u0026#34;graduation_date\u0026#34; : { \u0026#34;gte\u0026#34; : \u0026#34;2019-05-01\u0026#34;, \u0026#34;lte\u0026#34; : \u0026#34;2019-05-15\u0026#34; } } IP 地址范围字段 #  您可以使用两种格式指定 IP 地址范围字段：范围表示法和 CIDR 表示法。\n创建一个有 IP 地址范围字段两个格式的映射\nPUT testindex { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;ip_address_range\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;ip_range\u0026#34; }, \u0026#34;ip_address_cidr\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;ip_range\u0026#34; } } } } 索引一个包含这两个 IP 地址范围字段格式的文档\nPUT testindex/_doc/2 { \u0026#34;ip_address_range\u0026#34; : { \u0026#34;gte\u0026#34; : \u0026#34;10.24.34.0\u0026#34;, \u0026#34;lte\u0026#34; : \u0026#34;10.24.35.255\u0026#34; }, \u0026#34;ip_address_cidr\u0026#34; : \u0026#34;10.24.34.0/24\u0026#34; } 查询范围字段 #  您可以使用 Term 查询 或 Range 查询对范围字段进行搜索。\nTerm 查询 #  Term 查询可以使用一个具体值匹配到所有符合条件的范围字段。\n例如，以下查询将返回文档 1，因为 3.5 位于文档 1 的范围 [1.0, 4.0] 之内。\nGET testindex/_search { \u0026#34;query\u0026#34; : { \u0026#34;term\u0026#34; : { \u0026#34;gpa\u0026#34; : { \u0026#34;value\u0026#34; : 3.5 } } } } 范围查询 #  对范围字段的范围查询会返回位于指定范围内的文档。\n例如，可以查询 2019 年的所有毕业日期，并以“MM/dd/yyyy”格式提供日期范围。\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;graduation_date\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;01/01/2019\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;12/31/2019\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;MM/dd/yyyy\u0026#34;, \u0026#34;relation\u0026#34; : \u0026#34;within\u0026#34; } } } } 上面的查询在“within”和“intersects”关系中会返回文档 1，但在“contains”关系中不会返回它。\n参数详解 #  下面列出了范围字段的设置参数，所有参数都是可选的\n   参数 描述 默认值     boost 浮点值，指定字段对相关性评分的权重，大于 1.0 提高相关性，0.0 到 1.0 降低相关性。 1.0   coerce 布尔值，允许将小数截断为整数值，并将字符串转换为数值类型。 true   index 布尔值，指定字段是否可被搜索。 true   store 布尔值，指定字段值是否单独存储并可从 _source 外检索。 false    ","subcategory":null,"summary":"","tags":null,"title":"范围字段类型（Range）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/range-field-type/"},{"category":null,"content":"Completion 自动补全字段类型 #  自动补全字段类型通过补全建议器提供自动补全功能。补全建议器是一个前缀建议器，所以它只匹配文本的开头部分。补全建议器会创建一个内存中的数据结构，这提供了更快的查找速度，但会导致内存使用增加。在使用此功能之前，你需要将所有可能的补全项上传到索引中。\n代码样例 #  创建一个包含补全字段的映射：\nPUT chess_store { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;suggestions\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;completion\u0026#34; }, \u0026#34;product\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } 将建议内容索引到 Easysearch 中：\nPUT chess_store/_doc/1 { \u0026#34;suggestions\u0026#34;: { \u0026#34;input\u0026#34;: [\u0026#34;Books on openings\u0026#34;, \u0026#34;Books on endgames\u0026#34;], \u0026#34;weight\u0026#34; : 10 } } 参数 #  下表列出了补全字段接受的参数。\n   参数 描述     input 可能的补全项列表，可以是字符串或字符串数组。不能包含 \\u0000 (null),\\u001f (信息分隔符一) 或 \\u001e (信息分隔符二)。必需。   weight 用于对建议进行排序的正整数或正整数字符串。可选。    可以按以下方式索引多个建议：\nPUT chess_store/_doc/2 { \u0026#34;suggestions\u0026#34;: [ { \u0026#34;input\u0026#34;: \u0026#34;Chess set\u0026#34;, \u0026#34;weight\u0026#34;: 20 }, { \u0026#34;input\u0026#34;: \u0026#34;Chess pieces\u0026#34;, \u0026#34;weight\u0026#34;: 10 }, { \u0026#34;input\u0026#34;: \u0026#34;Chess board\u0026#34;, \u0026#34;weight\u0026#34;: 5 } ] } 或者，你可以使用以下简写方式（注意在这种表示法中不能提供 weight 参数）：\nPUT chess_store/_doc/3 { \u0026#34;suggestions\u0026#34; : [ \u0026#34;Chess clock\u0026#34;, \u0026#34;Chess timer\u0026#34; ] } 查询补全字段类型 #  要查询补全字段类型，需要指定要搜索的前缀和要查找建议的字段名称。\n查询以单词 \u0026ldquo;chess\u0026rdquo; 开头的内容建议：\nGET chess_store/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;product-suggestions\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;chess\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggestions\u0026#34; } } } } 返回内容包含自动补全建议：\n{ \u0026#34;took\u0026#34; : 3, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 0, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : null, \u0026#34;hits\u0026#34; : [ ] }, \u0026#34;suggest\u0026#34; : { \u0026#34;product-suggestions\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;chess\u0026#34;, \u0026#34;offset\u0026#34; : 0, \u0026#34;length\u0026#34; : 5, \u0026#34;options\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;Chess set\u0026#34;, \u0026#34;_index\u0026#34; : \u0026#34;chess_store\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34; : 20.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : [ { \u0026#34;input\u0026#34; : \u0026#34;Chess set\u0026#34;, \u0026#34;weight\u0026#34; : 20 }, { \u0026#34;input\u0026#34; : \u0026#34;Chess pieces\u0026#34;, \u0026#34;weight\u0026#34; : 10 }, { \u0026#34;input\u0026#34; : \u0026#34;Chess board\u0026#34;, \u0026#34;weight\u0026#34; : 5 } ] } }, { \u0026#34;text\u0026#34; : \u0026#34;Chess clock\u0026#34;, \u0026#34;_index\u0026#34; : \u0026#34;chess_store\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : [ \u0026#34;Chess clock\u0026#34;, \u0026#34;Chess timer\u0026#34; ] } } ] } ] } } 在返回内容中，_score 字段包含了在索引时设置的 weight 参数值。text 字段填充了建议的 input 参数。\n默认情况下，响应包含整个文档，包括 _source 字段，这可能会影响性能。要只返回 suggestions 字段，你可以在 _source 参数中指定。你还可以通过指定 size 参数来限制返回的建议数量。\nGET chess_store/_search { \u0026#34;_source\u0026#34;: \u0026#34;suggestions\u0026#34;, \u0026#34;suggest\u0026#34;: { \u0026#34;product-suggestions\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;chess\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggestions\u0026#34;, \u0026#34;size\u0026#34; : 3 } } } } 返回内容会包含建议内容：\n{ \u0026#34;took\u0026#34; : 5, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 0, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : null, \u0026#34;hits\u0026#34; : [ ] }, \u0026#34;suggest\u0026#34; : { \u0026#34;product-suggestions\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;chess\u0026#34;, \u0026#34;offset\u0026#34; : 0, \u0026#34;length\u0026#34; : 5, \u0026#34;options\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;Chess set\u0026#34;, \u0026#34;_index\u0026#34; : \u0026#34;chess_store\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34; : 20.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : [ { \u0026#34;input\u0026#34; : \u0026#34;Chess set\u0026#34;, \u0026#34;weight\u0026#34; : 20 }, { \u0026#34;input\u0026#34; : \u0026#34;Chess pieces\u0026#34;, \u0026#34;weight\u0026#34; : 10 }, { \u0026#34;input\u0026#34; : \u0026#34;Chess board\u0026#34;, \u0026#34;weight\u0026#34; : 5 } ] } }, { \u0026#34;text\u0026#34; : \u0026#34;Chess clock\u0026#34;, \u0026#34;_index\u0026#34; : \u0026#34;chess_store\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : [ \u0026#34;Chess clock\u0026#34;, \u0026#34;Chess timer\u0026#34; ] } } ] } ] } }  要利用源内容过滤功能，请在 _search API 上使用建议功能。_suggest API 不支持源内容过滤。\n 自动补全查询参数 #  下表列出了自动补全查询接受的参数。\n   参数 描述     field 一个字符串，指定要运行查询的字段。必需。   size 一个整数，指定返回的建议的最大数量。可选。默认值为 5。   skip_duplicates 一个布尔值，指定是否跳过重复的建议。可选。默认值为 false。    自动补全的模糊查询 #  要允许模糊匹配，可以为补全查询指定 fuzziness 参数。在这种情况下，即使用户输入错误的搜索词，自动补全查询仍然会返回结果。此外，匹配查询的前缀越长，文档的得分越高。\nGET chess_store/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;product-suggestions\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;chesc\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggestions\u0026#34;, \u0026#34;size\u0026#34; : 3, \u0026#34;fuzzy\u0026#34; : { \u0026#34;fuzziness\u0026#34; : \u0026#34;AUTO\u0026#34; } } } } }  要使用所有默认的模糊选项，可以指定 \u0026quot;fuzzy\u0026quot;: {} 或 \u0026quot;fuzzy\u0026quot;: true。\n 下表列出了自动补全的模糊查询接受的参数。所有参数都是可选的。\n   参数 描述     fuzziness 模糊度可以设置为以下之一：1. 一个整数，指定允许的最大 Levenshtein 距离。2. AUTO：0-2 个字符的字符串必须完全匹配，3-5 个字符的字符串允许 1 次编辑，超过 5 个字符的字符串允许 2 次编辑。默认值为 AUTO。   min_length 一个整数，指定输入的最小长度，以开始返回建议。如果搜索词的长度小于 min_length，则不返回建议。默认值为 3。   prefix_length 一个整数，指定匹配的前缀的最小长度，以开始返回建议。如果 prefix_length 的前缀不匹配，但搜索词仍然在 Levenshtein 距离内，则不返回建议。默认值为 1。   transpositions 一个布尔值，指定将相邻字符的交换（transpositions）计为一次编辑，而不是两次编辑。示例：建议的 input 参数是 abcde，fuzziness 是 1。如果 transpositions 设置为 true，则 abdce 匹配，但如果 transpositions 设置为 false，则 abdce 不匹配。默认值为 true。   unicode_aware 一个布尔值，指定是否使用 Unicode 代码点来测量编辑距离、转置和长度。如果 unicode_aware 设置为 true，则测量速度较慢。默认值为 false，在这种情况下，距离以字节为单位测量。    正则表达式查询 #  可以使用正则表达式来定义自动补全查询的前缀。\n例如，要搜索以 \u0026ldquo;a\u0026rdquo; 开头并且后面有 \u0026ldquo;d\u0026rdquo; 的字符串，可以使用以下查询：\nGET chess_store/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;product-suggestions\u0026#34;: { \u0026#34;regex\u0026#34;: \u0026#34;a.*d\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggestions\u0026#34; } } } } 返回内容匹配字符串 \u0026ldquo;abcde\u0026rdquo;：\n{ \u0026#34;took\u0026#34; : 2, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 0, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : null, \u0026#34;hits\u0026#34; : [ ] }, \u0026#34;suggest\u0026#34; : { \u0026#34;product-suggestions\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;a.*d\u0026#34;, \u0026#34;offset\u0026#34; : 0, \u0026#34;length\u0026#34; : 4, \u0026#34;options\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;abcde\u0026#34;, \u0026#34;_index\u0026#34; : \u0026#34;chess_store\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34; : 20.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : [ { \u0026#34;input\u0026#34; : \u0026#34;abcde\u0026#34;, \u0026#34;weight\u0026#34; : 20 } ] } } ] } ] } } ","subcategory":null,"summary":"","tags":null,"title":"自动补全字段类型（Completion）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/autocomplete-field-type/completion/"},{"category":null,"content":"网络配置 #  本页介绍 easysearch.yml 中与网络绑定、端口和 HTTP 行为相关的配置项。这些都是静态设置，修改后需要重启节点生效。\n 核心网络参数 #  network.host #  network.host: 0.0.0.0    项目 说明     参数 network.host   默认值 _local_（仅本机 127.0.0.1）   属性 静态   说明 节点绑定的主机名或 IP 地址。同时设置 network.bind_host 和 network.publish_host。这是最常修改的网络配置    特殊值：\n   值 含义     _local_ 回环地址（127.0.0.1），仅本机可访问   _site_ 内网地址（如 192.168.x.x / 10.x.x.x）   _global_ 公网地址   0.0.0.0 绑定所有网卡接口   具体 IP 绑定到指定 IP 地址     重要：一旦将 network.host 设为非 _local_ 的值，Easysearch 将进入\u0026quot;生产模式\u0026quot;，执行更严格的启动检查（bootstrap checks）。\n network.bind_host #  network.bind_host: 0.0.0.0    项目 说明     参数 network.bind_host   默认值 随 network.host   属性 静态   说明 节点监听传入请求的具体地址。一般情况下只需设置 network.host，仅在需要区分绑定和通告地址时使用    network.publish_host #  network.publish_host: 192.168.1.10    项目 说明     参数 network.publish_host   默认值 随 network.host   属性 静态   说明 节点向集群中其他节点通告的地址。在多网卡环境中，应显式设置为其他节点可达的内网 IP    何时需要单独设置：\n 服务器有多个网卡（如内网 + 外网） 运行在 Docker 中需要通告宿主机 IP NAT 环境下需要通告映射后的地址  示例（多网卡）：\nnetwork.bind_host: 0.0.0.0 # 监听所有接口 network.publish_host: 192.168.1.10 # 通告内网地址  HTTP 端口 #  http.port #  http.port: 9200    项目 说明     参数 http.port   默认值 9200-9300   属性 静态   说明 HTTP REST API 监听端口。客户端通过此端口与 Easysearch 交互。支持单个值或端口范围    端口范围行为：指定范围时（如 9200-9300），节点将绑定到该范围内的第一个可用端口。生产环境建议设置为固定端口。\ntransport.port #  transport.port: 9300    项目 说明     参数 transport.port   默认值 9300-9400   属性 静态   说明 节点间内部通信（Transport 层）端口。用于集群内节点间的数据传输、分片恢复、集群状态同步等。客户端不会直接连接此端口     注意：在所有具有 master 资格的节点上，请将 transport.port 设置为单个固定值，避免因端口漂移导致发现失败。\n  HTTP 调优参数 #  请求大小限制 #     参数 默认值 说明     http.max_content_length 100mb HTTP 请求体最大大小。超过此大小的请求将被拒绝。如果有大量 bulk 写入需求，可适当调大   http.max_header_size 8kb HTTP 请求头（header）最大大小   http.max_initial_line_length 4kb HTTP 初始行（URL）最大长度。如果使用超长查询字符串，可能需要调大    示例（大 bulk 写入场景）：\nhttp.max_content_length: 200mb 压缩 #     参数 默认值 说明     http.compression true 是否启用 HTTP 响应压缩（gzip）。启用可以减少网络传输量，但会增加少量 CPU 开销    CORS（跨域资源共享） #     参数 默认值 说明     http.cors.enabled false 是否启用 CORS。仅在浏览器前端直接访问 Easysearch API 时需要开启   http.cors.allow-origin — 允许的 CORS 来源。支持正则表达式。\u0026quot;*\u0026quot; 表示允许所有来源（不推荐用于生产环境）   http.cors.allow-methods OPTIONS, HEAD, GET, POST, PUT, DELETE 允许的 HTTP 方法   http.cors.allow-headers X-Requested-With, Content-Type, Content-Length 允许的自定义请求头   http.cors.allow-credentials false 是否允许发送凭据（cookies）   http.cors.max-age 1728000（20 天） 预检请求（preflight）的缓存时间（秒）    示例（允许特定域名）：\nhttp.cors.enabled: true http.cors.allow-origin: \u0026#34;/https?://localhost(:[0-9]+)?/\u0026#34; 示例（允许所有来源，仅用于开发）：\nhttp.cors.enabled: true http.cors.allow-origin: \u0026#34;*\u0026#34; http.cors.allow-headers: \u0026#34;X-Requested-With, Content-Type, Content-Length, Authorization\u0026#34;  网络参数速查表 #     参数 默认值 属性 说明 生产环境建议     network.host _local_ 静态 绑定地址 设为具体内网 IP   network.bind_host 随 network.host 静态 监听地址 多网卡时配合使用   network.publish_host 随 network.host 静态 通告地址 多网卡/Docker 需显式设置   http.port 9200-9300 静态 HTTP 端口 设为固定值 9200   transport.port 9300-9400 静态 节点通信端口 设为固定值 9300   http.max_content_length 100mb 静态 请求体上限 大 bulk 场景可调至 200mb   http.compression true 静态 HTTP 压缩 保持默认   http.cors.enabled false 静态 CORS 开关 按需开启     配置示例 #  生产集群（单网卡） #  network.host: 192.168.1.10 http.port: 9200 transport.port: 9300 生产集群（多网卡 / Docker） #  network.bind_host: 0.0.0.0 network.publish_host: 192.168.1.10 http.port: 9200 transport.port: 9300 开发环境（本机访问） #  network.host: 0.0.0.0 http.port: 9200 transport.port: 9300 http.cors.enabled: true http.cors.allow-origin: \u0026#34;*\u0026#34;  延伸阅读 #    集群发现 — 节点间如何发现彼此  安全配置 — HTTPS（TLS）配置  系统调优 — TCP 网络内核参数  ","subcategory":null,"summary":"","tags":null,"title":"网络配置","url":"/easysearch/main/docs/deployment/config/node-settings/network/"},{"category":null,"content":"Date nanoseconds 字段类型 #  日期纳秒字段类型与 日期 字段类型类似，它存储一个日期。然而，date 以毫秒分辨率存储日期，而 date_nanos 以纳秒分辨率存储日期。日期以 long 值的形式存储，表示自纪元以来的纳秒数。因此，支持的日期范围大约是 1970-2262 年。\n对 date_nanos 字段的查询被转换为对字段值的 long 表示形式的范围查询。然后使用字段上设置的格式将存储的字段和聚合结果转换为字符串。\n date_nano 字段支持 date 支持的所有格式和参数。你可以使用 || 分隔的多种格式。\n 对于 date_nanos 字段，你可以使用 strict_date_optional_time_nanos 格式来保留纳秒值。如果你在将字段映射为 date_nanos 时没有指定格式，默认格式是 strict_date_optional_time||epoch_millis，它允许你以 strict_date_optional_time 或 epoch_millis 格式传递值。strict_date_optional_time 格式支持纳秒的日期，但 epoch_millis 格式仅支持毫秒的日期。\n示例 #  创建一个具有 strict_date_optional_time_nanos 格式的 date_nanos 类型的 date 字段的映射：\nPUT testindex/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date_nanos\u0026#34;, \u0026#34;format\u0026#34; : \u0026#34;strict_date_optional_time_nanos\u0026#34; } } } 将两个文档写入到索引中：\nPUT testindex/_doc/1 { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719622Z\u0026#34; } PUT testindex/_doc/2 { \u0026quot;date\u0026quot;: \u0026quot;2022-06-15T10:12:52.382719624Z\u0026quot; } 你可以使用范围查询来搜索日期范围：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;date\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719621Z\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719623Z\u0026#34; } } } } 响应包含日期在指定范围内的文档：\n{ \u0026#34;took\u0026#34;: 43, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719622Z\u0026#34; } } ] } } 在查询带有 date_nanos 字段的文档时，你可以使用 fields 或 docvalue_fields：\nGET testindex/_search { \u0026#34;fields\u0026#34;: [\u0026#34;date\u0026#34;] } GET testindex/_search { \u0026quot;docvalue_fields\u0026quot; : [ { \u0026quot;field\u0026quot; : \u0026quot;date\u0026quot; } ] } 上述任一查询的响应都包含两个已索引的文档：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719622Z\u0026#34; }, \u0026#34;fields\u0026#34;: { \u0026#34;date\u0026#34;: [\u0026#34;2022-06-15T10:12:52.382719622Z\u0026#34;] } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719624Z\u0026#34; }, \u0026#34;fields\u0026#34;: { \u0026#34;date\u0026#34;: [\u0026#34;2022-06-15T10:12:52.382719624Z\u0026#34;] } } ] } } 你可以按如下方式对 date_nanos 字段进行排序：\nGET testindex/_search { \u0026#34;sort\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;asc\u0026#34; } } 响应包含排序后的文档：\n{ \u0026#34;took\u0026#34;: 5, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: null, \u0026#34;_source\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719622Z\u0026#34; }, \u0026#34;sort\u0026#34;: [1655287972382719700] }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: null, \u0026#34;_source\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719624Z\u0026#34; }, \u0026#34;sort\u0026#34;: [1655287972382719700] } ] } } 你也可以使用 Painless 脚本来访问字段的纳秒部分：\nGET testindex/_search { \u0026#34;script_fields\u0026#34; : { \u0026#34;my_field\u0026#34; : { \u0026#34;script\u0026#34; : { \u0026#34;lang\u0026#34; : \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34; : \u0026#34;doc[\u0026#39;date\u0026#39;].value.nano\u0026#34; } } } } 响应仅包含字段的纳秒部分：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;fields\u0026#34;: { \u0026#34;my_field\u0026#34;: [382719622] } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;fields\u0026#34;: { \u0026#34;my_field\u0026#34;: [382719624] } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"纳秒日期字段类型（Date Nanos）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/date-field-type/date-nanos/"},{"category":null,"content":"match_only_text 字段类型 #  Introduced 1.10.0\n简介 #  match_only_text 是一个为全文搜索优化的字段类型，是 text 类型的变体。它通过省略词条位置、词频和规范化信息来减少存储需求,适合对存储成本敏感但仍需要基本全文搜索功能的场景。\n主要特点 #    存储优化:\n 不存储位置信息 不存储词频信息 不存储规范化信息 显著减少索引大小    评分机制:\n 禁用评分计算 所有匹配文档得分统一为 1.0    查询支持:\n 支持大多数查询类型 不支持 interval 查询 不支持 span 查询 支持但不优化短语查询    使用场景 #  适合用于:\n 需要快速查找包含特定词条的文档 对存储成本敏感的大数据集 不需要复杂相关性排序的场景  不适合用于:\n 需要基于相关性排序的查询 依赖词条位置或顺序的查询 需要精确短语匹配的场景  映射示例 #  PUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;match_only_text\u0026#34; } } } } 参数配置 #     参数 说明 默认值     analyzer 分析器设置 standard   boost 评分提升因子 1.0   eager_global_ordinals 是否预加载全局序号 false   fielddata 是否启用 fielddata false   fields 多字段定义 -   index 是否创建索引 true   meta 字段元数据 -    注意: 虽然支持多种参数设置,但建议保持默认配置以维持其优化效果。\n从 text 类型迁移 #  使用 Reindex API 可以将现有的 text 字段迁移到 match_only_text:\n// 1. 创建新索引 PUT destination { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;match_only_text\u0026#34; } } } } // 2. 重建索引 POST _reindex { \u0026quot;source\u0026quot;: { \u0026quot;index\u0026quot;: \u0026quot;source\u0026quot; }, \u0026quot;dest\u0026quot;: { \u0026quot;index\u0026quot;: \u0026quot;destination\u0026quot; } } 使用建议 #\n   评估需求:\n 确认是否需要复杂的相关性排序 确认是否需要位置相关的查询    存储优化:\n 尽量使用默认设置 避免启用额外的存储功能    查询限制:\n 避免使用依赖位置信息的查询 注意短语查询的性能影响    性能考虑:\n 适合大规模数据集 有助于减少存储成本    ","subcategory":null,"summary":"","tags":null,"title":"纯匹配文本字段类型（Match Only Text）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/match_only_text/"},{"category":null,"content":"索引生命周期管理 #  索引生命周期管理（Index Lifecycle Management, ILM）为您提供了一种集成化、自动化的方式来高效管理时序数据。 通过配置 ILM 策略，您可以根据性能、可用性与数据保留需求，自动执行索引的滚动、归档和清理等操作。\n 从 1.15.2 版本开始，index-management 已经成为 modules 的一部分，不需要单独安装插件。\n 典型应用场景 #   自动滚动生成新索引：当现有索引达到指定大小或文档数量时，自动创建新索引。 周期性轮换索引：按天、周或月创建新索引，并将历史索引归档。 强制数据保留策略：自动删除过期索引，确保合规与存储成本可控。 分层存储：将热数据分配到高性能节点，冷数据迁移到廉价存储节点。   策略管理 API #  创建策略 #  创建一个新的生命周期策略。\n请求 #  PUT _ilm/policy/{policyID}    参数 描述 类型 是否必需     policyID 策略 ID。 string 是    查询参数 #     参数 描述 类型 默认值     if_seq_no 仅当匹配此序列号时更新。 long —   if_primary_term 仅当匹配此主分片任期时更新。 long —    请求体 #  策略支持 ES 兼容的 phases 格式。系统内部会自动转换为 states 格式。\nPUT _ilm/policy/my_lifecycle { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;hot\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;0ms\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;rollover\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;max_size\u0026#34;: \u0026#34;50gb\u0026#34;, \u0026#34;max_docs\u0026#34;: 100000000 }, \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 100 } } }, \u0026#34;warm\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;forcemerge\u0026#34;: { \u0026#34;max_num_segments\u0026#34;: 1 }, \u0026#34;allocate\u0026#34;: { \u0026#34;require\u0026#34;: { \u0026#34;box_type\u0026#34;: \u0026#34;warm\u0026#34; } }, \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 50 } } }, \u0026#34;delete\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;90d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;wait_for_snapshot\u0026#34;: { \u0026#34;policy\u0026#34;: \u0026#34;daily-backup\u0026#34; }, \u0026#34;delete\u0026#34;: {} } } } } }  操作名称映射：策略中使用的 ES 兼容名称会自动映射到内部操作名：\n set_priority → index_priority allocate → allocation forcemerge → force_merge readonly → read_only   响应示例 #  { \u0026#34;acknowledged\u0026#34;: true }  应用策略到索引模板 #  要让策略自动生效，需要在索引模板中指定策略名称和滚动索引的别名。\nPUT /_index_template/my_template { \u0026#34;index_patterns\u0026#34;: [\u0026#34;log-test-*\u0026#34;], \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;index.lifecycle.name\u0026#34;: \u0026#34;my_lifecycle\u0026#34;, \u0026#34;index.lifecycle.rollover_alias\u0026#34;: \u0026#34;log-test\u0026#34; } } } 创建初始写入索引 #  设置策略后，需要手动创建第一个托管索引并将其指定为写入索引。索引名称必须匹配模板模式且以数字结尾（后续 rollover 会自动递增）。\nPUT /log-test-000001 { \u0026#34;aliases\u0026#34;: { \u0026#34;log-test\u0026#34;: { \u0026#34;is_write_index\u0026#34;: true } } }  获取策略 #  通过 policyID 获取策略详情。\n请求 #  GET _ilm/policy/{policyID} 响应示例 #  { \u0026#34;_id\u0026#34;: \u0026#34;my_lifecycle\u0026#34;, \u0026#34;_version\u0026#34;: 3, \u0026#34;_seq_no\u0026#34;: 27, \u0026#34;_primary_term\u0026#34;: 2, \u0026#34;policy\u0026#34;: { \u0026#34;policy_id\u0026#34;: \u0026#34;my_lifecycle\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;last_updated_time\u0026#34;: 1682049301487, \u0026#34;schema_version\u0026#34;: 17, \u0026#34;default_state\u0026#34;: \u0026#34;hot\u0026#34;, \u0026#34;states\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;hot\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;retry\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;backoff\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;delay\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;rollover\u0026#34;: { \u0026#34;min_size\u0026#34;: \u0026#34;50gb\u0026#34;, \u0026#34;min_doc_count\u0026#34;: 100000000, \u0026#34;min_index_age\u0026#34;: \u0026#34;7d\u0026#34; } }, { \u0026#34;retry\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;backoff\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;delay\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;index_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 100 } } ], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;warm\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;30d\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;warm\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;retry\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;backoff\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;delay\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;force_merge\u0026#34;: { \u0026#34;max_num_segments\u0026#34;: 1 } }, { \u0026#34;retry\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;backoff\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;delay\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;allocation\u0026#34;: { \u0026#34;require\u0026#34;: { \u0026#34;box_type\u0026#34;: \u0026#34;warm\u0026#34; }, \u0026#34;include\u0026#34;: {}, \u0026#34;exclude\u0026#34;: {}, \u0026#34;wait_for\u0026#34;: false } }, { \u0026#34;retry\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;backoff\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;delay\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;index_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 50 } } ], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;90d\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;retry\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;backoff\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;delay\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;wait_for_snapshot\u0026#34;: { \u0026#34;policy\u0026#34;: \u0026#34;daily-backup\u0026#34; } }, { \u0026#34;retry\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;backoff\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;delay\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;delete\u0026#34;: {} } ], \u0026#34;transitions\u0026#34;: [] } ], \u0026#34;ism_template\u0026#34;: [ { \u0026#34;index_patterns\u0026#34;: [], \u0026#34;priority\u0026#34;: 100, \u0026#34;last_updated_time\u0026#34;: 1682049301487 } ] } }  注意：创建策略时使用的是 ES 兼容的 phases 格式，但获取策略时返回的是内部 states 格式。操作名称也会转换为内部名称（如 set_priority 变为 index_priority）。\n  获取所有策略 #  不指定 policyID 即可列出所有策略。\n请求 #  GET _ilm/policy 查询参数 #     参数 描述 类型 默认值     size 每页返回数量。 int 20   from 分页起始偏移量。 int 0   sortField 排序字段。 string policy.policy_id.keyword   sortOrder 排序方向。 string asc   queryString 搜索过滤条件。 string *    响应示例 #  { \u0026#34;policies\u0026#34;: [ { \u0026#34;_id\u0026#34;: \u0026#34;my_lifecycle\u0026#34;, \u0026#34;_seq_no\u0026#34;: 27, \u0026#34;_primary_term\u0026#34;: 2, \u0026#34;policy\u0026#34;: { \u0026#34;...\u0026#34; : \u0026#34;...\u0026#34; } } ], \u0026#34;total_policies\u0026#34;: 1 }  更新策略 #  更新已有策略。必须在请求中指定 if_seq_no 和 if_primary_term 进行乐观并发控制。\n请求 #  PUT _ilm/policy/{policyID}?if_seq_no=27\u0026amp;if_primary_term=2 请求体格式与创建策略相同。\n请求示例 #  PUT _ilm/policy/my_lifecycle?if_seq_no=27\u0026amp;if_primary_term=2 { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;hot\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;0ms\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;rollover\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;14d\u0026#34;, \u0026#34;max_size\u0026#34;: \u0026#34;100gb\u0026#34; } } }, \u0026#34;delete\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;180d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;delete\u0026#34;: {} } } } } }  删除策略 #  通过 policyID 删除策略。\n请求 #  DELETE _ilm/policy/{policyID} 请求示例 #  DELETE _ilm/policy/my_lifecycle 响应示例 #  { \u0026#34;_index\u0026#34;: \u0026#34;.easysearch-ilm-config\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;my_lifecycle\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;result\u0026#34;: \u0026#34;deleted\u0026#34;, \u0026#34;forced_refresh\u0026#34;: true, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 10, \u0026#34;_primary_term\u0026#34;: 1 }  索引策略管理 API #  将策略应用到索引 #  将指定策略应用到一个或多个索引。\n请求 #  POST /_ilm/add/{index}    参数 描述 类型 是否必需     index 索引名称，支持逗号分隔的多个索引。 string 是    请求体 #  { \u0026#34;policy_id\u0026#34;: \u0026#34;my_lifecycle\u0026#34; } 响应示例 #  { \u0026#34;updated_indices\u0026#34;: 3, \u0026#34;failures\u0026#34;: false, \u0026#34;failed_indices\u0026#34;: [] }  移除索引策略 #  从指定索引上移除生命周期策略。\n请求 #  POST /_ilm/remove/{index} 请求示例 #  POST /_ilm/remove/log-test-000001 响应示例 #  { \u0026#34;updated_indices\u0026#34;: 1, \u0026#34;failures\u0026#34;: false, \u0026#34;failed_indices\u0026#34;: [] }  变更索引策略 #  将索引当前的策略替换为新策略。\n请求 #  POST /_ilm/change_policy/{index} 请求体 #  { \u0026#34;policy_id\u0026#34;: \u0026#34;new_lifecycle_policy\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;warm\u0026#34;, \u0026#34;include\u0026#34;: [ { \u0026#34;state\u0026#34;: \u0026#34;hot\u0026#34; } ], \u0026#34;is_safe\u0026#34;: false }    参数 描述 类型 是否必需     policy_id 要切换到的新策略 ID。 string 是   state 切换后索引在新策略中的目标状态。 string 否   include 按当前状态过滤要变更的索引。 array 否   is_safe 是否为安全切换。 boolean 否    响应示例 #  { \u0026#34;updated_indices\u0026#34;: 1, \u0026#34;failures\u0026#34;: false, \u0026#34;failed_indices\u0026#34;: [] }  查看索引策略状态 #  获取索引的当前生命周期管理状态。支持使用索引模式匹配多个索引。\n请求 #  GET /_ilm/explain/{index} 查询参数 #     参数 描述 类型 默认值     show_policy 是否在响应中包含完整策略。 boolean false   validate_action 是否包含操作验证结果。 boolean false   local 是否使用本地集群状态。 boolean false   size 每页返回数量。 int 20   from 分页起始偏移量。 int 0   sortField 排序字段。 string managed_index.index   sortOrder 排序方向。 string asc   queryString 搜索过滤条件。 string *    请求示例 #  GET /_ilm/explain/log-test* 响应示例 #  { \u0026#34;log-test-000003\u0026#34;: { \u0026#34;index.lifecycle.name\u0026#34;: \u0026#34;my_lifecycle\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;log-test-000003\u0026#34;, \u0026#34;index_uuid\u0026#34;: \u0026#34;58YtMBxaRcGBvOhHcJwQ5w\u0026#34;, \u0026#34;policy_id\u0026#34;: \u0026#34;my_lifecycle\u0026#34;, \u0026#34;policy_seq_no\u0026#34;: -2, \u0026#34;policy_primary_term\u0026#34;: 0, \u0026#34;rolled_over\u0026#34;: false, \u0026#34;index_creation_date\u0026#34;: 1682060631245, \u0026#34;state\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hot\u0026#34;, \u0026#34;start_time\u0026#34;: 1682060706227 }, \u0026#34;action\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;rollover\u0026#34;, \u0026#34;start_time\u0026#34;: 1682060764614, \u0026#34;index\u0026#34;: 0, \u0026#34;failed\u0026#34;: false, \u0026#34;consumed_retries\u0026#34;: 0, \u0026#34;last_retry_time\u0026#34;: 0 }, \u0026#34;step\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;attempt_rollover\u0026#34;, \u0026#34;start_time\u0026#34;: 1682060764614, \u0026#34;step_status\u0026#34;: \u0026#34;condition_not_met\u0026#34; }, \u0026#34;retry_info\u0026#34;: { \u0026#34;failed\u0026#34;: false, \u0026#34;consumed_retries\u0026#34;: 0 }, \u0026#34;info\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Pending rollover of index [index=log-test-000003]\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: { \u0026#34;condition\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;current\u0026#34;: \u0026#34;2.2m\u0026#34;, \u0026#34;creationDate\u0026#34;: 1682060631245 }, \u0026#34;min_size\u0026#34;: { \u0026#34;condition\u0026#34;: \u0026#34;50gb\u0026#34;, \u0026#34;current\u0026#34;: \u0026#34;0b\u0026#34; }, \u0026#34;min_doc_count\u0026#34;: { \u0026#34;condition\u0026#34;: 100000000, \u0026#34;current\u0026#34;: 0 } } }, \u0026#34;enabled\u0026#34;: true }, \u0026#34;total_managed_indices\u0026#34;: 1 }  重试失败操作 #  当生命周期管理操作执行失败时，使用此 API 重试。\n请求 #  POST /_ilm/retry/{index} 请求体（可选） #  { \u0026#34;state\u0026#34;: \u0026#34;hot\u0026#34; } 指定 state 可以让索引从特定状态重新开始执行。\n响应示例 #  { \u0026#34;updated_indices\u0026#34;: 1, \u0026#34;failures\u0026#34;: false, \u0026#34;failed_indices\u0026#34;: [] }  状态转换条件 #  在策略的 states 格式中，每个状态可以定义转换条件。当条件满足时，索引自动进入下一个状态。\n   条件 描述 类型 示例     min_index_age 索引的最小年龄。 string \u0026quot;30d\u0026quot;   min_doc_count 索引的最小文档数量（须 \u0026gt;0）。 long 1000000   min_size 索引的最小大小（须 \u0026gt;0）。 string \u0026quot;50gb\u0026quot;   min_rollover_age 自上次 rollover 以来的最小时间。 string \u0026quot;7d\u0026quot;   cron 基于 cron 表达式的时间条件。 object 见下方     操作参考 #  所有操作都支持以下通用可选参数：\n   参数 描述 类型 默认值     timeout 操作超时时间。 string 无   retry.count 失败时的最大重试次数。 long 3   retry.backoff 重试退避策略。 string exponential   retry.delay 重试间隔时间。 string 1m     操作名称兼容性：在 phases 格式中，可以使用 ES 兼容名称（左列）；在 states 格式中，使用内部名称（右列）：\n   ES 兼容名称 内部名称     set_priority index_priority   allocate allocation   forcemerge force_merge   readonly read_only      rollover #  当现有索引满足指定的滚动条件时，将写入目标切换到新索引。\n滚动目标可以是数据流或索引别名。当目标为数据流时，新索引将变为数据流的写索引，并且其代数将递增。\n如果要滚动索引别名，别名及其写索引必须满足以下条件：\n 索引名称必须符合模式 ^.*-\\d+$，例如 my-index-000001。 必须将 index.lifecycle.rollover_alias 配置为要滚动的别名。 索引必须是别名的写索引。     参数 描述 类型 是否必需     min_size / max_size 当索引主分片总大小达到此值时触发滚动（不含副本）。 string 否   min_doc_count / max_docs 当索引文档数达到此值时触发滚动（不含副本和未刷新文档）。 long 否   min_index_age / max_age 当索引年龄达到此值时触发滚动。 string 否   min_primary_shard_size / max_primary_shard_size 当最大主分片大小达到此值时触发滚动。 string 否     每对参数中，左侧是内部名称，右侧是 ES 兼容别名。两种写法均可使用。\n PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;hot\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;rollover\u0026#34;: { \u0026#34;max_size\u0026#34;: \u0026#34;50gb\u0026#34;, \u0026#34;max_age\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;max_docs\u0026#34;: 100000000 } } } } } }  set_priority / index_priority #  设置索引的优先级，用于节点恢复后的索引恢复顺序。优先级高的索引先恢复。\n   参数 描述 类型 是否必需 验证     priority 索引优先级，值越大越优先。 int 是 必须 ≥ 0    PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;hot\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 100 } } }, \u0026#34;warm\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 50 } } } } } }  forcemerge / force_merge #  通过合并 Lucene 段（segment）来减少段的数量，优化查询性能。此操作在合并前会自动将索引设置为 read_only 状态。\n执行步骤：设置只读 → 执行 force merge → 等待合并完成（共 3 步）。\n   参数 描述 类型 是否必需 验证     max_num_segments 每个分片合并后的目标段数量。 int 是 必须 \u0026gt; 0    PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;warm\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;forcemerge\u0026#34;: { \u0026#34;max_num_segments\u0026#34;: 1 } } } } } }  readonly / read_only #  将托管索引设置为只读，即 index.blocks.write 设为 true。\n 注意：此设置不会阻止索引刷新（refresh）操作。\n PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;warm\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;readonly\u0026#34;: {} } } } } }  read_write #  将托管索引设置为可读写状态，撤消 read_only 操作的效果。\nPUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;warm\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;read_write\u0026#34;: {} } } } } }  replica_count #  设置索引的副本分片数量。\n   参数 描述 类型 是否必需 验证     number_of_replicas 要设置的副本分片数量。 int 是 必须 ≥ 0    PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;warm\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;replica_count\u0026#34;: { \u0026#34;number_of_replicas\u0026#34;: 1 } } } } } }  allocate / allocation #  将索引分配到具有特定属性集的节点上。例如，将 require 设置为 { \u0026quot;box_type\u0026quot;: \u0026quot;warm\u0026quot; }，可以将索引数据迁移到 warm 类型节点。\n   参数 描述 类型 是否必需 默认值     require 索引必须分配到具有所有指定属性的节点。 object 条件必需 ¹ {}   include 索引可以分配到具有任一指定属性的节点。 object 条件必需 ¹ {}   exclude 索引不会分配到具有任一指定属性的节点。 object 条件必需 ¹ {}   wait_for 是否等待分配完成后再进入下一步操作。 boolean 否 false     ¹ require、include、exclude 中至少有一个必须非空。\n PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;warm\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;allocate\u0026#34;: { \u0026#34;require\u0026#34;: { \u0026#34;box_type\u0026#34;: \u0026#34;warm\u0026#34; } } } }, \u0026#34;cold\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;allocate\u0026#34;: { \u0026#34;include\u0026#34;: { \u0026#34;box_type\u0026#34;: \u0026#34;cold,frozen\u0026#34; }, \u0026#34;wait_for\u0026#34;: true } } } } } }  close #  关闭托管索引。关闭的索引仍然存在于磁盘上，但不消耗 CPU 或内存资源，也不能被读取、写入或搜索。\n如果需要长期保留数据但短期内无需搜索，且磁盘空间充足，关闭索引是一个好选择。需要时可以重新打开，比从快照恢复更简便。\nPUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;cold\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;close\u0026#34;: {} } } } } }  open #  打开一个已关闭的托管索引，恢复其读写和搜索能力。\nPUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;warm\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;open\u0026#34;: {} } } } } }  delete #  删除托管索引，将其从集群中完全移除。\n   参数 描述 类型 是否必需 验证     timestamp_field 用于判断索引数据年龄的时间戳字段。须与 min_data_age 配对使用。 string 否 ¹ —   min_data_age 索引中最新数据的最小年龄要求。须与 timestamp_field 配对使用。 string 否 ¹ 必须以天为单位（d），最小 1d     ¹ timestamp_field 和 min_data_age 必须同时指定或同时省略。\n 当两个参数都指定时，系统会：\n 查询索引中按指定时间戳字段排序的最新文档。 计算最新文档时间与当前时间的差值。 仅当差值 ≥ min_data_age 时才执行删除。  这种机制可以防止意外删除仍在活跃使用的数据。\n基本用法：\nPUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;delete\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;delete\u0026#34;: {} } } } } } 带条件检查的删除：\nPUT _ilm/policy/conditional_delete_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;delete\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;delete\u0026#34;: { \u0026#34;timestamp_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;min_data_age\u0026#34;: \u0026#34;90d\u0026#34; } } } } } } 上面的示例中，即使索引满足了 30 天的 min_age 条件进入删除阶段，也只有当索引中最新的文档（基于 @timestamp 字段）已经至少 90 天未更新时，才会执行实际的删除操作。\n snapshot #  将索引创建为快照备份。\n执行步骤：执行快照 → 等待快照完成（共 2 步）。\n   参数 描述 类型 是否必需     repository 已注册的快照仓库名称。 string 是   snapshot 快照名称。支持 Mustache 模板变量。 string 是    PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;cold\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;snapshot\u0026#34;: { \u0026#34;repository\u0026#34;: \u0026#34;my_repository\u0026#34;, \u0026#34;snapshot\u0026#34;: \u0026#34;{{ctx.index}}\u0026#34; } } } } } }  shrink #  将索引缩小到更少的主分片数量。\n执行步骤：迁移分片 → 等待迁移完成 → 执行缩小 → 等待缩小完成（共 4 步）。\n必须指定以下三个参数中的恰好一个来确定目标分片数：\n   参数 描述 类型 验证     num_new_shards 目标主分片数量。 int 必须 \u0026gt; 0   max_shard_size 每个分片的最大大小，系统据此计算分片数。 string 必须 \u0026gt; 0   percentage_of_source_shards 目标分片数为原分片数的百分比。 double 必须 \u0026gt; 0.0 且 \u0026lt; 1.0       可选参数 描述 类型 默认值     target_index_name_template 目标索引名称模板（Mustache 格式，语言须为 mustache）。 object 无   aliases 新索引的别名列表。 array 无   force_unsafe 是否允许不安全缩小。 boolean 无    PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;warm\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;shrink\u0026#34;: { \u0026#34;num_new_shards\u0026#34;: 1 } } } } } } 按最大分片大小缩小：\nPUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;warm\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;shrink\u0026#34;: { \u0026#34;max_shard_size\u0026#34;: \u0026#34;50gb\u0026#34; } } } } } }  alias #  管理索引别名。可以在生命周期操作中添加或移除别名。\n   参数 描述 类型 是否必需     actions 别名操作列表，每个操作为 ADD 或 REMOVE。 array 是     注意：别名操作中不能指定 index/indices 参数，操作自动应用于当前托管索引。\n PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;warm\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;alias\u0026#34;: { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;alias\u0026#34;: \u0026#34;my-data-archive\u0026#34; } }, { \u0026#34;remove\u0026#34;: { \u0026#34;alias\u0026#34;: \u0026#34;my-data-active\u0026#34; } } ] } } } } } }  rollup #  在生命周期操作中自动创建 rollup 作业，将细粒度时序数据汇总为粗粒度数据以节省存储空间。\n执行步骤：创建 Rollup 作业 → 等待 Rollup 完成（共 2 步）。\n   参数 描述 类型 是否必需     ism_rollup ISM Rollup 配置对象。 object 是    ISM Rollup 配置 #     参数 描述 类型 是否必需 验证     description Rollup 作业描述。 string 是 —   target_index 存储 rollup 结果的目标索引名称。 string 是 不能为空   page_size 每批处理的文档数。 int 是 1 ~ 10000   dimensions 维度定义列表。 array 是 第一个必须是 date_histogram   metrics 指标聚合定义列表。 array 是 —    PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;cold\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;rollup\u0026#34;: { \u0026#34;ism_rollup\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Hourly rollup\u0026#34;, \u0026#34;target_index\u0026#34;: \u0026#34;rollup-my-index\u0026#34;, \u0026#34;page_size\u0026#34;: 1000, \u0026#34;dimensions\u0026#34;: [ { \u0026#34;date_histogram\u0026#34;: { \u0026#34;source_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;fixed_interval\u0026#34;: \u0026#34;1h\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;UTC\u0026#34; } }, { \u0026#34;terms\u0026#34;: { \u0026#34;source_field\u0026#34;: \u0026#34;host.name\u0026#34; } } ], \u0026#34;metrics\u0026#34;: [ { \u0026#34;source_field\u0026#34;: \u0026#34;cpu.usage\u0026#34;, \u0026#34;metrics\u0026#34;: [ { \u0026#34;avg\u0026#34;: {} }, { \u0026#34;max\u0026#34;: {} }, { \u0026#34;min\u0026#34;: {} } ] } ] } } } } } } }  wait_for_snapshot #  在删除索引之前，等待指定的快照管理策略（SLM）完成执行。确保被删除索引的快照是可用的。\n   参数 描述 类型 是否必需     policy 要等待的快照管理策略（SLM）名称。 string 是    PUT _ilm/policy/my_policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;delete\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;wait_for_snapshot\u0026#34;: { \u0026#34;policy\u0026#34;: \u0026#34;daily-backup\u0026#34; }, \u0026#34;delete\u0026#34;: {} } } } } }  完整示例 #  以下示例展示一个典型的多阶段生命周期策略，涵盖从数据摄入到归档删除的完整流程：\nPUT _ilm/policy/production_lifecycle { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;hot\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;0ms\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;rollover\u0026#34;: { \u0026#34;max_size\u0026#34;: \u0026#34;50gb\u0026#34;, \u0026#34;max_age\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;max_docs\u0026#34;: 100000000 }, \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 100 } } }, \u0026#34;warm\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;forcemerge\u0026#34;: { \u0026#34;max_num_segments\u0026#34;: 1 }, \u0026#34;shrink\u0026#34;: { \u0026#34;num_new_shards\u0026#34;: 1 }, \u0026#34;allocate\u0026#34;: { \u0026#34;require\u0026#34;: { \u0026#34;box_type\u0026#34;: \u0026#34;warm\u0026#34; } }, \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 50 }, \u0026#34;replica_count\u0026#34;: { \u0026#34;number_of_replicas\u0026#34;: 1 } } }, \u0026#34;cold\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;90d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;readonly\u0026#34;: {}, \u0026#34;allocate\u0026#34;: { \u0026#34;require\u0026#34;: { \u0026#34;box_type\u0026#34;: \u0026#34;cold\u0026#34; } }, \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 0 } } }, \u0026#34;delete\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;365d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;wait_for_snapshot\u0026#34;: { \u0026#34;policy\u0026#34;: \u0026#34;nightly-backup\u0026#34; }, \u0026#34;delete\u0026#34;: {} } } } } } 应用策略 #  PUT /_index_template/production_logs { \u0026#34;index_patterns\u0026#34;: [\u0026#34;logs-prod-*\u0026#34;], \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;index.lifecycle.name\u0026#34;: \u0026#34;production_lifecycle\u0026#34;, \u0026#34;index.lifecycle.rollover_alias\u0026#34;: \u0026#34;logs-prod\u0026#34; } } } PUT /logs-prod-000001 { \u0026#34;aliases\u0026#34;: { \u0026#34;logs-prod\u0026#34;: { \u0026#34;is_write_index\u0026#34;: true } } } 检查生命周期状态 #  GET /_ilm/explain/logs-prod*?show_policy=true 手动为已有索引添加策略 #  POST /_ilm/add/old-logs-2024 { \u0026#34;policy_id\u0026#34;: \u0026#34;production_lifecycle\u0026#34; } ","subcategory":null,"summary":"","tags":null,"title":"索引生命周期","url":"/easysearch/main/docs/features/data-retention/ilm/"},{"category":null,"content":"_index 元数据字段 #  当跨多个索引进行查询时，您可能需要根据文档所在的索引来过滤结果。_index 字段根据文档的索引来匹配文档。\n相关指南（先读这些） #    映射基础  元数据字段  以下示例创建两个索引，products 和 customers，并向每个索引添加一个文档：\nPUT products/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Widget X\u0026#34; } PUT customers/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;John Doe\u0026quot; } 然后，您可以查询这两个索引，并使用 _index 属性过滤结果，如以下示例请求所示：\nGET products,customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;_index\u0026#34;: [\u0026#34;products\u0026#34;, \u0026#34;customers\u0026#34;] } }, \u0026#34;aggs\u0026#34;: { \u0026#34;index_groups\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;size\u0026#34;: 10 } } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;_index\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ], \u0026#34;script_fields\u0026#34;: { \u0026#34;index_name\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;_index\u0026#39;].value\u0026#34; } } } } 在此示例中：\n query 部分使用 terms 查询来匹配来自 products 和 customers 索引的文档。 aggs 部分对 _index 字段执行 terms 聚合，按索引对结果进行分组。 sort 部分按 _index 字段以降序对结果进行排序。 script_fields 部分向搜索结果添加一个名为 index_name 的新字段，其中包含每个文档的 _index 字段值。  在 _index 字段上查询 #  _index 字段表示文档所在的索引。您可以在查询中使用此字段来过滤、聚合、排序或检索搜索结果的索引信息。\n由于 _index 字段会自动添加到每个文档中，您可以像使用其他字段一样在查询中使用它。例如，您可以使用 terms 查询来匹配来自多个索引的文档。以下示例查询返回来自 products 和 customers 索引的所有文档：\n{ \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;_index\u0026#34;: [\u0026#34;products\u0026#34;, \u0026#34;customers\u0026#34;] } } } ","subcategory":null,"summary":"","tags":null,"title":"索引名称元数据字段（_index）","url":"/easysearch/main/docs/features/mapping-and-analysis/metadata-field/index-field/"},{"category":null,"content":"Index 参数 #  index 参数控制字段值是否被索引（写入倒排索引）。未被索引的字段无法被搜索，但仍然会存储在 _source 中，且如果启用了 doc_values，仍可用于排序和聚合。\n相关指南（先读这些） #    映射基础  映射模式与最佳实践  参数选项 #     值 说明     true 字段被索引，可搜索。默认值。   false 字段不被索引，无法搜索。减少磁盘空间和索引时间。    示例 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;internal_code\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;index\u0026#34;: false } } } } 上面的映射中，internal_code 字段不被索引，无法用于搜索查询，但：\n 仍然出现在 _source 中（可被返回） 如果 doc_values 为 true（keyword 默认），仍可用于排序和聚合  适用场景 #     场景 建议     字段仅用于展示，不需要搜索 \u0026quot;index\u0026quot;: false   字段用于排序/聚合，不需要搜索 \u0026quot;index\u0026quot;: false（保持 doc_values: true）   字段需要搜索 \u0026quot;index\u0026quot;: true（默认）   仅存储原始值，不搜索不聚合 \u0026quot;index\u0026quot;: false, \u0026quot;doc_values\u0026quot;: false    注意事项 #   index 参数在索引创建后无法修改，需要重建索引 对 index: false 的字段执行搜索查询会返回错误 与 enabled: false 不同：enabled: false 会完全跳过字段的解析，而 index: false 仍然会解析和存储字段值  ","subcategory":null,"summary":"","tags":null,"title":"索引参数（Index）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/"},{"category":null,"content":"管道处理器 #  pipeline 处理器允许管道引用和包含另一个预定义的管道。当您有一组需要在多个管道之间共享的常见处理器时，这非常有用。您不必在每个管道中重新定义这些常见处理器，而是可以创建一个包含共享处理器的单独基础管道，然后使用管道处理器从其他管道中引用该基础管道。\n以下是为 pipeline 处理器提供的语法：\n{ \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;general-pipeline\u0026#34; } } 配置参数 #  下表列出了 pipeline 处理器所需的和可选参数。\n   参数 是否必填 描述     name 必填 要执行的管道的名称。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  步骤 1：创建一个管道 #  以下查询创建了一个名为 general-pipeline 的一般管道，然后创建了一个名为 outer-pipeline 的新管道，该管道引用了 general-pipeline :\nPUT _ingest/pipeline/general-pipeline { \u0026#34;description\u0026#34;: \u0026#34;a general pipeline\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;uppercase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;protocol\u0026#34; }, \u0026#34;remove\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;name\u0026#34; } } ] } PUT _ingest/pipeline/outer-pipeline { \u0026quot;description\u0026quot;: \u0026quot;an outer pipeline referencing the general pipeline\u0026quot;, \u0026quot;processors\u0026quot;: [ { \u0026quot;pipeline\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;general-pipeline\u0026quot; } } ] } 步骤 2（可选）：测试管道 #\n  建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/outer-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;protocol\u0026#34;: \u0026#34;https\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;test\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;protocol\u0026#34;: \u0026#34;HTTPS\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-05-24T02:43:43.700735801Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPOST testindex1/_doc/1?pipeline=outer-pipeline { \u0026#34;protocol\u0026#34;: \u0026#34;https\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34; } 请求将文档中的 protocol 字段转换为大写并从索引中移除字段名，如下面的响应所示：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 2, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 1, \u0026#34;_primary_term\u0026#34;: 1 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 响应显示将 protocol 字段转换为大写并移除了字段名称的文档：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;_seq_no\u0026#34;: 1, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;protocol\u0026#34;: \u0026#34;HTTPS\u0026#34; } } ","subcategory":null,"summary":"","tags":null,"title":"管道处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/pipeline/"},{"category":null,"content":"Null Value 参数 #  null_value 参数指定一个替代值，用于在字段值为 null 或缺失时代替索引。这使得 null 值可以被搜索和聚合。\n相关指南（先读这些） #    映射基础  结构化搜索  基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;status\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;null_value\u0026#34;: \u0026#34;UNKNOWN\u0026#34; } } } } 写入和查询：\nPUT my-index/_doc/1 { \u0026#34;status\u0026#34;: null } PUT my-index/_doc/2 { \u0026quot;status\u0026quot;: \u0026quot;active\u0026quot; }\n搜索 null 值的文档 GET my-index/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;status\u0026quot;: \u0026quot;UNKNOWN\u0026quot; } } } 文档 1 会被上面的查询找到，因为 null 被替换为 \u0026quot;UNKNOWN\u0026quot; 进行索引。\n重要行为 #     行为 说明     _source _source 中仍然显示原始的 null 值，不会被替换   索引 替换值会被写入倒排索引和 doc_values   类型匹配 替换值的类型必须与字段类型一致   默认值 默认为 null（即不替换，null 值的字段被视为缺失）    按字段类型的示例 #  \u0026#34;properties\u0026#34;: { \u0026#34;count\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;null_value\u0026#34;: 0 }, \u0026#34;is_active\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;null_value\u0026#34;: false }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34;, \u0026#34;null_value\u0026#34;: { \u0026#34;lat\u0026#34;: 0, \u0026#34;lon\u0026#34;: 0 } } } 与 exists 查询的关系 #  如果没有设置 null_value，null 或缺失的字段可以通过 exists 查询的否定来查找：\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must_not\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;status\u0026#34; } } } } } 如果设置了 null_value，那么 null 值的文档也会被 exists 查询匹配到（因为有一个替代值被索引了）。\n注意事项 #   null_value 仅影响索引和搜索，不影响 _source 中的原始值 空数组 [] 和显式 null 的处理方式相同 对 text 字段设置 null_value 没有意义（因为 text 字段不支持精确匹配） 该参数可以通过 PUT mapping API 更新  ","subcategory":null,"summary":"","tags":null,"title":"空值参数（Null Value）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/null_value/"},{"category":null,"content":"Similarity 参数 #  similarity 参数指定字段使用的相关性评分算法。不同的算法适合不同类型的数据和搜索场景。\n相关指南（先读这些） #    评分基础  映射基础  内置算法 #     值 算法 说明     BM25 Okapi BM25 默认值。适合大多数全文搜索场景。   boolean 布尔模型 不计算相关性分数，匹配的文档得分为查询的 boost 值。适合不需要相关性排序的过滤场景。   DFR Divergence from Randomness 基于随机性散度模型的评分算法。   DFI Divergence from Independence 基于独立性散度模型的评分算法。   IB Information Based 基于信息论的评分算法。   LMDirichlet Dirichlet 语言模型 使用 Dirichlet 先验的语言模型平滑方法。   LMJelinekMercer Jelinek-Mercer 语言模型 使用线性插值的语言模型平滑方法。   自定义名称 自定义相似度 在索引 settings 中定义的自定义评分算法。    示例 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;BM25\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;boolean\u0026#34; } } } } BM25 参数调优 #  BM25 的行为可以通过索引 settings 自定义：\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;similarity\u0026#34;: { \u0026#34;custom_bm25\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;BM25\u0026#34;, \u0026#34;k1\u0026#34;: 1.2, \u0026#34;b\u0026#34;: 0.75 } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;custom_bm25\u0026#34; } } } }    参数 默认值 说明     k1 1.2 词频饱和度。值越大，词频的影响越大。   b 0.75 字段长度归一化。0 表示不考虑长度，1 表示完全归一化。    何时更改 similarity #     场景 建议     标准全文搜索 使用默认 BM25   纯过滤/精确匹配场段 使用 boolean，减少评分计算开销   短文本字段（如标题） 调低 b 值，减少长度影响   长文本字段（如正文） 保持默认 b 值     其他内置相似度算法 #  DFR（Divergence from Randomness） #  基于\u0026quot;词频偏离随机分布的程度\u0026quot;来计算相关性。模型由三部分组成：基本模型、后置信息量模型和归一化方法。\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;similarity\u0026#34;: { \u0026#34;my_dfr\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;DFR\u0026#34;, \u0026#34;basic_model\u0026#34;: \u0026#34;g\u0026#34;, \u0026#34;after_effect\u0026#34;: \u0026#34;l\u0026#34;, \u0026#34;normalization\u0026#34;: \u0026#34;h2\u0026#34;, \u0026#34;normalization.h2.c\u0026#34;: \u0026#34;3.0\u0026#34; } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;my_dfr\u0026#34; } } } }    参数 可选值 说明     basic_model be, d, g, if, in, ine, p 基本模型   after_effect no, b, l 后置信息量模型   normalization no, h1, h2, h3, z 归一化方法    归一化参数（按所选归一化方法）：\n   参数 说明     normalization.h1.c h1 归一化的 c 参数（默认 1）   normalization.h2.c h2 归一化的 c 参数（默认 1）   normalization.h3.c h3 归一化的 c 参数（默认 800）   normalization.z.z z 归一化的 z 参数（默认 0.3）    DFI（Divergence from Independence） #  基于词频偏离\u0026quot;独立假设\u0026quot;的程度来计算相关性。对于短文本和精确匹配场景效果较好。\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;similarity\u0026#34;: { \u0026#34;my_dfi\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;DFI\u0026#34;, \u0026#34;independence_measure\u0026#34;: \u0026#34;standardized\u0026#34; } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;my_dfi\u0026#34; } } } }    参数 可选值 说明     independence_measure standardized, saturated, chisquared 独立性度量方法    IB（Information Based） #  基于信息论原理，将词频看作信息内容的度量。由分布模型和 Lambda 参数构成。\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;similarity\u0026#34;: { \u0026#34;my_ib\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;IB\u0026#34;, \u0026#34;distribution\u0026#34;: \u0026#34;ll\u0026#34;, \u0026#34;lambda\u0026#34;: \u0026#34;df\u0026#34;, \u0026#34;normalization\u0026#34;: \u0026#34;h2\u0026#34;, \u0026#34;normalization.h2.c\u0026#34;: \u0026#34;3.0\u0026#34; } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;my_ib\u0026#34; } } } }    参数 可选值 说明     distribution ll, spl 分布模型（log-logistic 或 smoothed power-law）   lambda df, ttf Lambda 参数（文档频率或总词频）   normalization 同 DFR 归一化方法    LMDirichlet（Dirichlet 语言模型） #  使用 Dirichlet 先验进行平滑的语言模型方法。对短文本和小文档集表现较好。\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;similarity\u0026#34;: { \u0026#34;my_lm_dirichlet\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;LMDirichlet\u0026#34;, \u0026#34;mu\u0026#34;: 2000 } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;my_lm_dirichlet\u0026#34; } } } }    参数 默认值 说明     mu 2000 Dirichlet 先验参数。值越大，平滑效果越强。    LMJelinekMercer（Jelinek-Mercer 语言模型） #  使用线性插值将文档模型与集合模型混合的语言模型方法。适合长查询文本。\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;similarity\u0026#34;: { \u0026#34;my_lm_jm\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;LMJelinekMercer\u0026#34;, \u0026#34;lambda\u0026#34;: 0.1 } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;my_lm_jm\u0026#34; } } } }    参数 默认值 说明     lambda 0.1 插值权重。0 = 仅文档模型，1 = 仅集合模型。较小的值偏重文档，较大的值偏重集合频率。     算法选择建议 #     算法 适用场景 特点     BM25 通用全文搜索 默认选择，鲁棒性最强   boolean 过滤/精确匹配 无评分开销   DFR 学术/高级调优 多参数组合，灵活性强   DFI 短文本搜索 基于独立性假设   IB 信息检索研究 基于信息论   LMDirichlet 短文档/短查询 适合稀疏文本   LMJelinekMercer 长查询文本 适合冗长查询     大多数场景下 BM25 已经是最优选择，只有在明确了解评分需求并经过充分测试后，才建议切换为其他算法。\n 注意事项 #   similarity 在索引创建后可以通过关闭索引 → 更新设置 → 重新打开的方式修改 更改 similarity 后，已索引的文档不会自动重新评分，需要重建索引才能完全生效  ","subcategory":null,"summary":"","tags":null,"title":"相似度算法参数（Similarity）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/similarity/"},{"category":null,"content":"Join 字段类型 #  join 字段类型用于在同一索引中的文档之间建立父/子关系。\n相关指南（先读这些） #    Parent-Child 建模  映射模式  代码样例 #  模拟创建一个映射来建立一个产品和其品牌之间的父/子关系：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;product_to_brand\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;relations\u0026#34;: { \u0026#34;brand\u0026#34;: \u0026#34;product\u0026#34; } } } } } 索引一个父文档：\nPUT testindex1/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Brand 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;brand\u0026#34; } } 您也可以使用更简单的格式：\nPUT testindex1/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Brand 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: \u0026#34;brand\u0026#34; }  路由要求：在索引子文档时，您需要指定 routing 查询参数，因为同一父/子层级中的父文档和子文档必须索引在同一分片上。每个子文档在 parent 字段中引用其父文档的 ID。更多路由与性能考虑，请参考 Parent-Child 建模章节。\n 为每个父文档索引两个子文档：\nPUT testindex1/_doc/3?routing=1 { \u0026#34;name\u0026#34;: \u0026#34;Product 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } PUT testindex1/_doc/4?routing=1 { \u0026#34;name\u0026#34;: \u0026#34;Product 2\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } 查询 join 字段 #  当您查询 join 字段时，返回内容中包含指定返回文档是父文档还是子文档的子字段。对于子对象，还会返回父文档 ID。\n搜索所有文档 #  GET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 返回内容：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Brand 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;brand\u0026#34; } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Product 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Product 2\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } } ] } } 搜索父文档的所有子文档 #  查找与 Brand 1 相关的所有产品：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;has_parent\u0026#34;: { \u0026#34;parent_type\u0026#34;: \u0026#34;brand\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Brand 1\u0026#34; } } } } } 返回内容：\n{ \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Product 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Product 2\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } } ] } } 搜索子文档的父文档 #  查找 Product 1 的父文档：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;has_child\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Product 1\u0026#34; } } } } } 返回内容：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Brand 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;brand\u0026#34; } } } ] } } 具有多个子文档的父文档 #  一个父文档可以有多个子文档。创建一个具有多个子文档的映射：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;parent_to_child\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;relations\u0026#34;: { \u0026#34;parent\u0026#34;: [\u0026#34;child 1\u0026#34;, \u0026#34;child 2\u0026#34;] } } } } } Join 字段类型注意事项 #    一个索引中只能有一个 join 字段映射。\n  在检索、更新或删除子文档时，您需要提供 routing 参数。这是因为同一关系中的父文档和子文档必须索引在同一分片上。\n  不支持一个子文档有多个父文档。\n  父子查询和聚合可能会很耗费资源。它们的性能取决于需要检索的子文档或父文档的数量。\n  您可以向现有的 join 字段添加新的关系。\n  ","subcategory":null,"summary":"","tags":null,"title":"父子关系字段类型（Join）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/object-field-type/join/"},{"category":null,"content":"_source 元数据字段 #  _source 字段包含已索引的原始 JSON 文档主体。虽然此字段不可搜索，但它会被存储，以便在执行获取请求（如 get 和 search）时可以返回完整文档。\n禁用 _source #  您可以通过将 enabled 参数设置为 false 来禁用 _source 字段，如以下示例所示：\nPUT sample-index1 { \u0026#34;mappings\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;enabled\u0026#34;: false } } }  注意：禁用 _source 字段可能会影响某些功能的可用性，例如 update、update_by_query 和 reindex API，以及使用原始索引文档查询或聚合的能力。\n 相关指南（先读这些） #    映射基础  元数据字段  包含或排除某些字段 #  您可以使用 includes 和 excludes 参数选择 _source 字段的内容。如以下示例：\nPUT logs { \u0026#34;mappings\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;includes\u0026#34;: [ \u0026#34;*.count\u0026#34;, \u0026#34;meta.*\u0026#34; ], \u0026#34;excludes\u0026#34;: [ \u0026#34;meta.description\u0026#34;, \u0026#34;meta.other.*\u0026#34; ] } } } 这些字段不会存储在 _source 中，但您仍然可以搜索它们，因为数据仍然被索引。\n搜索时过滤 _source #  除了在映射中配置，您还可以在搜索请求中动态控制 _source 返回的字段：\n禁用 _source 返回 #  GET my_index/_search { \u0026#34;_source\u0026#34;: false, \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 只返回指定字段 #  GET my_index/_search { \u0026#34;_source\u0026#34;: [\u0026#34;user\u0026#34;, \u0026#34;message\u0026#34;], \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 使用 includes/excludes #  GET my_index/_search { \u0026#34;_source\u0026#34;: { \u0026#34;includes\u0026#34;: [\u0026#34;user.*\u0026#34;], \u0026#34;excludes\u0026#34;: [\u0026#34;user.password\u0026#34;] }, \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 禁用 _source 的影响 #     功能 _source 启用 _source 禁用     GET /index/_doc/id ✅ 返回文档 ❌ 无内容   update API ✅ 正常 ❌ 不可用   update_by_query ✅ 正常 ❌ 不可用   reindex ✅ 正常 ❌ 不可用   highlight ✅ 正常 ⚠️ 需要额外配置   搜索和聚合 ✅ 正常 ✅ 正常     建议：除非有明确的磁盘空间优化需求，否则保持 _source 启用。禁用后会导致很多日常运维操作不可用。\n ","subcategory":null,"summary":"","tags":null,"title":"源数据元数据字段（_source）","url":"/easysearch/main/docs/features/mapping-and-analysis/metadata-field/source/"},{"category":null,"content":"Percolator 字段类型 #  percolator 字段类型将该字段视为查询处理。任何 JSON 对象字段都可以标记为 percolator 字段。通常，文档被索引并用于搜索，而 percolator 字段存储搜索条件，稍后通过 Percolate 查询将匹配文档到该条件。\n相关指南（先读这些） #    映射基础  渗透查询  参考代码 #  客户正在搜索价格在 400 美元或以下的桌子，并希望为此搜索创建警报。 创建一个映射，为查询字段分配一个 percolator 字段类型：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;search\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;percolator\u0026#34; } } }, \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34; }, \u0026#34;item\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } 索引一个查询\nPUT testindex1/_doc/1 { \u0026#34;search\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;item\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;table\u0026#34; } } }, { \u0026#34;range\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;lte\u0026#34;: 400.00 } } } ] } } } }  查询中引用的字段必须已经存在于映射中。\n 运行一个 percolate 查询\nGET testindex1/_search { \u0026#34;query\u0026#34; : { \u0026#34;bool\u0026#34; : { \u0026#34;filter\u0026#34; : { \u0026#34;percolate\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;search.query\u0026#34;, \u0026#34;document\u0026#34; : { \u0026#34;item\u0026#34; : \u0026#34;Mahogany table\u0026#34;, \u0026#34;price\u0026#34;: 399.99 } } } } } } 返回结果包含原来索引的查询内容\n{ \u0026#34;took\u0026#34; : 30, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 0.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 0.0, \u0026#34;_source\u0026#34; : { \u0026#34;search\u0026#34; : { \u0026#34;query\u0026#34; : { \u0026#34;bool\u0026#34; : { \u0026#34;filter\u0026#34; : [ { \u0026#34;match\u0026#34; : { \u0026#34;item\u0026#34; : { \u0026#34;query\u0026#34; : \u0026#34;table\u0026#34; } } }, { \u0026#34;range\u0026#34; : { \u0026#34;price\u0026#34; : { \u0026#34;lte\u0026#34; : 400.0 } } } ] } } } }, \u0026#34;fields\u0026#34; : { \u0026#34;_percolator_document_slot\u0026#34; : [ 0 ] } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"渗滤器字段类型（Percolator）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/percolator/"},{"category":null,"content":"混合搜索排序处理器（Hybrid Ranker Processor） #   需要 AI 插件\n hybrid_ranker_processor 是一种搜索阶段结果处理器（search phase results processor），运行于 Query 阶段和 Fetch 阶段之间，用于对混合搜索（hybrid 查询）返回的多路子查询得分进行归一化和融合，生成统一的最终得分排序。\n工作原理 #  在混合搜索中，多个子查询（如 BM25 关键词查询 + KNN 向量查询）各自返回独立的得分。这些得分的量纲和分布可能完全不同，无法直接比较。hybrid_ranker_processor 通过以下步骤解决这个问题：\n 得分归一化：将各子查询的得分归一化到可比较的范围 得分融合：使用融合算法（如 RRF）将多路归一化后的得分合并为统一的最终得分 根据最终得分重新排序搜索结果  RRF 算法 #  默认使用 Reciprocal Rank Fusion (RRF) 算法，其核心思想是基于排名而非原始分数进行融合：\n$$\\text{RRF}(d) = \\sum_{i=1}^{n} \\frac{1}{k + r_i(d)}$$\n其中 $k$ 为常数（默认 60），$r_i(d)$ 为文档 $d$ 在第 $i$ 个子查询结果中的排名。\n请求体字段 #     字段 类型 是否必填 说明     combination Object 否 得分融合配置   combination.technique String 否 融合算法名称。默认 rrf   combination.parameters Object 否 融合算法的参数   tag String 否 处理器标识标签   description String 否 处理器描述   ignore_failure Boolean 否 处理器失败时是否继续执行。默认 false    示例 #  基本用法 #  PUT /_search/pipeline/my_hybrid_pipeline { \u0026#34;phase_results_processors\u0026#34;: [ { \u0026#34;hybrid_ranker_processor\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;ranker\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;RRF 混合排序\u0026#34; } } ] } 完整混合搜索管道 #  通常与 semantic_query_enricher 和 hybrid_score_explanation 配合使用：\nPUT /_search/pipeline/full_hybrid_pipeline { \u0026#34;rewrite_processors\u0026#34;: [ { \u0026#34;semantic_query_enricher\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;sk-xxxxxxxx\u0026#34;, \u0026#34;default_model_id\u0026#34;: \u0026#34;text-embedding-v3\u0026#34; } } ], \u0026#34;phase_results_processors\u0026#34;: [ { \u0026#34;hybrid_ranker_processor\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;ranker\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;使用 RRF 进行混合排序\u0026#34;, \u0026#34;combination\u0026#34;: { \u0026#34;technique\u0026#34;: \u0026#34;rrf\u0026#34; } } } ], \u0026#34;response_processors\u0026#34;: [ { \u0026#34;hybrid_score_explanation\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;explanation\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;添加混合评分解释\u0026#34; } } ] } 执行混合搜索：\nGET /my_index/_search?search_pipeline=full_hybrid_pipeline { \u0026#34;query\u0026#34;: { \u0026#34;hybrid\u0026#34;: { \u0026#34;queries\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;集群安全配置\u0026#34; } }, { \u0026#34;semantic\u0026#34;: { \u0026#34;embedding\u0026#34;: { \u0026#34;query_text\u0026#34;: \u0026#34;如何配置集群安全\u0026#34; } } } ] } } } 管道将自动完成：语义查询增强 → BM25 + KNN 并行检索 → RRF 得分融合 → 返回统一排序结果。\n相关文档 #    混合搜索：完整的混合搜索工作流和示例  语义查询增强处理器：自动注入 Embedding 模型信息  混合评分解释处理器：为混合搜索结果添加评分解释  ","subcategory":null,"summary":"","tags":null,"title":"混合搜索排序处理器","url":"/easysearch/main/docs/features/query-dsl/search-pipelines/hybrid-ranker-processor/"},{"category":null,"content":"Thai 分词器 #  thai 分词器使用 Java 内置的泰语分词算法（BreakIterator）对泰语文本进行分词。对于非泰语文本，其行为与 standard 分词器相同。\n示例 #  POST _analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;thai\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;การทดสอบ\u0026#34; } 相关指南 #    Standard 分词器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"泰语分词器（Thai）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/thai/"},{"category":null,"content":"Format 参数 #  format 参数指定日期字段的解析格式。Easysearch 内置了多种日期格式，也支持自定义格式字符串。\n相关指南（先读这些） #    映射基础  Date 字段类型  基本用法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34; } } } } 多格式支持 #  使用 || 分隔符可以指定多个格式，Easysearch 会依次尝试每种格式进行解析：\n\u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; } 内置日期格式 #     格式 说明 示例     epoch_millis 毫秒时间戳 1618000000000   epoch_second 秒时间戳 1618000000   date_optional_time / strict_date_optional_time ISO 8601 日期，时间部分可选 2021-04-10T08:00:00Z   basic_date yyyyMMdd 20210410   basic_date_time yyyyMMdd\u0026rsquo;T\u0026rsquo;HHmmss.SSSZ 20210410T080000.000Z   date yyyy-MM-dd 2021-04-10   date_hour_minute_second yyyy-MM-dd\u0026rsquo;T\u0026rsquo;HH:mm:ss 2021-04-10T08:00:00   date_time yyyy-MM-dd\u0026rsquo;T\u0026rsquo;HH:mm:ss.SSSZ 2021-04-10T08:00:00.000Z   hour_minute_second HH:mm:ss 08:00:00     完整的内置格式列表请参考 Date 字段类型。\n 自定义格式字符串 #     符号 含义 示例     yyyy 四位年份 2021   MM 两位月份 04   dd 两位日期 10   HH 24 小时制小时 08   mm 分钟 30   ss 秒 45   SSS 毫秒 123   Z 时区偏移 +0800    默认格式 #  如果不指定 format，日期字段默认使用 strict_date_optional_time||epoch_millis，这意味着它接受：\n ISO 8601 格式的日期/日期时间字符串 毫秒时间戳  注意事项 #   format 参数在索引创建后无法修改，需要重建索引 写入的日期值必须匹配指定的某一种格式，否则会报解析错误 如果需要同时接受多种格式，使用 || 分隔 strict_ 前缀表示严格模式，不允许省略前导零（如月份 4 必须写为 04）  ","subcategory":null,"summary":"","tags":null,"title":"格式参数（Format）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/format/"},{"category":null,"content":"Date 字段类型 #  在 Easysearch 中，日期可以表示为以下几种形式：\n 一个长整型值，对应自纪元以来的毫秒数（必须为非负数）。日期在内部以此形式存储。 一个格式化的字符串。 一个整数值，对应自纪元以来的秒数（必须为非负数）。   要表示日期范围，可以使用 date range 字段类型。\n 代码样例 #  创建一个有两种日期格式的 date 字段\nPUT testindex { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;release_date\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34; : \u0026#34;strict_date_optional_time||epoch_millis\u0026#34; } } } } 参数说明 #  下表列出了日期字段类型支持的参数，所有参数均为可选项。    参数 描述 默认值     boost 浮点值，指定字段对相关性评分的权重。值大于 1.0 增加相关性，0.0 到 1.0 降低相关性。 1.0   doc_values 布尔值，指定是否将字段存储到磁盘，以便用于聚合、排序或 script 脚本操作。 true   format 用于解析日期的格式。 strict_date_time_no_millis || strict_date_optional_time || epoch_millis   ignore_malformed 布尔值，指定是否忽略格式错误的值而不抛出异常。 false   index 布尔值，指定字段是否可搜索。 true   locale 指定基于区域和语言的日期表示格式。 ROOT（区域和语言中立的本地设置）   meta 接受字段的元数据。    null_value 指定替代 null 的值，必须与字段类型一致。如果未指定，字段值为 null 时会被视为缺失值。 null   store 布尔值，指定字段值是否单独存储并可从 _source 字段外检索。 false    格式 #  Easysearch 提供内置的日期格式，但您也可以自定义日期格式。\n您可以指定多个日期格式，并使用 || 将它们分隔开。\n内置格式 #  大多数日期格式都有一个以 strict_ 开头的对应格式。当格式以 strict_ 开头时，日期必须严格符合格式中指定的位数要求。例如，如果格式设置为 strict_year_month_day（\u0026quot;yyyy-MM-dd\u0026quot;），月份和日期必须是两位数字。因此，\u0026quot;2020-06-09\u0026quot; 是有效的，而 \u0026quot;2020-6-9\u0026quot; 是无效的。\n Epoch 时间定义为 1970 年 1 月 1 日 00:00:00 UTC。\n  y：年份。\nY：基于周的年份。\nM：月份。\nw：年的序数周，从 01 到 53。\nd：日期（天）。\nD：年的序数日，从 001 到 365（闰年为 366）。\ne：周的序数日，从 1（星期一）到 7（星期日）。\nH：小时，从 0 到 23。\nm：分钟。\ns：秒。\nS：秒的小数部分。\nZ：时区偏移（例如，+0400；-0400；-04:00）。\n 数字化日期格式 #     格式名称 描述 示例     epoch_millis 自纪元以来的毫秒数。最小值为 -2⁶³，最大值为 2⁶³ − 1。 1553391286000   epoch_second 自纪元以来的秒数。最小值为 -2⁶³ ÷ 1000，最大值为 (2⁶³ − 1) ÷ 1000。 1553391286    基础日期格式 #  基本日期格式的各个组成部分之间没有分隔符。例如：“20190323”。\n   格式名称 描述 模式和示例     Dates     basic_date_time 基本日期和时间格式，以 T 分隔。 \u0026quot;yyyyMMddTHHmmss.SSSZ\u0026quot; \u0026quot;20190323T213446.123-04:00\u0026quot;   basic_date_time_no_millis 不含毫秒的基本日期和时间格式，以 T 分隔。 \u0026quot;yyyyMMddTHHmmssZ\u0026quot; \u0026quot;20190323T213446-04:00\u0026quot;   basic_date 包含四位数年份、两位数月份和两位数日期的基本格式。 \u0026quot;yyyyMMdd\u0026quot; \u0026quot;20190323\u0026quot;   Times     basic_time 包含小时、分钟、秒、毫秒和时区偏移的时间格式。 \u0026quot;HHmmss.SSSZ\u0026quot; \u0026quot;213446.123-04:00\u0026quot;   basic_time_no_millis 不含毫秒的基本时间格式。 \u0026quot;HHmmssZ\u0026quot; \u0026quot;213446-04:00\u0026quot;   T times     basic_t_time 以 T 开头的基本时间格式。 \u0026quot;THHmmss.SSSZ\u0026quot; \u0026quot;T213446.123-04:00\u0026quot;   basic_t_time_no_millis 以 T 开头的不含毫秒的基本时间格式。 \u0026quot;THHmmssZ\u0026quot; \u0026quot;T213446-04:00\u0026quot;   Ordinal dates     basic_ordinal_date_time 完整的序数日期和时间格式。 \u0026quot;yyyyDDDTHHmmss.SSSZ\u0026quot; \u0026quot;2019082T213446.123-04:00\u0026quot;   basic_ordinal_date_time_no_millis 不含毫秒的完整序数日期和时间格式。 \u0026quot;yyyyDDDTHHmmssZ\u0026quot; \u0026quot;2019082T213446-04:00\u0026quot;   basic_ordinal_date 包含四位数年份和三位数年内序数日期的格式。 \u0026quot;yyyyDDD\u0026quot; \u0026quot;2019082\u0026quot;   Week-based dates     basic_week_date_time\nstrict_basic_week_date_time 以 T 分隔的完整周日期和时间格式。 \u0026quot;YYYYWwweTHHmmss.SSSSSSSSSZ\u0026quot; \u0026quot;2019W126213446.123-04:00\u0026quot;   basic_week_date_time_no_millis\nstrict_basic_week_date_time_no_millis 不含毫秒的基本周日期和时间格式，以 T 分隔。 \u0026quot;YYYYWwweTHHmmssZ\u0026quot; \u0026quot;2019W126213446-04:00\u0026quot;   basic_week_date\nstrict_basic_week_date 包含四位数年份、两位数周编号和一位数周内天编号的完整周日期格式，以 W 分隔。 \u0026quot;YYYYWwwe\u0026quot; \u0026quot;2019W126\u0026quot;    完整日期格式 #  完整日期格式的各个组成部分，日期部分使用 - 作为分隔符，时间部分使用 : 作为分隔符。 例如：\u0026quot;2019-03-23T21:34\u0026quot;。\n日期 #     格式名称 描述 模式和示例     date_optional_time / strict_date_optional_time 通用日期和时间格式，年份必需，月份、日期和时间可选。时间用 T 分隔。 多种格式：\n2019--03--23T21:34:46.123456789--04:00\n2019-03-23T21:34:46\n2019-03-23T21:34\n2019   strict_date_optional_time_nanos 通用日期和时间格式，年份必需，月份、日期和时间可选。如果指定时间，必须包含小时、分钟和秒；秒的小数部分可选，最多 9 位，精度为纳秒，时间用 T 分隔。 多种格式：\n2019-03-23T21:34:46.123456789-04:00\n2019-03-23T21:34:46\n2019   date_time / strict_date_time 完整日期和时间，以 T 分隔，包含毫秒和时区偏移。 \u0026quot;yyyy-MM-ddTHH:mm:ss.SSSZ\u0026quot;\n2019-03-23T21:34:46.123-04:00   date_time_no_millis / strict_date_time_no_millis 完整日期和时间，不包含毫秒，以 T 分隔。 \u0026quot;yyyy-MM-dd'T'HH:mm:ssZ\u0026quot;\n2019-03-23T21:34:46-04:00   datehour_minute_second_fraction / strict* 完整日期、小时、分钟、秒以及 1 到 9 位的小数秒，以 T 分隔。 \u0026quot;yyyy-MM-ddTHH:mm:ss.SSSSSSSSS\u0026quot;\n2019-03-23T21:34:46.123456789\n2019-03-23T21:34:46.1   datehour_minute_second_millis / strict* 包含日期、小时、分钟、秒和 3 位毫秒，以 T 分隔。 \u0026quot;yyyy-MM-ddTHH:mm:ss.SSS\u0026quot;\n2019-03-23T21:34:46.123   datehour_minute_second / strict* 包含日期、小时、分钟和秒，以 T 分隔。 \u0026quot;yyyy-MM-ddTHH:mm:ss\u0026quot;\n2019-03-23T21:34:46   datehour_minute / strict* 包含日期、小时和分钟，以 T 分隔。 \u0026quot;yyyy-MM-ddTHH:mm\u0026quot;\n2019-03-23T21:34   datehour / strict* 包含日期和小时，以 T 分隔。 \u0026quot;yyyy-MM-ddTHH\u0026quot;\n2019-03-23T21   date / strict_date 包含年、月、日。 \u0026quot;yyyy-MM-dd\u0026quot;\n2019-03-23   year_month_day / strict_year_month_day 与日期格式相同，包含年、月、日。 \u0026quot;yyyy-MM-dd\u0026quot;\n2019-03-23   year_month / strict_year_month 包含年和月。 \u0026quot;yyyy-MM\u0026quot;\n2019-03   year / strict_year 仅包含年份。 \u0026quot;yyyy\u0026quot;\n2019   rfc3339_lenient 兼容 RFC3339 的日期格式，支持多种模式并解析速度更快。 \u0026quot;2019\u0026quot;\n2019-03\n2019-03-23\n2019-03-23T21:34Z\n2019-03-23T21:34:46.123456789-04:00    时间 #     格式名称 描述 模式和示例     time / strict_time 两位数小时、两位数分钟、两位数秒、1 到 9 位小数秒和时区偏移。 \u0026quot;HH:mm:ss.SSSSSSSSSZ\u0026quot;\n21:34:46.123456789-04:00\n21:34:46.1-04:00   time_no_millis / strict_time_no_millis 两位数小时、两位数分钟、两位数秒和时区偏移，不包含毫秒。 \u0026quot;HH:mm:ssZ\u0026quot;\n21:34:46-04:00   hour_minute_second_fraction / strict_hour_minute_second_fraction 两位数小时、两位数分钟、两位数秒以及 1 到 9 位小数秒。 \u0026quot;HH:mm:ss.SSSSSSSSS\u0026quot;\n21:34:46.1\n21:34:46.123456789   hour_minute_second_millis / strict_hour_minute_second_millis 两位数小时、两位数分钟、两位数秒以及 3 位毫秒。 \u0026quot;HH:mm:ss.SSS\u0026quot;\n21:34:46.123   hour_minute_second / strict_hour_minute_second 两位数小时、两位数分钟和两位数秒。 \u0026quot;HH:mm:ss\u0026quot;\n21:34:46   hour_minute / strict_hour_minute 两位数小时和两位数分钟。 \u0026quot;HH:mm\u0026quot;\n21:34   hour / strict_hour 两位数小时。 \u0026quot;HH\u0026quot;\n21    时区 #     格式名称 描述 模式和示例     t_time / strict_t_time 以 T 开头的格式，包括两位数小时、两位数分钟、两位数秒、1 到 9 位小数秒和时区偏移。 \u0026quot;THH:mm:ss.SSSSSSSSSZ\u0026quot;\nT21:34:46.123456789-04:00\nT21:34:46.1-04:00   t_time_no_millis / strict_t_time_no_millis 以 T 开头的格式，包括两位数小时、两位数分钟、两位数秒和时区偏移，不包含毫秒。 \u0026quot;THH:mm:ssZ\u0026quot;\nT21:34:46-04:00    完整时间 #     格式名称 描述 模式和示例     ordinal_date_time / strict_ordinal_date_time 完整的序数日期和时间格式，以 T 分隔，包含毫秒和时区偏移。 \u0026quot;yyyy-DDDTHH:mm:ss.SSSZ\u0026quot;\n2019-082T21:34:46.123-04:00   ordinal_date_time_no_millis / strict_ordinal_date_time_no_millis 完整的序数日期和时间格式，以 T 分隔，不包含毫秒，仅包含时区偏移。 \u0026quot;yyyy-DDDTHH:mm:ssZ\u0026quot;\n2019-082T21:34:46-04:00   ordinal_date / strict_ordinal_date 完整的序数日期格式，包含四位数年份和三位数年内序号日期。 \u0026quot;yyyy-DDD\u0026quot;\n2019-082    基于周的时间 #     格式名称 描述 模式和示例     week_date_time / strict_week_date_time 基于周的完整日期和时间，以 T 分隔。包含四位数年份、两位数周编号、一位数周内天编号、时间和时区偏移，时间可包含 1-9 位小数秒。 \u0026quot;YYYY-Www-eTHH:mm:ss.SSSSSSSSSZ\u0026quot;\n2019-W12-6T21:34:46.1-04:00\n2019-W12-6T21:34:46.123456789-04:00   week_date_time_no_millis / strict_week_date_time_no_millis 基于周的完整日期和时间，以 T 分隔，不包含毫秒。包含四位数年份、两位数周编号、一位数周内天编号、时间和时区偏移。 \u0026quot;YYYY-Www-eTHH:mm:ssZ\u0026quot;\n2019-W12-6T21:34:46-04:00   week_date / strict_week_date 基于周的完整日期，包含四位数年份、两位数周编号和一位数周内天编号。 \u0026quot;YYYY-Www-e\u0026quot;\n2019-W12-6   weekyear_week_day / strict_weekyear_week_day 包含四位数基于周的年份、两位数周编号和一位数天编号的日期格式。 \u0026quot;YYYY-'W'ww-e\u0026quot;\n2019-W12-6   weekyear_week / strict_weekyear_week 包含四位数基于周的年份和两位数周编号的日期格式。 \u0026quot;YYYY-Www\u0026quot;\n2019-W12   weekyear / strict_weekyear 包含四位数基于周的年份。 \u0026quot;YYYY\u0026quot;\n2019    定制格式 #  您可以为日期字段创建自定义格式。例如，以下请求将日期格式指定为常见的 \u0026quot;MM/dd/yyyy\u0026quot; 格式：\nPUT testindex { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;release_date\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34; : \u0026#34;MM/dd/yyyy\u0026#34; } } } } 索引写入一个日期文档\nPUT testindex/_doc/21 { \u0026#34;release_date\u0026#34; : \u0026#34;03/21/2019\u0026#34; } 在搜索精确日期时，需要使用与指定格式相同的日期格式：\nGET testindex/_search { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34;: { \u0026#34;release_date\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;03/21/2019\u0026#34; } } } } 范围查询默认使用字段的映射格式。您也可以通过提供 format 参数，以不同的格式指定日期范围：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;release_date\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2019-01-01\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2019-12-31\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34; } } } } 日期计算 #  日期字段类型支持使用日期数学运算来指定查询中的时间段。例如，在 范围查询中的 gt、gte、lt 和 lte 参数，以及 日期范围聚合中的 from 和 to 参数，都可以接受日期数学表达式。\n日期数学表达式包含一个固定日期，后面可以选择性地跟随一个或多个数学运算表达式。固定日期可以是 now（自纪元以来的当前日期和时间，以毫秒为单位），或者以 || 结尾的字符串，用于指定日期（例如，2022-05-18||）。日期必须采用 默认格式（默认为 strict_date_time_no_millis||strict_date_optional_time||epoch_millis）。\n 如果字段映射中指定了多个日期格式，Easysearch 将使用第一个格式将纪元时间值（毫秒）转换为字符串。 如果字段映射未指定格式，Easysearch 将使用 strict_date_optional_time 格式将纪元时间值转换为字符串。\n 日期数学运算支持以下数学运算符\n   运算符 描述 示例     + 加法 +1M：增加 1 个月。   - 减法 -1y：减少 1 年。   / 向下舍入 /h：舍入到当前小时的开始。    日期数学运算支持以下时间单位：\n y：年 M：月 w：周 d：天 h 或 H：小时 m：分钟 s：秒\n 示例表达式 #  以下表达式展示了日期数学运算的用法：\nnow+1M：自纪元以来当前日期和时间的毫秒数加 1 个月。\n2022-05-18||/M：将日期 2022-05-18 舍入到该月份的开始，解析为 2022-05-01。\n2022-05-18T15:23||/h：将时间 15:23 于 2022-05-18 舍入到小时的开始，解析为 2022-05-18T15。\n2022-05-18T15:23:17.789||+2M-1d/d：将时间 15:23:17.789 于 2022-05-18 加 2 个月减 1 天，然后舍入到当天的开始，解析为 2022-07-17。\n在范围查询中使用日期运算 #  以下示例展示了在 范围查询中使用日期数学运算。\n创建一个叫 release_date 的日期字段索引：\nPUT testindex { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;release_date\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;date\u0026#34; } } } } 写入两个文档：\nPUT testindex/_doc/1 { \u0026#34;release_date\u0026#34;: \u0026#34;2022-09-14\u0026#34; } PUT testindex/_doc/2 { \u0026#34;release_date\u0026#34;: \u0026#34;2022-11-15\u0026#34; } 以下查询搜索 release_date 从 2022/09/14 的前 2 个月到后 1 天范围内的文档。最后的时间范围会被四舍五入到 2022/09/14 当天的开始时间：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;release_date\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2022-09-14T15:23||/d\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2022-09-14||+2M+1d\u0026#34; } } } } 返回内容包含两个文档：\n{ \u0026#34;took\u0026#34; : 1, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 2, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;release_date\u0026#34; : \u0026#34;2022-11-14\u0026#34; } }, { \u0026#34;_index\u0026#34; : \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;release_date\u0026#34; : \u0026#34;2022-09-14\u0026#34; } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"日期字段类型（Date）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/date-field-type/date/"},{"category":null,"content":"Doc Values 参数 #  默认情况下，Easysearch 会为搜索目的索引大多数字段的字段值。doc_values 参数启用文档到词项的正排查找，用于排序、聚合和脚本等操作。\ndoc_values 参数接受以下选项：\n   选项 描述     true 启用字段的 doc_values。默认值为 true。   false 禁用字段的 doc_values。    相关指南（先读这些） #    映射基础  映射模式  示例：创建启用和禁用 doc_values 的索引 #  以下示例请求创建一个索引，其中一个字段启用 doc_values，另一个字段禁用：\nPUT my-index-001 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;status_code\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;session_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;doc_values\u0026#34;: false } } } } 工作原理 #  doc_values 是一种列式存储结构，与倒排索引互补：\n   数据结构 方向 用途     倒排索引 词项 → 文档 搜索：给定词项，找到匹配的文档   doc_values 文档 → 值 排序、聚合、脚本：给定文档，获取值    几乎所有字段类型都默认开启 doc_values，text 字段除外（text 字段使用 fielddata 来实现类似功能，但不推荐）。\n何时禁用 doc_values #  禁用 doc_values 可以减少磁盘空间和索引时间，但该字段将 无法用于排序、聚合和脚本。适合以下场景：\n 字段仅用于过滤（如 session_id），不需要排序或聚合 日志场景中某些字段只做搜索匹配，不做统计分析   注意：doc_values 在索引创建后无法修改。如需更改，必须重新创建索引并 reindex 数据。\n 与 fielddata 的区别 #     特性 doc_values fielddata     存储位置 磁盘（索引时构建） 堆内存（查询时按需加载）   适用类型 keyword、数值、日期等 text 字段（不推荐）   性能 磁盘 I/O，内存占用低 消耗大量堆内存，可能引发 OOM   推荐度 ✅ 推荐 ⚠️ 仅在无替代方案时使用    ","subcategory":null,"summary":"","tags":null,"title":"文档值参数（Doc Values）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/doc_values/"},{"category":null,"content":"_id 元数据字段 #  Easysearch 中的每个文档都有一个唯一的 _id 字段。此字段已被索引，允许你使用 GET API 或 ids 查询 检索文档。\n 注意：如果您未提供 _id 值，则 Easysearch 会自动为文档生成一个。\n 相关指南（先读这些） #    映射基础  元数据字段  以下示例请求创建一个名为 test-index1 的索引，并添加两个具有不同 _id 值的文档：\nPUT test-index1/_doc/1 { \u0026#34;text\u0026#34;: \u0026#34;Document with ID 1\u0026#34; } PUT test-index1/_doc/2?refresh=true { \u0026quot;text\u0026quot;: \u0026quot;Document with ID 2\u0026quot; } 您可以使用 _id 字段查询文档，如以下示例请求所示：\nGET test-index1/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;_id\u0026#34;: [\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;] } } } 返回 _id 值为 1 和 2 的两个文档：\n{ \u0026#34;took\u0026#34;: 10, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;test-index1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Document with ID 1\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;test-index1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Document with ID 2\u0026#34; } } ] } _id 字段的限制 #  虽然 _id 字段可以在各种查询中使用，但它在聚合、排序和脚本中的使用受到限制。如果您需要对 _id 字段进行排序或聚合，建议将 _id 内容复制到另一个启用了 doc_values 的字段中。\n","subcategory":null,"summary":"","tags":null,"title":"文档 ID 元数据字段（_id）","url":"/easysearch/main/docs/features/mapping-and-analysis/metadata-field/id/"},{"category":null,"content":"数据生命周期与保留策略 #  这篇从“业务数据要活多久”的角度出发，帮你把几块能力串在一起：\n 时间序列索引设计（按时间切分索引） 热/温/冷 分层与硬件资源规划 自动化的保留与归档流程（ILM/SLM 思路） data-retention 里讲到的删除/关闭/快照等具体动作  如果你只想记住一句话：先画清楚数据时间轴，再用索引 + 模板 + 生命周期策略把它搬进 Easysearch。\n1. 先画一条“数据时间轴” #  通常可以按“价值 + 访问频率”粗分为几段：\n 实时区（Hot）：最近几小时～几天，写入密集、查询频繁，对延迟和可用性要求最高。 近线/历史区（Warm/Cold）：最近几周～几个月，主要用于排查与报表，对延迟容忍度更高。 归档区（Archive）：为了合规或审计留存多年，几乎不查，只要“能找回来”。  每一段都需要明确三件事：\n 保留多久（例如：实时 7 天、近线 90 天、归档 3 年） 大致容量（每天多少文档/多少 GB） 访问模式（实时 OLTP vs 批量统计/偶尔查询）  有了这条时间轴，后面的索引结构、硬件资源和自动化策略才有落点。\n2. 把时间轴映射到索引与模板 #  2.1 按时间切索引：一天/一周/一月？ #  结合业务特征和容量估算，选择合适的时间粒度：\n 高流量日志：通常是按天建索引：logs-2026.02.04 中等流量：可以按周：metrics-2026.w05 低流量：甚至可以按月：audit-2026.02  核心目标：\n 单个分片不要过大（影响恢复和迁移） 单个索引不要包含“过长时间跨度”的数据（否则删除/归档粒度太粗）  2.2 用索引模板固化生命周期相关设置 #  为一类时间序列业务准备一个模板，例如 logs-*：\n 默认 number_of_shards / number_of_replicas 分词与 mapping 策略 默认别名，比如写入别名 logs-write、查询别名 logs-recent （可选）与生命周期策略关联的元数据标签  详细的模板能力和写法见：ingest-and-storage/index-templates.md。\n3. 生命周期策略：从“写入”一路走到“删除” #  无论底层实现是基于 ILM/SLM 还是自建定时任务，整条链路大致会包含这些步骤：\n 创建与写入阶段（Hot）  通过写入别名把新数据写入当前“活跃索引” 控制索引大小（按天切/按体积切，避免单索引过大）   降温阶段（Warm/Cold）  当索引不再写入：迁移到更便宜的节点（冷热分层） 可选：对旧索引执行 _forcemerge，减少段数量   冻结/关闭阶段  对极少访问但仍需在线保留的数据，可以关闭索引减少运行时资源   归档阶段（Archive）  定期为即将删除的索引做快照，存到更便宜的长期存储   删除阶段（Delete）  删除已快照的旧索引，释放线上集群的磁盘与资源    在 Easysearch 中，可以通过：\n 索引设置 + 节点属性，实现热/温/冷节点的迁移 快照与恢复 API，完成归档与按需回溯 定时任务或运维平台，把这些动作自动化  更细致的“每一步怎么做”可以参见 operations/data-retention.md。\n4. ILM/SLM 思路融入你的流程 #  下面是 ILM/SLM 的典型管理模式，可以直接用在 Easysearch 中：\n 索引生命周期（ILM 思路）  Hot：创建索引、写入、滚动生成新索引 Warm：降低副本数、迁移到温节点、forcemerge Cold：迁往更便宜的节点，降低资源占用 Delete：到达保留期后删除索引   快照生命周期（SLM 思路）  定期对某些索引模式做快照（比如每天一次） 为快照本身设置保留策略（如保留最近 N 份）    在 Easysearch 的实践里，你可以：\n 在索引模板中给不同阶段的索引打好标签（如 lifecycle: hot/warm/cold） 让外部调度系统或平台根据标签和时间窗触发相应动作 把“写入别名 + 滚动新索引 + 生命周期动作”统一当成一个 pipeline 来设计  5. 与业务/合规/运维一起确认的 checklist #  在真正落地前，建议至少一起确认这些问题：\n 合规要求  不同类型数据（用户行为日志、审计日志、交易记录）的最短/最长保留期？ 是否需要“不可篡改”“只能追加”等额外约束？   查询需求  实时排查需要多大的时间窗口？（例如近 7 天） 报表和稽核通常会查多久之前的数据？（例如近 6 个月） 归档数据是否允许通过线下导出/恢复再分析？   成本与资源  热节点预算 vs 温/冷节点预算？ 归档存储（对象存储等）的价格与可用性？   操作窗口  夜间是否有允许执行 forcemerge/快照/迁移的维护窗口？ 是否有统一的任务编排平台可以承载这些周期任务？    这些答案会直接影响你最终选用的“粒度 + 保留期 + 分层策略”。\n6. 小结与延伸阅读 #   先画清业务侧的数据时间轴，再决定索引粒度与保留期 把“按时间切索引 + 热/温/冷分层 + 快照归档”当成一个整体生命周期来设计 把生命周期策略固化进索引模板与标准化流程，而不是靠人工记忆  进一步的实现细节和 API 说明，可以参考：\n  数据退役与保留：让老索引优雅下线  索引模板  索引与分片设计（实践指南）  参考手册（API 与参数） #  如果你在落地冷热分层与保留策略时需要具体参数，可以参考：\n  数据生命周期  索引模板  ILM API  备份与恢复  ","subcategory":null,"summary":"","tags":null,"title":"数据生命周期与保留策略","url":"/easysearch/main/docs/best-practices/data-lifecycle/"},{"category":null,"content":"Numeric 字段类型 #  下表列出了 Easysearch 支持的所有数字字段类型。\n   字段数据类型 描述     byte 有符号的 8 位整数。最小值为 -128，最大值为 127。   double 双精度 64 位 IEEE 754 浮点数。最小值为 2^−1074，最大值为 (2 − 2^−52) · 2^1023。有效位数为 53，有效数字位为 15.95。   float 单精度 32 位 IEEE 754 浮点数。最小值为 2^−149，最大值为 (2 − 2^−23) · 2^127。有效位数为 24，有效数字位为 7.22。   half_float 半精度 16 位 IEEE 754 浮点数。最小值为 2^−24，最大值为 65504。有效位数为 11，有效数字位为 3.31。   integer 有符号的 32 位整数。最小值为 -2^31，最大值为 2^31 - 1。   long 有符号的 64 位整数。最小值为 -2^63，最大值为 2^63 - 1。   short 有符号的 16 位整数。最小值为 -2^15，最大值为 2^15 - 1。   scaled_float 一个浮点值，它会被乘以双精度缩放因子并存储为长整型值。     Integer、long、float 和 double 字段类型都有对应的 范围字段类型。\n  如果你的数字字段用来做标识符（如 ID），你可以将此字段映射为 keyword 以优化 term 查询的速度。如果你需要对此字段使用范围查询，你可以将此字段同时映射为数字字段类型和关键字字段类型。\n 示例 #  创建一个映射，其中 integer_value 是一个整数字段：\nPUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;integer_value\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } 写入一个integer_value的文档 PUT testindex/_doc/1 { \u0026quot;integer_value\u0026quot;: 123 } Scaled float 字段类型 #\n scaled float 字段类型是一个浮点值，它会被乘以缩放因子并存储为长整型值。它接受所有数字字段类型的可选参数，还需要一个额外的 scaling_factor 参数。在创建 scaled float 需要添加缩放因子 scale factor。\n scaled float 对于节省磁盘空间很有用。较大的 scaling_factor 值会带来更好的精度，但会占用更多空间。\n Scaled float 示例 #  创建一个映射，其中 scaled 是一个 scaled_float 字段：\nPUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;scaled\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;scaled_float\u0026#34;, \u0026#34;scaling_factor\u0026#34;: 10 } } } } 写入一个包含 scaled_float 字段的文档 PUT testindex/_doc/1 { \u0026quot;scaled\u0026quot;: 2.3 } scaled 值将被存储为 23。\n参数 #  下表列出了数字字段类型接受的参数。所有参数都是可选的。\n   参数 描述     boost 浮点值，指定此字段的权重。值大于 1.0 会增加此字段的相关性。值在 0.0 和 1.0 之间会降低此字段的相关性。默认值为 1.0。   coerce 布尔值，指定是否允许强制类型转换。如果为 true，则字符串会被强制转换为数字，并且小数会被截断为整数。默认为 true。   doc_values 布尔值，指定是否应该将字段添加到列存储中。默认为 true。   ignore_malformed 布尔值，指定是否应该忽略格式错误的数字。默认为 false。   index 布尔值，指定字段是否应该被索引以用于搜索。默认为 true。   meta 接受此字段的元数据。   null_value 用于替代 null 的值。必须与字段类型相同。如果未指定此参数，当字段值为 null 时，该字段将被视为缺失。默认为 null。   store 布尔值，指定字段值是否应该被存储并且可以与 _source 字段分开检索。默认为 false。    scaled float 有一个额外的必需参数：scaling_factor。\n   参数 描述     scaling_factor 一个双精度值，它会被乘以字段值并四舍五入为最接近的长整型。必需参数。    ","subcategory":null,"summary":"","tags":null,"title":"数值字段类型（Numeric）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/numeric-field/"},{"category":null,"content":"Search Analyzer 参数 #  search_analyzer 参数指定查询时使用的分析器，可以与索引时的 analyzer 不同。这允许在写入和查询时使用不同的文本分析策略。\n相关指南（先读这些） #    Analyzer 参数  文本分析  映射基础  分析器选择优先级 #  Easysearch 按以下优先级选择查询时分析器：\n 查询中指定的 analyzer 参数（如 match 查询的 analyzer 字段） 映射中字段的 search_analyzer 索引设置 analysis.analyzer.default_search 映射中字段的 analyzer standard 分析器  示例 #  写入时最大化召回，查询时精确匹配 #  PUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;ik_smart_search\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_smart\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; } } } }    阶段 分析器 策略     索引时 ik_max_word 最细粒度分词，生成更多词项，最大化召回   查询时 ik_smart 智能分词，生成更少词项，提高精确度    同义词扩展只在查询时生效 #  \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;synonym_analyzer\u0026#34; } 这样索引时不扩展同义词（节省空间），查询时通过同义词分析器扩展查询词。\n常见用法 #     场景 索引分析器 查询分析器     中文搜索 ik_max_word（最大召回） ik_smart（精确匹配）   同义词扩展 standard 带同义词过滤的自定义分析器   拼音搜索 pinyin standard（用户输入可能是拼音也可能是汉字）    注意事项 #   如果不设置 search_analyzer，查询时默认使用字段的 analyzer 索引时和查询时使用不同分析器时，需确保两者产生的词项能够正确匹配 可以在具体查询中通过 analyzer 参数临时覆盖  ","subcategory":null,"summary":"","tags":null,"title":"搜索分析器参数（Search Analyzer）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/search_analyzer/"},{"category":null,"content":"排序 #  默认情况下，全文查询按相关性 _score 排序。你也可以按任意字段值升序/降序排序，或使用地理距离、脚本等高级排序方式。\n相关指南 #    分页与排序   基础字段排序 #  通过 sort 参数指定排序字段和顺序：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;Henry IV\u0026#34; } } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;line_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } 排序参数 #     参数 说明 可选值     order 排序方向 asc（升序）、desc（降序）   mode 多值字段的聚合模式 min、max、avg、sum、median   missing 缺失值的处理方式 _first（排最前）、_last（排最后）、自定义值   unmapped_type 未映射字段的假设类型 字段类型名（如 long、date）     多级排序 #  sort 是数组，可以配置多级排序。当主排序键相同时，使用次排序键：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;Henry IV\u0026#34; } } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;line_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } }, { \u0026#34;speech_number\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] }  多值字段排序（mode） #  对于数组类型的数值字段，通过 mode 选择聚合值用于排序：\nGET products/_search { \u0026#34;sort\u0026#34;: [ { \u0026#34;price\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;avg\u0026#34; } } ] }  缺失值处理（missing） #  控制当文档缺少排序字段时的排列位置：\nGET products/_search { \u0026#34;sort\u0026#34;: [ { \u0026#34;price\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34;, \u0026#34;missing\u0026#34;: \u0026#34;_last\u0026#34; } } ] }  按 keyword 字段排序 #  被分析过的 text 字段不能直接排序，因为倒排索引只包含拆分后的 term。常见做法是使用 keyword 子字段：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;Henry IV\u0026#34; } } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;play_name.keyword\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] }  地理距离排序（_geo_distance） #  按地理位置距离排序，适合\u0026quot;附近的店铺\u0026quot;等场景：\nGET restaurants/_search { \u0026#34;sort\u0026#34;: [ { \u0026#34;_geo_distance\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.715, \u0026#34;lon\u0026#34;: -73.998 }, \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;km\u0026#34;, \u0026#34;distance_type\u0026#34;: \u0026#34;arc\u0026#34; } } ] } _geo_distance 参数 #     参数 说明 默认值     location（字段名） 参考坐标点 必填   order 排序方向 asc   unit 距离单位（km、m、mi、yd 等） m   distance_type 距离计算方式 arc（精确）或 plane（快速近似）   mode 多点字段的取值模式 min     脚本排序（_script） #  使用 Painless 脚本自定义排序逻辑：\nGET products/_search { \u0026#34;sort\u0026#34;: [ { \u0026#34;_script\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;price\u0026#39;].value * doc[\u0026#39;discount\u0026#39;].value\u0026#34; }, \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34; } } ] } _script 参数 #     参数 说明     type 排序值类型（number 或 string）   script.source Painless 脚本   script.params 脚本参数   order 排序方向     嵌套字段排序（nested） #  对 nested 对象内的字段排序：\nGET products/_search { \u0026#34;sort\u0026#34;: [ { \u0026#34;reviews.rating\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34;, \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;reviews\u0026#34;, \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;reviews.verified\u0026#34;: true } } } } } ] } nested.filter 可选，用于只对满足条件的嵌套文档参与排序计算。\n 按 _score 排序 #  显式按相关性分数排序（默认行为），通常配合其他排序字段使用：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;to be\u0026#34; } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;_score\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } }, { \u0026#34;line_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34; } } ] }  提示：使用自定义 sort 后，默认不再返回 _score。如果需要同时获取相关性分数，设置 \u0026quot;track_scores\u0026quot;: true。\n ","subcategory":null,"summary":"","tags":null,"title":"排序","url":"/easysearch/main/docs/features/query-dsl/sort/"},{"category":null,"content":"Flattened Text 字段类型 #  flattened_text 类型是一种特殊的数据结构，适用于存储和查询嵌套层次的数据，同时保留类似于 text 类型的灵活搜索特性，例如分词和全文匹配。它在处理结构化或者半结构化数据时非常有用，例如 JSON 对象或动态键值对映射。\n相关指南（先读这些） #    映射基础  映射模式  全文搜索  定义映射 #  { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;flattened_text\u0026#34; } } } 特性 #    扁平化存储\n 将嵌套的 JSON 对象转换为扁平结构 保留完整的路径信息 支持点号访问内部字段    文本分析\n 支持标准分词器 支持短语查询 支持全文搜索功能    内部索引结构 每个 flattened_text 字段在 lucene 层面会创建多个子字段:\n {field} - 存储所有键 {field}._value - 存储所有值 {field}._valueAndPath - 存储 \u0026ldquo;path=value\u0026rdquo; 格式    索引示例 #  PUT my_index/_doc/1 { \u0026#34;my_field\u0026#34;: { \u0026#34;key1\u0026#34;: { \u0026#34;subkey1\u0026#34;: { \u0026#34;subkey2\u0026#34;: \u0026#34;quick brown fox\u0026#34; } } } } 查询示例 #  精确路径匹配 #  // Match Query - 指定完整路径 GET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;my_field.key1.subkey1.subkey2\u0026#34;: \u0026#34;quick brown fox\u0026#34; } } } // Match Phrase Query - 指定完整路径的短语匹配 GET my_index/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;my_field.key1.subkey1.subkey2\u0026quot;: \u0026quot;quick brown fox\u0026quot; } } } 通过根字段字段查询 #\n // Match Query - 匹配所有值 GET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;my_field\u0026#34;: \u0026#34;quick brown fox\u0026#34; } } } // Match Phrase Query - 短语匹配所有值 GET my_index/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;my_field\u0026quot;: \u0026quot;quick brown fox\u0026quot; } } } 使用场景 #\n  需要对 JSON 对象进行全文检索 需要保持对象结构但又想避免字段爆炸 对象结构不固定或未知  限制 #   不支持聚合操作 不支持排序 不支持通配符查询  对比 #  与普通 text 和 flattened 类型相比:\n vs text:支持嵌套对象访问 vs flattened:支持全文检索功能  ","subcategory":null,"summary":"","tags":null,"title":"扁平化文本字段类型（Flattened Text）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/object-field-type/flattened_text/"},{"category":null,"content":"Flattened 字段类型 #  在 Easysearch 中，您不需要在索引文档之前指定映射。如果您不指定映射，Easysearch 会使用动态映射自动映射文档中的每个字段及其子字段。当您摄取诸如日志之类的文档时，您可能事先不知道每个字段的子字段名称和类型。在这种情况下，动态映射所有新的子字段可能会快速导致\u0026quot;映射爆炸\u0026quot;，其中不断增长的字段数量可能会降低集群的性能。\nflattened 字段类型通过将整个 JSON 对象视为字符串来解决这个问题。可以使用标准的点路径表示法访问 JSON 对象中的子字段，但它们不会被索引成单独的字段以供快速查找。\n 注意：点表示法（a.b）中的字段名最大长度为 2^24 − 1。\n 相关指南（先读这些） #    映射基础  映射模式  Flat 对象字段类型提供以下优势：\n 高效读取：获取性能类似于关键字字段。 内存效率：将整个复杂的 JSON 对象存储在一个字段中而不索引其所有子字段，可以减少索引中的字段数量。 空间效率：Easysearch 不会为 flat 对象中的子字段创建倒排索引，从而节省空间。 迁移兼容性：您可以将数据从支持类似 flat 字段的数据库系统迁移到 Easysearch。  当字段及其子字段主要用于读取而不是用作搜索条件时，应将字段映射为 flat 对象，因为子字段不会被索引。当对象具有大量字段或您事先不知道内容时，flat 对象非常有用。\nFlat 对象支持带有和不带有点路径表示法的精确匹配查询。有关支持的查询类型的完整列表，请参见支持的查询。\n 在文档中搜索特定嵌套字段的值可能效率低下，因为它可能需要对索引进行完整扫描，这可能是一个昂贵的操作。\n Flat 对象不支持：\n 特定类型的解析。 数值运算，如数值比较或数值排序。 文本分析。 高亮显示。 使用点表示法(a.b)的子字段聚合。 按子字段过滤。  支持的查询 #  Flat 对象字段类型支持以下查询：\n Term Terms Terms set Prefix Range Match Multi-match Query string Simple query string Exists Wildcard  限制 #  以下限制适用于 Easysearch 中的 flat 对象：\n Flat 对象不支持开放参数。 不支持使用 Painless 脚本和通配符查询来检索子字段的值。  使用 flat 对象 #  以下示例说明怎么将字段映射为 flat 对象、怎么索引带有 flat 对象字段的文档以及在这些文档中怎么去搜索 flat 对象的值。\n首先，创建索引的映射，其中 issue 的类型为 flattened：\nPUT /test-index/ { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;issue\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;flattened\u0026#34; } } } } 然后，索引两个带有 flat 对象字段的文档：\nPUT /test-index/_doc/1 { \u0026#34;issue\u0026#34;: { \u0026#34;number\u0026#34;: \u0026#34;123456\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;2.1\u0026#34;, \u0026#34;backport\u0026#34;: [ \u0026#34;2.0\u0026#34;, \u0026#34;1.3\u0026#34; ], \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;API\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;enhancement\u0026#34; } } } } PUT /test-index/_doc/2 { \u0026quot;issue\u0026quot;: { \u0026quot;number\u0026quot;: \u0026quot;123457\u0026quot;, \u0026quot;labels\u0026quot;: { \u0026quot;version\u0026quot;: \u0026quot;2.2\u0026quot;, \u0026quot;category\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;API\u0026quot;, \u0026quot;level\u0026quot;: \u0026quot;bug\u0026quot; } } } } 要搜索 flat 对象的值，可以使用 GET 或 POST 请求。即使您不知道字段名称，也可以在整个 flat 对象中搜索叶值。例如，以下请求搜索所有标记为 bug 的问题：\nGET /test-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: {\u0026#34;issue\u0026#34;: \u0026#34;bug\u0026#34;} } } 或者，如果您知道要搜索的子字段名称，请使用点表示法提供字段的路径：\nGET /test-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: {\u0026#34;issue.labels.category.level\u0026#34;: \u0026#34;bug\u0026#34;} } } 在这两种情况下，响应都是相同的，并且包含文档 2：\n{ \u0026#34;took\u0026#34;: 1, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.0303539, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;test-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.0303539, \u0026#34;_source\u0026#34;: { \u0026#34;issue\u0026#34;: { \u0026#34;number\u0026#34;: \u0026#34;123457\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;2.2\u0026#34;, \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;API\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;bug\u0026#34; } } } } } ] } } 使用前缀查询，您可以搜索所有以 2. 开头的版本的问题：\nGET /test-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34;: {\u0026#34;issue.labels.version\u0026#34;: \u0026#34;2.\u0026#34;} } } 使用范围查询，您可以搜索版本 2.0-2.1 的所有问题：\nGET /test-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;issue\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2.1\u0026#34; } } } } 将子字段定义为 flat 对象 #  您可以将 JSON 对象的子字段定义为 flat 对象。例如，使用以下查询将 issue.labels 定义为 flattened：\nPUT /test-index/ { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;issue\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;number\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, \u0026#34;labels\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;flattened\u0026#34; } } } } } } 因为 issue.number 不是 flat 对象的一部分，所以您可以使用它来聚合和排序文档。\n","subcategory":null,"summary":"","tags":null,"title":"扁平化字段类型（Flattened）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/object-field-type/flattened/"},{"category":null,"content":"Ignore Above 参数 #  ignore_above 参数指定字符串的最大长度限制。超过此长度的字符串不会被索引或存储在 doc_values 中，但仍会出现在 _source 中。\n该参数主要用于 keyword 字段类型，防止超长字符串占用过多索引空间。\n相关指南（先读这些） #    映射基础  Keyword 字段类型  参数选项 #     值 说明     正整数 超过此字符数的值不被索引。   默认值 2147483647（几乎不限制）。动态映射创建的 keyword 子字段默认为 256。    示例 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;tag\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 100 } } } } 写入测试：\nPUT my-index/_doc/1 { \u0026#34;tag\u0026#34;: \u0026#34;short_tag\u0026#34; } PUT my-index/_doc/2 { \u0026quot;tag\u0026quot;: \u0026quot;this_is_a_very_long_tag_value_that_exceeds_one_hundred_characters_and_therefore_should_not_be_indexed_at_all_in_the_inverted_index\u0026quot; } \n文档 1 的 tag 会被正常索引，可以搜索和聚合 文档 2 的 tag 不会被索引，无法通过 term 查询找到，也不会出现在聚合结果中，但仍然存在于 _source 中  动态映射的默认行为 #  当 Easysearch 动态检测到字符串字段时，会自动创建如下映射：\n\u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 256 } } } 这意味着超过 256 个字符的值不会在 keyword 子字段中被索引。如果你的数据中有超长字段值（如文章正文、日志消息），需要调整这个值：\n 需要对长字符串做精确匹配或聚合 → 增大 ignore_above 字段值确实很短（如标签、状态码） → 使用默认或更小的值 完全不需要 keyword 子字段 → 在映射中去掉 fields.keyword  与 index: false 的区别 #     特性 ignore_above index: false     作用范围 仅跳过超长值 跳过所有值   短值 正常索引 也不索引   _source 所有值都保留 所有值都保留   适用类型 keyword 任意字段类型    注意事项 #   ignore_above 统计的是字符数，不是字节数。对于中文等多字节字符，一个中文字符算一个字符 被忽略的值仍然保存在 _source 中，_source 的大小不受影响 该参数可以通过 PUT mapping API 动态更新（无需重建索引）  ","subcategory":null,"summary":"","tags":null,"title":"忽略超长参数（Ignore Above）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/ignore_above/"},{"category":null,"content":"Ignore Malformed 参数 #  ignore_malformed 参数控制在写入格式错误的数据时，是否忽略该值而不是拒绝整个文档。\n默认情况下，写入一个类型不匹配的值（如向数值字段写入字符串）会导致整个文档被拒绝。启用 ignore_malformed 后，格式错误的值会被静默忽略，文档的其他字段仍然正常索引。\n相关指南（先读这些） #    映射基础  Index API  参数选项 #     值 说明     false 格式错误的值导致整个文档被拒绝。默认值。   true 格式错误的值被忽略，文档其余部分正常索引。    字段级示例 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;ignore_malformed\u0026#34;: true }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } PUT my-index/_doc/1 { \u0026quot;price\u0026quot;: \u0026quot;not_a_number\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;测试产品\u0026quot; } 文档 1 会被成功写入：\n name 字段正常索引 price 字段的值 \u0026quot;not_a_number\u0026quot; 被忽略（不索引、不存入 doc_values），但仍保留在 _source 中  索引级默认值 #  可以在索引 settings 中设置全局默认值：\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;index.mapping.ignore_malformed\u0026#34;: true }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; }, \u0026#34;strict_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;ignore_malformed\u0026#34;: false } } } } 上面的配置中，所有字段默认忽略格式错误（来自 settings），但 strict_field 显式覆盖为 false（严格模式）。\n适用场景 #     场景 建议     数据源不可控（如爬虫、第三方系统） ignore_malformed: true   数据源可控、严格校验 ignore_malformed: false（默认）   日志/监控场景，某些字段偶尔格式异常 ignore_malformed: true   金融/订单等关键数据 ignore_malformed: false    被忽略的字段行为 #     行为 说明     _source ✅ 保留原始值   倒排索引 ❌ 不索引（无法搜索）   doc_values ❌ 不存储（无法排序/聚合）   _ignored 元数据字段 ✅ 记录被忽略的字段名    可以通过 _ignored 字段查找包含被忽略字段的文档：\nGET my-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;_ignored\u0026#34; } } } 注意事项 #   ignore_malformed 不适用于 object 和 nested 字段类型（JSON 结构错误总是会报错） 被忽略的字段仍占用 _source 空间 开启此参数可能掩盖数据质量问题，建议配合监控 _ignored 字段使用  ","subcategory":null,"summary":"","tags":null,"title":"忽略格式错误参数（Ignore Malformed）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/ignore_malformed/"},{"category":null,"content":"_ignored 元数据字段 #  _ignored 字段帮助您管理文档中与格式错误数据相关的问题。由于在 索引映射中启用了 ignore_malformed 设置，此字段用于存储在数据索引过程中被忽略的字段名称。\n_ignored 字段允许您搜索和识别包含被忽略字段的文档，以及被忽略的具体字段名称。这对于故障排除很有用。\n您可以使用 term、terms 和 exists 查询来查询 _ignored 字段。\n 注意：只有当索引映射中启用了 ignore_malformed 设置时，才会填充 _ignored 字段。如果 ignore_malformed 设置为 false（默认值），则格式错误的字段将导致整个文档被拒绝，并且不会填写 _ignored 字段。\n 相关指南（先读这些） #    映射基础  元数据字段  以下示例展示了如何使用 _ignored 字段：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;_ignored\u0026#34; } } } 使用 _ignored 字段的索引请求示例 #  以下示例向 test-ignored 索引添加一个新文档，其中 ignore_malformed 设置为 true，这样在数据索引时不会抛出错误：\nPUT test-ignored { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;length\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;ignore_malformed\u0026#34;: true } } } } POST test-ignored/_doc { \u0026quot;title\u0026quot;: \u0026quot;correct text\u0026quot;, \u0026quot;length\u0026quot;: \u0026quot;not a number\u0026quot; }\nGET test-ignored/_search { \u0026quot;query\u0026quot;: { \u0026quot;exists\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;_ignored\u0026quot; } } } 示例返回内容 #\n { \u0026#34;took\u0026#34;: 42, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;test-ignored\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;qcf0wZABpEYH7Rw9OT7F\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_ignored\u0026#34;: [ \u0026#34;length\u0026#34; ], \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;correct text\u0026#34;, \u0026#34;length\u0026#34;: \u0026#34;not a number\u0026#34; } } ] } } 忽略指定字段 #  您可以使用 term 查询来查找特定字段被忽略的文档，如以下示例请求所示：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;_ignored\u0026#34;: \u0026#34;created_at\u0026#34; } } } 返回内容 #  { \u0026#34;took\u0026#34;: 51, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 45, \u0026#34;successful\u0026#34;: 45, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 0, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] } } ","subcategory":null,"summary":"","tags":null,"title":"忽略字段元数据字段（_ignored）","url":"/easysearch/main/docs/features/mapping-and-analysis/metadata-field/ignored/"},{"category":null,"content":"Coerce 参数 #  coerce 映射参数控制数据在索引期间如何将其值转换为预期的字段数据类型。此参数让您可以验证数据是否按照预期的字段类型正确格式化和索引。这提高了搜索结果的准确性。\n相关指南（先读这些） #    映射基础  映射模式  代码样例 #  以下示例演示如何使用 coerce 映射参数。\n启用 coerce 去索引文档 #  PUT products { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;coerce\u0026#34;: true } } } } PUT products/_doc/1 { \u0026quot;name\u0026quot;: \u0026quot;Product A\u0026quot;, \u0026quot;price\u0026quot;: \u0026quot;19.99\u0026quot; } 在此示例中，price 字段被定义为 integer 类型，且 coerce 设置为 true。在索引文档时，字符串值 19.99 被强制转换为整数 19。\n禁用 coerce 的文档索引 #  PUT orders { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;quantity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;coerce\u0026#34;: false } } } } PUT orders/_doc/1 { \u0026quot;item\u0026quot;: \u0026quot;Widget\u0026quot;, \u0026quot;quantity\u0026quot;: \u0026quot;10\u0026quot; } 在此示例中，quantity 字段被定义为 integer 类型，且 coerce 设置为 false。在索引文档时，字符串值 10 不会被强制转换，由于类型不匹配，文档写入会被拒绝。\n设置索引级别coerce强制转换设置 #  PUT inventory { \u0026#34;settings\u0026#34;: { \u0026#34;index.mapping.coerce\u0026#34;: false }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;stock_count\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;coerce\u0026#34;: true }, \u0026#34;sku\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } PUT inventory/_doc/1 { \u0026quot;sku\u0026quot;: \u0026quot;ABC123\u0026quot;, \u0026quot;stock_count\u0026quot;: \u0026quot;50\u0026quot; } 在此示例中，索引级别的 index.mapping.coerce 设置为 false，这会禁用索引的强制转换。但是，stock_count 字段覆盖了此设置，这个字段可以自动转换。\n","subcategory":null,"summary":"","tags":null,"title":"强制类型转换参数（Coerce）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/coerce/"},{"category":null,"content":"并发控制与版本 #  当有多个客户端同时写入同一份文档时，如果不做任何并发控制，很容易出现\u0026quot;旧值覆盖新值\u0026quot;的问题。本页介绍如何用版本与乐观锁避免这类隐性数据错误。\n为什么需要并发控制？ #  考虑这样一个场景：\n 客户端 A 读取文档，准备把字段 count 从 1 改到 2 客户端 B 也读取了同一文档，把 count 从 1 改到 3  如果没有并发控制：\n A 先写入，文档变成 count=2 B 后写入，文档被覆盖为 count=3（A 的更新\u0026quot;丢了\u0026quot;）  很多时候这种问题不会立刻暴露，但会在数据对账或业务逻辑中造成难以解释的异常。\n乐观并发控制（Optimistic Concurrency Control） #  Easysearch 使用 _seq_no（序列号） 和 _primary_term（主分片任期） 实现乐观并发控制：\n _seq_no：每次对分片的写操作递增，全局唯一标识该分片上的操作顺序 _primary_term：当主分片发生切换（故障转移）时递增，用于区分不同\u0026quot;任期\u0026quot;的写入  每次读取文档时，响应中都会包含这两个值：\nGET /products/_doc/1 // 响应 { \u0026quot;_index\u0026quot;: \u0026quot;products\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_version\u0026quot;: 3, \u0026quot;_seq_no\u0026quot;: 5, \u0026quot;_primary_term\u0026quot;: 1, \u0026quot;found\u0026quot;: true, \u0026quot;_source\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;iPhone\u0026quot;, \u0026quot;count\u0026quot;: 10, \u0026quot;price\u0026quot;: 5999 } } 安全更新：if_seq_no + if_primary_term #\n 使用 if_seq_no 和 if_primary_term 参数，只有当前版本与预期一致时才允许更新：\n// 基于读取到的 _seq_no=5, _primary_term=1 进行更新 PUT /products/_doc/1?if_seq_no=5\u0026amp;if_primary_term=1 { \u0026#34;name\u0026#34;: \u0026#34;iPhone\u0026#34;, \u0026#34;count\u0026#34;: 9, \u0026#34;price\u0026#34;: 5999 }  版本匹配：更新成功，_seq_no 递增 版本不匹配（中途有人修改过）：返回 409 Conflict  // 冲突响应 { \u0026#34;error\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;version_conflict_engine_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;[1]: version conflict, required seqNo [5], primary term [1]. current document has seqNo [6] and primary term [1]\u0026#34; }, \u0026#34;status\u0026#34;: 409 } _update API 同样支持这两个参数：\nPOST /products/_doc/1/_update?if_seq_no=5\u0026amp;if_primary_term=1 { \u0026#34;doc\u0026#34;: { \u0026#34;count\u0026#34;: 9 } } 外部版本（External Versioning） #  当数据从外部系统（关系数据库、消息队列）同步到 Easysearch 时，可以使用外部系统的版本号来控制写入顺序：\n// 使用外部版本号 PUT /products/_doc/1?version=100\u0026amp;version_type=external { \u0026#34;name\u0026#34;: \u0026#34;iPhone\u0026#34;, \u0026#34;count\u0026#34;: 10, \u0026#34;price\u0026#34;: 5999 } version_type 的取值：\n   类型 行为     internal（默认） 使用 Easysearch 内部版本号，要求精确匹配   external / external_gte 使用外部版本号；只要新版本 \u0026gt; 当前版本就允许写入（external_gte 允许 ≥）    这在数据库→Easysearch 同步场景中非常实用：数据库的自增主键或时间戳可以直接作为版本号，保证更新顺序。\nretry_on_conflict：自动重试 #  对于\u0026quot;计数器类、顺序不敏感\u0026quot;的 update，可以通过 retry_on_conflict 自动重试冲突：\nPOST /products/_doc/1/_update?retry_on_conflict=3 { \u0026#34;script\u0026#34;: \u0026#34;ctx._source.count += 1\u0026#34; } 遇到版本冲突时，Easysearch 会自动重新读取最新文档并重试 update，最多重试指定次数。\n 适用场景：计数器累加、浏览量统计等\u0026quot;最终值正确即可\u0026quot;的场景。对于顺序敏感的业务（如库存扣减），不要依赖自动重试——应在客户端进行冲突处理。\n 幂等性设计 #  有了并发控制之后，重试逻辑需要考虑幂等性：\n 版本冲突类错误：盲目重试没有意义——需要回读最新文档，再基于新状态重新计算更新 临时性错误（网络抖动、资源不足）：可以在保证幂等的前提下做重试  实现幂等的常用手段：\n 使用稳定的 _id（按业务唯一键），相同 _id 的 index 操作天然幂等（覆盖写） 使用 op_type=create，保证同一 _id 只创建一次 在文档中显式记录操作版本/时间戳，在更新逻辑中进行检查  与业务语义的结合 #  并发控制策略要与业务语义配合设计：\n   场景 推荐策略     累计类字段（总访问量） retry_on_conflict + 脚本自增，关注整体趋势即可   库存 / 余额 / 配额 if_seq_no + if_primary_term 严格乐观锁，客户端处理冲突   状态类字段（订单状态） 乐观锁 + 状态机收敛到单一负责方   外部系统同步 version_type=external，以外部系统版本号为准   日志 / 事件 op_type=create + 稳定 _id，保证不重复写入    小结 #   _seq_no + _primary_term 是 Easysearch 中乐观并发控制的基础，通过 if_seq_no 和 if_primary_term 参数实现条件更新 version_type=external 适用于从外部系统同步数据的场景 retry_on_conflict 适用于顺序不敏感的累加类操作 设计文档结构与更新流程时，要预先想清楚并发模型与幂等策略  下一步可以继续阅读：\n  _source 与存储字段  索引设置  文档设计  ","subcategory":null,"summary":"","tags":null,"title":"并发控制与版本","url":"/easysearch/main/docs/fundamentals/concurrency-and-versioning/"},{"category":null,"content":"布尔字段类型 #  布尔字段类型接受 true 或 false 值，也支持字符串形式的 \u0026ldquo;true\u0026rdquo; 或 \u0026ldquo;false\u0026rdquo;。此外，还可以使用空字符串 \u0026quot;\u0026quot; 表示 false 值。\n参考代码 #  创建一个由 a,b,c 三个布尔字段组成的 mapping\nPUT testindex { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;a\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;boolean\u0026#34; }, \u0026#34;b\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;boolean\u0026#34; }, \u0026#34;c\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;boolean\u0026#34; } } } } 索引由布尔值组成的文档\nPUT testindex/_doc/1 { \u0026#34;a\u0026#34; : true, \u0026#34;b\u0026#34; : \u0026#34;true\u0026#34;, \u0026#34;c\u0026#34; : \u0026#34;\u0026#34; } 因此，字段 a 和 b 将被设置为 true，而字段 c 将被设置为 false。\n要搜索所有 c 为 false 的文档，可以使用以下查询示例：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34; : { \u0026#34;c\u0026#34; : false } } } 参数说明 #  下表列出了布尔字段类型支持的参数，所有参数均为可选项。\n   参数 描述 默认值     boost 浮点值，指定字段对相关性评分的权重，大于 1.0 提高相关性，0.0 到 1.0 降低相关性。 1.0   doc_values 布尔值，指定是否将字段存储到磁盘以供聚合、排序或脚本操作使用。 true   index 布尔值，指定字段是否可被搜索。 true   meta 接受字段的元数据。 无   null_value 替代 null 的值，类型必须与字段一致。如果未指定，当值为 null 时视为缺失。 null   store 布尔值，指定字段值是否单独存储并可从 _source 外检索。 false    在聚合和 script 中使用布尔值 #  在布尔字段的聚合操作中：key 返回数值，true 对应 1，false 对应 0；key_as_string 返回字符串表示，\u0026ldquo;true\u0026rdquo; 或 \u0026ldquo;false\u0026rdquo;。Script 操作直接返回布尔值 true 或 false。\n参考代码 #  运行一个 term 聚合在字段 a 上面\nGET testindex/_search { \u0026#34;aggs\u0026#34;: { \u0026#34;agg1\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;a\u0026#34; } } }, \u0026#34;script_fields\u0026#34;: { \u0026#34;a\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;a\u0026#39;].value\u0026#34; } } } } Script 将字段 a 的值返回为 true，key 将其值返回为数值 1，而 key_as_string 则返回字符串 \u0026ldquo;true\u0026rdquo;。\n{ \u0026#34;took\u0026#34; : 1133, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;testindex\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;fields\u0026#34; : { \u0026#34;a\u0026#34; : [ true ] } } ] }, \u0026#34;aggregations\u0026#34; : { \u0026#34;agg1\u0026#34; : { \u0026#34;doc_count_error_upper_bound\u0026#34; : 0, \u0026#34;sum_other_doc_count\u0026#34; : 0, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : 1, \u0026#34;key_as_string\u0026#34; : \u0026#34;true\u0026#34;, \u0026#34;doc_count\u0026#34; : 1 } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"布尔字段类型（Boolean）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/boolean/"},{"category":null,"content":"Nested 字段类型 #  nested 字段类型是一种特殊的 对象字段类型，用于将数组中的对象作为独立文档索引，避免扁平化导致的\u0026quot;笛卡尔积假匹配\u0026quot;问题。\n相关指南（先读这些） #    Nested 建模  映射模式  扁平化 vs Nested #  任何对象字段都可以包含一个对象数组。默认情况下，数组中的每个对象都会被动态映射为对象字段类型并以扁平化形式存储。这意味着数组中的对象会被分解成单独的字段，每个字段在所有对象中的值会被存储在一起。有时需要使用 Nested 嵌套类型来将嵌套对象作为一个整体保存，以便您可以对其关联性执行搜索。\n扁平化形式 #  默认情况下，每个嵌套对象都被动态映射为对象字段类型。任何对象字段都可以包含一个对象数组。\nPUT testindex1/_doc/100 { \u0026#34;patients\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 56, \u0026#34;smoker\u0026#34;: true}, {\u0026#34;name\u0026#34;: \u0026#34;Mary Major\u0026#34;, \u0026#34;age\u0026#34;: 85, \u0026#34;smoker\u0026#34;: false} ] } 当这些对象被存储时，它们会被扁平化，因此它们的内部表示形式具有每个字段的所有值的数组：\n{ \u0026#34;patients.name\u0026#34;: [\u0026#34;John Doe\u0026#34;, \u0026#34;Mary Major\u0026#34;], \u0026#34;patients.age\u0026#34;: [56, 85], \u0026#34;patients.smoker\u0026#34;: [true, false] } 一些查询会在这种表示形式中正确工作。如果您搜索年龄大于 75 或者 吸烟的病人 \u0026quot;patients.smoker\u0026quot;: true，文档 id 100 应该匹配。\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;patients.smoker\u0026#34;: true } }, { \u0026#34;range\u0026#34;: { \u0026#34;patients.age\u0026#34;: { \u0026#34;gte\u0026#34;: 75 } } } ] } } } 查询正确地返回文档 id 100：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.3616575, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;_score\u0026#34;: 1.3616575, \u0026#34;_source\u0026#34;: { \u0026#34;patients\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;56\u0026#34;, \u0026#34;smoker\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Mary Major\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;85\u0026#34;, \u0026#34;smoker\u0026#34;: false } ] } } ] } } 或者，如果您搜索年龄大于 75 并且是吸烟的病人，文档 100 不应该匹配。\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;patients.smoker\u0026#34;: true } }, { \u0026#34;range\u0026#34;: { \u0026#34;patients.age\u0026#34;: { \u0026#34;gte\u0026#34;: 75 } } } ] } } } 然而，这个查询仍然错误地返回文档 100。这是因为当创建每个字段的值数组时，年龄和吸烟患者之间的关联关系丢失了。\nNested 嵌套字段类型 #  Nested 嵌套对象虽然作为独立的文档存储，但是父对象持有对其子对象的引用，这样就保留了子对象之间的关联关系。要将对象标记为嵌套类型，需要创建一个带有嵌套字段类型的映射。\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;patients\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34; } } } } 然后，索引一个带有嵌套字段类型的文档：\nPUT testindex1/_doc/100 { \u0026#34;patients\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 56, \u0026#34;smoker\u0026#34;: true}, {\u0026#34;name\u0026#34;: \u0026#34;Mary Major\u0026#34;, \u0026#34;age\u0026#34;: 85, \u0026#34;smoker\u0026#34;: false} ] } 您可以使用以下嵌套查询来搜索年龄大于 75 或者 吸烟的患者：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;patients\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;patients.smoker\u0026#34;: true } }, { \u0026#34;range\u0026#34;: { \u0026#34;patients.age\u0026#34;: { \u0026#34;gte\u0026#34;: 75 } } } ] } } } } } 查询正确地返回了两个患者：\n{ \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.8465736, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;_score\u0026#34;: 0.8465736, \u0026#34;_source\u0026#34;: { \u0026#34;patients\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 56, \u0026#34;smoker\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Mary Major\u0026#34;, \u0026#34;age\u0026#34;: 85, \u0026#34;smoker\u0026#34;: false } ] } } ] } } 您可以使用以下嵌套查询来搜索年龄大于 75 并且 吸烟的患者：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;patients\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;patients.smoker\u0026#34;: true } }, { \u0026#34;range\u0026#34;: { \u0026#34;patients.age\u0026#34;: { \u0026#34;gte\u0026#34;: 75 } } } ] } } } } } 如预期一样，前面的查询没有返回任何结果：\n{ \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 0, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] } } 参数说明 #  下表列出了对象字段类型接受的参数。所有参数都是可选的。\n   参数 说明     dynamic 指定是否可以动态地向对象添加新字段。有效值为 true、false、strict 和 strict_allow_templates。默认为 true。   include_in_parent 一个布尔值，指定子嵌套对象中的所有字段是否也应以扁平化形式添加到父文档中。默认为 false。   include_in_root 一个布尔值，指定子嵌套对象中的所有字段是否也应以扁平化形式添加到根文档中。默认为 false。   properties 此对象的字段，可以是任何支持的类型。如果 dynamic 设置为 true，则可以动态地向此对象添加新属性。    ","subcategory":null,"summary":"","tags":null,"title":"嵌套字段类型（Nested）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/object-field-type/nested/"},{"category":null,"content":"Properties 参数 #  properties 参数用于定义对象（object）和嵌套（nested）类型字段的子字段映射。它是构建层级文档结构的核心参数。\n相关指南 #    映射基础  dynamic 参数  使用位置 #  properties 出现在三个层级：\n   层级 说明     mappings.properties 顶层字段定义   mappings.properties.\u0026lt;object\u0026gt;.properties 对象字段的子字段   mappings.properties.\u0026lt;nested\u0026gt;.properties 嵌套字段的子字段    示例 #  基本层级结构 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;address\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;city\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;zip\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } 嵌套类型 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;comments\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;author\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } } } } } } 多层嵌套 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;department\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;manager\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } } } 对应的文档字段以 . 分隔：department.manager.email。\nobject 与 nested 的区别 #     特性 object（默认） nested     内部对象独立性 数组中的对象会被扁平化，失去关联关系 每个对象独立存储，保持内部字段关联   查询方式 普通查询 需要使用 nested 查询   性能 更快 较慢（每个嵌套对象作为独立 Lucene 文档）   适用场景 非数组对象或不需要精确关联的数组 需要精确匹配数组中特定对象内字段关联    注意事项 #   properties 不需要指定 type，它本身就是对象结构的定义方式 省略 type 时，字段自动被视为 object 类型 已有字段可以新增子字段（通过 Update Mapping API），但已有子字段的类型不可更改 嵌套对象过多会影响索引和查询性能，建议通过 index.mapping.nested_objects.limit 控制数量  ","subcategory":null,"summary":"","tags":null,"title":"属性参数（Properties）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/properties/"},{"category":null,"content":"Object 字段类型 #  object 字段类型包含一个 JSON 对象（一组名称/值对）。JSON 对象中的值可以是另一个 JSON 对象。在映射对象字段时不需要指定 object 作为类型，因为 object 是默认类型。\n相关指南（先读这些） #    映射基础  映射模式  代码示例 #  创建一个带有对象字段的映射：\nPUT testindex1/_mappings { \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34; : { \u0026#34;name\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34; }, \u0026#34;id\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;keyword\u0026#34; } } } } } 索引一个包含对象字段的文档：\nPUT testindex1/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;123456\u0026#34; } } 嵌套对象在内部存储为扁平的键/值对。要引用嵌套对象中的字段，使用 parent field.child field（例如，patient.id）。\n搜索 ID 为 123456 的患者信息：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;patient.id\u0026#34;: \u0026#34;123456\u0026#34; } } } 参数 #  下表列出了对象字段类型接受的参数。所有参数都是可选的。\n   参数 描述     dynamic 指定是否可以动态地向对象添加新字段。有效值为 true、false、strict 和 strict_allow_templates。默认为 true。   enabled 一个布尔值，指定是否应解析对象的 JSON 内容。如果 enabled 设置为 false，则对象的内容不会被索引或搜索，但仍可以从 _source 字段中检索。默认为 true。   properties 此对象字段的属性（即子字段设置），可以是任何支持的类型。如果 dynamic 设置为 true，则可以动态地向此对象添加新属性。    dynamic 参数 #  dynamic 参数指定是否可以向已经索引的对象动态添加新字段。\n例如，您最初可以创建一个只有一个字段的 patient 对象的映射：\nPUT testindex1/_mappings { \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34; : { \u0026#34;name\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34; } } } } } 然后，您在 patient 中索引一个带有新 id 字段的文档：\nPUT testindex1/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;123456\u0026#34; } } 结果，id 字段被添加到映射中：\n{ \u0026#34;testindex1\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } } } } dynamic 参数有以下有效值：\n   值 描述     true 可以动态地向映射添加新字段。这是默认值。   false 不能动态地向映射添加新字段。如果检测到新字段，它不会被索引或搜索。但是，仍然可以从 _source 字段中检索它。   strict 当动态地向映射添加新字段时，会抛出异常。要向对象添加新字段，必须先将其添加到映射中。   strict_allow_templates 如果新检测到的字段匹配映射中的任何预定义动态模板，则它们会被添加到映射中；如果它们不匹配任何模板，则会抛出异常。     内部对象会继承其父对象的 dynamic 参数值，除非它们声明了自己的 dynamic 参数值。\n ","subcategory":null,"summary":"","tags":null,"title":"对象字段类型（Object）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/object-field-type/object/"},{"category":null,"content":"Store 参数 #  store 参数指定字段值是否应被独立存储，以便可以脱离 _source 单独检索。\n默认情况下，字段值在 _source 中存储，但不会被独立存储。当你需要检索某个字段的值时，Easysearch 会加载整个 _source 文档并提取该字段。如果一个文档非常大，而你只需要获取其中一两个小字段的值，使用 store: true 可能更高效。\n相关指南（先读这些） #    _source 与字段存储  映射基础  参数选项 #     值 说明     true 字段值被独立存储，可通过 stored_fields 参数单独检索。   false 字段值不独立存储。默认值。    示例 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;store\u0026#34;: true }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;store\u0026#34;: true } } } } 检索独立存储的字段：\nGET my-index/_search { \u0026#34;stored_fields\u0026#34;: [\u0026#34;title\u0026#34;, \u0026#34;created_at\u0026#34;], \u0026#34;_source\u0026#34;: false } 适用场景 #     场景 建议     文档很大（如包含大段正文），只需返回少数字段 考虑 store: true   文档较小，返回大部分字段 使用默认 _source，无需 store   已禁用 _source（\u0026quot;_source\u0026quot;: {\u0026quot;enabled\u0026quot;: false}） 必须用 store: true 保存需要返回的字段    注意事项 #   每个 store: true 的字段都会额外占用磁盘空间 绝大多数场景下，直接使用 _source（配合 _source filtering）即可，不需要 store store: true 的字段在 _source 中仍然存在（除非禁用了 _source）  ","subcategory":null,"summary":"","tags":null,"title":"存储参数（Store）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/store/"},{"category":null,"content":"Fielddata 参数 #  fielddata 参数控制 text 字段是否可以用于排序、聚合和脚本。\n默认情况下，text 字段不支持排序和聚合——因为 text 字段使用倒排索引，按词项而非完整值存储，无法高效地做正排查找。fielddata 通过将整个倒排索引加载到 JVM 堆内存来实现这一功能，但这 非常消耗内存，通常不推荐使用。\n相关指南（先读这些） #    映射基础  映射模式与最佳实践  Doc Values 参数  参数选项 #     值 说明     false 禁用 fielddata。对 text 字段执行排序/聚合会报错。默认值。   true 启用 fielddata。允许对 text 字段排序和聚合，但会大量消耗堆内存。    示例 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fielddata\u0026#34;: true } } } }  ⚠️ 警告：在生产环境中启用 fielddata 可能导致大量内存消耗，甚至触发断路器（circuit breaker）或 OOM。\n 更好的替代方案 #     需求 推荐方案 说明     对字符串做排序/聚合 使用 keyword 子字段 \u0026quot;fields\u0026quot;: { \u0026quot;raw\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; } }   对字符串做全文搜索 + 聚合 text + keyword 多字段 搜索用 text，聚合用 keyword 子字段   确实需要对分词后的词项做聚合 fielddata + fielddata_frequency_filter 过滤低频词项，减少内存开销    fielddata_frequency_filter #  如果确实需要使用 fielddata，可以通过频率过滤减少内存占用：\nPUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fielddata\u0026#34;: true, \u0026#34;fielddata_frequency_filter\u0026#34;: { \u0026#34;min\u0026#34;: 0.01, \u0026#34;max\u0026#34;: 0.5, \u0026#34;min_segment_size\u0026#34;: 500 } } } } }    参数 说明     min 词项最低频率（占比），低于此频率的词项不加载   max 词项最高频率（占比），高于此频率的词项不加载   min_segment_size 分段最小文档数，小于此值的分段不加载 fielddata    fielddata 与 doc_values 的对比 #     特性 fielddata doc_values     适用类型 text 字段 keyword、数值、日期等   存储位置 JVM 堆内存（查询时加载） 磁盘（索引时构建）   内存消耗 ⚠️ 非常大 ✅ 极低   默认状态 关闭 开启   推荐度 ❌ 不推荐 ✅ 推荐    ","subcategory":null,"summary":"","tags":null,"title":"字段数据参数（Fielddata）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/fielddata/"},{"category":null,"content":"_field_names 元数据字段 #  _field_names 字段索引包含非空值的字段名称。可以使用 exists 查询来识别指定字段是否具有非空值的文档。\n但是，只有当 doc_values 和 norms 都被禁用时，_field_names 才会索引字段名称。如果启用了 doc_values 或 norms 中的任何一个，则 exists 查询仍然可以工作，但不会依赖 _field_names 字段。\n相关指南（先读这些） #    映射基础  元数据字段  映射示例 #  PUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;_field_names\u0026#34;: { \u0026#34;enabled\u0026#34;: true }, \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;norms\u0026#34;: false }, \u0026#34;description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34;, \u0026#34;doc_values\u0026#34;: false } } } } ","subcategory":null,"summary":"","tags":null,"title":"字段名称元数据字段（_field_names）","url":"/easysearch/main/docs/features/mapping-and-analysis/metadata-field/field-names/"},{"category":null,"content":"Fields 参数（Multi-fields） #  fields 参数允许你对同一个字段以多种方式进行索引。这是 Easysearch 中最常用的映射技巧之一，典型场景是：一个字段同时需要全文搜索（text）和精确匹配/排序/聚合（keyword）。\n相关指南（先读这些） #    映射基础  映射模式与最佳实践  基本语法 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;raw\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } 上面的映射中，title 字段被索引为 text（用于全文搜索），同时通过 fields.raw 创建了一个 keyword 子字段（用于精确匹配、排序和聚合）。\n查询时使用方式：\n 全文搜索：\u0026quot;match\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;搜索引擎\u0026quot; } 精确匹配：\u0026quot;term\u0026quot;: { \u0026quot;title.raw\u0026quot;: \u0026quot;Easysearch 分布式搜索引擎\u0026quot; } 排序：\u0026quot;sort\u0026quot;: [{ \u0026quot;title.raw\u0026quot;: \u0026quot;asc\u0026quot; }] 聚合：\u0026quot;aggs\u0026quot;: { \u0026quot;titles\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;title.raw\u0026quot; } } }  常见用法 #  1. text + keyword（最典型） #  \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 256 } } }  提示：Easysearch 的动态映射默认就为字符串字段创建这种 text + keyword 的多字段结构。\n 2. 同一字段使用不同分析器 #  \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;standard\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; }, \u0026#34;pinyin\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;pinyin\u0026#34; } } } 这样可以在搜索时选择最合适的分析器：\n content：中文分词搜索 content.standard：英文/通用搜索 content.pinyin：拼音搜索  3. keyword + 不同 normalizer #  \u0026#34;status\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;lower\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;lowercase\u0026#34; } } } 注意事项 #     要点 说明     子字段继承 子字段不会继承父字段的参数，必须独立配置   存储开销 每个子字段都会独立存储索引数据，增加磁盘空间   动态映射 默认动态映射会为字符串自动创建 text + keyword 子字段   更新限制 已有索引的 fields 可以追加新子字段，但不能修改已有子字段   查询方式 使用 字段名.子字段名 的点号语法访问子字段    与 copy_to 的对比 #     特性 fields（多字段） copy_to     作用 同一字段多种索引方式 将值复制到另一个独立字段   访问方式 field.subfield 直接使用目标字段名   _source 不出现在 _source 中 不出现在 _source 中   典型场景 text + keyword 多个字段合并到一个搜索字段    ","subcategory":null,"summary":"","tags":null,"title":"多字段参数（Fields）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/fields/"},{"category":null,"content":"复杂查询 #  本章介绍子查询、JOIN 连接和条件表达式等进阶 SQL 功能。\n 子查询（Subquery） #  子查询是嵌套在另一个 SQL 语句中的完整 SELECT 语句，用括号括起来。Easysearch SQL 支持两种形式的子查询。\nWHERE IN 子查询 #  在 WHERE 子句中使用 IN 加子查询来过滤符合条件的文档。底层会被转换为等效的 JOIN 查询执行。\nPOST /_sql { \u0026#34;query\u0026#34;: \u0026#34;\u0026#34;\u0026#34; SELECT a1.firstname, a1.lastname, a1.balance FROM accounts a1 WHERE a1.account_number IN ( SELECT a2.account_number FROM accounts a2 WHERE a2.balance \u0026gt; 10000 ) \u0026#34;\u0026#34;\u0026#34; } 结果：\n   a1.firstname a1.lastname a1.balance     Amber Duke 39225   Nanette Bates 32838    FROM 子查询 #  在 FROM 子句中使用子查询作为派生表，子查询必须指定别名。\nPOST /_sql { \u0026#34;query\u0026#34;: \u0026#34;\u0026#34;\u0026#34; SELECT a.f, a.l, a.a FROM ( SELECT firstname AS f, lastname AS l, age AS a FROM accounts WHERE age \u0026gt; 30 ) AS a \u0026#34;\u0026#34;\u0026#34; } 结果：\n   f l a     Amber Duke 32   Dale Adams 33   Hattie Bond 36    FROM 子查询中可以包含聚合和 GROUP BY：\nSELECT avg_balance FROM ( SELECT AVG(balance) AS avg_balance FROM accounts GROUP BY gender, age ) AS a 也支持多层嵌套子查询：\nSELECT name FROM ( SELECT lastname AS name, age FROM ( SELECT * FROM accounts WHERE gender = \u0026#39;M\u0026#39; ) AS accounts WHERE age \u0026lt; 35 ) AS accounts  JOIN #  JOIN 子句将两个索引的数据按关联条件进行组合。\n语法结构参考：\n  INNER JOIN #  内连接返回两个索引中满足连接条件的匹配文档。INNER 关键字可省略。\nPOST /_sql { \u0026#34;query\u0026#34;: \u0026#34;\u0026#34;\u0026#34; SELECT a.account_number, a.firstname, a.lastname, e.id, e.name FROM accounts a JOIN employees_nested e ON a.account_number = e.id \u0026#34;\u0026#34;\u0026#34; } 结果：\n   a.account_number a.firstname a.lastname e.id e.name     6 Hattie Bond 6 Jane Smith     底层使用 Block Hash Join 算法：先基于第一个查询的结果构建哈希表，然后逐行探测第二个查询的结果进行匹配。\n CROSS JOIN #  交叉连接（笛卡尔积）返回两个索引所有文档的组合，不使用 ON 子句。\nPOST /_sql { \u0026#34;query\u0026#34;: \u0026#34;\u0026#34;\u0026#34; SELECT a.account_number, a.firstname, a.lastname, e.id, e.name FROM accounts a JOIN employees_nested e \u0026#34;\u0026#34;\u0026#34; } 结果（4 × 3 = 12 行）：\n   a.account_number a.firstname a.lastname e.id e.name     1 Amber Duke 3 Bob Smith   1 Amber Duke 4 Susan Smith   1 Amber Duke 6 Jane Smith   6 Hattie Bond 3 Bob Smith   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;     ⚠️ 交叉连接可能产生大量结果，在中等规模索引上也可能触发熔断器以防止内存溢出。\n LEFT OUTER JOIN #  左外连接保留左表（第一个索引）的所有文档，即使右表没有匹配记录也会返回（右表字段填充 NULL）。OUTER 关键字可省略。\nPOST /_sql { \u0026#34;query\u0026#34;: \u0026#34;\u0026#34;\u0026#34; SELECT a.account_number, a.firstname, a.lastname, e.id, e.name FROM accounts a LEFT JOIN employees_nested e ON a.account_number = e.id \u0026#34;\u0026#34;\u0026#34; } 结果：\n   a.account_number a.firstname a.lastname e.id e.name     1 Amber Duke NULL NULL   6 Hattie Bond 6 Jane Smith   13 Nanette Bates NULL NULL   18 Dale Adams NULL NULL    JOIN 类型总结 #     类型 关键字 ON 子句 说明     内连接 [INNER] JOIN ... ON 必需 只返回匹配行   交叉连接 JOIN（无 ON） 不使用 笛卡尔积   左外连接 LEFT [OUTER] JOIN ... ON 必需 保留左表所有行     当前不支持 RIGHT JOIN 和 FULL OUTER JOIN。\n  CASE 条件表达式 #  CASE 表达式用于在 SQL 中实现条件逻辑，类似于编程语言中的 if-else。\n搜索 CASE #  SELECT account_number, CASE WHEN balance \u0026gt; 30000 THEN \u0026#39;high\u0026#39; WHEN balance \u0026gt; 10000 THEN \u0026#39;medium\u0026#39; ELSE \u0026#39;low\u0026#39; END AS level FROM accounts 简单 CASE #  SELECT firstname, CASE gender WHEN \u0026#39;M\u0026#39; THEN \u0026#39;Male\u0026#39; WHEN \u0026#39;F\u0026#39; THEN \u0026#39;Female\u0026#39; ELSE \u0026#39;Unknown\u0026#39; END AS gender_label FROM accounts  相关链接 #    SQL 查询总览  基础查询语法  ","subcategory":null,"summary":"","tags":null,"title":"复杂查询","url":"/easysearch/main/docs/features/sql/complex/"},{"category":null,"content":"Copy To 参数 #  copy_to 参数允许您将多个字段的值复制到单个字段中。如果您经常跨多个字段搜索，此参数会很有用，因为这样可以达到搜索一组字段的效果。\n只有字段值被复制，而不是分析器产生的词项。原始的 _source 字段保持不变，并且可以使用 copy_to 参数将相同的值复制到多个字段。但是，字段间不支持递归复制；相反，应该直接使用 copy_to 从源字段复制到多个目标字段。\n相关指南（先读这些） #    映射基础  多字段搜索  代码样例 #  以下示例使用 copy_to 参数通过产品的名称和描述进行搜索，并将这些值复制到单个字段中：\nPUT my-products-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;copy_to\u0026#34;: \u0026#34;product_info\u0026#34; }, \u0026#34;description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;copy_to\u0026#34;: \u0026#34;product_info\u0026#34; }, \u0026#34;product_info\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34; } } } } PUT my-products-index/_doc/1 { \u0026quot;name\u0026quot;: \u0026quot;Wireless Headphones\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;High-quality wireless headphones with noise cancellation\u0026quot;, \u0026quot;price\u0026quot;: 99.99 }\nPUT my-products-index/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Bluetooth Speaker\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Portable Bluetooth speaker with long battery life\u0026quot;, \u0026quot;price\u0026quot;: 49.99 } 在此示例中，name 和 description 字段的值被复制到 product_info 字段中。现在您可以通过查询 product_info 字段来搜索产品，如下所示：\nGET my-products-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;product_info\u0026#34;: \u0026#34;wireless headphones\u0026#34; } } } 响应 #  { \u0026#34;took\u0026#34;: 20, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.9061546, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my-products-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.9061546, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Wireless Headphones\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;High-quality wireless headphones with noise cancellation\u0026#34;, \u0026#34;price\u0026#34;: 99.99 } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"复制到参数（Copy To）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/copy_to/"},{"category":null,"content":"地理形状字段类型（Geo Shape） #  geo_shape 字段类型包含地理形状，例如多边形或地理点的集合。为了索引地理形状，Easysearch 会将形状分割成三角形网格，并将每个三角形存储在 BKD 树中。这提供了 10^-7 度的精度，代表了接近完美的空间分辨率。这个过程的性能主要受到您正在索引的多边形顶点数量多少的影响。\nGeo-shapes 可以用来判断查询的形状与索引的形状的关系：\n intersects：查询的形状与索引的形状有重叠（默认） disjoint：查询的形状与索引的形状完全不重叠 within：索引的形状完全被包含在查询的形状中 contains：索引的形状完全包含查询的形状   注意：Geo-shapes 不能用于计算距离、排序、打分以及聚合。如需距离排序，请使用 geo_point 字段。\n 代码样例 #  创建一个带有地理形状字段类型的映射：\nPUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_shape\u0026#34; } } } } 格式说明 #  地理形状可以用以下格式索引：\n  GeoJSON  Well-Known Text (WKT)   在 GeoJSON 和 WKT 中，坐标必须在坐标数组中按照 经度, 纬度 的顺序指定。注意在这种格式中经度是在前面的。\n 地理形状类型 #  下表描述了可能的地理形状类型以及它们与 GeoJSON 和 WKT 类型的关系。\n   Easysearch 类型 GeoJSON 类型 WKT 类型 描述     point Point POINT 由纬度和经度指定的地理点。Easysearch 使用世界大地测量系统 (WGS84) 坐标。   linestring LineString LINESTRING 由两个或更多点指定的线。可以是直线或连接的线段路径。   polygon Polygon POLYGON 由坐标形式的顶点列表指定的多边形。多边形必须是闭合的，这意味着最后一个点必须与第一个点相同。因此，要创建一个 n 边形，需要 n+1 个顶点。最少需要四个顶点，这会创建一个三角形。   multipoint MultiPoint MULTIPOINT 不连接的离散相关点的数组。   multilinestring MultiLineString MULTILINESTRING 线串的数组。   multipolygon MultiPolygon MULTIPOLYGON 多边形的数组。   geometrycollection GeometryCollection GEOMETRYCOLLECTION 可能是不同类型的地理形状的集合。   envelope N/A BBOX 由左上和右下顶点指定的边界矩形。    Point 点位 #  一个点代表着由经度和纬度指定的单个坐标对。\n在 GeoJSON 格式中索引一个点：\nPUT testindex/_doc/1 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;point\u0026#34;, \u0026#34;coordinates\u0026#34; : [74.0060, 40.7128] } } 在 WKT 格式中索引一个点数据：\nPUT testindex/_doc/1 { \u0026#34;location\u0026#34; : \u0026#34;POINT (74.0060 40.7128)\u0026#34; } Linestring 线串 #  一个线串是由两个或更多点指定的线。如果点是共线的，则线串是直线。否则，线串表示由线段组成的路径。\n在 GeoJSON 格式中索引一个线串：\nPUT testindex/_doc/2 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;linestring\u0026#34;, \u0026#34;coordinates\u0026#34; : [[74.0060, 40.7128], [71.0589, 42.3601]] } } 在 WKT 格式中索引一个线串：\nPUT testindex/_doc/2 { \u0026#34;location\u0026#34; : \u0026#34;LINESTRING (74.0060 40.7128, 71.0589 42.3601)\u0026#34; } Polygon 多边形 #  一个多边形是由坐标形式的顶点列表指定的。多边形必须是闭合的，这意味着最后一个点必须与第一个点相同。在下面的例子中，创建了一个三角形，使用了四个点。\n GeoJSON 要求您以逆时针顺序列出多边形的顶点。WKT 不对顶点顺序施加任何限制。\n 在 GeoJSON 格式中索引一个多边形（三角形）：\nPUT testindex/_doc/3 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [ [74.0060, 40.7128], [73.7562, 42.6526], [71.0589, 42.3601], [74.0060, 40.7128] ] ] } } 在 WKT 格式中索引一个多边形（三角形）：\nPUT testindex/_doc/3 { \u0026#34;location\u0026#34; : \u0026#34;POLYGON ((74.0060 40.7128, 71.0589 42.3601, 73.7562 42.6526, 74.0060 40.7128))\u0026#34; } 多边形可以有内部的洞。在这种情况下，坐标字段将包含多个数组。第一个数组表示外部多边形，每个后续数组表示一个洞。洞表示为多边形，并指定为坐标数组。\n GeoJSON 要求您以逆时针顺序列出多边形的顶点，并以顺时针顺序列出洞的顶点。WKT 不对顶点顺序施加任何限制。\n 在 GeoJSON 格式中索引一个多边形（三角形）和一个三角形洞：\nPUT testindex/_doc/4 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [ [74.0060, 40.7128], [73.7562, 42.6526], [71.0589, 42.3601], [74.0060, 40.7128] ], [ [72.6734, 41.7658], [73.0515, 41.5582], [72.6506, 41.5623], [72.6734, 41.7658] ] ] } } 在 WKT 格式中索引一个多边形（三角形）和一个三角形洞：\nPUT testindex/_doc/4 { \u0026#34;location\u0026#34; : \u0026#34;POLYGON ((74.0060 40.7128, 71.0589 42.3601, 73.7562 42.6526, 74.0060 40.7128), (72.6734 41.7658, 72.6506 41.5623, 73.0515 41.5582, 72.6734 41.7658))\u0026#34; } 您可以通过以顺时针或逆时针顺序列出顶点来指定 Easysearch 中的多边形。这对于不跨越日期线（小于 180°）的多边形来说是有效的。然而，跨越日期线（大于 180°）的多边形可能是模糊的，因为 WKT 不对顶点顺序施加任何限制。因此，您必须通过以逆时针顺序列出顶点来指定跨越日期线的多边形。\n您可以定义一个 orientation 方向参数来指定顶点遍历顺序：\nPUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_shape\u0026#34;, \u0026#34;orientation\u0026#34; : \u0026#34;left\u0026#34; } } } } 随后索引的文档可以覆盖 orientation 设置：\nPUT testindex/_doc/3 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;polygon\u0026#34;, \u0026#34;orientation\u0026#34; : \u0026#34;cw\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [[74.0060, 40.7128], [71.0589, 42.3601], [73.7562, 42.6526], [74.0060, 40.7128]] ] } } Multipoint 多点位 #  一个多点是由不连接的离散相关点组成的数组。\n在 GeoJSON 格式中索引一个多点：\nPUT testindex/_doc/6 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;multipoint\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [74.0060, 40.7128], [71.0589, 42.3601] ] } } 在 WKT 格式中索引一个多点：\nPUT testindex/_doc/6 { \u0026#34;location\u0026#34; : \u0026#34;MULTIPOINT (74.0060 40.7128, 71.0589 42.3601)\u0026#34; } Multilinestring 多线串 #  一个多线串是由线串组成的数组。\n在 GeoJSON 格式中索引一个多线串：\nPUT testindex/_doc/2 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;multilinestring\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [[74.0060, 40.7128], [71.0589, 42.3601]], [[73.7562, 42.6526], [72.6734, 41.7658]] ] } } 在 WKT 格式中索引一个多线串：\nPUT testindex/_doc/2 { \u0026#34;location\u0026#34; : \u0026#34;MULTILINESTRING ((74.0060 40.7128, 71.0589 42.3601), (73.7562 42.6526, 72.6734 41.7658))\u0026#34; } Multipolygon 多个多边形 #  多个多边形是由多边形组成的数组。在这个例子中，第一个多边形包含一个洞，第二个不包含。\n在 GeoJSON 格式中索引多个多边形：\nPUT testindex/_doc/4 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;multipolygon\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [ [ [74.0060, 40.7128], [73.7562, 42.6526], [71.0589, 42.3601], [74.0060, 40.7128] ], [ [73.0515, 41.5582], [72.6506, 41.5623], [72.6734, 41.7658], [73.0515, 41.5582] ] ], [ [ [73.9146, 40.8252], [73.8871, 41.0389], [73.6853, 40.9747], [73.9146, 40.8252] ] ] ] } } 在 WKT 格式中索引多个多边形：\nPUT testindex/_doc/4 { \u0026#34;location\u0026#34; : \u0026#34;MULTIPOLYGON (((74.0060 40.7128, 71.0589 42.3601, 73.7562 42.6526, 74.0060 40.7128), (72.6734 41.7658, 72.6506 41.5623, 73.0515 41.5582, 72.6734 41.7658)), ((73.9146 40.8252, 73.6853 40.9747, 73.8871 41.0389, 73.9146 40.8252)))\u0026#34; } Geometry collection 几何集合 #  一个几何集合是可能是不同类型的地理形状的集合。\n在 GeoJSON 格式中索引一个几何集合：\nPUT testindex/_doc/7 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34;: \u0026#34;geometrycollection\u0026#34;, \u0026#34;geometries\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;point\u0026#34;, \u0026#34;coordinates\u0026#34;: [74.0060, 40.7128] }, { \u0026#34;type\u0026#34;: \u0026#34;linestring\u0026#34;, \u0026#34;coordinates\u0026#34;: [[73.7562, 42.6526], [72.6734, 41.7658]] } ] } } 在 WKT 格式中索引一个几何集合：\nPUT testindex/_doc/7 { \u0026#34;location\u0026#34; : \u0026#34;GEOMETRYCOLLECTION (POINT (74.0060 40.7128), LINESTRING(73.7562 42.6526, 72.6734 41.7658))\u0026#34; } Envelope 边界矩阵 #  一个边界矩形是由左上和右下顶点指定的。在 GeoJSON 格式中，边界矩形的坐标是 [[minLon, maxLat], [maxLon, minLat]]。\n在 GeoJSON 格式中索引一个边界矩形：\nPUT testindex/_doc/2 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;envelope\u0026#34;, \u0026#34;coordinates\u0026#34; : [[71.0589, 42.3601], [74.0060, 40.7128]] } } 在 WKT 格式中，使用 BBOX (minLon, maxLon, maxLat, minLat)。\n在 WKT 格式中索引一个边界矩形：\nPUT testindex/_doc/8 { \u0026#34;location\u0026#34; : \u0026#34;BBOX (71.0589, 74.0060, 42.3601, 40.7128)\u0026#34; } 参数说明 #  以下表格列出了地理形状字段类型接受的参数。所有参数都是可选的。\n   参数 描述     coerce 一个布尔值，指定是否自动关闭未闭合的线环。默认为 false。   ignore_malformed 一个布尔值，指定是否忽略格式错误的 GeoJSON 或 WKT 地理形状，而不是抛出异常。默认为 false（抛出异常）。   ignore_z_value 特定于具有三个坐标的点。如果 ignore_z_value 为 true，则第三个坐标不会被索引，但仍会存储在 _source 字段中。如果 ignore_z_value 为 false，则会抛出异常。默认为 true。   orientation 指定地理形状坐标列表中顶点的遍历顺序。orientation 可以取以下值：1. RIGHT：逆时针。使用以下字符串（大写或小写）指定 RIGHT 方向：right、counterclockwise、ccw。 2. LEFT：顺时针。使用以下字符串（大写或小写）指定 LEFT 方向：left、clockwise、cw。这个值可以被单个文档覆盖。 默认为 RIGHT。    精度与距离误差 #  在映射中可以通过两个参数控制形状的索引精度：\n精度（precision） #  precision 参数控制生成的 geohash 最大长度。默认精度为 9，等同于约 5m × 5m 的 geohash 单元。可以使用数字（geohash 长度）或距离值（如 50m、2km）：\nPUT /attractions { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_shape\u0026#34;, \u0026#34;precision\u0026#34;: \u0026#34;100m\u0026#34; } } } }  精度越低，需要索引的单元越少，检索越快，但形状准确性越差。减少 1-2 个等级的精度就能带来明显的性能收益。\n 距离误差（distance_error_pct） #  distance_error_pct 指定地理形状可以接受的最大错误率，默认为 0.025（2.5%）。这意味着大的形状（如国家）比小的形状（如纪念碑）容许更模糊的边界：\nPUT /areas { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;boundary\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_shape\u0026#34;, \u0026#34;distance_error_pct\u0026#34;: 0.05 } } } }  容许更大的错误率，对应需要索引的单元就越少，索引体积和查询开销也越低。\n 预索引形状 #  当查询中使用的形状很大且复杂时，在每次查询中传递完整坐标代价昂贵。可以将形状预先索引到一个独立索引中，查询时引用形状 ID：\nPUT /shapes/_doc/amsterdam { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34;: [[ [4.88330,52.38617], [4.87463,52.37254], [4.87875,52.36369], [4.88939,52.35850], [4.89840,52.35755], [4.91909,52.36217], [4.92656,52.36594], [4.93368,52.36615], [4.93342,52.37275], [4.92690,52.37632], [4.88330,52.38617] ]] } } 查询时引用预索引形状：\nGET /attractions/_search { \u0026#34;query\u0026#34;: { \u0026#34;geo_shape\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;indexed_shape\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;shapes\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;amsterdam\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;location\u0026#34; } } } } } 相关文档 #    Geo Shape 查询  地理位置搜索  地理位置实践  ","subcategory":null,"summary":"","tags":null,"title":"地理形状字段类型（Geo Shape）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/geo-field-type/geo-shape/"},{"category":null,"content":"地理坐标点字段类型（Geo Point） #  geo_point 字段类型包含由纬度（latitude）和经度（longitude）指定的地理点。地理坐标点可以用来计算两个坐标间的距离，判断一个坐标是否在一个区域中，或在聚合中。\n代码示例 #  创建一个带有 Geopoint 地理点字段类型的映射：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; } } } }  地理坐标点不能被动态映射自动检测，必须显式声明字段类型为 geo_point。\n 地理点格式 #  Geopoint 地理点可以用以下格式索引：\n 包含纬度和经度的对象  PUT testindex1/_doc/1 { \u0026#34;point\u0026#34;: { \u0026#34;lat\u0026#34;: 40.71, \u0026#34;lon\u0026#34;: 74.00 } } 写入包含纬度,经度 的文档  PUT testindex1/_doc/2 { \u0026#34;point\u0026#34;: \u0026#34;40.71,74.00\u0026#34; } geohash 格式的文档  PUT testindex1/_doc/3 { \u0026#34;point\u0026#34;: \u0026#34;txhxegj0uyp3\u0026#34; } [经度, 纬度] 格式的数组  PUT testindex1/_doc/4 { \u0026#34;point\u0026#34;: [74.00, 40.71] }  Well-Known Text POINT 格式，格式为 \u0026ldquo;POINT(经度 纬度)\u0026rdquo;  PUT testindex1/_doc/5 { \u0026#34;point\u0026#34;: \u0026#34;POINT (74.00 40.71)\u0026#34; }  注意坐标顺序：地理坐标点用字符串形式表示时是纬度在前，经度在后（\u0026quot;latitude,longitude\u0026quot;），而数组形式表示时是经度在前，纬度在后（[longitude,latitude]）——顺序刚好相反。这是为了兼容 GeoJSON 格式规范。\n 参数说明 #  下表列出了 Geopoint 地理点字段类型接受的参数。所有参数都是可选的。\n   参数 描述     ignore_malformed 布尔值，指定是否忽略格式错误的值而不抛出异常。纬度的有效值范围是 [-90, 90]。经度的有效值范围是 [-180, 180]。默认为 false。   ignore_z_value 特定于具有三个坐标的点。如果 ignore_z_value 为 true，则第三个坐标不会被索引，但仍会存储在 _source 字段中。如果 ignore_z_value 为 false，则会抛出异常。   null_value 用于替代 null 的值。必须与字段类型相同。如果未指定此参数，当字段值为 null 时，该字段将被视为缺失。默认为 null。    支持的查询 #  geo_point 字段支持以下地理查询：\n   查询类型 说明      Geo Bounding Box 矩形区域过滤，最高效的地理过滤器    Geo Distance 指定距离内的圆形范围过滤    Geo Polygon 多边形区域过滤     性能提示：geo_bounding_box 是最高效的地理过滤器。建议先用矩形过滤排除大部分文档，再配合更精确的距离过滤。布尔过滤器 bool 会自动优先执行高效过滤。\n 按距离排序 #  检索结果可以按与指定点的距离排序：\nGET /attractions/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;geo_bounding_box\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;top_left\u0026#34;: { \u0026#34;lat\u0026#34;: 40.8, \u0026#34;lon\u0026#34;: -74.0 }, \u0026#34;bottom_right\u0026#34;: { \u0026#34;lat\u0026#34;: 40.4, \u0026#34;lon\u0026#34;: -73.0 } } } } } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;_geo_distance\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.715, \u0026#34;lon\u0026#34;: -73.998 }, \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;km\u0026#34;, \u0026#34;distance_type\u0026#34;: \u0026#34;plane\u0026#34; } } ] }  unit 决定返回结果中 sort 值的距离单位（km、m、mi 等） distance_type 可选 arc（默认，精度高）或 plane（速度快，靠近两极精度下降） 支持对多个坐标点排序，使用 sort_mode 指定取最小（min）、最大（max）或平均（avg）距离  按距离打分 #  如果距离只是排序因素之一（还需结合全文匹配、流行度等），可在 function_score 查询中使用距离衰减函数：\nGET /attractions/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pizza\u0026#34; } }, \u0026#34;functions\u0026#34;: [ { \u0026#34;gauss\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;origin\u0026#34;: { \u0026#34;lat\u0026#34;: 40.715, \u0026#34;lon\u0026#34;: -73.998 }, \u0026#34;scale\u0026#34;: \u0026#34;2km\u0026#34; } } } ] } } } function_score 还可以配合 rescore 限制只对前 N 条结果计算距离，提升性能。\n相关文档 #    地理位置搜索  地理位置实践  ","subcategory":null,"summary":"","tags":null,"title":"地理坐标点字段类型（Geo Point）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/geo-field-type/geo-point/"},{"category":null,"content":"Enabled 参数 #  enabled 参数允许您控制 Easysearch 是否解析字段的内容。此参数可以应用于顶级映射定义和对象字段。\nenabled 参数接受以下值：\n   参数 描述     true 字段被解析和索引。默认值为 true。   false 字段不被解析或索引，但仍可从 _source 字段中检索。当 enabled 设置为 false 时，Easysearch 将字段的值存储在 _source 字段中，但不索引或解析其内容。这对于您想要存储但不需要搜索、排序或聚合的字段很有用。    相关指南（先读这些） #    映射基础  映射模式  示例：使用 enabled 参数 #  在以下示例请求中，session_data 字段被禁用。Easysearch 将其内容存储在 _source 字段中，但不对其进行索引或解析：\nPUT my-index-002 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;user_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;last_updated\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;session_data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;enabled\u0026#34;: false } } } } 使用场景 #  存储原始 JSON 但不索引 #  enabled: false 非常适合那些需要随文档一起返回但不需要被搜索的数据：\nPUT app_logs { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;level\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;raw_request\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;enabled\u0026#34;: false } } } } 在这个例子中，raw_request 中存储了完整的 HTTP 请求体，便于调试时查看原始数据，但不会对其内容建立索引。\n禁用整个映射的索引 #  还可以在顶层映射上使用 enabled: false，使整个索引变成纯存储模式：\nPUT archive { \u0026#34;mappings\u0026#34;: { \u0026#34;enabled\u0026#34;: false } } 这会让索引接受任意 JSON 数据而不解析字段，适合纯归档用途。\n注意事项 #     行为 enabled: true（默认） enabled: false     搜索（match/term/\u0026hellip;） ✅ 支持 ❌ 不支持   排序和聚合 ✅ 支持 ❌ 不支持   从 _source 获取 ✅ 支持 ✅ 支持   动态映射 ✅ 自动检测字段类型 ❌ 不检测     注意：enabled 参数在索引创建后无法修改。如需更改，必须重建索引。\n ","subcategory":null,"summary":"","tags":null,"title":"启用参数（Enabled）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/enabled/"},{"category":null,"content":"K-NN 向量字段类型 #  相关指南（先读这些） #    向量搜索  向量字段建模  映射基础  关于向量 #  在索引文档和运行查询时都需要指定向量类型。在这两种情况下，您都使用相同的 JSON 结构来定义向量类型。每个向量类型还有一个简写形式，这在使用不支持嵌套文档的工具时会很方便。以下示例展示了如何在索引向量时指定它们。\nknn_dense_float_vector 密集向量类型 #  假设您已经定义了一个映射，其中 my_vec 的类型为 knn_dense_float_vector。\nPOST /my-index/_doc { \u0026#34;my_vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.1, 0.2, 0.3, ...] # 1 } } POST /my-index/_doc { \u0026#34;my_vec\u0026#34;: [0.1, 0.2, 0.3, ...] # 2 } 说明 #  1\t向量中所有浮点值的 JSON 列表。长度应与映射中的dims匹配。 2\t#1 的简写形式。\nknn_sparse_bool_vector 稀疏向量类型 #  假设您已经定义了一个映射，其中 my_vec 的类型为 knn_sparse_bool_vector。\nPOST /my-index/_doc { \u0026#34;my_vec\u0026#34;: { \u0026#34;true_indices\u0026#34;: [1, 3, 5, ...], # 1 \u0026#34;total_indices\u0026#34;: 100, # 2 } } POST /my-index/_doc { \u0026#34;my_vec\u0026#34;: [[1, 3, 5, ...], 100] # 3 } 说明 #  1\t向量中为 true 的索引的 JSON 列表。 2\t索引的向量总数。这应该与映射中的dims匹配。 3\t#1 和 #2 的简写形式。一个包含两个项目的列表，第一个项目是 true_indices，第二个是 total_indices。\n映射 #  在索引向量数据之前，您需要先定义一个映射，指定向量数据类型、索引模型和模型参数。这决定了索引向量支持哪些查询。\n基本结构 #  基本映射结构如下：\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { # 1 \u0026#34;my_vec\u0026#34;: { # 2 \u0026#34;type\u0026#34;: \u0026#34;knn_sparse_bool_vector\u0026#34;, # 3 \u0026#34;knn\u0026#34;: { # 4 \u0026#34;dims\u0026#34;: 100, # 5 \u0026#34;model\u0026#34;: \u0026#34;exact\u0026#34;, # 6 ... # 7 } } } } 说明 #  1\t文档字段的字典。与 PUT Mapping API 相同。 2\t向量的字段名称。 3\t要存储的向量类型。 4\tknn 的设置。 5\t向量的维度。存储在此字段 my_vec 中的所有向量必须具有相同的维度。 6\t模型类型。这和模型参数将决定您可以运行什么样的搜索。请参见下面的模型部分。 7\t额外的模型参数。请参见下面的模型部分。\nknn_sparse_bool_vector 数据类型 #  这种类型针对每个索引为 true 或 false 的向量进行了优化,大部分的情况为 false。例如，您可以表示文档的词袋编码，其中每个索引对应词汇表中的一个词，而任何单个文档只包含所有词的很小一部分。在内部，Knn 通过只存储 true 的数据来节省空间。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_sparse_bool_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 25000, # 2 ... # 3 } } } } 说明 #  1\t类型名称。 2\t向量的维度。 3\t额外的模型参数。请参见下面的模型部分。\nknn_dense_float_vector 数据类型 #  这种类型针对每个向量都是浮点数、所有索引都有值且维度通常不超过约 1000 的向量进行了优化。例如，您可以存储词嵌入或图像向量。在内部，Knn 使用 Java Float 类型来存储值。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 100, # 2 ... # 3 } } } } 说明 #  1\t类型名称。 2\t向量的维度。不应超过几千。如果超过，请考虑进行某种维度降低。 3\t额外的模型参数。请参见下面的模型部分。\n特定模型映射 #  特定模型映射允许您运行特定模型的搜索。这些搜索不利用任何索引结构，运行时间复杂度为 O(n^2)，其中 n 是文档总数。\n使用此配置时，您不需要提供任何 \u0026quot;model\u0026quot;: \u0026quot;...\u0026quot; 值或任何模型参数。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_(dense_float | sparse_bool)_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 100, # 2 } } } } 说明 #  1\t向量数据类型。支持 dense float 和 sparse bool 两种类型 2\t向量维度。\nJaccard LSH 映射 #  使用 Minhash 算法对稀疏布尔向量进行哈希和存储，以支持近似 Jaccard 相似度查询。\n该实现受到 《Mining Massive Datasets》第 3 章、Spark MinHash 实现、 tdebatty/java-LSH Github 项目和 Minhash for Dummies 博客文章的影响。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_sparse_bool_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 25000, # 2 \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, # 3 \u0026#34;similarity\u0026#34;: \u0026#34;jaccard\u0026#34;, # 4 \u0026#34;L\u0026#34;: 99, # 5 \u0026#34;k\u0026#34;: 1 # 6 } } } } 说明 #  1\t向量数据类型。必须是稀疏布尔向量。 2\t向量维度。 3\t模型类型。 4\t相似度度量。 5\t哈希表的数量。通常增加这个值会提高召回率。 6\t组合形成单个哈希值的哈希函数数量。通常，增加这个值会提高精确度。\n汉明 LSH 映射 #  使用 位采样算法对稀疏布尔向量进行哈希和存储，以支持近似汉明相似度查询。\n与标准位采样方法的唯一区别是它采样并组合 k 个位来形成单个哈希值。例如，如果设置 L = 100，k = 3，它会从向量中采样 100 * 3 = 300 个位，并将每 3 个位连接起来形成每个哈希值，总共生成 100 个哈希值。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_sparse_bool_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 25000, # 2 \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, # 3 \u0026#34;similarity\u0026#34;: \u0026#34;hamming\u0026#34;, # 4 \u0026#34;L\u0026#34;: 99, # 5 \u0026#34;k\u0026#34;: 2 # 6 } } } } 说明 #  1\t向量数据类型。必须是稀疏布尔向量。 2\t向量维度。 3\t模型类型。 4\t相似度度量。 5\t哈希表的数量。通常增加这个值会提高召回率。 6\t组合形成单个哈希值的哈希函数数量。通常增加这个值会提高精确度。\n余弦 LSH 映射 #  使用 随机投影算法对密集浮点向量进行哈希和存储，以支持近似余弦相似度查询。\n参考 Mining Massive Datasets第三章\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 100, # 2 \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, # 3 \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, # 4 \u0026#34;L\u0026#34;: 99, # 5 \u0026#34;k\u0026#34;: 1 # 6 } } } } 说明 #  1\t向量数据类型。必须是密集浮点向量。 2\t向量维度。 3\t模型类型。 4\t相似度度量。 5\t哈希表的数量。通常增加这个值会提高召回率。 6\t组合形成单个哈希值的哈希函数数量。通常，增加这个值会提高精确度。\nL2 LSH 映射 #  使用 稳定分布方法对密集浮点向量进行哈希和存储，以支持近似 L2（欧几里得）相似度查询。\n参考 Mining Massive Datasets第三章\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 100, # 2 \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, # 3 \u0026#34;similarity\u0026#34;: \u0026#34;l2\u0026#34;, # 4 \u0026#34;L\u0026#34;: 99, # 5 \u0026#34;k\u0026#34;: 1, # 6 \u0026#34;w\u0026#34;: 3 # 7 } } } } 说明 #  1\t向量数据类型。必须是密集浮点向量。 2\t向量维度。 3\t模型类型。 4\t相似度度量。 5\t哈希表的数量。通常增加这个值会提高召回率。 6\t组合形成单个哈希值的哈希函数数量。通常增加这个值会提高精确度。 7\t整数桶宽度。这决定了两个向量在投影到第三个公共向量上时必须多么接近，才能共享一个哈希值。典型值是低位整数。\n排列 LSH 映射 #  使用 Amato 等人在 《Large-Scale Image Retrieval with Elasticsearch》中描述的模型。\n该模型通过向量中绝对值最大的 k 个索引（向量中的位置）来描述向量。直觉是，每个索引对应于某个潜在概念，绝对值高的索引比绝对值低的索引携带更多关于其各自概念的信息。该方法的研究主要集中在余弦相似度上，尽管实现也支持 L1 和 L2。\n例子 #  向量 [10, -2, 0, 99, 0.1, -8, 42, -13, 6, 0.1] 中 k = 4 的索引为 [4, 7, -8, 1]。索引是 1 索引的，负值的索引被否定（因此 -8）。索引可以根据其排名可选地重复。在本例中，索引将被重复 [4, 4, 4, 4, 7, 7, 7, -8, -8, 1]。索引 4 具有最高的绝对值，因此它被重复 k - 0 = 4 次。索引 7 具有第二高的绝对值，因此它被重复 k - 1 = 3 次，依此类推。搜索算法将得分计算为存储向量表示和查询向量表示的交集的大小。因此，对于表示为 [2, 2, 2, 2, 7, 7, 7, 4, 4, 5] 的查询向量，交集为 [7, 7, 7, 4, 4]，得分为 5。在一些实验中，重复实际上降低了召回率，因此建议您尝试使用和不使用重复。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 100, # 2 \u0026#34;model\u0026#34;: \u0026#34;permutation_lsh\u0026#34;, # 3 \u0026#34;k\u0026#34;: 10, # 4 \u0026#34;repeating\u0026#34;: true # 5 } } } } 说明 #  1\t向量数据类型。必须是密集浮点向量。 2\t向量维度。 3\t模型类型。 4\t选择的顶部索引数量。 5\t是否根据其排名重复索引。请参见上面的重复注释。\n","subcategory":null,"summary":"","tags":null,"title":"向量字段类型（K-NN）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/knn/"},{"category":null,"content":"Word Delimiter 分词过滤器 #  word_delimiter 分词过滤器用于根据预定义的字符拆分词元，还能根据可定制规则对词元进行可选的规范化处理。\n 注意：我们建议尽可能使用 word_delimiter_graph 过滤器而非 word_delimiter 过滤器，因为 word_delimiter 过滤器有时会生成无效的词元图。\n  提示：word_delimiter 过滤器可用于从零件编号或产品 ID 等复杂标识符中去除标点符号。在这种情况下，它最好与 keyword 分词器配合使用。对于带连字符的单词，建议使用 synonym_graph 分词过滤器而非 word_delimiter 过滤器，因为用户搜索这些词项时，常常既会搜索带连字符的形式，也会搜索不带连字符的形式。\n 相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  默认情况下，该过滤器应用以下规则：\n   描述 输入 输出     将非字母数字字符视为分隔符 ultra-fast ultra, fast   去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder   当字母大小写发生转换时拆分词元 Easysearch Easy, search   当字母和数字之间发生转换时拆分词元 T1000 T, 1000   去除词元末尾的所有格形式（\u0026lsquo;s） John's John     重要提示：不要将该过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并影响诸如 catenate_all 或 preserve_original 等选项的效果。我们建议将此过滤器与关键(keyword)字分词器或空白(whitespace)分词器配合使用。\n 参数说明 #  你可以使用以下参数配置单词分隔符分词过滤器。\n   参数 必需/可选 数据类型 描述     catenate_all 可选 布尔值 从一系列字母数字部分生成连接后的词元。例如，quick-fast-200 会变成 [ quickfast200, quick, fast, 200 ]。默认值为 false。   catenate_numbers 可选 布尔值 连接数字序列。例如，10-20-30 会变成 [ 102030, 10, 20, 30 ]。默认值为 false。   catenate_words 可选 布尔值 连接字母单词。例如，high-speed-level 会变成 [ highspeedlevel, high, speed, level ]。默认值为 false。   generate_number_parts 可选 布尔值 如果为 true，输出中会包含纯数字词元（仅由数字组成的词元）。默认值为 true。   generate_word_parts 可选 布尔值 如果为 true，输出中会包含纯字母词元（仅由字母字符组成的词元）。默认值为 true。   preserve_original 可选 布尔值 在输出中，除了生成的词元外，还保留原始词元（可能包含非字母数字分隔符）。例如，auto-drive-300 会变成 [ auto - drive - 300, auto, drive, 300 ]。如果为 true，该过滤器会生成索引不支持的多位置词元，因此请勿在索引分词器中使用此过滤器，或者在该过滤器之后使用 flatten_graph 过滤器。默认值为 false。   protected_words 可选 字符串数组 指定不应被拆分的词元。   protected_words_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含不应被分隔的词元，词元需分行列出。   split_on_case_change 可选 布尔值 在连续字母大小写不同（一个为小写，另一个为大写）的位置拆分词元。例如，Easysearch 会变成 [ Easy, search ]。默认值为 true。   split_on_numerics 可选 布尔值 在连续字母和数字的位置拆分词元。例如，v8engine 会变成 [ v, 8, engine ]。默认值为 true。   stem_english_possessive 可选 布尔值 去除英语所有格结尾，如 \u0026lsquo;s。默认值为 true。   type_table 可选 字符串数组 一个自定义映射，用于指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。例如，要将连字符（-）视为字母数字字符，可指定 [\u0026quot;- =\u0026gt; ALPHA\u0026quot;]，这样单词就不会在连字符处拆分。有效类型有：\n- ALPHA：字母\n- ALPHANUM：字母数字\n- DIGIT：数字\n- LOWER：小写字母\n- SUBWORD_DELIM：非字母数字分隔符\n- UPPER：大写字母   type_table_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含自定义字符映射。该映射指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。有关有效类型，请参阅 type_table。    参考样例 #  以下示例请求创建了一个名为 my-custom-index 的新索引，并配置了一个带有单词分隔符过滤器（word_delimiter）的分词器。\nPUT /my-custom-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;custom_word_delimiter_filter\u0026#34; ] } }, \u0026#34;filter\u0026#34;: { \u0026#34;custom_word_delimiter_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;word_delimiter\u0026#34;, \u0026#34;split_on_case_change\u0026#34;: true, \u0026#34;split_on_numerics\u0026#34;: true, \u0026#34;stem_english_possessive\u0026#34;: true } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-custom-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;FastCar\u0026#39;s Model2023\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Fast\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;Car\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;Model\u0026#34;, \u0026#34;start_offset\u0026#34;: 10, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;2023\u0026#34;, \u0026#34;start_offset\u0026#34;: 15, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 } ] } ","subcategory":null,"summary":"","tags":null,"title":"单词分隔分词过滤器（Word Delimiter）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/word-delimiter/"},{"category":null,"content":"Dynamic 参数 #  dynamic 参数指定是否可以动态地将新检测到的字段添加到映射中。它接受下表中列出的参数。\n   参数 描述     true 指定可以动态地将新字段添加到映射中。默认值为 true。   false 指定不能动态地将新字段添加到映射中。如果检测到新字段，则不会对其进行索引或搜索，但可以从 _source 字段中检索。   strict 当检测到文档中有新字段时，索引操作失败，抛出异常。    相关指南（先读这些） #    映射基础  映射模式  示例：创建 dynamic 设置为 true 的索引 #  通过以下命令创建一个 dynamic 设置为 true 的索引：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: true } } 通过以下命令，索引一个包含两个字符串字段的对象字段 patient 的文档：\nPUT testindex1/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;123456\u0026#34; } } 通过以下命令确认映射按预期工作：\nGET testindex1/_mapping 对象字段 patient 和两个子字段 name 和 id 被添加到映射中，如以下响应所示：\n{ \u0026#34;testindex1\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 256 } } }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 256 } } } } } } } } } 示例：创建 dynamic 设置为 false 的索引 #  通过以下命令，创建一个具有显式映射且 dynamic 设置为 false 的索引：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: false, \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } 通过以下命令，索引一个文档，其中包含两个字符串字段的对象字段 patient和额外未映射的 room 和 floor\nPUT testindex1/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;123456\u0026#34; }, \u0026#34;room\u0026#34;: \u0026#34;room1\u0026#34;, \u0026#34;floor\u0026#34;: \u0026#34;1\u0026#34; } 通过以下命令确认映射是否按预期工作：\nGET testindex1/_mapping 以下返回内容显示新字段 room 和 floor 未添加到映射中，映射保持不变：\n{ \u0026#34;testindex1\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } } 示例：创建 dynamic 设置为 strict 的索引 #  通过以下命令，创建一个具有显式映射且 dynamic 设置为 strict 的索引：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: \u0026#34;strict\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } 通过以下命令，索引一个文档，其中包含两个字符串字段的对象字段 patient 和额外未映射的 room 和 floor\nPUT testindex1/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;123456\u0026#34; }, \u0026#34;room\u0026#34;: \u0026#34;room1\u0026#34;, \u0026#34;floor\u0026#34;: \u0026#34;1\u0026#34; } 注意会抛出异常，如下所示：\n{ \u0026#34;error\u0026#34;: { \u0026#34;root_cause\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;strict_dynamic_mapping_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;mapping set to strict, dynamic introduction of [room] within [_doc] is not allowed\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;strict_dynamic_mapping_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;mapping set to strict, dynamic introduction of [room] within [_doc] is not allowed\u0026#34; }, \u0026#34;status\u0026#34;: 400 } ","subcategory":null,"summary":"","tags":null,"title":"动态映射参数（Dynamic）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/dynamic/"},{"category":null,"content":"Boost 参数 #   ⚠️ 已弃用：映射级别的 boost 参数已被弃用，将在未来版本中移除。请改为在查询时通过 boost 参数控制字段权重。\n boost 映射参数用于在搜索查询期间增加或减少字段的相关性分数。它允许你在计算文档的整体相关性分数时，对特定字段应用更多或更少的权重。默认值为 1.0。\nboost 参数作为字段分数的乘数应用。例如，如果一个字段的 boost 值为 2，则该字段的分数的权重将翻倍。相反，boost 值为 0.5 将使该字段的分数的权重减半。\n相关指南（先读这些） #    映射基础  相关性：加权与调参  代码样例 #  以下是在 Easysearch 映射中使用 boost 参数的示例：\nPUT my-index1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;boost\u0026#34;: 2 }, \u0026#34;description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;boost\u0026#34;: 1 }, \u0026#34;tags\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;boost\u0026#34;: 1.5 } } } } 在此示例中，title 字段的提升值为 2，这意味着它对整体相关性分数的权重是描述字段（提升值为 1）的两倍。tags 字段的提升值为 1.5，因此它的权重是描述字段的一倍半。\n当您想要对某些字段赋予更多权重时，boost 参数特别有用。例如，您可能想要将 title 字段的权重提升得比 description 字段更高，因为标题更能反映文档的相关性。\n推荐做法：不要在映射中设置 boost，而是在查询时使用 boost 参数：\nGET my-index1/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;Easysearch\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title^2\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;tags^1.5\u0026#34;] } } } 这种方式更灵活，可以随时调整权重而无需重建索引。\nboost 参数是一个乘法因子，而不是加法因子。这意味着与具有较低权重值的字段相比，具有较高权重值的字段对整体相关性分数的影响将不成比例地大。在使用 boost 参数时，建议您从小值（1.5 或 2）开始，并测试其对搜索结果的影响。过高的权重值可能会扭曲相关性分数，并导致意外或不理想的搜索结果。\n","subcategory":null,"summary":"","tags":null,"title":"加权参数（Boost）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/boost/"},{"category":null,"content":"Alias 别名字段类型 #  别名字段类型为现有字段创建另一个名称。您可以在搜索和字段功能的 API 操作中使用别名字段，但存在一些例外情况。要设置别名，必须在 path 参数中指定原始字段名称。\n参考代码 #  PUT movies { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;year\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;date\u0026#34; }, \u0026#34;release_date\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;alias\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;year\u0026#34; } } } } 参数说明 #   path：指向原始字段的完整路径，包括所有父对象。例如，parent.child.field_name。此参数为必填项。  别名（Alias）字段 #  别名（Alias）字段必须遵循以下规则：\n 一个别名字段只能引用一个原始字段。 在嵌套对象中，别名必须与原始字段位于相同的嵌套层级。   要更改别名引用的字段，需要更新映射配置。但请注意，之前存储的 Percolator 查询中的别名仍会继续引用原始字段，不会自动更新为新的字段引用。\n 原始字段 #  别名的原始字段必须遵守以下规则：\n 原始字段必须在别名字段创建之前定义。 原始字段不能是对象类型，也不能是另一个别名字段。  可以使用别名字段的搜索 API #  您可以在以下搜索 API 的只读操作中使用别名：\n 查询 (Queries) 排序 (Sorts) 聚合 (Aggregations) 存储字段 (stored_fields) 文档值字段 (docvalue_fields) 建议 (Suggestions) 高亮显示 (Highlights) 访问字段值的脚本 (Scripts)  也可以在字段功能 API 操作中使用别名 #  要在字段功能 API 中使用别名，请将其指定在 fields 参数中。\nGET movies/_field_caps?fields=release_date 例外情况 #  别名不能用于以下场景：\n 写入请求（如更新请求）。 多字段（multi-fields）或 copy_to 的目标字段。 作为 _source 参数用于结果过滤。 接受字段名称的 API（如词向量 term vectors）。 terms、more_like_this 和 geo_shape 这类查询不支持别名字段，因为别名字段在检索文档时无法使用。  通配符（Wildcards） #  在搜索和字段功能的通配符(Wildcards)查询中，原始字段和别名都会与通配符模式匹配。\nGET movies/_field_caps?fields=release* ","subcategory":null,"summary":"","tags":null,"title":"别名字段类型（Alias）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/alias/"},{"category":null,"content":"Analyzer 参数 #  analyzer 映射参数用于定义在索引和搜索期间应用于文本字段的文本分析过程。\n相关指南（先读这些） #    映射基础  文本分析基础  代码样例 #  以下示例配置定义了一个名为 my_custom_analyzer 的自定义分词器：\nPUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_stop_filter\u0026#34;, \u0026#34;my_stemmer\u0026#34; ] } }, \u0026#34;filter\u0026#34;: { \u0026#34;my_stop_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;stopwords\u0026#34;: [\u0026#34;the\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;and\u0026#34;, \u0026#34;or\u0026#34;] }, \u0026#34;my_stemmer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;english\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_text_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;search_quote_analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34; } } } } 在此示例中，my_custom_analyzer 使用标准分词器，将所有标记转换为小写，应用自定义停用词过滤器，并应用英语词干提取器。\n您可以映射一个文本字段，使其在索引和搜索操作中都使用此自定义分词器：\n\u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_text_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34; } } } 三种 Analyzer 参数 #  Easysearch 支持在映射中指定三种不同的分析器，作用于不同阶段：\n   参数 作用阶段 说明     analyzer 索引时 + 搜索时 同时用于索引和搜索的默认分析器。   search_analyzer 仅搜索时 覆盖搜索时使用的分析器。适合索引和搜索使用不同分析策略的场景。   search_quote_analyzer 短语搜索时 用于 match_phrase 等短语查询。适合短语搜索不做词干提取。    示例：索引和搜索使用不同分析器 #  一个常见场景是索引时使用同义词扩展，但搜索时使用标准分析器：\nPUT products { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;synonym_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;my_synonyms\u0026#34;] } }, \u0026#34;filter\u0026#34;: { \u0026#34;my_synonyms\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;synonym\u0026#34;, \u0026#34;synonyms\u0026#34;: [\u0026#34;手机,手持电话,移动电话\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;product_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;synonym_analyzer\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;standard\u0026#34; } } } } 常用内置分析器 #     分析器名称 描述 适用场景     standard 按 Unicode 文本分割规则分词，转小写，去停用词（可选） 大多数西语文本   simple 按非字母字符分割，转小写 简单文本   whitespace 按空格分割，不做其他处理 结构化文本   keyword 不分词，整个字段值作为一个词项 精确值   ik_smart IK 智能分词（中文） 中文搜索（推荐）   ik_max_word IK 最大化分词（中文） 中文索引（高召回）     提示：对于中文内容，推荐使用 IK 分词器。可以使用 _analyze API 测试分析效果：\nPOST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中国人民银行\u0026#34; }  ","subcategory":null,"summary":"","tags":null,"title":"分析器参数（Analyzer）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/analyzer/"},{"category":null,"content":"全文检索 #  全文检索的核心特点是：对 text 字段做分词与评分，用“相关性”来排序结果。本页只关注最常用的几类查询及常见坑。\n前提：字段必须是可分析（text）类型 #  全文查询（match/match_phrase/multi_match 等）应该作用在 text 字段 上：\n 写入时会通过 analyzer 做分词、归一化（大小写、同义词等） 查询时会用同一个 analyzer 处理查询词，再去匹配倒排索引  如果字段是 keyword/数值/日期，更适合使用 Term 级别查询。\nmatch：最常用的全文查询 #  match 会：\n 对查询字符串分词 按字段的 analyzer 处理 把多个词项组合成一个全文查询，并参与 _score 计算  示例：\n{ \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;分布式 搜索 引擎\u0026#34; } } } 常见参数：\n operator：or（默认）或 and minimum_should_match：要求最少命中多少词  一个直觉对比：\n operator: \u0026quot;and\u0026quot;：所有词都必须出现，召回会明显变少，但结果通常更“干净” minimum_should_match: \u0026quot;75%\u0026quot;：允许部分词缺失，在“可搜到”和“不要太多噪声”之间找折中  示例（用户搜索“分布式 搜索 引擎 调优”）：\n{ \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;分布式 搜索 引擎 调优\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: \u0026#34;75%\u0026#34; } } } 这样“差一个词”的结果还能被召回，但完全不相关的长条内容会被剪掉。\nmatch_phrase：短语匹配（词序 + 距离） #  match_phrase 适合“词与词之间顺序和距离都很重要”的场景，例如：\n 人名、书名、精确短语 “搜索 引擎” vs “引擎 搜索” 要区分  示例：\n{ \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;分布式 搜索 引擎\u0026#34;, \u0026#34;slop\u0026#34;: 1 } } } }  slop 表示允许的词间“位移”（更大的 slop 意味着更宽松，但可能匹配更多噪声）。  multi_match：在多个字段上搜同一串词 #  适用于“用户输入一句话，你想同时在多个字段上查”的场景，例如 title + content + tags：\n{ \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;日志 搜索 性能\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title^3\u0026#34;, \u0026#34;content\u0026#34;, \u0026#34;tags^2\u0026#34;], \u0026#34;type\u0026#34;: \u0026#34;best_fields\u0026#34; } } } 说明：\n fields：字段列表，可以用 ^ 做字段级 boost（title^3 表示标题权重更高） type 常用：  best_fields：看哪一个字段匹配得最好（默认） most_fields：多个字段共同贡献相关性    简单选择策略 #   只在单字段上搜：优先用 match 在一句话级文本上搜“短语”：用 match_phrase，必要时调整 slop 在多字段上搜同一串词：用 multi_match，配合字段 boost  如果你已经在用“多字段搜索”套路（标题 + 正文 + 标签），可以把 multi_match 理解成“语法糖版的 bool + 多个 match”，更完整的多字段策略见 多字段搜索一节。\n常见坑与注意点 #    对 keyword 字段用 match：\n由于 keyword 不分词，match 行为会退化为近似 term，用法容易混乱。\n→ 建议：为需要全文检索的字段提供 text 版本（multi-fields），keyword 只做结构化过滤/聚合。\n  忽略分析器的影响：\n查询结果不符合预期时，先用 Analyze API 看一下文本是如何被分词与归一化的。\n  滥用 operator: and：\n在很多用户输入场景，使用 and 容易导致“搜不到任何东西”；更推荐用 minimum_should_match 细调。\n  与结构化过滤结合 #  在实际业务里，全文检索几乎总是和结构化过滤一起用：\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;搜索 性能\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title^2\u0026#34;, \u0026#34;content\u0026#34;] } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } }, { \u0026#34;range\u0026#34;: { \u0026#34;created_at\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-30d/d\u0026#34; } } } ] } } } 全文部分负责\u0026quot;排序谁更相关\u0026quot;，结构化部分负责\u0026quot;限定边界、减少噪声\u0026quot;。\n小结 #   全文检索基于倒排索引，通过分析器对文本进行分词和归一化 match 查询是最常用的全文检索查询，支持多词匹配和相关性评分 multi_match 可以在多个字段上执行全文检索，支持字段权重提升 短语查询和模糊查询可以处理更复杂的匹配需求 全文检索通常与结构化过滤结合使用  下一步可以继续阅读：\n  分页与排序  高亮  相关性  参考手册（API 与参数） #    全文查询（功能手册）  搜索操作概览（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"全文检索","url":"/easysearch/main/docs/features/fulltext-search/fulltext-search/"},{"category":null,"content":"Meta 参数 #  meta 参数允许为字段附加自定义元数据。这些元数据不影响搜索或索引行为，仅用于记录字段的业务含义或管理信息。\n相关指南 #    映射基础  示例 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;latency\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;meta\u0026#34;: { \u0026#34;unit\u0026#34;: \u0026#34;ms\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;接口响应延迟\u0026#34; } }, \u0026#34;cpu_usage\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34;, \u0026#34;meta\u0026#34;: { \u0026#34;unit\u0026#34;: \u0026#34;percent\u0026#34;, \u0026#34;metric_type\u0026#34;: \u0026#34;gauge\u0026#34; } } } } } 典型用途 #     场景 示例 meta 键值     标注计量单位 \u0026quot;unit\u0026quot;: \u0026quot;ms\u0026quot;, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;   标记指标类型 \u0026quot;metric_type\u0026quot;: \u0026quot;counter\u0026quot;, \u0026quot;metric_type\u0026quot;: \u0026quot;gauge\u0026quot;   记录字段用途 \u0026quot;description\u0026quot;: \u0026quot;用户最后登录时间\u0026quot;   标记数据来源 \u0026quot;source\u0026quot;: \u0026quot;nginx_access_log\u0026quot;   团队归属信息 \u0026quot;owner\u0026quot;: \u0026quot;data-team\u0026quot;    通过 API 查看 #  字段元数据会在 Get Mapping API 的响应中返回：\nGET my-index/_mapping 响应中可以看到 meta 信息原样返回。\n约束 #     约束 说明     键数量 最多 5 个键   键长度 最长 20 个字符   值类型 仅支持字符串   值长度 最长 50 个字符    注意事项 #   meta 纯粹是描述性的，不会被索引、搜索或聚合 可以通过 Update Mapping API 修改已有字段的 meta，无需重建索引 适合配合 Kibana/INFINI Console 等可视化工具，为字段提供人类可读的附加信息  ","subcategory":null,"summary":"","tags":null,"title":"元数据参数（Meta）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/meta/"},{"category":null,"content":"_meta 元数据字段 #  _meta 字段是一个映射属性，允许您为索引映射附加自定义元数据。您的应用程序可以使用这些元数据来存储与您的用例相关的信息，如版本控制、所有权、分类或审计。\n相关指南（先读这些） #    映射基础  元数据字段  用法 #  您可以在创建新索引或更新现有索引的映射时定义 _meta 字段，如以下示例所示：\nPUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;_meta\u0026#34;: { \u0026#34;application\u0026#34;: \u0026#34;MyApp\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.2.3\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;John Doe\u0026#34; }, \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } 在此示例中，添加了三个自定义元数据字段：application、version 和 author。您的应用程序可以使用这些字段来存储有关索引的任何相关信息，例如它所属的应用程序、应用程序版本或索引的作者。\n您可以使用 Put Mapping API 操作更新 _meta 字段，如以下示例所示：\nPUT my-index/_mapping { \u0026#34;_meta\u0026#34;: { \u0026#34;application\u0026#34;: \u0026#34;MyApp\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.3.0\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jane Smith\u0026#34; } } 检索元数据信息 #  您可以使用 Get Mapping API 操作检索索引的 _meta 信息，如以下示例所示：\nGET my-index/_mapping 返回包含 _meta 字段的完整索引映射：\n{ \u0026#34;my-index\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;_meta\u0026#34;: { \u0026#34;application\u0026#34;: \u0026#34;MyApp\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.3.0\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jane Smith\u0026#34; }, \u0026#34;properties\u0026#34;: { \u0026#34;description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"元信息字段（_meta）","url":"/easysearch/main/docs/features/mapping-and-analysis/metadata-field/meta/"},{"category":null,"content":"Java 客户端快速入门 #  本页面帮助你快速跑通 Easysearch Java API Client 连接 Easysearch 的完整流程。更深入的用法请参阅 Java 客户端详细指南。\nEasysearch Java API Client 是 Easysearch 的官方 Java 客户端，提供了强类型、流式构建器风格的 API 接口：\n 全新 2.0.x 版本，更轻量，移除冗余依赖 兼容 Easysearch 各版本 支持阻塞和异步两种调用方式 使用 Jackson 无缝集成应用类  添加依赖 #  Maven：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.infinilabs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;easysearch-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Gradle：\nimplementation \u0026#39;com.infinilabs:easysearch-client:2.0.2\u0026#39;  💡 已发布到 Maven 中央仓库： mvnrepository.com/artifact/com.infinilabs/easysearch-client，需要 JDK 8 或以上版本。\n 建立连接 #  import com.infinilabs.clients.easysearch.EasysearchClient; import com.infinilabs.clients.json.jackson.JacksonJsonpMapper; import com.infinilabs.clients.transport.rest_client.RestClientTransport; import com.infinilabs.clients.transport.EasysearchTransport; import org.apache.http.HttpHost; import org.apache.http.auth.AuthScope; import org.apache.http.auth.UsernamePasswordCredentials; import org.apache.http.impl.client.BasicCredentialsProvider; import org.apache.http.conn.ssl.NoopHostnameVerifier; import org.apache.http.nio.conn.ssl.SSLIOSessionStrategy; import org.elasticsearch.client.RestClient;\nimport javax.net.ssl.SSLContext; import org.apache.http.ssl.SSLContextBuilder;\npublic static EasysearchClient create() throws Exception { HttpHost[] hosts = new HttpHost[]{new HttpHost(\u0026quot;localhost\u0026quot;, 9200, \u0026quot;https\u0026quot;)};\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// SSL 配置（开发环境信任所有证书，生产环境请配置 CA 证书）   SSLContext sslContext = SSLContextBuilder.create() .loadTrustMaterial(null, (chains, authType) -\u0026gt; true) .build(); SSLIOSessionStrategy sessionStrategy = new SSLIOSessionStrategy(sslContext, NoopHostnameVerifier.INSTANCE);\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 认证   BasicCredentialsProvider credsProv = new BasicCredentialsProvider(); credsProv.setCredentials(AuthScope.ANY, new UsernamePasswordCredentials(\u0026quot;admin\u0026quot;, \u0026quot;your_password\u0026quot;));\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 创建 RestClient   RestClient restClient = RestClient.builder(hosts) .setHttpClientConfigCallback(httpBuilder -\u0026gt; httpBuilder .setDefaultCredentialsProvider(credsProv) .setSSLStrategy(sessionStrategy) .disableAuthCaching() ) .setRequestConfigCallback(reqBuilder -\u0026gt; reqBuilder.setConnectTimeout(30000).setSocketTimeout(300000) ) .build();\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 创建 EasysearchClient   EasysearchTransport transport = new RestClientTransport( restClient, new JacksonJsonpMapper()); return new EasysearchClient(transport); } 索引文档 #\n EasysearchClient client = create(); // 使用 Map Map\u0026lt;String, Object\u0026gt; doc = Map.of( \u0026quot;title\u0026quot;, \u0026quot;Easysearch 入门\u0026quot;, \u0026quot;views\u0026quot;, 100 ); client.index(i -\u0026gt; i.index(\u0026quot;articles\u0026quot;).id(\u0026quot;1\u0026quot;).document(doc)); 搜索 #\n var response = client.search(s -\u0026gt; s .index(\u0026#34;articles\u0026#34;) .query(q -\u0026gt; q.match(m -\u0026gt; m.field(\u0026#34;title\u0026#34;).query(\u0026#34;Easysearch\u0026#34;))), Map.class ); response.hits().hits().forEach(hit -\u0026gt; System.out.println(hit.source()) ); 关闭连接 #\n // 关闭底层 RestClient restClient.close(); 注意事项 #     事项 说明     客户端 使用官方 com.infinilabs:easysearch-client:2.0.2   JDK 版本 需要 JDK 8 或以上   证书 生产环境应配置 CA 证书，而非信任所有证书     💡 如果你有已有项目使用 Elasticsearch 7.10 的 RestHighLevelClient，也可以兼容连接 Easysearch（需开启 elasticsearch.api_compatibility: true）。但新项目推荐使用 Easysearch 官方客户端。\n 相关文档 #    Java 客户端完整指南：连接池、批量操作、生产配置  官方 Java Client API 文档  使用 Curl 访问 Easysearch  入门教程  ","subcategory":null,"summary":"","tags":null,"title":"使用 Java Client 连接 Easysearch","url":"/easysearch/main/docs/quick-start/connect/java-client/"},{"category":null,"content":"Position Increment Gap 参数 #  position_increment_gap 参数控制数组中相邻文本值之间的位置间隔。该参数影响短语查询（match_phrase）在跨数组元素时的匹配行为。\n相关指南 #    全文搜索  analyzer 参数  默认值 #  默认值为 100。\n为什么需要 position_increment_gap #  当一个 text 字段接受数组值时，各元素的文本会被拼接分析。例如：\nPUT my-index/_doc/1 { \u0026#34;tags\u0026#34;: [\u0026#34;quick brown\u0026#34;, \u0026#34;fox jumps\u0026#34;] } 分析后的词条位置：\n   词条 位置     quick 0   brown 1   fox 102 （1 + 100 + 1）   jumps 103    默认间隔 100 使得 match_phrase 查询 \u0026quot;brown fox\u0026quot; 不会匹配该文档，因为 brown（位置 1）和 fox（位置 102）之间有 100 个位置的间隔，远大于短语查询允许的 slop。\n示例 #  允许跨元素匹配 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;tags\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;position_increment_gap\u0026#34;: 0 } } } } 设为 0 后，\u0026quot;brown fox\u0026quot; 的短语查询将跨元素匹配。\n自定义间隔 #  PUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;names\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;position_increment_gap\u0026#34;: 50 } } } } 何时调整 #     场景 建议     严格避免跨数组元素匹配 保持默认值 100   允许跨元素短语匹配 设为 0   需要精细控制跨元素匹配距离 设为合适的正整数    注意事项 #   仅对 text 类型字段的数组值有意义 如果字段不存储数组值，该参数无影响 与 match_phrase 查询的 slop 参数配合使用  ","subcategory":null,"summary":"","tags":null,"title":"位置增量间隔参数（Position Increment Gap）","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-parameters/position_increment_gap/"},{"category":null,"content":"从数据库同步数据（JDBC / ETL） #  将关系型数据库（MySQL、PostgreSQL 等）中的数据同步到 Easysearch 是一个常见需求。本文介绍全量导入和增量同步的主要方案和实践。\n相关指南（先读这些） #    Logstash 接入  SeaTunnel 集成  Bulk API  同步方案概览 #     方案 全量 增量 实时性 复杂度 说明     Logstash JDBC Input ✅ ✅ 分钟级 低 定时轮询数据库，适合中小规模   SeaTunnel ✅ ✅ 分钟级 中 分布式 ETL，适合大数据量   Canal / Debezium (CDC) ❌ ✅ 秒级 高 基于 binlog，实时捕获变更   自研同步程序 ✅ ✅ 灵活 高 完全自定义，适合特殊需求    Logstash JDBC Input（推荐入门） #  基本配置 #  # logstash-jdbc.conf input { jdbc { jdbc_driver_library =\u0026gt; \u0026#34;/path/to/mysql-connector-java.jar\u0026#34; jdbc_driver_class =\u0026gt; \u0026#34;com.mysql.cj.jdbc.Driver\u0026#34; jdbc_connection_string =\u0026gt; \u0026#34;jdbc:mysql://db-host:3306/mydb\u0026#34; jdbc_user =\u0026gt; \u0026#34;reader\u0026#34; jdbc_password =\u0026gt; \u0026#34;password\u0026#34; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 增量同步：记录上次的最大 update_time\u0026lt;/span\u0026gt; use_column_value \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt; tracking_column \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;update_time\u0026amp;#34;\u0026lt;/span\u0026gt; tracking_column_type \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;timestamp\u0026amp;#34;\u0026lt;/span\u0026gt; last_run_metadata_path \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;/tmp/.logstash_jdbc_last_run\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;# 每 5 分钟执行一次\u0026lt;/span\u0026gt; schedule \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;*/5 * * * *\u0026amp;#34;\u0026lt;/span\u0026gt; statement \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;SELECT id, title, content, category, update_time  FROM articles WHERE update_time \u0026gt; :sql_last_value ORDER BY update_time ASC\u0026quot; } }\nfilter { mutate { copy =\u0026gt; { \u0026quot;id\u0026quot; =\u0026gt; \u0026quot;[@metadata][_id]\u0026quot; } remove_field =\u0026gt; [\u0026quot;@version\u0026quot;, \u0026quot;@timestamp\u0026quot;] } }\noutput { elasticsearch { hosts =\u0026gt; [\u0026quot;https://easysearch-node1:9200\u0026quot;] user =\u0026gt; \u0026quot;admin\u0026quot; password =\u0026gt; \u0026quot;your_password\u0026quot; ssl =\u0026gt; true ssl_certificate_verification =\u0026gt; false index =\u0026gt; \u0026quot;articles\u0026quot; document_id =\u0026gt; \u0026quot;%{[@metadata][_id]}\u0026quot; action =\u0026gt; \u0026quot;index\u0026quot; } } 关键参数说明 #\n    参数 说明     tracking_column 增量同步的追踪列，通常是 update_time 或自增 id   schedule Cron 表达式，控制轮询频率   last_run_metadata_path 记录上次同步位置的文件路径   clean_run 设为 true 可忽略上次记录，重新全量同步    增量同步策略 #  基于时间戳 #  适合有 update_time 或 modified_at 列的表：\nWHERE update_time \u0026gt; :sql_last_value ORDER BY update_time ASC  注意：需要确保 update_time 在记录更新时会被自动刷新。\n 基于自增 ID #  适合只有新增、没有修改的场景（如日志表）：\nWHERE id \u0026gt; :sql_last_value ORDER BY id ASC 处理删除操作 #  上述两种方式都无法感知已删除的记录。常见处理方式：\n   方案 说明     软删除标记 数据库中用 is_deleted 字段标记，同步时在 Easysearch 中过滤   全量覆盖 定期全量重建索引，替换旧索引（适合数据量不大的场景）   CDC 方案 使用 Canal/Debezium 捕获 DELETE 事件，实时同步删除    Mapping 设计建议 #  同步前应提前创建好索引和映射，避免依赖动态映射产生不理想的字段类型：\nPUT articles { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34; }, \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;update_time\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||epoch_millis\u0026#34; } } } } 性能优化 #   批量大小：Logstash 默认每次读取 100000 行，可通过 jdbc_fetch_size 调整 并行写入：配置 Logstash output 的 workers 数量来提升写入吞吐 索引刷新：大批量导入时将 refresh_interval 设为 -1，完成后恢复 副本数量：全量导入时可临时将 number_of_replicas 设为 0，完成后恢复  ","subcategory":null,"summary":"","tags":null,"title":"从数据库同步数据（JDBC / ETL）","url":"/easysearch/main/docs/integrations/ingest/jdbc-etl/"},{"category":null,"content":"二进制字段类型 #  二进制字段类型包含以 Base64 编码存储的二进制值，这些值不可被搜索。\n参考代码 #  创建包含二进制字段的映射\nPUT testindex { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;binary_value\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;binary\u0026#34; } } } } 索引一个二进制值的文档\nPUT testindex/_doc/1 { \u0026#34;binary_value\u0026#34; : \u0026#34;bGlkaHQtd29rfx4=\u0026#34; }  使用 = 作为填充字符。不允许嵌入换行符。\n 参数说明 #  以下参数均为可选参数\n   参数 数据类型 默认值 描述     doc_values Boolean false 是否存储在磁盘上，用于聚合、排序或脚本。   store Boolean false 是否单独存储字段值，使其可以独立于 _source 被检索。    使用场景 #  二进制字段适合存储小型二进制数据，例如：\n 文件的 MD5/SHA 摘要 小图标或缩略图 证书或密钥的 Base64 编码   注意：二进制字段 不可搜索，不会建立倒排索引。它只是被存储在 _source 中随文档一起返回。\n 完整示例 #  PUT attachments { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;filename\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;content_type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;binary\u0026#34; } } } } PUT attachments/_doc/1 { \u0026#34;filename\u0026#34;: \u0026#34;logo.png\u0026#34;, \u0026#34;content_type\u0026#34;: \u0026#34;image/png\u0026#34;, \u0026#34;data\u0026#34;: \u0026#34;iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg==\u0026#34; }  提示：对于大文件，建议使用外部存储（如对象存储），在 Easysearch 中只存储文件的引用路径。\n ","subcategory":null,"summary":"","tags":null,"title":"二进制字段类型（Binary）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/binary/"},{"category":null,"content":"Apache Superset 集成 #  Apache Superset 是一款强大的开源数据可视化与 BI 平台。通过 Elasticsearch 连接器，Superset 可以直接查询 Easysearch 中的数据并构建交互式看板。\n相关指南 #    SQL 查询接口  Grafana 集成  前置条件 #     条件 说明     Superset 版本 2.0+ 推荐   Python 驱动 elasticsearch-dbapi 包   网络可达 Superset 服务器能够访问 Easysearch 端口   Easysearch SQL 功能 确保 SQL 插件已启用    安装驱动 #  在 Superset 运行环境中安装 Elasticsearch 驱动：\npip install elasticsearch-dbapi 如果使用 Docker 部署的 Superset，需要在自定义镜像中添加此依赖：\nFROMapache/superset:latest RUN pip install elasticsearch-dbapi 配置数据源 #  1. 添加数据库连接 #  在 Superset 中进入 Settings → Database Connections → + Database，选择 Elasticsearch。\n连接字符串格式：\nelasticsearch+https://admin:password@easysearch-host:9200 如果使用 HTTP（开发环境）：\nelasticsearch+http://admin:password@easysearch-host:9200 2. 高级配置 #  在 Advanced → Other → Engine Parameters 中可以设置额外参数：\n{ \u0026#34;connect_args\u0026#34;: { \u0026#34;verify_certs\u0026#34;: false, \u0026#34;timeout\u0026#34;: 60 } }    参数 说明 默认值     verify_certs 是否验证 SSL 证书 true   timeout 查询超时（秒） 30   http_compress 是否启用 HTTP 压缩 false    3. 测试连接 #  点击 Test Connection 确认连接成功。\n创建数据集与图表 #  添加数据集 #   进入 Datasets → + Dataset 选择刚配置的 Easysearch 数据库 Schema 留空，Table 选择要分析的索引名 保存数据集  适合的图表类型 #     图表类型 适用场景 对应查询能力     时间序列折线 日志趋势、指标监控 date_histogram 聚合   柱状图/饼图 分类分布统计 terms 聚合   数字卡片 KPI 指标展示 sum / avg / count 聚合   表格 明细数据查看 SELECT 查询   热力图 时间 × 维度交叉分析 多级聚合    性能优化建议 #   索引设计：为 Superset 常用查询场景预创建聚合友好的索引映射 查询超时：对大索引设置合理的 timeout 参数，避免长时间查询阻塞 缓存策略：在 Superset 中为慢查询的数据集启用缓存，减少对 Easysearch 的重复请求 采样查询：在探索阶段使用 LIMIT 限制返回行数 专用账户：为 Superset 创建只读的专用账户，限制其可访问的索引范围  ","subcategory":null,"summary":"","tags":null,"title":"Superset 集成","url":"/easysearch/main/docs/integrations/clients/superset/"},{"category":null,"content":"RAID 配置指南 #  本文讨论 Easysearch 环境中 RAID 的选型与配置建议。\nRAID 与 Easysearch 的关系 #  Easysearch 自身通过副本机制实现数据冗余。因此 RAID 在 Easysearch 场景中的定位与传统数据库不同：\n   方案 数据冗余 性能 适用场景     无 RAID + 副本 由 ES 副本保障 最高 推荐方案   RAID-0 无 高 多 SATA 盘聚合（有副本前提下）   RAID-1 镜像 中 单节点无副本（不推荐）   RAID-5/6 校验 较低 不推荐（写放大严重）   RAID-10 镜像+条带 中高 预算充足且需要本地冗余     核心建议：优先使用 Easysearch 副本，而非 RAID 来保障数据安全。\n 推荐方案：JBOD（多路径） #  JBOD（Just a Bunch of Disks）是 Easysearch 场景的最佳方案——每块盘独立挂载，通过多路径配置：\n# easysearch.yml path.data: - /data1/easysearch - /data2/easysearch - /data3/easysearch - /data4/easysearch 优势：\n 无 RAID 写放大 单盘故障只影响该盘上的分片（副本在其他节点） 最大化利用磁盘吞吐  如果必须用 RAID #  RAID-0（条带化） #  # 创建 RAID-0（2 块盘条带化） mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc # 格式化 mkfs.xfs -f /dev/md0\n# 挂载 mount -o noatime /dev/md0 /data/easysearch \n适用于：多块 SATA SSD，需要聚合带宽 风险：任意一块盘故障，整个阵列数据丢失（依赖 ES 副本恢复）  RAID-10（镜像+条带） #  # 创建 RAID-10（4 块盘） mdadm --create /dev/md0 --level=10 --raid-devices=4 /dev/sdb /dev/sdc /dev/sdd /dev/sde  适用于：单节点无法设置副本的特殊场景 代价：50% 磁盘空间用于镜像  不推荐 RAID-5/6 #  RAID-5/6 的奇偶校验计算在 Easysearch 的高随机写入场景下会导致严重的写放大：\n 每次写入需要读-改-写校验块 段合并时写放大尤其明显 重建时间长（TB 级别可能需要数小时）  监控 #  # 查看 RAID 状态 cat /proc/mdstat # 或使用 mdadm mdadm \u0026ndash;detail /dev/md0 延伸阅读 #\n   NVMe 配置  生产环境部署  ","subcategory":null,"summary":"","tags":null,"title":"RAID 配置","url":"/easysearch/main/docs/deployment/advanced-config/raid/"},{"category":null,"content":"RAG 与 LLM 集成 #  检索增强生成（Retrieval-Augmented Generation，RAG）是一种将搜索引擎与大语言模型（LLM）结合的架构模式。Easysearch 作为高性能检索层，可以为 LLM 提供精准的上下文信息，显著提升生成质量。\n相关指南（先读这些） #    Embedding 服务接入  向量工作流与 Hybrid 检索  Hybrid Search API  LangChain 集成  RAG 架构概览 #  典型的 RAG 流程如下：\n用户提问 ↓ 1. 查询改写（可选） ↓ 2. 检索阶段：在 Easysearch 中搜索相关文档 - 全文搜索（BM25） - 向量搜索（kNN） - 混合搜索（Hybrid） ↓ 3. 结果裁剪：选取 Top-K 段落，控制 token 数量 ↓ 4. Prompt 组装：将检索结果 + 用户问题拼接为 Prompt ↓ 5. LLM 生成：调用 LLM API 生成答案 ↓ 6. 返回给用户（可附上来源引用） 检索策略设计 #  选择搜索模式 #     模式 优势 劣势 适用场景     BM25 精确匹配，无需向量化 无法理解语义相似性 关键词明确的 FAQ   kNN 语义理解，跨表述匹配 需要 Embedding 服务 语义搜索、知识问答   Hybrid 兼顾精确和语义 复杂度稍高 推荐首选方案    Hybrid 检索示例 #  POST knowledge_base/_search { \u0026#34;size\u0026#34;: 5, \u0026#34;_source\u0026#34;: [\u0026#34;title\u0026#34;, \u0026#34;content\u0026#34;, \u0026#34;source_url\u0026#34;], \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;content_vector\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.34, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 50 } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;如何配置 Easysearch 集群\u0026#34;, \u0026#34;boost\u0026#34;: 0.3 } } } ] } } } Prompt 组装策略 #  将检索结果拼入 Prompt 时，需要注意 LLM 的上下文窗口限制：\n你是一个技术助手。请根据以下参考资料回答用户的问题。 如果参考资料中没有足够的信息，请说明你不确定。 参考资料 【来源 1】{title_1} {content_1}\n【来源 2】{title_2} {content_2}\n用户问题 {user_question}\n回答 Token 预算管理 #\n    LLM 上下文窗口 建议检索条数 每条最大长度     GPT-4o 128K tokens 5~10 条 1000~2000 字   Claude 3.5 200K tokens 5~15 条 1000~3000 字   通义千问 8K~128K tokens 3~8 条 500~1500 字   本地小模型（7B） 4K~8K tokens 2~3 条 300~500 字    多轮对话 #  在多轮问答场景下，需要将历史对话纳入检索和生成流程：\n 历史感知检索：将用户最新问题 + 最近几轮对话合并后做检索 对话历史管理：只保留最近 N 轮对话，避免超出 token 限制 话题切换检测：当用户转换话题时，清空之前的检索上下文  效果优化 #     优化方向 具体做法     数据质量 对文档做分段（chunk），每段 200~500 字，保留完整语义   查询改写 用 LLM 对用户问题做扩展/改写，提升检索召回率   重排序 对检索结果用 Cross-Encoder 重排序，提升 Top-K 精度   引用追溯 返回结果中附上文档来源链接，增强可信度   答案验证 让 LLM 判断生成答案是否基于检索结果，减少幻觉    ","subcategory":null,"summary":"","tags":null,"title":"RAG 与 LLM 集成","url":"/easysearch/main/docs/integrations/ai/rag-and-llm/"},{"category":null,"content":"Nested 建模 #  Nested 解决的是这样一种典型需求：数组里是一组对象，而不是一堆无关字段的拼接，查询时既要对数组元素内部做精确匹配，又不想被\u0026quot;笛卡尔积假匹配\u0026quot;坑到。\n什么时候需要 nested？ #  先看一个常见例子：订单里有多条明细 items：\n{ \u0026#34;order_id\u0026#34;: \u0026#34;O-1\u0026#34;, \u0026#34;items\u0026#34;: [ { \u0026#34;sku\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;price\u0026#34;: 100, \u0026#34;qty\u0026#34;: 1 }, { \u0026#34;sku\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;price\u0026#34;: 200, \u0026#34;qty\u0026#34;: 2 } ] } 如果你把 items.sku、items.price、items.qty 都当成普通多值字段：\n items.sku = [\u0026ldquo;A\u0026rdquo;, \u0026ldquo;B\u0026rdquo;] items.price = [100, 200] items.qty = [1, 2]  此时一个查询：\n items.sku = \u0026quot;A\u0026quot; 且 items.price = 200  在\u0026quot;扁平多值字段\u0026quot;模型下是会命中的（因为它只在每个字段内部看是否包含该值），但现实里并不存在 sku=A 且 price=200 这一条明细。这就是经典的\u0026quot;笛卡尔积假匹配\u0026quot;。\n要避免这种问题，就需要 nested。\n如何定义 nested 字段 #  在 Mapping 中，把数组元素声明为 nested 类型（示意）：\n{ \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;order_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;sku\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, \u0026#34;qty\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } } } 含义：\n items 里的每个元素在底层会作为一个\u0026quot;子文档\u0026quot;存储 nested 查询可以在\u0026quot;同一个子文档\u0026quot;内部组合多个条件，从而避免混淆  nested 查询的基本用法 #  仍以\u0026quot;查找包含 sku=A 且 price=200 的订单\u0026quot;为例：\n{ \u0026#34;query\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;items.sku\u0026#34;: \u0026#34;A\u0026#34; } }, { \u0026#34;range\u0026#34;: { \u0026#34;items.price\u0026#34;: { \u0026#34;gte\u0026#34;: 200 } } } ] } } } } } 关键点：\n path：指定 nested 字段路径（这里是 items） query：在该 path 内部再写一套 Query DSL（通常用 bool）  查询逻辑：\n 先在所有 items 子文档中查找满足条件的元素 再把命中的子文档\u0026quot;折叠\u0026quot;回其所属父文档，返回对应的订单  nested 聚合 #  如果你想对 nested 字段里的数据做聚合（例如统计所有明细中的 SKU 分布），需要显式进入 nested 上下文（示意）：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;items\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;items\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;by_sku\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;items.sku\u0026#34; } } } } } } 如果还要回到父文档维度再做聚合，可配合 reverse_nested 使用。\nnested 的代价与取舍 #  优点：\n 消除笛卡尔积假匹配，语义清晰 支持在数组元素内做复杂组合条件与聚合  成本：\n nested 元素在底层是\u0026quot;多个子文档\u0026quot;，文档数量会放大 查询、聚合在 nested 上下文中会有额外开销 设计不当时，可能让索引体积与查询复杂度都变大  经验建议：\n nested 非常适合\u0026quot;真正的一组对象数组\u0026quot;，且数组长度适中（例如订单明细、标签属性等） 如果数组非常长、结构复杂，且查询场景简单，考虑用反范式/预计算字段替代  小结 #   Nested 解决数组里是一组对象时的\u0026quot;笛卡尔积假匹配\u0026quot;问题 在 Mapping 中把数组元素声明为 nested 类型 nested 查询可以在\u0026quot;同一个子文档\u0026quot;内部组合多个条件 nested 聚合需要显式进入 nested 上下文 nested 的代价是文档数量放大和查询开销增加  下一步可以继续阅读：\n  Parent-Child 建模  反范式与权衡  映射模式  参考手册（API 与参数） #    关联查询（功能手册）  Nested 字段类型  ","subcategory":null,"summary":"","tags":["最佳实践","数据建模"],"title":"Nested 建模","url":"/easysearch/main/docs/best-practices/data-modeling/nested/"},{"category":null,"content":"MacOS 环境下使用 Easysearch #  目前，有多种方案可以在 MacOS 下体验 Easysearch。可以选择使用 Docker 方式安装，或者使用本地安装包进行安装。\n前置要求 #   macOS 10.15 (Catalina) 或更高版本 JDK 11+（推荐 JDK 17）。Bundle 包已内置 JDK，无需单独安装。 至少 4 GB 可用内存  方案一：Docker 安装（推荐） #  如果您的 MacOS 环境上有 Docker（Docker Desktop、OrbStack 等），可以用最简单的方式启动 Easysearch：\ndocker run -d --name easysearch \\  -p 9200:9200 \\  -e \u0026#34;EASYSEARCH_INITIAL_ADMIN_PASSWORD=MyTest@2024\u0026#34; \\  -e \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; \\  infinilabs/easysearch:latest  详细 Docker 配置请参考 Docker 环境下使用 Easysearch。\n 方案二：本地安装 #  使用一键安装脚本进行安装，请按照以下步骤操作：\n# 打开终端，使用 bash 执行以下命令 mkdir -p /Users/$(whoami)/data/easysearch # 下载最新版本的 Easysearch 并安装 curl -sSL http://get.infini.cloud | bash -s \u0026ndash; -p easysearch -d /Users/$(whoami)/data/easysearch\n# 进入 Easysearch 目录 cd /Users/$(whoami)/data/easysearch\n# 初始化 Easysearch bin/initialize.sh -s\n# 运行 Easysearch bin/easysearch -d -p pid \n注意：初始化过程中会生成随机密码，并不会保存到日志文件中，只会在终端显示一次，请妥善保存。\n 验证安装 #  # 使用初始化时输出的密码 curl -ku admin:YOUR_PASSWORD https://localhost:9200 # 预期输出包含 # \u0026quot;cluster_name\u0026quot; : \u0026quot;easysearch\u0026quot;, # \u0026quot;tagline\u0026quot; : \u0026quot;You Know, For Easy Search!\u0026quot; 常见问题 #\n 忘记 admin 密码 #  # 使用密码重置脚本 cd /Users/$(whoami)/data/easysearch bin/reset_admin_password.sh macOS 安全提示 #  如果遇到\u0026quot;无法验证开发者\u0026quot;的提示，可以在 系统偏好设置 → 安全性与隐私 中允许运行，或执行：\nxattr -d com.apple.quarantine bin/easysearch 停止 Easysearch #  # 使用 PID 文件停止 kill $(cat /Users/$(whoami)/data/easysearch/pid) 延伸阅读 #    Docker 部署  测试环境部署  系统调优  ","subcategory":null,"summary":"","tags":null,"title":"macOS","url":"/easysearch/main/docs/deployment/install-guide/macos/"},{"category":null,"content":"Lucene 表达式 脚本语言 #  Lucene Expressions 是一种轻量级的数学表达式脚本语言，编译为 Java 字节码，性能极高。适用于简单的数值计算场景，如自定义排序和评分。\n特点 #     特性 说明     性能 极高，直接编译为字节码，无解释器开销   安全 完全沙盒，只能进行数学运算   功能范围 仅支持数值运算和比较   数据访问 只能访问 doc values（doc['field']）   返回类型 始终返回 double    语法 #  基本运算 #  doc[\u0026#39;price\u0026#39;].value * 0.9 doc[\u0026#39;score\u0026#39;].value + doc[\u0026#39;bonus\u0026#39;].value (doc[\u0026#39;a\u0026#39;].value + doc[\u0026#39;b\u0026#39;].value) / 2 数学函数 #     函数 说明 示例     abs(x) 绝对值 abs(doc['delta'].value)   sqrt(x) 平方根 sqrt(doc['area'].value)   pow(x, y) x 的 y 次方 pow(doc['base'].value, 2)   exp(x) e 的 x 次方 exp(-0.01 * doc['age'].value)   ln(x) 自然对数 ln(doc['count'].value + 1)   log10(x) 以 10 为底的对数 log10(doc['views'].value + 1)   log2(x) 以 2 为底的对数 log2(doc['size'].value)   ceil(x) 向上取整 ceil(doc['score'].value)   floor(x) 向下取整 floor(doc['score'].value)   max(a, b) 较大值 max(doc['a'].value, doc['b'].value)   min(a, b) 较小值 min(doc['a'].value, 100)   sin(x), cos(x), tan(x) 三角函数 sin(doc['angle'].value)   asin(x), acos(x), atan(x) 反三角函数 acos(doc['cos_val'].value)   atan2(y, x) 二参数反正切 atan2(doc['y'].value, doc['x'].value)   haversin(lat1, lon1, lat2, lon2) 球面距离（千米） haversin(doc['lat'].value, doc['lon'].value, 39.9, 116.4)    比较与条件 #  Expressions 不支持 if-else，但支持三元运算和布尔转换：\n// 布尔值会转为 double（true=1.0, false=0.0） doc[\u0026#39;in_stock\u0026#39;].value ? doc[\u0026#39;price\u0026#39;].value : 0 // 比较运算 doc['age'].value \u0026gt; 18 ? 1 : 0 使用示例 #\n 自定义排序 #  GET products/_search { \u0026#34;sort\u0026#34;: [ { \u0026#34;_script\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;expression\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;sales\u0026#39;].value * 0.7 + doc[\u0026#39;rating\u0026#39;].value * 0.3\u0026#34; }, \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } function_score 中使用 #  GET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;搜索\u0026#34; } }, \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;expression\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;_score * ln(doc[\u0026#39;views\u0026#39;].value + 1)\u0026#34; } } } } } 地理距离评分 #  GET shops/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;expression\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;1.0 / (1 + haversin(doc[\u0026#39;location\u0026#39;].lat, doc[\u0026#39;location\u0026#39;].lon, 39.9042, 116.4074))\u0026#34; } } } } } Expressions vs Painless #     对比项 Expressions Painless     性能 ⚡ 极高 ⚡ 高   功能范围 仅数学运算 通用（字符串、数组、条件…）   数据访问 仅 doc values doc values + _source + ctx   控制流 仅三元运算 if/else、for、while   字符串操作 ❌ ✅   适用场景 简单评分/排序公式 复杂业务逻辑     建议：如果只需要数值计算（如评分公式、距离计算），优先使用 Expressions 以获得最佳性能。对于需要条件逻辑、字符串处理、数组操作等复杂场景，使用 Painless。\n 限制 #   只能返回 double 类型的值 只能通过 doc['field'] 访问 doc values 中的数值字段 不支持字符串操作、数组操作 不支持 if-else（只能用三元运算符） 不支持变量声明、循环等语句 不能用于 Update、Ingest 等修改文档的场景  ","subcategory":null,"summary":"","tags":null,"title":"Lucene 表达式","url":"/easysearch/main/docs/features/scripting/expressions/"},{"category":null,"content":"LangChain 集成 #   LangChain 是最流行的 LLM 应用开发框架。通过将 Easysearch 作为 Vector Store，可以构建 RAG（Retrieval-Augmented Generation）应用，让大模型基于企业知识库进行问答。\n架构概览 #  用户提问 → LangChain ↓ 1. Embedding 模型将问题转为向量 ↓ 2. Easysearch 向量检索（kNN）找到相关文档 ↓ 3. 将相关文档 + 问题发送给 LLM ↓ 4. LLM 生成基于上下文的回答 ↓ 用户得到答案 安装 #  pip install langchain langchain-community elasticsearch 连接 Easysearch #  from elasticsearch import Elasticsearch es = Elasticsearch( hosts=[\u0026quot;https://localhost:9200\u0026quot;], basic_auth=(\u0026quot;admin\u0026quot;, \u0026quot;your-password\u0026quot;), verify_certs=False # 自签名证书时使用 )\n# 验证连接 print(es.info()) 作为 Vector Store 使用 #\n 1. 准备 Embedding 模型 #  from langchain_community.embeddings import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings( model_name=\u0026quot;BAAI/bge-base-zh-v1.5\u0026quot; # 中文 embedding 模型 ) 2. 创建 Vector Store #\n from langchain_community.vectorstores import ElasticsearchStore vector_store = ElasticsearchStore( es_connection=es, index_name=\u0026quot;langchain-docs\u0026quot;, embedding=embeddings, ) 3. 写入文档 #\n from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.document_loaders import TextLoader # 加载文档 loader = TextLoader(\u0026quot;knowledge_base.txt\u0026quot;, encoding=\u0026quot;utf-8\u0026quot;) documents = loader.load()\n# 分块 text_splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=50 ) docs = text_splitter.split_documents(documents)\n# 写入 Easysearch vector_store.add_documents(docs) 4. 相似度搜索 #\n results = vector_store.similarity_search( query=\u0026#34;Easysearch 如何配置安全？\u0026#34;, k=5 ) for doc in results: print(doc.page_content[:100]) 构建 RAG 问答链 #  from langchain.chains import RetrievalQA from langchain_community.llms import Ollama # 或使用其他 LLM # 初始化 LLM llm = Ollama(model=\u0026quot;qwen2.5\u0026quot;) # 本地部署的模型\n# 构建 RAG 链 retriever = vector_store.as_retriever(search_kwargs={\u0026quot;k\u0026quot;: 5}) qa_chain = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026quot;stuff\u0026quot;, retriever=retriever, return_source_documents=True )\n# 提问 response = qa_chain.invoke({\u0026quot;query\u0026quot;: \u0026quot;Easysearch 的安全功能有哪些？\u0026quot;}) print(response[\u0026quot;result\u0026quot;]) 混合搜索（关键词 + 向量） #\n vector_store = ElasticsearchStore( es_connection=es, index_name=\u0026#34;langchain-docs\u0026#34;, embedding=embeddings, strategy=ElasticsearchStore.ApproxRetrievalStrategy( hybrid=True # 启用混合搜索 ) ) 注意事项 #     注意项 说明     Embedding 维度 需与 knn_dense_float_vector 字段的 dims 参数匹配   HTTPS Easysearch 默认启用 HTTPS，注意证书配置   API 兼容 开启 elasticsearch.api_compatibility: true   kNN 插件 确保已安装 knn 插件    延伸阅读 #    RAG 与 LLM 集成  向量搜索  AI 集成总览  ","subcategory":null,"summary":"","tags":null,"title":"LangChain 集成","url":"/easysearch/main/docs/integrations/third-party/langchain/"},{"category":null,"content":"k-NN 查询 API #  先决条件 #  要运行 k-NN 搜索，必须安装 knn 插件，参考 插件安装。\n 注意：从 1.11.1 版本起，创建 k-NN 索引时不再需要配置 index.knn 参数。\n  向量字段的映射参数、各索引模型和相似度函数的详细说明，请参阅 向量字段类型参考。\n 查询语法 #  所有向量搜索都通过 knn_nearest_neighbors 查询完成：\nGET /\u0026lt;index\u0026gt;/_search { \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;\u0026lt;vector_field\u0026gt;\u0026#34;, \u0026#34;vec\u0026#34;: { ... }, \u0026#34;model\u0026#34;: \u0026#34;\u0026lt;model\u0026gt;\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;\u0026lt;similarity\u0026gt;\u0026#34;, \u0026#34;candidates\u0026#34;: \u0026lt;n\u0026gt; } } } 查询参数 #     参数 类型 必填 说明     field String ✅ 向量字段名称   vec Object ✅ 查询向量，支持三种格式（见下方）   model String ✅ 索引模型：lsh、exact、permutation_lsh、sparse_indexed   similarity String ✅ 相似度函数，必须与映射中的配置一致   candidates Integer 否 近似搜索的候选数量。仅用于 lsh/permutation_lsh/sparse_indexed 模型。越大越精准但越慢，推荐 size 的 5～10 倍   probes Integer 否 仅用于 l2 相似度的 LSH 查询：探测数量    查询向量格式 #  1. 直接提供值（最常用） #  \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.03, 0.88, 0.45, ...] } 用于稠密浮点向量（knn_dense_float_vector），数组长度必须与映射中的 dims 一致。\n2. 引用已索引向量 #  \u0026#34;vec\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;my-vectors\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;doc-1\u0026#34;, \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34; } 用已存在文档的向量作为查询向量，避免在请求中传递完整向量。参数说明：\n   参数 说明     index 源文档所在的索引名   id 源文档 ID   field 源文档中的向量字段名    3. 稀疏布尔向量 #  \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [[0, 3, 5, 12, 88], 1000] } 用于稀疏布尔向量（knn_sparse_bool_vector），格式为 [[true_indices], total_dims]。\n LSH 近似搜索 #  最常用的查询方式，通过局部敏感哈希实现高效近似最近邻搜索。\n稠密浮点向量 + LSH #  以 GloVe 词向量为例，查找与 \u0026ldquo;bread\u0026rdquo; 最相似的词：\nGET /glove-vectors/_search { \u0026#34;size\u0026#34;: 5, \u0026#34;_source\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;my_vec\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [-0.37436, -0.11959, -0.87609, -1.1217, 1.2788, 0.48323, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 50 } } } 响应示例 #  { \u0026#34;took\u0026#34;: 214, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 400, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;glove-vectors\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;WYPvCYkBkPNAx5w6LJ6H\u0026#34;, \u0026#34;_score\u0026#34;: 2, \u0026#34;_source\u0026#34;: { \u0026#34;word\u0026#34;: \u0026#34;bread\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;glove-vectors\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;PYPvCYkBkPNAx5w6Mbui\u0026#34;, \u0026#34;_score\u0026#34;: 1.8483887, \u0026#34;_source\u0026#34;: { \u0026#34;word\u0026#34;: \u0026#34;baked\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;glove-vectors\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;RoPvCYkBkPNAx5w6NsYl\u0026#34;, \u0026#34;_score\u0026#34;: 1.8451341, \u0026#34;_source\u0026#34;: { \u0026#34;word\u0026#34;: \u0026#34;toast\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;glove-vectors\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;o4PvCYkBkPNAx5w6LKCI\u0026#34;, \u0026#34;_score\u0026#34;: 1.84022, \u0026#34;_source\u0026#34;: { \u0026#34;word\u0026#34;: \u0026#34;butter\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;glove-vectors\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;koPvCYkBkPNAx5w6LKeK\u0026#34;, \u0026#34;_score\u0026#34;: 1.8374994, \u0026#34;_source\u0026#34;: { \u0026#34;word\u0026#34;: \u0026#34;soup\u0026#34; } } ] } }  使用 cosine 相似度时，得分范围为 [0, 2]。完全相同的向量得分为 2。\n 稀疏布尔向量 + LSH #  索引文档 #  PUT /sparse-features/_doc/1 { \u0026#34;label\u0026#34;: \u0026#34;文档A\u0026#34;, \u0026#34;features\u0026#34;: [[0, 3, 5, 12, 88, 200, 456], 1000] } 查询示例 #  GET /sparse-features/_search { \u0026#34;size\u0026#34;: 5, \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;features\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [[0, 5, 12, 100, 456], 1000] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;jaccard\u0026#34;, \u0026#34;candidates\u0026#34;: 50 } } }  精确搜索 #  精确模型计算查询向量与所有索引向量的精确相似度，不使用任何近似索引结构。映射时不需要指定 model，详见 向量字段类型参考。\n查询 #  GET /exact-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;my_vec\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.1, 0.2, 0.3, ...] }, \u0026#34;model\u0026#34;: \u0026#34;exact\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; } } }  ⚠️ 精确搜索的时间复杂度为 O(n²)，适用于文档数量较少的场景。\n 精确搜索支持的相似度：\n   向量类型 可用相似度     knn_dense_float_vector cosine、l1、l2   knn_sparse_bool_vector jaccard、hamming     引用已索引向量 #  使用索引中已有文档的向量作为查询向量，适用于\u0026quot;查找相似文档\u0026quot;的场景：\nGET /my-vectors/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;my-vectors\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;doc-1\u0026#34;, \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34; }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 100 } } }  源文档可以在同一索引或不同索引中，只要向量类型和维度匹配即可。\n  sparse_indexed 模型 #  sparse_indexed 模型使用倒排索引结构，专为稀疏布尔向量设计。映射配置详见 向量字段类型参考。\n查询 #  GET /sparse-idx/_search { \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;features\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [[1, 10, 50, 200, 1000], 5000] }, \u0026#34;model\u0026#34;: \u0026#34;sparse_indexed\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;jaccard\u0026#34;, \u0026#34;candidates\u0026#34;: 50 } } }  permutation_lsh 模型 #  permutation_lsh 是基于排列的局部敏感哈希，仅适用于稠密浮点向量。映射配置详见 向量字段类型参考。\n查询 #  GET /perm-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.1, 0.2, ...] }, \u0026#34;model\u0026#34;: \u0026#34;permutation_lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 100 } } }  与 bool 查询组合 #  向量搜索可以嵌入 bool 查询，与过滤条件组合：\nGET /my-vectors/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.03, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 100 } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;tech\u0026#34; } } ] } } } 与 function_score 集成 #  向量相似度可作为 function_score 的评分函数：\nGET /my-vectors/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;搜索引擎\u0026#34; } }, \u0026#34;functions\u0026#34;: [ { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.03, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 100 }, \u0026#34;weight\u0026#34;: 2 } ], \u0026#34;boost_mode\u0026#34;: \u0026#34;multiply\u0026#34; } } }  相关页面 #    向量字段类型参考：字段类型、映射参数、各索引模型详解  向量搜索指南：完整的使用流程与 Hybrid 检索  向量字段建模：多向量设计、维度选择、模型选型策略  ","subcategory":null,"summary":"","tags":null,"title":"k-NN 查询 API","url":"/easysearch/main/docs/features/vector-search/knn_api/"},{"category":null,"content":"IP 地址字段类型 #  IP 字段类型用于存储 IPv4 或 IPv6 格式的 IP 地址。\n 要表示 IP 地址范围，可以使用 IP 范围字段类型\n 参考代码 #  创建一个有 IP 地址的 mapping\nPUT testindex { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;ip_address\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;ip\u0026#34; } } } } 索引一个有 IP 地址的文档\nPUT testindex/_doc/1 { \u0026#34;ip_address\u0026#34; : \u0026#34;10.24.34.0\u0026#34; } 查询一个特定 IP 地址的索引\nGET testindex/_doc/1 { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;ip_address\u0026#34;: \u0026#34;10.24.34.0\u0026#34; } } } 搜索 IP 地址及其关联的网络掩码 #  您可以使用无类别域间路由 (CIDR) 表示法查询索引中的 IP 地址。在 CIDR 表示法中，通过斜杠 / 分隔 IP 地址和前缀长度（0–32）。例如，前缀长度为 24 表示匹配所有具有相同前 24 位的 IP 地址。\n查询 IPV4 格式的参考代码 #  GET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;ip_address\u0026#34;: \u0026#34;10.24.34.0/24\u0026#34; } } } 查询 IPV6 格式的参考代码 #  GET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;ip_address\u0026#34;: \u0026#34;2001:DB8::/24\u0026#34; } } } 如果在 query_string 查询中使用 IPv6 格式的 IP 地址，需要对 : 字符进行转义，因为它们会被解析为特殊字符。可以通过将 IP 地址用引号括起来，并使用 \\ 转义引号来实现。\nGET testindex/_search { \u0026#34;query\u0026#34; : { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;ip_address:\\\u0026#34;2001:DB8::/24\\\u0026#34;\u0026#34; } } } 参数说明 #  下面是 IP 字段的参数说明，都是可选参数 参数说明：\n   参数 描述 默认值     boost 浮点值，指定字段对相关性评分的权重。大于 1.0 提高相关性，0.0 到 1.0 降低相关性。 1.0   doc_values 布尔值，指定是否将字段存储到磁盘，以便用于聚合、排序或 script 操作。 true   ignore_malformed 布尔值，指定是否忽略格式错误的值而不抛出异常。 false   index 布尔值，指定字段是否可被搜索。 true   null_value 指定替代 null 的值，必须与字段类型一致。如果未指定，值为 null 时视为缺失。 null   store 布尔值，指定字段值是否单独存储，并可在 _source 字段外被检索。 false    ","subcategory":null,"summary":"","tags":null,"title":"IP 地址字段类型（IP）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/ip/"},{"category":null,"content":"IK 分析器 #  IK 分析器是一款专为处理中文文本设计的分析器，高效且智能。支持 ik_smart 和 ik_max_word 两种分词模式。\n相关指南（先读这些） #    文本分析基础  文本分析：识别词元  IK 分词器安装 #  IK 分词插件安装命令如下：\nbin/easysearch-plugin install analysis-ik 同时也需要安装 ingest-common 插件：\nbin/easysearch-plugin install ingest-common 如果觉得比较麻烦，也可以直接使用 Easysearch 的 bundle 包安装部署。\n使用样例 #  下面的命令样例展示了 IK 的使用方式。\n# 1.创建索引 PUT index_ik\n2.创建映射关系 POST index_ik/_mapping { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;ik_max_word\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;ik_smart\u0026quot; } } }\n3.写入文档 POST index_ik/_create/1 {\u0026quot;content\u0026quot;:\u0026quot;美国留给伊拉克的是个烂摊子吗\u0026quot;}\nPOST index_ik/_create/2 {\u0026quot;content\u0026quot;:\u0026quot;公安部：各地校车将享最高路权\u0026quot;}\nPOST index_ik/_create/3 {\u0026quot;content\u0026quot;:\u0026quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船\u0026quot;}\nPOST index_ik/_create/4 {\u0026quot;content\u0026quot;:\u0026quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首\u0026quot;}\n4.高亮查询 POST index_ik/_search { \u0026quot;query\u0026quot; : { \u0026quot;match\u0026quot; : { \u0026quot;content\u0026quot; : \u0026quot;中国\u0026quot; }}, \u0026quot;highlight\u0026quot; : { \u0026quot;pre_tags\u0026quot; : [\u0026quot;\u0026lt;tag1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;tag2\u0026gt;\u0026quot;], \u0026quot;post_tags\u0026quot; : [\u0026quot;\u0026lt;/tag1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;/tag2\u0026gt;\u0026quot;], \u0026quot;fields\u0026quot; : { \u0026quot;content\u0026quot; : {} } } }\n返回内容 { \u0026quot;took\u0026quot;: 14, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 5, \u0026quot;successful\u0026quot;: 5, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: 2, \u0026quot;max_score\u0026quot;: 2, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;fulltext\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 2, \u0026quot;_source\u0026quot;: { \u0026quot;content\u0026quot;: \u0026quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首\u0026quot; }, \u0026quot;highlight\u0026quot;: { \u0026quot;content\u0026quot;: [ \u0026quot;\u0026lt;tag1\u0026gt;中国\u0026lt;/tag1\u0026gt;驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首 \u0026quot; ] } }, { \u0026quot;_index\u0026quot;: \u0026quot;index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;fulltext\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 2, \u0026quot;_source\u0026quot;: { \u0026quot;content\u0026quot;: \u0026quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船\u0026quot; }, \u0026quot;highlight\u0026quot;: { \u0026quot;content\u0026quot;: [ \u0026quot;均每天扣1艘\u0026lt;tag1\u0026gt;中国\u0026lt;/tag1\u0026gt;渔船 \u0026quot; ] } } ] } } 关于 ik_smart 和 ik_max_word #\n ik_smart 和 ik_max_word 是 IK 分词器的两个分词模式。\nik_max_word：将文本根据词典内容做最细粒度的切分。例如,ik_max_word 会将”中华人民共和国国歌”切分到“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国、共和,和,国国,国歌”,详尽生成各种可能的组合。\nik_smart：执行文本粗粒度的切分。例如：ik_smart 会把 ”中华人民共和国国歌”到“中华人民共和国,国歌”。\nik_smart 比较适合短语查询，而 ik_max_word 更合适精准匹配。值得注意的是，由于 ik_smart 做了分词切割的优化，其的分词结果并不是 ik_max_word 的分词结果的子集。\n字段级别词典设置 #  Easysearch 在 IK 分词器原有的功能上增加了自定义字段级别词典的功能。\n自定义字段级别词典的功能支持用户对不同的字段设置不同的分词词库，用户既可以指定 IK 完全使用自有的词库，也支持在 IK 默认的词库上增加自定义的词库内容。\n自定义词库内容 #  默认的词库索引是 .analysis_ik 索引，IK 插件自动初始化的 .analysis_ik 索引。\n用户可以自定义使用某个索引替代 .analysis_ik（设置参数下面会提及），但是要保持和 .analysis_ik 一个的 mapping 结构和使用预设的 pipeline：ik_dicts_default_date_pipeline。\n.analysis_ik 词库需要存储的格式如下：\nPOST .analysis_ik/_doc { \u0026#34;dict_key\u0026#34;: \u0026#34;test_dic\u0026#34;, \u0026#34;dict_type\u0026#34;: \u0026#34;main_dicts\u0026#34;, \u0026#34;dict_content\u0026#34;: \u0026#34;\u0026#34;\u0026#34;中华人民共和国 中文万岁 秋水共长天\u0026#34;\u0026#34;\u0026#34; } 主要使用字段\n dict_content：词典内容字段。各个词典以换行符分隔。 dict_key：自定义词典名。对应自定义词典中设置的 dict_key。 dict_type：字典类型，可选 \u0026ldquo;main_dicts\u0026rdquo;, \u0026ldquo;stopwords_dicts\u0026rdquo;, \u0026ldquo;quantifier_dicts\u0026rdquo; 三个值。其中任意 dict_key 的\u0026quot;main_dicts\u0026quot;必须存在。  如果是对默认词库的自定义增加，dict_key 就写\u0026quot;default\u0026quot;，同时需要使用默认的词库索引 .analysis_ik 。\n设置自定义词库 #  自定义词库的生效主要通过自定义 tokenizer 进行设置。\nPUT my-index-000001 { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_tokenizer\u0026#34; } }, \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;custom_dict_enable\u0026#34;: true, \u0026#34;load_default_dicts\u0026#34;:true, \u0026#34;lowcase_enable\u0026#34;: true, \u0026#34;dict_key\u0026#34;: \u0026#34;test_dic\u0026#34;, \u0026#34;dict_index\u0026#34;:\u0026#34;custom_index\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;test_ik\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34; } } } } 其中\n custom_dict_enable：布尔值，默认 false，true 则可以定制词典读取路径，否则 load_default_dicts / dict_key / dict_index 均失效。 load_default_dicts：布尔值，默认 true，定制的词典是否包含默认的词典库。 lowcase_enable：布尔值，默认为 true，是否大小写敏感，false 则保留原来文本的大小写。 dict_key：string。对应词库索引中的 dict_key 字段内容。如果词典名不匹配，则会装载错误或者直接报错 。 dict_index: string。词库索引名称，默认是 .analysis_ik。可以自定义，但是要保持和 mapping 结构以及 pipeline 一致。  词库内容的自动新增 #  词库的追加内容是能自动被程序探测的，这个主要依赖于 .analysis_ik 的时间戳字段和 pipeline 执行。\nik 只对词库索引的新增词典内容进行自动追加，不会对存量词典的修改进修改或者删除。如果需要对某个存量自定义词典进行修改或者删除，请进行全量 reload。\n# 词典索引写入需要的默认时间戳 pipeline GET _ingest/pipeline/ik_dicts_default_date_pipeline { \u0026#34;ik_dicts_default_date_pipeline\u0026#34;: { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;upload_dicts_timestamp\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{_ingest.timestamp}}\u0026#34;, \u0026#34;override\u0026#34;: true } } ] } } 词典索引的结构 GET .analysis.ik { \u0026quot;.analysis.ik\u0026quot;: { \u0026quot;aliases\u0026quot;: {}, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;dict_content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot; }, \u0026quot;dict_key\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;dict_type\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;upload_dicts_timestamp\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot; } } }, \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;number_of_shards\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;provided_name\u0026quot;: \u0026quot;.analysis.ik\u0026quot;, \u0026quot;default_pipeline\u0026quot;: \u0026quot;ik_dicts_default_date_pipeline\u0026quot;, \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;pattern_tokenizer\u0026quot; } }, \u0026quot;tokenizer\u0026quot;: { \u0026quot;pattern_tokenizer\u0026quot;: { \u0026quot;pattern\u0026quot;: \u0026quot;\\n\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;pattern\u0026quot; } } }, \u0026quot;number_of_replicas\u0026quot;: \u0026quot;1\u0026quot; } } } } 这里 ik_dicts_default_date_pipeline 会对每一条写入词库的数据赋予当前 upload_dicts_timestamp 时间戳。ik 会记录当前词库的最大时间戳，然后每分钟都会去查询一次词库索引现有的最大时间戳。如果查到词库索引的最大的时间戳大于上次记录到的时间戳，则对这段时间内的词库内容都进行加载。\n全量 reload #  reload API 通过对词典库的全量重新加载来实现词典库的更新或者删除。用户可以通过下面的命令实现：\n# 测试索引准备 PUT my-index-000001 { \u0026quot;settings\u0026quot;: { \u0026quot;number_of_shards\u0026quot;: 3, \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_tokenizer\u0026quot; } }, \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_tokenizer\u0026quot;: {\n \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;ik_smart\u0026amp;#34;, \u0026amp;#34;custom_dict_enable\u0026amp;#34;: true, \u0026amp;#34;load_default_dicts\u0026amp;#34;:false, # 这里不包含默认词库 \u0026amp;#34;lowcase_enable\u0026amp;#34;: true, \u0026amp;#34;dict_key\u0026amp;#34;: \u0026amp;#34;test_dic\u0026amp;#34; } } }  }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;test_ik\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot; } } } }\n原来词库分词效果，只预置了分词“自强不息” GET my-index-000001/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;:\u0026quot;自强不息，杨树林\u0026quot; }\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;自强不息\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;CN_WORD\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;杨\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;树\u0026quot;, \u0026quot;start_offset\u0026quot;: 6, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;林\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 3 } ] }\n更新词库 POST .analysis_ik/_doc { \u0026quot;dict_key\u0026quot;: \u0026quot;test_dic\u0026quot;, \u0026quot;dict_type\u0026quot;: \u0026quot;main_dicts\u0026quot;, \u0026quot;dict_content\u0026quot;:\u0026quot;杨树林\u0026quot; }\n删除词库，词库文档的id为coayoJcBFHNnLYAKfTML DELETE .analysis_ik/_doc/coayoJcBFHNnLYAKfTML?refresh=true\n重载词库 POST _ik/_reload {}\n更新后的词库效果 GET my-index-000001/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;:\u0026quot;自强不息，杨树林\u0026quot; }\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;自\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 1, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;强\u0026quot;, \u0026quot;start_offset\u0026quot;: 1, \u0026quot;end_offset\u0026quot;: 2, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;不\u0026quot;, \u0026quot;start_offset\u0026quot;: 2, \u0026quot;end_offset\u0026quot;: 3, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;息\u0026quot;, \u0026quot;start_offset\u0026quot;: 3, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;杨树林\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;CN_WORD\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } 这里是实现索引里全部的词库更新。\n也可以实现某个单独的词库全量更新\nPOST _ik/_reload {\u0026#34;dict_key\u0026#34;:\u0026#34;test_dic”} ","subcategory":null,"summary":"","tags":null,"title":"IK 中文分析器（IK）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/ik-analyzer/"},{"category":null,"content":"Delete by Query #  Delete by Query 删除所有匹配查询条件的文档。 适用于批量清理过期数据、删除特定条件的文档等场景。\n内部流程：scroll 遍历匹配文档 → 逐批执行 bulk delete → 返回统计结果。\n 请求格式 #  POST /\u0026lt;index\u0026gt;/_delete_by_query 路径参数 #     参数 必需 说明     \u0026lt;index\u0026gt; 是 目标索引，支持逗号分隔多索引和通配符    查询参数 #     参数 类型 默认值 说明     refresh boolean false 操作完成后是否刷新受影响的分片   timeout time 1m 超时时间   wait_for_completion boolean true 为 true 时同步等待；为 false 时立即返回 task ID   wait_for_active_shards string — 操作前需要的活跃分片数量   requests_per_second float 无限制 每秒请求数限制。-1 = 不限制   slices int/string 1 并行切片数。\u0026quot;auto\u0026quot; = 按分片数自动拆分   scroll time — scroll 上下文存活时间   scroll_size int 1000 每批 scroll 获取的文档数   conflicts string abort 版本冲突处理：abort = 中止；proceed = 跳过继续   max_docs int 全部 最多删除的文档数量   search_timeout time — 搜索阶段超时   q string — 简单查询字符串   df string — q 参数的默认字段   default_operator string OR q 参数的默认运算符   analyzer string — q 参数使用的分析器   analyze_wildcard boolean false 是否分析通配符   lenient boolean — 宽松解析模式   routing string — 路由值   preference string — 查询偏好   terminate_after int 0 每分片最多处理的文档数   expand_wildcards string open 通配符展开方式   ignore_unavailable boolean false 忽略不存在的索引   allow_no_indices boolean true 允许通配符不匹配任何索引    请求体 #  { \u0026#34;query\u0026#34;: { ... }, \u0026#34;max_docs\u0026#34;: 1000, \u0026#34;conflicts\u0026#34;: \u0026#34;proceed\u0026#34; }    字段 类型 说明     query object 筛选条件，语法同 Query DSL。必需   max_docs int 最多删除的文档数   conflicts string 冲突处理策略     示例 #  基本用法 #  删除所有 status 为 \u0026quot;expired\u0026quot; 的文档：\nPOST /website/_delete_by_query { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;expired\u0026#34; } } } 删除索引中的所有文档 #  POST /website/_delete_by_query { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } }  如果需要清空整个索引，删除并重建索引通常比 delete_by_query 更高效。\n 使用查询字符串 #  POST /website/_delete_by_query?q=status:expired 跳过版本冲突 #  并发场景下，其他进程可能同时修改了文档。设置 conflicts=proceed 跳过冲突继续删除：\nPOST /website/_delete_by_query?conflicts=proceed { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;date\u0026#34;: { \u0026#34;lt\u0026#34;: \u0026#34;2023-01-01\u0026#34; } } } } 流量控制 #  限制每秒处理 200 条，降低集群负载：\nPOST /website/_delete_by_query?requests_per_second=200 { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;archived\u0026#34; } } } 异步执行 #  POST /website/_delete_by_query?wait_for_completion=false { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;date\u0026#34;: { \u0026#34;lt\u0026#34;: \u0026#34;2022-01-01\u0026#34; } } } } 响应返回 task ID：\n{ \u0026#34;task\u0026#34;: \u0026#34;node_id:task_id\u0026#34; } 查询进度：\nGET /_tasks/node_id:task_id 并行切片 #  POST /website/_delete_by_query?slices=auto { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;expired\u0026#34; } } } 限制删除数量 #  最多删除 1000 条匹配文档：\nPOST /website/_delete_by_query { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;max_docs\u0026#34;: 1000 }  响应字段 #  { \u0026#34;took\u0026#34;: 83, \u0026#34;timed_out\u0026#34;: false, \u0026#34;total\u0026#34;: 100, \u0026#34;deleted\u0026#34;: 100, \u0026#34;batches\u0026#34;: 1, \u0026#34;version_conflicts\u0026#34;: 0, \u0026#34;noops\u0026#34;: 0, \u0026#34;retries\u0026#34;: { \u0026#34;bulk\u0026#34;: 0, \u0026#34;search\u0026#34;: 0 }, \u0026#34;failures\u0026#34;: [] }    字段 说明     took 操作耗时（毫秒）   total 匹配的文档总数   deleted 成功删除的文档数   version_conflicts 版本冲突次数   noops 无操作次数   batches scroll 批次数   retries.bulk bulk 重试次数   retries.search search 重试次数   failures 失败详情数组     参考导航 #     需求 参见     单条文档删除  Delete API   按查询条件批量更新  Update by Query   跨索引迁移数据  Reindex   数据生命周期管理  数据生命周期与保留策略    ","subcategory":null,"summary":"","tags":null,"title":"Delete by Query","url":"/easysearch/main/docs/features/document-operations/delete-by-query/"},{"category":null,"content":"语义查询增强处理器（Semantic Query Enricher Processor） #   需要 AI 插件\n semantic_query_enricher 查询重写处理器用于拦截搜索请求，自动为其中的语义查询（semantic / hybrid 等基于 Embedding 模型的查询）注入模型连接信息（URL、API Key、vendor、model ID），使用户在编写查询时无需手动指定这些参数。\n工作原理 #  当搜索请求到达管道时，处理器会遍历查询树，找到所有继承自 ModelBaseQueryBuilder 的查询节点（如 semantic 查询、hybrid 查询中的语义子查询），并为它们绑定 Embedding 请求参数：\n 从处理器配置中读取 url、vendor、api_key 根据 default_model_id 或 vector_field_model_id 为每个语义查询字段分配模型 ID 用户只需在查询中写 semantic 查询和查询文本，无需关心模型连接细节  请求体字段 #     字段 类型 是否必填 说明     url String 是 Embedding 模型服务的 URL   vendor String 是 模型提供商标识（如 openai、dashscope、ollama 等）   api_key String 否 模型服务的 API Key。存储时会自动加密   default_model_id String 条件必填 默认模型 ID，应用于所有未指定模型的语义查询字段。与 vector_field_model_id 至少提供一个   vector_field_model_id Object 条件必填 按向量字段指定模型 ID 的映射。格式：{\u0026quot;field_name\u0026quot;: \u0026quot;model_id\u0026quot;}。与 default_model_id 至少提供一个   tag String 否 处理器标识标签   description String 否 处理器描述   ignore_failure Boolean 否 处理器失败时是否继续执行。默认 false    示例 #  使用全局默认模型 #  PUT /_search/pipeline/my_semantic_pipeline { \u0026#34;rewrite_processors\u0026#34;: [ { \u0026#34;semantic_query_enricher\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;enricher\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;为语义查询注入 Embedding 模型信息\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;sk-xxxxxxxx\u0026#34;, \u0026#34;default_model_id\u0026#34;: \u0026#34;text-embedding-v3\u0026#34; } } ] } 配置管道后，用户查询只需写：\nGET /my_index/_search?search_pipeline=my_semantic_pipeline { \u0026#34;query\u0026#34;: { \u0026#34;semantic\u0026#34;: { \u0026#34;embedding\u0026#34;: { \u0026#34;query_text\u0026#34;: \u0026#34;如何配置集群安全\u0026#34; } } } } 处理器会自动为 semantic 查询注入 Embedding 服务连接信息。\n按字段指定不同模型 #  当索引中不同向量字段使用不同 Embedding 模型时：\nPUT /_search/pipeline/multi_model_pipeline { \u0026#34;rewrite_processors\u0026#34;: [ { \u0026#34;semantic_query_enricher\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;sk-xxxxxxxx\u0026#34;, \u0026#34;vector_field_model_id\u0026#34;: { \u0026#34;title_embedding\u0026#34;: \u0026#34;text-embedding-v3\u0026#34;, \u0026#34;content_embedding\u0026#34;: \u0026#34;text-embedding-v2\u0026#34; } } } ] } 相关文档 #    AI 文本嵌入搜索：完整的语义搜索工作流  混合搜索：semantic_query_enricher 与 hybrid_ranker_processor 配合使用  ","subcategory":null,"summary":"","tags":null,"title":"语义查询增强处理器","url":"/easysearch/main/docs/features/query-dsl/search-pipelines/semantic-query-enricher-processor/"},{"category":null,"content":"小写处理器 #  lowercase 处理器将特定字段中的所有文本转换为小写字母。\n语法 #  以下是为 lowercase 处理器提供的语法：\n{ \u0026#34;lowercase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_name\u0026#34; } } 配置参数 #  下表列出了 lowercase 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要转换的数据的字段名称。支持模板使用。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。如果设置为 true ，则处理器在字段不存在或为 null 时不会修改文档。默认为 false 。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。   target_field 可选 要存储解析数据的字段名称。默认为 field 。默认情况下， field 将就地更新。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 lowercase-title 的管道，该管道使用 lowercase 处理器将文档的 title 字段转换为小写：\nPUT _ingest/pipeline/lowercase-title { \u0026#34;description\u0026#34; : \u0026#34;Pipeline that lowercases the title field\u0026#34;, \u0026#34;processors\u0026#34; : [ { \u0026#34;lowercase\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;title\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/lowercase-title/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;WAR AND PEACE\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;war and peace\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-08-22T17:39:39.872671834Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=lowercase-title { \u0026#34;title\u0026#34;: \u0026#34;WAR AND PEACE\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"小写处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/lowercase/"},{"category":null,"content":"CJK Width 分词过滤器 #  cjk_width 分词过滤器通过将全角 ASCII 字符转换为其标准（半角）ASCII 等效字符，并将半角片假名字符转换为全角等效字符，来对中文、日文和韩文（CJK）词元进行规范化处理。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  转换全角 ASCII 字符 #  在 CJK 文本中，ASCII 字符（如字母和数字）可能会以全角形式出现，占据两个半角字符的空间。全角 ASCII 字符在东亚排版中通常用于与 CJK 字符的宽度对齐。然而，出于索引和搜索的目的，这些全角字符需要规范化为其标准（半角）ASCII 等效字符。\n以下示例说明了 ASCII 字符的规范化过程：\n全角: ＡＢＣＤＥ １２３４５ 规范化 (半角): ABCDE 12345 转换半角片假名字符 #  CJK 宽度分词过滤器会将半角片假名字符转换为对应的全角片假名字符，全角片假名是日语文本中使用的标准形式。如下例所示，这种规范化处理对于文本处理和搜索的一致性至关重要：\n半角 katakana: ｶﾀｶﾅ 规范化 (全角) katakana: カタカナ 参考样例 #  以下示例请求创建了一个名为 cjk_width_example_index 的新索引，并定义了一个包含 cjk_width 过滤器的分词器：\nPUT /cjk_width_example_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;cjk_width_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;cjk_width\u0026#34;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /cjk_width_example_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;cjk_width_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Ｔｏｋｙｏ 2024 ｶﾀｶﾅ\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Tokyo\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;2024\u0026#34;, \u0026#34;start_offset\u0026#34;: 6, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;カタカナ\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;KATAKANA\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"CJK 宽度分词过滤器（CJK Width）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/cjk-width/"},{"category":null,"content":"CJK Bigram 分词过滤器 #  cjk_bigram 分词过滤器是专门为处理东亚语言（如中文、日文和韩文，简称 CJK）而设计的，这些语言通常不使用空格来分隔单词。二元组是指在词元字符串中两个相邻元素的序列，这些元素可以是字符或单词。对于 CJK 语言来说，二元组有助于近似确定单词边界，并捕捉那些能够传达意义的重要字符对。\n相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  参数说明 #  CJK 二元分词过滤器可以通过两个参数进行配置：ignore_scripts（忽略字符集）和 output_unigrams（输出一元组）。\nignore_scripts（忽略字符集） #  CJK 二元分词过滤器会忽略所有非 CJK 字符集（如拉丁字母或西里尔字母等书写系统），并且仅将 CJK 文本分词为二元组。可使用此选项指定要忽略的 CJK 字符集。该选项有以下有效值：\n han（汉字）：汉字字符集处理汉字。汉字是用于中国、日本和韩国书面语言的意符文字。该过滤器可帮助处理文本相关任务，例如对用中文、日文汉字或韩文汉字书写的文本进行分词、规范化或词干提取。 hangul（韩文）：韩文（谚文）字符集处理韩文字符，这些字符是韩语所特有的，不存在于其他东亚字符集中。 hiragana（平假名）：平假名字符集处理平假名，平假名是日语书写系统中使用的两种音节文字之一。平假名通常用于日语的本土词汇、语法元素以及某些形式的标点符号。 katakana（片假名）：片假名字符集处理片假名，片假名是日语的另一种音节文字。片假名主要用于外来借词、拟声词、科学名称以及某些日语词汇。  output_unigrams（输出一元组） #  当此选项设置为 true 时，会同时输出一元组（单个字符）和二元组。默认值为 false。\n参考样例 #  以下示例请求创建了一个名为 devanagari_example_index 的新索引，并定义了一个分词器，该分词器使用了 cjk_bigram_filter 过滤器，且将 ignored_scripts 参数设置为 katakana（片假名）：\nPUT /cjk_bigram_example { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;cjk_bigrams_no_katakana\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;cjk_bigrams_no_katakana_filter\u0026#34; ] } }, \u0026#34;filter\u0026#34;: { \u0026#34;cjk_bigrams_no_katakana_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;cjk_bigram\u0026#34;, \u0026#34;ignored_scripts\u0026#34;: [ \u0026#34;katakana\u0026#34; ], \u0026#34;output_unigrams\u0026#34;: true } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /cjk_bigram_example/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;cjk_bigrams_no_katakana\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;東京タワーに行く\u0026#34; } 测试文本：“東京タワーに行く”\n東京 (Kanji for \u0026#34;Tokyo\u0026#34;) タワー (Katakana for \u0026#34;Tower\u0026#34;) に行く (Hiragana and Kanji for \u0026#34;go to\u0026#34;) 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;東\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;SINGLE\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;東京\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;DOUBLE\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0, \u0026#34;positionLength\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;京\u0026#34;, \u0026#34;start_offset\u0026#34;: 1, \u0026#34;end_offset\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;SINGLE\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;タワー\u0026#34;, \u0026#34;start_offset\u0026#34;: 2, \u0026#34;end_offset\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;KATAKANA\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;に\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;SINGLE\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;に行\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;DOUBLE\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3, \u0026#34;positionLength\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;行\u0026#34;, \u0026#34;start_offset\u0026#34;: 6, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;SINGLE\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;行く\u0026#34;, \u0026#34;start_offset\u0026#34;: 6, \u0026#34;end_offset\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;DOUBLE\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4, \u0026#34;positionLength\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;く\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;SINGLE\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 } ] } ","subcategory":null,"summary":"","tags":null,"title":"CJK 二元组分词过滤器（CJK Bigram）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/cjk-bigram/"},{"category":null,"content":"API #  通过 REST API 可以管理用户、角色、角色映射、权限集合和租户。\nAPI 的访问控制 #  您可以控制哪些角色可以访问安全相关的 API，在配置文件 easysearch.yml:\nsecurity.restapi.roles_enabled: [\u0026#34;\u0026lt;role\u0026gt;\u0026#34;, ...] 如果希望阻止访问特定的 API：\nsecurity.restapi.endpoints_disabled.\u0026lt;role\u0026gt;.\u0026lt;endpoint\u0026gt;: [\u0026#34;\u0026lt;method\u0026gt;\u0026#34;, ...] 参数 endpoint 可以是:\n PRIVILEGE ROLE ROLE_MAPPING USER CONFIG CACHE  参数 method 可以是:\n GET PUT POST DELETE PATCH  例如，以下配置授予三个角色对 REST API 的访问权限，但随后会阻止 test-role 发送 PUT, POST, DELETE, 或 PATCH 到 _security/role 或 _security/user :\nsecurity.restapi.roles_enabled: [\u0026#34;superuser\u0026#34;, \u0026#34;security\u0026#34;, \u0026#34;test-role\u0026#34;] security.restapi.endpoints_disabled.test-role.ROLE: [\u0026#34;PUT\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;PATCH\u0026#34;] security.restapi.endpoints_disabled.test-role.USER: [\u0026#34;PUT\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;PATCH\u0026#34;] 要为 API 配置 使用 PUT 和 PATCH 方法，请将以下行添加到 easysearch.yml：\nsecurity.unsupported.restapi.allow_securityconfig_modification: true 隐藏的保留资源 #  通过修改以下参数，您可以将用户、角色、角色映射和权限集合标记为保留。随后将无法使用 REST API 修改这些资源。\ndashboard_user: reserved: true 同样，可以将用户、角色、角色映射和权限集合标记为隐藏。REST API 不会返回此标志设置为 true 的资源：\ndashboard_user: hidden: true 隐藏的资源默认就是保留的资源。\n账号信息 #  获取账号信息 #  返回当前用户的帐户详细信息。例如，如果您是以 admin 用户身份对请求进行登录的，则响应将包含该用户的详细信息。\n请求 #  GET _security/account 示例 #  curl -XGET -k -u booksuser:password \u0026#39;https://localhost:9200/_security/account?pretty\u0026#39; { \u0026#34;username\u0026#34; : \u0026#34;booksuser\u0026#34;, \u0026#34;reserved\u0026#34; : false, \u0026#34;hidden\u0026#34; : false, \u0026#34;builtin\u0026#34; : true, \u0026#34;external_roles\u0026#34; : [ ], \u0026#34;attributes\u0026#34; : [ ], \u0026#34;roles\u0026#34; : [ \u0026#34;booksrole\u0026#34; ] } 修改密码 #  修改当前用户的密码。\n请求 #  PUT _security/account { \u0026#34;current_password\u0026#34; : \u0026#34;old-password\u0026#34;, \u0026#34;password\u0026#34; : \u0026#34;new-password\u0026#34; } 示例 #  ✗ curl-json -XPUT -k -u booksuser:password \u0026#39;https://localhost:9200/_security/account\u0026#39; -d\u0026#39;{ \u0026#34;current_password\u0026#34; : \u0026#34;password\u0026#34;, \u0026#34;password\u0026#34; : \u0026#34;admin1\u0026#34; }\u0026#39; {\u0026quot;status\u0026quot;:\u0026quot;OK\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;'booksuser' updated.\u0026quot;}% \n权限集合 #  获取权限集合 #  获取一个权限集合信息。\n请求 #  GET _security/privilege/\u0026lt;name\u0026gt; 示例 #  { \u0026#34;custom_action_group\u0026#34;: { \u0026#34;reserved\u0026#34;: false, \u0026#34;hidden\u0026#34;: false, \u0026#34;privileges\u0026#34;: [ \u0026#34;dashboard_all_read\u0026#34;, \u0026#34;indices:admin/aliases/get\u0026#34;, \u0026#34;indices:admin/aliases/exists\u0026#34; ], \u0026#34;description\u0026#34;: \u0026#34;My custom action group\u0026#34;, \u0026#34;static\u0026#34;: false } } 获取权限集合列表 #  获取所有的权限集合列表。\n请求 #  GET _security/privilege/ 示例 #  { \u0026#34;read\u0026#34;: { \u0026#34;reserved\u0026#34;: true, \u0026#34;hidden\u0026#34;: false, \u0026#34;privileges\u0026#34;: [ \u0026#34;indices:data/read*\u0026#34;, \u0026#34;indices:admin/mappings/fields/get*\u0026#34; ], \u0026#34;type\u0026#34;: \u0026#34;index\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Allow all read operations\u0026#34;, \u0026#34;static\u0026#34;: true }, ... } 删除一个权限集合 #  请求 #  DELETE _security/privilege/\u0026lt;name\u0026gt; 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;privilege SEARCH deleted.\u0026#34; } 创建一个权限集合 #  创建或替换指定的限集合。\n请求 #  PUT _security/privilege/\u0026lt;name\u0026gt; { \u0026#34;privileges\u0026#34;: [ \u0026#34;indices:data/write/index*\u0026#34;, \u0026#34;indices:data/write/update*\u0026#34;, \u0026#34;indices:admin/mapping/put\u0026#34;, \u0026#34;indices:data/write/bulk*\u0026#34;, \u0026#34;read\u0026#34;, \u0026#34;write\u0026#34; ] } 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;CREATED\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;my-action-group\u0026#39; created.\u0026#34; } 修改权限集合 #  修改权限集合的属性。\n请求 #  PATCH _security/privilege/\u0026lt;name\u0026gt; [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/privileges\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;indices:admin/create\u0026#34;, \u0026#34;indices:admin/mapping/put\u0026#34;] } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;privilege SEARCH deleted.\u0026#34; } 修改一批权限集合 #  一次修改多个权限集合。\n请求 #  PATCH _security/privilege [ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/CREATE_INDEX\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;privileges\u0026#34;: [\u0026#34;indices:admin/create\u0026#34;, \u0026#34;indices:admin/mapping/put\u0026#34;] } }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/CRUD\u0026#34; } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;privilege SEARCH deleted.\u0026#34; }  用户 #  这些 API 允许您创建、更新和删除内部用户。\n获取用户 #  请求 #  GET _security/user/\u0026lt;username\u0026gt; 实例 #  { \u0026#34;admin\u0026#34;: { \u0026#34;hash\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;captains\u0026#34;, \u0026#34;starfleet\u0026#34;], \u0026#34;attributes\u0026#34;: { \u0026#34;attribute1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;attribute2\u0026#34;: \u0026#34;value2\u0026#34; } } } 获取用户列表 #  请求 #  GET _security/user/ 示例 #  { \u0026#34;admin\u0026#34;: { \u0026#34;hash\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;captains\u0026#34;, \u0026#34;starfleet\u0026#34;], \u0026#34;attributes\u0026#34;: { \u0026#34;attribute1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;attribute2\u0026#34;: \u0026#34;value2\u0026#34; } } } 删除用户 #  请求 #  DELETE _security/user/\u0026lt;username\u0026gt; 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;user admin deleted.\u0026#34; } 创建用户 #  创建或替换指定的用户。您必须指定 password (明文)或 hash (哈希后的密码)。如果您指定 password,安全模块会在存储密码之前自动对密码进行哈希处理。\n请注意，您在 roles 数组中提供的任何角色都必须已经存在，安全模块才能将用户映射到该角色。要查看预定义的角色，请参阅 这里。有关如何创建角色的说明，请参阅 创建角色。\n请求 #  PUT _security/user/\u0026lt;username\u0026gt; { \u0026#34;password\u0026#34;: \u0026#34;adminpass\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;maintenance_staff\u0026#34;, \u0026#34;weapons\u0026#34;], \u0026#34;external_roles\u0026#34;: [\u0026#34;captains\u0026#34;, \u0026#34;starfleet\u0026#34;], \u0026#34;attributes\u0026#34;: { \u0026#34;attribute1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;attribute2\u0026#34;: \u0026#34;value2\u0026#34; } } 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;CREATED\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;User admin created\u0026#34; } 修改用户 #  修改用户信息。\nRequest #  PATCH _security/user/\u0026lt;username\u0026gt; [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/external_roles\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;klingons\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/roles\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;ship_manager\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/attributes\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;newattribute\u0026#34;: \u0026#34;newvalue\u0026#34; } } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;admin\u0026#39; updated.\u0026#34; } 修改一批用户 #  一次修改一批用户的操作。\n请求 #  PATCH _security/user [ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spock\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;testpassword1\u0026#34;, \u0026#34;external_roles\u0026#34;: [\u0026#34;testrole1\u0026#34;] } }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/worf\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;testpassword2\u0026#34;, \u0026#34;external_roles\u0026#34;: [\u0026#34;testrole2\u0026#34;] } }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/riker\u0026#34; } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Resource updated.\u0026#34; }  角色 #  获取角色 #  获取一个角色信息。\n请求 #  GET _security/role/\u0026lt;role\u0026gt; 示例 #  { \u0026#34;test-role\u0026#34;: { \u0026#34;reserved\u0026#34;: false, \u0026#34;hidden\u0026#34;: false, \u0026#34;cluster\u0026#34;: [\u0026#34;cluster_composite_ops\u0026#34;, \u0026#34;indices_monitor\u0026#34;], \u0026#34;indices\u0026#34;: [ { \u0026#34;names\u0026#34;: [\u0026#34;movies*\u0026#34;], \u0026#34;query\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;field_security\u0026#34;: [], \u0026#34;field_mask\u0026#34;: [], \u0026#34;privileges\u0026#34;: [\u0026#34;read\u0026#34;] } ], \u0026#34;static\u0026#34;: false } } 获取角色列表 #  请求 #  GET _security/role/ 示例 #  { \u0026#34;manage_snapshots\u0026#34;: { \u0026#34;reserved\u0026#34;: true, \u0026#34;hidden\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;Provide the minimum permissions for managing snapshots\u0026#34;, \u0026#34;cluster\u0026#34;: [ \u0026#34;manage_snapshots\u0026#34; ], \u0026#34;indices\u0026#34;: [{ \u0026#34;names\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;field_security\u0026#34;: [], \u0026#34;field_mask\u0026#34;: [], \u0026#34;privileges\u0026#34;: [ \u0026#34;indices:data/write/index\u0026#34;, \u0026#34;indices:admin/create\u0026#34; ] }], \u0026#34;static\u0026#34;: true }, ... } 删除一个角色 #  请求 #  DELETE _security/role/\u0026lt;role\u0026gt; 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;role test-role deleted.\u0026#34; } 创建一个角色 #  请求 #  PUT _security/role/\u0026lt;role\u0026gt; { \u0026#34;cluster\u0026#34;: [ \u0026#34;cluster_composite_ops\u0026#34;, \u0026#34;indices_monitor\u0026#34; ], \u0026#34;indices\u0026#34;: [{ \u0026#34;names\u0026#34;: [ \u0026#34;movies*\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;field_security\u0026#34;: [], \u0026#34;field_mask\u0026#34;: [], \u0026#34;privileges\u0026#34;: [ \u0026#34;read\u0026#34; ] }] } 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;test-role\u0026#39; updated.\u0026#34; } 修改角色 #  修改角色的属性。\n请求 #  PATCH _security/role/\u0026lt;role\u0026gt; [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/indices/0/field_security\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;myfield1\u0026#34;, \u0026#34;myfield2\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/indices/0/query\u0026#34; } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;\u0026lt;role\u0026gt;\u0026#39; updated.\u0026#34; } 批量修改角色 #  一次操作批量修改角色信息。\n请求 #  PATCH _security/role [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/role1/indices/0/field_security\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;test1\u0026#34;, \u0026#34;test2\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/role1/indices/0/query\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/role2/cluster\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;manage_snapshots\u0026#34;] } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Resource updated.\u0026#34; }  角色映射 #  获取一个角色映射 #  获取一个角色映射信息。\n请求 #  GET _security/role_mapping/\u0026lt;role\u0026gt; 示例 #  { \u0026#34;role_starfleet\u0026#34;: { \u0026#34;external_roles\u0026#34;: [ \u0026#34;starfleet\u0026#34;, \u0026#34;captains\u0026#34;, \u0026#34;defectors\u0026#34;, \u0026#34;cn=ldaprole,ou=groups,dc=example,dc=com\u0026#34; ], \u0026#34;hosts\u0026#34;: [\u0026#34;*.starfleetintranet.com\u0026#34;], \u0026#34;users\u0026#34;: [\u0026#34;worf\u0026#34;] } } 获取角色映射列表。 #  获取所有的角色映射列表。\n请求 #  GET _security/role_mapping 示例 #  { \u0026#34;role_starfleet\u0026#34;: { \u0026#34;external_roles\u0026#34;: [ \u0026#34;starfleet\u0026#34;, \u0026#34;captains\u0026#34;, \u0026#34;defectors\u0026#34;, \u0026#34;cn=ldaprole,ou=groups,dc=example,dc=com\u0026#34; ], \u0026#34;hosts\u0026#34;: [\u0026#34;*.starfleetintranet.com\u0026#34;], \u0026#34;users\u0026#34;: [\u0026#34;worf\u0026#34;] } } 删除角色映射 #  请求 #  DELETE _security/role_mapping/\u0026lt;role\u0026gt; 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;my-role\u0026#39; deleted.\u0026#34; } 创建角色映射 #  请求 #  PUT _security/role_mapping/\u0026lt;role\u0026gt; { \u0026#34;external_roles\u0026#34; : [ \u0026#34;starfleet\u0026#34;, \u0026#34;captains\u0026#34;, \u0026#34;defectors\u0026#34;, \u0026#34;cn=ldaprole,ou=groups,dc=example,dc=com\u0026#34; ], \u0026#34;hosts\u0026#34; : [ \u0026#34;*.starfleetintranet.com\u0026#34; ], \u0026#34;users\u0026#34; : [ \u0026#34;worf\u0026#34; ] } 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;CREATED\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;my-role\u0026#39; created.\u0026#34; } 修改角色映射 #  请求 #  PATCH _security/role_mapping/\u0026lt;role\u0026gt; [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/users\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;myuser\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/external_roles\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;mybackendrole\u0026#34;] } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;my-role\u0026#39; updated.\u0026#34; } 批量修改角色映射 #  请求 #  PATCH _security/role_mapping [ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/human_resources\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;users\u0026#34;: [\u0026#34;user1\u0026#34;], \u0026#34;external_roles\u0026#34;: [\u0026#34;backendrole2\u0026#34;] } }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/finance\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;users\u0026#34;: [\u0026#34;user2\u0026#34;], \u0026#34;external_roles\u0026#34;: [\u0026#34;backendrole2\u0026#34;] } } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Resource updated.\u0026#34; } 缓存 #  刷新缓存 #  请求 #  DELETE _security/cache 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Cache flushed successfully.\u0026#34; } admin #  修改admin密码 #  进入Easysearch的config路径下，通过证书的方式进行密码修改，具体可查看 这里。\n示例 #  cd easysearch/config #执行 admin 密码修改 curl -k -XPUT --cert admin.crt --key admin.key -H \u0026#39;Content-Type: application/json\u0026#39; \u0026#39;https://localhost:9200/_security/user/admin\u0026#39; -d \u0026#39; { \u0026#34;password\u0026#34;: \u0026#34;C0mp1exP@easysearch\u0026#34;, \u0026#34;external_roles\u0026#34;: [\u0026#34;admin\u0026#34;] }\u0026#39; #用新密码验证是否修改成功 curl -ku 'admin:C0mp1exP@easysearch' https://localhost:9200 \n","subcategory":null,"summary":"","tags":null,"title":"API 接口","url":"/easysearch/main/docs/operations/security/access-control/api/"},{"category":null,"content":"Easysearch Java API Client 使用文档 #  管理索引 #  使用客户端对索引进行管理\nString index = \u0026#34;test1\u0026#34;; if (client.indices().exists(r -\u0026gt; r.index(index)).value()) { LOGGER.info(\u0026#34;Deleting index \u0026#34; + index); DeleteIndexResponse deleteIndexResponse = client.indices().delete(new DeleteIndexRequest.Builder().index(index).build()); LOGGER.info(deleteIndexResponse.toString()); } LOGGER.info(\u0026quot;Creating index \u0026quot; + index); CreateIndexResponse createIndexResponse = client.indices().create(req -\u0026gt; req.index(index)); CloseIndexResponse closeIndexResponse = client.indices().close(req -\u0026gt; req.index(index)); OpenResponse openResponse = client.indices().open(req -\u0026gt; req.index(index)); RefreshResponse refreshResponse = client.indices().refresh(req -\u0026gt; req.index(index)); FlushResponse flushResponse = client.indices().flush(req -\u0026gt; req.index(index)); ForcemergeResponse forcemergeResponse = client.indices().forcemerge(req -\u0026gt; req.index(index).maxNumSegments(1L)); 也可以用异步方式执行\nEasysearchAsyncClient asyncClient = SampleClient.createAsyncClient(); asyncClient.indices().exists(req -\u0026gt; req.index(index)).thenCompose(exists -\u0026gt; { if (exists.value()) { LOGGER.info(\u0026#34;Deleting index \u0026#34; + index); return asyncClient.indices().delete(r -\u0026gt; r.index(index)).thenAccept(deleteResponse -\u0026gt; { LOGGER.info(deleteResponse); }); } return CompletableFuture.completedFuture(null); }).thenCompose(v -\u0026gt; { LOGGER.info(\u0026#34;Creating index \u0026#34; + index); return asyncClient.indices().create(req -\u0026gt; req.index(index));  \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;whenComplete\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;((\u0026lt;/span\u0026gt;createResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; throwable\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;throwable \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;!=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;null\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; LOGGER\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;error\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Error during index operations\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; throwable\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;else\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; LOGGER\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Index created successfully\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;})\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;get\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;30\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; TimeUnit\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;SECONDS\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt;  滚动索引 Rollover #\n 滚动索引是一种管理时序数据的有效方式，当索引满足特定条件时（如大小、文档数量、年龄），会自动创建新的索引。\n示例\n// 创建初始索引并设置别名 String index = \u0026#34;test-00001\u0026#34;; client.indices().create(req -\u0026gt; req.index(index).aliases(\u0026#34;test_log\u0026#34;, a -\u0026gt; a.isWriteIndex(true))); // 配置并执行滚动  RolloverResponse res = client.indices().rollover(req -\u0026gt; req .alias(\u0026quot;test_log\u0026quot;) // 指定别名  .conditions(c -\u0026gt; c .maxDocs(100L) // 文档数量达到100时滚动  .maxAge(b -\u0026gt; b.time(\u0026quot;7d\u0026quot;)) // 索引时间达到7天时滚动  .maxSize(\u0026quot;5gb\u0026quot;))); // 索引大小达到5GB时滚动 Mapping 设置 #\n 基本设置\nString index = \u0026#34;test1\u0026#34;; PutMappingResponse response = client.indices().putMapping(req -\u0026gt; req.index(index) .properties(\u0026#34;field1\u0026#34;, p -\u0026gt; p.keyword(k -\u0026gt; k)) // keyword类型  .properties(\u0026#34;field2\u0026#34;, p -\u0026gt; p.text(t -\u0026gt; t)) // text类型 ); 更完整的示例，包含多种常用字段类型\nresponse = client.indices().putMapping(req -\u0026gt; req.index(index) // 常用字段类型示例  .properties(\u0026#34;keyword_field\u0026#34;, p -\u0026gt; p.keyword(k -\u0026gt; k)) // keyword 类型  .properties(\u0026#34;text_field\u0026#34;, p -\u0026gt; p.text(t -\u0026gt; t)) // text 类型  .properties(\u0026#34;long_field\u0026#34;, p -\u0026gt; p.long_(l -\u0026gt; l)) // long 类型  .properties(\u0026#34;integer_field\u0026#34;, p -\u0026gt; p.integer(i -\u0026gt; i)) // integer 类型  .properties(\u0026#34;short_field\u0026#34;, p -\u0026gt; p.short_(s -\u0026gt; s)) // short 类型  .properties(\u0026#34;byte_field\u0026#34;, p -\u0026gt; p.byte_(b -\u0026gt; b)) // byte 类型  .properties(\u0026#34;double_field\u0026#34;, p -\u0026gt; p.double_(d -\u0026gt; d)) // double 类型  .properties(\u0026#34;float_field\u0026#34;, p -\u0026gt; p.float_(f -\u0026gt; f)) // float 类型  .properties(\u0026#34;date_field\u0026#34;, p -\u0026gt; p.date(d -\u0026gt; d)) // date 类型  .properties(\u0026#34;boolean_field\u0026#34;, p -\u0026gt; p.boolean_(b -\u0026gt; b)) // boolean 类型  .properties(\u0026#34;binary_field\u0026#34;, p -\u0026gt; p.binary(b -\u0026gt; b)) // binary 类型  .properties(\u0026#34;ip_field\u0026#34;, p -\u0026gt; p.ip(i -\u0026gt; i)) // ip 类型  .properties(\u0026#34;geo_point_field\u0026#34;, p -\u0026gt; p.geoPoint(g -\u0026gt; g)) // geo_point 类型  .properties(\u0026#34;flat_field\u0026#34;, p -\u0026gt; p.flattened(f -\u0026gt; f)) // flattened 类型  \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// date 类型   .properties(\u0026quot;date_field\u0026quot;, p -\u0026gt; p.date(d -\u0026gt; d.format(\u0026quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026quot;)))\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// object 类型   .properties(\u0026quot;object_field\u0026quot;, p -\u0026gt; p.object(o -\u0026gt; o .properties(\u0026quot;sub_field1\u0026quot;, sp -\u0026gt; sp.keyword(k -\u0026gt; k)) .properties(\u0026quot;sub_field2\u0026quot;, sp -\u0026gt; sp.long_(l -\u0026gt; l)) ) )\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// nested 类型   .properties(\u0026quot;nested_field\u0026quot;, p -\u0026gt; p.nested(n -\u0026gt; n .properties(\u0026quot;sub_field1\u0026quot;, sp -\u0026gt; sp.keyword(k -\u0026gt; k)) .properties(\u0026quot;sub_field2\u0026quot;, sp -\u0026gt; sp.text(t -\u0026gt; t)) ) ) ); 常用字段类型配置\nresponse = client.indices().putMapping(req -\u0026gt; req.index(index) // text 类型配置  .properties(\u0026#34;text_field2\u0026#34;, p -\u0026gt; p .text(t -\u0026gt; t .analyzer(\u0026#34;standard\u0026#34;) // 分析器  .searchAnalyzer(\u0026#34;standard\u0026#34;) // 搜索分析器  .fields(\u0026#34;raw\u0026#34;, f -\u0026gt; f.keyword(k -\u0026gt; k)) // 添加keyword子字段  .copyTo(\u0026#34;other_field\u0026#34;) // 复制到其他字段  ) )  \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// keyword 类型配置   .properties(\u0026quot;keyword_field2\u0026quot;, p -\u0026gt; p .keyword(k -\u0026gt; k .ignoreAbove(256) // 忽略超过长度的值  .nullValue(\u0026quot;NULL\u0026quot;) // null值替换  .docValues(true) // 是否开启doc_values  ) )\n \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// date 类型配置   .properties(\u0026quot;date_field2\u0026quot;, p -\u0026gt; p .date(d -\u0026gt; d .format(\u0026quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026quot;) // 日期格式  ) ) ); 更新索引 Settings #\n 可以用JSON 的方式更新 特定索引的设置\nString settings = \u0026#34;{\u0026#34; + \u0026#34; \\\u0026#34;index\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;number_of_replicas\\\u0026#34;: 2,\u0026#34; + \u0026#34; \\\u0026#34;refresh_interval\\\u0026#34;: \\\u0026#34;5s\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;max_result_window\\\u0026#34;: 50000,\u0026#34; + \u0026#34; \\\u0026#34;analysis\\\u0026#34;: {\\\u0026#34; + \u0026#34; \\\u0026#34;analyzer\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;my_analyzer\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;custom\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;tokenizer\\\u0026#34;: \\\u0026#34;standard\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;filter\\\u0026#34;: [\\\u0026#34;lowercase\\\u0026#34;, \\\u0026#34;asciifolding\\\u0026#34;]\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34;}\u0026#34;; // 如果要更新静态设置（比如分词器），需要先关闭索引 String index = \u0026quot;test1\u0026quot;; client.indices().close(c -\u0026gt; c.index(index)); // 更新设置 client.indices().putSettings(req -\u0026gt; req .index(index) .withJson(new StringReader(settings)) ); // 重新打开索引 client.indices().open(c -\u0026gt; c.index(index)); 创建带有自定义映射和设置的索引 #\n 使用 Easysearch Java 客户端创建带有自定义映射和设置的索引。\n使用 JSON 配置\nString settingsJson = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;asciifolding\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;tags\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;create_time\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; }, \u0026#34;update_time\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; }, \u0026#34;view_count\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } \u0026#34;\u0026#34;\u0026#34;; // 创建索引 client.indices().create(req -\u0026gt; req .index(\u0026quot;my_index\u0026quot;) .withJson(new StringReader(settingsJson)) );\n创建索引模板 #\n 使用 Easysearch Java 客户端创建索引模板。\nString templateJson = \u0026#34;{\u0026#34; + \u0026#34; \\\u0026#34;index_patterns\\\u0026#34;: [\\\u0026#34;log-*\\\u0026#34;], \u0026#34; + \u0026#34; \\\u0026#34;template\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;settings\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;number_of_shards\\\u0026#34;: 1,\u0026#34; + \u0026#34; \\\u0026#34;number_of_replicas\\\u0026#34;: 1,\u0026#34; + \u0026#34; \\\u0026#34;refresh_interval\\\u0026#34;: \\\u0026#34;5s\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;analysis\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;analyzer\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;my_analyzer\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;custom\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;tokenizer\\\u0026#34;: \\\u0026#34;standard\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;filter\\\u0026#34;: [\\\u0026#34;lowercase\\\u0026#34;]\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;mappings\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;_source\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;enabled\\\u0026#34;: true\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;properties\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;@timestamp\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;date\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;message\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;text\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;analyzer\\\u0026#34;: \\\u0026#34;my_analyzer\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;level\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;keyword\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;service\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;keyword\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;trace_id\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;keyword\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;metrics\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;object\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;properties\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;value\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;double\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;name\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;keyword\\\u0026#34;\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;priority\\\u0026#34;: 100\u0026#34; + \u0026#34;}\u0026#34;; // 创建或更新模板 PutIndexTemplateRequest request = PutIndexTemplateRequest.of(builder -\u0026gt; builder .name(\u0026quot;logs-template\u0026quot;) // 模板名称  .withJson(new StringReader(templateJson)) );\nclient.indices().putIndexTemplate(request); Bulk 批量写入 #\n EasysearchClient client = SampleClient.create(); String json2 = \u0026#34;{\u0026#34; + \u0026#34; \\\u0026#34;@timestamp\\\u0026#34;: \\\u0026#34;2023-01-08T22:50:13.059Z\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;agent\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;version\\\u0026#34;: \\\u0026#34;7.3.2\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;filebeat\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;ephemeral_id\\\u0026#34;: \\\u0026#34;3ff1f2c8-1f7f-48c2-b560-4272591b8578\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;hostname\\\u0026#34;: \\\u0026#34;ba-0226-msa-fbl-747db69c8d-ngff6\\\u0026#34;\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34;}\u0026#34;; BulkRequest.Builder br = new BulkRequest.Builder(); for (int i = 0; i \u0026lt; 10; i++) { br.operations(op -\u0026gt; op.index(idx -\u0026gt; idx.index(indexName).document(JsonData.fromJson(json2)))); } BulkResponse bulkResponse = client.bulk(br.build()); if (bulkResponse.errors()) { for (BulkResponseItem item : bulkResponse.items()) { System.out.println(item.toString()); } } 索引单个文档 #\n String json2 = \u0026#34;{\u0026#34; + \u0026#34; \\\u0026#34;@timestamp\\\u0026#34;: \\\u0026#34;2023-01-08T22:50:13.059Z\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;agent\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;version\\\u0026#34;: \\\u0026#34;7.3.2\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;filebeat\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;ephemeral_id\\\u0026#34;: \\\u0026#34;3ff1f2c8-1f7f-48c2-b560-4272591b8578\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;hostname\\\u0026#34;: \\\u0026#34;ba-0226-msa-fbl-747db69c8d-ngff6\\\u0026#34;\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34;}\u0026#34;; IndexRequest\u0026lt;JsonData\u0026gt; request = IndexRequest.of(i -\u0026gt; i .index(\u0026quot;logs\u0026quot;) .withJson(new StringReader(json2)) ); IndexResponse response = client.index(request); System.out.println(response);\n// 也可以这样 LogEntry logEntry = mapper.readValue(json2, LogEntry.class); IndexRequest\u0026lt;LogEntry\u0026gt; request2 = IndexRequest.of(i -\u0026gt; i .index(indexName) .id(logEntry.getAgent().getEphemeralId()) .document(logEntry) ); IndexResponse response2 = client.index(request2);\n// 或者这样 IndexRequest.Builder\u0026lt;LogEntry\u0026gt; indexReqBuilder = new IndexRequest.Builder\u0026lt;\u0026gt;(); indexReqBuilder.index(indexName); indexReqBuilder.id(logEntry.getAgent().getEphemeralId()); indexReqBuilder.document(logEntry); response2 = client.index(indexReqBuilder.build()); 删除文档 #\n DeleteRequest deleteRequest = new DeleteRequest.Builder() .index(indexName) .id(\u0026#34;3ff1f2c8-1f7f-48c2-b560-4272591b8578\u0026#34;) .build(); DeleteResponse response = client.delete(deleteRequest); deleteByQuery 删除 #  DeleteByQueryRequest deleteByQueryRequest = new DeleteByQueryRequest.Builder() .index(indexName) .query(q -\u0026gt; q.match(new MatchQuery.Builder() .field(\u0026#34;agent.type\u0026#34;).query(\u0026#34;filebeat\u0026#34;).build()) ).build(); DeleteByQueryResponse response = client.deleteByQuery(deleteByQueryRequest); 更新文档 #  UpdateRequest updateRequest = UpdateRequest.of(u -\u0026gt; u .index(indexName) .id(\u0026#34;3ff1f2c8-1f7f-48c2-b560-4272591b8578\u0026#34;) .doc(Map.of(\u0026#34;agent.type\u0026#34;, \u0026#34;logstash\u0026#34;)) ); UpdateResponse\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; response = client.update(updateRequest, Map.class); updateByQuery 更新 #\n Query query = Query.of(q -\u0026gt; q .term(t -\u0026gt; t .field(\u0026#34;agent.type\u0026#34;) .value(v -\u0026gt; v.stringValue(\u0026#34;filebeat\u0026#34;)) ) ); UpdateByQueryRequest updateByQueryRequest = UpdateByQueryRequest.of(u -\u0026gt; u .index(indexName).query(query).script(s -\u0026gt; s.inline(in -\u0026gt; in.source(\u0026#34;ctx._source.agent.type = params.param1\u0026#34;) .lang(\u0026#34;painless\u0026#34;) .params(Map.of(\u0026#34;param1\u0026#34;, JsonData.of(\u0026#34;logstash\u0026#34;))))).refresh(true) ); UpdateByQueryResponse response = client.updateByQuery(updateByQueryRequest); System.out.println(response.updated()); 搜索文档 #\n Query query = Query.of(q -\u0026gt; q .term(t -\u0026gt; t .field(\u0026#34;agent.type\u0026#34;) .value(v -\u0026gt; v.stringValue(\u0026#34;filebeat\u0026#34;)) ) ); SortOptions.Builder sb = new SortOptions.Builder(); SortOptions sortOptions = sb.field(fs -\u0026gt; fs.field(\u0026quot;@timestamp\u0026quot;).order(SortOrder.Desc)).build();\nfinal SearchRequest.Builder searchReq = new SearchRequest.Builder().allowPartialSearchResults(false) .index(indexName) .size(10) .sort(sortOptions) .source(sc -\u0026gt; sc.fetch(true)) .trackTotalHits(tr -\u0026gt; tr.enabled(true)) .query(query);\nSearchResponse\u0026lt;LogEntry\u0026gt; searchResponse = client.search(searchReq.build(), LogEntry.class); System.out.println(searchResponse.hits().total()); for (Hit\u0026lt;LogEntry\u0026gt; hit : searchResponse.hits().hits()) { System.out.println(JsonData.of(hit.source()).toJson(new JacksonJsonpMapper())); } 带子聚合的日期直方图聚合 #\n SearchRequest searchRequest = SearchRequest.of(s -\u0026gt; s .index(index) .size(0) // 不需要返回文档，只要聚合结果  .aggregations(\u0026#34;by_date\u0026#34;, a -\u0026gt; a .dateHistogram(dh -\u0026gt; dh .field(\u0026#34;create_time\u0026#34;) .calendarInterval(CalendarInterval.Month) // 按月聚合数据  .format(\u0026#34;yyyy-MM-dd\u0026#34;) .minDocCount(1) ).aggregations(\u0026#34;avg_price\u0026#34;, avg -\u0026gt; avg // 计算每月的平均价格  .avg(a1 -\u0026gt; a1 .field(\u0026#34;price\u0026#34;) ) ).aggregations(\u0026#34;avg_view_count\u0026#34;, avg -\u0026gt; avg // 计算每月的平均浏览次数  .avg(a1 -\u0026gt; a1 .field(\u0026#34;view_count\u0026#34;) ) ).aggregations(\u0026#34;by_status\u0026#34;, terms -\u0026gt; terms // 统计每月不同状态的文档数量  .terms(t -\u0026gt; t .field(\u0026#34;status\u0026#34;) .size(10) ) ).aggregations(\u0026#34;price_stats\u0026#34;, stats -\u0026gt; stats // 计算价格的统计信息（最小值、最大值、平均值、总和）  .stats(s1 -\u0026gt; s1 .field(\u0026#34;price\u0026#34;) ) ) )); SearchResponse\u0026lt;Void\u0026gt; response = client.search(searchRequest, Void.class); // 处理聚合结果 response.aggregations() .get(\u0026quot;by_date\u0026quot;) .dateHistogram() .buckets() .array() .forEach(bucket -\u0026gt; { // 基本信息  System.out.printf(\u0026quot;\\n日期: %s (文档数: %d)\\n\u0026quot;, bucket.keyAsString(), bucket.docCount());\n \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 平均值   System.out.printf(\u0026quot;平均价格: %.2f\\n\u0026quot;, bucket.aggregations().get(\u0026quot;avg_price\u0026quot;).avg().value()); System.out.printf(\u0026quot;平均浏览: %.2f\\n\u0026quot;, bucket.aggregations().get(\u0026quot;avg_view_count\u0026quot;).avg().value());\n \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 价格统计   StatsAggregate stats = bucket.aggregations().get(\u0026quot;price_stats\u0026quot;).stats(); System.out.printf(\u0026quot;价格统计: 最小值=%.2f, 最大值=%.2f, 平均值=%.2f\\n\u0026quot;, stats.min(), stats.max(), stats.avg());\n \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 状态分布   System.out.println(\u0026quot;状态分布:\u0026quot;); bucket.aggregations() .get(\u0026quot;by_status\u0026quot;) .sterms() .buckets() .array() .forEach(status -\u0026gt; System.out.printf(\u0026quot; %s: %d\\n\u0026quot;, status.key().stringValue(), status.docCount()));\n System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;----------------------------------------\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;});\u0026lt;/span\u0026gt;  Reindex #\n ReindexResponse response = client.reindex(r -\u0026gt; r .source(s -\u0026gt; s.index(\u0026#34;test1\u0026#34;)) .dest(d -\u0026gt; d.index(\u0026#34;test1_new_index\u0026#34;)) .script(sc -\u0026gt; sc .inline(i -\u0026gt; i .source( \u0026#34;if (ctx._source.price != null) { \u0026#34; + \u0026#34; ctx._source.price *= 1.1; \u0026#34; + // 价格上调10%  \u0026#34; ctx._source.updated = true; \u0026#34; + \u0026#34;}\u0026#34; ) .lang(\u0026#34;painless\u0026#34;) ) ) ); 异步方式 Reindex #  ReindexResponse response = client.reindex(r -\u0026gt; r .source(s -\u0026gt; s.index(\u0026#34;test1\u0026#34;)) .dest(d -\u0026gt; d.index(\u0026#34;test1_new_index\u0026#34;)) .script(sc -\u0026gt; sc .inline(i -\u0026gt; i .source( \u0026#34;if (ctx._source.price != null) { \u0026#34; + \u0026#34; ctx._source.price *= 1.1; \u0026#34; + // 价格上调10%  \u0026#34; ctx._source.updated = true; \u0026#34; + \u0026#34;}\u0026#34; ) .lang(\u0026#34;painless\u0026#34;) ) ).waitForCompletion(false) ); String taskId = response.task(); System.out.println(\u0026#34;Started reindex task: \u0026#34; + taskId); // 监控任务进度  boolean completed = false; while (!completed) { try { Thread.sleep(1000); // 每1秒检查一次  GetTasksResponse taskResponse = client.tasks().get(g -\u0026gt; g.taskId(taskId).waitForCompletion(false)); Info taskInfo = taskResponse.task(); if (taskInfo == null) { // 任务可能已完成  System.out.println(\u0026#34;Task completed or not found\u0026#34;); break; }  \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 获取任务状态   JsonData status = taskInfo.status(); if (status != null) { System.out.println(\u0026quot;Running time in millis: \u0026quot; + taskInfo.runningTimeInNanos() / 1_000_000L); System.out.println(\u0026quot;Current status: \u0026quot; + status.toJson()); }\n \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;taskResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;completed\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; completed \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Reindex completed successfully\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 获取结果   JsonData result = taskInfo.status(); if (result != null) { // 解析状态信息中的统计数据  String resultStr = result.toJson().toString(); System.out.println(\u0026quot;Final result: \u0026quot; + resultStr); } }\n \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;catch\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Exception e\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; e\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;printStackTrace\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;break\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt;  附测试用例 #\n public class APITest { EasysearchClient client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; ObjectMapper mapper \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; ObjectMapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; String indexName \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;test1\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;try\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; client \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; SampleClient\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;create\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;catch\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Exception e\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;throw\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; RuntimeException\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;e\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 创建索引并通过 JSON 设置 mappings 和 settings   @Test public void testCreateIndexByJSON() throws IOException { String index = \u0026quot;test1\u0026quot;; ESVersionInfo version = client.info().version(); LOGGER.info(\u0026quot;Server: \u0026quot; + version.number()); if (client.indices().exists(r -\u0026gt; r.index(index)).value()) { LOGGER.info(\u0026quot;Deleting index \u0026quot; + index); DeleteIndexResponse deleteIndexResponse = client.indices().delete(new DeleteIndexRequest.Builder().index(index).build()); LOGGER.info(deleteIndexResponse.toString());\n LOGGER\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Creating index \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; index\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// Settings and mappings in JSON format   String settingsJson = \u0026quot;{\\n\u0026quot; + \u0026quot; \u0026amp;#34;settings\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;number_of_shards\u0026amp;#34;: 1,\\n\u0026quot; + \u0026quot; \u0026amp;#34;number_of_replicas\u0026amp;#34;: 1,\\n\u0026quot; + \u0026quot; \u0026amp;#34;analysis\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;analyzer\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;my_analyzer\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;custom\u0026amp;#34;,\\n\u0026quot; + \u0026quot; \u0026amp;#34;tokenizer\u0026amp;#34;: \u0026amp;#34;standard\u0026amp;#34;,\\n\u0026quot; + \u0026quot; \u0026amp;#34;filter\u0026amp;#34;: [\u0026amp;#34;lowercase\u0026amp;#34;, \u0026amp;#34;asciifolding\u0026amp;#34;]\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;mappings\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;properties\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;id\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;keyword\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;title\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;text\u0026amp;#34;,\\n\u0026quot; + \u0026quot; \u0026amp;#34;analyzer\u0026amp;#34;: \u0026amp;#34;my_analyzer\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;content\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;text\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;status\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;keyword\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;tags\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;keyword\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;create_time\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;date\u0026amp;#34;,\\n\u0026quot; + \u0026quot; \u0026amp;#34;format\u0026amp;#34;: \u0026amp;#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;update_time\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;date\u0026amp;#34;,\\n\u0026quot; + \u0026quot; \u0026amp;#34;format\u0026amp;#34;: \u0026amp;#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;view_count\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;long\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;price\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;double\u0026amp;#34;\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; }\u0026quot;;\n \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// Create index using JSON string   CreateIndexResponse createIndexResponse = client.indices().create(req -\u0026gt; req .index(index) .withJson(new StringReader(settingsJson)) );\n LOGGER\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;createIndexResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; @Test \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;testCatHealth\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; ESVersionInfo version \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Server: \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; version\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;number\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; HealthResponse res \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;cat\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;health\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;res\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; @Test \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;testCatIndices\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; ESVersionInfo version \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Server: \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; version\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;number\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; IndicesResponse res \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;cat\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;indices\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;res\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; @Test \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;testCatNodes\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; ESVersionInfo version \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Server: \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; version\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;number\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; NodesResponse res \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;cat\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;nodes\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;res\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; @Test \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;testScrollQuery\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; SortOptions\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; sb \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SortOptions\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; SortOptions sortOptions \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; sb\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;fs \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; fs\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;@timestamp\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;order\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;SortOrder\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Desc\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; Time time \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Time\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;t \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; t\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;time\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;1m\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; searchReq \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;allowPartialSearchResults\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;false\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Collections\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;singletonList\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;indexName\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;size\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;10\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;sort\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sortOptions\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;source\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sc \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; sc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;fetch\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;trackTotalHits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;tr \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; tr\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;enabled\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;scroll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;time\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;query\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;q \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; q\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;matchAll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; MatchAllQuery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; SearchResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; searchResponse \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;search\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchReq\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(),\u0026lt;/span\u0026gt; LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;class\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; String scrollId \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;null\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; scrollId \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; searchResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;scrollId\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;total\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Hit\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; hit \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; searchResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;_id \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; hit\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;while\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; String finalScrollId \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; scrollId\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; ScrollResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; scrollResponse \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;scroll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sc \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; sc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;scrollId\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;finalScrollId\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;scroll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;time\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;),\u0026lt;/span\u0026gt; LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;class\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;scrollResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;isEmpty\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;break\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Hit\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; hit \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; scrollResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;_id \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; hit\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; scrollId \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; scrollResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;scrollId\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;scrollId \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;!=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;null\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;clear \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; scrollId\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; String finalScrollId \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; scrollId\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;clearScroll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;cs \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; cs\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;scrollId\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;finalScrollId\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; @Test \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;testQuery\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; MatchQuery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; mb \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; MatchQuery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; mb\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;agent\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;query\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;xxx\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; Query query \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Query\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;qb \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; qb\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;match\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;mb\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; Instant now \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Instant\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;now\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; Instant threeDaysAgo \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; now\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;minus\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Duration\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;ofDays\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;3\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;long\u0026lt;/span\u0026gt; threeDaysAgoMillis \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; threeDaysAgo\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;toEpochMilli\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; Query rangequery \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; RangeQuery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;r \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; r\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;@timestamp\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;lte\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;JsonData\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;now\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;toEpochMilli\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;())).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;gte\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;JsonData\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;threeDaysAgoMillis\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;))).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;_toQuery\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; Query boolquery \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Query\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;qb \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; qb\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;bool\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;b \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; b\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;must\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;query\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;must\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;rangequery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)));\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;boolquery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; MatchPhraseQuery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; builder \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; MatchPhraseQuery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; builder\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;agent\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;query\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;xxx\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; Query phraseQuery \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Query\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;qb \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; qb\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;matchPhrase\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;builder\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;phraseQuery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; @Test \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;testSearch\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; SortOptions\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; sb \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SortOptions\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; SortOptions sortOptions \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; sb\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;fs \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; fs\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;@timestamp\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;order\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;SortOrder\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Desc\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; searchReq \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;allowPartialSearchResults\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;false\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Collections\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;singletonList\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;indexName\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;size\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;10\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;sort\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sortOptions\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;source\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sc \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; sc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;fetch\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;trackTotalHits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;tr \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; tr\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;enabled\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;query\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;q \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; q\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;matchAll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; MatchAllQuery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; SearchResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; searchResponse \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;search\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchReq\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(),\u0026lt;/span\u0026gt; LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;class\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;total\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Hit\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; hit \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; searchResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;mapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;writeValueAsString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;hit\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;source\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; @Test \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;testTermsAgg\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; MatchAllQuery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; mb \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; MatchAllQuery\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; Query query \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Query\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;qb \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; qb\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;matchAll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;mb\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; searchReq \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;allowPartialSearchResults\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;false\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Collections\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;singletonList\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;indexName\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;query\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;query\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;size\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;0\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;trackTotalHits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;tr \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; tr\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;enabled\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;aggregations\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;hostname_group\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; a \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; a\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;terms\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;t \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; t\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;agent.hostname.keyword\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)));\u0026lt;/span\u0026gt; SearchResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; searchResponse \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;search\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchReq\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(),\u0026lt;/span\u0026gt; LogEntry\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;class\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;total\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; List\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;StringTermsBucket\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; buckets \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; searchResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;aggregations\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;get\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;hostname_group\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;sterms\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;buckets\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;array\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;StringTermsBucket bucket \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; buckets\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;bucket\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;docCount\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34; terms under \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; bucket\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;key\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;stringValue\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; @Test \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;testSingleDocument\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; Product product \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; Product\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;bk-1\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;City bike\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; 123\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; IndexRequest\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;Product\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; request \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; IndexRequest\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;i \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; i\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;products\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;product\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;getSku\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;document\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;product\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; IndexResponse response \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;request\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Indexed with version \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; response\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; @Test \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;testSingleDocumentDSLAsync\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; Exception \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; Product product \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; Product\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;bk-1\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;City bike\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; 123\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; EasysearchAsyncClient asyncClient \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; SampleClient\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;createAsyncClient\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; CompletableFuture\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;IndexResponse\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; future \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; asyncClient\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;i \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; i\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;products\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;product\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;getSku\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;document\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;product\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;whenComplete\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;((\u0026lt;/span\u0026gt;response\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; exception\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;exception \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;!=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;null\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;err\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Failed to index \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; exception\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;else\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Indexed with version \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; response\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;});\u0026lt;/span\u0026gt; IndexResponse response \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; future\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;get\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Async Indexed with version \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; response\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt;  }\n\n","subcategory":null,"summary":"","tags":null,"title":"API 使用","url":"/easysearch/main/docs/integrations/clients/client-api/client-api/"},{"category":null,"content":"Shingle 分词过滤器 #  shingle 分词过滤器用于从输入文本中生成词 n 元组（即词片）。例如，对于字符串 \u0026ldquo;slow green turtle\u0026rdquo;，词片过滤器会创建以下一元词片和二元词片：\u0026ldquo;slow\u0026rdquo;、\u0026ldquo;slow green\u0026rdquo;、\u0026ldquo;green\u0026rdquo;、\u0026ldquo;green turtle\u0026rdquo; 以及 \u0026ldquo;turtle\u0026rdquo;。\n这个分词过滤器常常与其他过滤器结合使用，通过对短语而不是单个词元进行索引来提高搜索的准确性。\n相关指南（先读这些） #    邻近匹配  文本分析：识别词元  文本分析：规范化  参数说明 #  词片分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     min_shingle_size 可选 整数 要连接的词元的最小数量。默认值为 2。   max_shingle_size 可选 整数 要连接的词元的最大数量。默认值为 2。   output_unigrams 可选 布尔值 是否将一元词（单个词元）包含在输出中。默认值为 true。   output_unigrams_if_no_shingles 可选 布尔值 如果未生成词片，是否输出一元词。默认值为 false。   token_separator 可选 字符串 用于将词元连接成词片的分隔符。默认值为一个空格（ ）。   filler_token 可选 字符串 插入到词元之间的空位置或间隙中的词元。默认值为下划线（_）。     如果 output_unigrams 和 output_unigrams_if_no_shingles 都设置为 true，则 output_unigrams_if_no_shingles 将被忽略。\n 参数说明 #  以下示例请求创建了一个名为“my-shingle-index”的新索引，并配置了一个带有词片过滤器的分词器。\nPUT /my-shingle-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_shingle_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;shingle\u0026#34;, \u0026#34;min_shingle_size\u0026#34;: 2, \u0026#34;max_shingle_size\u0026#34;: 2, \u0026#34;output_unigrams\u0026#34;: true } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_shingle_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_shingle_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-shingle-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_shingle_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;slow green turtle\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;slow\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;slow green\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;shingle\u0026#34;, \u0026#34;position\u0026#34;: 0, \u0026#34;positionLength\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;green turtle\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;shingle\u0026#34;, \u0026#34;position\u0026#34;: 1, \u0026#34;positionLength\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"词片分词过滤器（Shingle）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/shingle/"},{"category":null,"content":"脚本处理器（Script Processor） #  版本引入：1.14.0\nscript 查询重写处理器用于拦截搜索请求，并在请求中添加一个内联的 Painless 脚本，该脚本会在接收到请求时执行。脚本仅能操作以下请求字段：\n from size explain version seq_no_primary_term track_scores track_total_hits min_score terminate_after profile  请求体字段 #  下表列出了该处理器支持的所有配置字段。\n   字段 数据类型 说明     source 内联脚本 要执行的脚本代码。必填。   lang 字符串 脚本语言。可选，默认为 painless，目前仅支持 painless。   tag 字符串 处理器的唯一标识符，用于调试或跟踪。可选。   description 字符串 对该处理器的描述信息。可选。   ignore_failure 布尔值 若为 true，当此处理器执行失败时，Easysearch 将忽略错误并继续执行后续处理器。可选，默认值为 false。    示例 #  以下请求创建一个名为 explain_one_result 的搜索管道，其中包含一个 script 查询重写处理器。该脚本的作用是：当请求返回多个结果时自动关闭 explain 功能，因为 explain 是一项开销较大的操作；仅在返回单个结果时启用。\nPUT /_search/pipeline/explain_one_result { \u0026#34;description\u0026#34;: \u0026#34;一个仅对单个结果启用 explain 操作的搜索管道\u0026#34;, \u0026#34;rewrite_processors\u0026#34;: [ { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;if (ctx._source[\u0026#39;size\u0026#39;] \u0026gt; 1) { ctx._source[\u0026#39;explain\u0026#39;] = false } else { ctx._source[\u0026#39;explain\u0026#39;] = true }\u0026#34; } } ] } 说明 #   ctx._source 表示当前搜索请求的请求体内容。 上述脚本逻辑为：如果 size（返回结果数量）大于 1，则设置 \u0026quot;explain\u0026quot;: false，否则设置为 true。 这样可以避免在大量结果上执行高开销的评分解释（explain），从而提升性能。  使用搜索管道 #  在搜索请求中通过 search_pipeline 参数指定该管道：\nGET /my_index/_search?search_pipeline=explain_one_result { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;public\u0026#34; } }, \u0026#34;size\u0026#34;: 1 } ","subcategory":null,"summary":"","tags":null,"title":"脚本处理器","url":"/easysearch/main/docs/features/query-dsl/search-pipelines/script-processor/"},{"category":null,"content":"文档建模 #  这一页回答两个问题：应该把什么放进一个文档？字段怎么设计才适合搜索？ 这里只讲单个文档层面的建模，跨文档关系放在“数据建模”章节。\n什么是文档 #  在 Easysearch 中，一个 文档（Document） 是被序列化为 JSON 的最顶层对象，指定了唯一 ID 并存储到 Easysearch 中。例如：\n{ \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;age\u0026#34;: 42, \u0026#34;confirmed\u0026#34;: true, \u0026#34;join_date\u0026#34;: \u0026#34;2014-06-01\u0026#34;, \u0026#34;home\u0026#34;: { \u0026#34;lat\u0026#34;: 51.5, \u0026#34;lon\u0026#34;: 0.1 }, \u0026#34;accounts\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;facebook\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;johnsmith\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;twitter\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;johnsmith\u0026#34; } ] } 文档可以包含字符串、数字、布尔、日期、嵌套对象、数组等多种类型。\n文档元数据 #  每个文档都有三个核心元数据：\n   元数据 说明     _index 文档存放的索引，是逻辑命名空间   _id 文档的唯一标识符，可自定义或自动生成   _source 文档的原始 JSON 内容    此外，每个文档还有 _version 字段——每次对文档修改（包括删除）时版本号递增，用于并发控制。\n一个“好文档”的几个特征 #   字段含义清晰、类型正确（text/keyword/数值/日期等） 能支撑主要的搜索与聚合需求，而不是只反映存储表结构 重要的过滤/排序/聚合维度都有对应字段 更新策略是可控的（全量更新、部分更新、幂等写入等）  可以先从“业务查询需求”倒推字段设计，而不是从数据库表结构直接平铺。\n字段类型与多字段（multi-fields） #  同一份业务信息，往往需要不同的查询方式，例如“商品名称”：\n 既要支持全文检索（模糊匹配） 又要支持精确匹配/去重/聚合（关键词级别）  推荐做法是使用多字段（multi-fields），例如：\n name（text）：用于全文检索 name.keyword（keyword）：用于精确过滤、排序、聚合  实务建议：\n 人类可读文本：通常需要 text + keyword 两个视角 ID、编码、枚举：直接用 keyword/数值/布尔类型即可 金额、计数、比例：用适当的数值类型，并为排序/聚合做好 doc_values 支持  标识符与路由字段 #  _id 与业务字段的考虑：\n _id：内部唯一标识，用于幂等写入、精确读取/删除 业务 ID（如 user_id、order_id）：通常也会单独做成字段，用于过滤、聚合、权限控制等  在多租户/分库分表等场景下，可以考虑额外的路由字段（如 tenant_id），后续在索引设计和路由策略中使用（见“数据建模”“分布式基础”章节）。\n规范化与冗余 #  与传统数据库“强规范化”不同，面向搜索的文档往往会有适度冗余：\n 预先把常用的派生信息存进文档（如标准化后的地区名、拼音、缩写） 把查询高频的外键信息“带过来”，减少查询时的 join 需求  但冗余也要有边界：\n 冗余会放大存储与更新成本 冗余字段过多，会让 mapping 变得臃肿、难以维护  经验做法：\n 只冗余“确实会被高频查询/排序/聚合”的字段 对变动频率极高的冗余信息，要慎重评估更新成本  更新模式与幂等性 #  常见的文档更新模式：\n 全量替换（put index）：一次写入整个文档  简单可靠，适合文档相对较小、更新频率不高的场景   部分更新（update + doc/script）：只更新部分字段  适合字段很多但部分字段频繁变更的场景    从建模角度，需要考虑：\n 是否有字段可以晚一点异步更新（例如离线计算得出的统计值） 是否需要为“最终一致”的字段准备单独的更新通道  在“写入与 Bulk”“并发控制与版本”章节中会更详细讨论更新与幂等的实现，这里只强调：文档结构要能支撑你想要的更新模式。\n并发控制：乐观锁 #  Easysearch 使用乐观并发控制：不会阻塞操作，而是利用版本号来避免冲突。\n在更新文档时指定版本号，只有版本匹配时更新才会成功：\nPUT /website/_doc/1?if_seq_no=5\u0026amp;if_primary_term=1 { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Starting to get the hang of this...\u0026#34; } 如果版本冲突（其他进程已更新），Easysearch 返回 409 Conflict，应用程序可以选择重试或通知用户。\n常见场景：\n 不关心冲突：直接写入覆盖（如从主数据库同步数据） 需要避免冲突：使用 if_seq_no + if_primary_term 做乐观锁 外部版本号：如果主数据库已有版本号（如时间戳），可通过 version_type=external 复用  小结 #   文档是 JSON 对象，包含 _index、_id、_source 等元数据 以\u0026quot;搜索/过滤/聚合需求\u0026quot;驱动字段设计，而不是简单照搬表结构 合理利用 multi-fields 统一同时支持全文检索与精确过滤 为标识符与路由预留好字段，方便后续扩展数据建模与多租户策略 适度冗余换取查询简化与性能，但要控制冗余的数量与更新成本  下一步可以继续阅读：\n  文档操作  映射基础  数据建模  ","subcategory":null,"summary":"","tags":null,"title":"文档建模","url":"/easysearch/main/docs/fundamentals/document-model/"},{"category":null,"content":"KV 处理器 #  kv 处理器会自动提取特定事件字段或消息，这些字段或消息以 key=value 格式存在。这种结构化格式通过将数据根据键和值分组来组织您的数据。这对于分析、可视化和使用数据非常有帮助，例如用户行为分析、性能优化或安全调查。\n以下是为 kv 处理器提供的语法：\n{ \u0026#34;kv\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;field_split\u0026#34;: \u0026#34; \u0026#34;, \u0026#34;value_split\u0026#34;: \u0026#34; \u0026#34; } } 配置参数 #  下表列出了 kv 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要解析的数据的字段名称。   field_split 必填 键值对分割的正则表达式模式。   value_split 必填 从键值对中分割键和值的正则表达式模式，例如，等号 = 或冒号 : 。   exclude_keys 可选 要排除的文档中的键。默认为 null 。   include_keys 可选 用于过滤和插入的键。默认为包含所有键。   prefix 可选 添加到提取键的前缀。默认为 null 。   strip_brackets 可选 如果设置为 true ，则从提取的值中去除括号（ () 、 \u0026lt;\u0026gt;, 或 [] ）和引号（ ' 或 \u0026quot; ）。默认为 false 。   trim_key 可选 从提取键中修剪的字符字符串。   trim_value 可选 从提取值中修剪的字符字符串。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。默认为 false 。   target_field 可选 要插入提取的键的字段名称。默认为 null 。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 kv-pipeline 的管道，该管道使用 kv 处理器提取文档的 message 字段：\nPUT _ingest/pipeline/kv-pipeline { \u0026#34;description\u0026#34; : \u0026#34;Pipeline that extracts user profile data\u0026#34;, \u0026#34;processors\u0026#34; : [ { \u0026#34;kv\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;message\u0026#34;, \u0026#34;field_split\u0026#34;: \u0026#34; \u0026#34;, \u0026#34;value_split\u0026#34;: \u0026#34;=\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/kv-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;:{ \u0026#34;message\u0026#34;: \u0026#34;goodbye=everybody hello=world\u0026#34; } } ] } 以下示例响应确认，除了原始的 message 字段外，该文档还包含由键值对生成的字段：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;goodbye=everybody hello=world\u0026#34;, \u0026#34;goodbye\u0026#34;: \u0026#34;everybody\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-12-06T09:59:21.823292Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=kv-pipeline { \u0026#34;message\u0026#34;: \u0026#34;goodbye=everybody hello=world\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"KV 处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/kv/"},{"category":null,"content":"Java 客户端 #  Easysearch Java API Client 是 Easysearch 的官方 Java 客户端，提供了简洁、强大且类型安全的 API 接口。\n相关指南（先读这些） #     Java 客户端集成\n   客户端集成\n  全新重构的 2.0.x 版本，更轻量级的设计，移除冗余依赖。\n  兼容 Easysearch 各个版本。\n  为常用 Easysearch API 提供强类型的请求和响应。\n  API 均支持阻塞和异步方式。\n  使用流式构建器和函数式模式，以在创建复杂嵌套结构时编写简洁易读的代码。\n  通过使用 Jackson 无缝集成应用程序类。\n  快速开始 #  本页指导您完成Java客户端的安装过程，展示了如何实例化客户端，以及如何使用它执行基本的 Easysearch 操作。\n安装 #  easysearch-client 已经发布到 Maven https://mvnrepository.com/artifact/com.infinilabs/easysearch-client/2.0.2\n安装需要 jdk8 或以上版本\neasysearch-client 使用 Jackson 将业务代码和客户端 api 进行集成。\n在 Maven 项目中安装 #  相比 1.x 版本的客户端，新版客户端的安装更加简单，只需在您项目的 pom 文件的 dependencies 区域添加以下依赖以引入客户端\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.infinilabs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;easysearch-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 在 gradle 项目中安装 #\n 在您项目的 build.gradle 文件的 dependencies 区域添加以下依赖\ndependencies { implementation \u0026#39;com.infinilabs:easysearch-client:2.0.2\u0026#39; } 初始化客户端 #  假设您本地启动了 Easysearch 服务，并启用了安全通信加密和 security， 可以使用以下代码初始化客户端连接。\npublic static EasysearchClient create() throws NoSuchAlgorithmException, KeyStoreException, KeyManagementException { boolean https = true;  \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; HttpHost\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;[]\u0026lt;/span\u0026gt; hosts \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; HttpHost\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;[]{\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; HttpHost\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;localhost\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; 9200\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;https\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)};\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; SSLContext sslContext \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; SSLContextBuilder\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;create\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;loadTrustMaterial\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;null\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;chains\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; authType\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; SSLIOSessionStrategy sessionStrategy \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SSLIOSessionStrategy\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sslContext\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; NoopHostnameVerifier\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;INSTANCE\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; CredentialsProvider credentialsProvider \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; BasicCredentialsProvider\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; credentialsProvider\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setCredentials\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;AuthScope\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;ANY\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; UsernamePasswordCredentials\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;username\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;passwowd\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; RestClient restClient \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; RestClient\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;hosts\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setHttpClientConfigCallback\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;httpClientBuilder \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; httpClientBuilder\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setDefaultCredentialsProvider\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;credentialsProvider\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setSSLStrategy\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sessionStrategy\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;disableAuthCaching\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setRequestConfigCallback\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;requestConfigCallback \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; requestConfigCallback\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setConnectTimeout\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;30000\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setSocketTimeout\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;300000\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; EasysearchTransport transport \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; RestClientTransport\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt; restClient\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; JacksonJsonpMapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; EasysearchClient\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;transport\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt;  \n","subcategory":null,"summary":"","tags":null,"title":"Java 客户端","url":"/easysearch/main/docs/integrations/clients/client-api/java-client/"},{"category":null,"content":"版本发布日志 #  这里是 INFINI Easysearch 历史版本发布的相关说明。\nLatest (In development) #  Breaking changes #  Features #  Bug fix #  Improvements #  2.1.0 (2026-2-13) #  Breaking changes #  Features #   新增 Rules 规则引擎插件，提供高性能的规则匹配能力  支持 linux-x64 和 linux-aarch64 架构 支持 Ingest Pipeline 集成，数据写入时自动匹配规则并添加标签 支持复杂的规则表达式（AND/OR/NOT、near、正则、数值范围等） 支持百万级规则库，匹配性能是传统方案的上百倍。 支持多节点集群自动同步和广播编译规则 节点启动时自动同步缺失的规则库 规则库同步期间自动保护写入，确保规则完整性 本地元数据文件持久化记录编译历史，支持规则库文件丢失后的自动恢复   新增形态学分析插件（analysis-morphology），支持俄语和英语的形态分析  精准还原：基于词典将动词时态、名词格位等还原为标准原型（如 went → go） 词元扩展：同时索引原词与关联词根（如runner → runner, run），实现智能搜索匹配 高召回率：解决俄语复杂的变格与变位搜索难题，确保不同语法形式下均能精准检索   审计日志新增动态指定用户进行审计的功能 UI 插件新增如下能力  支持审计日志在线查看 支持审计日志模块动态配置 新增数据探索页面    Bug fix #  Improvements #   将“结巴”分词插件日志迁移至 Log4J，并降低周期性任务的日志级别以减少冗余  2.0.2 (2025-12-17) #  Breaking changes #  Features #   语义搜索新增支持 NestedQueryBuilder KNN mapping 的 L 和 k 参数支持大小写不敏感，提升易用性  Bug fix #   修复了开发者工具的主题颜色显示问题  Improvements #   UI 插件静态文件支持 gzip 压缩，加快页面加载 优化了图标资源大小 调整了内部构建流程和 CSP 策略  2.0.1 (2025-12-03) #  Breaking changes #  Features #  Bug fix #   修复 UI 模块一些显示问题  Improvements #  2.0.0 (2025-11-21) #  Breaking changes #   Easysearch 2.0.0 版本底层 Lucene 更新到 9.12.2 新增 ui 插件，为 Easysearch 提供了轻量级界面化管理功能，不再依赖第三方对集群进行管理，真正做到开箱即用  Features #   兼容 1.15.x 版本的索引，可无缝升级 新增 UI 插件，涵盖从集群，节点，索引，到分片等不同维度的监控和管理功能 支持关闭 security 进入 UI  Bug fix #  Improvements #   range 查询，按数字类型字段排序，相比旧版本效率大幅提升  1.15.6 (2025-10-31) #  Breaking changes #  Features #  Bug fix #   修复 bug  Improvements #  1.15.5 (2025-10-20) #  Breaking changes #  Features #  Bug fix #   修复索引不存在时没有正确返回客户端 index_not_found_exception 的错误  Improvements #  1.15.4 (2025-10-11) #  Breaking changes #  Features #  Bug fix #   修复 客户端请求的路径为 /ui 时不能正常显示欢迎页的问题  Improvements #   改进 rollup 索引的模版 增强对 cloud 的集成  1.15.3 (2025-09-28) #  Breaking changes #  Features #   为索引操作添加新的菜单项 节点详情新增树状统计字段存储 添加热点线程页面  Bug fix #  Improvements #  1.15.2 (2025-09-21) #  Breaking changes #  Features #   集群页面新增「临时配置」、「限流限速」以及「其他配置」页面 索引详情页面新增「限流」页面  Bug fix #  Improvements #   重构索引详情页面的「设置」页面 去掉 ILM 配置索引的前缀，并兼容旧索引 index-management 从plugin 移动到 modules 精简证书错误时的日志输出  1.15.1 (2025-09-14) #  Breaking changes #   更新创建搜索管道的 API 的 json 结构和说明文档  Features #   ui 插件新增  备份快照管理功能 跨集群复制管理功能 数据流管理功能    Bug fix #  Improvements #   改进 search_pipeline 的统计指标  1.15.0 (2025-09-08) #  Breaking changes #   新增 ui 插件，为 Easysearch 提供了轻量级界面化管理功能，不再依赖第三方对集群进行管理，真正做到开箱即用 正式发布混合搜索模块，结合了关键词搜索和语义搜索，以提升搜索相关性 针对安全模块的角色名称进行规范，废弃不符合规范的角色，  Features #   新增 ui 插件，涵盖从集群，节点，索引，到分片等不同维度的监控和管理功能。 ai 插件正式提供混合搜索能力 引入 license 允许动态的跨模板重用设置  Bug fix #  Improvements #   改进角色名称和描述 增加 数据流（Data streams）说明文档 更新搜索管道相关文档  1.14.0 (2025-07-25) #  Breaking changes #   AI 模块 从 modules 迁移至 plugins 目录下，方便调用 knn 插件 旧的文本向量化接口 _ai/embed 已不再支持，将在后续版本删除  Features #   插件模块新增完整的文本嵌入模型集成功能，涵盖从数据导入到向量检索的全流程 新增语义检索 API，简化向量搜索使用流程 新增语义检索处理器配置大模型信息 新增搜索管道（Search pipelines），轻松地在 Easysearch 内部处理查询请求和查询结果 多模型集成支持  OpenAI 向量模型：直接调用 OpenAI 的嵌入接口（如 text-embedding-3-small） Ollama 本地模型：支持离线环境或私有化部署的向量生成   ik 分词器提供 reload API，能够对存量自定义词典进行完整更新 ik 分词器能够通过词库索引对默认词库进行自定义添加  Bug fix #  Improvements #   增强数据摄取管道（ingest pipeline）  在数据索引阶段支持文本向量化，文档可自动生成向量表示 导入数据时通过 ingest 管道进行向量化时支持单条和批量模式，适配大模型的请求限制场景   更新 Easysearch Docker 初始化文档 ik 分词器优化自定义词库加载逻辑，减少内存占用  1.13.1 (2025-06-29) #  Breaking changes #  Features #   插件模块新增 dependencyModule 配置项，用于声明共享的 common 模块依赖  Bug fix #   修复了前缀查询请求在只包含一个字符时的空指针错误  Improvements #   精简了部署后的 modules和 plugins 大小  1.13.0 (2025-06-16) #  Breaking changes #  Features #   Rollup 新增对已创建 job 的 interval 和 page_size 参数更新的 api Rollup 索引数据增加 unique 字段标识当前 job 的数据 Rollup 配置增加 window_start_time 字段，当重建 Rollup 时 会把历史 metadata 的最新时间戳写到 window_start_time 里  Bug fix #  Improvements #   Rollup 会自动检测源数据索引是否有新增字段，并更新到 metrics ILM 删除索引时 增加对 Rollup 的运行状态判断，未处理完的索引不会删除  1.12.3 (2025-05-31) #  Breaking changes #  Features #   Rollup Job 支持断点续跑： 当检测到历史 Rollup 索引及元数据时，新创建的 Rollup Job 会自动从中断的状态点继续处理，避免重复计算，显著节省计算资源  Bug fix #   修复前端 Rollup 查询时如果只有历史数据，不能正确返回 pipeline 聚合结果的错误  Improvements #   优化 Rollup job 的 top_hits 聚合效率 提升启用 source_reuse 时的文档获取性能  1.12.2 (2025-05-18) #  Breaking changes #  Features #   NodeStats 新增 rollup 的 运行状态，方便监控 更新 ik-analyzer 文档  Bug fix #  Improvements #   当设置了 JAVA_HOME 时，优先使用本地 JDK 新增安装为 linux 服务的文档  1.12.1 (2025-04-28) #  Breaking changes #  Features #   索引合并新增按照日期范围合并的策略：通过 index.merge.policy.time_range_field 配置项指定合并依据的字段  Bug fix #   修复 ollama_url 不能动态更新的错误 修复 ollama api 未正确兼容单个文本请求 索引生命周期管理 delete action 按文档最新时间删除时修正为按降序排序  Improvements #  1.12.0 (2025-03-28) #  Breaking changes #  Features #   Rollup 新增 write_optimization 配置项，启用后采用自动生成文档 ID 的策略，大幅提升写入速度 Rollup 现在支持针对 job 级别配置 rollover 的 max docs  Bug fix #   Rollup 修复带有内嵌的 pipeline 聚合时不能和原始索引聚合正常合并的问题  Improvements #   优化了 rollup 索引字段名长度，减小 rollup job 运行时的内存占用  1.11.1 (2025-03-14) #  Breaking changes #  Features #   新增 AI 模块，集成 Ollama embedding API，支持文本向量化  Bug fix #   修复 DateRange 聚合在 Rollup 查询中无法正确合并的问题  Improvements #  针对用户使用体验进行了多项改进，包括：\n 弃用 KNN 模块中的 index.knn 配置项，（此配置项和其他功能经常发生冲突） 简化配置逻辑，该配置项将在后续版本中移除 将 KNN 搜索功能从插件形式集成为内置功能，无需额外安装即可使用 将跨集群复制（CCR）功能从插件形式集成为内置功能，开箱即用 优化索引配置更新验证：增加非动态配置项的值比对，避免误报  1.11.0 (2025-02-28) #  Features #   新增 wildcard 数据类型 新增 Point in time 搜索 API 新增异步搜索 API 数值字段添加 doc-values 搜索支持 日期字段添加 doc-values 搜索支持 新增 IK 分词器自定义词典使用文档  Bug fix #  Improvements #   优化 Lucene flush 的 segment 大小，减少 I/O 开销  1.10.2 (2025-02-17) #  Features #    lucene 版本更新\n lucene 版本更新到 8.11.4，是 lucene8.x 系列的最后一个版本 jna 更新到 5.12.1    IK 分词器: 增强词典配置的灵活性和可扩展性\n 支持字段级别的词典配置，用户可通过自定义 tokenizer 为不同索引、不同字段配置专用词典 优化词典管理机制 支持自定义词典与 IK 默认词典合并使用 词库数据存储在可配置的索引中，支持实时更新 可使用内置词库索引或自定义词库索引(需保持相同结构)    索引生命周期管理\n delete action 支持同时基于索引创建时间和文档最新时间戳来执行删除操作    Bug fix #   Rollup  修复了平均值(avg)聚合计算错误    Improvements #   优化 rollup 索引的创建流程  1.10.1 (2025-01-24) #  Features #   Rollup 增加支持聚合的种类  增加支持 Filter aggregation，某些场景可以用来替代 query 增加针对个别字段自定义 special_metrics 指标的配置项 增加支持 Bucket sort aggregation   Rollup 查询 API 提供了 debug 参数，有助于调试  Bug fix #   修复数据节点和 master 节点角色分离时，Security 索引创建失败问题  Improvements #   Rollup 查询 增加 response 标识是否有 rollup 数据 Rollup response total hits 不再为 0 Rollup job 支持 更新操作，通过更新索引文档实现 rollup.hours_before 配置项只影响查询时间范围，不影响写入  1.10.0 (2025-01-11) #  Features #   Rollup 功能增强：新增并发限制、任务失败自动重启功能，支持批量启动、停止 Job， 并支持 date_range 聚合。 字段类型功能优化：新增 flattened_text 和 match_only_text 字段类型，支持更多查询场景；  1.9.0 (2024-10-17) #  Breaking changes #  Features #    发布 rollup 功能\n  支持自动对 rollup 索引进行滚动，无需外部触发\n  支持 avg sum max min value_count percentiles 指标类型的聚合\n  支持 terms 聚合\n  支持对指标聚合进行 Pipeline 聚合\n  支持聚合前先对数据进行过滤\n  进行聚合查询时支持直接搜索原始索引，不用更改搜索代码\n  增加适配 logstash8.x 的请求 header\n  _cat/templates 增加 lifecycle 和 rollover 列的展示\n  Bug fix #   修复 rest-api template 测试错误  1.8.3 (2024-08-13) #  Bug fix #   修复特定场景下 lucene 栈溢出问题 修复特定场景下字节处理空指针问题  Improvements #   更新依赖库至安全版本 优化开启 source_reuse 后内存使用性能 增加初始化密码环境变量,可手工设置 EASYSEARCH_INITIAL_ADMIN_PASSWORD 环境变量。  1.8.2 (2024-06-06) #  Breaking changes #  Features #  Bug fix #   修复 source_reuse 与 索引 mapping 的 enable: false 冲突  Improvements #   升级部分依赖包版本，commons-collections to 3.2.2, snakeyaml to 2.0 优化 CCR 同步性能及调整 CCR 全局配置参数 优化插件配置命名，去除\u0026quot;plugins.\u0026quot; 优化配置文件目录获取命名  1.8.0 (2024-04-30) #  Breaking changes #  Features #   增加写入限流功能，可针对节点级（数据节点和协调节点）、分片级  Bug fix #   修复查询数据结果为空时，聚合出错问题 修复 Bundle 包在 MacOS 环境下 JDK 路径出错问题  Improvements #  1.7.1 (2024-03-01) #  Breaking changes #  Features #  Bug fix #   修复 _meta 不为空且 启用 source_reuse 时的映射解析错误 修复 source_reuse 下对多值还原不正确的问题 修复 source_reuse 和 alias 类型字段的冲突  Improvements #   改进跨集群复制的数据加载，增加对 source_reuse 索引的支持 内存断路器在触发 GC 时增加 full GC 回退 针对 String 类型的 TermsAgg 增加分片级别的内存断路检测  1.7.0 (2023-12-15) #  Breaking changes #  Features #   发布快照搜索功能 Beta 版本，此功能旨在提高对已备份数据的使用效率。让用户利用对象存储（如 AWS S3、MinIO、Microsoft Azure Storage、Google Cloud Storage 等）技术来大幅降低存储成本。  Bug fix #   修复单个节点场景下，从快照恢复多个 shard 的索引时，恢复不完整的问题 修复无法删除索引已关联的 ILM 策略问题  Improvements #   初始化脚本优化，新增重复执行判断  1.6.2 (2023-12-01) #  Breaking changes #  Features #  Bug fix #   修复跨集群复制（CCR）不能对自动滚动生成的索引进行同步的问题  Improvements #   优化初始化脚本，增加-s/-slient 自动安装参数。 新增含 jdk/plugins 的 bundle 安装包  1.6.1 (2023-10-19) #  Breaking changes #  Features #   增加 analysis-icu 插件  Bug fix #   修复 JDK 17 及更高版本运行告警及异常  Improvements #   安装脚本优化，避免脚本使用不当出现错误 source_reuse 增加对 float，double，geo_point，half_float，ip 类型字段的压缩 优化启用 source_reuse 时的写入速度，压缩的字段越多，写入速度越快  1.6.0 (2023-09-22) #  Breaking changes #  Features #   增加 _field_usage_stats api，统计索引每个字段的访问次数 新增 _disk_usage api，可以分析指定索引每个字段的磁盘占用大小 增加 flattened 类型，将 json 对象作为字符串处理，可以减少嵌套 json 型的文档的大小  Bug fix #  Improvements #   source_reuse 增加对 _source 中数字类型的值进行复用压缩，可进一步降低 _source 磁盘占用 改进 source_reuse 筛选字段的逻辑  1.5.0 (2023-09-08) #  Breaking changes #  Features #   增加 sql 插件，支持使用 REST 接口和 JDBC 进行 SQL 查询 支持 sql 常用函数、包括数学函数、三角函数、日期函数、字符串函数、聚合函数等 sql 语句可以嵌入全文检索 增加 jdbc 驱动，可以通过用户密码或证书连接到集群  Bug fix #   修复 knn 插件的配置项导致非 knn 索引的 setting 不能正常解析的 bug  Improvements #  1.4.0 (2023-07-21) #  Breaking changes #  Features #   索引生命周期管理增加 wait_for_snapshot 操作，在删除索引之前，等待执行指定的快照管理策略，这样可以确保已删除索引的快照可用 增加 analysis-hanlp 分词插件 增加 jieba 分词插件  Bug fix #   修复启用 index.source_reuse 时，对复杂多层 json 的 source 字段 解析不正确的 bug  Improvements #   更新索引生命周期管理 api 文档，增加策略应用和更新说明，增加 wait_for_snapshot 说明 执行 initialize.sh 命令时增加初始化确认提示，是否将 admin 密码记录日志。  1.3.0 (2023-06-30) #  Breaking changes #  Features #   增加 kNN 检索插件：  新增 knn_nearest_neighbors query api Mapping 新增 knn_dense_float_vector 和 knn_sparse_bool_vector 数据类型 支持近似 kNN 搜索和精确 kNN 搜索    Bug fix #  Improvements #   admin 用户默认由 initialize.sh 脚本生成随机密码,增强了安全性 增加适配 Windows 平台 增加 Docker 镜像  1.2.0 (2023-06-08) #  Breaking changes #  Features #   正式发布快照生命周期管理 (SLM) API, 支持定时备份和删除快照，以及保留快照的个数 增加 跨集群复制 (Cross-cluster replication) 功能：  支持手动或自动复制索引 支持暂停和恢复复制索引 支持取消指定索引的跨集群复制    Bug fix #   security 模块修复缺少某些角色验证属性的问题  Improvements #   兼容 ES6.0 版本的索引  1.1.1 (2023-05-25) #  Breaking changes #  Features #  Bug fix #   修复模板别名在某些场景不生效的 bug 防止 BigArray 在某些场景发生内存泄漏 修复 SourceValueFetcher 可能遗漏字段的 bug  Improvements #   easysearch.yml 增加 elasticsearch.api_compatibility 配置项, 兼容 logstash-oss, filebeat-oss, apm-server-oss 等 Elasticsearch 的客户端  1.1.0 (2023-05-12) #  Breaking changes #   Lucene 版本升级到 8.11.2  Features #   增加 ZSTD codec，引入 ZSTD 压缩算法，对存储字段，doc_values，词典进行压缩。 增加 index.source_reuse 索引级别配置，对 _source 进一步压缩。 提供索引生命周期管理 ILM 模块的功能，绝大部分 API 兼容 Elasticsearch  Bug fix #  Improvements #   减少冗余日志输出。 减少 modules 模块整体大小  1.0.0 (2023-04-06) #  Features #   兼容 Elasticsearch7.x 支持加密传输，权限控制等 security 相关功能 相比 Elasticsearch 更加轻量级  Bug fix #  Improvements #  ","subcategory":null,"summary":"","tags":null,"title":"Easysearch","url":"/easysearch/main/docs/release-notes/easysearch/"},{"category":null,"content":"重命名字段处理器（Rename Field Processor） #  版本引入：1.14.0\nrename_field 结果增强处理器用于拦截搜索响应，并对指定字段进行重命名。当你索引中的字段名称与应用程序使用的名称不一致时，该功能非常有用。例如，当索引中使用了新的字段名称，但应用程序仍期望接收旧字段名称时，可通过 rename_field 处理器在返回响应前完成字段名的映射，实现平滑过渡和向后兼容。\n请求体字段 #  下表列出了该处理器支持的所有配置字段。\n   字段 数据类型 说明     field 字符串 要重命名的原始字段名。必填。   target_field 字符串 新的字段名称。必填。   tag 字符串 处理器的唯一标识符。可选。   description 字符串 对该处理器的描述信息。可选。   ignore_failure 布尔值 若为 true，当此处理器执行失败时，Easysearch 将忽略错误并继续执行搜索管道中的其余处理器。可选，默认值为 false。    示例 #  以下示例演示如何在搜索管道中使用 rename_field 处理器。\n准备工作 #  创建一个名为 my_index 的索引，并索引一个包含 message 字段的文档：\nPOST /my_index/_doc/1 { \u0026#34;message\u0026#34;: \u0026#34;This is a public message\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34; } 创建搜索管道 #  以下请求创建一个名为 my_pipeline 的搜索管道，其中包含一个 rename_field 结果增强处理器，用于将字段 message 重命名为 notification：\nPUT /_search/pipeline/my_pipeline { \u0026#34;enrich_processors\u0026#34;: [ { \u0026#34;rename_field\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;notification\u0026#34; } } ] } 使用搜索管道 #  在不使用搜索管道的情况下，对 my_index 索引执行搜索：\nGET /my_index/_search 响应中包含原始字段 message：\n 响应  { \u0026#34;took\u0026#34;: 1, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;This is a public message\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34; } } ] } }  要使用搜索管道，请在请求中通过 search_pipeline 查询参数指定管道名称：\nGET /my_index/_search?search_pipeline=my_pipeline 此时，字段 message 已被重命名为 notification：\n 响应  { \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.0, \u0026#34;_source\u0026#34;: { \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34;, \u0026#34;notification\u0026#34;: \u0026#34;This is a public message\u0026#34; } } ] } }  你也可以使用 fields 参数来搜索文档中的特定字段：\nPOST /my_index/_search?pretty\u0026amp;search_pipeline=my_pipeline { \u0026#34;fields\u0026#34;: [\u0026#34;visibility\u0026#34;, \u0026#34;message\u0026#34;] } 在响应中，message 字段已被重命名为 notification，包括在 fields 返回结果中：\n 响应  { \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.0, \u0026#34;_source\u0026#34;: { \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34;, \u0026#34;notification\u0026#34;: \u0026#34;This is a public message\u0026#34; }, \u0026#34;fields\u0026#34;: { \u0026#34;visibility\u0026#34;: [ \u0026#34;public\u0026#34; ], \u0026#34;notification\u0026#34;: [ \u0026#34;This is a public message\u0026#34; ] } } ] } }  ","subcategory":null,"summary":"","tags":null,"title":"重命名字段处理器","url":"/easysearch/main/docs/features/query-dsl/search-pipelines/rename-field-processor/"},{"category":null,"content":"Edge N-gram 分词过滤器 #  edge_ngram 分词过滤器与 ngram 分词过滤器非常相似，二者都会将特定字符串拆分成不同长度的子字符串。不过，edge_ngram 分词过滤器仅从词元的开头（边缘）生成 n 元子字符串。在自动补全或前缀匹配等场景中，它特别有用，因为在这些场景里，你希望在用户输入单词或短语时就能匹配词项的开头部分。\n相关指南（先读这些） #    部分匹配  文本分析：识别词元  文本分析：规范化  参数说明 #  边缘 n 元分词过滤器可使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     min_gram 可选 整数 要生成的 n 元词项的最小长度。默认值为1。   max_gram 可选 整数 要生成的 n 元词项的最大长度。对于边缘 n 元分词过滤器，默认值为1；对于自定义分词过滤器，默认值为2。避免将此参数设置为较低的值。如果该值设置得过低，将只会生成非常短的 n 元词项，并且可能找不到搜索词。例如，将max_gram设置为 3，并且对单词“banana”建立索引，那么生成的最长词元将是“ban”。如果用户搜索“banana”，将不会返回任何匹配结果。你可以使用截断truncate分词过滤器作为搜索分析器来降低这种风险。   preserve_original 可选 布尔值 将原始词元包含在输出中。默认值为false。    参考样例 #  以下示例请求创建了一个名为 edge_ngram_example 的新索引，并配置了一个带有边缘 n 元过滤器的分词器：\nPUT /edge_ngram_example { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_edge_ngram\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;edge_ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 3, \u0026#34;max_gram\u0026#34;: 4 } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;my_edge_ngram\u0026#34;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /edge_ngram_example/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;slow green turtle\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;slo\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;slow\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;gre\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;gree\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;tur\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;turt\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"边缘 N-gram 分词过滤器（Edge N-gram）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/edge-n-gram/"},{"category":null,"content":"混合搜索 #  混合搜索结合了关键词搜索和语义搜索，以提升搜索相关性。要实现混合搜索，您需要建立一个在搜索时运行的搜索管道。该管道会在中间阶段拦截搜索结果，并通过处理流程对文档分数进行归一化和组合处理。\n要使用混合搜索，您需要配置搜索管道，添加混合排序处理器。\n相关指南（先读这些） #    向量搜索  搜索管道  混合排序处理器 #  混合排序处理器 hybrid_ranker_processor 是一种重排序处理器，运行在搜索执行的查询阶段和获取阶段之间。它会拦截查询阶段的结果，然后使用 倒数排序融合算法（RRF，Reciprocal Rank Fusion） 来合并不同查询子句，最终生成排序后的搜索结果列表。\n适用场景：\n 需要融合不同搜索技术（如关键词和语义搜索）的结果 子查询的原始分数不可直接比较（如 BM25 和 KNN）  算法原理 #  RRF 是一种多查询融合方法，其核心计算逻辑为：\n  对每个文档在不同子结果集给出的排名取倒数（如排名第 k 则得分为 1/(k+60)） 将各子结果集的倒数得分相加，生成统一排序分数 按最终分数降序输出结果集   RRF 的通用计算公式如下（其中 k 为平滑常数，默认 60，query_j_rank 表示混合查询中某文档在第 j 种查询方法返回结果中的排名）：\nrankScore(document_i) = sum(1/(k + query_1_rank), 1/(k + query_2_rank), ..., 1/(k + query_j_rank)) 请求体字段 #  下表列出了所有可用的请求字段。\n   字段 数据类型 说明     combination.technique String 必填。指定分数组合方式，目前仅支持 rrf（倒数排序融合）。   combination.rank_constant Integer 可选。计算倒数得分前，加到文档排名的常量值（必须 ≥ 1）。默认值：60。\n- 较大值（如 100）会使分数更均匀，降低高排名结果的影响。\n- 较小值（如 10）会增大排名间的分数差异，使高排名结果更具优势。    （注：RRF 算法适用于混合搜索场景，能够平衡关键词搜索和语义搜索的排序结果，提升整体相关性。）\n创建混合排序处理器 #  以下请求创建一个搜索管道，其中包含混合排序处理器：\nPUT /_search/pipeline/rrf-pipeline { \u0026#34;rerank_processors\u0026#34;: [ { \u0026#34;hybrid_ranker_processor\u0026#34;: { \u0026#34;combination\u0026#34;: { \u0026#34;technique\u0026#34;: \u0026#34;rrf\u0026#34;, \u0026#34;rank_constant\u0026#34;: 60 } } } ] } rrf-pipeline 仅用于演示，实际使用时建议配置混合搜索管道\n配置混合搜索管道 #  为充分发挥混合搜索的优势，建议配置语义查询增强处理器 semantic_query_enricher，通过结合关键词搜索的精确匹配能力和语义搜索的上下文理解能力，提升整体搜索效果。\n下面这个请求是在 Easysearch 中创建一个名为 search_model_aliyun 的搜索管道(search pipeline)。搜索管道允许你在搜索请求的不同阶段插入自定义逻辑。\nPUT /_search/pipeline/search_model_aliyun { \u0026#34;rewrite_processors\u0026#34;: [ { \u0026#34;semantic_query_enricher\u0026#34; : { \u0026#34;tag\u0026#34;: \u0026#34;tag1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;aliyun search embedding model\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;\u0026lt;api_key\u0026gt;\u0026#34;, \u0026#34;default_model_id\u0026#34;: \u0026#34;text-embedding-v4\u0026#34;, \u0026#34;vector_field_model_id\u0026#34;: { \u0026#34;text_vector\u0026#34;: \u0026#34;text-embedding-v4\u0026#34; } } } ], \u0026#34;rerank_processors\u0026#34;: [ { \u0026#34;hybrid_ranker_processor\u0026#34;: { \u0026#34;combination\u0026#34;: { \u0026#34;technique\u0026#34;: \u0026#34;rrf\u0026#34;, \u0026#34;rank_constant\u0026#34;: 60 } } } ], \u0026#34;enrich_processors\u0026#34;: [ { \u0026#34;hybrid_score_explanation\u0026#34;: {} } ] } 搜索管道结构： #  查询重写处理器 (rewrite_processors) #  { \u0026#34;semantic_query_enricher\u0026#34;: {} } 这部分配置了一个语义查询增强器，将查询文本转换为向量表示，用于后续的向量搜索。\n重排序处理器 (rerank_processors) #  { \u0026#34;hybrid_ranker_processor\u0026#34;: {} } 这部分配置了一个混合排序处理器。\n结果增强处理器 (enrich_processors)，可选 #  { \u0026#34;hybrid_score_explanation\u0026#34;: {} } 这部分配置了一个混合分数解释器，它会：\n 在搜索结果中添加解释信息 帮助理解不同部分(如文本匹配和语义相似度)对最终得分的贡献  使用混合查询 API 进行搜索 #  将 search_model_aliyun 设置为默认搜索管道\nPUT /my-index/_settings { \u0026#34;index.search.default_pipeline\u0026#34; : \u0026#34;search_model_aliyun\u0026#34; } 执行混合搜索 以下示例请求组合了两个查询子句： 语义查询和匹配查询。它使用上一步设置的默认搜索管道进行查询\nGET /my-index/_search { \u0026#34;_source\u0026#34;: { \u0026#34;exclude\u0026#34;: [ \u0026#34;text_vector\u0026#34; ] }, \u0026#34;query\u0026#34;: { \u0026#34;hybrid\u0026#34;: { \u0026#34;queries\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;input_text\u0026#34;: \u0026#34;夏季旅游首选\u0026#34; } }, { \u0026#34;semantic\u0026#34;: { \u0026#34;text_vector\u0026#34;: { \u0026#34;query_text\u0026#34;: \u0026#34;夏季旅游首选\u0026#34;, \u0026#34;candidates\u0026#34;: 10, \u0026#34;query_strategy\u0026#34;: \u0026#34;LSH_COSINE\u0026#34; } } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"混合搜索","url":"/easysearch/main/docs/integrations/ai/ai-api/hybrid-search/"},{"category":null,"content":"JSON 处理器 #  json 处理器将字符串值字段序列化为一个嵌套的映射，这对于各种数据处理和丰富任务可能很有用。\n以下是为 json 处理器提供的语法：\n{ \u0026#34;processor\u0026#34;: { \u0026#34;json\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;\u0026lt;field_name\u0026gt;\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;\u0026lt;target_field_name\u0026gt;\u0026#34;, \u0026#34;add_to_root\u0026#34;: \u0026lt;boolean\u0026gt; } } } 配置参数 #  下表列出了 json 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要反序列化的 JSON 格式字符串的字段名称。   target_field 可选 字段名称，用于存储反序列化的 JSON 数据。如果未提供，数据将存储在 field 字段中。如果 target_field 存在，其现有值将被新的 JSON 数据覆盖。   add_to_root 可选 一个布尔标志，用于确定是否将反序列化的 JSON 数据添加到文档的根（ true ）或存储在目标字段（ false ）中。如果 add_to_root 是 true ，则 target-field 无效。默认值是 false 。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 my-json-pipeline 的管道，该管道使用 json 处理器处理 JSON 数据，并使用附加信息丰富文档：\nPUT _ingest/pipeline/my-json-pipeline { \u0026#34;description\u0026#34;: \u0026#34;Example pipeline using the JsonProcessor\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;json\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;raw_data\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;parsed_data\u0026#34;, \u0026#34;on_failure\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;error_message\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Failed to parse JSON data\u0026#34; } }, { \u0026#34;fail\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Failed to process JSON data\u0026#34; } } ] } }, { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;processed_timestamp\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/my-json-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;raw_data\u0026#34;: \u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;John\\\u0026#34;,\\\u0026#34;age\\\u0026#34;:30,\\\u0026#34;city\\\u0026#34;:\\\u0026#34;New York\\\u0026#34;}\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;raw_data\u0026#34;: \u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;Jane\\\u0026#34;,\\\u0026#34;age\\\u0026#34;:25,\\\u0026#34;city\\\u0026#34;:\\\u0026#34;Los Angeles\\\u0026#34;}\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;processed_timestamp\u0026#34;: \u0026#34;2024-05-30T15:24:48.064472090Z\u0026#34;, \u0026#34;raw_data\u0026#34;: \u0026#34;\u0026#34;\u0026#34;{\u0026#34;name\u0026#34;:\u0026#34;John\u0026#34;,\u0026#34;age\u0026#34;:30,\u0026#34;city\u0026#34;:\u0026#34;New York\u0026#34;}\u0026#34;\u0026#34;\u0026#34;, \u0026#34;parsed_data\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;New York\u0026#34;, \u0026#34;age\u0026#34;: 30 } }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-05-30T15:24:48.06447209Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;processed_timestamp\u0026#34;: \u0026#34;2024-05-30T15:24:48.064543006Z\u0026#34;, \u0026#34;raw_data\u0026#34;: \u0026#34;\u0026#34;\u0026#34;{\u0026#34;name\u0026#34;:\u0026#34;Jane\u0026#34;,\u0026#34;age\u0026#34;:25,\u0026#34;city\u0026#34;:\u0026#34;Los Angeles\u0026#34;}\u0026#34;\u0026#34;\u0026#34;, \u0026#34;parsed_data\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Jane\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Los Angeles\u0026#34;, \u0026#34;age\u0026#34;: 25 } }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-05-30T15:24:48.064543006Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 my-index 的索引中：\nPOST my-index/_doc?pipeline=my-json-pipeline { \u0026#34;raw_data\u0026#34;: \u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;John\\\u0026#34;,\\\u0026#34;age\\\u0026#34;:30,\\\u0026#34;city\\\u0026#34;:\\\u0026#34;New York\\\u0026#34;}\u0026#34; } 响应确认，包含来自 raw_data 字段的 JSON 数据的文档已成功索引：\n{ \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;mo8yyo8BwFahnwl9WpxG\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 3, \u0026#34;_primary_term\u0026#34;: 2 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET my-index/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"JSON 处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/json/"},{"category":null,"content":"版本发布日志 #  这里是 INFINI Easysearch-client 历史版本发布的相关说明。\n2.0.2(2024-08-13) #  Improvements #   升级相关依赖项至安全版本  2.0.0(2024-04-17) #  Breaking changes #  Features #   发布全新的 Easysearch java 客户端 2.0 版本。 客户端经过完全重构，更加轻量级，避免冗余的第三方依赖。 为常用 Easysearch API 提供强类型的请求和响应。 API 均支持阻塞和异步方式。 使用流式构建器和函数式模式，以在创建复杂嵌套结构时编写简洁易读的代码。 通过使用 Jackson 无缝集成应用程序类。 自带 Java 低级别 REST 客户端，处理所有传输级别的问题：HTTP 连接池、重试、节点发现等。  Bug fix #  Improvements #  1.0.1(2023-11-14) #  Breaking changes #  Features #   正式发布 Easysearch Java 客户端。这一里程碑式的更新为开发人员带来了前所未有的便利性，使得与 Easysearch 集群的交互变得更加简洁和直观。现在，通过 Easysearch-client 客户端，开发者可以直接使用 Java 方法和数据结构来进行交互，而不再需要依赖于传统的 HTTP 方法和 JSON。这一变化大大简化了操作流程，使得数据管理和索引更加高效。高级客户端的功能范围包括处理数据操作，管理集群，包括查看和维护集群的健康状态，并对 Security 模块全面兼容。它提供了一系列 API，用于管理角色、用户、权限、角色映射和账户。这意味着安全性和访问控制现在可以更加细粒度地管理，确保了数据的安全性和合规性。  Bug fix #  Improvements #  ","subcategory":null,"summary":"","tags":null,"title":"Easysearch-client","url":"/easysearch/main/docs/release-notes/client/"},{"category":null,"content":"连接处理器 #  join 处理器将数组中的元素连接成一个单独的字符串值，每个元素之间使用指定的分隔符。如果提供的输入不是数组，则抛出异常。\n以下是为 join 处理器提供的语法：\n{ \u0026#34;join\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_name\u0026#34;, \u0026#34;separator\u0026#34;: \u0026#34;separator_string\u0026#34; } } 配置参数 #  下表列出了 join 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 应用连接操作符的字段名称。必须是数组。   separator 必填 字段值连接时使用的字符串分隔符。如果未指定，则值将无分隔符地连接。   target_field 可选 将清理后的值分配给的字段。如果未指定，则字段将就地更新。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 example-join-pipeline 的管道，该管道使用 join 处理器将 uri 字段的所有值连接起来，并用指定的分隔符 / 分隔：\nPUT _ingest/pipeline/example-join-pipeline { \u0026#34;description\u0026#34;: \u0026#34;Example pipeline using the join processor\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;join\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;uri\u0026#34;, \u0026#34;separator\u0026#34;: \u0026#34;/\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/example-join-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;uri\u0026#34;: [ \u0026#34;app\u0026#34;, \u0026#34;home\u0026#34;, \u0026#34;overview\u0026#34; ] } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;app/home/overview\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-05-24T02:16:01.00659117Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPOST testindex1/_doc/1?pipeline=example-join-pipeline { \u0026#34;uri\u0026#34;: [ \u0026#34;app\u0026#34;, \u0026#34;home\u0026#34;, \u0026#34;overview\u0026#34; ] } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 \n","subcategory":null,"summary":"","tags":null,"title":"连接处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/join/"},{"category":null,"content":"过滤查询处理器（filter query processor） #  版本引入：1.14.0\nfilter_query 查询重写处理器用于拦截搜索请求，并向该请求中添加一个额外的查询条件，从而对搜索结果进行过滤。当你不希望重写应用程序中已有的查询语句，但又需要对结果进行额外过滤时，此功能非常有用。\n请求体字段 #  下表列出了所有可用的请求字段。\n   字段 数据类型 说明     query 对象 使用 Easysearch 查询领域特定语言（DSL）编写的查询语句。必填。   tag 字符串 处理器的唯一标识符。可选。   description 字符串 对该处理器的描述信息。可选。   ignore_failure 布尔值 若为 true，则当此处理器执行失败时，Easysearch 将忽略该错误并继续执行搜索管道中的其余处理器。可选，默认值为 false。    示例 #  以下示例演示如何在搜索管道中使用 filter_query 处理器。\n准备工作 #  创建一个名为 my_index 的索引，并索引两个文档：一个公开，一个私有：\nPOST /my_index/_doc/1 { \u0026#34;message\u0026#34;: \u0026#34;This is a public message\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34; } POST /my_index/_doc/2 { \u0026#34;message\u0026#34;: \u0026#34;This is a private message\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;private\u0026#34; } 创建搜索管道 #  以下请求创建一个名为 my_pipeline 的搜索管道，其中包含一个 filter_query 查询重写处理器。该处理器使用 term 查询，仅返回可见性为“public”的文档：\nPUT /_search/pipeline/my_pipeline { \u0026#34;rewrite_processors\u0026#34;: [ { \u0026#34;filter_query\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;tag1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;此处理器将限制只返回可见性为公开的文档\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34; } } } } ] } 使用搜索管道 #  在不使用搜索管道的情况下，对 my_index 索引执行搜索：\nGET /my_index/_search 响应结果包含两个文档：\n 响应  { \u0026#34;took\u0026#34;: 47, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;This is a public message\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;This is a private message\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;private\u0026#34; } } ] } }  要使用搜索管道，请在请求中通过 search_pipeline 查询参数指定管道名称：\nGET /my_index/_search?search_pipeline=my_pipeline 此时响应仅包含 visibility 为 public 的文档：\n 响应  { \u0026#34;took\u0026#34;: 19, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.0, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;This is a public message\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34; } } ] } }   ","subcategory":null,"summary":"","tags":null,"title":"过滤查询处理器","url":"/easysearch/main/docs/features/query-dsl/search-pipelines/filter-query-processor/"},{"category":null,"content":"跨集群搜索权限 #  跨集群搜索允许集群中的节点对其他集群执行搜索请求。Easysearch 原生支持跨集群搜索，本页重点说明权限与配置要点。\n相关指南（先读这些） #    权限控制总览  分布式搜索基础  安全与多租户最佳实践  身份验证流程 #  当跨集群搜索通过 协调集群 访问 远程集群 时：\n 安全模块对协调集群上的用户进行身份验证。 安全模块在协调集群上获取用户的后端角色。 请求调用（包括经过身份验证的用户）将转发到远程集群。 在远程群集上评估用户的权限。  远程群集和协调集群可以分别配置不同的身份验证和授权配置，但我们建议在两者上使用相同的设置。\n权限信息 #  要查询远程集群上的索引，除了 READ 或 SEARCH 权限外，用户还需要具有以下索引权限：\nindices:admin/shards/search_shards role.yml 样例配置 #  humanresources: cluster: - CLUSTER_COMPOSITE_OPS_RO indices: \u0026#34;humanresources\u0026#34;: \u0026#34;*\u0026#34;: - READ - indices:admin/shards/search_shards # needed for CCS 配置流程 #  分别启动两个集群，如下：\n➜ curl -k \u0026#39;https://localhost:9200/_cluster/health?pretty\u0026#39; -u admin:xxxxxxxxxxxx { \u0026#34;cluster_name\u0026#34; : \u0026#34;easysearch\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;green\u0026#34;, \u0026#34;timed_out\u0026#34; : false, \u0026#34;number_of_nodes\u0026#34; : 1, \u0026#34;number_of_data_nodes\u0026#34; : 1, \u0026#34;active_primary_shards\u0026#34; : 1, \u0026#34;active_shards\u0026#34; : 1, \u0026#34;relocating_shards\u0026#34; : 0, \u0026#34;initializing_shards\u0026#34; : 0, \u0026#34;unassigned_shards\u0026#34; : 0, \u0026#34;delayed_unassigned_shards\u0026#34; : 0, \u0026#34;number_of_pending_tasks\u0026#34; : 0, \u0026#34;number_of_in_flight_fetch\u0026#34; : 0, \u0026#34;task_max_waiting_in_queue_millis\u0026#34; : 0, \u0026#34;active_shards_percent_as_number\u0026#34; : 100.0 } ➜ curl -k \u0026#39;https://localhost:9201/_cluster/health?pretty\u0026#39; -u admin:xxxxxxxxxxxx { \u0026#34;cluster_name\u0026#34; : \u0026#34;my-application22\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;green\u0026#34;, \u0026#34;timed_out\u0026#34; : false, \u0026#34;number_of_nodes\u0026#34; : 1, \u0026#34;number_of_data_nodes\u0026#34; : 1, \u0026#34;active_primary_shards\u0026#34; : 1, \u0026#34;active_shards\u0026#34; : 1, \u0026#34;relocating_shards\u0026#34; : 0, \u0026#34;initializing_shards\u0026#34; : 0, \u0026#34;unassigned_shards\u0026#34; : 0, \u0026#34;delayed_unassigned_shards\u0026#34; : 0, \u0026#34;number_of_pending_tasks\u0026#34; : 0, \u0026#34;number_of_in_flight_fetch\u0026#34; : 0, \u0026#34;task_max_waiting_in_queue_millis\u0026#34; : 0, \u0026#34;active_shards_percent_as_number\u0026#34; : 100.0 } 在协调群集上，添加远程群集名称和 IP 地址（端口为 9300）：\ncurl -k -XPUT -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9200/_cluster/settings\u0026#39; -d \u0026#39; { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.remote\u0026#34;: { \u0026#34;cluster1\u0026#34;: { \u0026#34;seeds\u0026#34;: [\u0026#34;127.0.0.1:9300\u0026#34;] }, \u0026#34;cluster2\u0026#34;: { \u0026#34;seeds\u0026#34;: [\u0026#34;127.0.0.1:9301\u0026#34;] } } } }\u0026#39; 在远程集群内索引一个文档：\ncurl -XPUT -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/books/_doc/1\u0026#39; -d \u0026#39;{\u0026#34;Dracula\u0026#34;: \u0026#34;Bram Stoker\u0026#34;}\u0026#39; 此时，跨集群搜索已经可以正常工作。您可以使用 admin 用户进行测试：\n✗ curl -XGET -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9200/cluster2:books/_search?pretty\u0026#39; { \u0026#34;took\u0026#34; : 57, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;_clusters\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;cluster2:books\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;Dracula\u0026#34; : \u0026#34;Bram Stoker\u0026#34; } } ] } } 继续测试，在两个集群上分别创建一个新用户：\ncurl -XPUT -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9200/_security/user/booksuser\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;password\u0026#34;:\u0026#34;password\u0026#34;}\u0026#39; curl -XPUT -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/_security/user/booksuser\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;password\u0026#34;:\u0026#34;password\u0026#34;}\u0026#39; Then run the same search as before with booksuser :\ncurl -XGET -k -u booksuser:password \u0026#39;https://localhost:9200/cluster2:books/_search?pretty\u0026#39; { \u0026#34;error\u0026#34; : { \u0026#34;root_cause\u0026#34; : [ { \u0026#34;type\u0026#34; : \u0026#34;security_exception\u0026#34;, \u0026#34;reason\u0026#34; : \u0026#34;no permissions for [indices:admin/shards/search_shards, indices:data/read/search] and User [name=booksuser, roles=[], requestedTenant=null]\u0026#34; } ], \u0026#34;type\u0026#34; : \u0026#34;security_exception\u0026#34;, \u0026#34;reason\u0026#34; : \u0026#34;no permissions for [indices:admin/shards/search_shards, indices:data/read/search] and User [name=booksuser, roles=[], requestedTenant=null]\u0026#34; }, \u0026#34;status\u0026#34; : 403 } 请注意权限错误。在远程群集上，创建具有适当权限的角色，并将 booksuser 映射到该角色：\ncurl -XPUT -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; \u0026#39;https://localhost:9200/_security/role/booksrole\u0026#39; -d \u0026#39;{\u0026#34;indices\u0026#34;:[{\u0026#34;names\u0026#34;:[\u0026#34;books\u0026#34;],\u0026#34;privileges\u0026#34;:[\u0026#34;indices:admin/shards/search_shards\u0026#34;,\u0026#34;indices:data/read/search\u0026#34;]}]}\u0026#39; curl -XPUT -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; \u0026#39;https://localhost:9200/_security/role_mapping/booksrole\u0026#39; -d \u0026#39;{\u0026#34;users\u0026#34; : [\u0026#34;booksuser\u0026#34;]}\u0026#39; curl -XPUT -k -u 'admin:xxxxxxxxxxxx' -H 'Content-Type: application/json' 'https://localhost:9201/_security/role/booksrole' -d '{\u0026quot;indices\u0026quot;:[{\u0026quot;names\u0026quot;:[\u0026quot;books\u0026quot;],\u0026quot;privileges\u0026quot;:[\u0026quot;indices:admin/shards/search_shards\u0026quot;,\u0026quot;indices:data/read/search\u0026quot;]}]}' curl -XPUT -k -u 'admin:xxxxxxxxxxxx' -H 'Content-Type: application/json' 'https://localhost:9201/_security/role_mapping/booksrole' -d '{\u0026quot;users\u0026quot; : [\u0026quot;booksuser\u0026quot;]}'\n两个集群都必须具有该用户，但只有远程集群需要角色和映射；在这种情况下，协调群集处理身份验证（即 “此请求是否包含有效的用户凭据？”），远程群集处理授权（即 “此用户是否可以访问此数据？”）。\n重新搜索一次：\ncurl -XGET -k -u booksuser:password \u0026#39;https://localhost:9200/cluster2:books/_search?pretty\u0026#39; { \u0026#34;took\u0026#34; : 5, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;_clusters\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;cluster2:books\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;Dracula\u0026#34; : \u0026#34;Bram Stoker\u0026#34; } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"跨集群搜索权限","url":"/easysearch/main/docs/operations/security/access-control/cross-cluster-search/"},{"category":null,"content":"ST Convert 字符过滤器 #  stconvert 字符过滤器在分词之前对中文文本进行简体与繁体之间的转换，使简繁体内容在搜索时可以互相匹配。\n 需要插件：此过滤器由 analysis-stconvert 插件提供，Easysearch 默认已集成。\n 转换方向 #     convert_type 说明     s2t 默认值。简体 → 繁体   t2s 繁体 → 简体    使用示例 #  简体转繁体（默认） #  PUT /my-chinese-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;s2t_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;stconvert\u0026#34;] } } } } } 繁体转简体 #  PUT /my-chinese-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;t2s_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stconvert\u0026#34;, \u0026#34;convert_type\u0026#34;: \u0026#34;t2s\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;t2s_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;t2s_filter\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;char_filter\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;stconvert\u0026#34;, \u0026#34;convert_type\u0026#34;: \u0026#34;t2s\u0026#34; } ], \u0026#34;text\u0026#34;: \u0026#34;國際化標準\u0026#34; } 结果：國際化標準 → 国际化标准\n参数说明 #     参数 类型 默认值 说明     convert_type String s2t 转换方向：s2t（简→繁）或 t2s（繁→简）    字符过滤器 vs 词元过滤器 #  analysis-stconvert 插件同时提供了字符过滤器、词元过滤器和分词器三种形式：\n   形式 类型名 处理阶段 说明     字符过滤器 stconvert 分词前 先转换再分词，分词结果更准确   词元过滤器 stconvert 分词后 分词后再转换   分词器 stconvert 分词阶段 边分词边转换     建议：中文场景推荐使用字符过滤器版本，先完成简繁转换再交给 IK 等分词器处理，可以避免因简繁差异导致的分词不一致。\n 典型应用场景 #  简繁体统一索引 #  将简繁体统一为简体索引，用户用任一种写法都能搜到：\nPUT /chinese-unified { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;t2s\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stconvert\u0026#34;, \u0026#34;convert_type\u0026#34;: \u0026#34;t2s\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;unified_chinese\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;t2s\u0026#34;], \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;unified_chinese\u0026#34; } } } } 这样索引 國際化 和 国际化 都会被归一化为 国际化，实现简繁体互搜。\n","subcategory":null,"summary":"","tags":null,"title":"简繁转换字符过滤器（ST Convert）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/character-filters/stconvert/"},{"category":null,"content":"文本向量化（已废弃） #   注意：本文档描述的功能已不再支持，将在下个版本删除，请使用新的 写入数据文本向量化替代。\n 本文档介绍如何在 Easysearch 中集成和使用预先部署的 Ollama 服务来生成文本嵌入向量。\n先决条件 #  需要预先部署好 Ollama 服务，现阶段集成的服务版本是 0.5.4。\n可以用以下命令测试服务是否正常：\ncurl http://localhost:11434/api/embed -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;nomic-embed-text:latest\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;Why is the sky blue?\u0026#34; }\u0026#39; 配置 Ollama 服务 #  可以通过 ollama_url 配置项指定 Ollama 服务的地址。您可以通过以下 API 查看当前配置：\nGET _cluster/settings?flat_settings=true\u0026amp;include_defaults=true\u0026amp;filter_path=*.ollama_url 如果没有修改，会输出默认值：\n{ \u0026#34;defaults\u0026#34;: { \u0026#34;ollama_url\u0026#34;: \u0026#34;http://localhost:11434\u0026#34; } } REST API #  POST /_ai/embed { \u0026#34;model\u0026#34;: \u0026#34;模型名称\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;文本内容\u0026#34; } 请求示例 #  POST /_ai/embed { \u0026#34;model\u0026#34;: \u0026#34;nomic-embed-text:latest\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;Llamas are members of the camelid family\u0026#34; } 批量生成 Embeddings #  可以一次为多个文本生成嵌入向量。\n请求格式：\nPOST /_ai/embed { \u0026#34;model\u0026#34;: \u0026#34;模型名称\u0026#34;, \u0026#34;input\u0026#34;: [\u0026#34;文本1\u0026#34;, \u0026#34;文本2\u0026#34;, ...] } 示例响应 #  { \u0026#34;embeddings\u0026#34;: [ [ 0.00971285, 0.04449268, -0.14063065, 0.0013162489, ...... ], [ 0.010429025, 0.014321253, -0.12902334, -0.03530379, 0.046050403, 0.04550423, ...... ] ] } 对于批量处理，建议一次发送多个文本而不是多次调用。\n如果收到连接错误，请检查 ollama_url 配置是否正确。\n","subcategory":null,"summary":"","tags":null,"title":"文本向量化（已废弃）","url":"/easysearch/main/docs/integrations/ai/ai-api/text-embeddings/"},{"category":null,"content":"写入与存储机制 #  当你向 Easysearch 索引一个文档时，数据要经历多个阶段才能最终安全地持久化到磁盘。理解这个过程，有助于你在性能调优和故障恢复时做出正确决策。\n写入的全景图 #  一个文档从写入请求到可被搜索再到持久化落盘，大致经历以下阶段：\n客户端请求 ↓ 协调节点路由到主分片 ↓ 主分片写入： 1. 追加到 translog（事务日志） 2. 写入内存索引缓冲区（in-memory buffer） ↓ refresh（默认每秒）: - 缓冲区内容写入新的段（segment）到文件系统缓存 - 新段可被搜索（近实时） - 缓冲区清空，translog 保留 ↓ flush（translog 超过 512MB 阈值时触发）: - 段从文件系统缓存 fsync 到磁盘 - 写入提交点（commit point） - translog 清空 ↓ 段合并（后台持续）: - 小段合并为大段 - 清理已删除文档 - 减少段数量，提升搜索性能 下面按阶段逐一展开。\n近实时搜索：为什么不是\u0026quot;实时\u0026quot; #  Easysearch 被称为 近实时（Near Real-Time, NRT） 搜索引擎——文档写入后并非立刻可搜索，但通常在一秒内就能被检索到。\n背后的原因在于：磁盘 I/O 是瓶颈。提交（commit）一个新的段到磁盘需要 fsync，开销非常大。如果每索引一个文档就执行一次 fsync，性能将无法接受。\nEasysearch 的做法是：将新段先写入文件系统缓存（OS page cache）——这一步代价很低；稍后再 fsync 到磁盘。只要新段进入了文件系统缓存，就可以像其他文件一样被打开和读取，从而对搜索可见。\n这就是\u0026quot;近实时\u0026quot;的由来：文档写入后最多延迟一个 refresh 周期（默认 1 秒）即可被搜索到。\nTranslog：写入安全网 #  每次写入操作首先追加到 translog（事务日志）。即使在 refresh 和 flush 之间发生崩溃，重启后 Easysearch 会重放 translog 恢复数据。\n 默认每次写请求完成后 fsync translog 到磁盘 在主分片和副本分片上都会写入 translog 客户端收到 200 OK 时，translog 已经持久化  translog 还提供实时的 CRUD：当你通过 ID 查询、更新、删除一个文档时，Easysearch 会先检查 translog 中是否有最近的变更，再去段里检索。这意味着它总是能获取到文档的最新版本。\n内存缓冲区：等待刷新 #  文档同时被写入内存索引缓冲区（in-memory buffer）。此时文档还不可搜索，需要等待下一次 refresh。\nRefresh：让文档可搜索 #  在 Easysearch 中，写入和打开一个新段的轻量过程叫做 refresh。默认情况下每个分片每秒自动刷新一次。\n执行流程：\n 将缓冲区内容写入新的段到文件系统缓存（不是磁盘） 新段被打开，文档变为可搜索 缓冲区清空，translog 保留  refresh API #  可以手动触发 refresh：\nPOST /_refresh // 刷新所有索引 POST /blogs/_refresh // 只刷新 blogs 索引  手动 refresh 在写测试时很有用，但不要在生产环境每次索引文档后都手动 refresh。你的应用需要理解并接受 Easysearch 的近实时特性。\n 调整刷新频率 #  并非所有场景都需要每秒刷新。批量导入或日志场景可以降低刷新频率以提升写入性能：\nPUT /my_logs { \u0026#34;settings\u0026#34;: { \u0026#34;refresh_interval\u0026#34;: \u0026#34;30s\u0026#34; } } refresh_interval 支持动态更新。在大批量导入时，可以先关闭自动刷新，导入完成后再恢复：\nPUT /my_logs/_settings { \u0026#34;refresh_interval\u0026#34;: -1 } // 关闭自动刷新 PUT /my_logs/_settings { \u0026quot;refresh_interval\u0026quot;: \u0026quot;1s\u0026quot; } // 恢复每秒刷新 \n⚠️ refresh_interval 的值需要带时间单位，例如 1s（1 秒）或 2m（2 分钟）。绝对值 1 表示 1 毫秒——会让集群陷入瘫痪。\n Flush：真正的持久化 #  Flush 将段从文件系统缓存 fsync 到物理磁盘，执行一次完整的提交（commit）：\n 所有在内存缓冲区的文档被写入一个新的段 缓冲区被清空 提交点（commit point）被写入磁盘，记录当前所有活跃的段 文件系统缓存通过 fsync 被刷新到磁盘 旧的 translog 被删除，创建新的 translog  Flush 默认在 translog 大小超过 512MB（index.translog.flush_threshold_size）时自动触发。Easysearch 没有基于时间的周期性 flush 定时器——flush 完全由 translog 大小驱动。\nflush API #  POST /blogs/_flush POST /_flush?wait_for_ongoing 通常不需要手动 flush。但在重启节点或关闭索引之前执行 flush 可以加速恢复——因为 translog 越短，恢复越快。\nTranslog 安全性 #  默认情况下 translog 在每次写请求完成之后执行 fsync（在主分片和副本分片都会发生）。对于大容量且偶尔丢失几秒数据也不严重的集群，可以使用异步 fsync：\nPUT /my_index/_settings { \u0026#34;index.translog.durability\u0026#34;: \u0026#34;async\u0026#34;, \u0026#34;index.translog.sync_interval\u0026#34;: \u0026#34;5s\u0026#34; }  ⚠️ 异步 translog 意味着在发生 crash 时，可能丢失 sync_interval 时间段的数据。如果不确定后果，请使用默认的 \u0026quot;index.translog.durability\u0026quot;: \u0026quot;request\u0026quot;。\n 段合并：后台优化 #  由于每次 refresh 都会创建一个新的段，段数量会快速增长。而段数目太多会带来问题：每个段都消耗文件句柄、内存和 CPU 资源，更重要的是每次搜索都要轮流检查每个段，段越多搜索越慢。\nEasysearch 通过在后台进行**段合并（merge）**来解决这个问题：\n 小段被合并到大段，大段再被合并到更大的段 已标记删除的文档在合并时被真正清理 合并过程中不会中断索引和搜索 Easysearch 默认对合并流程进行资源限制，保证搜索性能不受影响  强制合并（Force Merge） #  对于不再更新的只读索引（例如按天滚动的历史日志索引），可以使用强制合并将每个分片合并为一个单独的段：\nPOST /logstash-2024-10/_forcemerge?max_num_segments=1  ⚠️ 不要在活跃更新的索引上执行强制合并——后台合并流程已经能很好地完成工作。强制合并不受资源限制，可能消耗大量 I/O。\n 写入操作类型 #     操作 说明 内部行为     PUT /index/_doc/id 索引（创建或覆盖） 如果文档已存在，旧版本标记删除，新版本写入新段   POST /index/_doc 自动生成 ID 的索引 与上面相同，ID 由 Easysearch 生成   PUT /index/_doc/id/_create 仅创建（不覆盖） 如果 ID 已存在，返回 409 Conflict   POST /index/_update/id 部分更新 内部：读取 → 合并 → 重新索引整个文档   DELETE /index/_doc/id 删除 标记删除，段合并时真正清理   POST /index/_bulk 批量操作 按分片拆分并行执行，换行符分隔格式    文档的\u0026quot;不可变\u0026quot;本质 #  Easysearch 中有一个关键事实：\n 文档一旦写入段，就不会被修改。更新 = 标记旧版本删除 + 写入新版本。删除 = 标记删除 + 段合并时真正清理。\n 这种不可变性带来了并发安全、缓存友好和压缩优势，但也意味着频繁更新会产生大量需要合并的小段。\n性能调优要点 #     场景 建议     批量导入 关闭 refresh（-1），增大 bulk 批次（5-15MB），导入完成后恢复   日志场景 增大 refresh_interval（如 30s），减少段创建频率   高可靠性 保持 translog 默认同步模式（request）   容忍少量丢失 可用异步 translog（async，5s 间隔）   只读索引 执行 _forcemerge?max_num_segments=1 合并为单段    小结 #     阶段 作用 默认频率     Translog 写入安全保障，崩溃恢复 每次写请求   内存缓冲 暂存文档，等待 refresh —   Refresh 让文档可搜索（近实时） 每秒   Flush 将段持久化到磁盘 translog 超过 512MB 时   段合并 减少段数量，清理删除 后台持续     写入先走 translog 保证安全，再走内存缓冲等待 refresh Refresh 使文档可搜索（近实时），flush 使数据持久化到磁盘 更新和删除不是原地修改，而是\u0026quot;标记删除 + 写新版本\u0026quot; 段合并在后台自动清理删除标记、减少段数量  下一步可以继续阅读：\n  Lucene 底层原理：倒排索引、段结构、文件格式  分布式写入过程：文档如何在分片间路由和复制  文档操作：API 使用详情  最佳实践 #    数据生命周期与保留策略：冷热分层、自动清理、容量规划  索引与分片设计：分片规划、写入性能优化  ","subcategory":null,"summary":"","tags":null,"title":"写入与存储机制","url":"/easysearch/main/docs/fundamentals/write-and-storage/"},{"category":null,"content":"N-gram 分词过滤器 #  ngram 分词过滤器是一种强大的工具，用于将文本拆分为更小的组件，即 n-gram，这有助于提升部分匹配和模糊搜索能力。它通过将一个词元拆分成指定长度的子字符串来工作。这些过滤器在搜索应用程序中很常用，可用于支持自动补全、部分匹配以及容错拼写搜索。\n相关指南（先读这些） #    部分匹配  文本分析：模糊匹配  文本分析：识别词元  参数说明 #  n-gram 分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     min_gram 可选 整数 n-gram 的最小长度。默认值为 1。   max_gram 可选 整数 n-gram 的最大长度。默认值为 2。   preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 false。    参考样例 #  以下示例请求创建了一个名为“ngram_example_index”的新索引，并配置了一个带有 n-gram 过滤器的分词器：\nPUT /ngram_example_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;ngram_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 2, \u0026#34;max_gram\u0026#34;: 3 } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;ngram_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;ngram_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /ngram_example_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;ngram_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Search\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;se\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;sea\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;ea\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;ear\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;ar\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;arc\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;rc\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;rch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;ch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"N-gram 分词过滤器（N-gram）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/n-gram/"},{"category":null,"content":"Geotile 网格聚合 #  geotile_grid 聚合将 geo_point 值按照地图瓦片坐标分组。与 geohash_grid 类似，但使用的是 Web 地图标准的瓦片坐标系统（x/y/zoom），这使得它与 Google Maps、OpenStreetMap、Mapbox 等瓦片地图服务的集成更加方便。\n相关指南（先读这些） #    聚合基础教程  地理位置搜索  Geohash Grid 聚合   瓦片坐标系统 #  Web 地图使用 Slippy Map 瓦片坐标系统：\n zoom：缩放级别（0-29），级别越高网格越精细 x：水平瓦片索引（从左到右） y：垂直瓦片索引（从上到下）  瓦片标识符格式：{zoom}/{x}/{y}，如 14/8532/5765\n缩放级别与覆盖范围 #     Zoom 瓦片数量 每个瓦片覆盖范围     0 1 整个世界   1 4 ~ 20,000km   5 1,024 ~ 1,250km   10 ~100万 ~ 39km   15 ~10亿 ~ 1.2km   20 ~1万亿 ~ 38m     参数说明 #     参数 必需/可选 类型 说明     field 必需 String 包含 geo_point 值的字段名   precision 可选 Integer 缩放级别（0-29）。默认 7   bounds 可选 Object 限制聚合的地理边界框   size 可选 Integer 返回的最大 bucket 数量。默认 10000   shard_size 可选 Integer 每个分片返回的 bucket 数量     基础用法 #  简单瓦片网格聚合 #  GET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;tiles\u0026#34;: { \u0026#34;geotile_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 8 } } } } 响应示例 #  { \u0026#34;aggregations\u0026#34;: { \u0026#34;tiles\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;8/131/84\u0026#34;, \u0026#34;doc_count\u0026#34;: 256 }, { \u0026#34;key\u0026#34;: \u0026#34;8/131/85\u0026#34;, \u0026#34;doc_count\u0026#34;: 189 }, { \u0026#34;key\u0026#34;: \u0026#34;8/132/84\u0026#34;, \u0026#34;doc_count\u0026#34;: 145 } ] } } } 每个 bucket 的 key 是瓦片坐标 {zoom}/{x}/{y}，可以直接用于请求对应的地图瓦片。\n 配合边界框过滤 #  限制在特定区域内聚合：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;geo_bounding_box\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;top_left\u0026#34;: { \u0026#34;lat\u0026#34;: 40.8, \u0026#34;lon\u0026#34;: -74.1 }, \u0026#34;bottom_right\u0026#34;: { \u0026#34;lat\u0026#34;: 40.4, \u0026#34;lon\u0026#34;: -73.7 } } } } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;nyc_tiles\u0026#34;: { \u0026#34;geotile_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 12 } } } }  使用 bounds 参数 #  直接在聚合中指定边界范围：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;tiles\u0026#34;: { \u0026#34;geotile_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 10, \u0026#34;bounds\u0026#34;: { \u0026#34;top_left\u0026#34;: { \u0026#34;lat\u0026#34;: 40.8, \u0026#34;lon\u0026#34;: -74.1 }, \u0026#34;bottom_right\u0026#34;: { \u0026#34;lat\u0026#34;: 40.4, \u0026#34;lon\u0026#34;: -73.7 } } } } } }  嵌套子聚合 #  每个瓦片的中心点 #  GET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;tiles\u0026#34;: { \u0026#34;geotile_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 10 }, \u0026#34;aggs\u0026#34;: { \u0026#34;centroid\u0026#34;: { \u0026#34;geo_centroid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34; } } } } } } 每个瓦片的统计信息 #  GET /real_estate/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;tiles\u0026#34;: { \u0026#34;geotile_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 12 }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } }, \u0026#34;price_stats\u0026#34;: { \u0026#34;stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } } } } } }  在 Composite 聚合中使用 #  geotile_grid 可以作为 composite 聚合的源，实现分页遍历所有瓦片：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;tiles\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;size\u0026#34;: 100, \u0026#34;sources\u0026#34;: [ { \u0026#34;tile\u0026#34;: { \u0026#34;geotile_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 8 } } } ] } } } } 翻页时使用 after 参数：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;tiles\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;size\u0026#34;: 100, \u0026#34;sources\u0026#34;: [ { \u0026#34;tile\u0026#34;: { \u0026#34;geotile_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 8 } } } ], \u0026#34;after\u0026#34;: { \u0026#34;tile\u0026#34;: \u0026#34;8/131/85\u0026#34; } } } } }  与地图库集成 #  转换瓦片坐标为边界框 #  前端可以使用标准公式将瓦片坐标转换为经纬度边界：\nfunction tile2boundingBox(z, x, y) { const n = Math.PI - 2 * Math.PI * y / Math.pow(2, z); return { north: 180 / Math.PI * Math.atan(0.5 * (Math.exp(n) - Math.exp(-n))), south: 180 / Math.PI * Math.atan(0.5 * ( Math.exp(Math.PI - 2 * Math.PI * (y + 1) / Math.pow(2, z)) - Math.exp(-(Math.PI - 2 * Math.PI * (y + 1) / Math.pow(2, z))) )), west: x / Math.pow(2, z) * 360 - 180, east: (x + 1) / Math.pow(2, z) * 360 - 180 }; } // 解析瓦片 key function parseTileKey(key) { const [z, x, y] = key.split('/').map(Number); return { z, x, y }; } 在 Leaflet 中显示聚合结果 #\n // 假设 response 是 Easysearch 返回的聚合结果 const buckets = response.aggregations.tiles.buckets; buckets.forEach(bucket =\u0026gt; { const { z, x, y } = parseTileKey(bucket.key); const bounds = tile2boundingBox(z, x, y);\nL.rectangle([ [bounds.south, bounds.west], [bounds.north, bounds.east] ], { color: getColorByCount(bucket.doc_count), weight: 1, fillOpacity: 0.5 }).bindPopup(数量: \u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;${\u0026lt;/span\u0026gt;bucket.doc_count\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;).addTo(map); }); \n性能优化 #  1. 根据地图缩放级别动态调整精度 #  function getPrecisionForZoom(mapZoom) { // 瓦片精度通常与地图缩放级别相同或略低  return Math.min(mapZoom, 15); } 2. 限制返回数量 #  对于高精度查询，限制返回的瓦片数量：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;tiles\u0026#34;: { \u0026#34;geotile_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 15, \u0026#34;size\u0026#34;: 1000 } } } } 3. 配合边界框使用 #  始终使用 bounds 或 geo_bounding_box 过滤器，避免扫描全量数据。\n geotile_grid vs geohash_grid #     特性 geotile_grid geohash_grid     坐标系统 Web 瓦片 (x/y/zoom) Geohash 编码   精度范围 0-29 1-12   Key 格式 8/131/84 dr5rs   地图集成 ✅ 直接对应瓦片 ⚠️ 需要转换   网格形状 正方形（墨卡托投影） 矩形（可变）    选择建议：\n 与 Web 地图服务集成 → geotile_grid 通用地理聚合 → geohash_grid 需要与第三方 Geohash 服务交互 → geohash_grid   相关文档 #    Geohash Grid 聚合  Geo Bounds 聚合  Geo Centroid 聚合  Composite 聚合  Geohash 编码原理  ","subcategory":null,"summary":"","tags":null,"title":"Geotile 网格聚合","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/geotile-grid/"},{"category":null,"content":"身份模拟 #  用户模拟允许具备特定权限的用户以另外的身份来进行集群的访问。\n用户模拟可用于测试和故障排除，或允许系统服务安全地充当用户。\n在 REST 接口或 TCP 传输层上都可以进行用户模拟。\nREST 接口 #  要允许一个用户模拟另一个用户，请将以下内容添加到 easysearch.yml :\nsecurity.authcz.rest_impersonation_user: \u0026lt;AUTHENTICATED_USER\u0026gt;: - \u0026lt;IMPERSONATED_USER_1\u0026gt; - \u0026lt;IMPERSONATED_USER_2\u0026gt; 模拟用户字段支持通配符。将其设置为 * 允许 AUTHENTICATED_USER 来模拟任意用户。\n传输层配置 #  类似的配置方法如下：\nsecurity.authcz.impersonation_dn: \u0026#34;CN=spock,OU=client,O=client,L=Test,C=DE\u0026#34;: - worf 模拟其他用户 #  要模拟其他用户，请向系统提交请求，并将 HTTP 标头 security_run_as 设置为要模拟的用户的名称。例如：\ncurl -XGET -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; -k -H \u0026#34;security_run_as: user_1\u0026#34; https://localhost:9200/_security/authinfo?pretty ","subcategory":null,"summary":"","tags":null,"title":"身份模拟","url":"/easysearch/main/docs/operations/security/access-control/run-as/"},{"category":null,"content":"搜索请求文本向量化 #  Easysearch 使用搜索管道的 semantic_query_enricher 处理器，协助 semantic query，将文本转为向量。\n相关指南（先读这些） #    向量搜索  搜索管道  AI 集成  先决条件 #    服务兼容性 需满足以下任一条件：\n 支持与 OpenAI API 兼容的 embedding 接口 支持 Ollama embedding 接口    插件要求 必须安装 Easysearch 的以下插件：\nknn ai   数据准备 需预先完成：\n 创建向量索引 写入向量数据 参考 写入数据文本向量化    创建或更新 semantic_query_enricher 处理器 #  PUT /_search/pipeline/default_model_pipeline { \u0026#34;rewrite_processors\u0026#34;: [ { \u0026#34;semantic_query_enricher\u0026#34; : { \u0026#34;tag\u0026#34;: \u0026#34;tag1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Sets the default embedding model\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.openai.com/v1/embeddings\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;\u0026lt;api_key\u0026gt;\u0026#34;, \u0026#34;default_model_id\u0026#34;: \u0026#34;text-embedding-3-small\u0026#34;, \u0026#34;vector_field_model_id\u0026#34;: { \u0026#34;text_vector\u0026#34;: \u0026#34;text-embedding-3-small\u0026#34; } } } ] } 请求体字段： #  下表列出了用于创建或更新管道的请求体字段。\n   参数 是否必填 类型 说明     rewrite_processors 必填 数组 处理器列表，按顺序执行。示例中为 semantic_query_enricher。   tag 可选 字符串 处理器标签，用于调试或日志（如 \u0026quot;tag1\u0026quot;）。   description 可选 字符串 管道描述，用于说明用途（如 “Sets the default model ID at index and field levels”）。   url 必填 字符串 Embedding API 的完整 URL（如 https://poloai.top/v1/embeddings）。   vendor 必填 字符串 服务提供商标识，如 \u0026quot;openai\u0026quot;。   api_key 必填 字符串 API 密钥，需替换为实际值（如 \u0026quot;sk-xxx\u0026quot;）。   default_model_id 可选 字符串 全局默认模型 ID，当字段未在 vector_field_model_id 中配置时使用（如 text-embedding-3-small）。   vector_field_model_id 可选 对象 字段级模型映射，如 { \u0026quot;text_vector\u0026quot;: \u0026quot;text-embedding-3-small\u0026quot; }，优先级高于 default_model_id。    查看搜索管道 #  要查看刚创建的所有搜索管道，请使用以下请求：\nGET _search/pipeline/default_model_pipeline 响应包含您在上一节中设置的管道：\n{ \u0026#34;default_model_pipeline\u0026#34;: { \u0026#34;rewrite_processors\u0026#34;: [ { \u0026#34;semantic_query_enricher\u0026#34;: { \u0026#34;api_key\u0026#34;: \u0026#34;*************************************************vE\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Sets the default embedding model\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;tag1\u0026#34;, \u0026#34;vector_field_model_id\u0026#34;: { \u0026#34;text_vector\u0026#34;: \u0026#34;text-embedding-3-small\u0026#34; }, \u0026#34;default_model_id\u0026#34;: \u0026#34;text-embedding-3-small\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.openai.com/v1/embeddings\u0026#34; } } ] } } api_key 会被遮掩，防止泄露\n设置默认搜索管道 #  可以将 default_model_pipeline 设置为索引默认的搜索管道，避免每次请求提供搜索管道参数。\nPUT /my-index/_settings { \u0026#34;index.search.default_pipeline\u0026#34; : \u0026#34;default_model_pipeline\u0026#34; } 使用 semantic query 进行搜索 #  GET /my-index/_search { \u0026#34;_source\u0026#34;: \u0026#34;input_text\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;semantic\u0026#34;: { \u0026#34;text_vector\u0026#34;: { \u0026#34;query_text\u0026#34;: \u0026#34;这是另一个示例文本，这次使用 POST。\u0026#34;, \u0026#34;candidates\u0026#34;: 100, \u0026#34;query_strategy\u0026#34;: \u0026#34;LSH_COSINE\u0026#34; } } } } Semantic Query 参数说明 #     参数 类型 必填 默认值 说明     semantic string 是 - 查询类型标识，表示这是一个将文本自动转成向量的查询。   text_vector string 是 - 目标向量字段名（需在 mapping 中定义为 dense_vector 类型）   query_text string 是 - 需要编码为向量的文本内容   candidates integer 否 100 召回阶段每个 segment 考虑的候选文档数量，值越大召回率越高，性能开销越大   query_strategy enum 否 LSH_COSINE 近似最近邻算法策略，可选值：       - LSH_COSINE：局部敏感哈希 + 余弦相似度（model: lsh, similarity: cosine）       - LSH_L2：局部敏感哈希 + L2距离（model: lsh, similarity: l2）       - PERMUTATION_LSH：排列局部敏感哈希（model: permutation_lsh）    查询结果 #  { \u0026#34;took\u0026#34;: 3297, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;dewmA5gB0ulPrMzo2K-_\u0026#34;, \u0026#34;_score\u0026#34;: 2, \u0026#34;_source\u0026#34;: { \u0026#34;input_text\u0026#34;: \u0026#34;这是另一个示例文本，这次使用 POST。\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;dt2DAZgB6WXmvHRNYksX\u0026#34;, \u0026#34;_score\u0026#34;: 1.540483, \u0026#34;_source\u0026#34;: { \u0026#34;input_text\u0026#34;: \u0026#34;这是另一示例文本。\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;bulk_doc_2\u0026#34;, \u0026#34;_score\u0026#34;: 1.513369, \u0026#34;_source\u0026#34;: { \u0026#34;input_text\u0026#34;: \u0026#34;第二个批量处理的文本，指定了ID。\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;dd2DAZgB6WXmvHRNYksX\u0026#34;, \u0026#34;_score\u0026#34;: 1.4987004, \u0026#34;_source\u0026#34;: { \u0026#34;input_text\u0026#34;: \u0026#34;第一个批量处理的文本。\u0026#34; } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"搜索请求文本向量化","url":"/easysearch/main/docs/integrations/ai/ai-api/search-text-embedding/"},{"category":null,"content":"ICU Normalizer 字符过滤器 #  icu_normalizer 字符过滤器使用 ICU 库在分词之前对原始文本进行 Unicode 归一化。它可以将字符的多种编码形式统一为标准形式，确保相同的\u0026quot;视觉字符\u0026quot;在搜索时能够匹配。\n 需要插件：此过滤器由 analysis-icu 插件提供，Easysearch 默认已集成。\n 与词元过滤器版本的区别 #  ICU 插件同时提供了字符过滤器和词元过滤器两个版本：\n   版本 类型名 处理阶段 适用场景     字符过滤器 icu_normalizer 分词前 需要在分词前统一字符形式   词元过滤器 icu_normalizer 分词后 只需对词元进行归一化     建议：大多数场景使用字符过滤器版本效果更好，因为归一化发生在分词之前，可以避免因字符形式不同导致的分词差异。\n 参数说明 #     参数 类型 默认值 说明     name String nfkc_cf Unicode 归一化方式，见下方选项   mode String compose 归一化模式：compose（合成）或 decompose（分解）   unicode_set_filter String — 可选，ICU UnicodeSet 过滤条件，仅对匹配字符进行归一化    name 可选值 #     值 说明     nfc NFC 标准归一化（合成优先）   nfkc NFKC 兼容归一化（合成优先）   nfkc_cf 默认值。NFKC 归一化 + Case Folding（大小写折叠）    使用示例 #  基本用法（默认 nfkc_cf） #  PUT /my-icu-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_icu_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;icu_normalizer\u0026#34;] } } } } } 自定义归一化方式 #  PUT /my-icu-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;my_icu_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;icu_normalizer\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;nfc\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;compose\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;my_icu_normalizer\u0026#34;] } } } } } 测试效果 #  GET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;icu_normalizer\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;Ⅲ ﬁ ℃\u0026#34; } 归一化结果：ⅲ→iii、ﬁ→fi、℃→°c（NFKC 兼容分解 + case folding）。\n典型归一化效果 #     输入 nfkc_cf 输出 说明     Ⅲ iii 罗马数字分解   ﬁ fi 连字分解   Ａ a 全角→半角 + 小写   é（2 码点） é（1 码点） 组合字符合成   ㎞ km 兼容字符分解    适用场景 #   多语言索引：统一不同 Unicode 编码的相同字符 全角/半角混合：日语、中文文本中的全角字母和数字 连字与特殊符号：学术文本中的 ﬁ、ﬂ、℃ 等 组合字符：变音符号的合成/分解统一  ","subcategory":null,"summary":"","tags":null,"title":"ICU 归一化字符过滤器（ICU Normalizer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/character-filters/icu-normalizer/"},{"category":null,"content":"HTML 清理处理器 #  html_strip 处理器从传入文档的字符串字段中移除 HTML 标签。当从网页或其他可能包含 HTML 标记的来源索引数据时，此处理器非常有用。HTML 标签被替换为换行符（\\n）。\n以下是为 html_strip 处理器提供的语法：\n{ \u0026#34;html_strip\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;webpage\u0026#34; } } 配置参数 #  下表列出了 html_strip 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 要从中移除 HTML 标签的字符串字段。   target_field 可选 接收去除 HTML 标签后的纯文本版本的字段。如果未指定，则字段将就地更新。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。默认为 false 。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 strip-html-pipeline 的管道，使用 html_strip 处理器从描述字段中删除 HTML 标签，并将处理后的值存储在名为 cleaned_description 的新字段中：\nPUT _ingest/pipeline/strip-html-pipeline { \u0026#34;description\u0026#34;: \u0026#34;A pipeline to strip HTML from description field\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;html_strip\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;description\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;cleaned_description\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/strip-html-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;This is a \u0026lt;b\u0026gt;test\u0026lt;/b\u0026gt; description with \u0026lt;i\u0026gt;some\u0026lt;/i\u0026gt; HTML tags.\u0026#34; } } ] } 以下示例响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;This is a \u0026lt;b\u0026gt;test\u0026lt;/b\u0026gt; description with \u0026lt;i\u0026gt;some\u0026lt;/i\u0026gt; HTML tags.\u0026#34;, \u0026#34;cleaned_description\u0026#34;: \u0026#34;This is a test description with some HTML tags.\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-05-22T21:46:11.227974965Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 products 的索引中：\nPUT products/_doc/1?pipeline=strip-html-pipeline { \u0026#34;name\u0026#34;: \u0026#34;Product 1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is a \u0026lt;b\u0026gt;test\u0026lt;/b\u0026gt; product with \u0026lt;i\u0026gt;some\u0026lt;/i\u0026gt; HTML tags.\u0026#34; } 响应显示，请求已将文档索引到索引 products ，并将索引所有包含 HTML 标签的文档，同时将纯文本版本存储在 cleaned_description 字段中：\n{ \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET products/_doc/1 响应包括原始的 description 字段和移除 HTML 标签后的 cleaned_description 字段：\n{ \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;cleaned_description\u0026#34;: \u0026#34;This is a test product with some HTML tags.\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Product 1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is a \u0026lt;b\u0026gt;test\u0026lt;/b\u0026gt; product with \u0026lt;i\u0026gt;some\u0026lt;/i\u0026gt; HTML tags.\u0026#34; } } ","subcategory":null,"summary":"","tags":null,"title":"HTML 清理处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/html-strip/"},{"category":null,"content":"Geohash 网格聚合 #  geohash_grid 聚合将 geo_point 值按照 Geohash 编码分组到网格单元中。这对于在地图上可视化大量地理坐标点非常有用——将密集的点聚合成可管理的网格，每个网格显示该区域的统计信息。\n相关指南（先读这些） #    聚合基础教程  地理位置搜索  Geohash 编码原理   参数说明 #     参数 必需/可选 类型 说明     field 必需 String 包含 geo_point 值的字段名   precision 可选 Integer/String Geohash 精度级别（1-12）或距离字符串（如 1km）。默认 5   bounds 可选 Object 限制聚合的地理边界框，只处理该范围内的点   size 可选 Integer 返回的最大 bucket 数量。默认 10000   shard_size 可选 Integer 每个分片返回的 bucket 数量，用于提高精度。默认 max(10, size)    精度级别参考 #     精度 网格尺寸 适用场景     1 ~ 5,000km 大洲级别   2 ~ 1,250km 国家级别   3 ~ 156km 大区域   4 ~ 39km 城市群   5 ~ 4.9km 城市/区县   6 ~ 1.2km 街区   7 ~ 153m 街道   8 ~ 38m 建筑群     基础用法 #  简单网格聚合 #  GET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 5 } } } } 响应示例 #  { \u0026#34;aggregations\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;dr5rs\u0026#34;, \u0026#34;doc_count\u0026#34;: 125 }, { \u0026#34;key\u0026#34;: \u0026#34;dr5re\u0026#34;, \u0026#34;doc_count\u0026#34;: 89 }, { \u0026#34;key\u0026#34;: \u0026#34;dr5ru\u0026#34;, \u0026#34;doc_count\u0026#34;: 67 } ] } } } 每个 bucket 的 key 是 Geohash 值，可以用前端库解码为边界框坐标，在地图上绘制网格。\n 配合边界框过滤 #  为避免在大数据集上生成过多 bucket，推荐配合 geo_bounding_box 查询限制范围：\nGET /attractions/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;geo_bounding_box\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;top_left\u0026#34;: { \u0026#34;lat\u0026#34;: 40.8, \u0026#34;lon\u0026#34;: -74.1 }, \u0026#34;bottom_right\u0026#34;: { \u0026#34;lat\u0026#34;: 40.4, \u0026#34;lon\u0026#34;: -73.7 } } } } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;new_york_grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 6 } } } }  使用 bounds 参数 #  可以直接在聚合中使用 bounds 参数限制聚合的地理范围，效果更好：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 6, \u0026#34;bounds\u0026#34;: { \u0026#34;top_left\u0026#34;: { \u0026#34;lat\u0026#34;: 40.8, \u0026#34;lon\u0026#34;: -74.1 }, \u0026#34;bottom_right\u0026#34;: { \u0026#34;lat\u0026#34;: 40.4, \u0026#34;lon\u0026#34;: -73.7 } } } } } }  嵌套子聚合 #  可以在每个网格单元内计算更多统计信息：\n每个网格的平均评分 #  GET /restaurants/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 5 }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_rating\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;rating\u0026#34; } }, \u0026#34;avg_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } } } } } } 每个网格的中心点 #  GET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 5 }, \u0026#34;aggs\u0026#34;: { \u0026#34;centroid\u0026#34;: { \u0026#34;geo_centroid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34; } } } } } } 每个网格的边界框 #  GET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 5 }, \u0026#34;aggs\u0026#34;: { \u0026#34;cell_bounds\u0026#34;: { \u0026#34;geo_bounds\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34; } } } } } }  按距离指定精度 #  除了数字精度级别，还可以使用距离字符串：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: \u0026#34;1km\u0026#34; } } } } Easysearch 会自动选择最接近指定距离的精度级别。\n 高基数场景处理 #  控制返回数量 #  默认最多返回 10000 个 bucket。如果网格数量超过限制，只返回文档数最多的 bucket：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 7, \u0026#34;size\u0026#34;: 500 } } } } 提高精度 #  使用 shard_size 提高聚合精度（会增加内存消耗）：\nGET /locations/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;grid\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 6, \u0026#34;size\u0026#34;: 100, \u0026#34;shard_size\u0026#34;: 500 } } } }  地图可视化示例 #  热力图数据准备 #  GET /check_ins/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-7d/d\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;now\u0026#34; } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;heatmap\u0026#34;: { \u0026#34;geohash_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34;, \u0026#34;precision\u0026#34;: 6 }, \u0026#34;aggs\u0026#34;: { \u0026#34;centroid\u0026#34;: { \u0026#34;geo_centroid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;location\u0026#34; } } } } } } 前端可以将结果转换为热力图数据：\nconst heatmapData = response.aggregations.heatmap.buckets.map(bucket =\u0026gt; ({ lat: bucket.centroid.location.lat, lng: bucket.centroid.location.lon, weight: bucket.doc_count }));  与 geotile_grid 的对比 #     特性 geohash_grid geotile_grid     网格类型 Geohash 编码 地图瓦片坐标   精度参数 1-12 0-29 (zoom)   与 Web 地图集成 需要转换 直接对应瓦片   适用场景 通用聚合 瓦片地图渲染    如果你的地图使用标准瓦片服务（如 Google Maps、OpenStreetMap），geotile_grid 可能更方便。\n 相关文档 #    Geohash 编码原理  Geotile Grid 聚合  Geo Bounds 聚合  Geo Centroid 聚合  Geo Distance 聚合  ","subcategory":null,"summary":"","tags":null,"title":"Geohash 网格聚合","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/geohash-grid/"},{"category":null,"content":"Limit 分词过滤器 #  limit 分词过滤器用于限制分词链通过的词元数量。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参数说明 #  限制分词过滤器可以使用以下参数进行配置。\n   参数 必填/可选 数据类型 描述     max_token_count 可选 整数 要生成的词元的最大数量。默认值为 1。   consume_all_tokens 可选 布尔值 （专家级设置）即使结果超过 max_token_count，也会使用来自词元生成器的所有词元。当设置此参数时，输出仍然只包含 max_token_count 指定数量的词元。不过，词元生成器生成的所有词元都会被处理。默认值为 false。    参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有限制过滤器的分析器：\nPUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;three_token_limit\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;custom_token_limit\u0026#34; ] } }, \u0026#34;filter\u0026#34;: { \u0026#34;custom_token_limit\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;limit\u0026#34;, \u0026#34;max_token_count\u0026#34;: 3 } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;three_token_limit\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch is a powerful and flexible search engine.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;a\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"限制分词过滤器（Limit）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/limit/"},{"category":null,"content":"采样聚合 #  如果你正在聚合大量文档，可以使用 sampler 聚合将范围缩小到一小部分文档，从而获得更快的响应。sampler 聚合通过选择得分最高的文档来选取样本。\n结果是大致的，但能很好地反映真实数据的分布。sampler 聚合显著提高了查询性能，但估计的响应并不完全可靠。\n相关指南（先读这些） #    聚合基础  聚合性能优化  聚合场景实践  基本语法是：\n“aggs”: { \u0026#34;SAMPLE\u0026#34;: { \u0026#34;sampler\u0026#34;: { \u0026#34;shard_size\u0026#34;: 100 }, \u0026#34;aggs\u0026#34;: {...} } } 分片大小属性 #  shard_size 属性告诉 Easysearch 每个分片最多收集多少文档。\n以下示例将每个分片上收集的文档数量限制为 1,000，然后使用 terms 聚合对文档进行分组：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;sample\u0026#34;: { \u0026#34;sampler\u0026#34;: { \u0026#34;shard_size\u0026#34;: 1000 }, \u0026#34;aggs\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;agent.keyword\u0026#34; } } } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;sample\u0026#34; : { \u0026#34;doc_count\u0026#34; : 1000, \u0026#34;terms\u0026#34; : { \u0026#34;doc_count_error_upper_bound\u0026#34; : 0, \u0026#34;sum_other_doc_count\u0026#34; : 0, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\u0026#34;, \u0026#34;doc_count\u0026#34; : 368 }, { \u0026#34;key\u0026#34; : \u0026#34;Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.50 Safari/534.24\u0026#34;, \u0026#34;doc_count\u0026#34; : 329 }, { \u0026#34;key\u0026#34; : \u0026#34;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\u0026#34;, \u0026#34;doc_count\u0026#34; : 303 } ] } } } } ","subcategory":null,"summary":"","tags":null,"title":"采样聚合（Sampler）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/sampler/"},{"category":null,"content":"邻接矩阵聚合 #  adjacency_matrix 聚合允许你定义过滤表达式，并返回一个交集矩阵，矩阵中的每个非空单元格代表一个分组。你可以找到落入任何过滤器组合中的文档数量。\n使用 adjacency_matrix 聚合通过将数据可视化为图形来发现概念之间的关联。\n相关指南（先读这些） #    聚合基础  聚合场景实践  例如，下面查询可以分析不同制造公司之间的关联关系：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;interactions\u0026#34;: { \u0026#34;adjacency_matrix\u0026#34;: { \u0026#34;filters\u0026#34;: { \u0026#34;grpA\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;manufacturer.keyword\u0026#34;: \u0026#34;Low Tide Media\u0026#34; } }, \u0026#34;grpB\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;manufacturer.keyword\u0026#34;: \u0026#34;Elitelligence\u0026#34; } }, \u0026#34;grpC\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;manufacturer.keyword\u0026#34;: \u0026#34;Oceanavigations\u0026#34; } } } } } } } 返回内容\n{ ... \u0026#34;aggregations\u0026#34; : { \u0026#34;interactions\u0026#34; : { \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;grpA\u0026#34;, \u0026#34;doc_count\u0026#34; : 1553 }, { \u0026#34;key\u0026#34; : \u0026#34;grpA\u0026amp;grpB\u0026#34;, \u0026#34;doc_count\u0026#34; : 590 }, { \u0026#34;key\u0026#34; : \u0026#34;grpA\u0026amp;grpC\u0026#34;, \u0026#34;doc_count\u0026#34; : 329 }, { \u0026#34;key\u0026#34; : \u0026#34;grpB\u0026#34;, \u0026#34;doc_count\u0026#34; : 1370 }, { \u0026#34;key\u0026#34; : \u0026#34;grpB\u0026amp;grpC\u0026#34;, \u0026#34;doc_count\u0026#34; : 299 }, { \u0026#34;key\u0026#34; : \u0026#34;grpC\u0026#34;, \u0026#34;doc_count\u0026#34; : 1218 } ] } } } 让我们更仔细地查看结果\n{ \u0026#34;key\u0026#34; : \u0026#34;grpA\u0026amp;grpB\u0026#34;, \u0026#34;doc_count\u0026#34; : 590 }  grpA ：由 Low Tide Media 制造的产品。 grpB ：由 Elitelligence 制造的产品。 590 : 同时生产的产品数量。  ","subcategory":null,"summary":"","tags":null,"title":"邻接矩阵聚合（Adjacency Matrix）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/adjacency-matrix/"},{"category":null,"content":"速率聚合 #  rate 聚合是一个指标聚合，用于计算文档或字段值在指定时间单位内的速率。它必须嵌套在 date_histogram（或 composite 中的 date_histogram 源）内部使用。\nrate 聚合特别适合将不同时间粒度的数据统一到相同的速率单位进行对比。例如，当 date_histogram 按月分桶时，你可以用 rate 聚合将每个月的值换算为\u0026quot;每天\u0026quot;或\u0026quot;每年\u0026quot;的速率。\n相关指南（先读这些） #    聚合基础  日期直方图聚合  聚合场景实践  参数说明 #     参数 必需/可选 数据类型 描述     field 可选 String 要计算速率的数值字段。如果省略，则计算文档的速率（即文档数/时间单位）。   unit 可选 String 速率的时间单位。有效值：second、minute、hour、day、week、month、quarter、year。如果省略，使用 date_histogram 的 calendar_interval 作为单位。   script 可选 Object 使用脚本动态计算速率值。   missing 可选 Numeric 缺少字段值的文档所使用的替代值。    基本用法：文档速率 #  计算每月的文档数量，并将其换算为每天的速率：\nGET logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;by_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;daily_rate\u0026#34;: { \u0026#34;rate\u0026#34;: { \u0026#34;unit\u0026#34;: \u0026#34;day\u0026#34; } } } } } } 返回内容中，每个月桶内的 daily_rate 值是该月文档数量除以该月天数：\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;by_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2024-01-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1704067200000, \u0026#34;doc_count\u0026#34;: 310, \u0026#34;daily_rate\u0026#34;: { \u0026#34;value\u0026#34;: 10.0 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2024-02-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1706745600000, \u0026#34;doc_count\u0026#34;: 290, \u0026#34;daily_rate\u0026#34;: { \u0026#34;value\u0026#34;: 10.0 } } ] } } } 字段值速率 #  计算某个数值字段在单位时间内的速率。例如，按月统计销售额，并换算为年化速率：\nGET sales/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;by_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;annual_revenue_rate\u0026#34;: { \u0026#34;rate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;revenue\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;year\u0026#34; } } } } } } 这将每个月的收入总和乘以 12（近似值，实际会根据月份天数精确计算），得到年化收入速率。\n不同粒度之间的速率换算 #  rate 聚合会感知日历单位（月份天数、闰年等）。例如：\n date_histogram 按 month 分桶，rate 使用 unit: day → 结果 = 月总值 ÷ 该月天数 date_histogram 按 day 分桶，rate 使用 unit: month → 结果 = 日总值 × 该月天数 date_histogram 按 month 分桶，rate 使用 unit: year → 结果 = 月总值 × 12（近似）  与 composite 聚合配合 #  rate 也可以在 composite 聚合中使用，只要 composite 的源之一是 date_histogram：\nGET logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;by_host_and_month\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;sources\u0026#34;: [ { \u0026#34;host\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;host.keyword\u0026#34; } } }, { \u0026#34;month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; } } } ] }, \u0026#34;aggs\u0026#34;: { \u0026#34;error_rate_per_day\u0026#34;: { \u0026#34;rate\u0026#34;: { \u0026#34;unit\u0026#34;: \u0026#34;day\u0026#34; } } } } } } 注意事项 #   rate 聚合必须嵌套在 date_histogram 或包含 date_histogram 源的 composite 聚合内部，否则会报错。 rate 是叶子聚合（Leaf Aggregation），不能包含子聚合。 如果省略 field 和 script，rate 计算的是文档数量的速率。 unit 参数支持的有效值必须与 date_histogram 的 calendar_interval 使用的日历单位一致（均为日历感知单位）。  ","subcategory":null,"summary":"","tags":null,"title":"速率聚合（Rate）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/rate/"},{"category":null,"content":"过滤器聚合 #  filter 聚合是一个查询子句，就像一个搜索查询一样 — match 或 term 或 range。您可以使用 filter 聚合在创建分组之前将整个文档集缩小到特定的文档集。\n相关指南（先读这些） #    聚合基础  聚合场景实践  以下示例展示了 avg 聚合在过滤上下文中运行的情况。 avg 聚合仅聚合与 range 查询匹配的文档：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;low_value\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;taxful_total_price\u0026#34;: { \u0026#34;lte\u0026#34;: 50 } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_amount\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } } } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;low_value\u0026#34; : { \u0026#34;doc_count\u0026#34; : 1633, \u0026#34;avg_amount\u0026#34; : { \u0026#34;value\u0026#34; : 38.363175998928355 } } } } filter vs. 查询 + 聚合 #  filter 聚合与在查询中使用 bool.filter 有所不同：\n   方式 作用范围 典型用途     查询中的 filter 过滤整个搜索的文档集（影响所有聚合） 全局过滤条件   filter 聚合 仅在该聚合内过滤（不影响其他聚合） 对比分析（例如同时看\u0026quot;全部\u0026quot;和\u0026quot;高价\u0026quot;）    对比示例 #  以下请求同时计算全部商品均价和高价商品（\u0026gt;100）均价：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;all_avg_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } }, \u0026#34;expensive_avg_price\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;taxful_total_price\u0026#34;: { \u0026#34;gt\u0026#34;: 100 } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_amount\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } } } } } }  提示：如需同时使用多个过滤条件分桶，请使用 filters 聚合。\n ","subcategory":null,"summary":"","tags":null,"title":"过滤器聚合（Filter）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/filter/"},{"category":null,"content":"词项聚合 #  terms 聚合会动态为字段中的每个唯一词条创建一个分组。\n相关指南（先读这些） #    聚合基础  聚合场景实践  以下示例使用 terms 聚合来查找网络日志数据中每个响应代码的文档数量：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;response_codes\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;response.keyword\u0026#34;, \u0026#34;size\u0026#34;: 10 } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;response_codes\u0026#34; : { \u0026#34;doc_count_error_upper_bound\u0026#34; : 0, \u0026#34;sum_other_doc_count\u0026#34; : 0, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;200\u0026#34;, \u0026#34;doc_count\u0026#34; : 12832 }, { \u0026#34;key\u0026#34; : \u0026#34;404\u0026#34;, \u0026#34;doc_count\u0026#34; : 801 }, { \u0026#34;key\u0026#34; : \u0026#34;503\u0026#34;, \u0026#34;doc_count\u0026#34; : 441 } ] } } } 值以 key 键返回。 doc_count 指定每个分组中的文档数量。默认情况下，分组按 doc-count 的降序排列。\n 可以使用 terms 通过按升序计数排序（ \u0026ldquo;order\u0026rdquo;: {\u0026ldquo;count\u0026rdquo;: \u0026ldquo;asc\u0026rdquo;} ）来搜索不常见的值。然而，我们强烈不建议这种做法，因为在涉及多个分片时，它可能导致不准确的结果。一个全局上不常见的词项可能不会在每个单个分片上显得不常见，或者可能完全不在某些分片返回的最不常见结果中。相反，一个在某个分片上出现频率较低的词项可能在另一个分片上很常见。在这两种情况下，分片级别的聚合可能会遗漏稀有词项，导致整体结果不正确。我们建议使用 rare_terms 聚合代替 terms 聚合，它专门设计用于更准确地处理这些情况。\n 返回文档数量和分片大小参数 #  terms 聚合返回的分组数量由 size 参数控制，默认值为 10。\n此外，负责聚合的协调节点会提示每个分片返回其顶部唯一词项。每个分片返回的分组数量由 shard_size 参数控制。此参数与 size 参数不同，它作为增加分组文档计数准确性的机制存在。\n例如，假设 size 和 shard_size 参数的值都为 3。 terms 聚合会向每个分片请求其前三个唯一词项。协调节点会汇总结果以计算最终结果。如果一个分片包含的某个对象不在前三个中，那么它就不会出现在响应中。然而，通过增加此请求的 shard_size 值，每个分片可以返回更多的唯一词项，从而提高协调节点收到所有相关结果的可能性。\n默认情况下， shard_size 参数设置为 size * 1.5 + 10 。\n在使用并发分片搜索时， shard_size 参数也会应用于每个分片切片。\nshard_size 参数作为平衡 terms 聚合的性能和文档计数准确性的方式。较高的 shard_size 值将确保更高的文档计数准确性，但会导致更高的内存和计算使用。较低的 shard_size 值将更高效，但会导致较低的文档计数准确性。\n文档计数错误 #  返回内容还包括两个名为 doc_count_error_upper_bound 和 sum_other_doc_count 的键。\nterms 聚合返回最顶端的唯一词。因此，如果数据包含许多唯一词，那么其中一些可能不会出现在结果中。 sum_other_doc_count 字段表示被排除在响应之外的文档总和。在这种情况下，数字是 0，因为所有唯一值都出现在响应中。\ndoc_count_error_upper_bound 字段表示最终结果中排除的唯一值的最大可能计数。使用此字段来估计计数的误差范围。\ndoc_count_error_upper_bound 值和准确性的概念仅适用于使用默认排序方式（按文档数量降序）进行的聚合。这是因为当你按文档数量降序排序时，未返回的任何词项都保证包含等于或少于已返回词项的文档数。基于这一点，你可以计算 doc_count_error_upper_bound 。\nmin_doc_count 和 shard_min_doc_count 参数 #  您可以使用 min_doc_count 参数过滤掉任何出现次数少于 min_doc_count 的唯一词项。 min_doc_count 阈值仅应用于合并从所有分片中检索到的结果之后。每个分片都不知道某个词项的全局文档数量。如果全局最频繁的 shard_size 个词项与某个分片本地最频繁的词项之间存在显著差异，在使用 min_doc_count 参数时您可能会收到意外结果。\n另外， shard_min_doc_count 参数用于过滤掉分片返回给协调器且结果少于 shard_min_doc_count 的唯一词项。\n收集模式 #  有两种收集模式可用： depth_first 和 breadth_first 。 depth_first 收集模式以深度优先方式扩展聚合树的所有分支，并在扩展完成后才进行剪枝。\n然而，在使用嵌套的聚合时，返回的分组数量 cardinality 会被每个嵌套级别的字段 cardinality 相乘，这使得随着聚合的嵌套，分组数量容易出现组合爆炸。\n你可以使用 breadth_first 集合模式来解决这个问题。在这种情况下，聚合树的第一级将在展开到下一级之前应用剪枝，可能会大大减少计算分组的数量。\n此外，执行 breadth_first 收集还会带来内存开销，该开销与匹配文档的数量呈线性关系。这是因为 breadth_first 收集的工作方式是通过缓存并重放从父级层级修剪后的分组集。\n参数说明 #     参数 必需/可选 数据类型 描述     field 必填 String 用于分桶的字段名称。通常为 keyword 类型。   size 可选 Integer 返回的桶数量。默认 10。   shard_size 可选 Integer 每个分片返回的桶数量。默认 size * 1.5 + 10。值越大准确度越高，但性能开销也越大。   min_doc_count 可选 Integer 全局文档计数的最小阈值。低于此值的词项不返回。默认 1。   shard_min_doc_count 可选 Integer 分片级别的最小文档数过滤。默认无限制。   order 可选 Object 桶的排序方式。默认按 _count 降序。可按 _key 排序或按子聚合指标排序。   missing 可选 String 缺少字段值的文档所使用的替代值。默认忽略缺失值。   show_term_doc_count_error 可选 Boolean 在每个桶中显示 doc_count_error_upper_bound。默认 false。   collect_mode 可选 String 聚合收集模式。可选 depth_first（默认）或 breadth_first。   include 可选 String/Array 结果中要包含的词项。可以是正则表达式或值数组。   exclude 可选 String/Array 从结果中排除的词项。可以是正则表达式或值数组。   script 可选 Object 使用脚本动态生成分桶值。    更改排序顺序 #  默认情况下，桶按 doc_count 降序排列。使用 order 参数可以改变排序方式：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;response_codes\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;response.keyword\u0026#34;, \u0026#34;order\u0026#34;: { \u0026#34;_key\u0026#34;: \u0026#34;asc\u0026#34; } } } } } 也可以按子聚合的结果排序：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;response_codes\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;response.keyword\u0026#34;, \u0026#34;order\u0026#34;: { \u0026#34;avg_bytes\u0026#34;: \u0026#34;desc\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_bytes\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } } } } } } 缺失值处理 #  默认情况下，没有聚合字段的文档会被忽略。使用 missing 参数为这些文档指定一个替代值：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;response_codes\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;response.keyword\u0026#34;, \u0026#34;missing\u0026#34;: \u0026#34;N/A\u0026#34; } } } }  注意：对于文本字段，terms 聚合会根据分析结果创建分组。分析过程（如词干化）会影响分组的精确性。建议在 keyword 类型的字段上使用 terms 聚合。\n 考虑预聚合数据 #  虽然 doc_count 字段提供了关于分组中聚合的独立文档数量的表示，但 doc_count 单独无法正确地增加存储预聚合数据的文档。要考虑预聚合数据并准确计算分组中的文档数量，您可以使用 _doc_count 字段来添加单个汇总字段中的文档数量。当文档包含 _doc_count 字段时，所有分组聚合都会识别其值并累积增加分组 doc_count 。在使用 _doc_count 字段时，请记住这些注意事项：\n 该字段不支持嵌套数组；只能使用正整数。 如果文档不包含 _doc_count 字段，聚合会使用该文档将计数增加 1。  参考样例 #  PUT /my_index/_doc/1 { \u0026#34;response_code\u0026#34;: 404, \u0026#34;date\u0026#34;:\u0026#34;2022-08-05\u0026#34;, \u0026#34;_doc_count\u0026#34;: 20 } PUT /my_index/_doc/2 { \u0026quot;response_code\u0026quot;: 404, \u0026quot;date\u0026quot;:\u0026quot;2022-08-06\u0026quot;, \u0026quot;_doc_count\u0026quot;: 10 }\nPUT /my_index/_doc/3 { \u0026quot;response_code\u0026quot;: 200, \u0026quot;date\u0026quot;:\u0026quot;2022-08-06\u0026quot;, \u0026quot;_doc_count\u0026quot;: 300 }\nGET /my_index/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;response_codes\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot; : \u0026quot;response_code\u0026quot; } } } } 返回内容\n{ \u0026#34;took\u0026#34; : 20, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 3, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : null, \u0026#34;hits\u0026#34; : [ ] }, \u0026#34;aggregations\u0026#34; : { \u0026#34;response_codes\u0026#34; : { \u0026#34;doc_count_error_upper_bound\u0026#34; : 0, \u0026#34;sum_other_doc_count\u0026#34; : 0, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : 200, \u0026#34;doc_count\u0026#34; : 300 }, { \u0026#34;key\u0026#34; : 404, \u0026#34;doc_count\u0026#34; : 30 } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"词项聚合（Terms）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/terms/"},{"category":null,"content":"Normalization 分词过滤器 #  归一化分词过滤器旨在以减少文本差异（尤其是特殊字符差异）的方式对文本进行调整和简化。它主要用于通过对特定语言中的字符进行标准化处理，来应对书写上的差异。\n相关指南（先读这些） #    文本分析：规范化  文本分析基础  以下是可用的归一化分词过滤器：\n 阿拉伯语归一化: arabic_normalization 德语归一化: german_normalization 印地语归一化: hindi_normalization 印度语系归一化: indic_normalization 索拉尼语归一化: sorani_normalization 波斯语归一化: persian_normalization 斯堪的纳维亚语归一化: scandinavian_normalization 斯堪的纳维亚语折叠处理归一化: scandinavian_folding 塞尔维亚语归一化: serbian_normalization  参考样例 #  以下示例请求创建了一个名为 german_normalizer_example 的新索引，并配置了一个带有 german_normalization的分词器：\nPUT /german_normalizer_example { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;german_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;german_normalization\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;german_normalizer_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;german_normalizer\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /german_normalizer_example/_analyze { \u0026#34;text\u0026#34;: \u0026#34;Straße München\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;german_normalizer_analyzer\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;strasse\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;munchen\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 14, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"规范化分词过滤器（Normalization）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/normalization/"},{"category":null,"content":"范围聚合 #  range 聚合允许你为每个分组定义范围。\n相关指南（先读这些） #    聚合基础  聚合场景实践  例如，你可以找到在 1000 和 2000 之间、2000 和 3000 之间以及 3000 和 4000 之间的字节数。在 range 参数中，你可以将范围定义为数组对象。\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;number_of_bytes_distribution\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;from\u0026#34;: 1000, \u0026#34;to\u0026#34;: 2000 }, { \u0026#34;from\u0026#34;: 2000, \u0026#34;to\u0026#34;: 3000 }, { \u0026#34;from\u0026#34;: 3000, \u0026#34;to\u0026#34;: 4000 } ] } } } } 响应包含 from 键值，并排除 to 键值：\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;number_of_bytes_distribution\u0026#34; : { \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;1000.0-2000.0\u0026#34;, \u0026#34;from\u0026#34; : 1000.0, \u0026#34;to\u0026#34; : 2000.0, \u0026#34;doc_count\u0026#34; : 805 }, { \u0026#34;key\u0026#34; : \u0026#34;2000.0-3000.0\u0026#34;, \u0026#34;from\u0026#34; : 2000.0, \u0026#34;to\u0026#34; : 3000.0, \u0026#34;doc_count\u0026#34; : 1369 }, { \u0026#34;key\u0026#34; : \u0026#34;3000.0-4000.0\u0026#34;, \u0026#34;from\u0026#34; : 3000.0, \u0026#34;to\u0026#34; : 4000.0, \u0026#34;doc_count\u0026#34; : 1422 } ] } } } 参数说明 #     参数 必需/可选 数据类型 描述     field 必填 String 用于分桶的数值字段名称。   ranges 必填 Array 范围数组，每个元素可包含 from（含）和 to（不含）。   keyed 可选 Boolean 若为 true，以键值对形式返回桶。默认 false。   script 可选 Object 用脚本生成分桶的值，替代 field。     注意：每个范围的 from 值是 包含 的，to 值是 不包含 的（即 [from, to)）。\n 自定义分桶键名 #  默认的桶 key 使用 \u0026quot;from-to\u0026quot; 格式（如 \u0026quot;1000.0-2000.0\u0026quot;）。你可以为每个范围指定自定义的 key 使结果更易读：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;traffic_levels\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;keyed\u0026#34;: true, \u0026#34;ranges\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;小流量\u0026#34;, \u0026#34;to\u0026#34;: 1000 }, { \u0026#34;key\u0026#34;: \u0026#34;中流量\u0026#34;, \u0026#34;from\u0026#34;: 1000, \u0026#34;to\u0026#34;: 5000 }, { \u0026#34;key\u0026#34;: \u0026#34;大流量\u0026#34;, \u0026#34;from\u0026#34;: 5000 } ] } } } } 嵌套子聚合 #  可以在每个范围桶内嵌套其他聚合，例如查看每个流量区间的平均响应码：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;bytes_ranges\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;to\u0026#34;: 1000 }, { \u0026#34;from\u0026#34;: 1000, \u0026#34;to\u0026#34;: 5000 }, { \u0026#34;from\u0026#34;: 5000 } ] }, \u0026#34;aggs\u0026#34;: { \u0026#34;common_response\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;response.keyword\u0026#34;, \u0026#34;size\u0026#34;: 3 } } } } } }  提示：如需对日期字段进行范围分桶，请使用 日期范围聚合；对 IP 地址使用 IP 范围聚合。\n ","subcategory":null,"summary":"","tags":null,"title":"范围聚合（Range）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/range/"},{"category":null,"content":"自动间隔日期直方图聚合 #  与日期直方图聚合类似，其中你必须指定一个间隔，auto_date_histogram 是一个多分组聚合，根据你提供的分组数量和数据的时范围自动创建日期直方图分组。返回的实际分组数量总是小于或等于你指定的分组数量。当你在处理时间序列数据并希望在不同时间间隔上可视化或分析数据，而不需要手动指定间隔大小时，这种聚合特别有用。\n相关指南（先读这些） #    聚合基础  聚合场景实践  时间序列建模  间隔参数 #  分组间隔是根据收集的数据选择的，以确保返回的分组数量小于或等于请求的数量。\n下表列出了每个时间单位可能的返回间隔。\n   单位 间隔参数     Seconds 1、5、10 和 30 的倍数   Minutes 1、5、10 和 30 的倍数   Hours 1、3 和 12 的倍数   Days 1 和 7 的倍数   Months 1 和 3 的倍数   Years 1、5、10、20、50 和 100 的倍数    如果一个聚合返回的分组太多（例如，每天一个分组），Easysearch 会自动减少分组的数量以确保结果可管理。它不会返回请求的确切数量的每日分组，而是会减少大约 1/7。例如，如果你请求 70 个分组，但数据中包含太多的每日间隔，Easysearch 可能只会返回 10 个分组，将数据分组到更大的间隔（如周）中，以避免结果数量过多。这有助于优化聚合，并在数据过多时防止过多细节。\n参考样例 #  在以下示例中，你将搜索一个包含博客文章的索引。\n首先，为这个索引创建一个映射，并将 date_posted 字段指定为 date 类型：\nPUT blogs { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;date_posted\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34; : \u0026#34;yyyy-MM-dd\u0026#34; } } } } 接下来，将以下文档索引到 blogs 索引中：\nPUT blogs/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Semantic search in Easysearch\u0026#34;, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-04-17\u0026#34; } PUT blogs/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Sparse search in Easysearch\u0026quot;, \u0026quot;date_posted\u0026quot;: \u0026quot;2022-05-02\u0026quot; }\nPUT blogs/_doc/3 { \u0026quot;name\u0026quot;: \u0026quot;Distributed tracing with Data Prepper\u0026quot;, \u0026quot;date_posted\u0026quot;: \u0026quot;2022-04-25\u0026quot; }\nPUT blogs/_doc/4 { \u0026quot;name\u0026quot;: \u0026quot;Observability in Easysearch\u0026quot;, \u0026quot;date_posted\u0026quot;: \u0026quot;2023-03-23\u0026quot; } 要使用 auto_date_histogram 聚合，请指定包含日期或时间戳值的字段。例如，要将博客文章按 date_posted 聚合到两个分组中，请发送以下请求：\nGET /blogs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;auto_date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date_posted\u0026#34;, \u0026#34;buckets\u0026#34;: 2 } } } } 返回内容显示博客文章被聚合到两个分组中。间隔被自动设置为 1 年，所有三个 2022 年的博客文章被收集到一个分组中，而 2023 年的博客文章在另一个分组中：\n{ \u0026#34;took\u0026#34;: 20, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2022-01-01\u0026#34;, \u0026#34;key\u0026#34;: 1640995200000, \u0026#34;doc_count\u0026#34;: 3 }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2023-01-01\u0026#34;, \u0026#34;key\u0026#34;: 1672531200000, \u0026#34;doc_count\u0026#34;: 1 } ], \u0026#34;interval\u0026#34;: \u0026#34;1y\u0026#34; } } } 分组信息 #  每个分组包含以下信息：\n{ \u0026#34;key_as_string\u0026#34;: \u0026#34;2023-01-01\u0026#34;, \u0026#34;key\u0026#34;: 1672531200000, \u0026#34;doc_count\u0026#34;: 1 } 在 Easysearch 中，日期内部存储为 64 位整数，表示自纪元以来的毫秒时间戳。在聚合响应中，每个分组 key 以时间戳的形式返回。 key_as_string 值显示相同的时间戳，但根据 format 参数格式化为日期字符串。 doc_count 字段包含分组中的文档数量。\n参数说明 #  自动间隔日期直方图聚合接受以下参数。\n   参数 数据类型 描述     field String 要对其聚合的字段。该字段必须包含日期或时间戳值。必须提供 field 或 script 。   buckets Integer 期望的分组数量。返回的分组数量小于或等于期望的分组数量。可选。默认为 10 。   minimum_interval String 要使用的最小间隔。指定最小间隔可以使聚合过程更高效。有效值为 year 、 month 、 day 、 hour 、 minute 和 second 。可选。   time_zone String 指定使用除默认（UTC）之外的时间区进行分组划分和舍入。您可以指定 time_zone 参数为 UTC 偏移量，例如 -04:00 ，或 IANA 时间区 ID，例如 America/New_York 。可选。默认为 UTC 。有关更多信息，请参阅 Time zone。   format String 返回表示分组键的日期的格式。可选。默认为字段映射中指定的格式。有关更多信息，请参阅 Date format。   script String 一个用于将值聚合到分组中的文档级或值级脚本。必须使用 field 或 script 。   missing String 指定如何处理字段值缺失的文档。默认情况下，此类文档会被忽略。如果你在 missing 参数中指定一个日期值，所有字段值缺失的文档都会被收集到指定日期的分组中。    关于日期格式 #  如果你没有指定 format 参数，将使用字段映射中定义的格式（如前一个响应所示）。要修改格式，请指定 format 参数：\nGET /blogs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;auto_date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date_posted\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34; } } } } key_as_string 字段现在以指定的格式返回：\n{ \u0026#34;key_as_string\u0026#34;: \u0026#34;2023-01-01 00:00:00\u0026#34;, \u0026#34;key\u0026#34;: 1672531200000, \u0026#34;doc_count\u0026#34;: 1 } 或者，您可以指定一种内置的日期格式：\nGET /blogs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;auto_date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date_posted\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;basic_date_time_no_millis\u0026#34; } } } } key_as_string 字段现在以指定的格式返回：\n{ \u0026#34;key_as_string\u0026#34;: \u0026#34;20230101T000000Z\u0026#34;, \u0026#34;key\u0026#34;: 1672531200000, \u0026#34;doc_count\u0026#34;: 1 } 时区使用 #  默认情况下，日期以 UTC 格式存储和处理。 time_zone 参数允许您为分分组指定不同的时区。您可以指定 time_zone 参数为 UTC 偏移量，例如 -04:00 ，或 IANA 时区 ID，例如 America/New_York 。\n例如，将以下文档索引到索引中：\nPUT blogs1/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Semantic search in Easysearch\u0026#34;, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-04-17T01:00:00.000Z\u0026#34; } PUT blogs1/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Sparse search in Easysearch\u0026quot;, \u0026quot;date_posted\u0026quot;: \u0026quot;2022-04-17T04:00:00.000Z\u0026quot; }\n首先，不指定时区运行聚合操作：\nGET /blogs1/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;auto_date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date_posted\u0026#34;, \u0026#34;buckets\u0026#34;: 2, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34; } } } } 返回内容包含两个 3 小时的分组，从 2022 年 4 月 17 日凌晨 UTC 开始：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2022-04-17 01:00:00\u0026#34;, \u0026#34;key\u0026#34;: 1650157200000, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2022-04-17 04:00:00\u0026#34;, \u0026#34;key\u0026#34;: 1650168000000, \u0026#34;doc_count\u0026#34;: 1 } ], \u0026#34;interval\u0026#34;: \u0026#34;3h\u0026#34; } } } 现在，指定一个 time_zone 的 -02:00 ：\nGET /blogs1/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;auto_date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date_posted\u0026#34;, \u0026#34;buckets\u0026#34;: 2, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;, \u0026#34;time_zone\u0026#34;: \u0026#34;-02:00\u0026#34; } } } } 返回内容包含两个分组，其中开始时间偏移 2 小时，从 2022 年 4 月 16 日 23:00 开始：\n{ \u0026#34;took\u0026#34;: 17, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2022-04-16 23:00:00\u0026#34;, \u0026#34;key\u0026#34;: 1650157200000, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2022-04-17 02:00:00\u0026#34;, \u0026#34;key\u0026#34;: 1650168000000, \u0026#34;doc_count\u0026#34;: 1 } ], \u0026#34;interval\u0026#34;: \u0026#34;3h\u0026#34; } } } \n在使用有夏令时（DST）变化的时区时，接近转换点的分组的大小可能与相邻分组的大小略有不同。\n ","subcategory":null,"summary":"","tags":null,"title":"自动间隔日期直方图聚合（Auto Date Histogram）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/auto-interval-date-histogram/"},{"category":null,"content":"脚本指标聚合 #  scripted_metric 聚合是一个多值指标聚合，它返回根据指定脚本计算的指标。脚本有四个阶段：init、map、combine 和 reduce，每个聚合按顺序运行这些阶段，组合来自文档的结果。\n相关指南（先读这些） #    聚合基础  聚合场景实践  所有四个脚本共享一个可变对象，称为 state，该对象由你定义。state 在 init、map 和 combine 阶段时对每个分片是局部的。结果被传递到 states 数组中用于 reduce 阶段。因此，每个分片的 state 在分片在 reduce 步骤中组合之前是独立的。\n参数说明 #  scripted_metric 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     init_script 可选 String 一个脚本，在每个分片上处理任何文档之前执行一次。用于设置初始 state（例如，在 state 对象中初始化计数器或列表）。如果没有提供，state 在每个分片上开始时是一个空对象。   map_script 必需 String 一个脚本，针对聚合收集的每个文档执行。此脚本根据文档的数据更新 state。例如，您可以检查字段的值，然后递增计数器或在 state 中计算运行总和。   combine_script 必需 String 一个脚本，在每个分片上处理完所有文档后由 map_script 执行一次。此脚本将分片的 state 聚合为单个结果发送回协调节点。此脚本用于完成一个分片的计算（例如，汇总存储在 state 中的计数器或总计）。脚本应返回其分片的汇总值或结构。   reduce_script 必需 String 一个脚本在接收到所有分片的合并结果后，在协调节点上执行一次。这个脚本接收一个特殊变量 states，它是一个包含每个分片从 combine_script 输出的数组。reduce_script 遍历状态并生成最终的聚合输出（例如，添加分片总和或合并计数的映射）。reduce_script 返回的值是在聚合结果中报告的值。   params 可选 Object 除 reduce_script 外，所有脚本都可以访问用户定义的参数。    可返回的类型 #  脚本可以在内部使用任何有效的操作和对象。然而，存储在 state 或从任何脚本返回的数据必须属于允许的类型之一。这个限制存在是因为中间 state 需要在节点之间发送。允许的类型如下：\n 基本类型： int , long , float , double , boolean String 字符串 Map 映射（键和值仅允许为允许类型：基本类型、字符串、映射或数组） 数组（仅包含允许类型：基本类型、字符串、映射或数组）  state 可以是一个数字、字符串、map（对象）或数组（列表），也可以是这些的组合。例如，你可以使用 map 来累积多个计数器，使用数组来收集值，或使用单个数字来保持累计总和。如果你需要返回多个指标，可以将它们存储在 map 或数组中。如果你从 reduce_script 返回 map 作为最终值，聚合结果包含一个对象。如果你返回单个数字或字符串，结果是一个单一值。\n在脚本中使用参数 #  您可以使用 params 字段可选地向脚本传递自定义参数。这是一个用户定义的对象，其内容成为 init_script、map_script 和 combine_script 中的变量。reduce_script 不会直接接收 params，因为在 reduce 阶段，所有需要的数据都必须在 states 数组中。如果您需要在 reduce 阶段使用常量，可以将其作为每个分片 state 的一部分，或使用存储的脚本。所有参数都必须在全局 params 对象中定义。这确保它们在不同脚本阶段之间共享。如果您未指定任何 params，则 params 对象为空。\n例如，您可以在 params 中提供 threshold 或 field 名称，然后在脚本中引用 params.threshold 或 params.field ：\n\u0026#34;scripted_metric\u0026#34;: { \u0026#34;params\u0026#34;: { \u0026#34;threshold\u0026#34;: 100, \u0026#34;field\u0026#34;: \u0026#34;amount\u0026#34; }, \u0026#34;init_script\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;map_script\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;combine_script\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;reduce_script\u0026#34;: \u0026#34;...\u0026#34; } 参考样例 #\n 以下示例展示了使用 scripted_metric 的不同方法。\n计算交易净利润 #  以下示例展示了如何使用 scripted_metric 聚合来计算内置聚合不直接支持的定制指标。该数据集表示财务交易，其中每个文档被分类为 sale （收入）或 cost （支出），并包含一个 amount 字段。目标是通过从所有文档的总销售额中减去总成本来计算总净利润。\n创建索引：\nPUT transactions { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;amount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } 索引四笔交易，两笔销售（金额 80 和 130 ），以及两笔成本（ 10 和 30 ）：\nPUT transactions/_bulk?refresh=true { \u0026#34;index\u0026#34;: {} } { \u0026#34;type\u0026#34;: \u0026#34;sale\u0026#34;, \u0026#34;amount\u0026#34;: 80 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;type\u0026#34;: \u0026#34;cost\u0026#34;, \u0026#34;amount\u0026#34;: 10 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;type\u0026#34;: \u0026#34;cost\u0026#34;, \u0026#34;amount\u0026#34;: 30 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;type\u0026#34;: \u0026#34;sale\u0026#34;, \u0026#34;amount\u0026#34;: 130 } 要运行一个使用 scripted_metric 聚合来计算利润的搜索，请使用以下脚本：\n init_script 创建一个空列表，用于存储每个分片的事务值。 map_script 根据类型 sale 将每个文档的金额作为正数添加到 state.transactions 列表中，如果类型是 cost 则作为负数。在 map 阶段结束时，每个分片都有一个 state.transactions 列表，代表其收入和支出。 combine_script 处理 state.transactions 列表，并为分片计算一个 shardProfit 值。然后返回 shardProfit 作为分片的输出。 reduce_script 在协调节点上运行，接收 states 数组，其中包含每个分片的 shardProfit 值。它检查空条目，将这些值相加以计算总利润，并返回最终结果。  以下请求包含上面描述的脚本：\nGET transactions/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;total_profit\u0026#34;: { \u0026#34;scripted_metric\u0026#34;: { \u0026#34;init_script\u0026#34;: \u0026#34;state.transactions = []\u0026#34;, \u0026#34;map_script\u0026#34;: \u0026#34;state.transactions.add(doc[\u0026#39;type\u0026#39;].value == \u0026#39;sale\u0026#39; ? doc[\u0026#39;amount\u0026#39;].value : -1 * doc[\u0026#39;amount\u0026#39;].value)\u0026#34;, \u0026#34;combine_script\u0026#34;: \u0026#34;double shardProfit = 0; for (t in state.transactions) { shardProfit += t; } return shardProfit;\u0026#34;, \u0026#34;reduce_script\u0026#34;: \u0026#34;double totalProfit = 0; for (p in states) { if (p != null) { totalProfit += p; }} return totalProfit;\u0026#34; } } } } 返回 total_profit :\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;total_profit\u0026#34;: { \u0026#34;value\u0026#34;: 170 } } } 给 HTTP 响应代码分类 #\n 以下示例展示了如何使用 scripted_metric 聚合在单个聚合中返回多个值。数据集由包含 HTTP 响应码的 Web 服务器日志条目组成。目标是将这些响应分为三类：成功响应（2xx 状态码）、客户端或服务器错误（4xx 或 5xx 状态码）以及其他响应（1xx 或 3xx 状态码）。这种分类是通过在基于映射的聚合 state 中维护计数器来实现的。\n创建一个示例索引：\nPUT logs { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;response\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } 添加具有各种响应代码的示例文档：\nPUT logs/_bulk?refresh=true { \u0026#34;index\u0026#34;: {} } { \u0026#34;response\u0026#34;: \u0026#34;200\u0026#34; } { \u0026#34;index\u0026#34;: {} } { \u0026#34;response\u0026#34;: \u0026#34;201\u0026#34; } { \u0026#34;index\u0026#34;: {} } { \u0026#34;response\u0026#34;: \u0026#34;404\u0026#34; } { \u0026#34;index\u0026#34;: {} } { \u0026#34;response\u0026#34;: \u0026#34;500\u0026#34; } { \u0026#34;index\u0026#34;: {} } { \u0026#34;response\u0026#34;: \u0026#34;304\u0026#34; } state （每个分片上）是一个 map ，包含三个计数器： error 、 success 和 other 。\n要运行一个用于统计类别的脚本指标聚合，请使用以下脚本：\n init_script 将 error 、 success 和 other 的计数器初始化为 0 。 map_script 检查每个文档的响应代码，并根据响应代码递增相应的计数器。 combine_script 返回该分片的 state.responses map 。 reduce_script 合并所有分片的 maps 数组 ( states )。因此，它创建一个新的组合 map ，并将每个分片的 map 中的 error 、 success 和 other 计数添加进去。这个组合的 map 作为最终结果返回。  以下请求包含了上面描述的脚本：\nGET logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;responses_by_type\u0026#34;: { \u0026#34;scripted_metric\u0026#34;: { \u0026#34;init_script\u0026#34;: \u0026#34;state.responses = new HashMap(); state.responses.put(\u0026#39;success\u0026#39;, 0); state.responses.put(\u0026#39;error\u0026#39;, 0); state.responses.put(\u0026#39;other\u0026#39;, 0);\u0026#34;, \u0026#34;map_script\u0026#34;: \u0026#34;\u0026#34;\u0026#34; String code = doc[\u0026#39;response\u0026#39;].value; if (code.startsWith(\u0026#34;5\u0026#34;) || code.startsWith(\u0026#34;4\u0026#34;)) { // 4xx or 5xx -\u0026gt; count as error state.responses.error += 1; } else if (code.startsWith(\u0026#34;2\u0026#34;)) { // 2xx -\u0026gt; count as success state.responses.success += 1; } else { // anything else (e.g., 1xx, 3xx, etc.) -\u0026gt; count as other state.responses.other += 1; } \u0026#34;\u0026#34;\u0026#34;, \u0026#34;combine_script\u0026#34;: \u0026#34;return state.responses;\u0026#34;, \u0026#34;reduce_script\u0026#34;: \u0026#34;\u0026#34;\u0026#34; Map combined = new HashMap(); combined.error = 0; combined.success = 0; combined.other = 0; for (state in states) { if (state != null) { combined.error += state.error; combined.success += state.success; combined.other += state.other; } } return combined; \u0026#34;\u0026#34;\u0026#34; } } } } 在 value 对象中返回了三个值，展示了通过在 state 中使用 map ，脚本指标如何一次性返回多个指标\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 5, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;responses_by_type\u0026#34;: { \u0026#34;value\u0026#34;: { \u0026#34;other\u0026#34;: 1, \u0026#34;success\u0026#34;: 2, \u0026#34;error\u0026#34;: 2 } } } } 管理空分组（没有文档） #  当使用 scripted_metric 聚合作为分组聚合（例如 terms ）的子聚合时，需要考虑某些分片上不包含文档的分组。在这种情况下，这些分片会为聚合 state 返回 null 值。在 reduce_script 阶段， states 数组可能因此包含对应这些分片的 null 条目。为确保可靠执行， reduce_script 必须设计为能够优雅地处理 null 值。常见的方法是在访问或操作每个 state 之前加入条件检查，例如 if (state != null) 。若未实施此类检查，在跨分片处理空分组时可能导致运行时错误。\n性能考量 #  由于脚本指标为每个文档运行自定义代码，因此可能会在内存中积累大量的 state ，所以它们可能比内置聚合慢。每个分片的中间 state 必须序列化才能发送到协调节点。因此，如果您的 state 非常大，它可能会消耗大量内存和网络带宽。为了保持搜索效率，请尽可能使您的脚本轻量，并避免在 state 中积累不必要的数据。在发送之前，使用合并阶段来缩减 state 数据，如“从交易中计算净利润”所示，并且仅收集您真正需要以生成最终指标的数据。\n","subcategory":null,"summary":"","tags":null,"title":"脚本指标聚合（Scripted Metric）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/scripted-metric/"},{"category":null,"content":"缺失值聚合 #  如果你的索引中的文档完全不包含聚合字段，或者聚合字段的值为 null，请使用 missing 参数指定这些文档应该放入的分组的名称。\n相关指南（先读这些） #    聚合基础  聚合场景实践  以下示例将任何缺失的值添加到名为“N/A”的分组中：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;response_codes\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;response.keyword\u0026#34;, \u0026#34;size\u0026#34;: 10, \u0026#34;missing\u0026#34;: \u0026#34;N/A\u0026#34; } } } } 由于 min_doc_count 参数的默认值为 1， missing 参数在其响应中不会返回任何分组。将 min_doc_count 参数设置为 0 以在响应中查看“N/A”分组：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;response_codes\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;response.keyword\u0026#34;, \u0026#34;size\u0026#34;: 10, \u0026#34;missing\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;min_doc_count\u0026#34;: 0 } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;response_codes\u0026#34; : { \u0026#34;doc_count_error_upper_bound\u0026#34; : 0, \u0026#34;sum_other_doc_count\u0026#34; : 0, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;200\u0026#34;, \u0026#34;doc_count\u0026#34; : 12832 }, { \u0026#34;key\u0026#34; : \u0026#34;404\u0026#34;, \u0026#34;doc_count\u0026#34; : 801 }, { \u0026#34;key\u0026#34; : \u0026#34;503\u0026#34;, \u0026#34;doc_count\u0026#34; : 441 }, { \u0026#34;key\u0026#34; : \u0026#34;N/A\u0026#34;, \u0026#34;doc_count\u0026#34; : 0 } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"缺失值聚合（Missing）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/missing/"},{"category":null,"content":"统计聚合 #  stats 聚合是一个多值指标聚合，用于计算数值数据的汇总。这种聚合有助于快速了解数值字段的分布情况。它可以直接作用于字段，应用脚本来派生值，或处理缺少字段的文档。stats 聚合返回五个值：\n相关指南（先读这些） #     聚合基础\n   聚合场景实践\n  count : 收集到的值的数量\n  min : 最低值\n  max : 最高值\n  sum : 所有值的总和\n  avg : 值的平均数（总和除以数量）\n  参数说明 #  stats 聚合支持以下可选参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 要聚合的字段。必须是数值字段。   script 可选 Object 用于计算聚合自定义值的脚本。可单独使用或与 field 一起使用。   missing 可选 Number 用于缺少目标字段的文档的默认值。    参考样例 #  以下示例计算 stats 聚合的电力使用情况。\n创建一个名为 power_usage 的索引，并添加包含给定小时内消耗的千瓦时 (kWh) 数量的文档：\nPUT /power_usage/_bulk?refresh=true {\u0026#34;index\u0026#34;: {}} {\u0026#34;device_id\u0026#34;: \u0026#34;A1\u0026#34;, \u0026#34;kwh\u0026#34;: 1.2} {\u0026#34;index\u0026#34;: {}} {\u0026#34;device_id\u0026#34;: \u0026#34;A2\u0026#34;, \u0026#34;kwh\u0026#34;: 0.7} {\u0026#34;index\u0026#34;: {}} {\u0026#34;device_id\u0026#34;: \u0026#34;A3\u0026#34;, \u0026#34;kwh\u0026#34;: 1.5} 要在所有文档中对 kwh 字段计算统计信息，使用一个名为 consumption_stats 的 stats 聚合，聚合字段为 kwh 。将 size 设置为 0 表示不应返回文档命中：\nGET /power_usage/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;consumption_stats\u0026#34;: { \u0026#34;stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;kwh\u0026#34; } } } } 返回内容为索引中的三个文档包含 count 、 min 、 max 、 avg 和 sum 值：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;consumption_stats\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;min\u0026#34;: 0.699999988079071, \u0026#34;max\u0026#34;: 1.5, \u0026#34;avg\u0026#34;: 1.1333333452542622, \u0026#34;sum\u0026#34;: 3.400000035762787 } } } 每个分组运行 stats 聚合 #  您可以通过在 device_id 字段中将 stats 聚合嵌套在 terms 聚合中来为每个设备计算单独的统计信息。 terms 聚合根据唯一的 device_id 值将文档分组，而 stats 聚合在每个分组内计算汇总统计信息：\nGET /power_usage/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;per_device\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;device_id.keyword\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;device_usage_stats\u0026#34;: { \u0026#34;stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;kwh\u0026#34; } } } } } } 返回为每个 device_id 返回一个分组，每个分组内包含计算出的 count 、 min 、 max 、 avg 和 sum 字段：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;per_device\u0026#34;: { \u0026#34;doc_count_error_upper_bound\u0026#34;: 0, \u0026#34;sum_other_doc_count\u0026#34;: 0, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;A1\u0026#34;, \u0026#34;doc_count\u0026#34;: 1, \u0026#34;device_usage_stats\u0026#34;: { \u0026#34;count\u0026#34;: 1, \u0026#34;min\u0026#34;: 1.2000000476837158, \u0026#34;max\u0026#34;: 1.2000000476837158, \u0026#34;avg\u0026#34;: 1.2000000476837158, \u0026#34;sum\u0026#34;: 1.2000000476837158 } }, { \u0026#34;key\u0026#34;: \u0026#34;A2\u0026#34;, \u0026#34;doc_count\u0026#34;: 1, \u0026#34;device_usage_stats\u0026#34;: { \u0026#34;count\u0026#34;: 1, \u0026#34;min\u0026#34;: 0.699999988079071, \u0026#34;max\u0026#34;: 0.699999988079071, \u0026#34;avg\u0026#34;: 0.699999988079071, \u0026#34;sum\u0026#34;: 0.699999988079071 } }, { \u0026#34;key\u0026#34;: \u0026#34;A3\u0026#34;, \u0026#34;doc_count\u0026#34;: 1, \u0026#34;device_usage_stats\u0026#34;: { \u0026#34;count\u0026#34;: 1, \u0026#34;min\u0026#34;: 1.5, \u0026#34;max\u0026#34;: 1.5, \u0026#34;avg\u0026#34;: 1.5, \u0026#34;sum\u0026#34;: 1.5 } } ] } } } 这使您能够通过单个查询比较不同设备的使用统计信息。\n使用脚本计算派生值 #  您也可以使用脚本计算 stats 聚合中使用的值。当指标来自文档字段或需要转换时，这很有用。\n例如，在运行 stats 聚合之前，将千瓦时（kWh）转换为瓦时（Wh），因为 1 kWh 等于 1,000 Wh ，你可以使用一个将每个值乘以 1,000 的脚本。以下脚本 doc['kwh'].value * 1000 用于推导每个文档的输入值：\nGET /power_usage/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;usage_wh_stats\u0026#34;: { \u0026#34;stats\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;kwh\u0026#39;].value * 1000\u0026#34; } } } } } 返回的 stats 聚合反映了 1200 、 700 和 1500 Wh 的值\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;usage_wh_stats\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;min\u0026#34;: 699.999988079071, \u0026#34;max\u0026#34;: 1500, \u0026#34;avg\u0026#34;: 1133.3333452542622, \u0026#34;sum\u0026#34;: 3400.000035762787 } } } 使用带有字段的值脚本 #  当将字段与转换结合使用时，你可以同时指定 field 和 script 。这允许使用 _value 变量来在脚本中引用字段的值。\n以下示例在计算 stats 聚合之前将每个能量读数增加 5%：\nGET /power_usage/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;adjusted_usage\u0026#34;: { \u0026#34;stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;kwh\u0026#34;, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;_value * 1.05\u0026#34; } } } } } 缺省值 #\n 如果某些文档不包含目标字段，它们默认会被排除在聚合之外。要使用默认值包含它们，你可以指定 missing 参数。\n以下请求将缺失的 kwh 值视为 0.0 ：\nGET /power_usage/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;consumption_with_default\u0026#34;: { \u0026#34;stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;kwh\u0026#34;, \u0026#34;missing\u0026#34;: 0.0 } } } } \n","subcategory":null,"summary":"","tags":null,"title":"统计聚合（Stats）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/stats/"},{"category":null,"content":"统计桶聚合 #  stats_bucket 聚合是一个同级聚合，它为先前聚合的分组返回各种统计信息（count、min、max、avg 和 sum）。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  stats_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。    参考样例 #  以下示例创建一个以一个月为间隔的日期直方图。 sum 子聚合计算每个月所有字节的总和。最后， stats_bucket 聚合从这些总和中返回 count 、 avg 、 sum 、 min 和 max 统计信息：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } } } }, \u0026#34;stats_monthly_bytes\u0026#34;: { \u0026#34;stats_bucket\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;visits_per_month\u0026gt;sum_of_bytes\u0026#34; } } } } 返回内容 #  该聚合返回每个分组的五个基本统计数据：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 } } ] }, \u0026#34;stats_monthly_bytes\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;min\u0026#34;: 2804103, \u0026#34;max\u0026#34;: 39103067, \u0026#34;avg\u0026#34;: 26575229.666666668, \u0026#34;sum\u0026#34;: 79725689 } } } \n","subcategory":null,"summary":"","tags":null,"title":"统计桶聚合（Stats Bucket）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/stats-bucket/"},{"category":null,"content":"Classic 分词器 #  classic 分词器会解析文本，并应用英语语法规则将文本拆分为词元。它包含以下特定的逻辑来处理匹配规则：\n相关指南（先读这些） #     文本分析：识别词元\n   文本分析基础\n  首字母缩写词\n  电子邮件地址\n  域名\n  某些类型的标点符号\n   这种词元生成器最适合处理英语文本。对于其他语言，尤其是那些具有不同语法结构的语言，它可能无法产生最佳效果。\n 经典词元生成器按如下方式解析文本：\n 标点符号：在大多数标点符号处拆分文本，并移除标点字符。后面不跟空格的点号会被视为词元的一部分。 连字符：在连字符处拆分单词，但当词元中存在数字时除外。当词元中存在数字时，该词元不会被拆分，而是被当作产品编号处理。 电子邮件：识别电子邮件地址和主机名，并将它们作为单个词元保留。  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用经典词元生成器的分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_classic_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;classic\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_classic_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_classic_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;For product AB3423, visit X\u0026amp;Y at example.com, email info@example.com, or call the operator\u0026#39;s phone number 1-800-555-1234. P.S. 你好.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;For\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;AB3423\u0026#34;, \u0026#34;start_offset\u0026#34;: 12, \u0026#34;end_offset\u0026#34;: 18, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;visit\u0026#34;, \u0026#34;start_offset\u0026#34;: 20, \u0026#34;end_offset\u0026#34;: 25, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;X\u0026amp;Y\u0026#34;, \u0026#34;start_offset\u0026#34;: 26, \u0026#34;end_offset\u0026#34;: 29, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;COMPANY\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;at\u0026#34;, \u0026#34;start_offset\u0026#34;: 30, \u0026#34;end_offset\u0026#34;: 32, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 }, { \u0026#34;token\u0026#34;: \u0026#34;example.com\u0026#34;, \u0026#34;start_offset\u0026#34;: 33, \u0026#34;end_offset\u0026#34;: 44, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;HOST\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 6 }, { \u0026#34;token\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;start_offset\u0026#34;: 46, \u0026#34;end_offset\u0026#34;: 51, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 7 }, { \u0026#34;token\u0026#34;: \u0026#34;info@example.com\u0026#34;, \u0026#34;start_offset\u0026#34;: 52, \u0026#34;end_offset\u0026#34;: 68, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;EMAIL\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 8 }, { \u0026#34;token\u0026#34;: \u0026#34;or\u0026#34;, \u0026#34;start_offset\u0026#34;: 70, \u0026#34;end_offset\u0026#34;: 72, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 9 }, { \u0026#34;token\u0026#34;: \u0026#34;call\u0026#34;, \u0026#34;start_offset\u0026#34;: 73, \u0026#34;end_offset\u0026#34;: 77, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 10 }, { \u0026#34;token\u0026#34;: \u0026#34;the\u0026#34;, \u0026#34;start_offset\u0026#34;: 78, \u0026#34;end_offset\u0026#34;: 81, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 11 }, { \u0026#34;token\u0026#34;: \u0026#34;operator\u0026#39;s\u0026#34;, \u0026#34;start_offset\u0026#34;: 82, \u0026#34;end_offset\u0026#34;: 92, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;APOSTROPHE\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 12 }, { \u0026#34;token\u0026#34;: \u0026#34;phone\u0026#34;, \u0026#34;start_offset\u0026#34;: 93, \u0026#34;end_offset\u0026#34;: 98, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 13 }, { \u0026#34;token\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;start_offset\u0026#34;: 99, \u0026#34;end_offset\u0026#34;: 105, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 14 }, { \u0026#34;token\u0026#34;: \u0026#34;1-800-555-1234\u0026#34;, \u0026#34;start_offset\u0026#34;: 106, \u0026#34;end_offset\u0026#34;: 120, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 15 }, { \u0026#34;token\u0026#34;: \u0026#34;P.S.\u0026#34;, \u0026#34;start_offset\u0026#34;: 122, \u0026#34;end_offset\u0026#34;: 126, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ACRONYM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 16 }, { \u0026#34;token\u0026#34;: \u0026#34;你\u0026#34;, \u0026#34;start_offset\u0026#34;: 127, \u0026#34;end_offset\u0026#34;: 128, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;CJ\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 17 }, { \u0026#34;token\u0026#34;: \u0026#34;好\u0026#34;, \u0026#34;start_offset\u0026#34;: 128, \u0026#34;end_offset\u0026#34;: 129, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;CJ\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 18 } ] } 词元类型 #  经典（classic）词元生成器产生的词元有以下类型：\n词元类型及描述 #     词元类型 描述     \u0026lt;ALPHANUM\u0026gt; 由字母、数字或两者组合而成的字母数字词元。   \u0026lt;APOSTROPHE\u0026gt; 包含撇号的词元，常用于所有格或缩写形式（例如 John's）。   \u0026lt;ACRONYM\u0026gt; 首字母缩写词或缩写，通常以句点结尾（例如 P.S. 或 U.S.A.）。   \u0026lt;COMPANY\u0026gt; 代表公司名称的词元（例如 X\u0026amp;Y）。如果这些词元没有自动生成，你可能需要进行自定义配置或使用过滤器。   \u0026lt;EMAIL\u0026gt; 与电子邮件地址匹配的词元，包含 @ 符号和域名（例如 support@widgets.co 或 info@example.com）。   \u0026lt;HOST\u0026gt; 与网站或主机名匹配的词元，通常包含 www. 或诸如 .com 之类的域名后缀（例如 www.example.com 或 example.org）。   \u0026lt;NUM\u0026gt; 仅包含数字或类似数字序列的词元（例如 1-800、12345 或 3.14）。   \u0026lt;CJ\u0026gt; 代表中文或日文字符的词元。   \u0026lt;ACRONYM_DEP\u0026gt; 已弃用的首字母缩写词处理方式（例如，旧版本中具有不同解析规则的首字母缩写词）。很少使用，主要用于与旧版词元生成器规则保持向后兼容。    ","subcategory":null,"summary":"","tags":null,"title":"经典分词器（Classic）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/classic/"},{"category":null,"content":"累积和聚合 #  cumulative_sum 聚合是一个父聚合，用于计算上一个聚合的存储分组的累积总和。\n累积和是给定序列的部分和的序列。例如，序列 {a, b, c, ...} 的累积和为 a、a+b、a+b+c 等。您可以使用累积总和来可视化字段随时间的变化率。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  cumulative_sum 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合存储分组的路径。请参阅存储分组路径 。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。    参考样例 #  以下示例创建间隔为一个月的日期直方图。sum 子聚合计算每个月所有字节的总和。最后，cumulative_sum 聚合计算每个月存储分组的累积字节数：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;no-of-bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;cumulative_bytes\u0026#34;: { \u0026#34;cumulative_sum\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;no-of-bytes\u0026#34; } } } } } } 返回内容\n{ \u0026#34;took\u0026#34;: 8, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;no-of-bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 }, \u0026#34;cumulative_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;no-of-bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 }, \u0026#34;cumulative_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 41907170 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;no-of-bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 }, \u0026#34;cumulative_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 79725689 } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"累积和聚合（Cumulative Sum）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/cumulative-sum/"},{"category":null,"content":"索引压缩 #  索引编码 #  索引编码决定索引的存储字段如何被压缩和存储在磁盘上。索引编码由静态的 index.codec 设置来控制，该设置指定压缩算法。这个设置会影响索引分片的大小和索引操作的性能。\nEasysearch 提供了多种基于索引编码的压缩方案，以降低索引的存储成本。\ndefault – 该编码使用LZ4算法和预设字典，优先考虑性能而非压缩比。与best_compression相比，它提供更快的索引和搜索操作，但可能导致更大的索引/分片大小。如果在索引设置中未提供编码，则默认使用LZ4作为压缩算法。\nbest_compression – 该编码底层使用zlib算法进行压缩。它能实现高压缩比，从而减小索引大小。然而，这可能会增加索引操作期间的额外CPU使用，并可能随后导致较高的索引和搜索延迟。\n从 Easysearch 1.1 开始，增加了基于 Zstandard 压缩算法的新编码方式。这种算法在压缩比和速度之间提供了良好的平衡。\nZSTD 与默认编解码器相比，该编解码器提供了与best_compression编解码器相当的压缩比，CPU使用合理，索引和搜索性能也有所提高。\nsource 复用 #  source_reuse： 启用 source_reuse 配置项能够去除 _source 字段中与 doc_values 或倒排索引重复的部分，从而有效减小索引总体大小，这个功能对日志类索引效果尤其明显。\nsource_reuse 支持对以下数据类型进行压缩：keyword，integer，long，short，boolean，float，half_float，double，geo_point，ip， 如果是 text 类型，需要默认启用 keyword 类型的 multi-field 映射。 以上类型必须启用 doc_values 映射（默认启用）才能压缩。\n使用限制 #    当索引里包含 nested 类型映射，或插件额外提供的数据类型时，不能启用 source_reuse，例如 knn 索引。\n  使用 source_reuse 压缩时，keyword 类型的字段最好不要设置 ignore_above 属性，设置过短的值可能会导致字段内容无法展示。\n  压缩效果对比 #  Easysearch 压缩效果对比如下\n 使用 Nginx 日志作为数据样本 就 Elasticsearch 6.4.3 和 Easysearch 1.1 进行对比 默认未开启压缩 开启 best_compression 压缩 开启 ZSTD 压缩 开启 ZSTD 加 _source 优化  Easysearch 1.1 版本 相比 Elasticsearch 索引整体大小降低了40%~50%。\nElasticsearch v6.4.3\n   index pri rep docs.count store.size pri.store.size     nginxt_default 5 1 1000000 413.1mb 413.1mb   nginx_best 1 1 1000000 316.2mb 316.2mb    Easysearch v1.1\n   index pri rep docs.count store.size pri.store.size     nginxt_default 1 1 1000000 321.6mb 321.6mb   nginx_best 1 1 1000000 262.8mb 262.8mb   nginx_zstd 1 1 1000000 205.6mb 205.6mb   nginx_reuse 1 1 1000000 157mb 157mb    如何启用 #  单个索引配置示例 #  创建并设置索引的 codec 为ZSTD：\nPUT test-index { \u0026#34;settings\u0026#34;: { \u0026#34;index.codec\u0026#34;: \u0026#34;ZSTD\u0026#34; } } 创建并设置索引的 codec 为ZSTD，并启用 source_reuse：\nPUT test-index { \u0026#34;settings\u0026#34;: { \u0026#34;index.codec\u0026#34;: \u0026#34;ZSTD\u0026#34;, \u0026#34;index.source_reuse\u0026#34;: \u0026#34;true\u0026#34; } } 索引模板配置示例 #  在 index_template 配置 ZSTD 和 source_reuse：\nPUT _index_template/daily_logs { \u0026#34;index_patterns\u0026#34;: [ \u0026#34;logs-2020-*\u0026#34; ], \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;index.codec\u0026#34;: \u0026#34;ZSTD\u0026#34;, \u0026#34;index.source_reuse\u0026#34;: true }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;field1\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"索引压缩","url":"/easysearch/main/docs/operations/data-management/index-compression/"},{"category":null,"content":"稀有词项聚合 #  rare_terms 聚合是一个分组聚合，用于识别数据集中的不常见词项。与 terms 聚合（查找最常见的词项）不同，rare_terms 聚合查找出现频率最低的词项。rare_terms 聚合适用于异常检测、长尾分析和异常报告等应用。\n相关指南（先读这些） #    聚合基础  聚合场景实践   可以使用 terms 通过按升序计数排序（ \u0026ldquo;order\u0026rdquo;: {\u0026ldquo;count\u0026rdquo;: \u0026ldquo;asc\u0026rdquo;} ）来搜索不常见的值。然而，我们强烈不建议这种做法，因为在涉及多个分片时，它可能导致不准确的结果。一个全局上不常见的词项可能不会在每个单个分片上显得不常见，或者可能完全不在某些分片返回的最不常见结果中。相反，一个在某个分片上出现频率较低的词项可能在另一个分片上很常见。在这两种情况下，分片级别的聚合可能会遗漏稀有词项，导致整体结果不正确。我们建议使用 rare_terms 聚合代替 terms 聚合，它专门设计用于更准确地处理这些情况。\n 近似结果 #  计算 rare_terms 聚合的精确结果需要编译所有分片上的值完整映射，这需要过多的运行时内存。因此， rare_terms 聚合结果被近似处理。\nrare_terms 计算中的大多数错误是假阴性或“遗漏”的值，这些值定义了聚合检测测试的灵敏度。 rare_terms 聚合使用 CuckooFilter 算法以实现适当的灵敏度和可接受的内存使用平衡。有关 CuckooFilter 算法的描述，请参阅这篇论文。\n控制灵敏度 #  rare_terms 聚合算法中的灵敏度误差被衡量为被遗漏的稀有值的比例，或 false negatives/target values 。例如，如果聚合在包含 5,000 个稀有值的数据集中遗漏了 100 个稀有值，灵敏度误差为 100/5000 = 0.02 ，或 2%。\n您可以调整 precision 参数在 rare_terms 聚合中来控制灵敏度和内存使用之间的权衡。\n这些因素也会影响灵敏度和内存的权衡：\n 唯一值的总数 数据集中稀有项的比例  以下指南可以帮助你决定使用哪个 precision 值。\n计算内存使用 #  运行时内存使用以绝对值描述，通常以 RAM 的 MB 为单位。\n内存使用随唯一项数量的增加而线性增长。线性扩展系数因 precision 参数而异，大致在每百万个唯一值 1.0 到 2.5 MB 之间。对于默认的 precision 设置为 0.001 时，内存成本约为每百万个唯一值 1.75 MB。\n管理灵敏度误差 #  敏感性误差随唯一值总数的增加而线性增长。有关估算唯一值数量的信息，请参阅基数聚合。\n在默认 precision 设置下，即使对于具有 1000 万至 2000 万个唯一值的集合，敏感性误差也极少超过 2.5%。对于 precision 设置为 0.00001 时，敏感性误差极少超过 0.6%。然而，非常低的稀有值绝对数量可能导致误差率出现较大波动（如果有两个稀有值，漏掉其中一个会导致 50% 的误差率）。\n与其他聚合的兼容性 #  rare_terms 聚合使用广度优先收集模式，与某些子聚合和嵌套配置中需要深度优先收集模式的聚合不兼容。\n参数说明 #  rare_terms 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必填 String 用于分析稀有词的字段。必须是数值类型或具有 keyword 映射的文本类型。   max_doc_count 可选 Integer 一个词被考虑为罕见词所需的最大文档数量。默认值是 1 。最大值是 100 。   precision 可选 Integer 控制用于识别罕见词的算法的精确度。较高的值提供更精确的结果，但会消耗更多内存。默认值是 0.001 。最小（最精确的允许值）是 0.00001 。   include 可选 Array/regex 结果中要包含的词。可以是正则表达式或值的数组。   exclude 可选 Array/regex 从结果中排除的词项。可以是正则表达式或值数组。   missing 可选 String 用于没有聚合字段值的文档的值。    参考样例 #  以下请求数据中仅出现一次的所有目的地机场代码：\nGET sample_data_flights/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rare_destination\u0026#34;: { \u0026#34;rare_terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;DestAirportID\u0026#34;, \u0026#34;max_doc_count\u0026#34;: 1 } } } } 返回内容显示，有两个机场符合仅在数据中出现一次的标准：\n{ \u0026#34;took\u0026#34;: 12, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;rare_destination\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;ADL\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: \u0026#34;BUF\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 } ] } } } 文档数量限制 #\n 使用 max_doc_count 参数指定 rare_terms 聚合可以返回的最大文档数量。 rare_terms 返回的词项数量没有限制，因此较大的 max_doc_count 值可能会返回非常大的结果集。因此， 100 是允许的最大 max_doc_count 值。\n以下请求数据中最多出现两次的所有目的地机场代码：\nGET sample_data_flights/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rare_destination\u0026#34;: { \u0026#34;rare_terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;DestAirportID\u0026#34;, \u0026#34;max_doc_count\u0026#34;: 2 } } } } 响应显示，有七个目的地机场代码符合出现次数在两次或更少文档中的标准，包括前一个示例中的两个：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;rare_destination\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;ADL\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: \u0026#34;BUF\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: \u0026#34;ABQ\u0026#34;, \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: \u0026#34;AUH\u0026#34;, \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: \u0026#34;BIL\u0026#34;, \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: \u0026#34;BWI\u0026#34;, \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: \u0026#34;MAD\u0026#34;, \u0026#34;doc_count\u0026#34;: 2 } ] } } } 过滤 (include 和 exclude) #  使用 include 和 exclude 参数来过滤 rare_terms 聚合返回的值。这两个参数可以在同一个聚合中包含。 exclude 过滤器具有优先级；任何被排除的值都会从结果中移除，无论它们是否被明确包含。\ninclude 和 exclude 的参数可以是正则表达式（regex），包括字符串字面量，或数组。混合正则表达式和数组参数会导致错误。例如，以下组合是不允许的：\n\u0026#34;rare_terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;DestAirportID\u0026#34;, \u0026#34;max_doc_count\u0026#34;: 2, \u0026#34;exclude\u0026#34;: [\u0026#34;ABQ\u0026#34;, \u0026#34;AUH\u0026#34;], \u0026#34;include\u0026#34;: \u0026#34;A.*\u0026#34; } 示例：过滤 #  以下示例修改了前面的示例，以包含所有以“A”开头的机场代码，但排除“ABQ”机场代码：\nGET sample_data_flights/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rare_destination\u0026#34;: { \u0026#34;rare_terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;DestAirportID\u0026#34;, \u0026#34;max_doc_count\u0026#34;: 2, \u0026#34;include\u0026#34;: \u0026#34;A.*\u0026#34;, \u0026#34;exclude\u0026#34;: \u0026#34;ABQ\u0026#34; } } } } 响应显示了符合过滤条件的那两个机场代码：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;rare_destination\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;ADL\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: \u0026#34;AUH\u0026#34;, \u0026#34;doc_count\u0026#34;: 2 } ] } } } 示例：使用数组输入进行过滤 #  以下示例数据中最多出现两次的 所有目的地机场代码，但指定了要排除的机场代码数组：\nGET sample_data_flights/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rare_destination\u0026#34;: { \u0026#34;rare_terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;DestAirportID\u0026#34;, \u0026#34;max_doc_count\u0026#34;: 2, \u0026#34;exclude\u0026#34;: [\u0026#34;ABQ\u0026#34;, \u0026#34;BIL\u0026#34;, \u0026#34;MAD\u0026#34;] } } } } 响应中排除了排除的机场代码：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;rare_destination\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;ADL\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: \u0026#34;BUF\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: \u0026#34;AUH\u0026#34;, \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: \u0026#34;BWI\u0026#34;, \u0026#34;doc_count\u0026#34;: 2 } ] } } } \n","subcategory":null,"summary":"","tags":null,"title":"稀有词项聚合（Rare Terms）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/rare-terms/"},{"category":null,"content":"移动平均聚合 #  moving_avg 聚合是一个父级管道聚合，它计算有序数据集中窗口（相邻子集）内指标的一系列平均值。\n要创建一个 moving_avg 聚合，您首先创建一个 histogram 或 date_histogram 聚合。然后，您可以选择在直方图聚合中嵌入一个指标聚合。最后，您在直方图中嵌入 moving_avg 聚合，并将 buckets_path 参数设置为要跟踪的嵌入指标。\n相关指南（先读这些） #    聚合基础  聚合场景实践  窗口的大小是窗口中连续数据值的数量。在每次迭代中，算法计算窗口中所有数据点的平均值，然后向前滑动一个数据值，排除上一个窗口的第一个值，并包含下一个窗口的第一个值。\n例如，给定数据 [1, 5, 8, 23, 34, 28, 7, 23, 20, 19] ，一个窗口大小为 5 的移动平均如下：\n(1 + 5 + 8 + 23 + 34) / 5 = 14.2 (5 + 8 + 23 + 34 + 28) / 5 = 19.6 (8 + 23 + 34 + 28 + 7) / 5 = 20 ... moving_avg 聚合通常应用于时间序列数据，以平滑噪声或短期波动，并识别趋势。指定较小的窗口大小以平滑小规模波动。指定较大的窗口大小以平滑高频波动或随机噪声，使低频趋势更加明显。\n参数说明 #  moving_avg 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。   window 可选 Numerical 窗口中包含的数据点数量。默认为 5 。   model 可选 String 要使用的加权移动平均模型。选项为 ewma 、 holt 、 holt_winters 、 linear 和 simple 。默认为 simple 。参见模型。   settings 可选 Object 调整窗口的参数。参见模型。   predict 可选 Numerical 要追加到结果末尾的预测值数量。默认为 0 。    参考样例 #  以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， moving_avg 聚合计算这些总和的每月移动平均值：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;my_date_histogram\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;moving_avg\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;sum_of_bytes\u0026#34; } } } } } } 该聚合从第二个分组开始返回 moving_avg 值。第一个分组没有移动平均值，因为没有足够的前置数据点来计算它：\n{ \u0026#34;took\u0026#34;: 5, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;my_date_histogram\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 20953585 } } ] } } } 示例：预测数据 #\n 你可以使用 moving_avg 聚合来预测未来的分组。\n以下示例将上一个示例的间隔减少到一周，并在响应的末尾附加了五个预测的一周分组：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;my_date_histogram\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;week\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;moving_avg\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;sum_of_bytes\u0026#34;, \u0026#34;predict\u0026#34;: 5 } } } } } } 返回内容包含五个预测。请注意，预测的分组的 doc_count 是 0 :\n{ \u0026#34;took\u0026#34;: 5, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;my_date_histogram\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-24T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1742774400000, \u0026#34;doc_count\u0026#34;: 249, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 1531493 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-31T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743379200000, \u0026#34;doc_count\u0026#34;: 1617, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9213161 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 1531493 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-07T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743984000000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9188671 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 5372327 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-14T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1744588800000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9244851 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 6644441.666666667 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-21T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745193600000, \u0026#34;doc_count\u0026#34;: 1609, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9061045 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 7294544 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-28T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745798400000, \u0026#34;doc_count\u0026#34;: 1554, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 8713507 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 7647844.2 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-05T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746403200000, \u0026#34;doc_count\u0026#34;: 1710, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9544718 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9084247 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-12T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1747008000000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9155820 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9150558.4 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-19T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1747612800000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9025078 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9143988.2 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-26T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1748217600000, \u0026#34;doc_count\u0026#34;: 895, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 5047345 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9100033.6 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-06-02T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1748822400000, \u0026#34;doc_count\u0026#34;: 0, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 8297293.6 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-06-09T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1749427200000, \u0026#34;doc_count\u0026#34;: 0, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 8297293.6 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-06-16T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1750032000000, \u0026#34;doc_count\u0026#34;: 0, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 8297293.6 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-06-23T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1750636800000, \u0026#34;doc_count\u0026#34;: 0, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 8297293.6 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-06-30T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1751241600000, \u0026#34;doc_count\u0026#34;: 0, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 8297293.6 } } ] } } } 模型 #  moving_avg 聚合支持五种模型，这些模型在如何对移动窗口中的值进行加权方面有所不同。\n使用 model 参数来指定要使用的模型。\n   模型名称 模型关键词 加权方式     简单 simple 窗口中所有值的未加权平均值。   线性 linear 使用线性权重衰减，更重视近期值。   指数加权移动平均 ewma 使用指数递减权重，更重视近期值。   Holt holt 使用第二个指数项来平滑长期趋势。   Holt-Winters holt_winters 使用第三个指数项来平滑周期（季节性）效应。    可以使用 settings 对象来设置模型的属性。下表显示了每个模型的可用设置。\n   模型 参数 允许的值 默认值 描述     simple 无 Numeric array 无 窗口中所有值的算术平均值。   linear 无 Numeric array 无 窗口中所有值的加权平均值，较新的值权重更大。   ewma alpha [0, 1] 0.3 衰减参数。更高的值会给最近的数据点赋予更大的权重。   holt alpha [0, 1] 0.3 用于级别组件的衰减参数。    beta [0, 1] 0.1 趋势成分的衰减参数。   holt_winters alpha [0, 1] 0.3 水平成分的衰减参数。    beta [0, 1] 0.3 趋势成分的衰减参数。    gamma [0, 1] 0.3 季节成分的衰减参数。    type add, mult add 定义季节性建模方式：加性或乘性。    period Integer 1 构成周期的分组的数量。    pad Boolean true 是否为 mult 类型模型中的 0 值添加一个小的偏移量，以避免除以零的错误。    示例：Holt 模型 #  holt 模型使用 alpha 和 beta 参数控制的指数衰减计算权重。\n以下请求使用 window 大小为 6 、 alpha 值为 0.4 、 beta 值为 0.2 的 Holt 模型计算总每周字节数的移动平均值：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;my_date_histogram\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;week\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;moving_avg\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;sum_of_bytes\u0026#34;, \u0026#34;window\u0026#34;: 6, \u0026#34;model\u0026#34;: \u0026#34;holt\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;alpha\u0026#34;: 0.4, \u0026#34;beta\u0026#34;: 0.2 } } } } } } } 移动平均数从第二个分组开始：\n{ \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;my_date_histogram\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-24T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1742774400000, \u0026#34;doc_count\u0026#34;: 249, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 1531493 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-31T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743379200000, \u0026#34;doc_count\u0026#34;: 1617, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9213161 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 1531493 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-07T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743984000000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9188671 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 4604160.2 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-14T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1744588800000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9244851 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 6806684.584000001 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-21T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745193600000, \u0026#34;doc_count\u0026#34;: 1609, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9061045 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 8341230.127680001 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-28T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745798400000, \u0026#34;doc_count\u0026#34;: 1554, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 8713507 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9260724.7236736 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-05T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746403200000, \u0026#34;doc_count\u0026#34;: 1710, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9544718 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9657431.903375873 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-12T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1747008000000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9155820 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9173999.55240704 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-19T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1747612800000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9025078 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9172040.511275519 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-26T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1748217600000, \u0026#34;doc_count\u0026#34;: 895, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 5047345 }, \u0026#34;moving_avg_of_sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9108804.964619776 } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"移动平均聚合（Moving Avg）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/moving-avg/"},{"category":null,"content":"移动函数聚合 #  moving_fn 聚合是一个父级管道聚合，它在滑动窗口上执行脚本。滑动窗口在从父级 histogram 或 date_histogram 聚合中提取的一系列值上移动。窗口一次向右移动一个分组；moving_fn 每次窗口移动时都会运行脚本。\n使用 moving_fn 聚合在滑动窗口内的数据上执行任何数值计算。你可以使用 moving_fn 用于以下目的：\n 趋势分析 异常值检测 自定义时间序列分析 自定义平滑算法 数字信号处理 (DSP)  相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  moving_fn 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   script 必需 String 或 Object 为每个数据窗口计算值的脚本。可以是内联脚本、存储脚本或脚本文件。该脚本可以访问在 buckets_path 参数中定义的变量名。   window 必需 Integer 滑动窗口中的分组的数量。必须是正整数。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。   shift 可选 Integer 窗口要移动的分组的数量。可以是正数（向未来的分组右移）或负数（向过去的分组左移）。默认是 0 ，将窗口立即放置在当前分组的左侧。参见移动窗口。    移动函数的工作原理 #  moving_fn 聚合操作在有序分组序列上的滑动窗口上。从父聚合中的第一个分组开始， moving_fn 执行以下操作：\n 收集由 window 和 shift 参数指定的分组中的子序列（窗口）的值。 将这些值作为数组传递给由 script 指定的函数。 使用 script 从数组中计算出一个值。 将此值作为当前分组的结果返回。 向前移动一个分组并重复此过程。   “过去”和“未来”值暗示时间序列数据，这是移动窗口函数最常见的用例。更一般地说，它们分别指任何有序数据序列中的先前值和即将到来的值。\n moving_fn 应用的脚本可以是一个预定义函数或自定义脚本。分组值通过 values 数组提供给脚本。脚本返回一个双精度值作为结果。结果值 NaN 和 +/- Inf 是允许的，但 null 是不允许的。\n窗口大小 #  window 参数指定定义窗口大小的分组的数量。\n传递给 script 函数的数组是零索引的。其值在脚本中通过 values[0] 到 values[n] 访问，其中 n = values.length - 1 。\n移动窗口 #  shift 参数控制移动窗口相对于当前分组的位置。根据您的分析需求设置 shift ，需要历史背景、当前数据还是未来预测。默认值是 0 ，仅显示过去值（不包括当前分组）。\nshift 的常用值如下：\n 0:仅过去值。不包括当前值。 1:过去值，包括当前值。 window/2:将窗口围绕当前值居中。 window:未来的值，包括当前值。  当窗口扩展到序列开头或结尾的可用数据之外时， window 会自动缩小，仅使用可用点：\n预定义函数 #  moving_fn 聚合支持多种预定义函数，可用于替代自定义脚本。这些函数可通过 MovingFunctions 上下文访问。例如，你可以通过 MovingFunctions.max(values) 访问 max 函数。\n下表描述了预定义函数。\n   函数 模型关键词 描述     最大值 max 窗口中的最大值。   最小值 min 窗口中的最小值。   求和 sum 窗口中值的总和。   无权平均 unweightedAvg 窗口内所有值的未加权平均值，等于 sum / window 。   线性加权平均 linearWeightedAvg 使用线性衰减权重的加权平均，更重视近期值。   指数加权移动平均 ewma 使用指数衰减权重的加权平均，更重视近期值。   Holt holt 使用第二个指数项的加权平均，用于平滑长期趋势。   Holt-Winters holt_wimnters\t 使用第三个指数项的加权平均，用于平滑周期性（季节性）效应。   标准差 stdDev 窗口中值的总和。    所有预定义函数都以 values 数组作为其第一个参数。对于需要额外参数的函数，按顺序在 values 后传递这些参数。例如，通过将 script 值设置为 MovingFunctions.stdDev(values, MovingFunctions.unweightedAvg(values)) 来调用 stdDev 函数。\n下表显示了每个模型所需的设置。\n   函数 参数 参数类型 默认值 描述     max 无 Numeric array 无 窗口的最大值。   min 无 Numeric array 无 窗口的最小值。   sum 无 Numeric array 无 窗口中所有值的总和。   unweightedAvg 无 Numeric array 无 窗口中所有值的算术平均值。   linearWeightedAvg 无 Numeric array 无 窗口中所有值的加权平均值，较新的值权重更大。   ewma alpha [0, 1] 0.3 衰减参数。更高的值会给最近的数据点赋予更大的权重。   holt alpha [0, 1] 0.3 用于级别组件的衰减参数。    beta [0, 1] 0.1 趋势成分的衰减参数。   holt_winters alpha [0, 1] 0.3 用于级别组件的衰减参数。    beta [0, 1] 0.3 趋势成分的衰减参数。    gamma [0, 1] 0.3 季节成分的衰减参数。    type add, mult add 定义季节性如何建模：加性或乘性。    period Integer 1 构成周期的分组数。    pad Boolean true 是否为 0 类型的模型对 mult 值添加一个小的偏移量以避免除以零的错误。   stdDev avg double 无 窗口的标准差。要计算有意义的标准差，请使用滑动窗口数组的平均值，通常为 MovingFunctions.unweightedAvg(values) 。     预定义函数不支持缺少参数的函数签名。因此，即使使用默认值，您也必须提供额外的参数。\n 示例：预定义函数 #  以下示例创建一个日期直方图，间隔为一周。 sum 子聚合计算每周所有记录的字节总和。最后， moving_fn 聚合使用 window 大小为 5 、默认 shift 为 0 ，以及无权重的平均值来计算字节总和的标准差：\nPOST sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;my_date_histo\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;week\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;the_sum\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;moving_fn\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;the_sum\u0026#34;, \u0026#34;window\u0026#34;: 5, \u0026#34;script\u0026#34;: \u0026#34;MovingFunctions.stdDev(values, MovingFunctions.unweightedAvg(values))\u0026#34; } } } } } } 返回内容显示了移动窗口的标准差，从第二个分组开始为零值。对于空窗口或只包含无效值（ null 或 NaN ）的窗口， stdDev 函数返回 0 。\n{ \u0026#34;took\u0026#34;: 15, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;my_date_histo\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-24T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1742774400000, \u0026#34;doc_count\u0026#34;: 249, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 1531493 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: null } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-31T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743379200000, \u0026#34;doc_count\u0026#34;: 1617, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9213161 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 0 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-07T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743984000000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9188671 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 3840834 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-14T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1744588800000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9244851 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 3615414.498228507 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-21T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745193600000, \u0026#34;doc_count\u0026#34;: 1609, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9061045 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 3327358.65618917 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-28T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745798400000, \u0026#34;doc_count\u0026#34;: 1554, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 8713507 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 3058812.9440705855 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-05T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746403200000, \u0026#34;doc_count\u0026#34;: 1710, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9544718 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 195603.33146038183 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-12T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1747008000000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9155820 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 270085.92336040025 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-19T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1747612800000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9025078 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 269477.75659701484 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-26T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1748217600000, \u0026#34;doc_count\u0026#34;: 895, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 5047345 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 267356.5422566652 } } ] } } } 自定义脚本 #  你可以提供一个任意的自定义脚本来计算 moving_fn 结果。自定义脚本使用 Painless 脚本语言。\n示例：使用自定义脚本 #  以下示例创建一个日期直方图，间隔为一周。 sum 子聚合计算每周所有应税收入的总额。 moving_fn 脚本返回当前值之前的两个值中较大的那个值，如果两个值不可用，则返回 NaN 。\nPOST sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;my_date_histo\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;order_date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;week\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;the_sum\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;moving_fn\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;the_sum\u0026#34;, \u0026#34;window\u0026#34;: 2, \u0026#34;script\u0026#34;: \u0026#34;return (values.length \u0026lt; 2 ? Double.NaN : (values[0]\u0026gt;values[1] ? values[0] : values[1]))\u0026#34; } } } } } } 该示例返回从第三个分组开始的计算结果，其中存在足够的前置数据来执行计算：\n{ \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;my_date_histo\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-24T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1742774400000, \u0026#34;doc_count\u0026#34;: 582, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 41455.5390625 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: null } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-31T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743379200000, \u0026#34;doc_count\u0026#34;: 1048, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 79448.60546875 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: null } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-07T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743984000000, \u0026#34;doc_count\u0026#34;: 1048, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 78208.4296875 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 79448.60546875 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-14T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1744588800000, \u0026#34;doc_count\u0026#34;: 1073, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 81277.296875 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 79448.60546875 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-21T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745193600000, \u0026#34;doc_count\u0026#34;: 924, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 70494.2578125 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 81277.296875 } } ] } } } 示例：移动平均聚合 #  holt 模型是一种移动平均，它使用由 alpha 和 beta 参数控制的指数衰减权重。以下示例创建一个日期直方图，间隔为一周。 sum 子聚合计算每周所有字节的总和。最后， moving_fn 聚合使用 Holt 模型计算字节总和的加权平均值，模型大小为 window ，默认值为 shift ， alpha 值为 0.3 ， beta 值为 0.1 ：\nPOST sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;my_date_histogram\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;week\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;the_sum\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;moving_fn\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;the_sum\u0026#34;, \u0026#34;window\u0026#34;: 6, \u0026#34;script\u0026#34;: \u0026#34;MovingFunctions.holt(values, 0.3, 0.1)\u0026#34; } } } } } } 聚合返回从第二个分组开始的移动 holt 平均值：\n{ \u0026#34;took\u0026#34;: 16, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;my_date_histogram\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-24T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1742774400000, \u0026#34;doc_count\u0026#34;: 249, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 1531493 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: null } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-31T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743379200000, \u0026#34;doc_count\u0026#34;: 1617, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9213161 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 1531493 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-07T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743984000000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9188671 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 3835993.3999999994 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-14T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1744588800000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9244851 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 5603111.707999999 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-21T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745193600000, \u0026#34;doc_count\u0026#34;: 1609, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9061045 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 6964515.302359998 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-28T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745798400000, \u0026#34;doc_count\u0026#34;: 1554, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 8713507 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 7930766.089341199 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-05T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746403200000, \u0026#34;doc_count\u0026#34;: 1710, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9544718 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 8536788.607547803 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-12T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1747008000000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9155820 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 9172269.837272028 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-19T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1747612800000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 9025078 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 9166173.88436614 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-26T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1748217600000, \u0026#34;doc_count\u0026#34;: 895, \u0026#34;the_sum\u0026#34;: { \u0026#34;value\u0026#34;: 5047345 }, \u0026#34;the_movavg\u0026#34;: { \u0026#34;value\u0026#34;: 9123157.830417283 } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"移动函数聚合（Moving Function）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/moving-function/"},{"category":null,"content":"矩阵统计聚合 #  matrix_stats 聚合是一个多值指标聚合，以矩阵形式为两个或多个字段生成协方差统计。\n 注意：matrix_stats 聚合不支持脚本。\n 相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  matrix_stats 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 用于计算矩阵统计的一组字段。   missing 可选 Object 用于替代缺失值的值。默认情况下，会忽略缺失值。参见缺失值。   mode 可选 String 用作多值或数组字段样本的值。允许的值是 avg 、 min 、 max 、 sum 和 median 。默认是 avg 。    参考样例 #  以下示例返回数据中 taxful_total_price 和 products.base_price 字段的统计信息：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;matrix_stats_taxful_total_price\u0026#34;: { \u0026#34;matrix_stats\u0026#34;: { \u0026#34;fields\u0026#34;: [\u0026#34;taxful_total_price\u0026#34;, \u0026#34;products.base_price\u0026#34;] } } } } 返回内容包含聚合结果：\n{ \u0026#34;took\u0026#34;: 250, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;matrix_stats_taxful_total_price\u0026#34;: { \u0026#34;doc_count\u0026#34;: 4675, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;products.base_price\u0026#34;, \u0026#34;count\u0026#34;: 4675, \u0026#34;mean\u0026#34;: 34.99423943014724, \u0026#34;variance\u0026#34;: 360.5035285833702, \u0026#34;skewness\u0026#34;: 5.530161335032689, \u0026#34;kurtosis\u0026#34;: 131.1630632404217, \u0026#34;covariance\u0026#34;: { \u0026#34;products.base_price\u0026#34;: 360.5035285833702, \u0026#34;taxful_total_price\u0026#34;: 846.6489362233169 }, \u0026#34;correlation\u0026#34;: { \u0026#34;products.base_price\u0026#34;: 1, \u0026#34;taxful_total_price\u0026#34;: 0.8444765264325269 } }, { \u0026#34;name\u0026#34;: \u0026#34;taxful_total_price\u0026#34;, \u0026#34;count\u0026#34;: 4675, \u0026#34;mean\u0026#34;: 75.05542864304839, \u0026#34;variance\u0026#34;: 2788.1879749835425, \u0026#34;skewness\u0026#34;: 15.812149139923994, \u0026#34;kurtosis\u0026#34;: 619.1235507385886, \u0026#34;covariance\u0026#34;: { \u0026#34;products.base_price\u0026#34;: 846.6489362233169, \u0026#34;taxful_total_price\u0026#34;: 2788.1879749835425 }, \u0026#34;correlation\u0026#34;: { \u0026#34;products.base_price\u0026#34;: 0.8444765264325269, \u0026#34;taxful_total_price\u0026#34;: 1 } } ] } } } 下表描述了返回内容的字段。\n   统计 描述     count 用于聚合的文档数量。   mean 从样本计算得到的字段平均值。   variance 均值偏差的平方，衡量数据分布的离散程度。   skewness 衡量分布相对于均值的不对称性指标。   kurtosis 衡量分布尾部重量的指标。随着尾部变轻，峰度减小。通过评估峰度和偏度来判断一个总体是否可能呈正态分布。   covariance 衡量两个字段之间联合变差的指标。正值表示它们的值朝同一方向变动。   correlation 标准化协方差，用于衡量两个字段之间关系强度的指标。可能的值范围从-1 到 1（包含两端），表示完全负相关到完全正相关。值为 0 表示变量之间没有可识别的关系。    缺省值 #  要定义如何处理缺失值，请使用 missing 参数。默认情况下，会忽略缺失值。\n例如，创建一个索引，其中文档 1 缺少 gpa 和 class_grades 字段：\nPOST _bulk { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Jonathan Powers\u0026#34;, \u0026#34;gpa\u0026#34;: 3.85, \u0026#34;class_grades\u0026#34;: [3.0, 3.9, 4.0] } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Jane Doe\u0026#34;, \u0026#34;gpa\u0026#34;: 3.52, \u0026#34;class_grades\u0026#34;: [3.2, 2.1, 3.8] } 首先，运行不带 missing 参数的 matrix_stats 聚合：\nGET students/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;matrix_stats_taxful_total_price\u0026#34;: { \u0026#34;matrix_stats\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;gpa\u0026#34;, \u0026#34;class_grades\u0026#34; ], \u0026#34;mode\u0026#34;: \u0026#34;avg\u0026#34; } } } } 在计算矩阵统计时忽略缺失值：\n{ \u0026#34;took\u0026#34;: 5, \u0026#34;timed_out\u0026#34;: false, \u0026#34;terminated_early\u0026#34;: true, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;matrix_stats_taxful_total_price\u0026#34;: { \u0026#34;doc_count\u0026#34;: 2, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;gpa\u0026#34;, \u0026#34;count\u0026#34;: 2, \u0026#34;mean\u0026#34;: 3.684999942779541, \u0026#34;variance\u0026#34;: 0.05444997482300096, \u0026#34;skewness\u0026#34;: 0, \u0026#34;kurtosis\u0026#34;: 1, \u0026#34;covariance\u0026#34;: { \u0026#34;gpa\u0026#34;: 0.05444997482300096, \u0026#34;class_grades\u0026#34;: 0.09899998760223136 }, \u0026#34;correlation\u0026#34;: { \u0026#34;gpa\u0026#34;: 1, \u0026#34;class_grades\u0026#34;: 0.9999999999999991 } }, { \u0026#34;name\u0026#34;: \u0026#34;class_grades\u0026#34;, \u0026#34;count\u0026#34;: 2, \u0026#34;mean\u0026#34;: 3.333333333333333, \u0026#34;variance\u0026#34;: 0.1800000381469746, \u0026#34;skewness\u0026#34;: 0, \u0026#34;kurtosis\u0026#34;: 1, \u0026#34;covariance\u0026#34;: { \u0026#34;gpa\u0026#34;: 0.09899998760223136, \u0026#34;class_grades\u0026#34;: 0.1800000381469746 }, \u0026#34;correlation\u0026#34;: { \u0026#34;gpa\u0026#34;: 0.9999999999999991, \u0026#34;class_grades\u0026#34;: 1 } } ] } } } 要设置缺失字段为 0 ，请将 missing 参数作为键值映射提供。尽管 class_grades 是数组字段，但 matrix_stats 聚合会将多值数字字段展平为每个文档的平均值，因此您必须将单个数字作为缺失值提供：\nGET students/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;matrix_stats_taxful_total_price\u0026#34;: { \u0026#34;matrix_stats\u0026#34;: { \u0026#34;fields\u0026#34;: [\u0026#34;gpa\u0026#34;, \u0026#34;class_grades\u0026#34;], \u0026#34;mode\u0026#34;: \u0026#34;avg\u0026#34;, \u0026#34;missing\u0026#34;: { \u0026#34;gpa\u0026#34;: 0, \u0026#34;class_grades\u0026#34;: 0 } } } } } 在计算矩阵统计时，会用 0 替换任何缺失的 gpa 或 class_grades 值：\n{ \u0026#34;took\u0026#34;: 23, \u0026#34;timed_out\u0026#34;: false, \u0026#34;terminated_early\u0026#34;: true, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;matrix_stats_taxful_total_price\u0026#34;: { \u0026#34;doc_count\u0026#34;: 3, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;gpa\u0026#34;, \u0026#34;count\u0026#34;: 3, \u0026#34;mean\u0026#34;: 2.456666628519694, \u0026#34;variance\u0026#34;: 4.55363318017324, \u0026#34;skewness\u0026#34;: -0.688130006360758, \u0026#34;kurtosis\u0026#34;: 1.5, \u0026#34;covariance\u0026#34;: { \u0026#34;gpa\u0026#34;: 4.55363318017324, \u0026#34;class_grades\u0026#34;: 4.143944374667273 }, \u0026#34;correlation\u0026#34;: { \u0026#34;gpa\u0026#34;: 1, \u0026#34;class_grades\u0026#34;: 0.9970184390038257 } }, { \u0026#34;name\u0026#34;: \u0026#34;class_grades\u0026#34;, \u0026#34;count\u0026#34;: 3, \u0026#34;mean\u0026#34;: 2.2222222222222223, \u0026#34;variance\u0026#34;: 3.793703722777191, \u0026#34;skewness\u0026#34;: -0.6323693521730989, \u0026#34;kurtosis\u0026#34;: 1.5000000000000002, \u0026#34;covariance\u0026#34;: { \u0026#34;gpa\u0026#34;: 4.143944374667273, \u0026#34;class_grades\u0026#34;: 3.793703722777191 }, \u0026#34;correlation\u0026#34;: { \u0026#34;gpa\u0026#34;: 0.9970184390038257, \u0026#34;class_grades\u0026#34;: 1 } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"矩阵统计聚合（Matrix Stats）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/matrix-stats/"},{"category":null,"content":"直方图聚合 #  histogram 聚合根据指定的间隔对文档进行分组。\n使用 histogram 聚合，您可以非常轻松地可视化给定范围内文档中值的分布。\n相关指南（先读这些） #    聚合基础  聚合场景实践  以下示例将 number_of_bytes 字段按 10,000 个间隔进行分组：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;number_of_bytes\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;interval\u0026#34;: 10000 } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;number_of_bytes\u0026#34; : { \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : 0.0, \u0026#34;doc_count\u0026#34; : 13372 }, { \u0026#34;key\u0026#34; : 10000.0, \u0026#34;doc_count\u0026#34; : 702 } ] } } 参数说明 #  histogram 聚合支持以下参数。\n   参数 必需/可选 数据类型 描述     interval\t 必填 Numeric 构造每个分组所使用的字段值宽度。   min_doc_count 可选 Integer 桶中至少需要包含的文档数量才会出现在结果中。默认为 0，设为 1 可隐藏空桶。   offset 可选 Numeric 桶边界的偏移量。例如 interval=10，offset=5 时桶为 [5,15)、[15,25)\u0026hellip;。默认为 0。   extended_bounds 可选 Object 包含 min 和 max，强制直方图覆盖指定范围（即使没有数据）。不会过滤文档。   hard_bounds 可选 Object 包含 min 和 max，限制直方图的范围，超出范围的桶会被丢弃。   order 可选 Object 桶的排序方式。默认按 key 升序。   keyed 可选 Boolean 若为 true，以键值对而非数组形式返回结果。默认为 false。    显示空桶 #  设置 min_doc_count 为 0 会返回所有桶（包括没有文档的桶），配合 extended_bounds 可以强制覆盖完整范围：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;bytes_distribution\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;interval\u0026#34;: 5000, \u0026#34;min_doc_count\u0026#34;: 0, \u0026#34;extended_bounds\u0026#34;: { \u0026#34;min\u0026#34;: 0, \u0026#34;max\u0026#34;: 20000 } } } } } 嵌套子聚合 #  直方图聚合的每个桶内都可以嵌套子聚合。例如，查看每个字节范围桶中的平均响应时间：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;bytes_ranges\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;interval\u0026#34;: 5000 }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_response\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;response_time\u0026#34; } } } } } } ","subcategory":null,"summary":"","tags":null,"title":"直方图聚合（Histogram）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/histogram/"},{"category":null,"content":"百分位桶聚合 #  percentiles_bucket 聚合是一个同级聚合，用于计算分位数的位置。\npercentiles_bucket 聚合精确计算分位数，不使用近似或插值。每个分位数都返回为目标分位数小于或等于的最近值。\npercentiles_bucket 聚合需要将整个值列表临时保存在内存中，即使对于大型数据集也是如此。相比之下，percentiles 指标聚合使用更少的内存，但会近似百分比。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  percentiles_bucket聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。   percents 可选 List 一个包含任意数量数值百分比值的列表，这些值将被包含在输出中。有效值为 0.0 到 100.0（含）。默认为 [1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0] 。   keyed 可选 Boolean 是否将输出格式化为字典，而不是键值对对象数组。默认为 true （以键值对格式化输出）。    参考样例 #  以下示例创建一个以一周为间隔的日期直方图。 sum 子聚合为每周汇总 taxful_total_price 。最后， percentiles_bucket 聚合计算这些汇总的每周百分位数值：\nPOST sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;weekly_sales\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;order_date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;week\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;total_price\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } } } }, \u0026#34;percentiles_monthly_sales\u0026#34;: { \u0026#34;percentiles_bucket\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;weekly_sales\u0026gt;total_price\u0026#34; } } } } 聚合返回每周价格总计的默认百分位数值：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;weekly_sales\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-24T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1742774400000, \u0026#34;doc_count\u0026#34;: 582, \u0026#34;total_price\u0026#34;: { \u0026#34;value\u0026#34;: 41455.5390625 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-31T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743379200000, \u0026#34;doc_count\u0026#34;: 1048, \u0026#34;total_price\u0026#34;: { \u0026#34;value\u0026#34;: 79448.60546875 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-07T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743984000000, \u0026#34;doc_count\u0026#34;: 1048, \u0026#34;total_price\u0026#34;: { \u0026#34;value\u0026#34;: 78208.4296875 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-14T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1744588800000, \u0026#34;doc_count\u0026#34;: 1073, \u0026#34;total_price\u0026#34;: { \u0026#34;value\u0026#34;: 81277.296875 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-21T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745193600000, \u0026#34;doc_count\u0026#34;: 924, \u0026#34;total_price\u0026#34;: { \u0026#34;value\u0026#34;: 70494.2578125 } } ] }, \u0026#34;percentiles_monthly_sales\u0026#34;: { \u0026#34;values\u0026#34;: { \u0026#34;1.0\u0026#34;: 41455.5390625, \u0026#34;5.0\u0026#34;: 41455.5390625, \u0026#34;25.0\u0026#34;: 70494.2578125, \u0026#34;50.0\u0026#34;: 78208.4296875, \u0026#34;75.0\u0026#34;: 79448.60546875, \u0026#34;95.0\u0026#34;: 81277.296875, \u0026#34;99.0\u0026#34;: 81277.296875 } } } } 示例：选项修改 #  下一个示例使用与上一个示例相同的数据计算百分位数，但有以下不同：\n percents 参数指定仅计算第 25、50 和 75 个百分位数。 使用 format 参数追加字符串格式输出。 通过将 keyed 参数设置为 false ，结果以键值对对象形式显示（追加字符串值）。  POST sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;weekly_sales\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;order_date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;week\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;total_price\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } } } }, \u0026#34;percentiles_monthly_sales\u0026#34;: { \u0026#34;percentiles_bucket\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;weekly_sales\u0026gt;total_price\u0026#34;, \u0026#34;percents\u0026#34;: [25.0, 50.0, 75.0], \u0026#34;format\u0026#34;: \u0026#34;$#,###.00\u0026#34;, \u0026#34;keyed\u0026#34;: false } } } } 选项修改的输出：\n{ \u0026#34;took\u0026#34;: 5, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;weekly_sales\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-24T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1742774400000, \u0026#34;doc_count\u0026#34;: 582, \u0026#34;total_price\u0026#34;: { \u0026#34;value\u0026#34;: 41455.5390625 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-31T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743379200000, \u0026#34;doc_count\u0026#34;: 1048, \u0026#34;total_price\u0026#34;: { \u0026#34;value\u0026#34;: 79448.60546875 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-07T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743984000000, \u0026#34;doc_count\u0026#34;: 1048, \u0026#34;total_price\u0026#34;: { \u0026#34;value\u0026#34;: 78208.4296875 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-14T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1744588800000, \u0026#34;doc_count\u0026#34;: 1073, \u0026#34;total_price\u0026#34;: { \u0026#34;value\u0026#34;: 81277.296875 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-21T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745193600000, \u0026#34;doc_count\u0026#34;: 924, \u0026#34;total_price\u0026#34;: { \u0026#34;value\u0026#34;: 70494.2578125 } } ] }, \u0026#34;percentiles_monthly_sales\u0026#34;: { \u0026#34;values\u0026#34;: [ { \u0026#34;key\u0026#34;: 25, \u0026#34;value\u0026#34;: 70494.2578125, \u0026#34;25.0_as_string\u0026#34;: \u0026#34;$70,494.26\u0026#34; }, { \u0026#34;key\u0026#34;: 50, \u0026#34;value\u0026#34;: 78208.4296875, \u0026#34;50.0_as_string\u0026#34;: \u0026#34;$78,208.43\u0026#34; }, { \u0026#34;key\u0026#34;: 75, \u0026#34;value\u0026#34;: 79448.60546875, \u0026#34;75.0_as_string\u0026#34;: \u0026#34;$79,448.61\u0026#34; } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"百分位桶聚合（Percentiles Bucket）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/percentiles-bucket/"},{"category":null,"content":"百分位数聚合 #  percentiles 聚合估计数值字段在给定百分位处的值。这对于理解分布边界很有用。\n例如，load_time 的 95th 百分位 = 120ms 表示 95% 的值小于或等于 120 毫秒。\n与 cardinality 指标类似，percentiles 指标也是近似的。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  percentiles 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 用于计算百分位数的数值字段。   percents 可选 Array of doubles 返回百分位数列表。默认为 [1, 5, 25, 50, 75, 95, 99] 。   keyed 可选 Boolean 如果设置为 false ，则将结果作为数组返回。否则将结果作为 JSON 对象返回。默认值为 true 。   tdigest.compression 可选 Double 控制 tdigest 算法的准确性和内存使用。参见使用 tdigest 进行精度调整。   hdr.number_of_significant_value_digits 可选 Integer HDR 直方图的精度设置。参见 HDR 直方图。   missing 可选 Numeric 当文档中目标字段缺失时使用的默认值。   script 可选 Object 用于计算自定义值而不是使用字段的脚本。支持内联和存储脚本。    参考样例 #  首先，创建一个索引：\nPUT /latency_data { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;load_time\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } 添加示例数值以说明百分位数计算：\nPOST /latency_data/_bulk { \u0026#34;index\u0026#34;: {} } { \u0026#34;load_time\u0026#34;: 20 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;load_time\u0026#34;: 40 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;load_time\u0026#34;: 60 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;load_time\u0026#34;: 80 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;load_time\u0026#34;: 100 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;load_time\u0026#34;: 120 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;load_time\u0026#34;: 140 } 百分位数聚合 #  以下示例计算 load_time 字段的默认百分位数集：\nGET /latency_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;load_time_percentiles\u0026#34;: { \u0026#34;percentiles\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;load_time\u0026#34; } } } } 默认情况下，会返回第 1、5、25、50、75、95 和 99 个百分位数：\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;load_time_percentiles\u0026#34;: { \u0026#34;values\u0026#34;: { \u0026#34;1.0\u0026#34;: 20, \u0026#34;5.0\u0026#34;: 20, \u0026#34;25.0\u0026#34;: 40, \u0026#34;50.0\u0026#34;: 80, \u0026#34;75.0\u0026#34;: 120, \u0026#34;95.0\u0026#34;: 140, \u0026#34;99.0\u0026#34;: 140 } } } } 自定义分位数 #  您可以使用 percents 数组指定确切的百分位数：\nGET /latency_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;load_time_percentiles\u0026#34;: { \u0026#34;percentiles\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;load_time\u0026#34;, \u0026#34;percents\u0026#34;: [50, 90, 99] } } } } 返回内容仅包含所请求的三个百分位聚合：\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;load_time_percentiles\u0026#34;: { \u0026#34;values\u0026#34;: { \u0026#34;50.0\u0026#34;: 80, \u0026#34;90.0\u0026#34;: 140, \u0026#34;99.0\u0026#34;: 140 } } } } 键值响应 #  可以通过将 keyed 参数设置为 false 将返回的聚合格式从 JSON 对象更改为键值对列表：\nGET /latency_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;load_time_percentiles\u0026#34;: { \u0026#34;percentiles\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;load_time\u0026#34;, \u0026#34;keyed\u0026#34;: false } } } } 响应以值数组的形式提供百分位数：\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;load_time_percentiles\u0026#34;: { \u0026#34;values\u0026#34;: [ { \u0026#34;key\u0026#34;: 1, \u0026#34;value\u0026#34;: 20 }, { \u0026#34;key\u0026#34;: 5, \u0026#34;value\u0026#34;: 20 }, { \u0026#34;key\u0026#34;: 25, \u0026#34;value\u0026#34;: 40 }, { \u0026#34;key\u0026#34;: 50, \u0026#34;value\u0026#34;: 80 }, { \u0026#34;key\u0026#34;: 75, \u0026#34;value\u0026#34;: 120 }, { \u0026#34;key\u0026#34;: 95, \u0026#34;value\u0026#34;: 140 }, { \u0026#34;key\u0026#34;: 99, \u0026#34;value\u0026#34;: 140 } ] } } } 使用 tdigest 进行精确度调整 #\n tdigest 算法是计算百分位数的默认方法。它提供了一种内存高效的方式来估计百分位数排名，尤其是在处理浮点数据（如响应时间或延迟）时。\n与精确的百分位数计算不同， tdigest 使用概率方法将值分组到质心——即总结分布的小型簇。这种方法能够在无需将所有原始数据存储在内存中的情况下，为大多数百分位数提供准确的估计。\n该算法设计为在分布的尾部附近（即低百分位数（如第 1 位）和高百分位数（如第 99 位））具有高度准确性，这些通常是性能分析中最重要的一部分。您可以使用 compression 参数控制结果的精度。\n较高的 compression 值意味着使用更多的质心，这会增加准确性（尤其是在尾部），但需要更多的内存和 CPU。较低的 compression 值会减少内存使用并加快执行速度，但结果可能不够准确。\n适合使用 tdigest 的情况：\n 您的数据包含浮点值，例如响应时间、延迟或持续时间。 您需要在极端百分位数中获得准确的结果，例如第 1 位或第 99 位。  避免使用 tdigest 的情况：\n 您只处理整数数据，并希望获得最大速度。 您不太在意分布尾部的准确性，更倾向于快速聚合（可以考虑使用 hdr 代替）。  以下示例将 tdigest.compression 设置为 200 :\nGET /latency_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;load_time_percentiles\u0026#34;: { \u0026#34;percentiles\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;load_time\u0026#34;, \u0026#34;tdigest\u0026#34;: { \u0026#34;compression\u0026#34;: 200 } } } } } HDR 直方图 #\n 高动态范围（HDR）直方图是计算百分位的另一种方法。它特别适用于处理大型数据集和延迟测量。它专为速度而设计，同时支持宽动态范围值，并保持固定且可配置的精度水平。\n与 tdigest 不同，后者在分布的尾部（极端百分位）提供更高的准确性，HDR 优先考虑速度和范围内均匀的准确性。当分组的数量很大且不需要对稀有值进行极端精确度时，它效果最佳。\n例如，如果你正在测量从 1 微秒到 1 小时的范围内的响应时间，并使用 3 位有效数字配置 HDR，它将记录值精度为 ±1 微秒（直到 1 毫秒）和 ±3.6 秒（接近 1 小时）。\n这种权衡使得 HDR 比 tdigest 快得多，但内存消耗更大。\n下表展示了 HDR 显著数字的分解情况。\n   有效数字 相对精度（最大误差）     1 1 份在 10 份中 = 10%   2 1 份在 100 份中 = 1%   3 1 份在 1000 中 = 0.1%   4 1 份在 10000 中 = 0.01%   5 1 份在 100000 中 = 0.001%    如果您需要使用 HDR，则应：\n 正在跨多个分组进行聚合。 不需要在尾部的百分位数上要求极端的精确度。 确保有足够的内存可用。  你应该避免 HDR，如果：\n 尾部精度很重要。 你正在分析偏斜或稀疏的数据分布。  以下示例中将 hdr.number_of_significant_value_digits 设置为 3 ：\nGET /latency_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;load_time_percentiles\u0026#34;: { \u0026#34;percentiles\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;load_time\u0026#34;, \u0026#34;hdr\u0026#34;: { \u0026#34;number_of_significant_value_digits\u0026#34;: 3 } } } } } 缺省值 #  使用 missing 设置为不包含目标字段的文档配置备用值：\nGET /latency_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;load_time_percentiles\u0026#34;: { \u0026#34;percentiles\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;load_time\u0026#34;, \u0026#34;missing\u0026#34;: 0 } } } } Script 脚本 #\n 可以使用脚本动态计算值，而不是指定字段。当您需要应用转换（如货币转换或应用权重）时，这很有用。\n内联脚本 #  使用脚本计算派生值：\nGET /latency_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;adjusted_percentiles\u0026#34;: { \u0026#34;percentiles\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;load_time\u0026#39;].value * 1.2\u0026#34; }, \u0026#34;percents\u0026#34;: [50, 95] } } } } 存储脚本 #\n 首先，使用以下请求创建一个示例脚本：\nPOST _scripts/load_script { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;doc[params.field].value * params.multiplier\u0026#34; } } 然后用 percentiles 聚合中的存储脚本，提供 params 存储脚本所需的：\nGET /latency_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;adjusted_percentiles\u0026#34;: { \u0026#34;percentiles\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;load_script\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;load_time\u0026#34;, \u0026#34;multiplier\u0026#34;: 1.2 } }, \u0026#34;percents\u0026#34;: [50, 95] } } } } ","subcategory":null,"summary":"","tags":null,"title":"百分位数聚合（Percentiles）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/percentile/"},{"category":null,"content":"百分位排名聚合 #  percentile_ranks 聚合估计低于或等于给定阈值的观测值百分比。这对于了解特定值在值分布中的相对位置很有用。\n例如，您可以使用百分位排名聚合来学习交易金额 45 与数据集中其他交易值相比如何。百分位排名聚合返回一个值，如 82.3，这意味着 82.3% 的交易额低于或等于 45。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  percentile_ranks 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 用于计算百分位数的数值字段。   values 必需 Array of doubles 用于计算百分位数的值。   keyed 可选 Boolean 如果设置为 false ，则将结果作为数组返回。否则将结果作为 JSON 对象返回。默认值为 true 。   tdigest.compression 可选 Double 控制 tdigest 算法的准确性和内存使用。参见使用 tdigest 进行精度调整。   hdr.number_of_significant_value_digits 可选 Integer HDR 直方图的精度设置。参见 HDR 直方图。   missing 可选 Numeric 当文档中目标字段缺失时使用的默认值。   script 可选 Object 用于计算自定义值而不是使用字段的脚本。支持内联和存储脚本。    参考样例 #  首先，创建一个示例索引：\nPUT /transaction_data { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;amount\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } 添加示例数值以说明百分位数排名计算：\nPOST /transaction_data/_bulk { \u0026#34;index\u0026#34;: {} } { \u0026#34;amount\u0026#34;: 10 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;amount\u0026#34;: 20 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;amount\u0026#34;: 30 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;amount\u0026#34;: 40 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;amount\u0026#34;: 50 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;amount\u0026#34;: 60 } { \u0026#34;index\u0026#34;: {} } { \u0026#34;amount\u0026#34;: 70 } 运行 percentile_ranks 聚合来计算某些值与整体分布的比较情况：\nGET /transaction_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rank_check\u0026#34;: { \u0026#34;percentile_ranks\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;amount\u0026#34;, \u0026#34;values\u0026#34;: [25, 55] } } } } 表明 28.6%的值小于或等于 25 ，71.4%的值小于或等于 55 ：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 7, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;rank_check\u0026#34;: { \u0026#34;values\u0026#34;: { \u0026#34;25.0\u0026#34;: 28.57142857142857, \u0026#34;55.0\u0026#34;: 71.42857142857143 } } } } 键值 #\n 可以通过将 keyed 参数设置为 false 来更改返回的聚合格式，从 JSON 对象更改为键值对列表：\nGET /transaction_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rank_check\u0026#34;: { \u0026#34;percentile_ranks\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;amount\u0026#34;, \u0026#34;values\u0026#34;: [25, 55], \u0026#34;keyed\u0026#34;: false } } } } 返回内容包含一个数组而不是对象：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 7, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;rank_check\u0026#34;: { \u0026#34;values\u0026#34;: [ { \u0026#34;key\u0026#34;: 25, \u0026#34;value\u0026#34;: 28.57142857142857 }, { \u0026#34;key\u0026#34;: 55, \u0026#34;value\u0026#34;: 71.42857142857143 } ] } } } 使用 tdigest 进行精确度调整 #  默认情况下，百分位数排名使用 tdigest 算法计算。您可以通过指定 tdigest.compression 参数来控制准确性和内存使用之间的权衡。更高的值提供更好的准确性，但需要更多的内存。有关 tdigest 工作原理的更多信息，请参阅使用 tdigest 进行精度调整。 以下示例中将 tdigest.compression 设置为 200 ：\nGET /transaction_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rank_check\u0026#34;: { \u0026#34;percentile_ranks\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;amount\u0026#34;, \u0026#34;values\u0026#34;: [25, 55], \u0026#34;tdigest\u0026#34;: { \u0026#34;compression\u0026#34;: 200 } } } } } HDR 直方图 #\n 作为 tdigest 的替代方案，您可以使用高动态范围（HDR）直方图算法，该算法更适合大量分组和快速处理。有关 HDR 直方图的工作原理的更多信息，请参阅 HDR 直方图。\n如果您需要使用 HDR，则应：\n 正在跨多个分组进行聚合。 不需要在尾部的百分位数上要求极端的精确度。 确保有足够的内存可用。  你应该避免 HDR，如果：\n 尾部精度很重要。 你正在分析偏斜或稀疏的数据分布。  以下示例中将 hdr.number_of_significant_value_digits 设置为 3 ：\nGET /transaction_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rank_check\u0026#34;: { \u0026#34;percentile_ranks\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;amount\u0026#34;, \u0026#34;values\u0026#34;: [25, 55], \u0026#34;hdr\u0026#34;: { \u0026#34;number_of_significant_value_digits\u0026#34;: 3 } } } } } 缺省值 #  如果某些文档缺少目标字段，你可以通过设置 missing 参数来指示查询使用备用值。以下示例确保没有 amount 字段的文档被视为其值为 0 ，并包含在百分位数计算中：\nGET /transaction_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rank_check\u0026#34;: { \u0026#34;percentile_ranks\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;amount\u0026#34;, \u0026#34;values\u0026#34;: [25, 55], \u0026#34;missing\u0026#34;: 0 } } } } Script 脚本 #  除了指定字段，您还可以使用脚本动态计算值。这适用于需要应用转换的情况，例如货币转换或应用权重。\n内联脚本 #  以下示例使用内联脚本计算转换后的值 30 和 60 与增加 10%的 amount 字段值的百分位数排名：\nGET /transaction_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rank_check\u0026#34;: { \u0026#34;percentile_ranks\u0026#34;: { \u0026#34;values\u0026#34;: [30, 60], \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;amount\u0026#39;].value * 1.1\u0026#34; } } } } } 存储脚本 #\n 要使用存储脚本，首先使用以下请求创建它：\nPOST _scripts/percentile_script { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;doc[params.field].value * params.multiplier\u0026#34; } } 然后用 percentile_ranks 聚合中的存储脚本：\nGET /transaction_data/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;rank_check\u0026#34;: { \u0026#34;percentile_ranks\u0026#34;: { \u0026#34;values\u0026#34;: [30, 60], \u0026#34;script\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;percentile_script\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;amount\u0026#34;, \u0026#34;multiplier\u0026#34;: 1.1 } } } } } } \n","subcategory":null,"summary":"","tags":null,"title":"百分位排名聚合（Percentile Ranks）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/percentile-ranks/"},{"category":null,"content":"父文档聚合 #  parent 聚合是一个分组聚合，根据您索引中定义的父子关系创建一个包含父级文档的分组。此聚合使您能够对具有相同匹配子级文档的父级文档执行分析，从而实现强大的层次结构数据分析。\nparent 聚合与 join 字段类型一起工作，该字段类型在同一个索引中的文档内建立父子关系。\nparent 聚合识别具有匹配子文档的父文档，而 children 聚合识别匹配特定子关系的子文档。这两种聚合都使用子关系名称作为输入。\n相关指南（先读这些） #    聚合基础  Parent-Child 建模  聚合场景实践  参数说明 #  parent 聚合具有以下参数：\n   参数 必需/可选 数据类型 描述     type\t 必填 String join 字段中的子类型名称。    参考样例 #  以下示例构建了一个包含三名员工的小公司数据库。每个员工记录都与一个父部门记录存在 join 子关系。\n首先，创建一个 company 索引，其中包含一个 join 字段，该字段将部门（父级）映射到员工（子级）：\nPUT /company { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;join_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;relations\u0026#34;: { \u0026#34;department\u0026#34;: \u0026#34;employee\u0026#34; } }, \u0026#34;department_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;employee_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;salary\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, \u0026#34;hire_date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } } } } 接下来，用三个部门和三个员工填充数据。父子关系在以下表格中展示。\n   部门（父关系） 员工（子关系）     Accounting Abel Anderson, Betty Billings   Engineering Carl Carter   HR none    routing 参数确保父级和子级文档存储在同一个分片上：\nPOST _bulk?routing=1 { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;department\u0026#34;, \u0026#34;department_name\u0026#34;: \u0026#34;Accounting\u0026#34;, \u0026#34;join_field\u0026#34;: \u0026#34;department\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;department\u0026#34;, \u0026#34;department_name\u0026#34;: \u0026#34;Engineering\u0026#34;, \u0026#34;join_field\u0026#34;: \u0026#34;department\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;department\u0026#34;, \u0026#34;department_name\u0026#34;: \u0026#34;HR\u0026#34;, \u0026#34;join_field\u0026#34;: \u0026#34;department\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;employee_name\u0026#34;: \u0026#34;Abel Anderson\u0026#34;, \u0026#34;salary\u0026#34;: 120000, \u0026#34;hire_date\u0026#34;: \u0026#34;2024-04-04\u0026#34;, \u0026#34;join_field\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;employee_name\u0026#34;: \u0026#34;Betty Billings\u0026#34;, \u0026#34;salary\u0026#34;: 140000, \u0026#34;hire_date\u0026#34;: \u0026#34;2023-05-05\u0026#34;, \u0026#34;join_field\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;6\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;employee_name\u0026#34;: \u0026#34;Carl Carter\u0026#34;, \u0026#34;salary\u0026#34;: 140000, \u0026#34;hire_date\u0026#34;: \u0026#34;2020-06-06\u0026#34;, \u0026#34;join_field\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;2\u0026#34; } } 最后，运行一个聚合操作，统计与一个或多个员工存在父子关系的所有部门：\nGET /company/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;all_departments\u0026#34;: { \u0026#34;parent\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;employee\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;departments\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;department_name\u0026#34; } } } } } } 返回内容\nall_departments 父聚合返回所有包含员工子文档的部门。请注意，人力资源部门没有体现：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 6, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;all_departments\u0026#34;: { \u0026#34;doc_count\u0026#34;: 2, \u0026#34;departments\u0026#34;: { \u0026#34;doc_count_error_upper_bound\u0026#34;: 0, \u0026#34;sum_other_doc_count\u0026#34;: 0, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;Accounting\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: \u0026#34;Engineering\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 } ] } } } } ","subcategory":null,"summary":"","tags":null,"title":"父文档聚合（Parent）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/parent/"},{"category":null,"content":"热门匹配聚合 #  top_hits 聚合是一种多值指标聚合，它根据聚合字段的相关性评分对匹配文档进行排名。\n相关指南（先读这些） #    聚合基础  聚合场景实践  您可以指定以下选项：\n from : 命中的起始位置。 size : 返回命中的最大数量。默认值为 3。 sort : 匹配命中的排序方式。默认情况下，命中的排序依据聚合查询的相关性得分。  以下示例返回数据中的前 5 个产品：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;top_hits_products\u0026#34;: { \u0026#34;top_hits\u0026#34;: { \u0026#34;size\u0026#34;: 5 } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;top_hits_products\u0026#34; : { \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 4675, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;sample_data_ecommerce\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;glMlwXcBQVLeQPrkHPtI\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;category\u0026#34; : [ \u0026#34;Women\u0026#39;s Accessories\u0026#34;, \u0026#34;Women\u0026#39;s Clothing\u0026#34; ], \u0026#34;currency\u0026#34; : \u0026#34;EUR\u0026#34;, \u0026#34;customer_first_name\u0026#34; : \u0026#34;rania\u0026#34;, \u0026#34;customer_full_name\u0026#34; : \u0026#34;rania Evans\u0026#34;, \u0026#34;customer_gender\u0026#34; : \u0026#34;FEMALE\u0026#34;, \u0026#34;customer_id\u0026#34; : 24, \u0026#34;customer_last_name\u0026#34; : \u0026#34;Evans\u0026#34;, \u0026#34;customer_phone\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;day_of_week\u0026#34; : \u0026#34;Sunday\u0026#34;, \u0026#34;day_of_week_i\u0026#34; : 6, \u0026#34;email\u0026#34; : \u0026#34;rania@evans-family.zzz\u0026#34;, \u0026#34;manufacturer\u0026#34; : [ \u0026#34;Tigress Enterprises\u0026#34; ], \u0026#34;order_date\u0026#34; : \u0026#34;2021-02-28T14:16:48+00:00\u0026#34;, \u0026#34;order_id\u0026#34; : 583581, \u0026#34;products\u0026#34; : [ { \u0026#34;base_price\u0026#34; : 10.99, \u0026#34;discount_percentage\u0026#34; : 0, \u0026#34;quantity\u0026#34; : 1, \u0026#34;manufacturer\u0026#34; : \u0026#34;Tigress Enterprises\u0026#34;, \u0026#34;tax_amount\u0026#34; : 0, \u0026#34;product_id\u0026#34; : 19024, \u0026#34;category\u0026#34; : \u0026#34;Women\u0026#39;s Accessories\u0026#34;, \u0026#34;sku\u0026#34; : \u0026#34;ZO0082400824\u0026#34;, \u0026#34;taxless_price\u0026#34; : 10.99, \u0026#34;unit_discount_amount\u0026#34; : 0, \u0026#34;min_price\u0026#34; : 5.17, \u0026#34;_id\u0026#34; : \u0026#34;sold_product_583581_19024\u0026#34;, \u0026#34;discount_amount\u0026#34; : 0, \u0026#34;created_on\u0026#34; : \u0026#34;2016-12-25T14:16:48+00:00\u0026#34;, \u0026#34;product_name\u0026#34; : \u0026#34;Snood - white/grey/peach\u0026#34;, \u0026#34;price\u0026#34; : 10.99, \u0026#34;taxful_price\u0026#34; : 10.99, \u0026#34;base_unit_price\u0026#34; : 10.99 }, { \u0026#34;base_price\u0026#34; : 32.99, \u0026#34;discount_percentage\u0026#34; : 0, \u0026#34;quantity\u0026#34; : 1, \u0026#34;manufacturer\u0026#34; : \u0026#34;Tigress Enterprises\u0026#34;, \u0026#34;tax_amount\u0026#34; : 0, \u0026#34;product_id\u0026#34; : 19260, \u0026#34;category\u0026#34; : \u0026#34;Women\u0026#39;s Clothing\u0026#34;, \u0026#34;sku\u0026#34; : \u0026#34;ZO0071900719\u0026#34;, \u0026#34;taxless_price\u0026#34; : 32.99, \u0026#34;unit_discount_amount\u0026#34; : 0, \u0026#34;min_price\u0026#34; : 17.15, \u0026#34;_id\u0026#34; : \u0026#34;sold_product_583581_19260\u0026#34;, \u0026#34;discount_amount\u0026#34; : 0, \u0026#34;created_on\u0026#34; : \u0026#34;2016-12-25T14:16:48+00:00\u0026#34;, \u0026#34;product_name\u0026#34; : \u0026#34;Cardigan - grey\u0026#34;, \u0026#34;price\u0026#34; : 32.99, \u0026#34;taxful_price\u0026#34; : 32.99, \u0026#34;base_unit_price\u0026#34; : 32.99 } ], \u0026#34;sku\u0026#34; : [ \u0026#34;ZO0082400824\u0026#34;, \u0026#34;ZO0071900719\u0026#34; ], \u0026#34;taxful_total_price\u0026#34; : 43.98, \u0026#34;taxless_total_price\u0026#34; : 43.98, \u0026#34;total_quantity\u0026#34; : 2, \u0026#34;total_unique_products\u0026#34; : 2, \u0026#34;type\u0026#34; : \u0026#34;order\u0026#34;, \u0026#34;user\u0026#34; : \u0026#34;rani\u0026#34;, \u0026#34;geoip\u0026#34; : { \u0026#34;country_iso_code\u0026#34; : \u0026#34;EG\u0026#34;, \u0026#34;location\u0026#34; : { \u0026#34;lon\u0026#34; : 31.3, \u0026#34;lat\u0026#34; : 30.1 }, \u0026#34;region_name\u0026#34; : \u0026#34;Cairo Governorate\u0026#34;, \u0026#34;continent_name\u0026#34; : \u0026#34;Africa\u0026#34;, \u0026#34;city_name\u0026#34; : \u0026#34;Cairo\u0026#34; }, \u0026#34;event\u0026#34; : { \u0026#34;dataset\u0026#34; : \u0026#34;sample_ecommerce\u0026#34; } } ... } ] } } } } ","subcategory":null,"summary":"","tags":null,"title":"热门匹配聚合（Top Hits）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/top-hits/"},{"category":null,"content":"求和聚合 #  sum 聚合是一种单值指标聚合，计算字段中匹配文档中提取的数值的总和。此聚合常用于计算诸如收入、数量或持续时间等指标的总计。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  sum 聚合接受以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 聚合的字段。必须是数值字段。   script 可选 Object 用于计算聚合自定义值的脚本。可以替代或与 field 结合使用。   missing 可选 Number 缺少目标字段时使用的默认值。    参考样例 #  以下示例演示了如何计算物流索引中记录的交付总重量。\n创建一个索引：\nPUT /deliveries { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;shipment_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;weight_kg\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } 添加示例文档：\nPOST /deliveries/_bulk?refresh=true {\u0026#34;index\u0026#34;: {}} {\u0026#34;shipment_id\u0026#34;: \u0026#34;S001\u0026#34;, \u0026#34;weight_kg\u0026#34;: 12.5} {\u0026#34;index\u0026#34;: {}} {\u0026#34;shipment_id\u0026#34;: \u0026#34;S002\u0026#34;, \u0026#34;weight_kg\u0026#34;: 7.8} {\u0026#34;index\u0026#34;: {}} {\u0026#34;shipment_id\u0026#34;: \u0026#34;S003\u0026#34;, \u0026#34;weight_kg\u0026#34;: 15.0} {\u0026#34;index\u0026#34;: {}} {\u0026#34;shipment_id\u0026#34;: \u0026#34;S004\u0026#34;, \u0026#34;weight_kg\u0026#34;: 10.3} 以下请求计算 deliveries 索引中所有文档的总权重，通过将 size 设置为 0 来忽略文档命中，并返回 weight_kg 的总和：\nGET /deliveries/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;total_weight\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;weight_kg\u0026#34; } } } } 返回包含值 45.6 ，对应于 12.5 + 7.8 + 15.0 + 10.3 的总和：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;total_weight\u0026#34;: { \u0026#34;value\u0026#34;: 45.6 } } } 使用脚本来计算值 #  你可以提供一个脚本来计算聚合中的值，而不是直接指定一个字段。这在值必须经过推导或调整时非常有用。\n在以下示例中，每个重量在使用脚本求和之前都会从千克转换为克：\nGET /deliveries/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;total_weight_grams\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;weight_kg\u0026#39;].value * 1000\u0026#34; } } } } } 内容中 total_weight_grams 为 45600 ：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;total_weight_grams\u0026#34;: { \u0026#34;value\u0026#34;: 45600 } } } 将字段与值脚本结合使用 #  你也可以同时指定 field 和 script ，使用特殊变量 _value 来引用字段的值。这在对现有字段值应用转换时非常有用。\n以下示例在求和之前将所有权重增加 10%：\nGET /deliveries/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;adjusted_weight\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;weight_kg\u0026#34;, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;Math.round(_value * 110) / 100.0\u0026#34; } } } } } 反映了原始总重量增加了 10%：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;adjusted_weight\u0026#34;: { \u0026#34;value\u0026#34;: 50.16 } } } 缺省值 #  缺少目标字段的文档默认会被忽略。要使用默认值包含它们，请使用 missing 参数。\n以下示例将默认值 0 分配给缺少的 weight_kg 字段。这确保了缺少该字段的文档被视为 weight_kg 设置为 0 并包含在聚合中。\nGET /deliveries/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;total_weight_with_missing\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;weight_kg\u0026#34;, \u0026#34;missing\u0026#34;: 0 } } } } \n","subcategory":null,"summary":"","tags":null,"title":"求和聚合（Sum）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/sum/"},{"category":null,"content":"求和桶聚合 #  sum_bucket 聚合是一个同级聚合，用于计算先前聚合中每个分组中指标的总和。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  sum_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。    参考样例 #  以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， sum_bucket 聚合通过汇总这些总和来计算每个月的总字节数：\nPOST sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } } } }, \u0026#34;sum_monthly_bytes\u0026#34;: { \u0026#34;sum_bucket\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;visits_per_month\u0026gt;sum_of_bytes\u0026#34; } } } } 返回内容 #  该聚合返回所有月度分组中的字节总和：\n{ \u0026#34;took\u0026#34;: 10, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 } } ] }, \u0026#34;sum_monthly_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 79725689 } } } ","subcategory":null,"summary":"","tags":null,"title":"求和桶聚合（Sum Bucket）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/sum-bucket/"},{"category":null,"content":"Pattern Replace 字符过滤器 #  pattern_replace 字符过滤器使你能够使用正则表达式来定义文本匹配替换的模式。对于文本转换的高阶需求场景，尤其是在处理复杂的字符串模式时，它是一种灵活的工具。\n这个过滤器会用替换符合匹配模式的所有匹配项，从而可以轻松地对输入文本进行替换、删除或复杂的修改。你可以在分词之前使用它对输入内容进行规范化处理。\n相关指南（先读这些） #    文本分析基础  文本分析：规范化  参考样例 #  为了规范电话号码，你可以使用正则表达式 [\\\\s()-]+去替换号码里的特殊格式：\n []：定义一个字符类，意味着它将匹配方括号内的任意一个字符。 \\\\s：匹配任何空白字符，如空格、制表符或换行符。 ()：匹配字面意义上的括号（( 或 )）。 -：匹配字面意义上的连字符（-）。 +：指定该模式应匹配前面字符的一次或多次出现。  模式 [\\\\s()-]+ 将匹配由一个或多个空白字符、括号或连字符组成的任意序列，并将其从输入文本中移除。这确保了电话号码得到规范处理，结果将仅包含数字。\n以下请求通过移除空格、连字符和括号来规范电话号码：\nGET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;char_filter\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;pattern_replace\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;[\\\\s()-]+\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;\u0026#34; } ], \u0026#34;text\u0026#34;: \u0026#34;(555) 123-4567\u0026#34; } 返回内容中包含生成的词元：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;5551234567\u0026#34;, \u0026#34;start_offset\u0026#34;: 1, \u0026#34;end_offset\u0026#34;: 14, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } 参数说明 #  pattern_replace 字符过滤器必须使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     pattern 必需 字符串 用于匹配输入文本部分内容的正则表达式。过滤器会识别并匹配此模式以执行替换操作。   replacement 可选 字符串 用于替换匹配内容的字符串。使用空字符串（\u0026quot;\u0026quot;）可移除匹配到的文本。默认值为空字符串（\u0026quot;\u0026quot;）。    创建自定义分词器 #  以下请求创建一个索引，该索引带有一个配置了 pattern_replace 字符过滤器的自定义分词器。此过滤器会从数字中移除货币符号以及千位分隔符（包括欧洲的 “.” 和美国的 “,”）：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;char_filter\u0026#34;: [ \u0026#34;pattern_char_filter\u0026#34; ] } }, \u0026#34;char_filter\u0026#34;: { \u0026#34;pattern_char_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern_replace\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;[$€,.]\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;\u0026#34; } } } } } 使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Total: $ 1,200.50 and € 1.100,75\u0026#34; } 返回内容中包含了生成的词元：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Total\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;120050\u0026#34;, \u0026#34;start_offset\u0026#34;: 9, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;and\u0026#34;, \u0026#34;start_offset\u0026#34;: 18, \u0026#34;end_offset\u0026#34;: 21, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;110075\u0026#34;, \u0026#34;start_offset\u0026#34;: 24, \u0026#34;end_offset\u0026#34;: 32, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 } ] } 使用捕获组 #  你可以在 replacement 参数中使用捕获组。例如，以下请求创建了一个自定义分词器，该分词器使用匹配替换字符过滤器将电话号码中的连字符替换为点号：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;char_filter\u0026#34;: [ \u0026#34;pattern_char_filter\u0026#34; ] } }, \u0026#34;char_filter\u0026#34;: { \u0026#34;pattern_char_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern_replace\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;(\\\\d+)-(?=\\\\d)\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;$1.\u0026#34; } } } } } 使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Call me at 555-123-4567 or 555-987-6543\u0026#34; } 返回内容中包含了生成的词元：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Call\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;me\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;at\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;555.123.4567\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 23, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;or\u0026#34;, \u0026#34;start_offset\u0026#34;: 24, \u0026#34;end_offset\u0026#34;: 26, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;555.987.6543\u0026#34;, \u0026#34;start_offset\u0026#34;: 27, \u0026#34;end_offset\u0026#34;: 39, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 } ] } ","subcategory":null,"summary":"","tags":null,"title":"正则替换字符过滤器（Pattern Replace）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/character-filters/pattern-replace/"},{"category":null,"content":"桶选择器聚合 #  bucket_selector 聚合是一个父管道聚合，它评估脚本以确定直方图（或 date_histogram）聚合返回的存储分组是否应包含在最终结果中。\n与创建新值的管道聚合不同，bucket_selector 聚合充当筛选器，根据指定的条件保留或删除整个存储分组。使用此聚合可根据存储分组的计算指标筛选存储分组。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  bucket_selector 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 Object 变量名称到分分组指标的映射，用于标识要在脚本中使用的指标。指标必须是数字。请参阅脚本变量 。   script 必需 String 或 Object 要执行的脚本。可以是内联脚本、存储脚本或脚本文件。该脚本可以访问 buckets_path 参数中定义的变量名称。必须返回布尔值。返回 false 的存储分组将从最终输出中删除。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认值为 skip 。详见 数据空缺。    参考样例 #  以下示例创建间隔为一周的日期直方图。sum 子聚合计算每周所有销售额的总和。最后，bucket_selector 聚合会筛选生成的每周存储分组，删除所有总值不超过 75,000 美元的存储分组：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_week\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;order_date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;week\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;weekly_sales\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;$#,###.00\u0026#34; } }, \u0026#34;avg_vendor_spend\u0026#34;: { \u0026#34;bucket_selector\u0026#34;: { \u0026#34;buckets_path\u0026#34;: { \u0026#34;weekly_sales\u0026#34;: \u0026#34;weekly_sales\u0026#34; }, \u0026#34;script\u0026#34;: \u0026#34;params.weekly_sales \u0026gt; 75000\u0026#34; } } } } } } 返回内容 #  聚合返回满足脚本条件的 sales_per_week 存储分组：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;sales_per_week\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-31T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743379200000, \u0026#34;doc_count\u0026#34;: 1048, \u0026#34;weekly_sales\u0026#34;: { \u0026#34;value\u0026#34;: 79448.60546875, \u0026#34;value_as_string\u0026#34;: \u0026#34;$79,448.61\u0026#34; } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-07T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743984000000, \u0026#34;doc_count\u0026#34;: 1048, \u0026#34;weekly_sales\u0026#34;: { \u0026#34;value\u0026#34;: 78208.4296875, \u0026#34;value_as_string\u0026#34;: \u0026#34;$78,208.43\u0026#34; } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-14T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1744588800000, \u0026#34;doc_count\u0026#34;: 1073, \u0026#34;weekly_sales\u0026#34;: { \u0026#34;value\u0026#34;: 81277.296875, \u0026#34;value_as_string\u0026#34;: \u0026#34;$81,277.30\u0026#34; } } ] } } } \n由于它返回布尔值而不是数值，因此 buckets_selector 聚合不采用格式参数。在此示例中，格式化的指标由 sum 子聚合在 value_as_string 结果中返回。将此与 bucket_script 聚合中的示例进行对比。\n ","subcategory":null,"summary":"","tags":null,"title":"桶选择器聚合（Bucket Selector）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/bucket-selector/"},{"category":null,"content":"桶脚本聚合 #  bucket_script 聚合是一个父管道聚合，它执行脚本以跨一组存储分组执行每个存储分组的数字计算。使用 bucket_script 聚合对分分组聚合中的多个指标执行自定义数值计算。例如，您可以：\n 计算派生指标和复合指标。 使用 if/else 语句应用条件逻辑。 计算特定于业务的 KPI，例如自定义评分指标。  相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  bucket_script 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 Object 一个变量名称到分分组指标的映射，用于识别脚本中使用的指标。这些指标必须是数值型。参见脚本变量 。   script 必需 String 或 Object 要执行的脚本。可以是内联脚本、存储脚本或脚本文件。脚本可以访问通过 buckets_path 参数定义的变量名。必须返回一个数值。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认值为 skip 。详见 数据空缺。   format 可选 String 一个 DecimalFormat 格式化字符串。将在聚合的 value_as_string 参数中返回格式化后的输出。    脚本变量 #  buckets_path 参数将脚本变量名称映射到父聚合的指标。然后可以在脚本中使用这些变量。\n 对于 bucket_script 和 bucket_selector 聚合， buckets_path 参数是一个对象而不是字符串，因为它必须引用多个分组指标。有关 buckets_path 字符串版本的描述，请参阅管道聚合页面。\n 以下 buckets_path 将 sales_sum 指标映射到 total_sales 脚本变量，并将 item_count 指标映射到 item_count 脚本变量：\n\u0026#34;buckets_path\u0026#34;: { \u0026#34;total_sales\u0026#34;: \u0026#34;sales_sum\u0026#34;, \u0026#34;item_count\u0026#34;: \u0026#34;item_count\u0026#34; } 映射的变量可以从 params 上下文中访问。例如：\n params.total_sales params.item_count  参考样例 #  以下示例创建了一个按月份间隔的一组日期直方图。 total_sales 子聚合计算了每个月销售的所有商品的税后总价。 vendor_count 聚合计算了每个月的唯一供应商总数。最后， avg_vendor_spend 聚合使用内联脚本计算每个月每个供应商的平均消费金额：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;order_date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;total_sales\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } }, \u0026#34;vendor_count\u0026#34;: { \u0026#34;cardinality\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.manufacturer.keyword\u0026#34; } }, \u0026#34;avg_vendor_spend\u0026#34;: { \u0026#34;bucket_script\u0026#34;: { \u0026#34;buckets_path\u0026#34;: { \u0026#34;sales\u0026#34;: \u0026#34;total_sales\u0026#34;, \u0026#34;vendors\u0026#34;: \u0026#34;vendor_count\u0026#34; }, \u0026#34;script\u0026#34;: \u0026#34;params.sales / params.vendors\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;$#,###.00\u0026#34; } } } } } } 返回内容 #\n 聚合返回格式化的每月平均供应商支出：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 721, \u0026#34;vendor_count\u0026#34;: { \u0026#34;value\u0026#34;: 21 }, \u0026#34;total_sales\u0026#34;: { \u0026#34;value\u0026#34;: 53468.1484375 }, \u0026#34;avg_vendor_spend\u0026#34;: { \u0026#34;value\u0026#34;: 2546.1023065476193, \u0026#34;value_as_string\u0026#34;: \u0026#34;$2,546.10\u0026#34; } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 3954, \u0026#34;vendor_count\u0026#34;: { \u0026#34;value\u0026#34;: 21 }, \u0026#34;total_sales\u0026#34;: { \u0026#34;value\u0026#34;: 297415.98046875 }, \u0026#34;avg_vendor_spend\u0026#34;: { \u0026#34;value\u0026#34;: 14162.665736607143, \u0026#34;value_as_string\u0026#34;: \u0026#34;$14,162.67\u0026#34; } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"桶脚本聚合（Bucket Script）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/bucket-script/"},{"category":null,"content":"桶排序聚合 #  bucket_sort 聚合是一个父聚合，它对其父多存储分组聚合生成的存储分组进行排序或截断。\n在 bucket_sort 聚合中，您可以按多个字段对存储分组进行排序，每个字段都有自己的排序顺序。可以按存储分组的键、文档计数或子聚合中的值进行排序。您还可以使用 from 和 size 参数来截断结果，无论是否进行排序。\n有关指定排序顺序的信息，请参阅排序结果。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  bucket_sort 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     gap_policy 可选 String 要应用于缺失数据的策略。有效值为 skip 和 insert_zeros。默认值为 skip。请参阅数据差距 。   sort 可选 String 要排序的字段列表。请参阅排序结果 。   from 可选 String 要返回的第一个结果的索引。必须是非负整数。默认值为 0。请参阅 from 和 size 参数 。   size 可选 String 要返回的最大结果数。必须是正整数。请参阅 from 和 size 参数 。     您必须至少提供一个 sort、from 和 size。\n 参考样例 #  以下示例创建间隔为一个月的日期直方图。sum 子聚合计算每个月所有字节的总和。最后，聚合按字节数降序对存储分组进行排序：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;total_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;bytes_bucket_sort\u0026#34;: { \u0026#34;bucket_sort\u0026#34;: { \u0026#34;sort\u0026#34;: [ { \u0026#34;total_bytes\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } } } } } } 返回内容 #\n 聚合按总字节数降序对存储分组重新排序：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 7072, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 40124337 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-06-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1748736000000, \u0026#34;doc_count\u0026#34;: 6056, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 34123131 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 946, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 5478221 } } ] } } } 示例：截取结果 #\n 要截取结果，请提供 from 和/或 size 参数。以下示例执行相同的排序，但返回两个存储分组，从第二个存储分组开始：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;total_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;bytes_bucket_sort\u0026#34;: { \u0026#34;bucket_sort\u0026#34;: { \u0026#34;sort\u0026#34;: [ { \u0026#34;total_bytes\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ], \u0026#34;from\u0026#34;: 1, \u0026#34;size\u0026#34;: 2 } } } } } } 聚合返回两个排序的存储分组：\n{ \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-06-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1748736000000, \u0026#34;doc_count\u0026#34;: 6056, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 34123131 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 946, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 5478221 } } ] } } } 要截取结果而不进行排序，请省略 sort 参数：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;total_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;bytes_bucket_sort\u0026#34;: { \u0026#34;bucket_sort\u0026#34;: { \u0026#34;from\u0026#34;: 1, \u0026#34;size\u0026#34;: 2 } } } } } } \n","subcategory":null,"summary":"","tags":null,"title":"桶排序聚合（Bucket Sort）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/bucket-sort/"},{"category":null,"content":"最小桶聚合 #  min_bucket 聚合是一个同级聚合，用于计算先前聚合中每个分组中某个指标的最小值。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  min_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。    参考样例 #  以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， min_bucket 聚合找到最小值——这些分组中最小的一个：\nPOST sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } } } }, \u0026#34;min_monthly_bytes\u0026#34;: { \u0026#34;min_bucket\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;visits_per_month\u0026gt;sum_of_bytes\u0026#34; } } } } 返回内容 #  min_bucket 聚合返回跨多个分组的指定指标的最小值。在这个示例中，它计算了 sum_of_bytes 指标在 visits_per_month 中的每月最小字节数。 value 字段显示了在所有分组中找到的最小值。 keys 数组包含观察到该最小值的分组的键。它是一个数组，因为多个分组可以具有相同的最小值。在这种情况下，所有匹配的分组键都会被包含。这确保了即使多个时间段（或分组项）具有相同的最小值，结果也是准确的：\n{ \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 } } ] }, \u0026#34;min_monthly_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103, \u0026#34;keys\u0026#34;: [ \u0026#34;2025-03-01T00:00:00.000Z\u0026#34; ] } } } \n","subcategory":null,"summary":"","tags":null,"title":"最小桶聚合（Min Bucket）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/min-bucket/"},{"category":null,"content":"Min Hash 分词过滤器 #  min_hash 分词过滤器用于基于最小哈希近似算法为词元生成哈希值，这对于检测文档间的相似度很有用。min_hash 分词过滤器会为一组词元（通常来自一个经过分词的字段）生成哈希值。\n相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  参数说明 #  最小哈希分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     hash_count 可选 整数 为每个词元生成的哈希值数量。增加此值通常会提高相似度估计的准确性，但会增加计算成本。默认值为 1。   bucket_count 可选 整数 要使用的哈希桶数量。这会影响哈希的粒度。更多的桶能提供更细的粒度并减少哈希冲突，但需要更多内存。默认值为 512。   hash_set_size 可选 整数 每个桶中要保留的哈希数量。这会影响哈希质量。更大的集合大小可能会带来更好的相似度检测效果，但会消耗更多内存。默认值为 1。   with_rotation 可选 布尔值 当设置为 true 时，如果 hash_set_size 为 1，过滤器会用从其右侧按环形顺序找到的第一个非空桶的值填充空桶。如果 bucket_count 参数大于 1，此设置会自动默认为 true；否则，默认为 false。    参考样例 #  以下示例请求创建了一个名为 minhash_index 的新索引，并配置了一个带有最小哈希过滤器的分词器：\nPUT /minhash_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;minhash_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;min_hash\u0026#34;, \u0026#34;hash_count\u0026#34;: 3, \u0026#34;bucket_count\u0026#34;: 512, \u0026#34;hash_set_size\u0026#34;: 1, \u0026#34;with_rotation\u0026#34;: false } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;minhash_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;minhash_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /minhash_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;minhash_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch is very powerful.\u0026#34; } 返回内容中包含了生成的词元（这些词元没有直观的可读性，因为它们代表的是哈希值）：\n{ \u0026#34;tokens\u0026#34; : [ { \u0026#34;token\u0026#34; : \u0026#34;\\u0000\\u0000㳠锯ੲ걌䐩䉵\u0026#34;, \u0026#34;start_offset\u0026#34; : 0, \u0026#34;end_offset\u0026#34; : 27, \u0026#34;type\u0026#34; : \u0026#34;MIN_HASH\u0026#34;, \u0026#34;position\u0026#34; : 0 }, { \u0026#34;token\u0026#34; : \u0026#34;\\u0000\\u0000㳠锯ੲ걌䐩䉵\u0026#34;, \u0026#34;start_offset\u0026#34; : 0, \u0026#34;end_offset\u0026#34; : 27, \u0026#34;type\u0026#34; : \u0026#34;MIN_HASH\u0026#34;, \u0026#34;position\u0026#34; : 0 }, ... ","subcategory":null,"summary":"","tags":null,"title":"最小哈希分词过滤器（Min Hash）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/min-hash/"},{"category":null,"content":"最小值聚合 #  min 聚合是一个单值指标聚合，返回字段的最小值。\n相关指南（先读这些） #    聚合基础  聚合场景实践   min 聚合使用 double （双精度）表示来比较数值字段。对于包含 long 或 unsigned_long 且绝对值大于 2 53 的字段，结果应被视为近似值，因为 double 尾数中的有效位数是 53。\n 参数说明 #  min 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 计算最小值的字段名称。   missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，则包含缺失值的文档将从聚合中排除。    参考样例 #  以下示例请求在样本数据中查找最便宜的商品——即 base_unit_price 值最小的商品：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;min_base_unit_price\u0026#34;: { \u0026#34;min\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.base_unit_price\u0026#34; } } } } 返回内容 #\n 如以下示例返回所示，聚合返回了 products.base_unit_price 的最小值：\n{ \u0026#34;took\u0026#34;: 15, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;min_base_unit_price\u0026#34;: { \u0026#34;value\u0026#34;: 5.98828125 } } } 可以使用聚合名称（ min_base_unit_price ）作为检索聚合内容的键名。\n缺省值 #  可以为聚合字段的缺失实例分配一个值。有关更多信息，请参阅缺失聚合。\nmin 通常会忽略缺失值。如果您使用 missing 分配一个低于任何现有值的值， min 会将此替换值作为最小值返回。\n","subcategory":null,"summary":"","tags":null,"title":"最小值聚合（Min）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/minimum/"},{"category":null,"content":"最大桶聚合 #  max_bucket 聚合是一个同级聚合，用于计算先前聚合中每个分组中某个指标的最大值。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  max_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。    参考样例 #  以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， max_bucket 聚合找到最大值——这些分组中最大的那个：\nPOST sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } } } }, \u0026#34;max_monthly_bytes\u0026#34;: { \u0026#34;max_bucket\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;visits_per_month\u0026gt;sum_of_bytes\u0026#34; } } } } 返回内容 #\n max_bucket 聚合返回跨多个分组的指定指标的最大值。在这个示例中，它计算了 sum_of_bytes 指标在 visits_per_month 中的每月最大字节数。 value 字段显示了在所有分组中找到的最大值。 keys 数组包含观察到该最大值的分组的键。它是一个数组，因为多个分组可以具有相同最大值。在这种情况下，所有匹配的分组键都会被包含。这确保了即使多个时间段（或分组项）具有相同最大值，结果也是准确的：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 } } ] }, \u0026#34;max_monthly_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067, \u0026#34;keys\u0026#34;: [ \u0026#34;2025-04-01T00:00:00.000Z\u0026#34; ] } } } \n","subcategory":null,"summary":"","tags":null,"title":"最大桶聚合（Max Bucket）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/max-bucket/"},{"category":null,"content":"最大值聚合 #  max 聚合是一个单值指标聚合，返回字段的最大值。\n相关指南（先读这些） #    聚合基础  聚合场景实践   max 聚合使用 double （双精度）表示来比较数值字段。对于包含 long 或 unsigned_long 超过 2^53 的整数值的字段，结果应被视为近似值，因为 double 的尾数中的有效位数是 53。\n 参数说明 #  max 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 计算最大值的字段名称。   missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，则包含缺失值的文档将从聚合中排除。    参考样例 #  以下示例请求在数据中查找最昂贵的商品——即 base_unit_price 值最大的商品：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;max_base_unit_price\u0026#34;: { \u0026#34;max\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.base_unit_price\u0026#34; } } } } 返回内容 #  如以下示例返回内容所示，聚合返回 products.base_unit_price 的最大值：\n{ \u0026#34;took\u0026#34;: 24, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;max_base_unit_price\u0026#34;: { \u0026#34;value\u0026#34;: 540 } } } 您可以使用聚合名称（ max_base_unit_price ）作为键从响应中检索聚合。\n缺省值 #  您可以为聚合字段的缺失实例分配一个值。有关更多信息，请参阅缺失聚合。 缺省值通常被 max 忽略。如果您使用 missing 分配一个大于任何现有值的值， max 将返回此替换值作为最大值。\n","subcategory":null,"summary":"","tags":null,"title":"最大值聚合（Max）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/maximum/"},{"category":null,"content":"显著词项聚合 #  significant_terms 聚合可以帮助你在相对于索引中其他数据的过滤子集中识别不寻常或有趣的分组出现情况。\n前景集是指你进行过滤的文档集合，背景集是指索引中所有文档的集合。significant_terms 聚合会检查前景集中的所有文档，并与背景集中的文档进行对比，从而为重要出现情况找到相应的分数。\n相关指南（先读这些） #    聚合基础  聚合场景实践  在示例网络日志数据中，每个文档都有一个包含访客 user-agent 的字段。此示例搜索来自 iOS 操作系统的所有请求。对这一前景集进行常规的 terms 聚合返回 Firefox，因为它在这个分组内有最多的文档数量。另一方面， significant_terms 聚合返回 Internet Explorer（IE），因为 IE 在前景集中的出现频率显著高于背景集。\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;machine.os.keyword\u0026#34;: [ \u0026#34;ios\u0026#34; ] } }, \u0026#34;aggs\u0026#34;: { \u0026#34;significant_response_codes\u0026#34;: { \u0026#34;significant_terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;agent.keyword\u0026#34; } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;significant_response_codes\u0026#34; : { \u0026#34;doc_count\u0026#34; : 2737, \u0026#34;bg_count\u0026#34; : 14074, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\u0026#34;, \u0026#34;doc_count\u0026#34; : 818, \u0026#34;score\u0026#34; : 0.01462731514608217, \u0026#34;bg_count\u0026#34; : 4010 }, { \u0026#34;key\u0026#34; : \u0026#34;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\u0026#34;, \u0026#34;doc_count\u0026#34; : 1067, \u0026#34;score\u0026#34; : 0.009062566630410223, \u0026#34;bg_count\u0026#34; : 5362 } ] } } } 如果 significant_terms 聚合没有返回任何结果，你可能没有使用查询来过滤结果。或者，前景集中词条的分布可能与背景集相同，这意味着前景集中没有什么异常。\n背景词条频率统计信息的默认来源是整个索引。你可以使用背景过滤器来缩小这个范围，以便更聚焦\n参数说明 #     参数 必需/可选 数据类型 描述     field 必填 String 要分析的 keyword 或 text 字段。   size 可选 Integer 返回的桶数量。默认 10。   min_doc_count 可选 Integer 前景集中至少出现此次数的词项才纳入分析。默认 3。   shard_min_doc_count 可选 Integer 分片级别的最小文档数。默认 1。   background_filter 可选 Object 自定义背景集的过滤条件，缩小对比范围。   mutual_information 可选 Object 使用互信息作为显著性评分算法。   chi_square 可选 Object 使用卡方检验作为显著性评分算法。   gnd 可选 Object 使用 Google 标准化距离作为评分算法。   jlh 可选 Object 使用 JLH 评分（默认算法）。    实际应用：异常检测 #  significant_terms 是发现异常和关联的强大工具。典型场景：\n 安全分析：在出现异常流量的 IP 段中，哪些 URL 路径显著高于正常水平？ 电商推荐：购买了某商品的用户，还显著倾向于购买哪些商品？ 日志分析：在报错的请求中，哪些参数组合异常频繁？   提示：如果结果为空，通常意味着前景集的词频分布与背景集相似——即没有统计上的显著差异。尝试使用更具区分性的查询条件来定义前景集。\n ","subcategory":null,"summary":"","tags":null,"title":"显著词项聚合（Significant Terms）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/significant-terms/"},{"category":null,"content":"显著文本聚合 #  significant_text 聚合与 significant_terms 聚合类似，但它适用于原始文本字段。重要文本通过统计分析测量前景集和背景集之间流行度的变化。例如，当你搜索其股票缩写 TSLA 时，它可能会建议 Tesla。\nsignificant_text 聚合会动态重新分析源文本，过滤掉重复段落、模板化的页眉和页脚等噪声数据，这些数据可能会扭曲结果。\n重新分析高基数数据集可能是一项非常耗费 CPU 的操作。我们建议在采样聚合中使用 significant_text 聚合来将分析限制在少量最匹配文档中，例如 200。\n相关指南（先读这些） #    聚合基础  聚合性能优化  聚合场景实践  您可以设置以下参数：\n size - 返回的最大桶数量。默认 10。 min_doc_count - 返回匹配超过配置数量的 Top Hits 结果。我们不建议将 min_doc_count 设置为 1，因为它倾向于返回拼写错误或错别字。找到一个以上的词项实例有助于加强显著性不是偶然事件的结果。默认值 3 用于提供最小证据权重。 shard_size - 设置高值会增加稳定性（和准确性），但会牺牲计算性能。 shard_min_doc_count - 如果你的文本包含许多低频词，而你又不关心这些词（例如拼写错误），那么你可以将 shard_min_doc_count 参数设置为在分片级别上过滤候选词，以合理地确保即使合并本地显著文本频率也不会达到所需的 min_doc_count 。默认值为 1，直到你显式设置它之前没有影响。我们建议将此值设置得远低于 min_doc_count 值。 filter_duplicate_text - 是否过滤掉重复的文本段落（如模板化页眉/页脚）。默认 true。设为 false 可提升性能，但可能引入噪声。  假设你在一个 Easysearch 集群中索引了莎士比亚的全部作品。你可以在 text_entry 字段中找到与“breathe”相关的显著文本：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;breathe\u0026#34; } }, \u0026#34;aggregations\u0026#34;: { \u0026#34;my_sample\u0026#34;: { \u0026#34;sampler\u0026#34;: { \u0026#34;shard_size\u0026#34;: 100 }, \u0026#34;aggregations\u0026#34;: { \u0026#34;keywords\u0026#34;: { \u0026#34;significant_text\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;text_entry\u0026#34;, \u0026#34;min_doc_count\u0026#34;: 4 } } } } } } 返回内容\n\u0026#34;aggregations\u0026#34; : { \u0026#34;my_sample\u0026#34; : { \u0026#34;doc_count\u0026#34; : 59, \u0026#34;keywords\u0026#34; : { \u0026#34;doc_count\u0026#34; : 59, \u0026#34;bg_count\u0026#34; : 111396, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;breathe\u0026#34;, \u0026#34;doc_count\u0026#34; : 59, \u0026#34;score\u0026#34; : 1887.0677966101694, \u0026#34;bg_count\u0026#34; : 59 }, { \u0026#34;key\u0026#34; : \u0026#34;air\u0026#34;, \u0026#34;doc_count\u0026#34; : 4, \u0026#34;score\u0026#34; : 2.641295376716233, \u0026#34;bg_count\u0026#34; : 189 }, { \u0026#34;key\u0026#34; : \u0026#34;dead\u0026#34;, \u0026#34;doc_count\u0026#34; : 4, \u0026#34;score\u0026#34; : 0.9665839666414213, \u0026#34;bg_count\u0026#34; : 495 }, { \u0026#34;key\u0026#34; : \u0026#34;life\u0026#34;, \u0026#34;doc_count\u0026#34; : 5, \u0026#34;score\u0026#34; : 0.9090787433467572, \u0026#34;bg_count\u0026#34; : 805 } ] } } } } 与 breathe 最相关的文本是 air 、 dead 和 life 。\nsignificant_text 聚合有以下限制：\n 不支持子聚合，因为子聚合会带来较高的内存成本。作为解决方案，您可以使用带包含子句和子聚合的 terms 聚合添加一个后续查询。 不支持嵌套对象，因为它基于文档的 JSON 源进行工作。 文档计数可能会有一些（通常很小）的不准确，因为它基于从每个分片返回的样本进行求和。您可以使用 shard_size 参数来微调准确性和性能之间的权衡。默认情况下， shard_size 设置为 -1 以自动估计分片和 size 参数的数量。  统计信息中背景词频的默认来源是整个索引。您可以使用背景过滤器缩小此范围，以便更聚焦：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;breathe\u0026#34; } }, \u0026#34;aggregations\u0026#34;: { \u0026#34;my_sample\u0026#34;: { \u0026#34;sampler\u0026#34;: { \u0026#34;shard_size\u0026#34;: 100 }, \u0026#34;aggregations\u0026#34;: { \u0026#34;keywords\u0026#34;: { \u0026#34;significant_text\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;text_entry\u0026#34;, \u0026#34;background_filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;speaker\u0026#34;: \u0026#34;JOHN OF GAUNT\u0026#34; } } } } } } } } ","subcategory":null,"summary":"","tags":null,"title":"显著文本聚合（Significant Text）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/significant-text/"},{"category":null,"content":"Mapping 字符过滤器 #  mapping 字符过滤器接受一个用于字符替换的键值对映射。每当该过滤器遇到与某个键匹配的字符串时，它就会用相应的值来替换这些字符。替换值可以是空字符串。\n该过滤器采用贪婪匹配方式，这意味着会匹配最长的匹配结果。\n在分词过程之前，需要进行特定文本替换的场景下，mapping 字符过滤器会很有帮助。\n相关指南（先读这些） #    文本分析基础  文本分析：规范化  参考样例 #  以下请求配置了一个映射字符过滤器，该过滤器可将罗马数字（如 I、II 或 III）转换为对应的阿拉伯数字（1、2 和 3）：\nGET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;char_filter\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;mapping\u0026#34;, \u0026#34;mappings\u0026#34;: [ \u0026#34;I =\u0026gt; 1\u0026#34;, \u0026#34;II =\u0026gt; 2\u0026#34;, \u0026#34;III =\u0026gt; 3\u0026#34;, \u0026#34;IV =\u0026gt; 4\u0026#34;, \u0026#34;V =\u0026gt; 5\u0026#34; ] } ], \u0026#34;text\u0026#34;: \u0026#34;I have III apples and IV oranges\u0026#34; } 返回内容中包含一个词元，其中罗马数字已被替换为阿拉伯数字：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;1 have 3 apples and 4 oranges\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 32, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } 参数说明 #  你可以使用以下任意一个参数来配置键值映射。\n   参数 必需/可选 数据类型 描述     mappings 可选 数组 格式为 key =\u0026gt; value 的键值对数组。在输入文本中找到的每个键都将被其对应的值替换。   mappings_path 可选 字符串 包含键值映射的 UTF-8 编码文件的路径。每个映射应在新的一行中以 key =\u0026gt; value 的格式呈现。该路径可以是绝对路径，也可以是相对于 Easysearch 配置目录的相对路径。    使用自定义映射字符过滤器 #  你可以通过定义自己的映射集来创建自定义映射字符过滤器。以下请求将创建一个自定义字符过滤器，用于替换文本中的常见缩写：\nPUT /test-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_abbr_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;char_filter\u0026#34;: [ \u0026#34;custom_abbr_filter\u0026#34; ] } }, \u0026#34;char_filter\u0026#34;: { \u0026#34;custom_abbr_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mapping\u0026#34;, \u0026#34;mappings\u0026#34;: [ \u0026#34;BTW =\u0026gt; By the way\u0026#34;, \u0026#34;IDK =\u0026gt; I don\u0026#39;t know\u0026#34;, \u0026#34;FYI =\u0026gt; For your information\u0026#34; ] } } } } } 使用以下请求来检查使用该分词器生成的词元：\nGET /test-index/_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;char_filter\u0026#34;: [ \u0026#34;custom_abbr_filter\u0026#34; ], \u0026#34;text\u0026#34;: \u0026#34;FYI, updates to the workout schedule are posted. IDK when it takes effect, but we have some details. BTW, the finalized schedule will be released Monday.\u0026#34; } 返回内容显示这些缩写已被替换：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;For your information, updates to the workout schedule are posted. I don\u0026#39;t know when it takes effect, but we have some details. By the way, the finalized schedule will be released Monday.\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 153, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"映射字符过滤器（Mapping）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/character-filters/mapping/"},{"category":null,"content":"日期范围聚合 #  date_range 聚合在概念上与 range 聚合相同，只是它允许执行日期计算。例如，你可以获取过去 10 天内的所有文档。为了使日期更易读，可以使用 format 参数包含格式：\n相关指南（先读这些） #    聚合基础  聚合场景实践  时间序列建模  GET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;number_of_bytes\u0026#34;: { \u0026#34;date_range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;MM-yyyy\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;now-10d/d\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; } ] } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;number_of_bytes\u0026#34; : { \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;03-2021-03-2021\u0026#34;, \u0026#34;from\u0026#34; : 1.6145568E12, \u0026#34;from_as_string\u0026#34; : \u0026#34;03-2021\u0026#34;, \u0026#34;to\u0026#34; : 1.615451329043E12, \u0026#34;to_as_string\u0026#34; : \u0026#34;03-2021\u0026#34;, \u0026#34;doc_count\u0026#34; : 0 } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"日期范围聚合（Date Range）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/date-range/"},{"category":null,"content":"日期直方图聚合 #  date_histogram 聚合使用日期计算来为时间序列数据生成直方图。\n相关指南（先读这些） #    聚合基础  聚合场景实践  时间序列建模  例如，您可以找到您的网站每月有多少次访问：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;logs_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;month\u0026#34; } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;logs_per_month\u0026#34; : { \u0026#34;buckets\u0026#34; : [ { \u0026#34;key_as_string\u0026#34; : \u0026#34;2020-10-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34; : 1601510400000, \u0026#34;doc_count\u0026#34; : 1635 }, { \u0026#34;key_as_string\u0026#34; : \u0026#34;2020-11-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34; : 1604188800000, \u0026#34;doc_count\u0026#34; : 6844 }, { \u0026#34;key_as_string\u0026#34; : \u0026#34;2020-12-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34; : 1606780800000, \u0026#34;doc_count\u0026#34; : 5595 } ] } } 返回内容包含三个月的日志。如果你绘制这些值，你可以看到你的网站每月请求流量的峰值和低谷。\n参数说明 #  date_histogram 聚合支持以下参数。\n   参数 必需/可选 数据类型 描述     date_histogram\t 必填 Object 一个指定日期时间文档字段、间隔、可选格式和时区的对象。   calendar_interval\t 必填 时间间隔 构建每个分组所使用的日期字段。   format\t 可选 String 日期格式字符串。如果省略，日期将输出为 64 位自纪元以来的毫秒整数。   time_zone\t 可选 String 表示 UTC 时间偏移的字符串，可以是 ISO 8601 UTC 偏移（\u0026quot;-07:00\u0026quot;）或 tz 数据库标识符（\u0026ldquo;America/Los_Angeles\u0026rdquo;）。    interval 与 calendar_interval / fixed_interval #  interval 参数已被标记为弃用，建议使用以下两个参数之一：\n   参数 说明     calendar_interval 基于日历的间隔，能感知月份天数和闰年。有效值：minute、hour、day、week、month、quarter、year   fixed_interval 固定长度间隔，不感知日历。有效值如 30m、1h、12h、1d    例如，按周统计日志量：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;logs_per_week\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;week\u0026#34; } } } } 设置时区 #  日期直方图默认使用 UTC 时区。对于中国地区的数据，通常需要设置时区以获得正确的日期分桶边界：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;logs_per_day\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;day\u0026#34;, \u0026#34;time_zone\u0026#34;: \u0026#34;+08:00\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34; } } } } 空桶填充 #  默认情况下，没有文档的时间桶不会在结果中出现。设置 min_doc_count 为 0 可以显示空桶，配合 extended_bounds 可以确保结果覆盖指定的完整时间范围：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;logs_per_day\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;day\u0026#34;, \u0026#34;min_doc_count\u0026#34;: 0, \u0026#34;extended_bounds\u0026#34;: { \u0026#34;min\u0026#34;: \u0026#34;2020-10-01\u0026#34;, \u0026#34;max\u0026#34;: \u0026#34;2020-12-31\u0026#34; } } } } }  提示：extended_bounds 不会过滤文档，只是确保在结果中包含该范围内的所有桶，即使桶内没有文档。\n ","subcategory":null,"summary":"","tags":null,"title":"日期直方图聚合（Date Histogram）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/date-histogram/"},{"category":null,"content":"Text 字段类型 #  text 字段类型包含经过分析器分析的字符串。它用于全文搜索，因为它允许部分匹配。对多个词条的搜索可以匹配其中的一部分而不是全部。根据分析器的不同，搜索结果可以是大小写不敏感的、词干化的、去除停用词的、应用同义词的等等。\n 提示：如果您需要进行精确值搜索，请将字段映射为 keyword 类型。\n 相关指南（先读这些） #    映射基础  映射模式  全文搜索  代码样例 #  创建一个带有 text 字段的映射：\nPUT movies { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;title\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34; } } } } 参数说明 #  下表列出了 text 字段类型接受的参数。所有参数都是可选的。\n   参数 描述     analyzer 用于此字段的分词器。默认情况下，它将在索引时和搜索时使用。要在搜索时覆盖它，请设置 search_analyzer 参数。默认是 standard 分词器，它使用基于语法的分词，并基于 Unicode 文本分段算法。   boost 浮点值，指定此字段对相关性分数的权重。大于 1.0 的值会增加字段的相关性。0.0 到 1.0 之间的值会降低字段的相关性。默认值为 1.0。   eager_global_ordinals 指定是否应在刷新时立即加载全局序号。如果该字段经常用于聚合，此参数应设置为 true。默认值为 false。   fielddata 布尔值，指定是否访问此字段的已分析标记以进行排序、聚合和脚本编写。默认值为 false。   fielddata_frequency_filter JSON 对象，指定仅将文档频率在 min 和 max 值之间的已分析标记加载到内存中（以绝对数字或百分比提供）。频率按段计算。参数：min、max、min_segment_size。默认加载所有已分析的标记。   fields 要以多种方式索引同一个字符串（例如，作为 keyword 和 text），请提供 fields 参数。您可以指定字段的一个版本用于搜索，另一个版本用于排序和聚合。   index 布尔值，指定字段是否应可搜索。默认值为 true。   index_options 指定要存储在索引中用于搜索和突出显示的信息。有效值：docs（仅文档编号）、freqs（文档编号和词频）、positions（文档编号、词频和词位置）、offsets（文档编号、词频、词位置以及开始和结束字符偏移量）。默认值为 positions。   index_phrases 布尔值，指定是否单独索引 2-gram。2-gram 是此字段字符串中两个连续单词的组合。导致精确短语查询更快但索引更大。当不删除停用词时效果最好。默认值为 false。   index_prefixes JSON 对象，指定单独索引词条前缀。前缀中的字符数在 min_chars 和 max_chars 之间（包含）。导致前缀搜索更快但索引更大。可选参数：min_chars、max_chars。默认 min_chars 为 2，max_chars 为 5。   meta 接受此字段的元数据。   norms 布尔值，指定在计算相关性分数时是否应使用字段长度。默认值为 true。norms 对全文搜索的评分非常重要，但会占用额外的磁盘空间。如果不需要对该字段进行评分（例如仅用于过滤），可设为 false。   position_increment_gap 当文本字段被分析时，它们被分配位置。如果一个字段包含一个字符串数组，并且这些位置是连续的，这将导致可能跨不同数组元素匹配。为防止这种情况，在连续的数组元素之间插入一个人工间隙。您可以通过指定整数 position_increment_gap 来更改此间隙。注意：如果 slop 大于 position_element_gap，可能会发生跨不同数组元素的匹配。默认值为 100。   similarity 用于计算相关性分数的排名算法。默认值为 BM25。   term_vector 布尔值，指定是否应存储此字段的词向量。默认值为 no。    词向量参数 #  词向量在分析过程中产生。它包含：\n 词条列表 每个词条的序数位置 搜索字符串在字段中的起始和结束字符偏移量 负载 Payloads（如果可用）。每个词条可以有与词条位置相关的自定义二进制数据  term_vector 字段包含一个接受以下参数的 JSON 对象：\n   参数 存储的值     no 无。这是默认值。   yes 字段中的词条。   with_offsets 词条和字符偏移量。   with_positions_offsets 词条、位置和字符偏移量。   with_positions_offsets_payloads 词条、位置、字符偏移量和负载。   with_positions 词条和位置。   with_positions_payloads 词条、位置和负载。     存储位置对于邻近查询很有用。存储字符偏移量对于突出显示很有用。\n 词向量参数示例 #  创建一个带有存储字符偏移量的词向量的文本字段的映射：\nPUT testindex { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;dob\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;with_positions_offsets\u0026#34; } } } } 索引一个带有文本字段的文档：\nPUT testindex/_doc/1 { \u0026#34;dob\u0026#34; : \u0026#34;The patient\u0026#39;s date of birth.\u0026#34; } 查询\u0026quot;date of birth\u0026quot;并在原始字段中突出显示它：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;dob\u0026#34;: \u0026#34;date of birth\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;dob\u0026#34;: {} } } } 返回内容中的\u0026quot;date of birth\u0026quot;被突出显示：\n{ \u0026#34;took\u0026#34;: 854, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.8630463, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.8630463, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;The patient\u0026#39;s date of birth.\u0026#34; }, \u0026#34;highlight\u0026#34;: { \u0026#34;text\u0026#34;: [\u0026#34;The patient\u0026#39;s \u0026lt;em\u0026gt;date\u0026lt;/em\u0026gt; \u0026lt;em\u0026gt;of\u0026lt;/em\u0026gt; \u0026lt;em\u0026gt;birth\u0026lt;/em\u0026gt;.\u0026#34;] } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"文本字段类型（Text）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/string-field-type/text/"},{"category":null,"content":"数据脱敏 #  如果您的数据里面包含一些敏感信息，除了通过 字段级权限 来进行访问控制，您还可以通过混淆字段里面的内容来进行脱敏。目前，字段数据脱敏仅适用于基于字符串的字段，支持加密哈希和正则替换字段的内容。\n字段脱敏与字段级权限一起可以在相同的角色级别和索引级别上工作。您可以允许某些角色查看明文格式的敏感字段，并为其他角色脱敏这些字段。带有脱敏字段的搜索结果可能如下所示：\n{ \u0026#34;_index\u0026#34;: \u0026#34;movies\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;year\u0026#34;: 2013, \u0026#34;directors\u0026#34;: [\u0026#34;Ron Howard\u0026#34;], \u0026#34;title\u0026#34;: \u0026#34;ca998e768dd2e6cdd84c77015feb29975f9f498a472743f159bec6f1f1db109e\u0026#34; } } 设置盐值 #  可以在 easysearch.yml 设置一个随机字符串:\nsecurity.compliance.salt: abcdefghijklmnopqrstuvqxyz1234567890    属性 说明     security.compliance.salt 生成哈希值时要使用的盐值。必须至少为 32 个字符。仅允许使用 ASCII 字符。选填。    配置脱敏字段 #  role.yml #  masked_movie: cluster: [] indices: - names: - movies privileges: - \u0026#34;read\u0026#34; field_mask: - \u0026#34;genres\u0026#34; - \u0026#34;title\u0026#34; REST API #  参照 创建角色.\n使用其它哈希算法 #  默认情况下，安全模块使用 BLAKE2b 算法，但您可以使用 JVM 提供的任何哈希算法。此列表通常包括 MD5、SHA-1、SHA-384 和 SHA-512。\n要指定其它算法，请将其添加到脱敏字段之后：\nmasked_movie: cluster: [] indices: - names: - movies privileges: - \u0026#34;read\u0026#34; field_mask: - \u0026#34;genres::SHA-512\u0026#34; - \u0026#34;title\u0026#34; 基于规则的字段脱敏 #  除了使用哈希，您还可以使用一个或多个正则表达式来替换字符串从而达到字段脱敏的效果。语法是 \u0026lt;field\u0026gt;::/\u0026lt;regular-expression\u0026gt;/::\u0026lt;replacement-string\u0026gt; 。如果使用多个正则表达式，则结果将从左向右传递，就像 shell 中的管道操作一样：\nmasked_movie: cluster: [] indices: - names: - movies privileges: - \u0026#34;read\u0026#34; field_mask: - \u0026#34;genres::/^[a-zA-Z]{1,3}/::XXX::/[a-zA-Z]{1,3}$/::YYY\u0026#34; - \u0026#34;title::/./::*\u0026#34; title 字段里面的表达式将字段中的每个字符更改为 *，因此您仍然可以辨别脱敏后字符串的长度。genres 字段的表达式将字符串的前三个字符更改为 XXX，将最后三个字符更改为 YYY。\n完整测试脚本如下：\nPOST movies/_doc/1 { \u0026#34;year\u0026#34;: 2013, \u0026#34;title\u0026#34;: \u0026#34;Rush\u0026#34;, \u0026#34;actors\u0026#34;: [ \u0026#34;Daniel Brühl\u0026#34;, \u0026#34;Chris Hemsworth\u0026#34;, \u0026#34;Olivia Wilde\u0026#34; ] } POST movies/_doc/2 { \u0026quot;directors\u0026quot;: [ \u0026quot;Ron Howard\u0026quot; ], \u0026quot;plot\u0026quot;: \u0026quot;A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\u0026quot;, \u0026quot;genres\u0026quot;: [ \u0026quot;Action\u0026quot;, \u0026quot;Biography\u0026quot;, \u0026quot;Drama\u0026quot;, \u0026quot;Sport\u0026quot; ] }\nPUT _security/user/medcl { \u0026quot;password\u0026quot;: \u0026quot;pass\u0026quot;, \u0026quot;roles\u0026quot;: [\u0026quot;masked_movie\u0026quot;] }\ncurl -XGET -k 'https://localhost:9200/movies/_search?pretty' -u medcl:pass\n{ \u0026quot;took\u0026quot; : 27, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 2, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.0, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;movies\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;actors\u0026quot; : [ \u0026quot;Daniel Brühl\u0026quot;, \u0026quot;Chris Hemsworth\u0026quot;, \u0026quot;Olivia Wilde\u0026quot; ], \u0026quot;year\u0026quot; : 2013, \u0026quot;title\u0026quot; : \u0026quot;****\u0026quot; } }, { \u0026quot;_index\u0026quot; : \u0026quot;movies\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;plot\u0026quot; : \u0026quot;A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\u0026quot;, \u0026quot;genres\u0026quot; : [ \u0026quot;XXXYYY\u0026quot;, \u0026quot;XXXgraYYY\u0026quot;, \u0026quot;XXYYY\u0026quot;, \u0026quot;XXYYY\u0026quot; ], \u0026quot;directors\u0026quot; : [ \u0026quot;Ron Howard\u0026quot; ] } } ] } } 对审计日志的影响 #\n 读取历史记录可让您跟踪对文档中敏感字段的读取操作。例如，您可以跟踪对客户记录的电子邮件字段的访问。对脱敏字段的访问从读取历史记录中排除了，因为用户只能看到哈希值，而不是字段的明文值。\n","subcategory":null,"summary":"","tags":null,"title":"数据脱敏","url":"/easysearch/main/docs/operations/security/access-control/field-masking/"},{"category":null,"content":"故障排查 #  本文提供一条\u0026quot;遇到问题时可以照着走\u0026quot;的诊断路线，包含具体的诊断命令和解决方案。\n 排查总体思路 #  遇到问题时，先快速判断三个问题：\n┌─────────────────────────────────────────────────────────────────────────┐ │ 1. 影响范围：单个索引/租户？还是整个集群？ │ ├─────────────────────────────────────────────────────────────────────────┤ │ 2. 症状类型：性能下降？错误增多？节点异常？数据不符合预期？ │ ├─────────────────────────────────────────────────────────────────────────┤ │ 3. 时间维度：长期慢性问题？还是某个时间点突然恶化？ │ └─────────────────────────────────────────────────────────────────────────┘  问题 1：集群状态变红/黄 #  快速诊断 #  # 1. 检查集群状态 GET _cluster/health 2. 查看未分配分片 GET _cat/shards?v\u0026amp;h=index,shard,prirep,state,unassigned.reason\u0026amp;s=state\n3. 分析未分配原因 GET _cluster/allocation/explain 常见原因与解决方案 #\n 原因 1：节点离线 #  诊断：\nGET _cat/nodes?v 检查节点数是否符合预期，缺失的节点需要检查：\n 进程是否存活：ps aux | grep easysearch 系统日志：journalctl -u easysearch Easysearch 日志：logs/{cluster_name}.log  解决方案：\n 恢复故障节点 如果节点无法恢复且有副本，集群会自动重新分配 如果是主分片丢失且无副本，需要从快照恢复  原因 2：磁盘空间不足 #  诊断：\nGET _cat/nodes?v\u0026amp;h=name,disk.used_percent,disk.avail GET _cluster/allocation/explain 典型响应：\n{ \u0026#34;deciders\u0026#34;: [ { \u0026#34;decider\u0026#34;: \u0026#34;disk_threshold\u0026#34;, \u0026#34;decision\u0026#34;: \u0026#34;NO\u0026#34;, \u0026#34;explanation\u0026#34;: \u0026#34;the node is above the high watermark [90%]\u0026#34; } ] } 解决方案：\n# 1. 查看磁盘使用情况 GET _cat/allocation?v 2. 清理旧索引（如果允许） DELETE old-logs-2025*\n3. 临时调整水位阈值（应急） PUT _cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;cluster.routing.allocation.disk.watermark.low\u0026quot;: \u0026quot;90%\u0026quot;, \u0026quot;cluster.routing.allocation.disk.watermark.high\u0026quot;: \u0026quot;95%\u0026quot;, \u0026quot;cluster.routing.allocation.disk.watermark.flood_stage\u0026quot;: \u0026quot;97%\u0026quot; } }\n4. 解除只读状态（如果被触发） PUT _all/_settings { \u0026quot;index.blocks.read_only_allow_delete\u0026quot;: null } 原因 3：分片分配设置问题 #\n 诊断：\n# 检查索引设置 GET problematic-index/_settings?filter_path=*.settings.index.routing 检查集群设置 GET _cluster/settings?include_defaults=true\u0026amp;filter_path=*.cluster.routing 常见问题：\n 索引指定了不存在的节点属性 集群分配被禁用  解决方案：\n# 修复索引分配设置 PUT problematic-index/_settings { \u0026#34;index.routing.allocation.include._name\u0026#34;: null, \u0026#34;index.routing.allocation.require._name\u0026#34;: null } 重新启用集群分配 PUT _cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;cluster.routing.allocation.enable\u0026quot;: \u0026quot;all\u0026quot; } } 原因 4：主分片丢失无法恢复 #\n 诊断：\nGET _cluster/allocation/explain { \u0026#34;index\u0026#34;: \u0026#34;problematic-index\u0026#34;, \u0026#34;shard\u0026#34;: 0, \u0026#34;primary\u0026#34;: true } 最后手段（会丢失数据）：\n# 分配空的主分片（仅在确认数据可以丢失时使用） POST _cluster/reroute { \u0026#34;commands\u0026#34;: [ { \u0026#34;allocate_empty_primary\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;problematic-index\u0026#34;, \u0026#34;shard\u0026#34;: 0, \u0026#34;node\u0026#34;: \u0026#34;node-1\u0026#34;, \u0026#34;accept_data_loss\u0026#34;: true } } ] }  问题 2：搜索/写入延迟升高 #  快速诊断 #  # 1. 检查节点资源 GET _cat/nodes?v\u0026amp;h=name,cpu,heap.percent,load_1m,disk.used_percent 2. 检查热点线程 GET _nodes/hot_threads\n3. 检查任务积压 GET _cat/pending_tasks?v GET _cat/thread_pool?v\u0026amp;h=node_name,name,active,queue,rejected 搜索延迟问题 #\n 诊断：找出慢查询 #  # 1. 配置慢查询日志 PUT /my-index/_settings { \u0026#34;index.search.slowlog.threshold.query.warn\u0026#34;: \u0026#34;5s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.info\u0026#34;: \u0026#34;2s\u0026#34; } 2. 分析查询性能 GET /my-index/_search { \u0026quot;profile\u0026quot;: true, \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;content\u0026quot;: \u0026quot;slow query\u0026quot; } } } 常见原因与解决方案 #\n    原因 诊断特征 解决方案     深度分页 from 值很大 使用 search_after 替代   高基数聚合 aggregation 耗时长 限制 size，使用近似聚合   通配符开头查询 wildcard 以 * 开头 改用 ngram 或前缀匹配   大结果集 size 值很大 减小 size，分批获取   分片过多 单查询命中数百分片 减少分片数，使用路由    优化深度分页 #  # 问题：深度分页性能差 GET /logs/_search { \u0026#34;from\u0026#34;: 10000, \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 解决：使用 search_after GET /logs/_search { \u0026quot;size\u0026quot;: 10, \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} }, \u0026quot;sort\u0026quot;: [{ \u0026quot;@timestamp\u0026quot;: \u0026quot;desc\u0026quot; }, { \u0026quot;_id\u0026quot;: \u0026quot;asc\u0026quot; }], \u0026quot;search_after\u0026quot;: [\u0026quot;2026-02-11T10:00:00.000Z\u0026quot;, \u0026quot;doc-id-123\u0026quot;] } 写入延迟问题 #\n 诊断：检查写入队列 #  # 1. 检查 bulk 队列 GET _cat/thread_pool/write?v\u0026amp;h=node_name,name,active,queue,rejected 2. 检查段合并压力 GET _cat/nodes?v\u0026amp;h=name,merges.current,merges.total\n3. 检查 refresh/flush 状态 GET _cat/indices?v\u0026amp;h=index,refresh.time,flush.total 常见原因与解决方案 #\n    原因 诊断特征 解决方案     Bulk 批次过大 单次请求超过 100MB 减小批次（5-15MB）   并发过高 queue 接近满，rejected \u0026gt; 0 降低写入并发   Mapping 爆炸 字段数不断增长 限制 total_fields.limit   频繁更新 大量 update/delete 改为追加写入 + 版本控制   合并压力 merges.current 持续高 调整 merge 策略    优化写入性能 #  # 1. 调整 refresh 间隔（写入期间） PUT /my-index/_settings { \u0026#34;index.refresh_interval\u0026#34;: \u0026#34;30s\u0026#34; } 2. 临时减少副本（批量导入期间） PUT /my-index/_settings { \u0026quot;index.number_of_replicas\u0026quot;: 0 }\n3. 调整 translog 策略 PUT /my-index/_settings { \u0026quot;index.translog.durability\u0026quot;: \u0026quot;async\u0026quot;, \u0026quot;index.translog.sync_interval\u0026quot;: \u0026quot;30s\u0026quot; } \n问题 3：节点频繁重启/离开集群 #  快速诊断 #  # 1. 检查系统日志 journalctl -u easysearch --since \u0026#34;1 hour ago\u0026#34; # 2. 检查 Easysearch 日志 tail -f logs/easysearch.log | grep -E \u0026quot;ERROR|WARN|OOM\u0026quot;\n# 3. 检查 GC 日志 cat logs/gc.log | grep \u0026quot;Full GC\u0026quot; 常见原因与解决方案 #\n 原因 1：内存不足 / OOM #  诊断特征：\njava.lang.OutOfMemoryError: Java heap space 解决方案：\n 检查堆内存设置：不超过可用内存的 50%，不超过 32GB 检查是否有内存泄漏的查询（如超大聚合） 增加节点或减少数据量  # 查看当前堆设置 grep -E \u0026#34;^-Xm[sx]\u0026#34; config/jvm.options # 调整堆内存（重启生效） # -Xms16g # -Xmx16g 原因 2：GC 压力过大 #\n 诊断：\nGET _nodes/stats/jvm?filter_path=nodes.*.jvm.gc 解决方案：\n 减小堆内存（如果接近 32GB） 检查是否有大量 Fielddata 加载 升级到 G1GC（推荐）  原因 3：磁盘 I/O 瓶颈 #  诊断：\n# 检查 I/O 等待 iostat -x 1 # 检查磁盘延迟 GET _nodes/stats/fs 解决方案：\n 升级到 SSD 检查是否有合并风暴 分散热点索引到多个节点   问题 4：查询结果不符合预期 #  搜不到应该存在的数据 #  诊断：\n# 1. 确认文档存在 GET /my-index/_doc/document-id 2. 检查 Mapping GET /my-index/_mapping\n3. 分析字段如何被分词 GET /my-index/_analyze { \u0026quot;field\u0026quot;: \u0026quot;content\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;搜索的关键词\u0026quot; }\n4. 检查查询如何解析 GET /my-index/_validate/query?explain { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;content\u0026quot;: \u0026quot;关键词\u0026quot; } } } 常见原因：\n   原因 诊断方法 解决方案     字段类型错误 _mapping 显示 keyword 而非 text 重建索引   分词器不匹配 _analyze 结果与预期不符 调整 analyzer   refresh 延迟 刚写入的数据 等待或手动 refresh   路由问题 使用了自定义路由 查询时指定路由    排序/相关性不合理 #  诊断：\nGET /my-index/_search { \u0026#34;explain\u0026#34;: true, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;关键词\u0026#34; } } } 查看每个文档的评分解释，分析是否有意外的加权。\n 问题 5：Bulk 写入被拒绝 #  诊断 #  # 1. 检查 rejected 计数 GET _cat/thread_pool/write?v 2. 检查队列设置 GET _cluster/settings?include_defaults=true\u0026amp;filter_path=*.thread_pool.write 典型错误：\n{ \u0026#34;error\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;es_rejected_execution_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;rejected execution of processing of [...]\u0026#34; } } 解决方案 #  # 1. 降低客户端并发 # 在客户端侧减少并发线程数 2. 减小 bulk 批次大小 每批 5-15MB，而不是 100MB 3. 增加队列大小（谨慎） PUT _cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;thread_pool.write.queue_size\u0026quot;: 1000 } }\n4. 客户端实现重试和指数退避 捕获 429/503 错误，等待后重试 \n诊断命令速查表 #  集群状态 #  # 整体健康 GET _cluster/health?pretty # 节点列表 GET _cat/nodes?v\u0026amp;h=name,ip,heap.percent,ram.percent,cpu,load_1m,node.role\n# 分片分布 GET _cat/shards?v\u0026amp;s=state\n# 未分配原因 GET _cluster/allocation/explain\n# 待处理任务 GET _cat/pending_tasks?v 性能诊断 #\n # 热点线程 GET _nodes/hot_threads # 线程池状态 GET _cat/thread_pool?v\u0026amp;h=node_name,name,active,queue,rejected\n# 段信息 GET _cat/segments?v\u0026amp;h=index,shard,segment,size\n# 索引统计 GET _cat/indices?v\u0026amp;s=store.size:desc 日志检查 #\n # 查看错误日志 grep -E \u0026#34;ERROR|Exception\u0026#34; logs/easysearch.log | tail -100 # 查看慢查询日志 tail -f logs/easysearch_index_search_slowlog.log\n# 查看 GC 日志 grep \u0026quot;Full GC\u0026quot; logs/gc.log \n小结 #     问题类型 首要诊断 关键 API     集群变红 未分配分片原因 _cluster/allocation/explain   性能下降 资源使用 + 热点线程 _cat/nodes、_nodes/hot_threads   节点异常 系统日志 + GC 日志 操作系统层面   查询异常 Mapping + Analyze _mapping、_analyze   写入拒绝 线程池队列 _cat/thread_pool    排查原则：\n 先看影响面，再缩小范围 先看监控趋势，再看实时状态 先排除资源问题，再查业务问题 保留现场（日志、配置），便于事后分析  下一步阅读：\n  监控：建立预警机制  容量规划：避免资源瓶颈  配置：优化集群配置  ","subcategory":null,"summary":"","tags":null,"title":"故障排查","url":"/easysearch/main/docs/operations/troubleshooting/"},{"category":null,"content":"Rank 字段类型 #  下表列出了 Easysearch 支持的所有 rank 字段类型。\n   字段数据类型 描述     rank_feature 提升或降低文档的相关性得分。   rank_features 提升或降低文档的相关性得分。用于特征列表稀疏的情况。     注意：rank_feature 和 rank_features 字段只能使用 rank_feature 查询进行查询。它们不支持聚合或排序。\n 相关指南（先读这些） #    映射基础  相关性：加权与调参  排序功能查询  Rank feature 字段类型 #  Rank feature 字段类型使用正浮点值来提升或降低文档在 rank_feature 查询中的相关性得分。默认情况下，该值会提升相关性得分。要降低相关性得分，请将可选参数 positive_score_impact 设置为 false。\n示例 #  创建一个包含 rank feature 字段的映射：\nPUT chessplayers { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;rating\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;rank_feature\u0026#34; }, \u0026#34;age\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;rank_feature\u0026#34;, \u0026#34;positive_score_impact\u0026#34;: false } } } } 索引三个文档，其中包含一个提升得分的 rank_feature 字段（rating）和一个降低得分的 rank_feature 字段（age）：\nPUT chessplayers/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;rating\u0026#34;: 2554, \u0026#34;age\u0026#34;: 75 } PUT chessplayers/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Kwaku Mensah\u0026quot;, \u0026quot;rating\u0026quot;: 2067, \u0026quot;age\u0026quot;: 10 }\nPUT chessplayers/_doc/3 { \u0026quot;name\u0026quot;: \u0026quot;Nikki Wolf\u0026quot;, \u0026quot;rating\u0026quot;: 1864, \u0026quot;age\u0026quot;: 22 } Rank feature 查询 #\n 使用 rank feature 查询，您可以按评分、年龄或同时按评分和年龄对选手进行排名。如果按评分排名，评分较高的选手将获得更高的相关性得分。如果按年龄排名，年龄较小的选手将获得更高的相关性得分。\n使用 rank feature 查询根据年龄和评分搜索选手：\nGET chessplayers/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;rating\u0026#34; } }, { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;age\u0026#34; } } ] } } } 当同时按年龄和评分排名时，年龄较小和评分较高的选手得分更好：\n{ \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.2093145, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;chessplayers\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.2093145, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Kwaku Mensah\u0026#34;, \u0026#34;rating\u0026#34;: 1967, \u0026#34;age\u0026#34;: 10 } }, { \u0026#34;_index\u0026#34;: \u0026#34;chessplayers\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.0150313, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Nikki Wolf\u0026#34;, \u0026#34;rating\u0026#34;: 1864, \u0026#34;age\u0026#34;: 22 } }, { \u0026#34;_index\u0026#34;: \u0026#34;chessplayers\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.8098284, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;rating\u0026#34;: 2554, \u0026#34;age\u0026#34;: 75 } } ] } } Rank features 字段类型 #  Rank features 字段类型与 rank feature 字段类型类似，但它更适合用于稀疏特征列表。Rank features 字段可以索引数值特征向量，这些向量后续用于在 rank_feature 查询中提升或降低文档的相关性得分。\n示例 #  创建一个包含 rank features 字段的映射：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;correlations\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;rank_features\u0026#34; } } } } 要索引包含 rank features 字段的文档，请使用带有字符串键和正浮点值的哈希映射：\nPUT testindex1/_doc/1 { \u0026#34;correlations\u0026#34;: { \u0026#34;young kids\u0026#34;: 1, \u0026#34;older kids\u0026#34;: 15, \u0026#34;teens\u0026#34;: 25.9 } } PUT testindex1/_doc/2 { \u0026quot;correlations\u0026quot;: { \u0026quot;teens\u0026quot;: 10, \u0026quot;adults\u0026quot;: 95.7 } } 使用 rank feature 查询检索文档：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;correlations.teens\u0026#34; } } } 响应按相关性得分排序：\n{ \u0026#34;took\u0026#34;: 123, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.6258503, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.6258503, \u0026#34;_source\u0026#34;: { \u0026#34;correlations\u0026#34;: { \u0026#34;young kids\u0026#34;: 1, \u0026#34;older kids\u0026#34;: 15, \u0026#34;teens\u0026#34;: 25.9 } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.39263803, \u0026#34;_source\u0026#34;: { \u0026#34;correlations\u0026#34;: { \u0026#34;teens\u0026#34;: 10, \u0026#34;adults\u0026#34;: 95.7 } } } ] } }  Rank feature 和 rank features 字段使用前 9 个有效位来保证精度，导致大约 0.4% 的相对误差。值的存储相对精度为 2^−8 = 0.00390625。\n ","subcategory":null,"summary":"","tags":null,"title":"排名特征字段类型（Rank Feature）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/rank/"},{"category":null,"content":"扩展统计聚合 #  extended_stats 聚合是 stats 聚合的更全面版本。除了 stats 提供的基本统计指标外，extended_stats 还计算以下内容：\n相关指南（先读这些） #     聚合基础\n   聚合场景实践\n  平方和\n  方差\n  总体方差\n  抽样方差\n  标准差\n  总体标准差\n  抽样标准差\n  标准差界限： ** 上限 ** 下限 ** 总体上限 ** 总体下限 ** 抽样上限 ** 抽样下限\n  其中标准差和方差是总体统计量；它们分别始终等于总体标准差和方差。\nstd_deviation_bounds 对象定义了一个范围，该范围在均值（默认为两个标准差）的上方和下方跨越指定的标准差数量。此对象始终包含在输出中，但仅对正态分布的数据才有意义。在解释这些值之前，请验证您的数据集是否遵循正态分布。\n参数说明 #  extended_stats 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 返回扩展统计信息的字段名称。   sigma 可选 Double（非负） 计算 std_deviation_bounds 区间所使用的均值上下标准差的数量。默认值为 2 。   missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，包含缺失值的文档将不会出现在扩展统计中。    参考样例 #  以下示例请求数据中 taxful_total_price 的扩展统计信息：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;extended_stats_taxful_total_price\u0026#34;: { \u0026#34;extended_stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } } } } 返回内容 #  该内容包含 taxful_total_price 的扩展统计信息：\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;extended_stats_taxful_total_price\u0026#34; : { \u0026#34;count\u0026#34; : 4675, \u0026#34;min\u0026#34; : 6.98828125, \u0026#34;max\u0026#34; : 2250.0, \u0026#34;avg\u0026#34; : 75.05542864304813, \u0026#34;sum\u0026#34; : 350884.12890625, \u0026#34;sum_of_squares\u0026#34; : 3.9367749294174194E7, \u0026#34;variance\u0026#34; : 2787.59157113862, \u0026#34;variance_population\u0026#34; : 2787.59157113862, \u0026#34;variance_sampling\u0026#34; : 2788.187974983536, \u0026#34;std_deviation\u0026#34; : 52.79764740155209, \u0026#34;std_deviation_population\u0026#34; : 52.79764740155209, \u0026#34;std_deviation_sampling\u0026#34; : 52.80329511482722, \u0026#34;std_deviation_bounds\u0026#34; : { \u0026#34;upper\u0026#34; : 180.6507234461523, \u0026#34;lower\u0026#34; : -30.53986616005605, \u0026#34;upper_population\u0026#34; : 180.6507234461523, \u0026#34;lower_population\u0026#34; : -30.53986616005605, \u0026#34;upper_sampling\u0026#34; : 180.66201887270256, \u0026#34;lower_sampling\u0026#34; : -30.551161586606312 } } } } 定义计算边界 #  可以通过将 sigma 参数设置为任何非负值来定义用于计算 std_deviation_bounds 区间的标准偏差数。\n示例：定义计算边界 #  设置标准偏差的数量为 std_deviation_bounds 到 3 :\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;extended_stats_taxful_total_price\u0026#34;: { \u0026#34;extended_stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34;, \u0026#34;sigma\u0026#34;: 3 } } } } 这会改变标准差界限：\n{ ... \u0026#34;aggregations\u0026#34;: { ... \u0026#34;std_deviation_bounds\u0026#34;: { \u0026#34;upper\u0026#34;: 233.44837084770438, \u0026#34;lower\u0026#34;: -83.33751356160813, \u0026#34;upper_population\u0026#34;: 233.44837084770438, \u0026#34;lower_population\u0026#34;: -83.33751356160813, \u0026#34;upper_sampling\u0026#34;: 233.46531398752978, \u0026#34;lower_sampling\u0026#34;: -83.35445670143353 } } } } 缺省值处理 #  可以为聚合字段的缺失实例分配一个值。有关更多信息，请参阅缺失聚合。 准备一个示例索引，通过导入以下文档：\nPOST _bulk { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;gpa\u0026#34;: 3.89, \u0026#34;grad_year\u0026#34;: 2022} { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Jonathan Powers\u0026#34;, \u0026#34;grad_year\u0026#34;: 2025 } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Jane Doe\u0026#34;, \u0026#34;gpa\u0026#34;: 3.52, \u0026#34;grad_year\u0026#34;: 2024 } 示例：替换缺省值 #  计算 extended_stats ，将缺失的 GPA 字段替换为 0 ：\nGET students/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;extended_stats_gpa\u0026#34;: { \u0026#34;extended_stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;gpa\u0026#34;, \u0026#34;missing\u0026#34;: 0 } } } } 在返回内容中， gpa 的所有缺失值被替换为 0 ：\n... \u0026#34;aggregations\u0026#34;: { \u0026#34;extended_stats_gpa\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;min\u0026#34;: 0, \u0026#34;max\u0026#34;: 3.890000104904175, \u0026#34;avg\u0026#34;: 2.4700000286102295, \u0026#34;sum\u0026#34;: 7.4100000858306885, \u0026#34;sum_of_squares\u0026#34;: 27.522500681877148, \u0026#34;variance\u0026#34;: 3.0732667526245145, \u0026#34;variance_population\u0026#34;: 3.0732667526245145, \u0026#34;variance_sampling\u0026#34;: 4.609900128936772, \u0026#34;std_deviation\u0026#34;: 1.7530735160353415, \u0026#34;std_deviation_population\u0026#34;: 1.7530735160353415, \u0026#34;std_deviation_sampling\u0026#34;: 2.147067797936705, \u0026#34;std_deviation_bounds\u0026#34;: { \u0026#34;upper\u0026#34;: 5.976147060680912, \u0026#34;lower\u0026#34;: -1.0361470034604534, \u0026#34;upper_population\u0026#34;: 5.976147060680912, \u0026#34;lower_population\u0026#34;: -1.0361470034604534, \u0026#34;upper_sampling\u0026#34;: 6.7641356244836395, \u0026#34;lower_sampling\u0026#34;: -1.8241355672631805 } } } } 示例：忽略缺省值 #  计算 extended_stats 但不分配 missing 参数：\nGET students/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;extended_stats_gpa\u0026#34;: { \u0026#34;extended_stats\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;gpa\u0026#34; } } } } 计算扩展统计信息，忽略包含缺失字段值的文档（默认行为）：\n... \u0026#34;aggregations\u0026#34;: { \u0026#34;extended_stats_gpa\u0026#34;: { \u0026#34;count\u0026#34;: 2, \u0026#34;min\u0026#34;: 3.5199999809265137, \u0026#34;max\u0026#34;: 3.890000104904175, \u0026#34;avg\u0026#34;: 3.7050000429153442, \u0026#34;sum\u0026#34;: 7.4100000858306885, \u0026#34;sum_of_squares\u0026#34;: 27.522500681877148, \u0026#34;variance\u0026#34;: 0.03422502293587115, \u0026#34;variance_population\u0026#34;: 0.03422502293587115, \u0026#34;variance_sampling\u0026#34;: 0.0684500458717423, \u0026#34;std_deviation\u0026#34;: 0.18500006198883057, \u0026#34;std_deviation_population\u0026#34;: 0.18500006198883057, \u0026#34;std_deviation_sampling\u0026#34;: 0.2616295967044675, \u0026#34;std_deviation_bounds\u0026#34;: { \u0026#34;upper\u0026#34;: 4.075000166893005, \u0026#34;lower\u0026#34;: 3.334999918937683, \u0026#34;upper_population\u0026#34;: 4.075000166893005, \u0026#34;lower_population\u0026#34;: 3.334999918937683, \u0026#34;upper_sampling\u0026#34;: 4.228259236324279, \u0026#34;lower_sampling\u0026#34;: 3.1817408495064092 } } } } 包含缺失 GPA 值的文档在此计算中被省略。注意 count 中的差异。\n","subcategory":null,"summary":"","tags":null,"title":"扩展统计聚合（Extended Stats）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/extended-stats/"},{"category":null,"content":"扩展统计桶聚合 #  extended_stats_bucket 聚合是 stats_bucket 同级聚合的更全面的版本。除了 stats_bucket 提供的基本统计度量外，extended_stats_bucket 还计算以下指标：\n 平方和 方差 总体方差 抽样方差 标准差 总体标准差 抽样标准差 标准差界限： ** 上限 ** 下限 ** 种群上限 ** 种群下限 ** 采样上限 ** 采样下限  标准差和方差是总体统计量；它们分别始终等于总体标准差和方差。\nstd_deviation_bounds 对象定义了一个范围，该范围在均值（默认为两个标准差）的上方和下方跨越指定的标准差数量。此对象始终包含在输出中，但仅对正态分布的数据才有意义。在解释这些值之前，请验证您的数据集是否遵循正态分布。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  extended_stats_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。   sigma 可选 Double 非负） 用于计算 std_deviation_bounds 区间的均值上方和下方的标准差数量。默认值为 2 。参见 extended_stats 中定义范围。    参考样例 #  以下示例创建一个以一个月为间隔的日期直方图。 sum 子聚合计算每个月的字节总和。最后， extended_stats_bucket 聚合返回这些总和的扩展统计信息：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } } } }, \u0026#34;stats_monthly_bytes\u0026#34;: { \u0026#34;extended_stats_bucket\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;visits_per_month\u0026gt;sum_of_bytes\u0026#34;, \u0026#34;sigma\u0026#34;: 3, \u0026#34;format\u0026#34;: \u0026#34;0.##E0\u0026#34; } } } } 返回内容 #  响应包含所选分组的扩展统计信息。请注意，标准偏差界限适用于 3-sigma 范围；更改 sigma （或让其默认为 2 ）将返回不同的结果：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 } } ] }, \u0026#34;stats_monthly_bytes\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;min\u0026#34;: 2804103, \u0026#34;max\u0026#34;: 39103067, \u0026#34;avg\u0026#34;: 26575229.666666668, \u0026#34;sum\u0026#34;: 79725689, \u0026#34;min_as_string\u0026#34;: \u0026#34;2.8E6\u0026#34;, \u0026#34;max_as_string\u0026#34;: \u0026#34;3.91E7\u0026#34;, \u0026#34;avg_as_string\u0026#34;: \u0026#34;2.66E7\u0026#34;, \u0026#34;sum_as_string\u0026#34;: \u0026#34;7.97E7\u0026#34;, \u0026#34;sum_of_squares\u0026#34;: 2967153221794459, \u0026#34;variance\u0026#34;: 282808242095406.25, \u0026#34;variance_population\u0026#34;: 282808242095406.25, \u0026#34;variance_sampling\u0026#34;: 424212363143109.4, \u0026#34;std_deviation\u0026#34;: 16816903.46334325, \u0026#34;std_deviation_population\u0026#34;: 16816903.46334325, \u0026#34;std_deviation_sampling\u0026#34;: 20596416.2694171, \u0026#34;std_deviation_bounds\u0026#34;: { \u0026#34;upper\u0026#34;: 77025940.05669643, \u0026#34;lower\u0026#34;: -23875480.72336309, \u0026#34;upper_population\u0026#34;: 77025940.05669643, \u0026#34;lower_population\u0026#34;: -23875480.72336309, \u0026#34;upper_sampling\u0026#34;: 88364478.47491796, \u0026#34;lower_sampling\u0026#34;: -35214019.141584635 }, \u0026#34;sum_of_squares_as_string\u0026#34;: \u0026#34;2.97E15\u0026#34;, \u0026#34;variance_as_string\u0026#34;: \u0026#34;2.83E14\u0026#34;, \u0026#34;variance_population_as_string\u0026#34;: \u0026#34;2.83E14\u0026#34;, \u0026#34;variance_sampling_as_string\u0026#34;: \u0026#34;4.24E14\u0026#34;, \u0026#34;std_deviation_as_string\u0026#34;: \u0026#34;1.68E7\u0026#34;, \u0026#34;std_deviation_population_as_string\u0026#34;: \u0026#34;1.68E7\u0026#34;, \u0026#34;std_deviation_sampling_as_string\u0026#34;: \u0026#34;2.06E7\u0026#34;, \u0026#34;std_deviation_bounds_as_string\u0026#34;: { \u0026#34;upper\u0026#34;: \u0026#34;7.7E7\u0026#34;, \u0026#34;lower\u0026#34;: \u0026#34;-2.39E7\u0026#34;, \u0026#34;upper_population\u0026#34;: \u0026#34;7.7E7\u0026#34;, \u0026#34;lower_population\u0026#34;: \u0026#34;-2.39E7\u0026#34;, \u0026#34;upper_sampling\u0026#34;: \u0026#34;8.84E7\u0026#34;, \u0026#34;lower_sampling\u0026#34;: \u0026#34;-3.52E7\u0026#34; } } } } ","subcategory":null,"summary":"","tags":null,"title":"扩展统计桶聚合（Extended Stats Bucket）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/extended-stats/"},{"category":null,"content":"序列差分聚合 #  serial_diff 聚合是一个父级管道聚合，用于计算当前分组中指标值与上一个分组中指标值之间的差值。它将结果存储在当前分组中。\n使用 serial_diff 聚合来计算具有指定滞后的时间段之间的变化。lag 参数（一个正整数值）指定要从中减去当前值的哪个先前分组的值。默认的 lag 值是 1，这意味着 serial_diff 从当前分组中的值减去立即前一个分组中的值。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  serial_diff 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。   lag 可选 Integer 用于从当前数据分组中减去的历史数据分组。必须是正整数。默认为 1 。    参考样例 #  以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， serial_diff 聚合计算这些总和之间的月度字节差异：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;monthly_bytes\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;total_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;monthly_bytes_change\u0026#34;: { \u0026#34;serial_diff\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;total_bytes\u0026#34;, \u0026#34;lag\u0026#34;: 1 } } } } } } 返回内容包含第二个月和第三个月的月度差异。（第一个月 serial_diff 无法计算，因为没有前一个月可以与之比较）：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;monthly_bytes\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 }, \u0026#34;monthly_bytes_change\u0026#34;: { \u0026#34;value\u0026#34;: 36298964 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 }, \u0026#34;monthly_bytes_change\u0026#34;: { \u0026#34;value\u0026#34;: -1284548 } } ] } } } 示例：多周期差分 #\n 使用更大的 lag 值来比较每个分组与过去更早时间发生的分组。以下示例计算每周字节数据的差分，滞后为 4（即每个分组与 4 周前的分组进行比较）。这会消除任何周期为 4 周的波动：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;monthly_bytes\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;week\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;total_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;monthly_bytes_change\u0026#34;: { \u0026#34;serial_diff\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;total_bytes\u0026#34;, \u0026#34;lag\u0026#34;: 4 } } } } } } 返回内容包含每周分组的列表。请注意， serial_diff 聚合直到第五个分组才开始，当出现一个 lag 为 4 的分组时：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;monthly_bytes\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-24T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1742774400000, \u0026#34;doc_count\u0026#34;: 249, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 1531493 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-31T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743379200000, \u0026#34;doc_count\u0026#34;: 1617, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9213161 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-07T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743984000000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9188671 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-14T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1744588800000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9244851 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-21T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745193600000, \u0026#34;doc_count\u0026#34;: 1609, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9061045 }, \u0026#34;monthly_bytes_change\u0026#34;: { \u0026#34;value\u0026#34;: 7529552 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-28T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1745798400000, \u0026#34;doc_count\u0026#34;: 1554, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 8713507 }, \u0026#34;monthly_bytes_change\u0026#34;: { \u0026#34;value\u0026#34;: -499654 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-05T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746403200000, \u0026#34;doc_count\u0026#34;: 1710, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9544718 }, \u0026#34;monthly_bytes_change\u0026#34;: { \u0026#34;value\u0026#34;: 356047 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-12T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1747008000000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9155820 }, \u0026#34;monthly_bytes_change\u0026#34;: { \u0026#34;value\u0026#34;: -89031 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-19T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1747612800000, \u0026#34;doc_count\u0026#34;: 1610, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 9025078 }, \u0026#34;monthly_bytes_change\u0026#34;: { \u0026#34;value\u0026#34;: -35967 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-26T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1748217600000, \u0026#34;doc_count\u0026#34;: 895, \u0026#34;total_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 5047345 }, \u0026#34;monthly_bytes_change\u0026#34;: { \u0026#34;value\u0026#34;: -3666162 } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"序列差分聚合（Serial Diff）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/serial-diff/"},{"category":null,"content":"平均桶聚合 #  avg_bucket 聚合是一个同级聚合，它计算上一个聚合的每个分组中的指标平均值。\n指定的指标必须是数值型的，且同级聚合必须是多分组聚合。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  avg_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合存储分组的路径。请参阅存储分组路径 。   gap_policy 可选 String 要应用于缺失数据的策略。有效值为 skip 和 insert_zeros。默认值为 skip。请参阅数据差距 。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出 。    参考样例 #  以下示例创建间隔为一个月的日期直方图。sum 子聚合计算每个月的字节总和。最后，avg_bucket 聚合根据这些总和计算每月的平均字节数：\nPOST sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } } } }, \u0026#34;avg_monthly_bytes\u0026#34;: { \u0026#34;avg_bucket\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;visits_per_month\u0026gt;sum_of_bytes\u0026#34; } } } } 返回内容 #  聚合返回每月存储分组的平均字节数：\n{ \u0026#34;took\u0026#34;: 43, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;visits_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;sum_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 } } ] }, \u0026#34;avg_monthly_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 26575229.666666668 } } } ","subcategory":null,"summary":"","tags":null,"title":"平均桶聚合（Avg Bucket）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/avg-bucket/"},{"category":null,"content":"平均值聚合 #  avg 聚合是一个单值指标聚合，它返回某个字段的平均值。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  avg 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 计算平均值的字段。   missing 可选 Float 要分配给字段缺失实例的值。默认情况下， avg 会在计算中忽略缺失值。    参考样例 #  以下示例请求计算示例数据中 taxful_total_price 字段的平均值：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_taxful_total_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } } } } 返回内容 #  返回内容包含 taxful_total_price 的平均值：\n{ \u0026#34;took\u0026#34;: 85, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;avg_taxful_total_price\u0026#34;: { \u0026#34;value\u0026#34;: 75.05542864304813 } } } 可以使用聚合名称 avg_taxful_total_price 作为从聚合获取结果的键名。\n缺失值处理 #  通过提取以下文档来准备示例索引。请注意，第二个文档缺少 gpa 值：\nPOST _bulk { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;gpa\u0026#34;: 3.89, \u0026#34;grad_year\u0026#34;: 2022} { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Jonathan Powers\u0026#34;, \u0026#34;grad_year\u0026#34;: 2025 } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Jane Doe\u0026#34;, \u0026#34;gpa\u0026#34;: 3.52, \u0026#34;grad_year\u0026#34;: 2024 } 示例：替换缺失值 #  取平均值，将缺失的 GPA 字段替换为 0 ：\nGET students/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_gpa\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;gpa\u0026#34;, \u0026#34;missing\u0026#34;: 0 } } } } 返回内容如下。可以与下一个忽略了缺失值的示例做个比较：\n{ \u0026#34;took\u0026#34;: 12, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;avg_gpa\u0026#34;: { \u0026#34;value\u0026#34;: 2.4700000286102295 } } } 示例：忽略缺失值 #  取平均值但不分配 missing 参数：\nGET students/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_gpa\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;gpa\u0026#34; } } } } 聚合器计算平均值，省略包含缺失字段值的文档（默认行为）：\n{ \u0026#34;took\u0026#34;: 255, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;avg_gpa\u0026#34;: { \u0026#34;value\u0026#34;: 3.7050000429153442 } } } ","subcategory":null,"summary":"","tags":null,"title":"平均值聚合（Avg）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/average/"},{"category":null,"content":"嵌套聚合 #  nested 聚合让你能够对嵌套对象内的字段进行聚合。nested 类型是对象数据类型的特殊版本，它允许对象数组以独立于彼此的方式进行索引，从而可以独立于彼此进行查询。\n相关指南（先读这些） #    聚合基础  Nested 建模  聚合场景实践  使用 object 类型，所有数据都存储在同一个文档中，因此搜索匹配可以跨越子文档。例如，想象一个 logs 索引，其中 pages 映射为 object 数据类型：\nPUT logs/_doc/0 { \u0026#34;response\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;pages\u0026#34;: [ { \u0026#34;page\u0026#34;: \u0026#34;landing\u0026#34;, \u0026#34;load_time\u0026#34;: 200 }, { \u0026#34;page\u0026#34;: \u0026#34;blog\u0026#34;, \u0026#34;load_time\u0026#34;: 500 } ] } Easysearch 合并所有看起来像这样的实体关系的子属性：\n{ \u0026#34;logs\u0026#34;: { \u0026#34;pages\u0026#34;: [\u0026#34;landing\u0026#34;, \u0026#34;blog\u0026#34;], \u0026#34;load_time\u0026#34;: [\u0026#34;200\u0026#34;, \u0026#34;500\u0026#34;] } } 所以，如果你想要用 pages=landing 和 load_time=500 搜索这个索引，即使 load_time 的 landing 值为 200，这个文档也符合条件。\n如果你想要确保不会发生这种跨对象匹配，将字段映射为 nested 类型：\nPUT logs { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;pages\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;page\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;load_time\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } } } 嵌套文档允许你索引相同的 JSON 文档，但会保持你的页面在不同的 Lucene 文档中，使得只有 pages=landing 和 load_time=200 这样的搜索能返回预期结果。内部上，嵌套对象将数组中的每个对象索引为一个单独的隐藏文档，这意味着每个嵌套对象都可以独立于其他对象进行查询。\n您必须指定相对于父级包含嵌套文档的嵌套路径：\nGET logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;response\u0026#34;: \u0026#34;200\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;pages\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;pages\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;min_load_time\u0026#34;: { \u0026#34;min\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;pages.load_time\u0026#34; } } } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;pages\u0026#34; : { \u0026#34;doc_count\u0026#34; : 2, \u0026#34;min_load_time\u0026#34; : { \u0026#34;value\u0026#34; : 200 } } } } ","subcategory":null,"summary":"","tags":null,"title":"嵌套聚合（Nested）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/nested/"},{"category":null,"content":"导数聚合 #  derivative 聚合是一个父聚合，用于计算聚合每个分组的一阶和二阶导数。\n对于有序的分组序列，derivative 将当前分组和前一个分组中的指标值之差近似为一阶导数。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  derivative 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。    示例：一阶导数 #  以下示例创建一个每月间隔的日期直方图。 sum 子聚合计算每个月所有字节的和。最后， derivative 聚合计算 sum 子聚合的一阶导数。一阶导数估计为当前月份和上个月字节数之间的差值：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;number_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;bytes_deriv\u0026#34;: { \u0026#34;derivative\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;number_of_bytes\u0026#34; } } } } } } 返回内容显示了为第二和第三个分组计算出的导数：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;number_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;number_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 }, \u0026#34;bytes_deriv\u0026#34;: { \u0026#34;value\u0026#34;: 36298964 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;number_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 }, \u0026#34;bytes_deriv\u0026#34;: { \u0026#34;value\u0026#34;: -1284548 } } ] } } } 第一个分组没有计算导数，因为该分组没有可用的前一个分组。\n示例：二阶导数 #  要计算二阶导数，将一个导数聚合连接到另一个导数聚合上：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;number_of_bytes\u0026#34;: { \u0026#34;sum\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } }, \u0026#34;bytes_1st_deriv\u0026#34;: { \u0026#34;derivative\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;number_of_bytes\u0026#34; } }, \u0026#34;bytes_2nd_deriv\u0026#34;: { \u0026#34;derivative\u0026#34;: { \u0026#34;buckets_path\u0026#34;: \u0026#34;bytes_1st_deriv\u0026#34; } } } } } } 返回内容\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;sales_per_month\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-03-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1740787200000, \u0026#34;doc_count\u0026#34;: 480, \u0026#34;number_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 2804103 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-04-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1743465600000, \u0026#34;doc_count\u0026#34;: 6849, \u0026#34;number_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 39103067 }, \u0026#34;bytes_1st_deriv\u0026#34;: { \u0026#34;value\u0026#34;: 36298964 } }, { \u0026#34;key_as_string\u0026#34;: \u0026#34;2025-05-01T00:00:00.000Z\u0026#34;, \u0026#34;key\u0026#34;: 1746057600000, \u0026#34;doc_count\u0026#34;: 6745, \u0026#34;number_of_bytes\u0026#34;: { \u0026#34;value\u0026#34;: 37818519 }, \u0026#34;bytes_1st_deriv\u0026#34;: { \u0026#34;value\u0026#34;: -1284548 }, \u0026#34;bytes_2nd_deriv\u0026#34;: { \u0026#34;value\u0026#34;: -37583512 } } ] } } } 由于该分组没有可用的前一个分组，因此第一个分组没有计算一阶导数。类似地，第一个和第二个分组也没有计算二阶导数。\n","subcategory":null,"summary":"","tags":null,"title":"导数聚合（Derivative）","url":"/easysearch/main/docs/features/aggregations/pipeline-aggregations/derivative/"},{"category":null,"content":"子文档聚合 #  children 聚合是一种存储分组聚合，它根据索引中定义的父子关系创建包含子文档的单个存储分组。\nchildren 聚合与 join 字段类型配合使用，以聚合与父文档关联的子文档。\nchildren 聚合标识与特定子关系名称匹配的子文档，而 parent 聚合标识具有匹配子文档的父文档。这两个聚合都采用子关系名称作为输入。\n相关指南（先读这些） #    聚合基础  Parent-Child 建模  聚合场景实践  参数说明 #  children 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     type 必填 String join 字段中的子类型的名称。这标识了要使用的父子关系。    参考样例 #  以下示例构建一个包含三名员工的小型公司数据库。每个员工记录都与父部门记录具有子联接关系。\n首先，创建一个 company 索引，其中包含一个将部门（父级）映射到员工（子级）的联接字段：\nPUT /company { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;join_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;relations\u0026#34;: { \u0026#34;department\u0026#34;: \u0026#34;employee\u0026#34; } }, \u0026#34;department_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;employee_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;salary\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, \u0026#34;hire_date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } } } } 接下来，使用三个部门和三名员工填充数据。下表显示了父子分配。\n   部门（父关系） 员工（子关系）     Accounting Abel Anderson, Betty Billings   Engineering Carl Carter   HR none    routing 参数可确保父文档和子文档存储在同一分片上，这是父子关系在 Easysearch 中正常运行所必需的：\nPOST _bulk?routing=1 { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;department\u0026#34;, \u0026#34;department_name\u0026#34;: \u0026#34;Accounting\u0026#34;, \u0026#34;join_field\u0026#34;: \u0026#34;department\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;department\u0026#34;, \u0026#34;department_name\u0026#34;: \u0026#34;Engineering\u0026#34;, \u0026#34;join_field\u0026#34;: \u0026#34;department\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;department\u0026#34;, \u0026#34;department_name\u0026#34;: \u0026#34;HR\u0026#34;, \u0026#34;join_field\u0026#34;: \u0026#34;department\u0026#34; } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;employee_name\u0026#34;: \u0026#34;Abel Anderson\u0026#34;, \u0026#34;salary\u0026#34;: 120000, \u0026#34;hire_date\u0026#34;: \u0026#34;2024-04-04\u0026#34;, \u0026#34;join_field\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;employee_name\u0026#34;: \u0026#34;Betty Billings\u0026#34;, \u0026#34;salary\u0026#34;: 140000, \u0026#34;hire_date\u0026#34;: \u0026#34;2023-05-05\u0026#34;, \u0026#34;join_field\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;company\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;6\u0026#34; } } { \u0026#34;type\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;employee_name\u0026#34;: \u0026#34;Carl Carter\u0026#34;, \u0026#34;salary\u0026#34;: 140000, \u0026#34;hire_date\u0026#34;: \u0026#34;2020-06-06\u0026#34;, \u0026#34;join_field\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;employee\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;2\u0026#34; } } 以下请求查询所有部门，然后筛选名为 Accounting 的部门。然后，它使用子关联聚合来选择与会计部门具有子关系的两个单据。最后，avg 子关联聚合返回会计员工工资的平均值：\nGET /company/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;join_field\u0026#34;: \u0026#34;department\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;department_name\u0026#34;: \u0026#34;Accounting\u0026#34; } } ] } }, \u0026#34;aggs\u0026#34;: { \u0026#34;acc_employees\u0026#34;: { \u0026#34;children\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;employee\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_salary\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;salary\u0026#34; } } } } } } 返回内容包含所选部门存储分组，查找部门的员工类型子级，并计算其工资的平均值 ：\n{ \u0026#34;took\u0026#34;: 379, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;acc_employees\u0026#34;: { \u0026#34;doc_count\u0026#34;: 2, \u0026#34;avg_salary\u0026#34;: { \u0026#34;value\u0026#34;: 110000 } } } } \n","subcategory":null,"summary":"","tags":null,"title":"子文档聚合（Children）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/children/"},{"category":null,"content":"多过滤器聚合 #  filters 聚合与 filter 聚合相同，但它允许你使用多个过滤器聚合。filter 聚合结果为一个分组，而 filters 聚合会返回多个分组，每个定义的过滤器对应一个分组。\n相关指南（先读这些） #    聚合基础  聚合场景实践  要为所有未匹配任何过滤器查询的文档创建一个分组，将 other_bucket 属性设置为 true ：\nGET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;200_os\u0026#34;: { \u0026#34;filters\u0026#34;: { \u0026#34;other_bucket\u0026#34;: true, \u0026#34;filters\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;response.keyword\u0026#34;: \u0026#34;200\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;machine.os.keyword\u0026#34;: \u0026#34;osx\u0026#34; } } ] }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_amount\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;bytes\u0026#34; } } } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;200_os\u0026#34; : { \u0026#34;buckets\u0026#34; : [ { \u0026#34;doc_count\u0026#34; : 12832, \u0026#34;avg_amount\u0026#34; : { \u0026#34;value\u0026#34; : 5897.852711970075 } }, { \u0026#34;doc_count\u0026#34; : 2825, \u0026#34;avg_amount\u0026#34; : { \u0026#34;value\u0026#34; : 5620.347256637168 } }, { \u0026#34;doc_count\u0026#34; : 1017, \u0026#34;avg_amount\u0026#34; : { \u0026#34;value\u0026#34; : 3247.0963618485744 } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"多过滤器聚合（Filters）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/filters/"},{"category":null,"content":"多样性采样聚合 #  diversified_sampler 聚合允许你通过去重包含相同 field 的文档来减少样本池分布的偏差。它通过使用 max_docs_per_value 和 field 设置来实现，这些设置限制了在分片上收集的 field 的最大文档数。max_docs_per_value 设置是一个可选参数，用于确定每个 field 将返回的最大文档数。此设置的默认值为 1。\n与 sampler 聚合类似，你可以使用 shard_size 设置来控制在任何单个分片上收集的最大文档数，如下面的示例所示：\n相关指南（先读这些） #    聚合基础  聚合性能优化  聚合场景实践  GET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;sample\u0026#34;: { \u0026#34;diversified_sampler\u0026#34;: { \u0026#34;shard_size\u0026#34;: 1000, \u0026#34;field\u0026#34;: \u0026#34;response.keyword\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;agent.keyword\u0026#34; } } } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;sample\u0026#34; : { \u0026#34;doc_count\u0026#34; : 3, \u0026#34;terms\u0026#34; : { \u0026#34;doc_count_error_upper_bound\u0026#34; : 0, \u0026#34;sum_other_doc_count\u0026#34; : 0, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\u0026#34;, \u0026#34;doc_count\u0026#34; : 2 }, { \u0026#34;key\u0026#34; : \u0026#34;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\u0026#34;, \u0026#34;doc_count\u0026#34; : 1 } ] } } } } 参数说明 #\n    参数 必需/可选 数据类型 描述     field 必填 String 用于去重的字段。每个唯一值最多保留 max_docs_per_value 个文档。   shard_size 可选 Integer 每个分片上收集的最大文档数。默认 100。   max_docs_per_value 可选 Integer 每个 field 值最多保留的文档数。默认 1。   execution_hint 可选 String 执行方式提示。可选 map、global_ordinals、bytes_hash。    sampler vs. diversified_sampler #     聚合 采样方式 适用场景     sampler 取评分最高的前 N 个文档 简单采样，快速获取近似结果   diversified_sampler 按某字段去重后采样 需要多样性，避免某类数据主导结果     提示：diversified_sampler 适用于需要公平代表各个分类的场景，例如在多种类商品中均匀采样后再做聚合分析。\n ","subcategory":null,"summary":"","tags":null,"title":"多样性采样聚合（Diversified Sampler）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/diversified-sampler/"},{"category":null,"content":"复合聚合 #  composite 聚合基于一个或多个文档字段或源创建分组。composite 聚合为每个单独源值的组合创建一个分组。默认情况下，如果一个或多个字段中缺少值，则这些组合会从结果中省略。\n相关指南（先读这些） #    聚合基础  聚合场景实践  每个源有四种类型的聚合之一：\n terms 类型按唯一（通常是 String ）值分组。 histogram 类型按指定宽度数值分组。 date_histogram 类型按指定宽度的日期或时间范围分组。 geotile_grid 类型按指定分辨率将地理点分组到网格中。  composite 聚合通过将其源键组合到分组中来工作。生成的分组是有序的，跨源(Across)和源内部(Within)都是：\n Across：分组按聚合请求中源的顺序嵌套。 Within:每个源中值的顺序决定了该源的分组顺序。排序方式根据源类型适当选择，可以是字母顺序、数字顺序、日期时间顺序或地理切片顺序。  考虑一下来自马拉松参与者索引的这些字段：\n{... \u0026#34;city\u0026#34;: \u0026#34;Albuquerque\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Bronze\u0026#34; ...} {... \u0026#34;city\u0026#34;: \u0026#34;Boston\u0026#34;, ...} {... \u0026#34;city\u0026#34;: \u0026#34;Chicago\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Bronze\u0026#34; ...} {... \u0026#34;city\u0026#34;: \u0026#34;Albuquerque\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Gold\u0026#34; ...} {... \u0026#34;city\u0026#34;: \u0026#34;Chicago\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Silver\u0026#34; ...} {... \u0026#34;city\u0026#34;: \u0026#34;Boston\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Bronze\u0026#34; ...} {... \u0026#34;city\u0026#34;: \u0026#34;Chicago\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Gold\u0026#34; ...} 假设请求指定源如下：\n... \u0026#34;sources\u0026#34;: [ { \u0026#34;marathon_city\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;city\u0026#34; }}}, { \u0026#34;participant_medal\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;place\u0026#34; }}} ], ...  你必须为每个源分配一个唯一的键名。\n 生成的 composite 包含以下分组，按顺序排列：\n{ \u0026#34;city\u0026#34;: \u0026#34;Albuquerque\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Bronze\u0026#34; } { \u0026#34;city\u0026#34;: \u0026#34;Albuquerque\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Gold\u0026#34; } { \u0026#34;city\u0026#34;: \u0026#34;Boston\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Bronze\u0026#34; } { \u0026#34;city\u0026#34;: \u0026#34;Boston\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Silver\u0026#34; } { \u0026#34;city\u0026#34;: \u0026#34;Chicago\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Bronze\u0026#34; } { \u0026#34;city\u0026#34;: \u0026#34;Chicago\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Gold\u0026#34; } { \u0026#34;city\u0026#34;: \u0026#34;Chicago\u0026#34;, \u0026#34;place\u0026#34;: \u0026#34;Silver\u0026#34; } 请注意， city 和 place 字段都是按字母顺序排列的。\n参数说明 #  composite 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     sources 必填 Array 源对象的数组。有效类型为 terms 、 histogram 、 date_histogram 和 geotile_grid 。   size 可选 Numeric 在结果中返回的 composite 分组的数量。默认值为 10 。参见 分页复合结果。   after 可选 String 一个键，用于指定从何处继续显示分页的 composite 分组。参见 分页复合结果。   order 可选 String 对于每个数据源，是否按升序或降序排列值。有效值为 asc 和 desc 。默认值为 asc 。   missing_bucket 可选 Boolean 对于每个数据源，是否包含缺失值文档。默认值为 false 。如果设置为 true ，Easysearch 会包含这些文档，并将 null 作为字段的键。空值在升序排列中排在最前面。     有关聚合特定参数，请参阅相应的聚合文档。\n Terms #  使用 terms 聚合来聚合字符串或布尔数据。更多信息，请参阅 Terms 聚合。\n 您可以使用 terms 源来为任何类型的数据创建复合分组。但是，由于 terms 源为每个唯一值创建分组，因此您通常使用 histogram 源来代替数值数据。\n 以下示例请求返回数据中星期几和客户性别的第一个 4 复合分组：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;sources\u0026#34;: [ { \u0026#34;day\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;day_of_week\u0026#34; }}}, { \u0026#34;gender\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;customer_gender\u0026#34; }}} ], \u0026#34;size\u0026#34;: 4 } } } } 由于此示例的数据集包含每个分组的有效数据，因此聚合会为性别和星期几的每一组合生成一个分组，最终产生 14 个总分组。\n由于请求指定了 size 个 4 ，响应包含前四个复合分组。由于源是 terms ，分组在跨源和源内部都是按升序字母顺序排列的：\n{ \u0026#34;took\u0026#34;: 51, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;after_key\u0026#34;: { \u0026#34;day\u0026#34;: \u0026#34;Monday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34; }, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: { \u0026#34;day\u0026#34;: \u0026#34;Friday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;FEMALE\u0026#34; }, \u0026#34;doc_count\u0026#34;: 399 }, { \u0026#34;key\u0026#34;: { \u0026#34;day\u0026#34;: \u0026#34;Friday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34; }, \u0026#34;doc_count\u0026#34;: 371 }, { \u0026#34;key\u0026#34;: { \u0026#34;day\u0026#34;: \u0026#34;Monday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;FEMALE\u0026#34; }, \u0026#34;doc_count\u0026#34;: 320 }, { \u0026#34;key\u0026#34;: { \u0026#34;day\u0026#34;: \u0026#34;Monday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34; }, \u0026#34;doc_count\u0026#34;: 259 } ] } } } 您可以使用返回的 after_key 来查看更多结果。请参阅下一节的示例。\n直方图 #  使用 histogram 个数据源创建数值数据的组合聚合。更多信息，请参阅直方图聚合。\n对于 histogram 个数据源，每个 composite 分组键中使用的名称是该键的直方图间隔中的最低值。每个源直方图间隔包含 [lower_bound, lower_bound + interval) 范围内的值。第一个间隔的名称是源字段中的最低值（对于升序值源）。\n以下示例请求根据分组宽度分别为 1 和 50 ，返回数据中数量和基本单位价格的前 6 个组合分组：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;sources\u0026#34;: [ { \u0026#34;quantity\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.quantity\u0026#34;, \u0026#34;interval\u0026#34;: 1 }}}, { \u0026#34;unit_price\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.base_unit_price\u0026#34;, \u0026#34;interval\u0026#34;: 50 }}} ], \u0026#34;size\u0026#34;: 6 } } } } 聚合返回两个 histogram 源的第一个 6 分组键和文档计数。与 terms 示例一样，分组在源字段之间和内部按顺序排列。然而在此情况下，顺序是数值的，基于每个直方图宽度的包含下界：\n{ \u0026#34;took\u0026#34;: 11, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;after_key\u0026#34;: { \u0026#34;quantity\u0026#34;: 2, \u0026#34;unit_price\u0026#34;: 150 }, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: { \u0026#34;quantity\u0026#34;: 1, \u0026#34;unit_price\u0026#34;: 0 }, \u0026#34;doc_count\u0026#34;: 17691 }, { \u0026#34;key\u0026#34;: { \u0026#34;quantity\u0026#34;: 1, \u0026#34;unit_price\u0026#34;: 50 }, \u0026#34;doc_count\u0026#34;: 5014 }, { \u0026#34;key\u0026#34;: { \u0026#34;quantity\u0026#34;: 1, \u0026#34;unit_price\u0026#34;: 100 }, \u0026#34;doc_count\u0026#34;: 482 }, { \u0026#34;key\u0026#34;: { \u0026#34;quantity\u0026#34;: 1, \u0026#34;unit_price\u0026#34;: 150 }, \u0026#34;doc_count\u0026#34;: 148 }, { \u0026#34;key\u0026#34;: { \u0026#34;quantity\u0026#34;: 1, \u0026#34;unit_price\u0026#34;: 200 }, \u0026#34;doc_count\u0026#34;: 32 }, { \u0026#34;key\u0026#34;: { \u0026#34;quantity\u0026#34;: 2, \u0026#34;unit_price\u0026#34;: 150 }, \u0026#34;doc_count\u0026#34;: 4 } ] } } } 每个字段的分组键是字段区间的下界。例如，第一个 composite 分组的 unit_price 键是 0 。\n要检索下一个 6 分组，请按如下方式将响应中的 after_key 对象提供给 after 参数：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;sources\u0026#34;: [ { \u0026#34;quantity\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.quantity\u0026#34;, \u0026#34;interval\u0026#34;: 1 }}}, { \u0026#34;unit_price\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.base_unit_price\u0026#34;, \u0026#34;interval\u0026#34;: 50 }}} ], \u0026#34;size\u0026#34;: 6, \u0026#34;after\u0026#34;: { \u0026#34;quantity\u0026#34;: 2, \u0026#34;unit_price\u0026#34;: 150 } } } } } 仅剩两个分组：\n{ \u0026#34;took\u0026#34;: 12, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;after_key\u0026#34;: { \u0026#34;quantity\u0026#34;: 2, \u0026#34;unit_price\u0026#34;: 500 }, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: { \u0026#34;quantity\u0026#34;: 2, \u0026#34;unit_price\u0026#34;: 200 }, \u0026#34;doc_count\u0026#34;: 8 }, { \u0026#34;key\u0026#34;: { \u0026#34;quantity\u0026#34;: 2, \u0026#34;unit_price\u0026#34;: 500 }, \u0026#34;doc_count\u0026#34;: 4 } ] } } } 日期直方图 #\n 要创建日期范围的组合聚合，请使用 date_histogram 聚合。有关详细信息，请参阅日期直方图聚合。\nEasysearch 将日期（包括 date_interval 分组键）表示为 long 表示自 Unix 时间以来的毫秒数的整数。您可以使用 format 参数格式化日期输出。这不会改变键的顺序。\nEasysearch 以 UTC 存储日期和时间。您可以使用 time_zone 参数以不同的时区显示输出结果。\n以下示例请求返回数据中，每个售出产品创建年份的第一组 4 复合分组，以及售出日期，基于分组宽度分别为 1 年和 1 天：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;sources\u0026#34;: [ { \u0026#34;product_creation_date\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.created_on\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;1y\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy\u0026#34; }}}, { \u0026#34;order_date\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;order_date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34; }}} ], \u0026#34;size\u0026#34;: 4 } } } } 聚合返回格式化的基于日期的分组键和计数。对于 date_interval 复合聚合，字段排序按日期：\n{ \u0026#34;took\u0026#34;: 21, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;after_key\u0026#34;: { \u0026#34;product_creation_date\u0026#34;: \u0026#34;2016\u0026#34;, \u0026#34;order_date\u0026#34;: \u0026#34;2025-02-23\u0026#34; }, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: { \u0026#34;product_creation_date\u0026#34;: \u0026#34;2016\u0026#34;, \u0026#34;order_date\u0026#34;: \u0026#34;2025-02-20\u0026#34; }, \u0026#34;doc_count\u0026#34;: 146 }, { \u0026#34;key\u0026#34;: { \u0026#34;product_creation_date\u0026#34;: \u0026#34;2016\u0026#34;, \u0026#34;order_date\u0026#34;: \u0026#34;2025-02-21\u0026#34; }, \u0026#34;doc_count\u0026#34;: 153 }, { \u0026#34;key\u0026#34;: { \u0026#34;product_creation_date\u0026#34;: \u0026#34;2016\u0026#34;, \u0026#34;order_date\u0026#34;: \u0026#34;2025-02-22\u0026#34; }, \u0026#34;doc_count\u0026#34;: 143 }, { \u0026#34;key\u0026#34;: { \u0026#34;product_creation_date\u0026#34;: \u0026#34;2016\u0026#34;, \u0026#34;order_date\u0026#34;: \u0026#34;2025-02-23\u0026#34; }, \u0026#34;doc_count\u0026#34;: 140 } ] } } } 地理网格 #\n 使用 geotile_grid 源将 geo_point 值聚合到代表地图瓦片的分组中。与其他复合聚合源一样，默认情况下，结果仅包括包含数据的分组。有关更多信息，请参阅 Geotile 网格聚合。\n每个单元格对应一个地图瓦片。单元格标签使用 {zoom}/{x}/{y} 格式。\n以下示例请求返回以 8 精度包含 geoip.location 字段中的位置的前 6 个瓦片：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;sources\u0026#34;: [ { \u0026#34;tile\u0026#34;: { \u0026#34;geotile_grid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;geoip.location\u0026#34;, \u0026#34;precision\u0026#34;: 8 } } } ], \u0026#34;size\u0026#34;: 6 } } } } 聚合返回指定的 geo_tiles 和点计数：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;after_key\u0026#34;: { \u0026#34;tile\u0026#34;: \u0026#34;8/122/104\u0026#34; }, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: { \u0026#34;tile\u0026#34;: \u0026#34;8/43/102\u0026#34; }, \u0026#34;doc_count\u0026#34;: 310 }, { \u0026#34;key\u0026#34;: { \u0026#34;tile\u0026#34;: \u0026#34;8/75/96\u0026#34; }, \u0026#34;doc_count\u0026#34;: 896 }, { \u0026#34;key\u0026#34;: { \u0026#34;tile\u0026#34;: \u0026#34;8/75/124\u0026#34; }, \u0026#34;doc_count\u0026#34;: 178 }, { \u0026#34;key\u0026#34;: { \u0026#34;tile\u0026#34;: \u0026#34;8/122/104\u0026#34; }, \u0026#34;doc_count\u0026#34;: 408 } ] } } } 组合源 #\n 您可以组合两个或多个任何不同类型的来源。\n以下示例请求返回由三种不同来源类型组成的分组：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;sources\u0026#34;: [ { \u0026#34;order_date\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;order_date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;1M\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM\u0026#34; }}}, { \u0026#34;gender\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;customer_gender\u0026#34; }}}, { \u0026#34;unit_price\u0026#34;: { \u0026#34;histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.base_unit_price\u0026#34;, \u0026#34;interval\u0026#34;: 200 }}} ], \u0026#34;size\u0026#34;: 10 } } } } 聚合返回混合类型的 composite 分组和文档计数：\n{ \u0026#34;took\u0026#34;: 11, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;after_key\u0026#34;: { \u0026#34;order_date\u0026#34;: \u0026#34;2025-03\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34;, \u0026#34;unit_price\u0026#34;: 200 }, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: { \u0026#34;order_date\u0026#34;: \u0026#34;2025-02\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;FEMALE\u0026#34;, \u0026#34;unit_price\u0026#34;: 0 }, \u0026#34;doc_count\u0026#34;: 1517 }, { \u0026#34;key\u0026#34;: { \u0026#34;order_date\u0026#34;: \u0026#34;2025-02\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34;, \u0026#34;unit_price\u0026#34;: 0 }, \u0026#34;doc_count\u0026#34;: 1369 }, { \u0026#34;key\u0026#34;: { \u0026#34;order_date\u0026#34;: \u0026#34;2025-02\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34;, \u0026#34;unit_price\u0026#34;: 200 }, \u0026#34;doc_count\u0026#34;: 6 }, { \u0026#34;key\u0026#34;: { \u0026#34;order_date\u0026#34;: \u0026#34;2025-02\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34;, \u0026#34;unit_price\u0026#34;: 400 }, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: { \u0026#34;order_date\u0026#34;: \u0026#34;2025-03\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;FEMALE\u0026#34;, \u0026#34;unit_price\u0026#34;: 0 }, \u0026#34;doc_count\u0026#34;: 3656 }, { \u0026#34;key\u0026#34;: { \u0026#34;order_date\u0026#34;: \u0026#34;2025-03\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;FEMALE\u0026#34;, \u0026#34;unit_price\u0026#34;: 200 }, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: { \u0026#34;order_date\u0026#34;: \u0026#34;2025-03\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34;, \u0026#34;unit_price\u0026#34;: 0 }, \u0026#34;doc_count\u0026#34;: 3530 }, { \u0026#34;key\u0026#34;: { \u0026#34;order_date\u0026#34;: \u0026#34;2025-03\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34;, \u0026#34;unit_price\u0026#34;: 200 }, \u0026#34;doc_count\u0026#34;: 7 } ] } } } 子聚合 #  组合聚合在结合子聚合时最为有用，子聚合可以揭示 composite 分组中文档的信息。\n以下示例请求比较了数据中每周每天按性别划分的平均支出：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;sources\u0026#34;: [ { \u0026#34;weekday\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;day_of_week\u0026#34; }}}, { \u0026#34;gender\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;customer_gender\u0026#34; }}} ], \u0026#34;size\u0026#34;: 6 }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_spend\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } } } } } } 该聚合返回前 6 个分组的平均 taxful_total_price 值：\n{ \u0026#34;took\u0026#34;: 30, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;composite_buckets\u0026#34;: { \u0026#34;after_key\u0026#34;: { \u0026#34;weekday\u0026#34;: \u0026#34;Saturday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34; }, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: { \u0026#34;weekday\u0026#34;: \u0026#34;Friday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;FEMALE\u0026#34; }, \u0026#34;doc_count\u0026#34;: 399, \u0026#34;avg_spend\u0026#34;: { \u0026#34;value\u0026#34;: 71.7733395989975 } }, { \u0026#34;key\u0026#34;: { \u0026#34;weekday\u0026#34;: \u0026#34;Friday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34; }, \u0026#34;doc_count\u0026#34;: 371, \u0026#34;avg_spend\u0026#34;: { \u0026#34;value\u0026#34;: 79.72514108827494 } }, { \u0026#34;key\u0026#34;: { \u0026#34;weekday\u0026#34;: \u0026#34;Monday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;FEMALE\u0026#34; }, \u0026#34;doc_count\u0026#34;: 320, \u0026#34;avg_spend\u0026#34;: { \u0026#34;value\u0026#34;: 72.1588623046875 } }, { \u0026#34;key\u0026#34;: { \u0026#34;weekday\u0026#34;: \u0026#34;Monday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34; }, \u0026#34;doc_count\u0026#34;: 259, \u0026#34;avg_spend\u0026#34;: { \u0026#34;value\u0026#34;: 86.1754946911197 } }, { \u0026#34;key\u0026#34;: { \u0026#34;weekday\u0026#34;: \u0026#34;Saturday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;FEMALE\u0026#34; }, \u0026#34;doc_count\u0026#34;: 365, \u0026#34;avg_spend\u0026#34;: { \u0026#34;value\u0026#34;: 73.53236301369863 } }, { \u0026#34;key\u0026#34;: { \u0026#34;weekday\u0026#34;: \u0026#34;Saturday\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;MALE\u0026#34; }, \u0026#34;doc_count\u0026#34;: 371, \u0026#34;avg_spend\u0026#34;: { \u0026#34;value\u0026#34;: 72.78092360175202 } } ] } } } 分页复合结果 #\n 如果请求导致超过 size 个分组，则返回 size 个分组。在这种情况下，结果包含一个 after_key 对象，其中包含列表中下一个分组的键。要检索请求的下一个 size 个分组，请再次发送请求，并在 after 参数中提供 after_key 。例如，请参阅 Histogram 中的请求。\n 始终使用 after_key 继续分页响应，而不是复制最后一个分组。两者有时是不同的。\n 使用索引排序提升性能 #  为了加快在大数据集上的复合聚合速度，你可以使用与聚合源相同的字段和顺序来对索引进行排序。当 index.sort.field 和 index.sort.order 与复合聚合中使用的源字段和顺序相匹配时，Easysearch 可以更高效地返回结果，并减少内存使用。虽然索引排序在索引过程中会带来轻微的开销，但复合聚合的查询性能提升非常显著。\n以下示例请求为 my-sorted-index 索引中的每个字段设置了排序字段和排序顺序：\nPUT /my-sorted-index { \u0026#34;settings\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;sort.field\u0026#34;: [\u0026#34;customer_id\u0026#34;, \u0026#34;timestamp\u0026#34;], \u0026#34;sort.order\u0026#34;: [\u0026#34;asc\u0026#34;, \u0026#34;desc\u0026#34;] } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;customer_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } 以下请求在 my-sorted-index 索引上创建复合聚合。由于该索引按 customer_id 升序排序，按 timestamp 降序排序，并且聚合源与该排序顺序匹配，因此此查询运行更快，且内存压力减小：\nGET /my-sorted-index/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;my_buckets\u0026#34;: { \u0026#34;composite\u0026#34;: { \u0026#34;size\u0026#34;: 1000, \u0026#34;sources\u0026#34;: [ { \u0026#34;customer\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;customer_id\u0026#34;, \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34; } } }, { \u0026#34;time\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } } ] } } } } ","subcategory":null,"summary":"","tags":null,"title":"复合聚合（Composite）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/composite/"},{"category":null,"content":"基数聚合 #  cardinality 聚合是一种单值指标聚合，用于计算字段的唯一值或不同值的数量。\n基数计数为近似值。有关更多信息，请参阅下面的控制精度。\n相关指南（先读这些） #    聚合基础  聚合场景实践  参数说明 #  cardinality 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需的 String 估计基数的字段。   precision_threshold 可选 Numeric 阈值，低于该阈值的计数预计接近准确值。有关更多信息，请参阅控制精度 。   execution_hint 可选 String 如何运行聚合，该参数会影响资源使用和聚合效率。有效值为 ordinals 和 direct 。   missing 可选 与 field 类型相同 用于存储字段缺失实例的 bucket。如果未提供，则忽略缺失值。    参考样例 #  以下示例请求查找数据中唯一产品 ID 的数量：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;unique_products\u0026#34;: { \u0026#34;cardinality\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.product_id\u0026#34; } } } } 返回内容 #  如以下内容所示，聚合返回 unique_products 变量中的基数计数：\n{ \u0026#34;took\u0026#34;: 176, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;unique_products\u0026#34;: { \u0026#34;value\u0026#34;: 7033 } } } 控制精度 #  准确的基数计算需要将所有值加载到哈希集中并返回其大小。这种方法扩展性较差；它可能需要大量内存并导致高延迟。\n您可以使用 precision_threshold 设置来控制内存和准确度之间的权衡。此参数设置一个阈值，低于该阈值的计数预计会接近准确度。高于此值的计数可能会降低准确度。\nprecision_threshold 的默认值为 3000，支持的最大值为 40000。\n基数聚合使用 HyperLogLog++ 算法 。基数计数通常在精度阈值以下非常准确，并且在大多数情况下，即使阈值低至 100，其与真实计数的误差也在 6% 以内。\n预计算哈希 #  对于高基数字符串字段，存储索引字段的哈希值并计算哈希值的基数可以节省计算和内存资源。请谨慎使用此方法；它仅适用于长字符串和/或高基数的集合。数值字段和内存消耗较低的字符串集合最好直接处理。\n示例：控制精度 #  将精度阈值设置为 10000 唯一值：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;unique_products\u0026#34;: { \u0026#34;cardinality\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;products.product_id\u0026#34;, \u0026#34;precision_threshold\u0026#34;: 10000 } } } } 返回内容与使用默认阈值的结果类似，但返回值略有不同。可以调整 precision_threshold 参数，以查看它如何影响基数估计。\n缺省值处理 #  可以为聚合字段的缺失内容分配一个值。有关详细信息，请参阅缺省聚合。\n替换基数聚合中的缺失值会将替换值添加到唯一值列表中，从而将实际基数增加 1。\n","subcategory":null,"summary":"","tags":null,"title":"基数聚合（Cardinality）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/cardinality/"},{"category":null,"content":"地理边界聚合 #  geo_bounds 聚合是一个多值指标聚合，用于计算包含一组 geo_point 或 geo_shape 对象的地理边界框。边界框以十进制编码的经纬度（lat-lon）对形式返回，作为矩形的左上角和右下角顶点。\n相关指南（先读这些） #    聚合基础  地理位置搜索  Geo 场景实践  参数说明 #  geo_bounds 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 计算地理边界所使用的包含地理点或地理形状的字段的名称。   wrap_longitude 可选 Boolean 是否允许边界框与国际日期变更线重叠。默认值为 true 。    参考样例 #  以下示例查询数据中每个订单的 geo_bounds 的 geoip.location （每个 geoip.location 是一个地理点）：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;geo\u0026#34;: { \u0026#34;geo_bounds\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;geoip.location\u0026#34; } } } } 返回内容 #\n 如以下示例响应所示，聚合返回包含 geoip.location 字段中所有地理点的 geobounds ：\n{ \u0026#34;took\u0026#34;: 16, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;geo\u0026#34;: { \u0026#34;bounds\u0026#34;: { \u0026#34;top_left\u0026#34;: { \u0026#34;lat\u0026#34;: 52.49999997206032, \u0026#34;lon\u0026#34;: -118.20000001229346 }, \u0026#34;bottom_right\u0026#34;: { \u0026#34;lat\u0026#34;: 4.599999985657632, \u0026#34;lon\u0026#34;: 55.299999956041574 } } } } } ","subcategory":null,"summary":"","tags":null,"title":"地理边界聚合（Geo Bounds）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/geobounds/"},{"category":null,"content":"地理距离聚合 #  geo_distance 聚合根据与一个起始 geo_point 字段距离将文档分组到同心圆中。它与 range 聚合相同，只是它作用于地理位置。\n相关指南（先读这些） #    聚合基础  地理位置搜索  Geo 场景实践  例如，你可以使用 geo_distance 聚合来查找你 1 公里范围内的所有披萨店。搜索结果仅限于你指定的 1 公里半径范围内，但你还可以添加另一个在 2 公里范围内找到的结果。\n您只能对映射为 geo_point 的字段使用 geo_distance 聚合。\n点是一个单一的地理坐标，例如您智能手机显示的当前位置。在 Easysearch 中，点表示如下：\n{ \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;point\u0026#34;, \u0026#34;coordinates\u0026#34;: { \u0026#34;lat\u0026#34;: 83.76, \u0026#34;lon\u0026#34;: -81.2 } } } 您还可以将纬度和经度指定为数组 [-81.20, 83.76] 或字符串 \u0026quot;83.76, -81.20\u0026quot;\n此表列出了 geo_distance 聚合的相关字段：\n   字段 必需/可选 描述     field 必需 指定您要处理的地理点字段。   origin 必需 指定用于计算距离的地理点。   ranges 必需 指定一组范围，根据文档与目标点的距离收集文档。   unit 可选 定义 ranges 数组中使用的单位。 unit 默认为 m （米），但你可以切换到其他单位，如 km （千米）、 mi （英里）、 in （英寸）、 yd （码）、 cm （厘米）和 mm （毫米）。   distance_type 可选 指定 Easysearch 如何计算距离。默认值为 sloppy_arc （更快但精度较低），也可以设置为 arc （较慢但最精确）或 plane （最快但精度最低）。由于误差范围较大，仅适用于小地理区域使用 plane 。    语法如下：\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;aggregation_name\u0026#34;: { \u0026#34;geo_distance\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_1\u0026#34;, \u0026#34;origin\u0026#34;: \u0026#34;x, y\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;to\u0026#34;: \u0026#34;value_1\u0026#34; }, { \u0026#34;from\u0026#34;: \u0026#34;value_2\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;value_3\u0026#34; }, { \u0026#34;from\u0026#34;: \u0026#34;value_4\u0026#34; } ] } } } } 这个示例根据与 geo-point 字段的以下距离形成分组：\n 少于 10 公里 从 10 到 20 公里 从 20 到 50 公里 从 50 到 100 公里 超过 100 公里  GET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;position\u0026#34;: { \u0026#34;geo_distance\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;geo.coordinates\u0026#34;, \u0026#34;origin\u0026#34;: { \u0026#34;lat\u0026#34;: 83.76, \u0026#34;lon\u0026#34;: -81.2 }, \u0026#34;ranges\u0026#34;: [ { \u0026#34;to\u0026#34;: 10 }, { \u0026#34;from\u0026#34;: 10, \u0026#34;to\u0026#34;: 20 }, { \u0026#34;from\u0026#34;: 20, \u0026#34;to\u0026#34;: 50 }, { \u0026#34;from\u0026#34;: 50, \u0026#34;to\u0026#34;: 100 }, { \u0026#34;from\u0026#34;: 100 } ] } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;position\u0026#34; : { \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;*-10.0\u0026#34;, \u0026#34;from\u0026#34; : 0.0, \u0026#34;to\u0026#34; : 10.0, \u0026#34;doc_count\u0026#34; : 0 }, { \u0026#34;key\u0026#34; : \u0026#34;10.0-20.0\u0026#34;, \u0026#34;from\u0026#34; : 10.0, \u0026#34;to\u0026#34; : 20.0, \u0026#34;doc_count\u0026#34; : 0 }, { \u0026#34;key\u0026#34; : \u0026#34;20.0-50.0\u0026#34;, \u0026#34;from\u0026#34; : 20.0, \u0026#34;to\u0026#34; : 50.0, \u0026#34;doc_count\u0026#34; : 0 }, { \u0026#34;key\u0026#34; : \u0026#34;50.0-100.0\u0026#34;, \u0026#34;from\u0026#34; : 50.0, \u0026#34;to\u0026#34; : 100.0, \u0026#34;doc_count\u0026#34; : 0 }, { \u0026#34;key\u0026#34; : \u0026#34;100.0-*\u0026#34;, \u0026#34;from\u0026#34; : 100.0, \u0026#34;doc_count\u0026#34; : 14074 } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"地理距离聚合（Geo Distance）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/geo-distance/"},{"category":null,"content":"地理中心点聚合 #  geo_centroid 聚合计算一组 geo_point 值的地理中心或焦点。它将中心位置作为纬度-经度对返回。\n相关指南（先读这些） #    聚合基础  地理位置搜索  Geo 场景实践  参数说明 #  geo_centroid 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 包含计算中心的地理点的字段的名称。    参考样例 #  以下示例返回数据中每个订单的 geo_centroid 的 geoip.location 。每个 geoip.location 都是一个地理点：\nGET /sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;centroid\u0026#34;: { \u0026#34;geo_centroid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;geoip.location\u0026#34; } } } } 返回内容 #  返回内容包括一个 centroid 对象，该对象具有 lat 和 lon 属性，表示所有索引数据点的中心位置：\n{ \u0026#34;took\u0026#34;: 35, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;centroid\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 35.54990372113027, \u0026#34;lon\u0026#34;: -9.079764742533712 }, \u0026#34;count\u0026#34;: 4675 } } } 中心位置位于摩洛哥以北约的大西洋中。鉴于数据库中订单的广泛地理分布，这并不是很有意义。\n嵌套在其他聚合下使用 #  您可以将 geo_centroid 聚合嵌套在 bucket 聚合中，以计算数据子集的中心。\n示例：嵌套在分组聚合下 #  您可以在字符串字段的 terms 分组下嵌套 geo_centroid 聚合。\n要找到每个大洲的订单的 geoip 中心位置，请在 geoip.continent_name 字段内对中心进行子聚合：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;continents\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;geoip.continent_name\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;centroid\u0026#34;: { \u0026#34;geo_centroid\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;geoip.location\u0026#34; } } } } } } 这将返回每个大陆分组的中心位置：\n{ \u0026#34;took\u0026#34;: 34, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4675, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;continents\u0026#34;: { \u0026#34;doc_count_error_upper_bound\u0026#34;: 0, \u0026#34;sum_other_doc_count\u0026#34;: 0, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;Asia\u0026#34;, \u0026#34;doc_count\u0026#34;: 1220, \u0026#34;centroid\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 28.023606536509163, \u0026#34;lon\u0026#34;: 47.83377046025068 }, \u0026#34;count\u0026#34;: 1220 } }, { \u0026#34;key\u0026#34;: \u0026#34;North America\u0026#34;, \u0026#34;doc_count\u0026#34;: 1206, \u0026#34;centroid\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 39.06542286878007, \u0026#34;lon\u0026#34;: -85.36152573149485 }, \u0026#34;count\u0026#34;: 1206 } }, { \u0026#34;key\u0026#34;: \u0026#34;Europe\u0026#34;, \u0026#34;doc_count\u0026#34;: 1172, \u0026#34;centroid\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 48.125767892293325, \u0026#34;lon\u0026#34;: 2.7529009746915243 }, \u0026#34;count\u0026#34;: 1172 } }, { \u0026#34;key\u0026#34;: \u0026#34;Africa\u0026#34;, \u0026#34;doc_count\u0026#34;: 899, \u0026#34;centroid\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 30.780756367941297, \u0026#34;lon\u0026#34;: 13.464182392125318 }, \u0026#34;count\u0026#34;: 899 } }, { \u0026#34;key\u0026#34;: \u0026#34;South America\u0026#34;, \u0026#34;doc_count\u0026#34;: 178, \u0026#34;centroid\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 4.599999985657632, \u0026#34;lon\u0026#34;: -74.10000007599592 }, \u0026#34;count\u0026#34;: 178 } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"地理中心点聚合（Geo Centroid）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/geocentroid/"},{"category":null,"content":"可变宽度直方图聚合 #  variable_width_histogram 聚合与标准 histogram 聚合类似，但它会自动调整每个桶的宽度，使数据点在各桶之间尽可能均匀分布。该聚合使用聚类算法，根据数据的实际分布动态确定最优的桶边界，而不是使用固定间隔。\n这在数据分布不均匀时特别有用 —— 例如，大部分值集中在某个范围内，但也有少量离群值。使用固定间隔的 histogram 可能导致大量空桶或单个桶内文档过多，而 variable_width_histogram 能自适应地解决这些问题。\n相关指南（先读这些） #    聚合基础  聚合场景实践  直方图聚合（固定宽度版本）  参数说明 #     参数 必需/可选 数据类型 描述     field 必填 String 要聚合的数值字段。必须为数值类型。也可使用 script 替代。   buckets 可选 Integer 期望的桶数量。实际返回的桶数量可能小于或等于此值。默认 10。必须大于 0。   shard_size 可选 Integer 每个分片上用于聚类的文档数量。值越大结果越精确，但内存消耗越大。默认为 buckets * 50。必须大于 1。   initial_buffer 可选 Integer 初始缓冲区大小，用于收集初始数据点以启动聚类算法。默认为 min(10 * shard_size, 50000)。必须大于 0 且不小于 buckets。   script 可选 Object 使用脚本动态生成聚合值。   missing 可选 Numeric 缺少字段值的文档所使用的替代值。    基本用法 #  以下示例将商品价格分成 5 个自适应宽度的桶：\nGET products/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;price_distribution\u0026#34;: { \u0026#34;variable_width_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34;, \u0026#34;buckets\u0026#34;: 5 } } } } 返回内容中，每个桶包含 min、max、key（桶的中心值）和 doc_count：\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;price_distribution\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;min\u0026#34;: 5.0, \u0026#34;key\u0026#34;: 15.3, \u0026#34;max\u0026#34;: 25.0, \u0026#34;doc_count\u0026#34;: 120 }, { \u0026#34;min\u0026#34;: 25.0, \u0026#34;key\u0026#34;: 42.7, \u0026#34;max\u0026#34;: 60.0, \u0026#34;doc_count\u0026#34;: 95 }, { \u0026#34;min\u0026#34;: 60.0, \u0026#34;key\u0026#34;: 85.2, \u0026#34;max\u0026#34;: 110.0, \u0026#34;doc_count\u0026#34;: 88 }, { \u0026#34;min\u0026#34;: 110.0, \u0026#34;key\u0026#34;: 150.0, \u0026#34;max\u0026#34;: 200.0, \u0026#34;doc_count\u0026#34;: 45 }, { \u0026#34;min\u0026#34;: 200.0, \u0026#34;key\u0026#34;: 350.5, \u0026#34;max\u0026#34;: 500.0, \u0026#34;doc_count\u0026#34;: 12 } ] } } } 注意与固定间隔 histogram 的区别 —— 每个桶的宽度不同，但文档数量更加均匀。\n调整精度 #  通过增加 shard_size 可以提高聚类精度（但会消耗更多内存）：\nGET products/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;price_distribution\u0026#34;: { \u0026#34;variable_width_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34;, \u0026#34;buckets\u0026#34;: 10, \u0026#34;shard_size\u0026#34;: 1000, \u0026#34;initial_buffer\u0026#34;: 5000 } } } } histogram vs. variable_width_histogram #     特性 histogram variable_width_histogram     桶宽度 固定（由 interval 决定） 可变（由数据分布决定）   桶边界 均匀分布 根据聚类算法自适应   文档分布 可能极不均匀 尽量均匀   参数 需指定 interval 需指定 buckets 数量   适用场景 数据分布均匀，需要固定区间 数据分布不均匀，需要自适应分桶    注意事项 #   variable_width_histogram 的结果是近似的，基于每个分片上的聚类算法。多分片环境下，结果可能因分片数据分布不同而有所差异。 initial_buffer 必须大于或等于 buckets，否则会报错。 shard_size 的 3/4 必须大于或等于 buckets，否则会报错。 该聚合不支持子聚合（如嵌套 avg 或 terms）。 仅支持数值类型字段。  ","subcategory":null,"summary":"","tags":null,"title":"可变宽度直方图聚合（Variable Width Histogram）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/variable-width-histogram/"},{"category":null,"content":"反向嵌套聚合 #  您可以将嵌套文档中的值聚合到其父文档中；这种聚合称为 reverse_nested。您可以使用 reverse_nested 在按嵌套对象中的字段分组后，聚合父文档中的字段。reverse_nested 聚合将\u0026quot;连接回\u0026quot;根页面，并为您的各种变体获取 load_time。\nreverse_nested 聚合是嵌套聚合中的一个子聚合。它接受一个名为 path 的选项。此选项定义 Easysearch 在计算聚合时在文档层次结构中向后退多少步。\n相关指南（先读这些） #    聚合基础  Nested 建模  聚合场景实践  GET logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;response\u0026#34;: \u0026#34;200\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;pages\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;pages\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;top_pages_per_load_time\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;pages.load_time\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;comment_to_logs\u0026#34;: { \u0026#34;reverse_nested\u0026#34;: {}, \u0026#34;aggs\u0026#34;: { \u0026#34;min_load_time\u0026#34;: { \u0026#34;min\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;pages.load_time\u0026#34; } } } } } } } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;pages\u0026#34; : { \u0026#34;doc_count\u0026#34; : 2, \u0026#34;top_pages_per_load_time\u0026#34; : { \u0026#34;doc_count_error_upper_bound\u0026#34; : 0, \u0026#34;sum_other_doc_count\u0026#34; : 0, \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : 200.0, \u0026#34;doc_count\u0026#34; : 1, \u0026#34;comment_to_logs\u0026#34; : { \u0026#34;doc_count\u0026#34; : 1, \u0026#34;min_load_time\u0026#34; : { \u0026#34;value\u0026#34; : null } } }, { \u0026#34;key\u0026#34; : 500.0, \u0026#34;doc_count\u0026#34; : 1, \u0026#34;comment_to_logs\u0026#34; : { \u0026#34;doc_count\u0026#34; : 1, \u0026#34;min_load_time\u0026#34; : { \u0026#34;value\u0026#34; : null } } } ] } } } } 返回内容显示日志索引有一页带有 load_time 200，还有一页带有 load_time 500。\n","subcategory":null,"summary":"","tags":null,"title":"反向嵌套聚合（Reverse Nested）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/reverse-nested/"},{"category":null,"content":"加权平均聚合 #  weighted_avg 聚合计算跨文档数值的加权平均值。当您想计算平均值，但希望某些数据点的权重大于其他数据点时，此功能非常有用。\n相关指南（先读这些） #    聚合基础  聚合场景实践  加权平均值使用公式 $\\frac{\\sum_{i=1}^n value_i \\cdot weight_i}{\\sum_{i=1}^n weight_i}$ 计算。\n参数说明 #  weighted_avg 聚合采用以下参数。\n   参数 必需/可选 描述     value 必需 定义如何获取要计算平均值的数值。需要 field 或 script 。   weight 必需 定义如何获取每个值的权重。需要 field 或 script 。   format 可选 DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。   value_type 可选 使用脚本或未映射字段时的值的类型提示。    可以在 value 或 weight 内指定以下参数。\n   参数 必需/可选 描述     field 可选 用于值或权重的文档字段。   missing 可选 字段缺失时使用的默认值或权重。请参阅缺失值。    参考样例 #  首先，创建索引并索引一些数据。请注意，产品 C 缺少 rating 和 num_reviews 字段：\nPOST _bulk { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Product A\u0026#34;, \u0026#34;rating\u0026#34;: 4.5, \u0026#34;num_reviews\u0026#34;: 100 } { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Product B\u0026#34;, \u0026#34;rating\u0026#34;: 3.8, \u0026#34;num_reviews\u0026#34;: 50 } { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Product C\u0026#34;} 以下请求使用 weighted_avg 聚合来计算加权平均产品评分。在此上下文中，每个产品的评分都由其 num_reviews 加权。这意味着，评论较多的产品对最终平均值的影响将大于评论较少的产品：\nGET /products/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;weighted_rating\u0026#34;: { \u0026#34;weighted_avg\u0026#34;: { \u0026#34;value\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;rating\u0026#34; }, \u0026#34;weight\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;num_reviews\u0026#34; }, \u0026#34;format\u0026#34;: \u0026#34;#.##\u0026#34; } } } } 返回内容 #  响应包含 weighted_rating ，计算结果为 weighted_avg = (4.5 * 100 + 3.8 * 50) / (100 + 50) = 4.27 。仅考虑同时包含 rating 和 num_reviews 值的文档 1 和 2：\n{ \u0026#34;took\u0026#34;: 18, \u0026#34;timed_out\u0026#34;: false, \u0026#34;terminated_early\u0026#34;: true, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;weighted_rating\u0026#34;: { \u0026#34;value\u0026#34;: 4.266666650772095, \u0026#34;value_as_string\u0026#34;: \u0026#34;4.27\u0026#34; } } } 缺省值 #  使用 missing 参数，您可以为缺少 value 字段或 weight 字段的文档指定默认值，而不是将它们从计算中排除。\n例如，您可以为没有评级的产品分配 3.0 的“平均”评级，并将 num_reviews 设置为 1，以赋予它们较小的非零权重：\nGET /products/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;weighted_rating\u0026#34;: { \u0026#34;weighted_avg\u0026#34;: { \u0026#34;value\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;rating\u0026#34;, \u0026#34;missing\u0026#34;: 3.0 }, \u0026#34;weight\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;num_reviews\u0026#34;, \u0026#34;missing\u0026#34;: 1 }, \u0026#34;format\u0026#34;: \u0026#34;#.##\u0026#34; } } } } 新的加权平均值计算为 weighted_avg = (4.5 * 100 + 3.8 * 50 + 3.0 * 1) / (100 + 50 + 1) = 4.26 ：\n{ \u0026#34;took\u0026#34;: 27, \u0026#34;timed_out\u0026#34;: false, \u0026#34;terminated_early\u0026#34;: true, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;weighted_rating\u0026#34;: { \u0026#34;value\u0026#34;: 4.258278129906055, \u0026#34;value_as_string\u0026#34;: \u0026#34;4.26\u0026#34; } } } ","subcategory":null,"summary":"","tags":null,"title":"加权平均聚合（Weighted Avg）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/weighted-avg/"},{"category":null,"content":"分布式搜索执行过程 #  当你向 Easysearch 发起一个 /_search 请求时，集群内部会发生一系列分布式协作。理解这个过程能帮助你解释很多“看起来奇怪”的现象：为什么分页越深越慢、为什么结果顺序会抖动、为什么加副本能提高吞吐、为什么有时相关性会“不一致”。\n在一个典型的搜索执行中，Easysearch 会经历两个阶段：\n Query phase（查询阶段）：找出每个分片上的 top-n 候选，并在协调节点合并成全局 top-n Fetch phase（取回阶段）：只取回最终需要返回的那一页文档内容，并做必要的“丰富”  Query phase：每个分片产出本地 top-n #  查询阶段会把请求广播到目标索引涉及的每个分片拷贝（主分片或副本分片）。每个分片本地执行查询，并构建一个 优先队列（priority queue），保存本分片的 top-n 匹配结果。\n优先队列的大小取决于分页参数：\nGET /_search { \u0026#34;from\u0026#34;: 90, \u0026#34;size\u0026#34;: 10 } 这个请求意味着需要找出“第 91～100 条”结果，所以每个分片需要构建长度为 from + size = 100 的优先队列，才有可能保证全局合并后不漏掉候选。\n查询阶段（概念流程）：\n 客户端把 search 请求发给某个节点，该节点成为协调节点 协调节点将请求转发到所有相关分片（主或副本） 每个分片本地执行查询，返回：  文档 ID 排序所需的值（例如 _score，或排序字段的值）   协调节点把所有分片返回的候选合并到全局优先队列，得到“全局有序的 top-(from+size)”   为什么副本能提高吞吐：搜索请求可以由主分片或副本分片处理，副本越多、硬件越多，可并行处理的搜索请求也越多。\n Fetch phase：只取回最终需要返回的那一页 #  查询阶段只确定“哪些文档应该在结果里”，但并没有把文档内容取回。取回阶段会：\n 协调节点根据 from/size 丢弃前 from 条，只保留最终要返回的 size 条 协调节点向这些文档所在的分片发起批量 GET（multi-get）请求 分片加载文档内容（通常来自 _source），并按需做“丰富”，例如高亮片段 协调节点汇总后返回给客户端  深分页（Deep pagination）：为什么会越来越慢 #  Query-then-fetch 支持 from/size 分页，但深分页会把成本放大：\n 每个分片都要构建 from + size 的队列 协调节点需要对 number_of_shards * (from + size) 的候选做合并排序 过程中会消耗更多 CPU、内存、网络带宽  经验上，深分页到 10,000～50,000 级别在一些场景仍可接受，但当 from 很大时，排序合并会变得非常沉重。更重要的是：深分页很少符合真实用户行为；大量深分页请求常来自爬虫/机器人。\n如果你需要批量遍历大量结果，更推荐使用更合适的检索方式（例如滚动/游标类方案，或 search_after，见「分页与排序」章节）。\n搜索选项：影响路由、稳定性与相关性 #  preference：让结果不“抖动” #  分片副本会被轮询使用。若你按某个字段排序，而多个文档在该排序键上“相同”，不同分片拷贝的合并顺序可能会导致结果顺序在刷新页面时发生变化（bouncing results）。\n使用 preference 传入一个稳定的值（例如 session id / user id），可以让同一个用户更倾向于命中同一组分片拷贝，从而减少顺序抖动：\nGET /_search?preference=\u0026lt;session-id\u0026gt; timeout：允许返回“部分结果” #  搜索总体耗时通常由“最慢分片 + 合并”决定。timeout 用来限制分片允许执行的最大时间：超时后可能返回部分结果，并在响应中用 timed_out 标记。\n需要注意：\n 超时检查往往是“分片逐文档评估”的，并不覆盖所有前置开销 某些昂贵步骤可能让总体耗时仍超过你设定的超时时间  routing：只搜相关分片 #  如果你在写入时使用了自定义 routing，使相关文档落在同一分片上，那么搜索时也可以带上 routing 值，避免扫描所有分片：\nGET /_search?routing=user_1,user_2 这对多租户/按用户隔离的数据模型尤其有效（见「多租户」章节）。\nsearch_type：相关性更精确（代价更高） #  默认搜索类型是 query_then_fetch。在某些需要更精确相关性统计的场景，可以选择 dfs_query_then_fetch（多一个分布式词频统计的预查询阶段），以获得更“全局一致”的词频/相关性计算，但代价是更高的开销：\nGET /_search?search_type=dfs_query_then_fetch 游标查询（Scroll） #  from/size 分页不适合大批量文档遍历。如果你需要从 Easysearch 中高效地取回大量文档，可以使用 Scroll 查询。\n游标查询类似于传统数据库中的 cursor，先做查询初始化，然后批量拉取结果：\nGET /my_index/_search?scroll=1m { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;sort\u0026#34; : [\u0026#34;_doc\u0026#34;], \u0026#34;size\u0026#34;: 1000 }  scroll=1m：保持游标查询窗口一分钟 _doc 排序：最高效的排序方式（去掉全局排序的开销）  查询返回结果中包含 _scroll_id 字段，传递它获取下一批结果：\nGET /_search/scroll { \u0026#34;scroll\u0026#34;: \u0026#34;1m\u0026#34;, \u0026#34;scroll_id\u0026#34;: \u0026#34;cXVlcnlUaGVuRmV0Y2g7NTsx...\u0026#34; }  ⚠️ 游标查询取的是某个时间点的快照数据，初始化之后索引上的任何变化会被忽略。每次查询都需要用前一次返回的 _scroll_id，当没有更多结果返回时，表示所有匹配文档已处理完毕。\n  💡 字段 size 作用于单个分片，所以每个批次实际返回的文档数量最大为 size * number_of_primary_shards。\n 小结 #   分布式搜索通常是 query/fetch 两阶段：先找候选，再取回文档内容 from/size 深分页会放大每分片队列与协调节点合并成本，应谨慎使用 preference 可减少结果顺序抖动；routing 可减少扫描分片范围 timeout 可能返回部分结果；dfs_query_then_fetch 以更高成本换取更一致的相关性统计 大批量遍历应使用 Scroll 游标查询或 search_after  下一步可以继续阅读：\n  分页与排序  多租户  相关性基础  ","subcategory":null,"summary":"","tags":null,"title":"分布式搜索执行过程","url":"/easysearch/main/docs/fundamentals/distributed-search/"},{"category":null,"content":"写入数据文本向量化 #  Easysearch 使用 Ingest 管道中的一系列处理器，可以对写入的数据进行处理，并且支持对文本进行向量化。本文档介绍如何在 Easysearch 中使用 text_embedding 处理器对写入数据进行向量化。\n相关指南（先读这些） #    向量搜索  向量字段建模  AI 集成  先决条件 #  支持与 OpenAI API 兼容的 embedding 接口，支持 Ollama embedding 接口。\n需要安装 Easysearch 的 knn 和 ai 插件。\n在生产环境中使用数据采集时，您的集群应至少包含一个节点，且该节点的节点角色权限设置为 ingest 。\n创建带有向量字段的索引 #  首先，需要创建一个包含 knn mapping 的索引，text_vector 是存储向量的字段，向量维度是 768。\nPUT /my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text_vector\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 768, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;L\u0026#34;: 99, \u0026#34;k\u0026#34;: 1 } } } } } 创建或更新 text_embedding 处理器 #  请求路径：\nPUT _ingest/pipeline/\u0026lt;pipeline-id\u0026gt; 请求示例：\nPUT _ingest/pipeline/text-embedding-pipeline { \u0026#34;description\u0026#34;: \u0026#34;用于生成文本嵌入向量的管道\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;text_embedding\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;https://api.openai.com/v1/embeddings\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;api_key\u0026#34;: \u0026#34;\u0026lt;api_key\u0026gt;\u0026#34;, \u0026#34;text_field\u0026#34;: \u0026#34;input_text\u0026#34;, \u0026#34;vector_field\u0026#34;: \u0026#34;text_vector\u0026#34;, \u0026#34;model_id\u0026#34;: \u0026#34;text-embedding-3-small\u0026#34;, \u0026#34;dims\u0026#34;: 768, \u0026#34;ignore_missing\u0026#34;: false, \u0026#34;ignore_failure\u0026#34;: false } } ] } 请求体字段： #  下表列出了用于创建或更新管道的请求体字段。\n   参数 是否必填 类型 说明     processors 必填 数组 处理器列表，按顺序执行。示例中仅包含 text_embedding 处理器。   description 可选 字符串 管道的描述信息，用于说明用途（如“生成文本嵌入向量”）。   url 必填 字符串 Embedding API 的完整 URL（如 https://api.openai.com/v1/embeddings ）。   vendor 必填 字符串 服务提供商标识，固定为 \u0026quot;openai\u0026quot; 或 \u0026quot;ollama\u0026quot;。   api_key 必填 字符串 API 密钥，需替换为实际值（如 \u0026quot;sk-xxx\u0026quot;）。   text_field 必填 字符串 输入文本的字段名（如 \u0026quot;input_text\u0026quot;），需与索引中的字段匹配。   vector_field 必填 字符串 存储向量的目标字段名（如 \u0026quot;text_vector\u0026quot;），必须包含在索引的 knn mapping 里。   model_id 必填 字符串 模型名称（如 \u0026quot;text-embedding-3-small\u0026quot; 或 \u0026quot;text-embedding-v3\u0026quot;）。   dims 必填 整数 向量维度（需与所选模型匹配，如 768 对应 text-embedding-3-small）,并与 mapping 里的 dims 保持一致。   ignore_missing 可选 布尔值 若 text_field 不存在是否跳过处理（默认 false）。   ignore_failure 可选 布尔值 若处理失败是否继续执行后续处理器（默认 false）。    若使用国内大模型，例如阿里云的千问，需替换为\nurl: https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings model_id: text-embedding-v3 dims: 根据模型确定\n路径参数： #     参数 是否必填 类型 说明     pipeline-id 必填 字符串 分配给管道的唯一标识符，即管道 ID。    若已部署 Ollama 服务，按下面示例使用：\nPUT _ingest/pipeline/ollama-embedding-pipeline { \u0026#34;description\u0026#34;: \u0026#34;Ollama embedding 示例\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;text_embedding\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://localhost:11434/api/embed\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;ollama\u0026#34;, \u0026#34;text_field\u0026#34;: \u0026#34;input_text\u0026#34;, \u0026#34;vector_field\u0026#34;: \u0026#34;text_vector\u0026#34;, \u0026#34;model_id\u0026#34;: \u0026#34;nomic-embed-text:latest\u0026#34;, \u0026#34;ignore_missing\u0026#34;: false, \u0026#34;ignore_failure\u0026#34;: false } } ] } 查看 text_embedding 处理器 #  查看 text_embedding 处理器与查看其他处理器的 api 一致：\nGET _ingest/pipeline 返回输出：\n{ \u0026#34;ollama-embedding-pipeline\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Ollama embedding 示例\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;text_embedding\u0026#34;: { \u0026#34;ignore_failure\u0026#34;: false, \u0026#34;vendor\u0026#34;: \u0026#34;ollama\u0026#34;, \u0026#34;vector_field\u0026#34;: \u0026#34;text_vector\u0026#34;, \u0026#34;text_field\u0026#34;: \u0026#34;input_text\u0026#34;, \u0026#34;ignore_missing\u0026#34;: false, \u0026#34;model_id\u0026#34;: \u0026#34;nomic-embed-text:latest\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://localhost:11434/api/embed\u0026#34; } } ] }, \u0026#34;text-embedding-pipeline\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;用于生成文本嵌入向量的管道\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;text_embedding\u0026#34;: { \u0026#34;ignore_failure\u0026#34;: false, \u0026#34;dims\u0026#34;: 768, \u0026#34;api_key\u0026#34;: \u0026#34;*************************************************vE\u0026#34;, \u0026#34;vendor\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;vector_field\u0026#34;: \u0026#34;text_vector\u0026#34;, \u0026#34;text_field\u0026#34;: \u0026#34;input_text\u0026#34;, \u0026#34;ignore_missing\u0026#34;: false, \u0026#34;model_id\u0026#34;: \u0026#34;text-embedding-3-small\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://poloai.top/v1/embeddings\u0026#34; } } ] } } api_key 会被遮掩，防止泄露\n使用 text_embedding 处理器进行写入时转换 #  与使用其他处理器一致,\nPOST /_bulk?pipeline=text-embedding-pipeline\u0026amp;pretty\u0026amp;refresh=wait_for { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34; } } { \u0026#34;input_text\u0026#34;: \u0026#34;第一个批量处理的文本。\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;bulk api example 1\u0026#34; } { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;bulk_doc_2\u0026#34; } } { \u0026#34;input_text\u0026#34;: \u0026#34;第二个批量处理的文本，指定了ID。\u0026#34;, \u0026#34;priority\u0026#34;: 1 } { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34; } } { \u0026#34;input_text\u0026#34;: \u0026#34;这是另一示例文本。\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;bulk\u0026#34;, \u0026#34;test\u0026#34;] } 查看写入结果\nGET my-index/_search 返回输出：\n{ \u0026#34;took\u0026#34;: 1, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;dd2DAZgB6WXmvHRNYksX\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;input_text\u0026#34;: \u0026#34;第一个批量处理的文本。\u0026#34;, \u0026#34;text_vector\u0026#34;: [ 0.012978158, 0.007739224, -0.015867598, 0.005287578, ...... 删除 text_embedding 处理器 #  与删除其他处理器一致\nDELETE _ingest/pipeline/text-embedding-pipeline ","subcategory":null,"summary":"","tags":null,"title":"写入数据文本向量化","url":"/easysearch/main/docs/integrations/ai/ai-api/ingest-text-embedding/"},{"category":null,"content":"Keyword Repeat 分词过滤器 #  keyword_repeat 分词过滤器会将词元的关键词版本发送到词元流中。当你希望在经过进一步的词元转换（例如词干提取或同义词扩展）之后，既保留原始词元，又保留其修改后的版本时，通常会使用这个过滤器。这些重复的词元使得原始的、未改变的词元版本能够与修改后的版本一起保留在最终的分析结果中。\n 注意：keyword_repeat 分词过滤器应该放置在词干提取过滤器之前。词干提取并非应用于每个词元，因此在词干提取之后，你可能会在同一位置得到重复的词元。为了去除重复词元，可以在词干提取器之后使用 remove_duplicates 分词过滤器。\n 相关指南（先读这些） #    文本分析：词干提取  文本分析：规范化  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词重复过滤器的分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_kstem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;kstem\u0026#34; }, \u0026#34;my_lowercase\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;lowercase\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;my_lowercase\u0026#34;, \u0026#34;keyword_repeat\u0026#34;, \u0026#34;my_kstem\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Stopped quickly\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;stopped\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;quickly\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;quick\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } 你可以通过在 _analyze 查询中添加以下参数，进一步研究关键词重复分词过滤器的影响：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Stopped quickly\u0026#34;, \u0026#34;explain\u0026#34;: true, \u0026#34;attributes\u0026#34;: \u0026#34;keyword\u0026#34; } 返回内容中包含了详细信息，例如分词情况、过滤操作以及特定分词过滤器的应用情况：\n{ \u0026#34;detail\u0026#34;: { \u0026#34;custom_analyzer\u0026#34;: true, \u0026#34;charfilters\u0026#34;: [], \u0026#34;tokenizer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Stopped\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;quickly\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] }, \u0026#34;tokenfilters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;my_lowercase\u0026#34;, \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;stopped\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;quickly\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] }, { \u0026#34;name\u0026#34;: \u0026#34;keyword_repeat\u0026#34;, \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;stopped\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0, \u0026#34;keyword\u0026#34;: true }, { \u0026#34;token\u0026#34;: \u0026#34;stopped\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0, \u0026#34;keyword\u0026#34;: false }, { \u0026#34;token\u0026#34;: \u0026#34;quickly\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1, \u0026#34;keyword\u0026#34;: true }, { \u0026#34;token\u0026#34;: \u0026#34;quickly\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1, \u0026#34;keyword\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;my_kstem\u0026#34;, \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;stopped\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0, \u0026#34;keyword\u0026#34;: true }, { \u0026#34;token\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0, \u0026#34;keyword\u0026#34;: false }, { \u0026#34;token\u0026#34;: \u0026#34;quickly\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1, \u0026#34;keyword\u0026#34;: true }, { \u0026#34;token\u0026#34;: \u0026#34;quick\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1, \u0026#34;keyword\u0026#34;: false } ] } ] } } ","subcategory":null,"summary":"","tags":null,"title":"关键字重复分词过滤器（Keyword Repeat）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keyword-repeat/"},{"category":null,"content":"Keyword Marker 分词过滤器 #  keyword_marker 分词过滤器用于防止某些词元被词干提取器或其他过滤器修改。该过滤器通过将指定的词元标记为关键词来实现这一点，这样就能避免词干提取或其他方式的处理。这确保了特定的单词能保持其原始形式。\n相关指南（先读这些） #    文本分析：词干提取  文本分析：规范化  参数说明 #  关键词标记分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     ignore_case 可选 布尔值 在匹配关键词时是否忽略字母大小写。默认值为false。   keywords 若未设置 keywords_path 或 keywords_pattern 则为必需 字符串列表 要标记为关键词的词元列表。   keywords_path 若未设置 keywords 或 keywords_pattern 则为必需 字符串 关键词列表的路径（相对于配置目录或绝对路径）。   keywords_pattern 若未设置 keywords 或 keywords_path 则为必需 字符串 用于匹配要标记为关键词的词元的正则表达式。    参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词标记过滤器的分词器。该过滤器将单词 example 标记为关键词：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;keyword_marker_filter\u0026#34;, \u0026#34;stemmer\u0026#34;] } }, \u0026#34;filter\u0026#34;: { \u0026#34;keyword_marker_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword_marker\u0026#34;, \u0026#34;keywords\u0026#34;: [\u0026#34;example\u0026#34;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Favorite example\u0026#34; } 返回中包含了生成的词元。请注意，虽然单词 favorite 进行了词干提取，但单词 example 未进行词干提取，因为它被标记为了关键词。\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;favorit\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;start_offset\u0026#34;: 9, \u0026#34;end_offset\u0026#34;: 16, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } 你可以通过在 _analyze 查询中添加以下参数，进一步研究关键词标记分词过滤器的影响：\nGET /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;This is an Easysearch example demonstrating keyword marker.\u0026#34;, \u0026#34;explain\u0026#34;: true, \u0026#34;attributes\u0026#34;: \u0026#34;keyword\u0026#34; } 这将在返回内容中生成类似如下的更多详细信息：\n{ \u0026#34;name\u0026#34;: \u0026#34;porter_stem\u0026#34;, \u0026#34;tokens\u0026#34;: [ ... { \u0026#34;token\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;start_offset\u0026#34;: 22, \u0026#34;end_offset\u0026#34;: 29, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4, \u0026#34;keyword\u0026#34;: true }, { \u0026#34;token\u0026#34;: \u0026#34;demonstr\u0026#34;, \u0026#34;start_offset\u0026#34;: 30, \u0026#34;end_offset\u0026#34;: 43, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5, \u0026#34;keyword\u0026#34;: false }, ... ] } ","subcategory":null,"summary":"","tags":null,"title":"关键字标记分词过滤器（Keyword Marker）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keyword-marker/"},{"category":null,"content":"Keyword 字段类型 #  keyword 字段类型包含未经分析的字符串。它只允许精确的大小写敏感匹配。\n默认情况下，keyword 字段既被索引（因为 index 已启用）也存储在磁盘上（因为 doc_values 已启用）。为了减少磁盘空间，您可以通过将 index 设置为 false 来指定不索引 keyword 字段。\n 提示：如果您需要对字段进行全文搜索，请将其映射为 text 类型。\n 相关指南（先读这些） #    映射基础  映射模式  结构化搜索  代码样例 #  以下查询创建了一个带有 keyword 字段的映射。将 index 设置为 false 指定将 genre 字段存储在磁盘上，并使用 doc_values 检索它：\nPUT movies { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;genre\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;keyword\u0026#34;, \u0026#34;index\u0026#34; : false } } } } 参数说明 #  下表列出了 keyword 字段类型接受的参数。所有参数都是可选的。\n   参数 描述     boost 浮点值，指定此字段对相关性分数的权重。大于 1.0 的值会增加字段的相关性。0.0 到 1.0 之间的值会降低字段的相关性。默认值为 1.0。   doc_values 布尔值，指定是否应将字段存储在磁盘上，以便可以用于聚合、排序或脚本。默认值为 true。   eager_global_ordinals 指定是否应在刷新时立即加载全局序号。如果该字段经常用于聚合，此参数应设置为 true。默认值为 false。   fields 要以多种方式索引同一个字符串（例如，作为 keyword 和 text），请提供 fields 参数。您可以指定字段的一个版本用于搜索，另一个版本用于排序和聚合。   ignore_above 任何长度超过此整数值的字符串都不应被索引。默认值为 2147483647。默认动态映射会创建一个 ignore_above 设置为 256 的 keyword 子字段。   index 布尔值，指定字段是否应可搜索。默认值为 true。要减少磁盘空间，请将 index 设置为 false。   index_options 要存储在索引中的信息，将在计算相关性分数时考虑。可以设置为 freqs 以获取词频。默认值为 docs。   meta 接受此字段的元数据。   normalizer 指定在索引之前如何预处理此字段（例如，转换为小写）。默认值为 null（无预处理）。   norms 布尔值，指定在计算相关性分数时是否应使用字段长度。默认值为 false。   null_value 用于替代 null 的值。必须与字段类型相同。如果未指定此参数，当值为 null 时，该字段将被视为缺失。默认值为 null。   similarity 用于计算相关性分数的排名算法。默认值为 BM25。   split_queries_on_whitespace 布尔值，指定全文查询是否应在空格上分割。默认值为 false。   store 布尔值，指定字段值是否应该被存储并且可以与 _source 字段分开检索。默认值为 false。    典型用法 #  1. 精确匹配查询 #  keyword 字段最常见的用途是精确匹配，配合 term 或 terms 查询使用：\nGET my-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;published\u0026#34; } } } 2. 聚合与排序 #  keyword 字段天然支持聚合和排序（通过 doc_values），无需额外配置：\nGET my-index/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;categories\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;category\u0026#34;, \u0026#34;size\u0026#34;: 20 } } } } 3. text + keyword 多字段模式 #  最常见的实战模式是让同一个字段同时支持全文搜索和精确匹配/聚合：\nPUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 256 } } } } } }  搜索时使用 title 字段（全文分词匹配） 聚合/排序时使用 title.keyword 字段（精确值）   动态映射默认就会为字符串创建这种 text + keyword 多字段结构。\n 4. 使用 normalizer 统一大小写 #  如果需要忽略大小写进行精确匹配，可以配置 normalizer：\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;normalizer\u0026#34;: { \u0026#34;lowercase_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;lowercase_normalizer\u0026#34; } } } } 5. 控制 ignore_above #  动态映射会默认为 keyword 子字段设置 ignore_above: 256。如果需要索引更长的字符串，可以手动设置：\nPUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 1024 } } } }  超过 ignore_above 长度的值仍然会存储在 _source 中，只是不会被索引和聚合。\n keyword 与 text 的对比 #     特性 keyword text     是否分词 ❌ 不分词 ✅ 经过分词器处理   精确匹配 ✅ 支持 ❌ 不支持（分词后无法精确匹配原始值）   全文搜索 ❌ 不支持 ✅ 支持   排序/聚合 ✅ 默认支持（doc_values） ⚠️ 需要启用 fielddata（不推荐）   适用场景 状态码、标签、ID、邮箱 文章标题、正文、描述    注意事项 #   keyword 字段的值区分大小写，如需忽略大小写请使用 normalizer 对于超高基数字段（如用户 ID、UUID），聚合时注意内存消耗 如果字段仅用于过滤而不需要评分，建议设置 norms: false（keyword 默认已是 false）以节省空间  ","subcategory":null,"summary":"","tags":null,"title":"关键字字段类型（Keyword）","url":"/easysearch/main/docs/features/mapping-and-analysis/field-types/string-field-type/keyword/"},{"category":null,"content":"全局聚合 #  global 聚合让你能跳出过滤聚合的聚合上下文。即使你包含了一个缩小文档集的过滤查询，global 聚合仍然对所有文档进行聚合，就好像过滤查询不存在一样。它忽略 filter 聚合，并隐式地假设 match_all 查询。\n相关指南（先读这些） #    聚合基础  聚合场景实践  以下示例返回索引中所有文档的 taxful_total_price 字段的 avg 值：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;taxful_total_price\u0026#34;: { \u0026#34;lte\u0026#34;: 50 } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;total_avg_amount\u0026#34;: { \u0026#34;global\u0026#34;: {}, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } } } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;total_avg_amount\u0026#34; : { \u0026#34;doc_count\u0026#34; : 4675, \u0026#34;avg_price\u0026#34; : { \u0026#34;value\u0026#34; : 75.05542864304813 } } } } 你可以看到， taxful_total_price 字段的平均值是 75.05，而不是 filter 示例中查询匹配时看到的 38.36。\n典型使用场景 #  对比分析：子集 vs. 全局 #  global 聚合最常见的用途是将某个查询子集的统计值与全局统计值进行对比。例如，同时查看低价商品均价和全部商品均价：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;taxful_total_price\u0026#34;: { \u0026#34;lte\u0026#34;: 50 } } }, \u0026#34;aggs\u0026#34;: { \u0026#34;filtered_avg\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } }, \u0026#34;all_products\u0026#34;: { \u0026#34;global\u0026#34;: {}, \u0026#34;aggs\u0026#34;: { \u0026#34;global_avg\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } } } } } } 返回结果中，filtered_avg 反映了查询命中文档（价格 ≤50）的均价，而 all_products.global_avg 反映了所有文档的均价。\n 注意：global 聚合只能作为顶层聚合使用，不能嵌套在其他聚合内部。\n ","subcategory":null,"summary":"","tags":null,"title":"全局聚合（Global）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/global/"},{"category":null,"content":"值计数聚合 #  value_count 聚合是一个单值指标聚合，用于计算聚合所基于的值的数量。\n例如，您可以将 value_count 指标与 avg 指标一起使用来查找聚合使用多少个数字来计算平均值。\n相关指南（先读这些） #    聚合基础  聚合场景实践  GET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;number_of_values\u0026#34;: { \u0026#34;value_count\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;taxful_total_price\u0026#34; } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;number_of_values\u0026#34; : { \u0026#34;value\u0026#34; : 4675 } } } 参数说明 #     参数 必需/可选 数据类型 描述     field 必填 String 要计数的字段名称。   script 可选 Object 使用脚本生成要计数的值，替代 field。   missing 可选 任意 为缺失该字段的文档提供默认值。    value_count vs. cardinality #  value_count 统计的是字段值的 总出现次数（包括重复值），而 cardinality 统计的是 唯一值 的数量：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;total_values\u0026#34;: { \u0026#34;value_count\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;customer_id\u0026#34; } }, \u0026#34;unique_customers\u0026#34;: { \u0026#34;cardinality\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;customer_id\u0026#34; } } } }    聚合 含义 示例结果     value_count 字段值总出现次数（含重复） 4675（总订单数）   cardinality 字段唯一值数量（去重） 46（独立客户数）    使用脚本 #  可以用脚本组合多个字段的值来计数：\nGET sample_data_ecommerce/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;product_count\u0026#34;: { \u0026#34;value_count\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;products.product_id\u0026#39;].size()\u0026#34; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"值计数聚合（Value Count）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/value-count/"},{"category":null,"content":"Keep Words 分词过滤器 #  keep_words 分词过滤器旨在在分析过程中仅保留特定的词。如果你有大量文本，但只对某些关键字或术语感兴趣，那么这个过滤器就会很有用。\n相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  参数说明 #  词保留分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     keep_words 若未配置 keep_words_path 则为必需 字符串列表 要保留的词的列表。   keep_words_path 若未配置 keep_words 则为必需 字符串 包含要保留的词列表的文件的路径。   keep_words_case 可选 布尔值 在比较时是否将所有词转换为小写。默认值为 false。    参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有词保留过滤器的分词器：\nPUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_keep_word\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;keep_words_filter\u0026#34; ] } }, \u0026#34;filter\u0026#34;: { \u0026#34;keep_words_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keep\u0026#34;, \u0026#34;keep_words\u0026#34;: [\u0026#34;example\u0026#34;, \u0026#34;world\u0026#34;, \u0026#34;easysearch\u0026#34;], \u0026#34;keep_words_case\u0026#34;: true } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_keep_word\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Hello, world! This is an Easysearch example.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;world\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 12, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;Easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 25, \u0026#34;end_offset\u0026#34;: 35, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 }, { \u0026#34;token\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;start_offset\u0026#34;: 36, \u0026#34;end_offset\u0026#34;: 43, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 6 } ] } ","subcategory":null,"summary":"","tags":null,"title":"保留词分词过滤器（Keep Words）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keep-words/"},{"category":null,"content":"Keep Types 分词过滤器 #  keep_types 分词过滤器是一种用于文本分析的分词过滤器，它可用于控制保留或丢弃哪些词元类型。不同的分词器会产生不同的词元类型，例如 \u0026lt;HOST\u0026gt;、\u0026lt;NUM\u0026gt; 或 \u0026lt;ALPHANUM\u0026gt;。\n 注意：keyword 分词器、simple_pattern 分词器和 simple_pattern_split 分词器不支持 keep_types 分词过滤器，因为这些分词器不支持词元类型属性。\n 相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  参数说明 #  保留类型分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     types 必需 字符串列表 要保留或丢弃的词元类型列表（由 mode 参数决定）。   mode 可选 字符串 是要包含(include)还是排除(exclude) types 中指定的词元类型。默认值为 include（包含）。    参考样例 #  以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有保留类型过滤器的分析器：\nPUT /test_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;keep_types_filter\u0026#34;] } }, \u0026#34;filter\u0026#34;: { \u0026#34;keep_types_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keep_types\u0026#34;, \u0026#34;types\u0026#34;: [\u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /test_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Hello 2 world! This is an example.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;hello\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;world\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;this\u0026#34;, \u0026#34;start_offset\u0026#34;: 15, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34;: 20, \u0026#34;end_offset\u0026#34;: 22, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;an\u0026#34;, \u0026#34;start_offset\u0026#34;: 23, \u0026#34;end_offset\u0026#34;: 25, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 }, { \u0026#34;token\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;start_offset\u0026#34;: 26, \u0026#34;end_offset\u0026#34;: 33, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 6 } ] } ","subcategory":null,"summary":"","tags":null,"title":"保留类型分词过滤器（Keep Types）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keep-types/"},{"category":null,"content":"中位数绝对偏差聚合 #  median_absolute_deviation 聚合是一个单值指标聚合。中位数绝对偏差是一种变异性指标，用于衡量相对于中位数的离散程度。\n相关指南（先读这些） #    聚合基础  聚合场景实践  与依赖平方误差项的标准偏差相比，中位数绝对偏差受异常值的影响较小，适用于描述非正态分布的数据。\n中位数绝对偏差按以下方式计算：\nmedian_absolute_deviation = median( | x\u0026lt;sub\u0026gt;i\u0026lt;/sub\u0026gt; - median(x\u0026lt;sub\u0026gt;i\u0026lt;/sub\u0026gt;) | )\n由于内存限制，Easysearch 估计 median_absolute_deviation ，而不是直接计算它。这种估计在计算上很昂贵。您可以调整估计精度和性能之间的权衡。有关更多信息，请参阅调整估计精度。\n参数说明 #  median_absolute_deviation 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 要计算中位数绝对偏差的数值字段的名称。   missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，则从估计中排除具有缺失值的文档。   compression 可选 Numeric 一个调整估计精度和性能之间平衡的参数。 compression 的值必须大于 0 。默认值为 1000 。    参考样例 #  以下示例计算数据集中 DistanceMiles 字段的绝对中位数偏差：\nGET sample_data_flights/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;median_absolute_deviation_DistanceMiles\u0026#34;: { \u0026#34;median_absolute_deviation\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;DistanceMiles\u0026#34; } } } } 返回内容 #  如以下返回内容所示，聚合返回了 median_absolute_deviation_DistanceMiles 变量中绝对中位数偏差的估计值：\n{ \u0026#34;took\u0026#34;: 490, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;median_absolute_deviation_DistanceMiles\u0026#34;: { \u0026#34;value\u0026#34;: 1830.917892238693 } } } 缺省值 #\n 在计算 median_absolute_deviation 时会忽略缺失值和空值。你可以为聚合字段的缺失实例分配一个值。有关更多信息，请参阅缺失聚合。\n调整估计精度 #  中位数绝对偏差使用 t-digest 数据结构进行计算，该结构采用一个 compression 参数来平衡性能和估计精度。 compression 的较低值可以提高性能，但可能会降低估计精度，如下面的请求所示：\nGET sample_data_flights/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;median_absolute_deviation_DistanceMiles\u0026#34;: { \u0026#34;median_absolute_deviation\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;DistanceMiles\u0026#34;, \u0026#34;compression\u0026#34;: 10 } } } } 估计误差取决于数据集，但通常低于 5%，即使对于值低至 100 的 compression 也是如此。（此处使用的低示例值 10 是为了说明权衡效应，并不推荐。）\n请注意，在以下返回中，计算时间（ took 时间）有所减少，并且估计参数的值略有下降。\n作为参考，Easysearch 的最佳估计（将 compression 任意设置得非常高）对于 DistanceMiles 的中值绝对偏差是 1831.076904296875 ：\n{ \u0026#34;took\u0026#34;: 1, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 10000, \u0026#34;relation\u0026#34;: \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] }, \u0026#34;aggregations\u0026#34;: { \u0026#34;median_absolute_deviation_DistanceMiles\u0026#34;: { \u0026#34;value\u0026#34;: 1836.265614211182 } } } ","subcategory":null,"summary":"","tags":null,"title":"中位数绝对偏差聚合（Median Absolute Deviation）","url":"/easysearch/main/docs/features/aggregations/metric-aggregations/median-absolute-deviation/"},{"category":null,"content":"Wrapper 查询 #  wrapper 查询允许您以 Base64 编码的 JSON 格式提交完整的查询。当查询必须嵌入到仅支持字符串值的上下文中时，它非常有用。\n仅当需要管理系统约束时才使用此查询。为了提高可读性和可维护性，最好尽可能使用基于 JSON 的标准查询。\n相关指南（先读这些） #    Query DSL 基础  专业查询（Specialized queries）  参考样例 #  使用以下映射创建名为 products 的索引：\nPUT /products { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } 索引示例文档：\nPOST /products/_bulk { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 1 } } { \u0026#34;title\u0026#34;: \u0026#34;Wireless headphones with noise cancellation\u0026#34; } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 2 } } { \u0026#34;title\u0026#34;: \u0026#34;Bluetooth speaker\u0026#34; } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 3 } } { \u0026#34;title\u0026#34;: \u0026#34;Over-ear headphones with rich bass\u0026#34; } 以 Base64 格式编码以下查询：\necho -n \u0026#39;{ \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;headphones\u0026#34; } }\u0026#39; | base64 执行编码的查询：\nPOST /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;wrapper\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;eyAibWF0Y2giOiB7ICJ0aXRsZSI6ICJoZWFkcGhvbmVzIiB9IH0=\u0026#34; } } } 结果包含两个匹配的文档：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.20098841, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.20098841, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Wireless headphones with noise cancellation\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 0.18459359, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Over-ear headphones with rich bass\u0026#34; } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"Wrapper 查询","url":"/easysearch/main/docs/features/query-dsl/specialized/wrapper/"},{"category":null,"content":"Wildcard 查询 #  使用 wildcard 查询来搜索匹配通配符模式的词项。通配符查询支持以下操作符。\n相关指南（先读这些） #    部分匹配  结构化搜索  通配符字段类型（Wildcard） — 专为高效通配符匹配设计的字段类型     操作符 描述     * 匹配零个或多个字符。   ? 匹配任意单个字符。   case_insensitive 若 true 为真，则通配符查询不区分大小写；若 false 为真，则通配符查询区分大小写。默认情况下 false 为真（区分大小写）。    若进行区分大小写的搜索，查找以 H 开头且以 Y 结尾的词，可使用以下请求：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;speaker\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;H*Y\u0026#34;, \u0026#34;case_insensitive\u0026#34;: false } } } } 如果你将 * 更改为 ? ，则不会有任何匹配，因为 ? 引用的是一个单一字符。\n 性能注意：通配符查询通常速度较慢，因为它们需要遍历大量的词项。避免在查询的开头使用通配符字符，因为这在资源和时间方面都可能是一项非常昂贵的操作。更多性能优化建议，请参考 部分匹配章节。\n 参数说明 #  查询接受字段名称（ \u0026lt;field\u0026gt; ）作为顶级参数：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;patt*rn\u0026#34;, ... } } } } \u0026lt;field\u0026gt; 接受以下参数。除了 value 之外，所有参数都是可选的。\n   参数 数据类型 描述     value String 用于匹配 \u0026lt;field\u0026gt; 中指定字段的词条的通配符模式。   boost Float 一个浮点数，用于指定该字段对相关性评分的权重。值大于 1.0 会增加字段的相关性。值在 0.0 到 1.0 之间会降低字段的相关性。默认值为 1.0。   case_insensitive Boolean 如果为 true，则允许不区分大小写地匹配值与索引字段值。默认值为 false （大小写敏感性由字段的映射决定）。   rewrite String 确定 Easysearch 如何重写和评分多词查询。有效值为 constant_score 、 scoring_boolean 、 constant_score_boolean 、 top_terms_N 、 top_terms_boost_N 和 top_terms_blended_freqs_N 。默认值为 constant_score 。     如果 search.allow_expensive_queries 设置为 false ，则不会执行通配符查询。\n 工作原理 #  wildcard 查询和 prefix、regexp 查询一样，是词级别的底层查询。它需要扫描倒排索引中的词列表来查找所有匹配的词项，然后收集每个词项对应的文档 ID。\n因此，以下原则同样适用：\n 通配符匹配的是倒排索引中的 单个词项，而不是原始字段值 对 text 类型字段使用时，匹配的是分词后的词项（例如小写化后的词） 对精确匹配原始值，应使用 keyword 类型字段  更多示例 #  匹配文件扩展名 #  GET files/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;filename.keyword\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;*.pdf\u0026#34; } } } } 匹配特定格式的编码 #  使用 ? 匹配固定长度的模式。例如匹配 A 开头、后跟两个任意字符、以 00 结尾的产品编码：\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;sku.keyword\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;A??00\u0026#34; } } } } 不区分大小写的匹配 #  GET logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;message.keyword\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;*error*\u0026#34;, \u0026#34;case_insensitive\u0026#34;: true } } } } 性能优化建议 #     模式 性能 说明     abc* ✅ 好 等同于 prefix 查询，可快速定位   a*z ⚠️ 中 先找前缀 a，再逐项过滤   *abc ❌ 差 左通配符，必须扫描所有词项   *abc* ❌ 差 双通配符，全量扫描     建议：对于左通配符（*keyword）这类查询，可考虑使用 keyword 字段配合自定义的 reverse token filter 在索引时将词项反转存储，然后用 prefix 查询替代。\n 相关查询对比 #     查询类型 语法灵活性 性能 适用场景     prefix 低（仅前缀） 较好 前缀匹配，如搜索建议   wildcard 中（* ?） 中等 简单的模式匹配   regexp 高（完整正则） 较差 复杂模式匹配     提示：如果需要频繁执行通配符查询（特别是左通配符 *keyword），建议使用 通配符字段类型（Wildcard） 替代 keyword 字段。wildcard 字段类型通过 N-gram 子串索引优化了通配符和正则匹配的性能。\n ","subcategory":null,"summary":"","tags":null,"title":"Wildcard 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/wildcard/"},{"category":null,"content":"Terms 查询 #  使用 terms 查询在同一字段中搜索多个词项。例如，以下查询搜索具有 ID 61809 和 61810 的文档：\n相关指南（先读这些） #    结构化搜索  Query DSL 基础  GET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;line_id\u0026#34;: [ \u0026#34;61809\u0026#34;, \u0026#34;61810\u0026#34; ] } } } 如果文档与数组中的任何词项匹配，则会返回该文档。\n默认情况下， terms 查询中允许的最大词项数量为 65,536。要更改最大词项数量，请更新 index.max_terms_count 设置。\n 为了更好的查询性能，请传递包含已排序词项的长期数组（按 UTF-8 字节值升序排序）。\n  根据高亮器类型和查询中词项的数量，高亮显示词项查询结果的能力可能无法保证。\n 参数说明 #  该查询接受以下参数。所有参数都是可选的。\n   参数 数据类型 描述     \u0026lt;field\u0026gt; String 要搜索的字段。只有当文档的字段值与查询中至少一个词项完全匹配（包括正确的空格和大小写）时，该文档才会出现在结果中。   boost Float 一个浮点数值，用于指定该字段对相关性分数的权重。大于 1.0 的值会增加字段的权重。介于 0.0 和 1.0 之间的值会降低字段的权重。默认值为 1.0。   _name String 查询标签的查询名称。可选。   value_type String 指定用于过滤的值类型。有效值为 default 和 bitmap 。如果省略，则值默认为 default 。    条件查找 #  条件查找功能会检索单个文档的字段值并将其用作搜索词。您可以使用条件查找功能来搜索大量词项。\n要使用条件查找功能，您必须启用 _source 映射字段，因为条件查找功能会从文档中获取值。默认情况下， _source 字段处于启用状态。\n条件查找会尝试从本地数据节点上的分片获取文档字段值。因此，使用具有单个主分片且在所有适用数据节点上都有完整副本的索引可以减少网络流量。\n参考用例 #  例如，创建一个包含学生数据的索引，将 student_id 映射为 keyword ：\nPUT students { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;student_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } 接下来，索引与学生对应的三个文档：\nPUT students/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Jane Doe\u0026#34;, \u0026#34;student_id\u0026#34; : \u0026#34;111\u0026#34; } PUT students/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Mary Major\u0026quot;, \u0026quot;student_id\u0026quot; : \u0026quot;222\u0026quot; }\nPUT students/_doc/3 { \u0026quot;name\u0026quot;: \u0026quot;John Doe\u0026quot;, \u0026quot;student_id\u0026quot; : \u0026quot;333\u0026quot; }\n创建一个单独的索引，其中包含班级信息，包括班级名称和与该班级注册的学生对应的学生 ID 数组：\nPUT classes/_doc/101 { \u0026#34;name\u0026#34;: \u0026#34;CS101\u0026#34;, \u0026#34;enrolled\u0026#34; : [\u0026#34;111\u0026#34; , \u0026#34;222\u0026#34;] } 要搜索参加 CS101 课程的学生，请指定与该课程对应的文档的文档 ID、该文档的索引以及词项所在字段的路径：\nGET students/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;student_id\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;classes\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;101\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;enrolled\u0026#34; } } } } 返回内容包含 students 索引中 ID 与 enrolled 数组中的一个值匹配的每个学生的文档：\n{ \u0026#34;took\u0026#34;: 13, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Jane Doe\u0026#34;, \u0026#34;student_id\u0026#34;: \u0026#34;111\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Mary Major\u0026#34;, \u0026#34;student_id\u0026#34;: \u0026#34;222\u0026#34; } } ] } } 示例：嵌套字段 #  第二个示例演示了如何查询嵌套字段。考虑以下文档的索引：\nPUT classes/_doc/102 { \u0026#34;name\u0026#34;: \u0026#34;CS102\u0026#34;, \u0026#34;enrolled_students\u0026#34; : { \u0026#34;id_list\u0026#34; : [\u0026#34;111\u0026#34; , \u0026#34;333\u0026#34;] } } 要搜索就读 CS102 的学生，请使用点路径符号指定 path 参数中字段的完整路径：\nGET students/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;student_id\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;classes\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;102\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;enrolled_students.id_list\u0026#34; } } } } 返回内容包含匹配的文档：\n{ \u0026#34;took\u0026#34;: 18, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Jane Doe\u0026#34;, \u0026#34;student_id\u0026#34;: \u0026#34;111\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;student_id\u0026#34;: \u0026#34;333\u0026#34; } } ] } } 参数说明 #  下表列出了条件查找参数。\n   参数 数据类型 描述     index String 用于获取字段值的索引的名称。必填。   id String 要从中获取字段值的文档的文档 ID。必填。   query Object 用于选择多个文档并从中获取字段值的查询对象。如果未提供 id ，则为必填项。   path String 要从中获取字段值的字段名称。使用点路径表示法指定嵌套字段。必填。   routing String 用于从中获取字段值的文档的自定义路由值。可选。如果在索引文档时提供了自定义路由值，则为必填项。   store Boolean 是否在存储字段（而非 _source 上执行查找。可选。    ","subcategory":null,"summary":"","tags":null,"title":"Terms 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/terms/"},{"category":null,"content":"Terms Set 查询 #  使用 terms_set 查询，您可以在指定字段中搜索匹配一定数量的精确词的文档。与 terms 查询类似，您可以指定返回文档所需的匹配词的最小数量。您可以直接在索引字段中指定这个数量，也可以通过脚本指定。\n相关指南（先读这些） #    结构化搜索  查询 DSL 基础  例如，假设有一个索引，其中包含学生的姓名和他们所选的课程。在设置该索引的映射时，您需要提供一个数值字段，以指定返回文档所需的最小匹配项数量：\nPUT students { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;classes\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;min_required\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } 接下来，索引两个与学生相关的文档：\nPUT students/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Mary Major\u0026#34;, \u0026#34;classes\u0026#34;: [ \u0026#34;CS101\u0026#34;, \u0026#34;CS102\u0026#34;, \u0026#34;MATH101\u0026#34; ], \u0026#34;min_required\u0026#34;: 2 } PUT students/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;John Doe\u0026quot;, \u0026quot;classes\u0026quot;: [ \u0026quot;CS101\u0026quot;, \u0026quot;MATH101\u0026quot;, \u0026quot;ENG101\u0026quot; ], \u0026quot;min_required\u0026quot;: 2 }\n现在搜索已经修读了以下至少两门课程的学生： CS101 ， CS102 ， MATH101 ：\nGET students/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms_set\u0026#34;: { \u0026#34;classes\u0026#34;: { \u0026#34;terms\u0026#34;: [ \u0026#34;CS101\u0026#34;, \u0026#34;CS102\u0026#34;, \u0026#34;MATH101\u0026#34; ], \u0026#34;minimum_should_match_field\u0026#34;: \u0026#34;min_required\u0026#34; } } } } 返回内容中包含了两份文档：\n{ \u0026#34;took\u0026#34; : 44, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 2, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.4544616, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.4544616, \u0026#34;_source\u0026#34; : { \u0026#34;name\u0026#34; : \u0026#34;Mary Major\u0026#34;, \u0026#34;classes\u0026#34; : [ \u0026#34;CS101\u0026#34;, \u0026#34;CS102\u0026#34;, \u0026#34;MATH101\u0026#34; ], \u0026#34;min_required\u0026#34; : 2 } }, { \u0026#34;_index\u0026#34; : \u0026#34;students\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34; : 0.5013843, \u0026#34;_source\u0026#34; : { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;classes\u0026#34; : [ \u0026#34;CS101\u0026#34;, \u0026#34;MATH101\u0026#34;, \u0026#34;ENG101\u0026#34; ], \u0026#34;min_required\u0026#34; : 2 } } ] } } 要指定文档应与脚本匹配的最小词项数，请在 minimum_should_match_script 字段中提供脚本：\nGET students/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms_set\u0026#34;: { \u0026#34;classes\u0026#34;: { \u0026#34;terms\u0026#34;: [ \u0026#34;CS101\u0026#34;, \u0026#34;CS102\u0026#34;, \u0026#34;MATH101\u0026#34; ], \u0026#34;minimum_should_match_script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;Math.min(params.num_terms, doc[\u0026#39;min_required\u0026#39;].value)\u0026#34; } } } } } 参数说明 #\n 查询接受字段的名称 （\u0026lt;field\u0026gt;） 作为顶级参数：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;terms_set\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { \u0026#34;terms\u0026#34;: [ \u0026#34;term1\u0026#34;, \u0026#34;term2\u0026#34; ], ... } } } } \u0026lt;field\u0026gt; 接受以下参数。除 terms 外，所有参数均为可选参数。\n   参数 数据类型 描述     terms Array of strings 要在  中指定的字段中搜索的词项数组。仅当所需的词项数与文档的字段值完全匹配且间距和大小写正确时，才会在结果中返回文档。   minimum_should_match_field String 数值字段的名称，用于指定在结果中返回文档所需的匹配项数。必须指定 minimum_should_match_field 或 minimum_should_match_script，但不能同时指定两者。   minimum_should_match_script String 返回结果中返回文档所需的匹配词数的脚本。必须指定 minimum_should_match_field 或 minimum_should_match_script，但不能同时指定两者。   boost Float 一个浮点值，用于指定此字段相对于相关性分数的权重。值高于 1.0 会增加字段的相关性。值介于 0.0 和 1.0 之间会降低字段的相关性。默认值为 1.0。    ","subcategory":null,"summary":"","tags":null,"title":"Terms Set 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/terms-set/"},{"category":null,"content":"Term 查询 #  使用 term 查询在字段中搜索确切的词项。例如，以下查询搜索包含确切的行号的行：\n相关指南（先读这些） #    结构化搜索  Query DSL 基础  GET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;line_id\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;61809\u0026#34; } } } } \n注意：term 查询仅匹配确切的词项，不会对查询文本进行分词。避免在 text 字段上使用 term 查询，应使用 keyword 字段或 match 查询。更多信息，请参阅 结构化搜索。\n 您可以在 case_insensitive 参数中指定查询应不区分大小写：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;speaker\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;HAMLET\u0026#34;, \u0026#34;case_insensitive\u0026#34;: true } } } } 返回内容包含匹配的文档，无论大小写是否有差异：\n\u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1582, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32700\u0026#34;, \u0026#34;_score\u0026#34;: 2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32701, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 9, \u0026#34;line_number\u0026#34;: \u0026#34;1.2.66\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;HAMLET\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;[Aside] A little more than kin, and less than kind.\u0026#34; } }, ... } 参数说明 #  查询接受字段名称（ \u0026lt;field\u0026gt; ）作为顶级参数：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;sample\u0026#34;, ... } } } } \u0026lt;field\u0026gt; 接受以下参数。除 value 外，所有参数都是可选的。\n   参数 数据类型 描述     value String 要在 \u0026lt;field\u0026gt; 中指定的字段中搜索的词项。只有当文档的字段值与词项完全匹配（包括正确的空格和大小写）时，该文档才会出现在结果中。   boost Float 一个浮点数值，用于指定该字段对相关性分数的权重。大于 1.0 的值会增加字段的权重。介于 0.0 和 1.0 之间的值会降低字段的权重。默认值为 1.0。   _name String 查询标签的查询名称。可选。   case_insensitive Boolean 如果为 true ，允许将值与索引字段的值进行不区分大小写的匹配。默认为 false （大小写敏感性由字段的映射决定）。    ","subcategory":null,"summary":"","tags":null,"title":"Term 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/term/"},{"category":null,"content":"Span Within 查询 #  span_within 查询匹配被另一个 span 查询所包围的跨度。它是 span_containing 的相反操作：span_containing 返回包含较小跨度的较大跨度，而 span_within 返回被较大跨度包围的较小跨度。\n例如，您可以使用 span_within 查询来：\n 查找出现在较长短语中的较短短语。 匹配在特定上下文中出现的词项。 识别被较大模式包围的小模式。  相关指南（先读这些） #    Span 查询  邻近匹配  查询 DSL 基础  参考样例 #  以下查询在包含“shirt”和“long”的跨度中搜索单词“dress”：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_within\u0026#34;: { \u0026#34;little\u0026#34;: { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;dress\u0026#34; } }, \u0026#34;big\u0026#34;: { \u0026#34;span_near\u0026#34;: { \u0026#34;clauses\u0026#34;: [ { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;shirt\u0026#34; } }, { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;long\u0026#34; } } ], \u0026#34;slop\u0026#34;: 2, \u0026#34;in_order\u0026#34;: false } } } } } 该查询匹配文档 1 的原因是：\n 单词“dress”出现在较大的跨度（“Long-sleeved dress shirt…”）中。 较大的跨度包含“shirt”和“long”，它们在彼此之间相隔 2 个词（它们之间有 2 个词）。  { \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.4677674, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.4677674, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Long-sleeved dress shirt with a formal collar and button cuffs. \u0026#34; } } ] } } 参数说明 #  下表列出了 span_within 查询支持的所有顶层参数。所有参数都是必需的。\n   参数 数据类型 描述     little Object 必须包含在 big span 内的跨度查询。这定义了你在较大上下文中搜索的跨度。   big Object 包含 span 查询，用于定义 little span 必须出现的边界。这为您的搜索建立了上下文。    ","subcategory":null,"summary":"","tags":null,"title":"Span Within 查询","url":"/easysearch/main/docs/features/query-dsl/span/span-within/"},{"category":null,"content":"Span Term 查询 #  span_term 查询是最基本的 span 查询，它匹配包含单个词的 span。它是更复杂的 span 查询的构建模块。\n例如，您可以使用 span_term 查询来：\n 查找可用于其他 span 查询的精确词匹配。 匹配特定单词同时保持位置信息。 创建可与其他 span 查询组合的基本 span。  相关指南（先读这些） #    Span 查询  查询 DSL 基础  参考样例 #  以下查询搜索确切的词“formal”：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;formal\u0026#34; } } } 或者，您可以在 value 参数中指定搜索词：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;formal\u0026#34; } } } } 您也可以指定 boost 值来提升文档得分：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;formal\u0026#34;, \u0026#34;boost\u0026#34;: 2 } } } } 该查询匹配文档 1 和文档 2，因为它们包含确切的词项“formal”。位置信息被保留以用于其他 span 查询。\n{ \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.498922, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.498922, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Beautiful long dress in red silk, perfect for formal events.\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.4466847, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Long-sleeved dress shirt with a formal collar and button cuffs. \u0026#34; } } ] } } 参数说明 #  下表列出了 span_term 查询支持的所有顶层参数。\n   参数 数据类型 描述     \u0026lt;field\u0026gt; 字符串或对象 要搜索的字段的名称。    ","subcategory":null,"summary":"","tags":null,"title":"Span Term 查询","url":"/easysearch/main/docs/features/query-dsl/span/span-term/"},{"category":null,"content":"Span Or 查询 #  span_or 查询组合多个 span 查询，并匹配它们 span 的并集。如果其中至少一个包含的 span 查询匹配，则发生匹配。\n例如，您可以使用 span_or 查询来：\n 查找匹配多个模式中的任意一个的 span。 将不同的 span 模式作为备选项组合。 在一个查询中匹配多个 span 变体。  相关指南（先读这些） #    Span 查询  查询 DSL 基础  参考样例 #  以下查询搜索“formal collar”或“button collar”在彼此 2 个词距离内出现：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_or\u0026#34;: { \u0026#34;clauses\u0026#34;: [ { \u0026#34;span_near\u0026#34;: { \u0026#34;clauses\u0026#34;: [ { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;formal\u0026#34; } }, { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;collar\u0026#34; } } ], \u0026#34;slop\u0026#34;: 0, \u0026#34;in_order\u0026#34;: true } }, { \u0026#34;span_near\u0026#34;: { \u0026#34;clauses\u0026#34;: [ { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;button\u0026#34; } }, { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;collar\u0026#34; } } ], \u0026#34;slop\u0026#34;: 2, \u0026#34;in_order\u0026#34;: true } } ] } } } 该查询在指定的 slop 距离内匹配文档 1（“…formal collar…”）和文档 3（“…button-down collar…”）。\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2.170027, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 2.170027, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Long-sleeved dress shirt with a formal collar and button cuffs. \u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.2509141, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Short-sleeved shirt with a button-down collar, can be dressed up or down.\u0026#34; } } ] } } 参数说明 #  下表列出了 span_or 查询支持的所有顶层参数。\n   参数 数据类型 描述     clauses Array 用于匹配的 span 查询数组。如果这些 span 查询中的任意一个匹配，则查询匹配。必须至少包含一个 span 查询。必填。    ","subcategory":null,"summary":"","tags":null,"title":"Span Or 查询","url":"/easysearch/main/docs/features/query-dsl/span/span-or/"},{"category":null,"content":"Span Not 查询 #  span_not 查询会排除与另一个 span 查询重叠的跨度。您还可以指定在排除的跨度之前或之后不允许匹配的距离范围。\n例如，您可以使用 span_not 查询来：\n 查找除在特定短语中出现时的词项外。 除非它们靠近特定词项，否则匹配跨度。 排除在特定距离内出现的其他模式匹配。  相关指南（先读这些） #    Span 查询  查询 DSL 基础  参考样例 #  以下查询搜索单词“dress”，但当它出现在短语“dress shirt”中时不搜索：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_not\u0026#34;: { \u0026#34;include\u0026#34;: { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;dress\u0026#34; } }, \u0026#34;exclude\u0026#34;: { \u0026#34;span_near\u0026#34;: { \u0026#34;clauses\u0026#34;: [ { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;dress\u0026#34; } }, { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;shirt\u0026#34; } } ], \u0026#34;slop\u0026#34;: 0, \u0026#34;in_order\u0026#34;: true } } } } } 该查询匹配文档 2，因为它包含单词“dress”（“Beautiful long dress…”）。文档 1 未匹配，因为它包含短语“dress shirt”，该短语被排除。文档 3 和 4 未匹配，因为它们包含单词“dress”的变体（“dressed”和“dresses”），并且查询是在原始字段中进行的。\n{ \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.94519633, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.94519633, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Beautiful long dress in red silk, perfect for formal events.\u0026#34; } } ] } } 参数说明 #  下表列出了 span_not 查询支持的所有顶层参数。\n   参数 数据类型 描述     include Object 你想查找匹配的 span 查询。必填。   exclude Object 应该排除匹配的 span 查询。必填。   pre Integer 指定 exclude span 不能出现在 include span 之前的指定词元位置数内。可选。默认为 0 。   post Integer 指定 exclude span 不能出现在 include span 之后的指定词元位置数内。可选。默认为 0 。   dist Integer 相当于将 pre 和 post 设置为相同的值。可选。    ","subcategory":null,"summary":"","tags":null,"title":"Span Not 查询","url":"/easysearch/main/docs/features/query-dsl/span/span-not/"},{"category":null,"content":"Span Near 查询 #  span_near 查询匹配彼此靠近的跨度。您可以指定跨度之间的距离，并指定它们是否需要按特定顺序出现。\n例如，您可以使用 span_near 查询来：\n 查找彼此之间距离在特定范围内的词项。 匹配词语按特定顺序出现的短语。 查找文本中彼此靠近的相关概念。  相关指南（先读这些） #    Span 查询  邻近匹配  查询 DSL 基础  参考样例 #  以下查询搜索任何形式的“sleeve”和“long”相邻出现，顺序不限：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_near\u0026#34;: { \u0026#34;clauses\u0026#34;: [ { \u0026#34;span_term\u0026#34;: { \u0026#34;description.stemmed\u0026#34;: \u0026#34;sleev\u0026#34; } }, { \u0026#34;span_term\u0026#34;: { \u0026#34;description.stemmed\u0026#34;: \u0026#34;long\u0026#34; } } ], \u0026#34;slop\u0026#34;: 1, \u0026#34;in_order\u0026#34;: false } } } 该查询匹配文档 1（“Long-sleeved…”）和文档 2（“…long fluttered sleeves…”）。在文档 1 中，词语是相邻的，而在文档 2 中，它们在指定的 slop 距离 1 内（它们之间有一个词）。\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.36496973, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.36496973, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Long-sleeved dress shirt with a formal collar and button cuffs. \u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 0.25312424, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;A set of two midi silk shirt dresses with long fluttered sleeves in black. \u0026#34; } } ] } } 参数说明 #  下表列出了 span_near 查询支持的所有顶层参数。\n   参数 数据类型 描述     clauses String 一组 span 查询，用于定义要匹配的词项或短语。所有指定的词项必须出现在定义的 slop 距离内。必填。   slop Integer 跨度之间最大数量的未匹配位置。必填。   in_order Boolean 跨度是否需要按照 clauses 数组中的顺序出现。可选。默认值为 false 。    ","subcategory":null,"summary":"","tags":null,"title":"Span Near 查询","url":"/easysearch/main/docs/features/query-dsl/span/span-near/"},{"category":null,"content":"Span Multi Term 查询 #  span_multi 查询允许您将多词查询（如 wildcard、fuzzy、prefix、range 或 regexp）包装为 span 查询。这使您能够在其他 span 查询中使用这些更灵活的匹配查询。\n例如，您可以使用 span_multi 查询来：\n 查找具有相同前缀的词语，并与其他词语靠近。 匹配跨度内单词的模糊变体。 在跨度查询中使用正则表达式。   注意：span_multi 查询可能匹配多个词。为了避免过度内存使用，您可以：\n 为多词查询设置 rewrite 参数。 使用 top_terms_* 重写方法。 如果你仅使用 span_multi 进行 prefix 查询，请考虑为文本字段启用 index_prefixes 选项。这将自动将字段上的任何 prefix 查询重写为匹配索引前缀的单词查询。   相关指南（先读这些） #    Span 查询  部分匹配  查询 DSL 基础  参考样例 #  span_multi 查询使用以下语法来包装 prefix 查询：\n\u0026#34;span_multi\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;prefix\u0026#34;: { \u0026#34;description\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;flutter\u0026#34; } } } } 以下查询搜索以“dress”开头的单词，在彼此最多 5 个单词的距离内靠近任何形式的“sleeve”：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_near\u0026#34;: { \u0026#34;clauses\u0026#34;: [ { \u0026#34;span_multi\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;prefix\u0026#34;: { \u0026#34;description\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;dress\u0026#34; } } } } }, { \u0026#34;field_masking_span\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;span_term\u0026#34;: { \u0026#34;description.stemmed\u0026#34;: \u0026#34;sleev\u0026#34; } }, \u0026#34;field\u0026#34;: \u0026#34;description\u0026#34; } } ], \u0026#34;slop\u0026#34;: 5, \u0026#34;in_order\u0026#34;: false } } } 查询匹配文档 1（“长袖连衣裙……”）和文档 4（“……长飘袖连衣裙……”），因为“dress”和“long”在两个文档中出现的最大距离内都存在。\n{ \u0026#34;took\u0026#34;: 5, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.7590723, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.7590723, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Long-sleeved dress shirt with a formal collar and button cuffs. \u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 0.84792376, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;A set of two midi silk shirt dresses with long fluttered sleeves in black. \u0026#34; } } ] } } 参数说明 #  下表列出了 span_multi 查询支持的所有顶层参数。所有参数都是必需的。\n   参数 数据类型 描述     match Object 要包装的多项式查询（可以是 prefix 、 wildcard 、 fuzzy 、 range 或 regexp ）。    ","subcategory":null,"summary":"","tags":null,"title":"Span Multi Term 查询","url":"/easysearch/main/docs/features/query-dsl/span/span-multi-term/"},{"category":null,"content":"Span First 查询 #  span_first 查询匹配从字段开头开始并在指定位置结束的跨度。当您想要查找出现在文档开头附近的词项或短语时，此查询很有用。\n例如，您可以使用 span_first 查询来执行以下搜索：\n 查找在字段的最初几个词中出现的特定词项的文档。 确保某些短语出现在文本的开头或附近。 仅在模式出现在指定距离内时匹配。  相关指南（先读这些） #    Span 查询  查询 DSL 基础  参考样例 #  以下查询搜索词干“dress”出现在描述的前 4 个位置：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_first\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;span_term\u0026#34;: { \u0026#34;description.stemmed\u0026#34;: \u0026#34;dress\u0026#34; } }, \u0026#34;end\u0026#34;: 4 } } } 该查询匹配文档 1 和 2：\n 文档 1 和 2 在第三位置包含单词 dress （“长袖连衣裙…”和“漂亮的连衣裙”）。索引单词的起始位置为 0，因此单词“dress”位于位置 2。 单词 dress 的位置必须小于 4 ，如 end 参数指定。  { \u0026#34;took\u0026#34;: 13, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.110377684, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.110377684, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Long-sleeved dress shirt with a formal collar and button cuffs. \u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.110377684, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Beautiful long dress in red silk, perfect for formal events.\u0026#34; } } ] } } match 参数可以包含任何类型的跨度查询，允许在字段的开始处匹配更复杂的模式。\n参数说明 #  下表列出了 span_first 查询支持的所有顶层参数。所有参数都是必需的。\n   参数 数据类型 描述     match Object 用于匹配的跨度查询。这定义了你在字段开始处搜索的模式。   end Integer span 查询匹配允许的最大结束位置（不包含）。例如， end: 4 匹配位置 0–3 的词。    ","subcategory":null,"summary":"","tags":null,"title":"Span First 查询","url":"/easysearch/main/docs/features/query-dsl/span/span-first/"},{"category":null,"content":"Span Field Masking 查询 #  field_masking_span 查询允许 span 查询通过\u0026quot;掩饰\u0026quot;查询的真实字段来匹配不同字段。这在处理多字段（相同内容使用不同分析器索引）或需要跨不同字段运行 span 查询（如 span_near 或 span_or，这通常是不允许的）时特别有用。\n例如，您可以使用 field_masking_span 查询来：\n 匹配原始字段及其词干版本中的词项。 在一个 span 操作中组合不同字段的 span 查询。 使用不同分析器索引的相同内容进行操作。   注意：在使用字段遮罩时，相关性分数是根据遮罩字段的特性（范数）计算的，而不是实际搜索的字段。这意味着如果遮罩字段与被搜索字段具有不同的属性（如长度或提升值），您可能会收到意外的评分结果。\n 相关指南（先读这些） #    Span 查询  多字段搜索  查询 DSL 基础  参考样例 #  以下查询在词干化字段中搜索单词“long”，并查找“sleeve”一词的变体附近：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_near\u0026#34;: { \u0026#34;clauses\u0026#34;: [ { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;long\u0026#34; } }, { \u0026#34;field_masking_span\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;span_term\u0026#34;: { \u0026#34;description.stemmed\u0026#34;: \u0026#34;sleev\u0026#34; } }, \u0026#34;field\u0026#34;: \u0026#34;description\u0026#34; } } ], \u0026#34;slop\u0026#34;: 1, \u0026#34;in_order\u0026#34;: true } } } 查询匹配文档 1 和文档 4：\n 词项“long”在两个文档的 description 字段中都出现。 文档 1 包含单词“sleeved”，文档 4 包含单词“sleeves”。 field_masking_span 使得词干化字段的匹配看起来像是出现在原始字段中。 这些词项在指定顺序中彼此相距 1 个位置（\u0026ldquo;long\u0026quot;必须出现在\u0026quot;sleeve\u0026quot;之前）。  { \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.7444251, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.7444251, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Long-sleeved dress shirt with a formal collar and button cuffs. \u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 0.4291246, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;A set of two midi silk shirt dresses with long fluttered sleeves in black. \u0026#34; } } ] } } 参数说明 #  下表列出了 field_masking_span 查询支持的所有顶层参数。所有参数都是必需的。\n   参数 数据类型 描述     query Object 要在实际字段上执行的 span 查询。   field String 用于遮蔽查询的字段名。其他 span 查询会把这个查询当作是在这个字段上执行。    ","subcategory":null,"summary":"","tags":null,"title":"Span Field Masking 查询","url":"/easysearch/main/docs/features/query-dsl/span/span-field-masking/"},{"category":null,"content":"Span Containing 查询 #  span_containing 查询会在较大的文本模式（如短语或一组单词）的边界内查找包含较小文本模式的匹配项。可以将其视为仅在特定更大的上下文中出现时才查找单词或短语。\n例如，您可以使用 span_containing 查询来执行以下搜索：\n 查找单词\u0026quot;quick\u0026quot;，但仅当它出现在同时提到狐狸和行为的句子中时。 确保某些词项出现在其他词项的上下文中——而不仅仅是在文档的任何地方。 搜索在更长的有意义的短语中出现的特定单词。  相关指南（先读这些） #    Span 查询  邻近匹配  查询 DSL 基础  参考样例 #  以下查询搜索在包含“silk”和“dress”词语的较大词组（不一定按该顺序）中，与“red”一词出现且彼此之间不超过 5 个词的情况：\nGET /clothing/_search { \u0026#34;query\u0026#34;: { \u0026#34;span_containing\u0026#34;: { \u0026#34;little\u0026#34;: { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;red\u0026#34; } }, \u0026#34;big\u0026#34;: { \u0026#34;span_near\u0026#34;: { \u0026#34;clauses\u0026#34;: [ { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;silk\u0026#34; } }, { \u0026#34;span_term\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;dress\u0026#34; } } ], \u0026#34;slop\u0026#34;: 5, \u0026#34;in_order\u0026#34;: false } } } } } 该查询匹配文档 1 的原因是：\n 它找到一个词组，其中“silk”和“dress”出现且彼此之间不超过 5 个词（“…dress in red silk…”）。词语“silk”和“dress”彼此之间相隔 2 个词（它们之间有 2 个词）。 在这个更大的跨度内，它找到了“red”这个词。  { \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.1577396, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;clothing\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.1577396, \u0026#34;_source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Beautiful long dress in red silk, perfect for formal events.\u0026#34; } } ] } } little 和 big 参数都可以包含任何类型的跨度查询，当需要时，允许进行复杂的嵌套跨度查询。\n参数说明 #  下表列出了 span_containing 查询支持的所有顶层参数。所有参数都是必需的。\n   参数 数据类型 描述     little Object 必须包含在 big 跨度内的跨度查询。这定义了你在较大上下文中搜索的跨度。   big Object 包含 span 查询，用于定义 little span 必须出现的边界。这为您的搜索建立了上下文。    ","subcategory":null,"summary":"","tags":null,"title":"Span Containing 查询","url":"/easysearch/main/docs/features/query-dsl/span/span-containing/"},{"category":null,"content":"Simple Query String 查询 #  使用 simple_query_string 查询在查询字符串中直接指定由正则表达式分隔的多个参数。简单查询字符串的语法比 query_string 查询宽松，因为它会丢弃字符串中的任何无效部分，并且不会因无效语法而返回错误。\n此查询使用简单语法根据特殊运算符解析查询字符串，并将字符串拆分为词项。解析后，查询会独立分析每个词项，然后返回匹配的文档。\n相关指南（先读这些） #    全文搜索  Query DSL 基础  以下查询对 title 字段执行模糊搜索：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;simple_query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;\\\u0026#34;rises wind the\\\u0026#34;~4 | *ising~2\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title\u0026#34;] } } } 简单字符串语法 #  查询字符串由词项和运算符组成。词项是一个单词（例如，在查询 wind rises 中，词项是 wind 和 rises ）。如果多个词项被引号包围，它们被视为一个短语，其中单词按出现的顺序匹配（例如， \u0026ldquo;wind rises\u0026rdquo; ）。 + 、 | 和 - 等运算符指定用于解释查询字符串中文本的布尔逻辑。\n操作符 #  简单查询字符串语法支持以下运算符。\n   操作符 描述     + 作为 AND 操作符。   \\| 作为 OR 操作符。   * 在词尾使用时，表示前缀查询。   \u0026quot; 将多个词括起来组成短语（例如，\u0026quot;wind rises\u0026quot;）。   (, ) 为优先级包装子句（例如，wind + (rises \\| rising)）。   ~n 在词后面使用时（例如，wnid~3），设置 fuzziness。在短语后面使用时，设置 slop。   - 否定该词。    所有前面的操作符都是保留字符。要将其作为原始字符而不是操作符引用，用反斜杠转义它们中的任何一个。在发送 JSON 请求时，使用 \\\\ 转义保留字符（因为反斜杠字符本身也是保留的，你必须用另一个反斜杠转义反斜杠）。\n默认操作符 #  默认操作符是 OR （除非你将 default_operator 设置为 AND ）。默认操作符决定了整体查询行为。例如，考虑一个包含以下文档的索引：\nPUT /customers/_doc/1 { \u0026#34;first_name\u0026#34;:\u0026#34;Amber\u0026#34;, \u0026#34;last_name\u0026#34;:\u0026#34;Duke\u0026#34;, \u0026#34;address\u0026#34;:\u0026#34;880 Holmes Lane\u0026#34; } PUT /customers/_doc/2 { \u0026quot;first_name\u0026quot;:\u0026quot;Hattie\u0026quot;, \u0026quot;last_name\u0026quot;:\u0026quot;Bond\u0026quot;, \u0026quot;address\u0026quot;:\u0026quot;671 Bristol Street\u0026quot; }\nPUT /customers/_doc/3 { \u0026quot;first_name\u0026quot;:\u0026quot;Nanette\u0026quot;, \u0026quot;last_name\u0026quot;:\u0026quot;Bates\u0026quot;, \u0026quot;address\u0026quot;:\u0026quot;789 Madison St\u0026quot; }\nPUT /customers/_doc/4 { \u0026quot;first_name\u0026quot;:\u0026quot;Dale\u0026quot;, \u0026quot;last_name\u0026quot;:\u0026quot;Amber\u0026quot;, \u0026quot;address\u0026quot;:\u0026quot;467 Hutchinson Court\u0026quot; } 以下查询试图找到地址中包含 street 或 st 且不包含 madison 的文档：\nGET /customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;simple_query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;address\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;street st -madison\u0026#34; } } } 然而，结果不仅包括预期的文档，还包括所有四个文档：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2.2039728, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 2.2039728, \u0026#34;_source\u0026#34;: { \u0026#34;first_name\u0026#34;: \u0026#34;Hattie\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Bond\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;671 Bristol Street\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.2039728, \u0026#34;_source\u0026#34;: { \u0026#34;first_name\u0026#34;: \u0026#34;Nanette\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Bates\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;789 Madison St\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;first_name\u0026#34;: \u0026#34;Amber\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Duke\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;880 Holmes Lane\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;first_name\u0026#34;: \u0026#34;Dale\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Amber\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;467 Hutchinson Court\u0026#34; } } ] } } 因为默认操作符是 OR ，这个查询包括包含 street 或 st 的文档（文档 2 和 3）以及不包含 madison 的文档（文档 1 和 4）。\n要正确表达查询意图，请在 -madison 前面加上 + ：\nGET /customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;simple_query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;address\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;street st +-madison\u0026#34; } } } 或者，将 AND 指定为默认操作符，并使用 street 和 st 的析取操作：\nGET /customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;simple_query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;address\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;st|street -madison\u0026#34;, \u0026#34;default_operator\u0026#34;: \u0026#34;AND\u0026#34; } } } 前面的查询返回文档 2：\n{ \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2.2039728, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 2.2039728, \u0026#34;_source\u0026#34;: { \u0026#34;first_name\u0026#34;: \u0026#34;Hattie\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Bond\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;671 Bristol Street\u0026#34; } } ] } } 限制操作符 #  要限制简单查询字符串解析器支持的操作符，请在 flags 参数中包含您想要支持的操作符，操作符之间用 | 分隔。例如，以下查询仅启用 OR 、 AND 和 FUZZY 操作符：\nGET /customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;simple_query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;address\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;bristol | madison +stre~2\u0026#34;, \u0026#34;flags\u0026#34;: \u0026#34;OR|AND|FUZZY\u0026#34; } } } 下列表格列出了所有可用的操作符标志。\n   标志 描述     AND (默认) 启用 + ( AND ) 操作符。   ESCAPE 启用 \\ 作为转义字符。   FUZZY 启用单词后的 ~n 操作符，其中 n 表示匹配允许的编辑距离。   NEAR 启用短语后的 ~n 操作符，其中 n 是允许匹配标记之间的最大位置数。与 SLOP 相同。   NONE 禁用所有操作符。   NOT 启用 - ( NOT ) 操作符。   OR 启用 \\| ( OR ) 操作符。   PHRASE 启用 \u0026quot; (引号) 用于短语搜索。   PRECEDENCE 启用 ( 和 ) (括号) 用于操作符优先级。   PREFIX 启用 * (前缀) 操作符。   SLOP 启用短语后的 ~n 操作符，其中 n 是允许匹配标记之间的最大位置数。与 NEAR 相同。   WHITESPACE 启用空格字符作为文本分割的字符。    通配符表达式 #  您可以使用 * 特殊字符指定通配符表达式，该字符替换零个或多个字符。例如，以下查询搜索所有以 name 结尾的字段：\nGET /customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;simple_query_string\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;Amber Bond\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;*name\u0026#34; ] } } } 加权 #  使用尖号（^）提升运算符来提升字段的相关性得分。[0, 1)范围内的值会降低相关性，而大于 1 的值会增加相关性。默认值为 1。\n例如，以下查询搜索 first_name 和 last_name 字段，并将 first_name 字段的匹配项提升 2 倍：\nGET /customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;simple_query_string\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;Amber\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name^2\u0026#34;, \u0026#34;last_name\u0026#34; ] } } } 多位置词元 #  对于多位置词元，简单查询字符串会创建一个匹配短语查询。因此，如果你指定 ml, machine learning 作为同义词并搜索 ml ，Easysearch 会搜索 ml OR \u0026quot;machine learning\u0026quot; 。\n或者，您可以使用连词来匹配多位置词元。如果您将 auto_generate_synonyms_phrase_query 设置为 false ，Easysearch 将搜索 ml OR (machine AND learning) 。\n例如，以下查询搜索文本 ml models 并指定不针对每个同义词自动生成匹配短语查询：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;simple_query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [\u0026#34;title\u0026#34;], \u0026#34;query\u0026#34;: \u0026#34;ml models\u0026#34;, \u0026#34;auto_generate_synonyms_phrase_query\u0026#34;: false } } } 对于此查询，Easysearch 创建以下布尔查询： (ml OR (machine AND learning)) models 。\n参数说明 #  以下表格列出了 simple_query_string 查询支持的一级参数。除了 query 之外的所有参数都是可选的。\n   参数 数据类型 描述     query String 用于搜索的查询字符串语法中可能包含的表达式。必填。   analyze_wildcard Boolean 指定 Easysearch 是否应尝试分析通配符词项。默认值为 false 。   analyzer String 用于对查询字符串文本进行分词的分词器。默认值为为 default_field 指定的索引时分词器。如果未为 default_field 指定分词器，则 analyzer 为索引的默认分词器。有关 index.query.default_field 的更多信息，请参阅动态索引级索引设置。   auto_generate_synonyms_phrase_query Boolean 指定是否应自动为多词同义词创建 match_phrase 查询。默认值为 true 。   default_operator String 如果查询字符串包含多个搜索词，文档是否需要所有词都匹配（ AND ）或只需一个词匹配（ OR ）才被视为匹配。有效值为：- OR : 字符串 to be 被解释为 to OR be;- AND : 字符串 to be 被解释为 to AND be;默认为 OR 。   fields 字符串数组 要搜索的字段列表（例如， \u0026ldquo;fields\u0026rdquo;: [\u0026ldquo;title^4\u0026rdquo;, \u0026ldquo;description\u0026rdquo;] ）。支持通配符。如果未指定，则默认为 index.query.default_field 设置，该设置默认为 [\u0026quot;*\u0026quot;] 。同时可以搜索的最大字段数由 indices.query.bool.max_clause_count 定义，默认为 1,024。   flags String 一个以 \\| 分隔的启用标志字符串（例如，AND\\|OR\\|NOT）。默认为 ALL，表示启用所有操作符。设置为 NONE 可禁用所有操作符。详情参见上方「限制操作符」章节。   fuzzy_max_expansions 正整数 查询可以扩展到的最大词数。模糊查询会扩展到与指定距离（ fuzziness ）内的匹配词。然后 Easysearch 尝试匹配这些词。默认值为 50 。   fuzzy_transpositions Boolean 将 fuzzy_transpositions 设置为 true （默认）会在 fuzziness 选项的插入、删除和替换操作中添加相邻字符的交换。例如，如果 fuzzy_transpositions 为真（交换“n”和“i”），则 wind 和 wnid 之间的距离为 1；如果为假（删除“n”，插入“n”），则距离为 2。如果 fuzzy_transpositions 为假， rewind 和 wnid 与 wind 的距离相同（2），尽管从更以人为中心的观点来看， wnid 是一个明显的拼写错误。对于大多数用例，默认值是一个不错的选择。   fuzzy_prefix_length Integer 模糊匹配时保持不变的开头字符数。默认为 0。   lenient Boolean 将 lenient 设置为 true 会忽略查询与文档字段之间的数据类型不匹配。例如， \u0026ldquo;8.2\u0026rdquo; 的查询字符串可以匹配类型为 float 的字段。默认值为 false 。   minimum_should_match 正整数或负整数、正百分比或负百分比、组合 如果查询字符串包含多个搜索词并且你使用 or 运算符，文档被考虑为匹配所需的匹配词数。例如，如果 minimum_should_match 为 2， wind often rising 不匹配 The Wind Rises. 如果 minimum_should_match 为 1 ，则匹配。详情请参阅 Minimum should match。   quote_field_suffix String 此选项支持使用与非精确匹配不同的分析方法来搜索精确匹配（用引号括起来）。例如，如果 quote_field_suffix 是 .exact，而你搜索 \u0026quot;lightly\u0026quot; 在 title 字段中，Easysearch 会在 title.exact 字段中搜索。这个第二个字段可能使用不同的类型（例如，keyword 而不是 text）或不同的分词器。   boost Float 用于调整该查询的相关性得分权重。默认值为 1.0。    ","subcategory":null,"summary":"","tags":null,"title":"Simple Query String 查询","url":"/easysearch/main/docs/features/fulltext-search/full-text/simple-query-string/"},{"category":null,"content":"Script 查询 #  使用 script 查询基于 Painless 脚本语言编写的自定义条件来过滤文档。此查询返回脚本评估结果为 true 的文档，从而实现无法使用标准查询表达的高级过滤逻辑。\n相关指南（先读这些） #    Query DSL 基础  结构化搜索  专业查询（Specialized queries）   性能注意：script 查询计算成本高，应谨慎使用。仅在必要时使用，并确保 search.allow_expensive_queries 已启用（默认为 true ）。有关更多信息，请参阅昂贵查询。\n 参考样例 #  使用以下映射创建一个名为 products 的索引：\nPUT /products { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34; }, \u0026#34;rating\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34; } } } } 使用以下请求索引示例文档：\nPOST /products/_bulk { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 1 } } { \u0026#34;title\u0026#34;: \u0026#34;Wireless Earbuds\u0026#34;, \u0026#34;price\u0026#34;: 99.99, \u0026#34;rating\u0026#34;: 4.5 } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 2 } } { \u0026#34;title\u0026#34;: \u0026#34;Bluetooth Speaker\u0026#34;, \u0026#34;price\u0026#34;: 79.99, \u0026#34;rating\u0026#34;: 4.8 } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 3 } } { \u0026#34;title\u0026#34;: \u0026#34;Noise Cancelling Headphones\u0026#34;, \u0026#34;price\u0026#34;: 199.99, \u0026#34;rating\u0026#34;: 4.7 } 基本脚本查询 #  返回评分高于 4.6 的产品：\nPOST /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;rating\u0026#39;].value \u0026gt; 4.6\u0026#34; } } } } 返回的命中结果仅包括 rating 值高于 4.6 的文档：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Bluetooth Speaker\u0026#34;, \u0026#34;price\u0026#34;: 79.99, \u0026#34;rating\u0026#34;: 4.8 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Noise Cancelling Headphones\u0026#34;, \u0026#34;price\u0026#34;: 199.99, \u0026#34;rating\u0026#34;: 4.7 } } ] } } 参数说明 #\n script 查询采用以下顶级参数。\n   参数 必需/可选 描述     script.source 必需 计算结果为 true 或 false 的脚本代码。   script.params 可选 脚本内部引用的用户定义参数。    使用脚本参数 #  你可以使用 params 来安全地注入值，利用脚本编译缓存：\nPOST /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;price\u0026#39;].value \u0026lt; params.max_price\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;max_price\u0026#34;: 100 } } } } } 返回的命中结果仅包括文档的 price 值小于 100 的文档：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Wireless Earbuds\u0026#34;, \u0026#34;price\u0026#34;: 99.99, \u0026#34;rating\u0026#34;: 4.5 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Bluetooth Speaker\u0026#34;, \u0026#34;price\u0026#34;: 79.99, \u0026#34;rating\u0026#34;: 4.8 } } ] } } 组合多个条件 #\n 使用以下查询来搜索产品，其 rating 高于 4.5 且 price 低于 100 :\nPOST /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;rating\u0026#39;].value \u0026gt; 4.5 \u0026amp;\u0026amp; doc[\u0026#39;price\u0026#39;].value \u0026lt; 100\u0026#34; } } } } 仅返回符合要求的文档：\n{ \u0026#34;took\u0026#34;: 12, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Bluetooth Speaker\u0026#34;, \u0026#34;price\u0026#34;: 79.99, \u0026#34;rating\u0026#34;: 4.8 } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"Script 查询","url":"/easysearch/main/docs/features/query-dsl/specialized/script/"},{"category":null,"content":"Script Score 查询 #  使用 script_score 查询通过脚本自定义分数计算。对于昂贵的评分函数，您可以使用 script_score 查询仅计算已过滤的返回文档的分数。\n相关指南（先读这些） #    相关性与打分策略  Query DSL 基础  参考样例 #  例如，以下请求创建一个包含一个文档的索引：\nPUT testindex1/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;multiplier\u0026#34;: 0.5 } 您可以使用 match 查询返回所有在 name 字段中包含 John 的文档：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34; } } } 在返回内容中，文档 1 的得分为 0.2876821 ：\n{ \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.2876821, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.2876821, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;multiplier\u0026#34;: 0.5 } } ] } } 现在我们通过使用一个脚本改变文档得分，该脚本将得分计算为 _score 字段的值乘以 multiplier 字段的值。在以下查询中，你可以通过 _score 变量访问文档的当前相关性得分，并将 multiplier 值作为 doc['multiplier'].value ：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34; } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;_score * doc[\u0026#39;multiplier\u0026#39;].value\u0026#34; } } } } 文档 1 的得分是原始得分的一半：\n{ \u0026#34;took\u0026#34;: 8, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.14384104, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.14384104, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;multiplier\u0026#34;: 0.5 } } ] } } 参数说明 #  script_score 查询支持以下顶级参数。\n   参数 数据类型 描述     query Object 用于搜索的查询。必填。   script Object 用于计算 query 返回的文档得分的脚本。必填。   min_score Float 排除得分低于 min_score 的文档从结果中。可选。   boost Float 通过给定的倍数提升文档的得分。小于 1.0 的值会降低相关性，大于 1.0 的值会增加相关性。默认值为 1.0。     script_score 查询计算的相关性得分不能为负。\n 使用内置函数自定义评分计算 #  要自定义评分计算，您可以使用其中一个内置的 Painless 函数。对于每个函数，Easysearch 都提供一个或多个可在脚本评分上下文中访问的 Painless 方法。您可以直接调用以下部分列出的 Painless 方法，而无需使用类名或实例名限定符。\n饱和度 #  饱和度函数计算饱和度，其中 value 是字段值， pivot 被选择以便当 value 大于 pivot 时评分大于 0.5，当 value 小于 pivot 时评分小于 0.5。评分在(0, 1)范围内。要应用饱和度函数，请调用以下 Painless 方法：\ndouble saturation(double \u0026lt;field-value\u0026gt;, double \u0026lt;pivot\u0026gt;) 以下示例查询在 articles 索引中搜索文本 neural search 。它结合了原始文档相关性得分与 article_rank 值，该值首先通过饱和函数进行转换：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;neural search\u0026#34; } }, \u0026#34;script\u0026#34; : { \u0026#34;source\u0026#34; : \u0026#34;_score + saturation(doc[\u0026#39;article_rank\u0026#39;].value, 11)\u0026#34; } } } } Sigmoid 函数 #\n 与饱和函数类似，sigmoid 函数计算得分为 score = value^exp/ (value^exp + pivot^exp) ，其中 value 是字段值， exp 是指数缩放因子， pivot 被选择为当 value 大于 pivot 时得分为大于 0.5，当 value 小于 pivot 时得分为小于 0.5。要应用 sigmoid 函数，请调用以下 Painless 方法：\ndouble sigmoid(double \u0026lt;field-value\u0026gt;, double \u0026lt;pivot\u0026gt;, double \u0026lt;exp\u0026gt;) 以下示例查询在 articles 索引中搜索文本 neural search 。它结合了原始文档相关性得分与 article_rank 值，该值首先通过 sigmoid 函数进行转换：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;neural search\u0026#34; } }, \u0026#34;script\u0026#34; : { \u0026#34;source\u0026#34; : \u0026#34;_score + sigmoid(doc[\u0026#39;article_rank\u0026#39;].value, 11, 2)\u0026#34; } } } } 随机得分 #  随机分数函数在[0, 1)范围内生成均匀分布的随机分数。要应用随机分数函数，请调用以下 Painless 方法之一：\n double randomScore(int \u0026lt;seed\u0026gt;) : 使用内部 Lucene 文档 ID 作为种子值。 double randomScore(int \u0026lt;seed\u0026gt;, String \u0026lt;field-name\u0026gt;)  以下查询使用 random_score 函数，并带有 seed 和 field ：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;neural search\u0026#34; } }, \u0026#34;script\u0026#34; : { \u0026#34;source\u0026#34; : \u0026#34;randomScore(20, \u0026#39;_seq_no\u0026#39;)\u0026#34; } } } } 衰减函数 #  使用衰减函数，可以根据邻近度或时效性对结果进行评分。您可以使用指数、高斯或线性衰减曲线来计算分数。要应用衰减函数，根据字段类型调用以下 Painless 方法之一：\n 数值字段：  double decayNumericGauss(double \u0026lt;origin\u0026gt;, double \u0026lt;scale\u0026gt;, double \u0026lt;offset\u0026gt;, double \u0026lt;decay\u0026gt;, double \u0026lt;field-value\u0026gt;) double decayNumericExp(double \u0026lt;origin\u0026gt;, double \u0026lt;scale\u0026gt;, double \u0026lt;offset\u0026gt;, double \u0026lt;decay\u0026gt;, double \u0026lt;field-value\u0026gt;) double decayNumericLinear(double \u0026lt;origin\u0026gt;, double \u0026lt;scale\u0026gt;, double \u0026lt;offset\u0026gt;, double \u0026lt;decay\u0026gt;, double \u0026lt;field-value\u0026gt;)   地理点字段：  double decayGeoGauss(String \u0026lt;origin\u0026gt;, String \u0026lt;scale\u0026gt;, String \u0026lt;offset\u0026gt;, double \u0026lt;decay\u0026gt;, GeoPoint \u0026lt;field-value\u0026gt;) double decayGeoExp(String \u0026lt;origin\u0026gt;, String \u0026lt;scale\u0026gt;, String \u0026lt;offset\u0026gt;, double \u0026lt;decay\u0026gt;, GeoPoint \u0026lt;field-value\u0026gt;) double decayGeoLinear(String \u0026lt;origin\u0026gt;, String \u0026lt;scale\u0026gt;, String \u0026lt;offset\u0026gt;, double \u0026lt;decay\u0026gt;, GeoPoint \u0026lt;field-value\u0026gt;)   日期字段：  double decayDateGauss(String \u0026lt;origin\u0026gt;, String \u0026lt;scale\u0026gt;, String \u0026lt;offset\u0026gt;, double \u0026lt;decay\u0026gt;, JodaCompatibleZonedDateTime \u0026lt;field-value\u0026gt;) double decayDateExp(String \u0026lt;origin\u0026gt;, String \u0026lt;scale\u0026gt;, String \u0026lt;offset\u0026gt;, double \u0026lt;decay\u0026gt;, JodaCompatibleZonedDateTime \u0026lt;field-value\u0026gt; double decayDateLinear(String \u0026lt;origin\u0026gt;, String \u0026lt;scale\u0026gt;, String \u0026lt;offset\u0026gt;, double \u0026lt;decay\u0026gt;, JodaCompatibleZonedDateTime \u0026lt;field-value\u0026gt;)    示例：数值字段 #  以下查询在数值字段上使用了指数衰减函数：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;neural search\u0026#34; } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;decayNumericExp(params.origin, params.scale, params.offset, params.decay, doc[\u0026#39;article_rank\u0026#39;].value)\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;origin\u0026#34;: 50, \u0026#34;scale\u0026#34;: 20, \u0026#34;offset\u0026#34;: 30, \u0026#34;decay\u0026#34;: 0.5 } } } } } 示例：地理点字段 #  以下查询在地理点字段上使用了高斯衰减函数：\nGET hotels/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hotel\u0026#34; } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;decayGeoGauss(params.origin, params.scale, params.offset, params.decay, doc[\u0026#39;location\u0026#39;].value)\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;origin\u0026#34;: \u0026#34;40.71,74.00\u0026#34;, \u0026#34;scale\u0026#34;: \u0026#34;300ft\u0026#34;, \u0026#34;offset\u0026#34;: \u0026#34;200ft\u0026#34;, \u0026#34;decay\u0026#34;: 0.25 } } } } } 示例：日期字段 #  以下查询在日期字段上使用了线性衰减函数：\nGET blogs/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;easysearch\u0026#34; } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;decayDateLinear(params.origin, params.scale, params.offset, params.decay, doc[\u0026#39;date_posted\u0026#39;].value)\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;origin\u0026#34;: \u0026#34;2022-04-24\u0026#34;, \u0026#34;scale\u0026#34;: \u0026#34;6d\u0026#34;, \u0026#34;offset\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;decay\u0026#34;: 0.25 } } } } } 词频函数 #  词频函数在评分脚本源中暴露词级统计信息。您可以使用这些统计信息来实现自定义信息检索和排序算法，例如查询时的基于流行度的乘法或加法评分提升。要应用词频函数，请调用以下 Painless 方法之一：\n int termFreq(String \u0026lt;field-name\u0026gt;, String \u0026lt;term\u0026gt;) ：检索特定词在字段中的词频。 long totalTermFreq(String \u0026lt;field-name\u0026gt;, String \u0026lt;term\u0026gt;) ：检索特定词在字段中的总词频。 long sumTotalTermFreq(String \u0026lt;field-name\u0026gt;) : 获取字段中总词频的求和。  以下查询将 fields 列表中每个字段的词频总和乘以 multiplier 值作为分数计算：\nGET /demo_index_v1/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; for (int x = 0; x \u0026lt; params.fields.length; x++) { String field = params.fields[x]; if (field != null) { return params.multiplier * totalTermFreq(field, params.term); } } return params.default_value;  \u0026amp;#34;\u0026amp;#34;\u0026amp;#34;, \u0026amp;#34;params\u0026amp;#34;: { \u0026amp;#34;fields\u0026amp;#34;: [\u0026amp;#34;title\u0026amp;#34;, \u0026amp;#34;description\u0026amp;#34;], \u0026amp;#34;term\u0026amp;#34;: \u0026amp;#34;ai\u0026amp;#34;, \u0026amp;#34;multiplier\u0026amp;#34;: 2, \u0026amp;#34;default_value\u0026amp;#34;: 1 } } } }  } } \n如果 search.allow_expensive_queries 设置为 false ，则不会执行 script_score 查询。\n ","subcategory":null,"summary":"","tags":null,"title":"Script Score 查询","url":"/easysearch/main/docs/features/query-dsl/specialized/script-score/"},{"category":null,"content":"Regexp 查询 #  使用 regexp 查询来搜索符合正则表达式的词项。有关编写正则表达式的更多信息，请参见正则表达式语法。\n相关指南（先读这些） #    部分匹配  结构化搜索  以下查询搜索以任何大写或小写字母开头的任何词项 amlet ：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;[a-zA-Z]amlet\u0026#34; } } } \n重要注意事项：\n 正则表达式应用于字段中的词条（即，标记/token），而不是整个字段。 默认情况下，正则表达式的最大长度为 1,000 个字符。要更改最大长度，请更新 index.max_regex_length 设置。 正则表达式使用 Lucene 语法，这与更标准的实现有所不同。请充分测试以确保获得预期的结果。 为了提高正则表达式查询的性能，避免使用没有前缀或后缀的通配符模式，例如 .* 或 .*?+ 。 regexp 查询可能会非常耗时，并且需要将 search.allow_expensive_queries 设置为 true 。在频繁执行 regexp 查询之前，请测试其对集群性能的影响，并考虑使用其他可能达到类似效果的查询。更多性能优化建议，请参考 部分匹配章节。   参数说明 #  查询接受字段名称（ \u0026lt;field\u0026gt; ）作为顶级参数：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[Ss]ample\u0026#34;, ... } } } } \u0026lt;field\u0026gt; 接受以下参数。除了 value 之外，所有参数都是可选的。\n   参数 数据类型 描述     value String 用于匹配指定在 \u0026lt;field\u0026gt; 字段中的项的正则表达式。   boost Float 一个浮点数，用于指定该字段对相关性评分的权重。值大于 1.0 会增加字段的相关性。值在 0.0 到 1.0 之间会降低字段的相关性。默认值为 1.0。   case_insensitive Boolean 如果 true ，则允许对正则表达式值与索引字段值进行不区分大小写的匹配。默认值为 false （大小写敏感性由字段的映射决定）。   flags String 启用 Lucene 正则表达式引擎的可选操作符。有效值请参见可选操作符。   flags_value Integer flags 的整数替代形式，直接使用 Lucene 正则表达式 flag 的位掩码数值。与 flags 互斥，二者只需指定一个。   max_determinized_states Integer Lucene 将正则表达式转换为具有多个确定状态的自动机。此参数指定查询所需的自动机状态的最大数量。使用此参数以防止资源消耗过高。要运行复杂的正则表达式，您可能需要增加此参数的值。默认值为 10,000。   rewrite String 确定 Easysearch 如何重写和评分多词查询。有效值为 constant_score 、 scoring_boolean 、 constant_score_boolean 、 top_terms_N 、 top_terms_boost_N 和 top_terms_blended_freqs_N 。默认值为 constant_score 。     如果 search.allow_expensive_queries 设置为 false ，则 regexp 查询将不被执行。\n 支持的正则语法 #  Easysearch 使用 Lucene 正则表达式语法，与标准正则语法（PCRE）有所不同。以下是常用元素速查：\n   语法 含义 示例     . 匹配任意单个字符 a.c → abc、adc   + 前一个字符出现一次或多次 ab+c → abc、abbc   * 前一个字符出现零次或多次 ab*c → ac、abc、abbc   ? 前一个字符出现零次或一次 ab?c → ac、abc   [abc] 字符类，匹配其中任意一个 [aeiou] → 任意元音   [a-z] 字符范围 [0-9] → 任意数字   (ab\\|cd) 交替，匹配其中任意一个 (cat\\|dog) → cat 或 dog   {n,m} 重复 n 到 m 次（需启用 INTERVAL flag） a{2,4} → aa、aaa、aaaa     注意：Lucene 不支持 \\d、\\w、\\s 等快捷方式。需要用 [0-9]、[a-zA-Z0-9_] 等字符类来代替。\n 在分析过的字段上使用 #  regexp 查询作用于倒排索引中的 单个词项，而不是原始字段值。例如：\n 字段值 \u0026quot;Quick brown fox\u0026quot; → 分词后为词项 quick、brown、fox regexp: \u0026quot;br.*\u0026quot; → ✅ 匹配（匹配词项 brown） regexp: \u0026quot;Qu.*\u0026quot; → ❌ 不匹配（词项是小写 quick，不是 Quick） regexp: \u0026quot;quick br.*\u0026quot; → ❌ 不匹配（正则表达式不会跨越多个词项）  因此，如果需要对原始完整字段值做正则匹配，应使用 keyword 类型的字段。\n替代方案 #  对于常见的部分匹配需求，下面这些方案往往性能更优：\n   需求 推荐方案     以某前缀开头  prefix 查询，开销更低   包含任意通配符  wildcard 查询，语法更简洁   搜索即输入（typeahead） 使用 index_prefixes 或 edge n-gram 分词器   复杂文本模式匹配 考虑在索引时使用自定义分析器进行预处理    ","subcategory":null,"summary":"","tags":null,"title":"Regexp 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/regexp/"},{"category":null,"content":"Rank Feature 查询 #  使用 rank_feature 查询根据文档中的数值（如相关性分数、人气或新鲜度）提升文档分数。如果你希望使用数值特征微调相关性排名，这种查询非常理想。与全文检索不同，rank_feature 仅关注数值信号；在复合查询（如 bool）中与其他查询结合时效果最佳。\nrank_feature 查询要求目标字段映射为 rank_feature 字段类型。这可以启用内部优化的评分，从而实现快速高效的提升。\n相关指南（先读这些） #    相关性与打分策略  Query DSL 基础  分数影响取决于字段值以及可选的 saturation 、 log 或 sigmoid 函数。这些函数在查询时动态应用以计算最终文档分数；它们不会更改或存储文档中的任何值。\n参数说明 #  rank_feature 查询支持以下参数。\n   参数 数据类型 必需/可选 描述     field String 必需 一个 rank_feature 或 rank_features 字段，用于影响文档评分。   boost Float 可选 应用于评分的乘数。默认值为 1.0 。0 到 1 之间的值会降低评分；大于 1 的值会提高评分。   saturation Object 可选 对特征值应用饱和函数。随着值的增加，增益会增长，但在 pivot 之后会趋于平稳。如果没有提供其他函数，则使用此默认函数。一次只能使用 saturation 、 log 或 sigmoid 中的一个函数。   log Object 可选 使用基于字段值的对数评分函数。适用于大范围值的场景。一次只能使用 saturation 、 log 或 sigmoid 中的一个函数。   sigmoid Object 可选 对评分影响应用 S 形曲线，由 pivot 和 exponent 控制。一次只能使用 saturation 、 log 或 sigmoid 中的一个函数。   positive_score_impact Boolean 可选 当 false 时，较低值会获得更高的评分。适用于像价格这样的特征，其中较小值更好。作为映射的一部分定义。默认值为 true 。    参考样例 #  以下示例展示了如何定义和使用 rank_feature 字段来影响文档评分。\n定义一个包含 rank_feature 字段的索引来表示信号，如 popularity :\nPUT /products { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;popularity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;rank_feature\u0026#34; } } } } 添加具有不同流行度值的示例产品：\nPOST /products/_bulk { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 1 } } { \u0026#34;title\u0026#34;: \u0026#34;Wireless Earbuds\u0026#34;, \u0026#34;popularity\u0026#34;: 1 } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 2 } } { \u0026#34;title\u0026#34;: \u0026#34;Bluetooth Speaker\u0026#34;, \u0026#34;popularity\u0026#34;: 10 } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 3 } } { \u0026#34;title\u0026#34;: \u0026#34;Portable Charger\u0026#34;, \u0026#34;popularity\u0026#34;: 25 } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 4 } } { \u0026#34;title\u0026#34;: \u0026#34;Smartwatch\u0026#34;, \u0026#34;popularity\u0026#34;: 50 } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 5 } } { \u0026#34;title\u0026#34;: \u0026#34;Noise Cancelling Headphones\u0026#34;, \u0026#34;popularity\u0026#34;: 100 } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 6 } } { \u0026#34;title\u0026#34;: \u0026#34;Gaming Laptop\u0026#34;, \u0026#34;popularity\u0026#34;: 250 } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 7 } } { \u0026#34;title\u0026#34;: \u0026#34;4K Monitor\u0026#34;, \u0026#34;popularity\u0026#34;: 500 } 基础排序功能查询 #  你可以使用 rank_feature 基于 popularity 分数来提升结果：\nPOST /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;popularity\u0026#34; } } } 这个查询本身不执行过滤。相反，它根据 popularity 的值对所有文档进行评分：更高的值会产生更高的分数：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 7, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.9252834, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;_score\u0026#34;: 0.9252834, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;4K Monitor\u0026#34;, \u0026#34;popularity\u0026#34;: 500 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;_score\u0026#34;: 0.86095566, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gaming Laptop\u0026#34;, \u0026#34;popularity\u0026#34;: 250 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34;: 0.71237755, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Noise Cancelling Headphones\u0026#34;, \u0026#34;popularity\u0026#34;: 100 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 0.5532503, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Smartwatch\u0026#34;, \u0026#34;popularity\u0026#34;: 50 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 0.38240916, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Portable Charger\u0026#34;, \u0026#34;popularity\u0026#34;: 25 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.19851118, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Bluetooth Speaker\u0026#34;, \u0026#34;popularity\u0026#34;: 10 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.024169207, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Wireless Earbuds\u0026#34;, \u0026#34;popularity\u0026#34;: 1 } } ] } } 与全文搜索结合 #\n 要筛选相关结果并根据热度提升它们，请使用以下请求。此查询对匹配“headphones”的所有文档进行排序，并提升那些热度更高的文档：\nPOST /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;headphones\u0026#34; } }, \u0026#34;should\u0026#34;: { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;popularity\u0026#34; } } } } } 权重提升 #  boost 参数允许你调整 rank_feature 子句的分数贡献。这在 bool 等复合查询中特别有用，你可以控制数值字段（如人气、新鲜度或相关性分数）对最终文档排序的影响程度。\n在以下示例中， bool 查询匹配包含“headphones”的文档，并使用 rank_feature 子句和 boost 为 2.0 的 rank_feature 子句来提升更受欢迎的结果。这使 rank_feature 分数对整体文档分数的贡献翻倍：\nPOST /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;headphones\u0026#34; } }, \u0026#34;should\u0026#34;: { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;boost\u0026#34;: 2.0 } } } } } 配置得分函数 #  默认情况下， rank_feature 查询使用一个 saturation 函数，其 pivot 值由字段派生。您可以显式地将函数设置为 saturation 、 log 或 sigmoid 。\n饱和函数 #  saturation 函数是 rank_feature 查询中使用的默认得分方法。它为具有较大特征值的文档分配更高的得分，但随着值超过指定的 pivot ，得分的增加会变得更加平缓。当您希望对非常大的值给予递减回报时，这很有用，例如，在避免过度奖励极高数字的同时提升 popularity 。计算得分的公式是 value of the rank_feature field / (value of the rank_feature field + pivot) 。产生的得分始终在 0 和 1 之间。如果未提供 pivot ，则使用索引中所有 rank_feature 值的近似几何平均值。\n以下示例使用 saturation ，其 pivot 为 50 ：\nPOST /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;saturation\u0026#34;: { \u0026#34;pivot\u0026#34;: 50 } } } } pivot 定义了评分增长减缓的点。大于 pivot 的值仍然会增加评分，但会随着收益递减，如返回的命中所示：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 7, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.9090909, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;_score\u0026#34;: 0.9090909, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;4K Monitor\u0026#34;, \u0026#34;popularity\u0026#34;: 500 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;_score\u0026#34;: 0.8333333, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gaming Laptop\u0026#34;, \u0026#34;popularity\u0026#34;: 250 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34;: 0.6666666, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Noise Cancelling Headphones\u0026#34;, \u0026#34;popularity\u0026#34;: 100 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 0.5, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Smartwatch\u0026#34;, \u0026#34;popularity\u0026#34;: 50 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 0.3333333, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Portable Charger\u0026#34;, \u0026#34;popularity\u0026#34;: 25 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.16666669, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Bluetooth Speaker\u0026#34;, \u0026#34;popularity\u0026#34;: 10 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.019607842, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Wireless Earbuds\u0026#34;, \u0026#34;popularity\u0026#34;: 1 } } ] } } LOG 函数 #\n 当 rank_feature 字段包含一个显著范围内的值时， log 函数很有用。它对 score 应用对数刻度，减少极高值的影响，并帮助在宽泛的值分布中实现评分的归一化。当低值之间的小差异应该比高值之间的大差异更有影响时，这尤其有用。评分使用公式 log(scaling_factor + rank_feature field) 计算。以下示例使用 scaling_factor 为 2 ：\nPOST /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;log\u0026#34;: { \u0026#34;scaling_factor\u0026#34;: 2 } } } } 在示例数据集中， popularity 字段的范围从 1 到 500 。 log 函数压缩了来自 250 和 500 等大值的 score 贡献，同时仍然允许具有 10 或 25 的文档获得有意义的分数。相比之下，如果你应用 saturation 函数，高于 pivot 的文档会迅速接近相同的最大分数：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 7, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 6.2186003, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;_score\u0026#34;: 6.2186003, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;4K Monitor\u0026#34;, \u0026#34;popularity\u0026#34;: 500 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;_score\u0026#34;: 5.529429, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gaming Laptop\u0026#34;, \u0026#34;popularity\u0026#34;: 250 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34;: 4.624973, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Noise Cancelling Headphones\u0026#34;, \u0026#34;popularity\u0026#34;: 100 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 3.9512436, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Smartwatch\u0026#34;, \u0026#34;popularity\u0026#34;: 50 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 3.295837, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Portable Charger\u0026#34;, \u0026#34;popularity\u0026#34;: 25 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 2.4849067, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Bluetooth Speaker\u0026#34;, \u0026#34;popularity\u0026#34;: 10 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.0986123, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Wireless Earbuds\u0026#34;, \u0026#34;popularity\u0026#34;: 1 } } ] } } SIGMOID 函数 #\n sigmoid 函数提供了一个平滑的 S 形评分曲线，这在你想控制评分影响的陡峭程度和中点时特别有用。评分使用公式 rank feature field value^exp / (rank feature field value^exp + pivot^exp) 计算得出。以下示例使用了一个配置了 pivot 和 exponent 的 sigmoid 函数。 pivot 定义了评分等于 0.5 的值。 exponent 控制曲线的陡峭程度。较低值会导致在 pivot 周围的过渡更加尖锐：\nPOST /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;sigmoid\u0026#34;: { \u0026#34;pivot\u0026#34;: 50, \u0026#34;exponent\u0026#34;: 0.5 } } } } sigmoid 函数平滑地提升了 pivot （在本例中为 50 ）周围的分数，对接近 pivot 的值给予适度偏好，同时使高低两端的分数趋于平缓：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 7, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.7597469, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;_score\u0026#34;: 0.7597469, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;4K Monitor\u0026#34;, \u0026#34;popularity\u0026#34;: 500 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;_score\u0026#34;: 0.690983, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gaming Laptop\u0026#34;, \u0026#34;popularity\u0026#34;: 250 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34;: 0.58578646, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Noise Cancelling Headphones\u0026#34;, \u0026#34;popularity\u0026#34;: 100 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 0.5, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Smartwatch\u0026#34;, \u0026#34;popularity\u0026#34;: 50 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 0.41421357, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Portable Charger\u0026#34;, \u0026#34;popularity\u0026#34;: 25 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.309017, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Bluetooth Speaker\u0026#34;, \u0026#34;popularity\u0026#34;: 10 } }, { \u0026#34;_index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.12389934, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Wireless Earbuds\u0026#34;, \u0026#34;popularity\u0026#34;: 1 } } ] } } 分数反转 #  默认情况下，较高的值会导致较高的分数。如果您希望较低的值产生较高的分数（例如，较低的价格更相关），请在索引创建期间将 positive_score_impact 设置为 false ：\nPUT /products_new { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;popularity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;rank_feature\u0026#34;, \u0026#34;positive_score_impact\u0026#34;: false } } } } ","subcategory":null,"summary":"","tags":null,"title":"Rank Feature 查询","url":"/easysearch/main/docs/features/query-dsl/specialized/rank-feature/"},{"category":null,"content":"Range 查询 #  您可以使用 range 查询搜索字段中的值范围。\n相关指南（先读这些） #    结构化搜索  Query DSL 基础  要搜索 line_id 值为 \u0026gt;= 10 和 \u0026lt;= 20 的文档，请使用以下请求：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;line_id\u0026#34;: { \u0026#34;gte\u0026#34;: 10, \u0026#34;lte\u0026#34;: 20 } } } } 运算符 #  范围查询中的字段参数接受以下可选运算符参数：\n gte：大于或等于 gt：大于 lte：小于或等于 lt：小于  日期字段 #  您可以对包含日期的字段使用范围查询。例如，假设您有一个products索引，并且想要查找 2019 年添加的所有产品：\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;created\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2019/01/01\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2019/12/31\u0026#34; } } } } 日期格式 #\n 要在查询中使用字段映射格式以外的日期格式，请在 format 字段中指定它。\n例如，如果products索引将created字段映射为 strict_date_optional_time，则可以为查询日期指定不同的格式，如下所示：\nGET /products/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;created\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;01/01/2022\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;31/12/2022\u0026#34;, \u0026#34;format\u0026#34;:\u0026#34;dd/MM/yyyy\u0026#34; } } } } 日期数学计算 #  range 查询支持对日期进行数学计算，可以使用 now 占位符来表示当前时间，非常适合构建滑动时间窗口。例如，查找最近一小时内创建的文档：\nGET logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-1h\u0026#34; } } } } 日期计算还可以应用到某个具体的日期上，在日期后加上双管符号（||）并紧跟日期数学表达式：\nGET logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2024-01-01||+1M\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2024-01-01||+2M\u0026#34; } } } } 常用的日期数学单位：\n   符号 含义 示例     y 年 now-1y（一年前）   M 月 now-6M（六个月前）   w 周 now-2w（两周前）   d 天 now-7d（七天前）   h 小时 now-12h（十二小时前）   m 分钟 now-30m（三十分钟前）   s 秒 now-60s（六十秒前）     日期计算是 日历相关 的——它知道每月的具体天数以及闰年等信息。\n 字符串范围 #  range 查询同样可以处理字符串字段，按照 字典顺序（lexicographically） 排列。在倒排索引中的词项就是按字典顺序排列的，这使得字符串范围查询成为可能。\nGET books/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;a\u0026#34;, \u0026#34;lt\u0026#34;: \u0026#34;b\u0026#34; } } } }  注意基数问题：数字和日期字段的索引方式使高效的范围计算成为可能，但字符串并非如此。Easysearch 需要为范围内的每个词项都执行 term 匹配，因此对高基数字段（唯一词项很多的字段）使用字符串范围会比较慢。字符串范围更适合在低基数字段上使用。\n 完整参数说明 #     参数 数据类型 描述     gte 数值/字符串/日期 大于或等于。可选。   gt 数值/字符串/日期 大于。可选。   lte 数值/字符串/日期 小于或等于。可选。   lt 数值/字符串/日期 小于。可选。   format String 日期字段的格式化模式，如 dd/MM/yyyy。可选。   time_zone String 用于将日期值转换为 UTC 的时区。例如 +08:00 或 Asia/Shanghai。可选。   boost Float 相关性权重。值 \u0026gt; 1.0 增加相关性，0.0~1.0 降低相关性。默认 1.0。   relation String 对 range 类型字段指示匹配方式：INTERSECTS（默认）、CONTAINS 或 WITHIN。可选。     传统替代语法：Easysearch 也支持 from/to（等同于 gt/lt）以及 include_lower/include_upper（Boolean，控制是否包含边界值）参数。推荐使用 gte/gt/lte/lt，语义更清晰。\n ","subcategory":null,"summary":"","tags":null,"title":"Range 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/range/"},{"category":null,"content":"Query String 查询 #  query_string 查询根据查询字符串语法解析查询字符串。它提供了创建强大而简洁的查询的功能，这些查询可以包含通配符并搜索多个字段。\n相关指南（先读这些） #    全文搜索  Query DSL 基础  查询字符串语法 — 通配符、模糊、范围、布尔等完整语法参考   使用注意：使用 query_string 查询的搜索不会返回嵌套文档。要搜索嵌套字段，请使用 nested 查询。\n  语法严格性：查询字符串查询具有严格的语法，在语法无效时会返回错误。因此，它不适合搜索框应用程序。对于不太严格的替代方案，可以考虑使用 simple_query_string 查询。如果你不需要查询语法支持，使用 match 查询。\n 参考样例 #  运行以下搜索时， query_string 查询会将 (new york city) OR (big apple) 拆分为两部分： new york city 和 big apple 。 content 字段的分词器随后会分别将每个部分转换为标记，然后返回匹配的文档。由于查询语法不使用空格作为运算符，因此 new york city 会按原样传递给分词器。\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;(new york city) OR (big apple)\u0026#34;, \u0026#34;default_field\u0026#34;: \u0026#34;content\u0026#34; } } } 参数说明 #  下表列出了 query_string 查询支持的参数。除 query 外，所有参数都是可选的。\n   参数 数据类型 描述     query String 用于搜索的查询字符串语法中可能包含的表达式。必填。   allow_leading_wildcard Boolean 指定是否允许 * 和 ? 作为搜索词的首字符。默认为 true 。   analyze_wildcard Boolean 指定 Easysearch 是否应尝试分析通配符词项。默认值为 false 。   analyzer String 用于对查询字符串文本进行分词的分词器。默认值为为 default_field 指定的索引时分词器。如果未为 default_field 指定分词器，则 analyzer 为索引的默认分词器。有关 index.query.default_field 的更多信息，请参阅动态索引级索引设置。   auto_generate_synonyms_phrase_query Boolean 指定是否自动为多词同义词创建匹配短语查询。例如，如果你将 ba, batting average 指定为同义词并搜索 ba ，Easysearch 会搜索 ba OR \u0026ldquo;batting average\u0026rdquo; （如果此选项为 true ）或 ba OR (batting AND average) （如果此选项为 false ）。默认值为 true 。   boost float 通过给定的乘数提升子句的权重。适用于在复合查询中对子句进行加权。[0, 1) 范围内的值会降低相关性，而大于 1 的值会增加相关性。默认值为 1 。   default_field String 如果查询字符串中未指定字段，则在此字段中搜索。支持通配符。默认值为 index.query.default_field 索引设置中指定的值。默认情况下，index.query.default_field 是 *，这意味着提取所有适用于 term 查询的字段并过滤元数据字段。如果未指定 prefix，则提取的字段将组合成一个查询。符合条件的字段不包括嵌套文档。搜索所有符合条件的字段可能是一个资源密集型操作。indices.query.bool.max_clause_count 搜索设置定义了字段数量和一次可以查询的词数乘积的最大值。indices.query.bool.max_clause_count 的默认值为 1,024。   default_operator String 如果查询字符串包含多个搜索词，文档是否需要所有词都匹配（ AND ）或只需一个词匹配（ OR ）才被视为匹配。有效值为：- OR : 字符串 to be 被解释为 to OR be;- AND : 字符串 to be 被解释为 to AND be;默认为 OR 。   enable_position_increments Boolean 当 true 时，生成的查询会考虑位置增量。当停用词被移除时，留下不想要的“间隙”时，此设置很有用。默认值为 true 。   fields 字符串数组 要搜索的字段列表（例如，\u0026quot;fields\u0026quot;: [\u0026quot;title^4\u0026quot;, \u0026quot;description\u0026quot;]）。支持通配符。如果未指定，则默认为 index.query.default_field 设置，该设置默认为 [\u0026quot;*\u0026quot;]。   fuzziness String 在确定词项是否与值匹配时，将一个词转换为另一个词所需的字符编辑次数（插入、删除、替换）。例如， wined 和 wind 之间的距离是 1。有效值是非负整数或 AUTO 。默认值 AUTO 根据每个词的长度选择值，对于大多数用例是一个不错的选择。   fuzzy_max_expansions 正整数 查询可以扩展到的最大词数。模糊查询会扩展到与指定距离（ fuzziness ）内的匹配词。然后 Easysearch 尝试匹配这些词。默认值为 50 。   fuzzy_transpositions Boolean 将 fuzzy_transpositions 设置为 true （默认）会在 fuzziness 选项的插入、删除和替换操作中添加相邻字符的交换。例如，如果 fuzzy_transpositions 为真（交换“n”和“i”），则 wind 和 wnid 之间的距离为 1；如果为假（删除“n”，插入“n”），则距离为 2。如果 fuzzy_transpositions 为假， rewind 和 wnid 与 wind 的距离相同（2），尽管从更以人为中心的观点来看， wnid 是一个明显的拼写错误。对于大多数用例，默认值是一个不错的选择。   lenient Boolean 将 lenient 设置为 true 会忽略查询与文档字段之间的数据类型不匹配。例如， \u0026ldquo;8.2\u0026rdquo; 的查询字符串可以匹配类型为 float 的字段。默认值为 false 。   max_determinized_states 正整数 Lucene 可以为包含正则表达式（例如 \u0026quot;query\u0026quot;: \u0026quot;/wind.+?/\u0026quot; ）的查询字符串创建的最大“状态”数量（复杂性的度量），数值越大允许使用更多内存的查询。默认值为 10,000。   minimum_should_match 正整数或负整数、正百分比或负百分比、组合 如果查询字符串包含多个搜索词并且你使用 or 运算符，文档被考虑为匹配所需的匹配词数。例如，如果 minimum_should_match 为 2， wind often rising 不匹配 The Wind Rises. 如果 minimum_should_match 为 1 ，则匹配。详情请参阅 Minimum should match。   phrase_slop Integer 允许在匹配的词之间出现的最大词数。如果 phrase_slop 为 2，则短语中匹配的词之间最多允许两个词。调换顺序的词有 2 的容差。默认值为 0 （精确短语匹配，其中匹配的词必须相邻）。   quote_analyzer String 用于在查询字符串中分词引号内文本的分词器。覆盖引号文本的 analyzer 参数。默认值为 default_field 指定的 search_quote_analyzer 。   quote_field_suffix String 此选项支持使用与非精确匹配不同的分析方法来搜索精确匹配（用引号括起来）。例如，如果 quote_field_suffix 是 .exact ，而你搜索 \u0026quot;lightly\u0026quot; 在 title 字段中，Easysearch 会在 lightly 字段中搜索单词 title.exact 。这个第二个字段可能使用不同的类型（例如， keyword 而不是 text ）或不同的分词器。   rewrite String 确定 Easysearch 如何重写和评分多词查询。有效值为 constant_score 、 scoring_boolean 、 constant_score_boolean 、 top_terms_N 、 top_terms_boost_N 和 top_terms_blended_freqs_N 。默认值为 constant_score 。   tie_breaker Float（0.0~1.0） 当查询使用多个字段时，tie_breaker 控制非最佳匹配字段对评分的贡献。0.0 只取最佳匹配字段得分，1.0 将所有匹配字段得分相加。默认值为 0.0。   time_zone String 指定从 UTC 偏移所需时区的小时数。如果查询字符串包含日期范围，则需要指定时区偏移数字。例如，对于包含日期范围 \u0026quot;query\u0026quot;: \u0026quot;wind rises release_date[2012-01-01 TO 2014-01-01]\u0026quot; 的查询，请设置 time_zone: \u0026quot;-08:00\u0026quot;。默认时区格式为 UTC。    query string 字符串查询语法 #  字符串查询 (query string) 语法基于 Apache Lucene 查询语法。下面会介绍相关的语法，详细的使用可以 参考这里\n在以下情况下，您可以使用查询字符串语法：\n在一个 query_string 查询中，例如：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the wind AND (rises OR rising)\u0026#34; } } } 如果你使用 HTTP 请求查询参数进行搜索，例如：\nGET _search?q=wind 字符串查询由词项和运算符组成。词项是一个单词（例如，在查询 wind rises 中，词项是 wind 和 rises ）。如果多个词项被引号包围，它们被视为一个短语，其中单词按出现的顺序匹配（例如， “wind rises” ）。运算符（如 OR 、 AND 和 NOT ）指定用于解释查询字符串中文本的布尔逻辑。\n本节中的示例使用一个包含以下映射和文档的索引：\nPUT /testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;english\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;english\u0026#34; } } } } } } PUT /testindex/_doc/1 { \u0026quot;title\u0026quot;: \u0026quot;The wind rises\u0026quot; }\nPUT /testindex/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Gone with the wind\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;A 1939 American epic historical film\u0026quot; }\nPUT /testindex/_doc/3 { \u0026quot;title\u0026quot;: \u0026quot;Windy city\u0026quot; }\nPUT /testindex/_doc/4 { \u0026quot;article title\u0026quot;: \u0026quot;Wind turbines\u0026quot; } 保留字符 #\n 以下是查询字符串查询的保留字符列表：\n+, -, =, \u0026amp;\u0026amp;, ||, \u0026gt;, \u0026lt;, !, (, ),{, }, [, ], ^, \u0026#34;, ~, *, ?, :, \\, / \n使用反斜杠（ \\ ）转义保留字符。在发送 JSON 请求时，使用双反斜杠（ \\ ）转义保留字符（因为反斜杠字符本身也是保留字符，你必须用另一个反斜杠来转义反斜杠）。\n 例如，要搜索表达式 2*3 ，请指定查询字符串： 2\\\\*3 。\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;title: 2\\\\*3\u0026#34; } } }  \u0026gt; 和 \u0026lt; 符号不能转义。它们被视为范围查询。\n 空白字符和空查询 #  空白字符不被视为操作符。如果查询字符串为空或只包含空白字符，查询不会返回结果。\n字段名 #  在冒号前指定字段名。下表包含带有字段名的示例查询。\n   query_string 查询中的查询 文档匹配的标准 从 testindex 索引中匹配文档     title: wind title 字段包含单词 wind 。 1、2   title: (wind OR windy) title 字段包含单词 wind 或单词 windy 。 1、2、3   title: \\\u0026quot;wind rises\\\u0026quot; title 字段包含短语 wind rises 。使用反斜杠转义引号。 1   article\\\\ title: wind article title 字段包含单词 wind 。用反斜杠转义空格字符。 4   title.\\\\*: rise 以 title. 开头的每个字段（在这个例子中是 title.english ）都包含单词 rise 。用反斜杠转义通配符。 1   _exists_: description The field description exists. 字段 description 存在。 2    通配符表达式 #  您可以使用特殊字符指定通配符表达式： ? 替换单个字符， * 替换零个或多个字符。\n参考样例 #  以下查询搜索标题包含 gone 的词，并且描述包含以 hist 开头的词：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;title: gone AND description: hist*\u0026#34; } } }  通配符查询可能会使用大量内存，这可能会降低性能。词首的通配符（例如 *cal ）是最昂贵的，因为匹配这些通配符的文档需要检查索引中的所有词。要禁用首部通配符，请将 allow_leading_wildcard 设置为 false 。\n 为了效率，纯通配符如 * 会被重写为 exists 查询。因此， description: * 通配符会匹配 description 字段包含空值的文档，但不会匹配 description 字段缺失或具有 null 值的文档。\n如果你将 analyze_wildcard 设置为 true ，Easysearch 将分析以 * 结尾的查询（例如 hist* ）。因此，Easysearch 将通过在第一个 n-1 个词上精确匹配，并在最后一个词上前缀匹配，来构建一个包含结果词的布尔查询。\n正则表达式 #  要在查询字符串中指定正则表达式模式，用正斜杠 ( / ) 将其包围，例如 title: /w[a-z]nd/ 。\n allow_leading_wildcard 参数不适用于正则表达式。例如，查询字符串如 /.*d/ 将检查索引中的所有词项。\n 模糊性 #  你可以使用 ~ 操作符运行模糊查询，例如 title: rise~ 。\n该查询会搜索包含与搜索词相似（在最大允许编辑距离内）的词项的文档。编辑距离定义为 Damerau-Levenshtein 距离，它测量将一个词项转换为另一个词项所需的单字符更改次数（插入、删除、替换或转换）。\n默认的编辑距离为 2，应该能捕获 80%的拼写错误。要更改默认编辑距离，请在 ~ 操作符后指定新的编辑距离。例如，要将编辑距离设置为 1 ，请使用查询 title: rise~1 。\n 不要混合使用模糊和通配符操作符。如果你同时指定了模糊和通配符操作符，其中一个操作符将不会被应用。例如，如果你可以搜索 wnid*~1 ，通配符操作符 * 将会被应用，但模糊操作符 ~1 将不会被应用。\n 邻近查询 #  邻近查询不需要搜索短语中的词语按指定顺序排列。它允许短语中的词语以不同的顺序出现或被其他词语分隔。邻近查询指定短语中词语的最大编辑距离。例如，以下查询在匹配指定短语中的词语时允许编辑距离为 4：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;title: \\\u0026#34;wind gone\\\u0026#34;~4\u0026#34; } } } 当 Easysearch 匹配文档时，文档中的词语与查询中指定的词语顺序越接近（即编辑距离越小），该文档的相关性得分就越高。\n范围查询 #  要为数值、字符串或日期字段指定范围，请使用方括号（ [min TO max] ）表示包含范围，使用花括号（ {min TO max} ）表示排除范围。您也可以混合使用方括号和花括号来包含或排除下限和上限（例如， {min TO max] ）。\n日期范围必须使用您在映射包含日期的字段时使用的格式提供。有关支持的日期格式的更多信息，请参阅格式。\n下表提供了范围语法示例。\n   数据类型 查询内容 字符串查询     Numeric 账户号码在 1 到 15（含）范围内的文档。 account_number: [1 TO 15] 或 account_number: (\u0026gt;=1 AND \u0026lt;=15) 或 account_number: (+\u0026gt;=1 +\u0026lt;=15)    账户号码为 15 及以上的文档。 account_number: [15 TO *] 或 account_number: \u0026gt;=15 （注意 \u0026gt;= 符号后没有空格）   String 姓氏从 Bates（包含）到 Duke（不包含）的文档。 [Bates TO Duke} 或 lastname: (\u0026gt;=Bates AND \u0026lt;Duke)     文档中姓氏在字母顺序上位于 Bates 之前。  lastname: { * TO Bates} 或 lastname: \u0026lt;Bates （注意 \u0026lt; 符号后没有空格）   Date 发布日期在 2023 年 3 月 21 日至 2023 年 9 月 25 日（含）之间的文档。 release_date: [03/21/2023 TO 09/25/2023]      作为在查询字符串中指定范围的一种替代方法，您可以使用范围查询，它提供了更可靠的语法。\n 加权 #  使用尖号（^）提升运算符来通过乘数提升文档的相关性分数。[0, 1)范围内的值会降低相关性，而大于 1 的值会增加相关性。默认值为 1。\n下表提供了提升示例。\n   类型 描述 字符串查询     词频提升 查找包含词 street 的所有地址，并提升包含词 Madison 的地址的权重。 address: Madison^2 street   短语提升 查找标题包含短语 wind rises 的文档，并提升其权重至 2 倍。 title: \\\u0026quot;wind rises\\\u0026quot;^2    查找标题包含 wind rises 的文档，并将包含短语 wind rises 的文档提升 2 倍权重。 title: (wind rises)^2    布尔运算符 #  当你在查询中提供搜索词时，默认情况下，查询会返回包含至少一个所提供词的文档。你可以使用 default_operator 参数为所有词指定运算符。因此，如果你将 default_operator 设置为 AND ，所有词都将被要求；而如果你将其设置为 OR ，所有词都将被视为可选。\n+ 和 - 运算符 #  如果你需要更精细地控制必需和可选的词项，可以使用 + 和 - 操作符。 + 操作符会使它后面的词项成为必需的，而 - 操作符会排除它后面的词项。\n例如，在查询字符串中， title: (gone +wind -turbines) 指定 gone 是可选的， wind 必须存在，而 turbines 则不能存在于匹配文档的标题中：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;title: (gone +wind -turbines)\u0026#34; } } } 查询返回了两个匹配的文档：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.3159468, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gone with the wind\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A 1939 American epic historical film\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.3438858, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } } 前面的查询等同于以下布尔查询：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;wind\u0026#34; } }, \u0026#34;should\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;gone\u0026#34; } }, \u0026#34;must_not\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;turbines\u0026#34; } } } } } 传统布尔运算符 #  或者，您可以使用以下布尔运算符： AND 、\u0026amp;\u0026amp; 、 OR 、 || 、 NOT 、 ! 。但是，这些运算符不遵循优先级规则，因此在使用多个布尔运算符时，您必须使用括号来指定优先级。例如，查询字符串 title: (gone +wind -turbines) 可以使用布尔运算符重写如下：\n运行以下包含重写查询字符串的查询：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;title: ((gone AND wind) OR wind) AND NOT turbines\u0026#34; } } } 该查询返回的结果与使用 + 和 - 运算符的查询相同。但是请注意，匹配文档的相关性分数与之前的结果不同：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.6166971, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gone with the wind\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A 1939 American epic historical film\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.3438858, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } } 分组 #  使用括号将多个子句或词项组合为子查询。例如，以下查询搜索包含 gone 或 rises 的文档，这些文档必须在标题中包含 wind ：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;title: (gone OR rises) AND wind\u0026#34; } } } 结果包含两个匹配的文档：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.5046883, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.3159468, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gone with the wind\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A 1939 American epic historical film\u0026#34; } } 您也可以使用分组来提升子查询结果或针对指定字段，例如 title:(gone AND wind) description:(historical film)^2 。\n搜索多个字段 #  要搜索多个字段，请使用 fields 参数。当你提供 fields 参数时，查询会被重写为 field_1: query OR field_2: query ... 。\n例如，以下查询会在 title 和 description 字段中搜索 wind 或 film ：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;description\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;wind AND film\u0026#34; } } } 前面的查询与不提供 fields 参数的以下查询等效：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;(title:wind OR description:wind) AND (title:film OR description:film)\u0026#34; } } } 搜索字段中的多个子字段 #  要搜索某个字段的全部内部字段，可以使用通配符。例如，要搜索 address 字段内的所有子字段，可以使用以下查询：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34; : { \u0026#34;fields\u0026#34; : [\u0026#34;address.*\u0026#34;], \u0026#34;query\u0026#34; : \u0026#34;New AND (York OR Jersey)\u0026#34; } } } 前面的查询与以下不提供 fields 参数的查询等效（注意 * 使用 \\\\ 转义）：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34; : { \u0026#34;query\u0026#34; : \u0026#34;address.\\\\*: New AND (York OR Jersey)\u0026#34; } } } 子字段加权 #  每个搜索词生成的子查询使用 dis_max 查询与 tie_breaker 结合。要提升单个字段的权重，使用 ^ 操作符。例如，以下查询将 title 字段的权重提升 2 倍：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;title^2\u0026#34;, \u0026#34;description\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;wind AND film\u0026#34; } } } 要提升字段的所有子字段，在通配符后指定提升操作符：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34; : { \u0026#34;fields\u0026#34; : [\u0026#34;work_address\u0026#34;, \u0026#34;address.*^2\u0026#34;], \u0026#34;query\u0026#34; : \u0026#34;New AND (York OR Jersey)\u0026#34; } } } 多字段搜索参数 #  在搜索多个字段时，您可以将额外的可选参数 type 传递给 query_string 查询。\n   参数 数据类型 描述     type String 确定 Easysearch 如何执行查询和评分结果。有效值为 best_fields 、 bool_prefix 、 most_fields 、 cross_fields 、 phrase 和 phrase_prefix 。默认值为 best_fields 。有关有效值的说明，请参阅多值查询类型。    query_string 查询中的同义词 #  query_string 查询支持使用 synonym_graph 令牌过滤器进行多词同义词扩展。如果你使用 synonym_graph 令牌过滤器，Easysearch 会为每个同义词创建一个匹配短语查询。\nauto_generate_synonyms_phrase_query 参数指定是否为多词同义词自动创建匹配短语查询。默认情况下， auto_generate_synonyms_phrase_query 为 true ，因此如果你指定 ml, machine learning 为同义词并搜索 ml ，Easysearch 会搜索 ml OR \u0026quot;machine learning\u0026quot; 。\n或者，您可以使用连词来匹配多词同义词。如果您将 auto_generate_synonyms_phrase_query 设置为 false ，Easysearch 将搜索 ml OR (machine AND learning) 。\n例如，以下查询搜索文本 ml models 并指定不针对每个同义词自动生成匹配短语查询：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;default_field\u0026#34;: \u0026#34;title\u0026#34;, \u0026#34;query\u0026#34;: \u0026#34;ml models\u0026#34;, \u0026#34;auto_generate_synonyms_phrase_query\u0026#34;: false } } } 对于此查询，Easysearch 创建以下布尔查询： (ml OR (machine AND learning)) models 。\nMinimum should match 最小匹配 #  query_string 查询会在每个运算符周围分割查询，并为整个输入创建一个布尔查询。 minimum_should_match 参数指定文档必须匹配的最小词数才能在搜索结果中返回。例如，以下查询指定 description 字段必须至少匹配两个词才能为每个搜索结果返回：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;description\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;historical epic film\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: 2 } } } 对于此查询，Easysearch 创建以下布尔查询： (description:historical description:epic description:film)~2 。\n最小值应与多个字段匹配 #  如果你在 query_string 查询中指定了多个字段，Easysearch 会为指定的字段创建 dis_max 查询。如果你没有显式指定查询项的操作符，整个查询文本被视为一个子句。Easysearch 使用这个单一子句为每个字段构建查询。最终的布尔查询包含一个对应于所有字段 dis_max 查询的单一子句，因此 minimum_should_match 参数不适用。\n例如，在以下查询中， historical epic heroic 被视为一个子句：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;description\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;historical epic heroic\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: 2 } } } 对于此查询，Easysearch 创建以下布尔查询： ((title:historical title:epic title:heroic) | (description:historical description:epic description:heroic)) 。\n如果你在查询词中添加显式操作符（ AND 或 OR ），每个词被视为一个独立的子句，可以应用 minimum_should_match 参数。例如，在以下查询中， historical 、 epic 和 heroic 被视为独立的子句：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;description\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;historical OR epic OR heroic\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: 2 } } } 对于此查询，Easysearch 创建以下布尔查询： ((title:historical | description:historical) (description:epic | title:epic) (description:heroic | title:heroic))~2 。该查询匹配至少三个子句中的两个。每个子句都表示在每个词项的 title 和 description 字段上的 dis_max 查询。\n或者，为了确保可以应用 minimum_should_match ，你可以将 type 参数设置为 cross_fields 。这表示在分析输入文本时，具有相同分词器的字段应该被组合在一起：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;description\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;historical epic heroic\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cross_fields\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: 2 } } } 对于此查询，Easysearch 创建以下布尔查询： ((title:historical | description:historical) (description:epic | title:epic) (description:heroic | title:heroic))~2 。\n然而，如果你使用不同的分词器，必须在查询中使用显式操作符，以确保 minimum_should_match 参数应用于每个词。\n注意 #   查询字符串查询可能会在内部转换为前缀查询。如果 search.allow_expensive_queries 设置为 false ，则不会执行前缀查询。如果 index_prefixes 被启用，则忽略 search.allow_expensive_queries 设置，并构建并执行优化后的查询。\n ","subcategory":null,"summary":"","tags":null,"title":"Query String 查询","url":"/easysearch/main/docs/features/fulltext-search/full-text/query-string/"},{"category":null,"content":"Prefix 查询 #  使用 prefix 查询可以搜索以特定前缀开头的词。例如，以下查询会搜索 speaker 字段包含以 KING H 开头的词的文档。\n相关指南（先读这些） #    部分匹配  结构化搜索  GET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34;: { \u0026#34;speaker\u0026#34;: \u0026#34;KING H\u0026#34; } } } 为了提供参数，您可以使用与前面的查询相同的形式，并使用以下扩展语法\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34;: { \u0026#34;speaker\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;KING H\u0026#34; } } } } 参数说明 #\n 查询接受字段名称（ \u0026lt;field\u0026gt; ）作为顶级参数：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;sample\u0026#34;, ... } } } } \u0026lt;field\u0026gt; 接受以下参数。除了 value 之外，所有参数都是可选的。\n   参数 数据类型 描述     value String 在由  指定的字段中搜索的词项。   boost Float 一个浮点值，用于指定该字段对相关性评分的权重。值大于 1.0 会增加字段的相关性。值在 0.0 到 1.0 之间会降低字段的相关性。默认值为 1.0。   case_insensitive Boolean 如果设置为 true ，则允许对字段索引值进行不区分大小写的匹配。默认值为 false （大小写敏感性由字段的映射决定）。   rewrite String 决定 Easysearch 如何重写和评分多词查询。有效值为 constant_score 、 scoring_boolean 、 constant_score_boolean 、 top_terms_N 、 top_terms_boost_N 和 top_terms_blended_freqs_N 。默认值为 constant_score 。     如果将 search.allow_expensive_queries 设置为 false ，则不会执行前缀查询。如果启用了 index_prefixes ，则会忽略 search.allow_expensive_queries 设置，并构建并运行一个优化后的查询。\n 工作原理 #  prefix 查询是一个 词级别（term-level） 的底层查询，它不会在搜索之前分析查询字符串——传入的前缀就是要查找的前缀。\n其执行过程如下：\n 扫描倒排索引的有序词列表，找到第一个以指定前缀开头的词 收集该词关联的文档 ID 移动到下一个词，如果也以该前缀开头则重复第 2 步 持续直到遇到不以该前缀开头的词  例如，倒排索引如下：\nTerm: Doc IDs: ------------------------- \u0026#34;SW5 0BE\u0026#34; | 5 \u0026#34;W1F 7HW\u0026#34; | 3 \u0026#34;W1V 3DG\u0026#34; | 1 \u0026#34;W2F 8HW\u0026#34; | 2 \u0026#34;WC1N 1LZ\u0026#34; | 4 ------------------------- 搜索前缀 W1 时，Easysearch 会依次匹配到 W1F 7HW 和 W1V 3DG，返回文档 3 和 1。\n性能与优化 #   注意：前缀越短，需要访问的词项越多。以 W 作为前缀可能需要扫描数百万个词项，对集群产生很大压力。\n 使用 index_prefixes 优化 #  在映射中对字段启用 index_prefixes，可以让 Easysearch 在索引阶段预先生成前缀的子字段，从而大幅提升前缀查询的性能：\nPUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;product_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index_prefixes\u0026#34;: { \u0026#34;min_chars\u0026#34;: 2, \u0026#34;max_chars\u0026#34;: 5 } } } } } 在分析过的字段上使用时 #  prefix 查询作用于倒排索引中的 单个词项，而不是原始字段值。如果对一个 text 字段使用 prefix 查询，它会匹配分词后的每个词项的前缀：\n 字段值 \u0026quot;Quick brown fox\u0026quot; → 词项 quick、brown、fox prefix: \u0026quot;bro\u0026quot; → ✅ 匹配（匹配词项 brown） prefix: \u0026quot;Qui\u0026quot; → ❌ 不匹配（索引中的词项是小写的 quick） prefix: \u0026quot;quick bro\u0026quot; → ❌ 不匹配（前缀不会跨词项匹配）  如果需要对精确值做前缀匹配，应使用 keyword 类型的字段。\n","subcategory":null,"summary":"","tags":null,"title":"Prefix 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/prefix/"},{"category":null,"content":"Percolate 查询 #  使用 percolate 查询来查找与给定文档匹配的已存储查询。此操作与常规搜索相反：常规搜索是查找与查询匹配的文档，而 percolate 查询是查找与文档匹配的查询。percolate 查询通常用于告警、通知和反向搜索用例。\n相关指南（先读这些） #    Query DSL 基础  专业查询（Specialized queries）  在使用 percolate 查询时，请考虑以下几点：\n 您可以在线提供文档进行 percolate 操作，或者从索引中获取现有文档。 文档和存储的查询必须使用相同的字段名称和类型。 您可以结合使用透查、过滤和评分来构建复杂的匹配系统。 percolate 查询被视为昂贵的查询，并且只有在集群设置 search.allow_expensive_queries 被设置为 true （默认值）时才会运行。如果此设置是 false ， percolate 查询将被拒绝。  percolate 查询在各种实时匹配场景中非常有用。一些常见的用例包括：\n 电子商务通知：用户可以注册对产品的兴趣，例如，“有新的苹果笔记本电脑时通知我”。当新产品文档被索引时，系统会找到所有匹配保存的查询的用户并发送警报。 工作警报：求职者根据首选的工作标题或地点保存查询，新的职位发布将与这些查询匹配以触发警报。 安全和警报系统：Percolate 传入的日志或事件数据与保存的规则或异常模式进行匹配。 新闻筛选：将传入的文章与保存的主题配置文件进行匹配，以分类或提供相关内容。  工作过程 #   保存的查询存储在一个特殊的 percolator 字段类型中。 文档会与所有保存的查询进行比较。 每个匹配的查询都会返回其 _id 。 如果启用了高亮显示，匹配的文本片段也会被返回。 如果发送了多个文档， _percolator_document_slot 会显示匹配的文档。  参考样例 #  以下示例展示了如何存储 percolate 查询，并使用不同方法对它们进行测试。\n首先，创建一个索引并配置其 mappings 字段类型以存储保存的查询：\nPUT /my_percolator_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;percolator\u0026#34; }, \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } 在 title 字段中添加一个匹配“apple”的查询：\nPOST /my_percolator_index/_doc/1 { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;apple\u0026#34; } } } 在 title 字段中添加一个匹配“banana”的查询：\nPOST /my_percolator_index/_doc/2 { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;banana\u0026#34; } } } 对内联文档进行渗透 #  测试内联文档与保存的查询进行比对：\nPOST /my_percolator_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;percolate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;document\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Fresh Apple Harvest\u0026#34; } } } } 返回结果提供了存储的 percolate 查询，该查询用于搜索包含“apple”一词的 title 字段，由 _id 标识： 1 ：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.13076457, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.13076457, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;apple\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot\u0026#34;: [ 0 ] } } ] } } 使用多个文档进行渗透 #  要在同一查询中测试多个文档，请使用以下请求：\nPOST /my_percolator_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;percolate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;documents\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Banana flavoured ice-cream\u0026#34; }, { \u0026#34;title\u0026#34;: \u0026#34;Apple pie recipe\u0026#34; }, { \u0026#34;title\u0026#34;: \u0026#34;Banana bread instructions\u0026#34; }, { \u0026#34;title\u0026#34;: \u0026#34;Cherry tart\u0026#34; } ] } } } _percolator_document_slot 字段帮助您通过索引识别与每个保存的查询匹配的每个文档：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.54726034, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.54726034, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;apple\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot\u0026#34;: [ 1 ] } }, { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.31506687, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;banana\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot\u0026#34;: [ 0, 2 ] } } ] } } 为现有索引文档进行渗透 #\n 您可以引用已存储在另一个索引中的现有文档，以检查匹配 percolate 查询。\n为您的文档创建一个单独的索引：\nPUT /products { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } 添加文档：\nPOST /products/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;Banana Smoothie Special\u0026#34; } 检查存储的查询是否与索引文档匹配：\nPOST /my_percolator_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;percolate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34; } } }  使用存储文档时，你必须同时提供 index 和 id 。\n 返回相应的查询：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.13076457, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.13076457, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;banana\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot\u0026#34;: [ 0 ] } } ] } } 批量渗透（多个文档） #\n 您可以在一个请求中检查多个文档：\nPOST /my_percolator_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;percolate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;documents\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Apple event coming soon\u0026#34; }, { \u0026#34;title\u0026#34;: \u0026#34;Banana farms expand\u0026#34; }, { \u0026#34;title\u0026#34;: \u0026#34;Cherry season starts\u0026#34; } ] } } } 每个匹配项都表示匹配的文档，位于 _percolator_document_slot 字段中：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.46484798, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.46484798, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;banana\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot\u0026#34;: [ 1 ] } }, { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.41211313, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;apple\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot\u0026#34;: [ 0 ] } } ] } } 使用命名查询进行多查询透传 #\n 您可以在命名查询中透传不同的文档：\nGET /my_percolator_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;percolate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;document\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Apple pie recipe\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;apple_doc\u0026#34; } }, { \u0026#34;percolate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;document\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Banana bread instructions\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;banana_doc\u0026#34; } } ] } } } 参数 name 被附加到 _percolator_document_slot 以提供匹配的查询：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.13076457, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.13076457, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;apple\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot_apple_doc\u0026#34;: [ 0 ] } }, { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.13076457, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;banana\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot_banana_doc\u0026#34;: [ 0 ] } } ] } } 这种方法使您能够为单个文档配置更自定义的查询逻辑。在以下示例中，第一个文档查询 title 字段，第二个文档查询 description 字段。还提供了 boost 参数：\nGET /my_percolator_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;constant_score\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;percolate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;document\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Apple pie recipe\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;apple_doc\u0026#34; } }, \u0026#34;boost\u0026#34;: 1.0 } }, { \u0026#34;constant_score\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;percolate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;document\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Banana bread with honey\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;banana_doc\u0026#34; } }, \u0026#34;boost\u0026#34;: 3.0 } } ] } } } 批量渗透与命名渗透比较 #  批量渗透（使用 documents ）和命名渗透（使用 bool 与 name ）均可用于渗透多个文档，但它们在结果标签、解释和控制方式上有所不同。它们提供功能相似的结果，但在结构上有重要差异，如下表所述。\n   特性 批量（ documents ） 命名（ bool + percolate + name ）     输入格式 每个 percolate 子句，文档数组 多个 percolate 子句，每个文档一个   每个文档的可追溯性 通过槽位索引（0，1，…） 通过名称（ apple_doc ， banana_doc ）   匹配槽位的响应字段 _percolator_document_slot: [0] _percolator_document_slot_\u0026lt;name\u0026gt;: [0]   高亮前缀 0_title, 1_title apple_doc_title, banana_doc_title   每个文档自定义控制 不支持 可自定义每个子句   支持提升和过滤器 不 是（每条子句）   性能 适用于大批量 子句较多时稍慢   用例 批量匹配任务，大型事件流 单文档跟踪、测试、自定义控制    高亮匹配 #  percolate 查询与常规查询处理高亮的方式不同：\n 在常规查询中，文档存储在索引中，搜索查询用于高亮匹配的词项。 在 percolate 查询中，角色被反转：保存的查询（在 percolator 索引中）用于高亮文档。  这意味着在 document 或 documents 中提供的文档是高亮的靶标，而 percolate 查询决定了要高亮的区域。\n突出显示单个文档 #  此示例使用 my_percolator_index 中先前定义的搜索。使用以下请求突出显示 title 字段中的匹配项：\nPOST /my_percolator_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;percolate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;document\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Apple banana smoothie\u0026#34; } } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;title\u0026#34;: {} } } } 匹配项根据匹配的查询突出显示：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.13076457, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.13076457, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;apple\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot\u0026#34;: [ 0 ] }, \u0026#34;highlight\u0026#34;: { \u0026#34;title\u0026#34;: [ \u0026#34;\u0026lt;em\u0026gt;Apple\u0026lt;/em\u0026gt; banana smoothie\u0026#34; ] } }, { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.13076457, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;banana\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot\u0026#34;: [ 0 ] }, \u0026#34;highlight\u0026#34;: { \u0026#34;title\u0026#34;: [ \u0026#34;Apple \u0026lt;em\u0026gt;banana\u0026lt;/em\u0026gt; smoothie\u0026#34; ] } } ] } } 突出显示多个文档 #\n 使用 documents 数组对多个文档进行透化时，每个文档会被分配一个槽索引。高亮键的格式如下，其中 \u0026lt;slot\u0026gt; 是你的 documents 数组中文档的索引：\n\u0026#34;\u0026lt;slot\u0026gt;_\u0026lt;fieldname\u0026gt;\u0026#34;: [ ... ] 使用以下命令对两个文档进行透化并高亮显示：\nPOST /my_percolator_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;percolate\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;documents\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Apple pie recipe\u0026#34; }, { \u0026#34;title\u0026#34;: \u0026#34;Banana smoothie ideas\u0026#34; } ] } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;title\u0026#34;: {} } } } 结果包含以文档槽为前缀的高亮字段，例如 0_title 和 1_title ：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.31506687, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.31506687, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;apple\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot\u0026#34;: [ 0 ] }, \u0026#34;highlight\u0026#34;: { \u0026#34;0_title\u0026#34;: [ \u0026#34;\u0026lt;em\u0026gt;Apple\u0026lt;/em\u0026gt; pie recipe\u0026#34; ] } }, { \u0026#34;_index\u0026#34;: \u0026#34;my_percolator_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.31506687, \u0026#34;_source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;banana\u0026#34; } } }, \u0026#34;fields\u0026#34;: { \u0026#34;_percolator_document_slot\u0026#34;: [ 1 ] }, \u0026#34;highlight\u0026#34;: { \u0026#34;1_title\u0026#34;: [ \u0026#34;\u0026lt;em\u0026gt;Banana\u0026lt;/em\u0026gt; smoothie ideas\u0026#34; ] } } ] } } 参数说明 #\n percolate 查询支持以下参数。\n   参数 必需/可选 描述     field 必需 包含存储的 percolate 查询的字段。   document 可选 用于匹配已保存查询的单个内联文档。   documents 可选 用于匹配已保存查询的多个内联文档数组。   index 可选 包含要匹配的文档的索引。   id 可选 从索引中获取文档的 ID。   routing 可选 获取文档时使用的路由值。   preference 可选 获取文档时对分片路由的偏好设置。   name 可选 为 percolate 子句指定的名称。在 bool 查询中使用多个 percolate 子句时很有帮助。    ","subcategory":null,"summary":"","tags":null,"title":"Percolate 查询","url":"/easysearch/main/docs/features/query-dsl/specialized/percolate/"},{"category":null,"content":"Parent ID 查询 #  parent_id 查询返回具有指定 ID 的父文档的子文档。您可以通过使用连接字段类型在相同索引中的文档之间建立父子关系。\n相关指南（先读这些） #    Parent-Child 建模  关联查询（Joining）  参考样例 #  在您运行一个 parent_id 查询之前，您的索引必须包含一个连接字段，以便建立父子关系。索引映射请求使用以下格式：\nPUT /example_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;relationship_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;relations\u0026#34;: { \u0026#34;parent_doc\u0026#34;: \u0026#34;child_doc\u0026#34; } } } } } 对于此示例，首先配置一个包含代表产品和其品牌的文档的索引，这些文档在 has_child 查询示例中有所描述。\n要搜索特定父文档的子文档，请使用 parent_id 查询。以下查询返回具有 ID 1 的父文档的子文档（产品）：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;parent_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34; } } } 返回子产品：\n{ \u0026#34;took\u0026#34;: 57, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.87546873, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 0.87546873, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Mechanical watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 150, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } } ] } } 参数说明 #  以下表格列出了所有由 parent_id 查询支持的一级参数。\n   参数 必填/可选 描述     type 必填 指定在 join 字段映射中定义的子关系名称。   id 必填 父文档的 ID。查询返回与该父文档关联的子文档。   ignore_unmapped 可选 指示是否忽略未映射的 type 字段，而不是抛出错误而不返回文档。您可以在查询多个索引时提供此参数，其中一些索引可能不包含 type 字段。默认为 false 。    ","subcategory":null,"summary":"","tags":null,"title":"Parent ID 查询","url":"/easysearch/main/docs/features/query-dsl/joining/parent-id/"},{"category":null,"content":"Nested 查询 #  nested 查询充当其他查询的包装器，用于搜索嵌套字段。嵌套字段对象被视为单独的文档进行搜索。如果对象匹配搜索条件，nested 查询将返回根级别的父文档。\n相关指南（先读这些） #    Nested 建模  关联查询（Joining）  参考样例 #  在运行 nested 查询之前，您的索引必须包含一个嵌套字段。\n要配置一个包含嵌套字段的示例索引，请发送以下请求：\nPUT /testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;age\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } } } 接下来，将一个文档索引到示例索引中：\nPUT /testindex/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 56 } } 要搜索嵌套的 patient 字段，请将您的查询包裹在 nested 查询中，并提供 path 给嵌套字段：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;patient\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;patient.name\u0026#34;: \u0026#34;John\u0026#34; } } } } } 查询返回匹配的文档：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.2876821, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.2876821, \u0026#34;_source\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 56 } } } ] } } 检索内部命中项 #\n 要返回与查询匹配的内部命中，请提供 inner_hits 参数：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;patient\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;patient.name\u0026#34;: \u0026#34;John\u0026#34; } }, \u0026#34;inner_hits\u0026#34;: {} } } } 结果包含额外的 inner_hits 字段。 _nested 字段标识了内部命中来源的具体内部对象。它包含嵌套命中及其相对于 _source 中位置的偏移量。由于排序和评分， inner_hits 中命中对象的位置通常与其在嵌套对象中的原始位置不同。\n默认情况下，返回的击中对象中的 _source 相对于 inner_hits 字段。在这个例子中， inner_hits 中的 _source 包含 name 和 age 字段，而不是顶层的 _source ，后者包含整个 patient 对象：\n{ \u0026#34;took\u0026#34;: 38, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.2876821, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.2876821, \u0026#34;_source\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 56 } }, \u0026#34;inner_hits\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.2876821, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_nested\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;patient\u0026#34;, \u0026#34;offset\u0026#34;: 0 }, \u0026#34;_score\u0026#34;: 0.2876821, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 56 } } ] } } } } ] } } 多级嵌套查询 #\n 您可以使用多级嵌套查询来搜索包含在其他嵌套对象内部的嵌套对象。在这个例子中，您将通过为层次结构的每一级指定嵌套查询来查询多个层级的嵌套字段。\n首先，创建一个具有多级嵌套字段的索引：\nPUT /patients { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;contacts\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;relationship\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;phone\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } } } 接下来，将一个文档索引到示例索引中：\nPUT /patients/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;contacts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Jane Doe\u0026#34;, \u0026#34;relationship\u0026#34;: \u0026#34;mother\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;5551111\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Joe Doe\u0026#34;, \u0026#34;relationship\u0026#34;: \u0026#34;father\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;5552222\u0026#34; } ] } } 要搜索嵌套的 patient 字段，请使用多级 nested 查询。以下查询搜索联系信息中包含名为 Jane 且关系为 mother 的人的病人：\nGET /patients/_search { \u0026#34;query\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;patient\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;patient.contacts\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;patient.contacts.relationship\u0026#34;: \u0026#34;mother\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;patient.contacts.name\u0026#34;: \u0026#34;Jane\u0026#34; } } ] } } } } } } } 查询返回具有匹配这些详细信息的联系记录的患者：\n{ \u0026#34;took\u0026#34;: 14, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.3862942, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;patients\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.3862942, \u0026#34;_source\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;contacts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Jane Doe\u0026#34;, \u0026#34;relationship\u0026#34;: \u0026#34;mother\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;5551111\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Joe Doe\u0026#34;, \u0026#34;relationship\u0026#34;: \u0026#34;father\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;5552222\u0026#34; } ] } } } ] } } 参数说明 #\n 以下表格列出了所有由 nested 查询支持的一级参数。\n   参数 必填/可选 描述     path 必填 指定要搜索的嵌套对象的路径。   query 必填 在指定的 path 中运行的查询。如果一个嵌套对象与查询匹配，则返回根父文档。您可以使用点符号搜索嵌套字段，例如 nested_object.subfield 。支持多级嵌套，并自动检测。因此，另一个嵌套查询内的内部 nested 查询会自动匹配正确的嵌套级别，而不是根级别。   ignore_unmapped\t 可选 指示是否忽略未映射的 path 字段，而不是抛出错误而不返回文档。您可以在查询多个索引时提供此参数，其中一些索引可能不包含 path 字段。默认为 false 。   score_mode 可选 定义匹配的内部文档的分数如何影响父文档的分数。有效值有：\n- avg : 使用所有匹配的内部文档的平均相关性分数。\n- max : 将匹配的内部文档中的最高相关性分数分配给父文档。\n- min : 将匹配的内部文档中的最低相关性分数分配给父文档。\n- sum : 将所有匹配的内部文档的相关性得分相加。\n- none : 忽略内部文档的相关性得分，并将得分 0 分配给父文档。\n默认为 avg 。   inner_hits 可选 如果提供，则返回与查询匹配的底层命中。    ","subcategory":null,"summary":"","tags":null,"title":"Nested 查询","url":"/easysearch/main/docs/features/query-dsl/joining/nested/"},{"category":null,"content":"Multi Match 查询 #  multi_match 查询与 match 查询类似。您可以使用 multi_match 查询来搜索多个字段。\n相关指南（先读这些） #    多字段搜索  全文搜索  字段权重 #  ^ 会\u0026quot;提升\u0026quot;某些字段的权重。提升是乘数，用于使一个字段中的匹配比其他字段中的匹配更重要。在以下示例中，title字段中匹配 \u0026ldquo;wind\u0026rdquo; 的权重比 plot 字段中匹配的权重高 _score 四倍：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wind\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title^4\u0026#34;, \u0026#34;plot\u0026#34;] } } } 结果是，像《The Wind Rises》和《Gone with the Wind》这样的电影出现在搜索结果的顶部附近，而像《Twister》这样的电影，其剧情简介中可能包含“wind”字，则出现在底部附近。\n您可以在字段名中使用通配符。例如，以下查询将搜索 speaker 字段以及所有以 play_ 开头的字段，例如 play_name 或 play_title ：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;hamlet\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;speaker\u0026#34;, \u0026#34;play_*\u0026#34;] } } } 如果您不提供 fields 参数，multi_match 查询将搜索 index.query.default_field 设置中指定的字段，该设置默认为 *。默认行为是提取映射中所有适用于词级查询的字段，过滤元数据字段，并将所有提取的字段组合起来构建查询。\n 查询中的子句最大数量由 indices.query.bool.max_clause_count 设置定义，默认为 1,024。\n 多字段查询类型 #  Easysearch 支持以下多字段查询类型，它们在内部执行查询的方式上有所不同：\n best_fields （默认）：返回匹配任何字段的文档。使用最佳匹配字段的 _score 。 most_fields ：返回匹配任何字段的文档。使用每个匹配字段的组合得分。 cross_fields : 将所有字段视为一个字段。处理具有相同 analyzer 的字段，并在任何字段中匹配词语。 phrase : 在每个字段上运行 match_phrase 查询。使用最佳匹配字段的 _score 。 phrase_prefix : 在每个字段上运行 match_phrase_prefix 查询。使用最佳匹配字段的 _score 。 bool_prefix : 在每个字段上运行 match_bool_prefix 查询。使用每个匹配字段的组合得分。  Best fields 最佳字段 #  如果你在搜索两个指定概念的字词，你希望包含这两个字词相邻的结果得分更高。\n例如，创建一个包含以下科学文章的索引：\nPUT /articles/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;Aurora borealis\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Northern lights, or aurora borealis, explained\u0026#34; } PUT /articles/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Sun deprivation in the Northern countries\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Using fluorescent lights for therapy\u0026quot; } 你可以搜索标题或描述中包含 northern lights 的文章：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;northern lights\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;best_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;description\u0026#34; ], \u0026#34;tie_breaker\u0026#34;: 0.3 } } } 前面的查询被执行为以下 dis_max 查询，每个字段都有一个 match 查询：\nGET /articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;dis_max\u0026#34;: { \u0026#34;queries\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;northern lights\u0026#34; }}, { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;northern lights\u0026#34; }} ], \u0026#34;tie_breaker\u0026#34;: 0.3 } } } 结果包含这两个文档，但文档 1 的得分更高，因为这两个词都在 description 字段中：\n{ \u0026#34;took\u0026#34;: 30, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.84407747, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;articles\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.84407747, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Aurora borealis\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Northern lights, or aurora borealis, explained\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;articles\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.6322521, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Sun deprivation in the Northern countries\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Using fluorescent lights for therapy\u0026#34; } } ] } } best_fields 查询使用最佳匹配字段的分数。如果你指定了 tie_breaker ，分数将使用以下算法计算：\n取最佳匹配字段的得分，并为所有其他匹配字段加上（ tie_breaker * _score ）。\nMost fields 大多数字段 #  使用 most_fields 查询针对包含相同文本但分析方式不同的多个字段。例如，原始字段可能包含使用 standard 分词器分析的文本，而另一个字段可能包含使用执行词干提取的 english 分词器分析的相同文本：\nPUT /articles { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;english\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;english\u0026#34; } } } } } } 考虑以下两个索引在 articles 中的文档：\nPUT /articles/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;Buttered toasts\u0026#34; } PUT /articles/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Buttering a toast\u0026quot; }\nstandard 分词器将标题 Buttered toast 分析为 [ buttered , toasts ]，并将标题 Buttering a toast 分析为 [ buttering , a , toast ]。另一方面，由于词干提取， english 分词器对两个标题都产生相同的词列表 [ butter , toast ]。\n您可以使用 most_fields 查询以返回尽可能多的文档：\nGET /articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;buttered toast\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;title.english\u0026#34; ], \u0026#34;type\u0026#34;: \u0026#34;most_fields\u0026#34; } } } 前面的查询作为以下布尔查询执行：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;buttered toasts\u0026#34; }}, { \u0026#34;match\u0026#34;: { \u0026#34;title.english\u0026#34;: \u0026#34;buttered toasts\u0026#34; }} ] } } } 要计算相关性分数，文档中所有 match 子句的分数会被加在一起，然后结果再除以 match 子句的数量。\n包含 title.english 字段会检索到与词干化标记匹配的第二份文档：\n{ \u0026#34;took\u0026#34;: 9, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.4418206, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;articles\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.4418206, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Buttered toasts\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;articles\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.09304003, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Buttering a toast\u0026#34; } } ] } } 由于第一个文档的 title 和 title.english 字段都匹配，因此它的相关性得分更高。\n操作符 Operator 和 最小值应匹配 minimum_should_match #  best_fields 和 most_fields 查询基于字段生成匹配查询（每个字段一个）。因此， minimum_should_match 和 operator 参数应用于每个字段，这通常不是期望的行为。\n例如，创建一个 customers 索引，其中包含以下文档：\nPUT customers/_doc/1 { \u0026#34;first_name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Doe\u0026#34; } PUT customers/_doc/2 { \u0026quot;first_name\u0026quot;: \u0026quot;Jane\u0026quot;, \u0026quot;last_name\u0026quot;: \u0026quot;Doe\u0026quot; } 如果你在 customers 索引中搜索 John Doe ，你可能会构建以下查询：\nGET customers/_validate/query?explain { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;best_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name\u0026#34;, \u0026#34;last_name\u0026#34; ], \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34; } } } and 运算符在此查询中的意图是找到匹配 John 和 Doe 的文档。然而，查询没有返回任何结果。您可以通过运行验证 API 来学习查询是如何执行的：\nGET customers/_validate/query?explain { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;best_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name\u0026#34;, \u0026#34;last_name\u0026#34; ], \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34; } } } 从返回内容中可以看出，查询试图将 John 和 Doe 匹配到 first_name 或 last_name 字段：\n{ \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;valid\u0026#34;: true, \u0026#34;explanations\u0026#34;: [ { \u0026#34;index\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;valid\u0026#34;: true, \u0026#34;explanation\u0026#34;: \u0026#34;((+first_name:john +first_name:doe) | (+last_name:john +last_name:doe))\u0026#34; } ] } 因为两个字段都不包含这两个词，所以没有返回结果。\n跨字段搜索的更好替代方案是使用 cross_fields 查询。与以字段为中心的 best_fields 和 most_fields 查询不同， cross_fields 查询是以词为中心的。\n跨字段 #  使用 cross_fields 查询来跨多个字段搜索数据。例如，如果索引包含客户数据，客户的名和姓位于不同的字段中。但是，当你搜索 John Doe 时，你希望获得文档，其中 John 在 first_name 字段中， Doe 在 last_name 字段中。\nmost_fields 查询在这种情况下无法工作，因为存在以下问题：\n operator 和 minimum_should_match 参数是按字段应用，而不是按词应用。 first_name 和 last_name 字段的词频可能导致意外结果。例如，如果某人的first_name恰好是 Doe ，包含这个名字的文档会被认为是一个更好的匹配，因为这个名字不会出现在其他任何文档中。  cross_fields 查询会将查询字符串分析为单独的词，然后在任何字段中搜索每个词，就像它们是一个字段一样。\n以下是 cross_fields 查询 John Doe 的内容：\nGET /customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cross_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name\u0026#34;, \u0026#34;last_name\u0026#34; ], \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34; } } } 返回包含唯一一份文档，其中同时包含 John 和 Doe ：\n{ \u0026#34;took\u0026#34;: 19, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.8754687, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.8754687, \u0026#34;_source\u0026#34;: { \u0026#34;first_name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Doe\u0026#34; } } ] } } 您可以使用验证 API 操作来了解前面的查询是如何执行的：\nGET /customers/_validate/query?explain { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cross_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name\u0026#34;, \u0026#34;last_name\u0026#34; ], \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34; } } } 从返回内容中可以看出，查询是在至少一个字段中搜索所有词项：\n{ \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;valid\u0026#34;: true, \u0026#34;explanations\u0026#34;: [ { \u0026#34;index\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;valid\u0026#34;: true, \u0026#34;explanation\u0026#34;: \u0026#34;+blended(terms:[last_name:john, first_name:john]) +blended(terms:[last_name:doe, first_name:doe])\u0026#34; } ] } 因此，通过混合所有字段的词频，解决了因词频差异而造成的问题，从而纠正了这些差异。\n cross_fields 查询通常只在各字段的 boost 值为 1 的情况下效果最佳。在其他情况下，由于提升值、词频和长度归一化对分数的贡献方式，混合的词统计信息可能无法产生有意义的得分。\n  fuzziness 参数不适用于 cross_fields 查询。\n 分词器的作用 #  cross_fields 查询仅能在具有相同分词器的字段上作为基于词条的查询工作。具有相同分词器的字段会被分组，这些分组会通过布尔查询组合在一起。\n例如，考虑一个索引，其中 first_name 和 last_name 字段使用默认的 standard 分词器进行分析，而它们的 .edge 子字段使用边缘 n 元分词器进行分析：\nPUT customers { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;my_tokenizer\u0026#34; } }, \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;edge_ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 2, \u0026#34;max_gram\u0026#34;: 10 } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;first_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;edge\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34; } } }, \u0026#34;last_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;edge\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34; } } } } } } 你在 customers 索引中索引了一个文档：\nPUT /customers/_doc/1 { \u0026#34;first_name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Doe\u0026#34; } 你可以使用 cross_fields 查询来跨字段搜索 John Doe :\nGET /customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cross_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name\u0026#34;, \u0026#34;first_name.edge\u0026#34;, \u0026#34;last_name\u0026#34;, \u0026#34;last_name.edge\u0026#34; ] } } } 要查看查询的执行情况，你可以运行验证 API：\nGET /customers/_validate/query?explain { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cross_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name\u0026#34;, \u0026#34;first_name.edge\u0026#34;, \u0026#34;last_name\u0026#34;, \u0026#34;last_name.edge\u0026#34; ] } } } 返回显示 last_name 和 first_name 字段被组合在一起并视为一个字段。类似地， last_name.edge 和 first_name.edge 字段被组合在一起并视为一个字段：\n{ \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;valid\u0026#34;: true, \u0026#34;explanations\u0026#34;: [ { \u0026#34;index\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;valid\u0026#34;: true, \u0026#34;explanation\u0026#34;: \u0026#34;(blended(terms:[last_name:john, first_name:john]) | (blended(terms:[last_name.edge:Jo, first_name.edge:Jo]) blended(terms:[last_name.edge:Joh, first_name.edge:Joh]) blended(terms:[last_name.edge:John, first_name.edge:John])))\u0026#34; } ] } 使用 operator 或 minimum_should_match 参数与多个字段组（如前文所述）可能会导致前一节中描述的问题。为避免这种情况，你可以将之前的查询重写为两个 cross_fields 子查询，通过布尔查询将它们组合起来，并将 minimum_should_match 应用于其中一个子查询：\nGET /customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cross_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name\u0026#34;, \u0026#34;last_name\u0026#34; ], \u0026#34;minimum_should_match\u0026#34;: \u0026#34;1\u0026#34; } }, { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cross_fields\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name.edge\u0026#34;, \u0026#34;last_name.edge\u0026#34; ] } } ] } } } 为所有字段创建一个分组，请在查询中指定一个分词器：\nGET customers/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cross_fields\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;first_name\u0026#34;, \u0026#34;last_name\u0026#34;, \u0026#34;*.edge\u0026#34; ] } } } 在先前查询上运行 Validate API 显示了查询是如何执行的：\n{ \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;valid\u0026#34;: true, \u0026#34;explanations\u0026#34;: [ { \u0026#34;index\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;valid\u0026#34;: true, \u0026#34;explanation\u0026#34;: \u0026#34;blended(terms:[last_name.edge:john, last_name:john, first_name:john, first_name.edge:john]) blended(terms:[last_name.edge:doe, last_name:doe, first_name:doe, first_name.edge:doe])\u0026#34; } ] } 短语 #  phrase 查询的行为与 best_fields 查询类似，但使用 match_phrase 查询而不是 match 查询。\n以下是在 best_fields 部分描述的索引中的 phrase 查询示例：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;northern lights\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;phrase\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;description\u0026#34; ] } } } 前面的查询被执行为以下 dis_max 查询，每个字段都有一个 match_phrase 查询：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;dis_max\u0026#34;: { \u0026#34;queries\u0026#34;: [ { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;northern lights\u0026#34; }}, { \u0026#34;match_phrase\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;northern lights\u0026#34; }} ] } } } 因为默认情况下， phrase 查询仅当词项按相同顺序出现时才匹配文本，结果中仅返回文档 1\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.84407747, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;articles\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.84407747, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Aurora borealis\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Northern lights, or aurora borealis, explained\u0026#34; } } ] } } 您可以使用 slop 参数来允许查询短语中的词之间包含其他词。例如，以下查询在 flourescent 和 therapy 之间最多包含两个词时接受文本匹配：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;fluorescent therapy\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;phrase\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;description\u0026#34; ], \u0026#34;slop\u0026#34;: 2 } } } 返回包含文档 2\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.7003825, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;articles\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.7003825, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Sun deprivation in the Northern countries\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Using fluorescent lights for therapy\u0026#34; } } ] } } 对于 slop 值小于 2 的情况，不会返回任何文档。\n fuzziness 参数不适用于 phrase 查询。\n 短语前缀 #  phrase_prefix 查询的行为与 phrase 查询类似，但使用 match_phrase_prefix 查询而不是 match_phrase 查询。\n以下是在 best_fields 部分描述的索引中的 phrase_prefix 查询示例：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;northern light\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;phrase_prefix\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;description\u0026#34; ] } } } 前面的查询被执行为以下 dis_max 查询，每个字段都有一个 match_phrase_prefix 查询：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;dis_max\u0026#34;: { \u0026#34;queries\u0026#34;: [ { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;northern light\u0026#34; }}, { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;northern light\u0026#34; }} ] } } } 您可以使用 slop 参数来允许在查询短语中的单词之间插入其他单词。\n fuzziness 参数不适用于 phrase_prefix 查询。\n 布尔前缀 #  bool_prefix 查询与 most_fields 查询类似地评分文档，但使用 match_bool_prefix 查询而不是 match 查询。\n以下是在 best_fields 部分描述的索引中的 bool_prefix 查询示例：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;li northern\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bool_prefix\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;description\u0026#34; ] } } } 前面的查询被执行为以下 dis_max 查询，每个字段都有一个 match_bool_prefix 查询：\nGET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;dis_max\u0026#34;: { \u0026#34;queries\u0026#34;: [ { \u0026#34;match_bool_prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;li northern\u0026#34; }}, { \u0026#34;match_bool_prefix\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;li northern\u0026#34; }} ] } } }  用于构建 terms 查询的 fuzziness 、 prefix_length 、 max_expansions 、 fuzzy_rewrite 和 fuzzy_transpositions 参数是受支持的，但它们不会对由最终 term 构建的前缀查询产生影响。\n 参数说明 #  该查询接受以下参数。除 query 外的所有参数都是可选的。\n   参数 数据类型 描述     query String 用于搜索的查询字符串。必填。   auto_generate_synonyms_phrase_query Boolean 指定是否自动为多词同义词创建匹配短语查询。例如，如果你将 ba,batting average 指定为同义词并搜索 ba ，Easysearch 会搜索 ba OR \u0026ldquo;batting average\u0026rdquo; （如果此选项为 true ）或 ba OR (batting AND average) （如果此选项为 false ）。默认值为 true 。   analyzer String 用于对查询字符串文本进行分词的分词器。默认值为为 default_field 指定的索引时分词器。如果未为 default_field 指定分词器，则 analyzer 为索引的默认分词器。有关 index.query.default_field 的更多信息，请参阅动态索引级索引设置。   boost Float 通过给定的乘数提升子句的权重。适用于在复合查询中对子句进行加权。[0, 1) 范围内的值会降低相关性，而大于 1 的值会增加相关性。默认值为 1 。   fields 字符串数组 要搜索的字段列表。如果你不提供 fields 参数， multi_match 查询会在 index.query.default_field 设置中指定的字段进行搜索，而 index.query.default_field 的默认值为 * 。   fuzziness String 在确定一个词是否匹配一个值时，将一个词改为另一个词所需的字符编辑次数（插入、删除、替换）。例如， wined 和 wind 之间的距离是 1。有效值为非负整数或 AUTO 。默认值 AUTO 根据每个词的长度选择值，对于大多数用例是一个不错的选择。不支持 phrase 、 phrase_prefix 和 cross_fields 查询。   fuzzy_rewrite String 确定 Easysearch 如何重写查询。有效值为 constant_score、scoring_boolean、constant_score_boolean、top_terms_N、top_terms_boost_N 和 top_terms_blended_freqs_N。如果 fuzziness 参数不是 0，查询默认使用 top_terms_blended_freqs_${max_expansions} 重写方法。默认值为 constant_score。   fuzzy_transpositions Boolean 将 fuzzy_transpositions 设置为 true （默认）会在 fuzziness 选项的插入、删除和替换操作中添加相邻字符的交换。例如，如果 fuzzy_transpositions 为真（交换“n”和“i”），则 wind 和 wnid 之间的距离为 1；如果为假（删除“n”，插入“n”），则距离为 2。如果 fuzzy_transpositions 为假， rewind 和 wnid 与 wind 的距离相同（2），尽管从更以人为中心的观点来看， wnid 是一个明显的拼写错误。对于大多数用例，默认值是一个不错的选择。   lenient Boolean 将 lenient 设置为 true 会忽略查询与文档字段之间的数据类型不匹配。例如， \u0026ldquo;8.2\u0026rdquo; 的查询字符串可以匹配类型为 float 的字段。默认值为 false 。   max_expansions 正整数 查询可以扩展到的最大词数。模糊查询会扩展到与指定距离（ fuzziness ）内的匹配词。然后 Easysearch 尝试匹配这些词。默认值为 50 。   minimum_should_match 正整数或负整数、正百分比或负百分比、组合 如果查询字符串包含多个搜索词并且你使用 or 运算符，文档被考虑为匹配所需的匹配词数。例如，如果 minimum_should_match 为 2， wind often rising 不匹配 The Wind Rises. 如果 minimum_should_match 为 1 ，则匹配。详情请参阅 Minimum should match。   operator String 如果查询字符串包含多个搜索词，文档是否需要所有词都匹配（ AND ）或只需一个词匹配（ OR ）才被视为匹配。有效值为：- OR : 字符串 to be 被解释为 to OR be ;- AND : 字符串 to be 被解释为 to AND be.默认为 OR 。   prefix_length 非负整数 不考虑模糊性的前导字符数量。默认为 0 。   slop 0 （默认）或一个正整数 控制查询中单词可以错乱的程度，仍然被视为匹配的程度。来自 Lucene 文档的说明：“查询短语中允许的其他单词数量。例如，要交换两个单词的顺序需要两次移动（第一次移动将单词叠放在一起），因此为了允许短语的重排序，slop 值必须至少为 2。值为零要求完全匹配。” 支持 phrase 和 phrase_prefix 查询类型。   tie_breaker Float 一个介于 0 和 1.0 之间的因子，用于给匹配多个查询子句的文档赋予更高的权重。更多信息，请参阅tie_breaker参数。   type String 多匹配查询类型。有效值为best_fields、most_fields、cross_fields、phrase、phrase_prefix、bool_prefix。默认值为best_fields。   zero_terms_query String 在某些情况下，分词器会从查询字符串中删除所有词项。例如， stop 分词器会从字符串 an but this 中删除所有词项。在这种情况下， zero_terms_query 指定是否匹配不匹配任何文档（ none ）或所有文档（ all ）。有效值为 none 和 all 。默认为 none 。     fuzziness 参数不适用于 phrase 、 phrase_prefix 和 cross_fields 查询。\n  slop 参数仅适用于 phrase 和 phrase_prefix 查询。\n tie_breaker 参数 #  每个词级混合查询将文档分数计算为组中任何字段返回的最佳分数。所有混合查询的分数相加产生最终分数。您可以通过使用 tie_breaker 参数更改分数的计算方式。 tie_breaker 参数接受以下值：\n 0.0（ best_fields 、 cross_fields 、 phrase 和 phrase_prefix 查询的默认值）：取组中任何字段返回的最佳单个分数。 1.0（ most_fields 和 bool_prefix 查询的默认值）：为组内所有字段添加分数。 一个在(0, 1)范围内的浮点值：取最佳匹配字段的最佳分数，并为所有其他匹配字段添加( tie_breaker * _score )。  ","subcategory":null,"summary":"","tags":null,"title":"Multi Match 查询","url":"/easysearch/main/docs/features/fulltext-search/full-text/multi-match/"},{"category":null,"content":"More Like This 查询 #  使用 more_like_this 查询查找与一个或多个给定文档相似的文档。这对于推荐引擎、内容发现以及识别数据集中的相关项目很有用。\nmore_like_this 查询分析输入文档或文本，并选择最能描述它们的词项。然后，它搜索包含这些重要词项的其他文档。\n相关指南（先读这些） #    相关性与打分策略  Query DSL 基础  前提条件 #  在使用 more_like_this 查询之前，请确保您目标字段已索引，且其数据类型为 text 或 keyword 。\n如果您在 like 部分引用文档，Easysearch 需要访问其内容。这通常通过 _source 字段完成，该字段默认启用。如果 _source 被禁用，您必须单独存储这些字段，或配置它们以保存 term_vector 数据。\n 在索引文档时保存 term_vector 信息可以大大加速 more_like_this 查询，因为引擎可以直接检索重要词项，而无需在查询时重新分析字段文本。\n 示例：无词向量优化 #  使用以下映射创建名为 articles-basic 的索引：\nPUT /articles-basic { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } 添加示例文档：\nPOST /articles-basic/_bulk { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 1 }} { \u0026#34;title\u0026#34;: \u0026#34;Exploring the Sahara Desert\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Sand dunes and vast landscapes.\u0026#34; } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 2 }} { \u0026#34;title\u0026#34;: \u0026#34;Amazon Rainforest Tour\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Dense jungle and exotic wildlife.\u0026#34; } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: 3 }} { \u0026#34;title\u0026#34;: \u0026#34;Mountain Adventures\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Snowy peaks and hiking trails.\u0026#34; } 使用以下请求进行查询：\nGET /articles-basic/_search { \u0026#34;query\u0026#34;: { \u0026#34;more_like_this\u0026#34;: { \u0026#34;fields\u0026#34;: [\u0026#34;content\u0026#34;], \u0026#34;like\u0026#34;: \u0026#34;jungle wildlife\u0026#34;, \u0026#34;min_term_freq\u0026#34;: 1, \u0026#34;min_doc_freq\u0026#34;: 1 } } } more_like_this 查询在 content 字段中搜索 jungle 和 wildlife ，仅匹配一个文档：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.9616582, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;articles-basic\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.9616582, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Amazon Rainforest Tour\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Dense jungle and exotic wildlife.\u0026#34; } } ] } } 示例：词项向量优化 #  使用以下映射创建名为 articles-optimized 的索引：\nPUT /articles-optimized { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;with_positions_offsets\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;with_positions_offsets\u0026#34; } } } } 将样本文档插入优化索引：\nPOST /articles-optimized/_bulk { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: \u0026#34;a1\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Diana\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;Wonder Woman\u0026#34;, \u0026#34;quote\u0026#34;: \u0026#34;Justice will come when it is deserved.\u0026#34; } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: \u0026#34;a2\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Clark\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;Superman\u0026#34;, \u0026#34;quote\u0026#34;: \u0026#34;Even in the darkest times, hope cuts through.\u0026#34; } { \u0026#34;index\u0026#34;: { \u0026#34;_id\u0026#34;: \u0026#34;a3\u0026#34; } } { \u0026#34;name\u0026#34;: \u0026#34;Bruce\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;Batman\u0026#34;, \u0026#34;quote\u0026#34;: \u0026#34;I am vengeance. I am the night. I am Batman!\u0026#34; } 查找 quote 字段中包含与“dark”和“night”相似的词的文档：\nGET /articles-optimized/_search { \u0026#34;query\u0026#34;: { \u0026#34;more_like_this\u0026#34;: { \u0026#34;fields\u0026#34;: [\u0026#34;quote\u0026#34;], \u0026#34;like\u0026#34;: \u0026#34;dark night\u0026#34;, \u0026#34;min_term_freq\u0026#34;: 1, \u0026#34;min_doc_freq\u0026#34;: 1 } } } more_like_this 查询搜索 dark 和 night 并返回以下命中结果：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.2363393, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;articles-optimized\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;a3\u0026#34;, \u0026#34;_score\u0026#34;: 1.2363393, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Bruce\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;Batman\u0026#34;, \u0026#34;quote\u0026#34;: \u0026#34;I am vengeance. I am the night. I am Batman!\u0026#34; } } ] } } 示例：使用多个文档和文本输入 #\n more_like_this 查询允许你在 like 参数中提供多个来源。你可以将自由文本与索引中的文档结合使用。如果你希望搜索结合多个示例的相关性信号，这会很有用。\n在以下示例中，直接提供了一个自定义文档。此外，还包括了来自 heroes 索引的具有 ID 5 的现有文档：\nGET /articles-optimized/_search { \u0026#34;query\u0026#34;: { \u0026#34;more_like_this\u0026#34;: { \u0026#34;fields\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;alias\u0026#34;], \u0026#34;like\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Diana\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;Wonder Woman\u0026#34;, \u0026#34;quote\u0026#34;: \u0026#34;Courage is not the absence of fear, but the triumph over it.\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;heroes\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34; } ], \u0026#34;min_term_freq\u0026#34;: 1, \u0026#34;min_doc_freq\u0026#34;: 1, \u0026#34;max_query_terms\u0026#34;: 25 } } } 返回的结果包含与查询中提供的 name 和 alias 字段最相似的文章：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2.140194, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;articles-optimized\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;a1\u0026#34;, \u0026#34;_score\u0026#34;: 2.140194, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Diana\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;Wonder Woman\u0026#34;, \u0026#34;quote\u0026#34;: \u0026#34;Justice will come when it is deserved.\u0026#34; } } ] } } \n当你希望根据一个尚未完全索引的新概念来提升结果，同时还想将其与现有索引文档中的知识结合起来时，请使用此模式。\n 参数说明 #  more_like_this 查询的唯一必需参数是 like 。其余参数具有默认值，但允许精细调整。以下为主要参数类别。\n文档输入参数 #  下表指定了文档输入参数。\n   参数 必需/可选 数据类型 描述     like 必需 字符串或对象的数组 定义要查找相似文档的文本或文档。您可以输入自由文本、索引中的真实文档或人工文档。除非被覆盖，否则与字段关联的分词器会处理文本。   unlike 可选 字符串或对象的数组 提供文本或文档，其中的词项应被排除，不用于影响查询。可用于指定负例。   fields 可选 字符串数组 列出用于分析文本的字段。如果未指定，则使用所有字段。    词项选择参数 #     参数 必需/可选 数据类型 描述     max_query_terms 可选 Integer 设置从输入中选择的最大词数。值越高，精确度越高，但执行速度会变慢。默认值为 25 。   min_term_freq 可选 Integer 在输入中出现的次数少于此值的词将被忽略。默认值为 2 。   min_doc_freq 可选 Integer 在少于此值数量的文档中出现的词将被忽略。默认值为 5 。   max_doc_freq 可选 Integer 在超过此限制数量的文档中出现的词将被忽略。可用于避免非常常见的词。默认值为无限制（2^31 - 1）。   min_word_length 可选 Integer 忽略比这个值更短的词。默认值是 0 。   max_word_length 可选 Integer 忽略比这个值更长的词。默认值是无限制。   stop_words 可选 字符串数组 定义在选取词项时完全忽略的一系列单词。   analyzer 可选 String 用于处理输入文本的自定义分词器。默认为 fields 中列出的第一个字段的分词器。    查询形成参数 #     参数 必需/可选 数据类型 描述     minimum_should_match 可选 String 指定最终查询中必须匹配的最小词数。值可以是百分比或固定数字。有助于微调召回率和精确率之间的平衡。默认值是 30%   fail_on_unsupported_field 可选 Boolean 确定如果目标字段中有一个不是兼容类型（ text 或 keyword ），是否抛出错误。设置为 false 以静默方式跳过不支持的字段。默认是 true 。   boost_terms 可选 Float 根据所选词项的词频-逆文档频率（TF–IDF）权重对词项应用提升。任何大于 0 的值都会使用指定的因子激活词项提升。默认是 0 。   include 可选 Boolean 如果 true ，则 like 中提供的源文档包含在结果命中中。默认是 false 。   boost\t 可选 Float 乘以整个 more_like_this 查询的相关性分数。默认值为 1.0 。    ","subcategory":null,"summary":"","tags":null,"title":"More Like This 查询","url":"/easysearch/main/docs/features/query-dsl/specialized/more-like-this/"},{"category":null,"content":"Match 查询 #  使用 match 查询在特定文档字段上执行全文搜索。如果你在 text 字段上运行 match 查询，match 查询会分析提供的搜索字符串，并返回匹配字符串中任意词的文档。如果你在精确值字段上运行 match 查询，它会返回匹配精确值的文档。搜索精确值字段的推荐方式是使用过滤（filter）查询，因为与普通查询不同，过滤（filter）查询会被缓存。\n相关指南（先读这些） #    全文检索  Query DSL 基础  以下示例展示了在 title 中对 wind 的基本 match 查询：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;wind\u0026#34; } } } 通过传递其他参数，您可以使用扩展语法：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wind\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;stop\u0026#34; } } } } 参考样例 #  在以下示例中，您将使用包含以下文档的索引：\nPUT testindex/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;Let the wind rise\u0026#34; } PUT testindex/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Gone with the wind\u0026quot;\n}\nPUT testindex/_doc/3 { \u0026quot;title\u0026quot;: \u0026quot;Rise is gone\u0026quot; }\n运算符（operator） #\n operator 参数控制多个词元之间的逻辑关系：or（默认）或 and。默认操作符是 OR，查询 wind rise 会被改为 wind OR rise。要指定 and 操作符，请使用以下查询：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wind rise\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34; } } } } 使用 operator: \u0026quot;and\u0026quot; 时，查询构建为 wind AND rise，返回结果：\n{ \u0026#34;took\u0026#34;: 17, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.2667098, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.2667098, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Let the wind rise\u0026#34; } } ] } } 最小匹配（minimum_should_match） #  minimum_should_match 参数控制文档必须匹配的最小词数。示例：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wind rise\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;or\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: 2 } } } } 当 minimum_should_match: 2 且只有两个词时，效果等同于 and 运算符，返回结果：\n{ \u0026#34;took\u0026#34;: 23, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.2667098, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.2667098, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Let the wind rise\u0026#34; } } ] } } 分词器（analyzer） #  analyzer 参数指定用于分析查询文本的分词器。默认使用字段映射中指定的分词器（通常是 standard）。要使用不同的分词器，可在查询中指定。例如，使用 english 分词器：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the wind rises\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;english\u0026#34; } } } } english 分词器会移除停用词并执行词干提取，返回结果：\n{ \u0026#34;took\u0026#34;: 19, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.2667098, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.2667098, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Let the wind rise\u0026#34; } } ] } } 空查询 #  在某些情况下，分词器可能会从查询中移除所有标记。例如， english 分词器会移除停用词，所以在查询 and OR or 中，所有标记都会被移除。要检查分词器的行为，你可以使用分析 API：\nGET testindex/_analyze { \u0026#34;analyzer\u0026#34; : \u0026#34;english\u0026#34;, \u0026#34;text\u0026#34; : \u0026#34;and OR or\u0026#34; } 正如预期，该查询没有产生任何标记：\n{ \u0026#34;tokens\u0026#34;: [] } 你可以通过 zero_terms_query 参数指定空查询的行为。将 zero_terms_query 设置为 all 会返回索引中的所有文档，而将其设置为 none 则不会返回任何文档：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;and OR or\u0026#34;, \u0026#34;analyzer\u0026#34; : \u0026#34;english\u0026#34;, \u0026#34;zero_terms_query\u0026#34;: \u0026#34;all\u0026#34; } } } } 模糊性查询 #  为了考虑拼写错误，您可以在查询中指定 fuzziness 作为以下任一选项：\n 一个整数，指定允许的最大 Damerau-Levenshtein 距离。 AUTO  0-2 个字符的字符串必须完全匹配。 3–5 个字符的字符串允许 1 次编辑。 超过 5 个字符的字符串允许 2 次编辑。    将 fuzziness 设置为默认的 AUTO 值在大多数情况下效果最佳：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wnid\u0026#34;, \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34; } } } } token wnid 匹配 wind ，查询返回文档 1 和 2：\n{ \u0026#34;took\u0026#34;: 31, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.47501624, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.47501624, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Let the wind rise\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.47501624, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gone with the wind\u0026#34; } } ] } } 前缀长度 #  拼写错误很少出现在单词的开头。因此，您可以指定匹配前缀的最小长度，以便在结果中返回文档。例如，您可以将前面的查询更改为包含 prefix_length ：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wnid\u0026#34;, \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34;, \u0026#34;prefix_length\u0026#34;: 2 } } } } 前面的查询没有返回结果。如果你将 prefix_length 改为 1，则会返回文档 1 和 2，因为词 wnid 的首字母没有拼写错误。\n移位错误 #  在前面这个例子中，词 wnid 包含一个移位错误（ in 被改为了 ni ）。默认情况下，模糊匹配允许移位错误，但你可以通过将 fuzzy_transpositions 设置为 false 来禁止它们：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wnid\u0026#34;, \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34;, \u0026#34;fuzzy_transpositions\u0026#34;: false } } } } 现在查询没有返回结果。\n同义词 #  如果你使用 synonym_graph 过滤器并且 auto_generate_synonyms_phrase_query 设置为 true （默认值），Easysearch 会将查询解析为词项，然后将这些词项组合起来生成一个短语查询以匹配多词同义词。例如，如果你指定 ba,batting average 作为同义词并搜索 ba ，Easysearch 会搜索 ba OR \u0026quot;batting average\u0026quot; 。\n要使用连词匹配多词同义词，将 auto_generate_synonyms_phrase_query 设置为 false ：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;good ba\u0026#34;, \u0026#34;auto_generate_synonyms_phrase_query\u0026#34;: false } } } } 生成的查询是 ba OR (batting AND average) 。\n参数说明 #  该查询将字段名称（ \u0026lt;field\u0026gt; ）作为顶级参数接受：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;text to search for\u0026#34;, ... } } } } \u0026lt;field\u0026gt; 接受以下参数。除 query 外，所有参数都是可选的。\n   参数 数据类型 描述     query String 用于搜索的查询字符串。必填。   auto_generate_synonyms_phrase_query Boolean 指定是否自动为多词同义词创建匹配短语查询。例如，如果你将 ba,batting average 指定为同义词并搜索 ba ，Easysearch 会搜索 ba OR \u0026ldquo;batting average\u0026rdquo; （如果此选项为 true ）或 ba OR (batting AND average) （如果此选项为 false ）。默认值为 true 。   analyzer String 用于对查询字符串文本进行分词的分词器。默认值为为 default_field 指定的索引时分词器。如果未为 default_field 指定分词器，则 analyzer 为索引的默认分词器。有关 index.query.default_field 的更多信息，请参阅动态索引级索引设置。   boost Float 通过给定的乘数提升子句的权重。适用于在复合查询中对子句进行加权。[0, 1) 范围内的值会降低相关性，而大于 1 的值会增加相关性。默认值为 1 。   fuzziness String 在确定一个词是否匹配一个值时，将其从一个词转换为另一个词所需的字符编辑次数（插入、删除、替换或转换）。例如， wined 和 wind 之间的距离是 1。有效值为非负整数或 AUTO 。默认值 AUTO 根据每个词的长度选择值，对于大多数用例是一个不错的选择。   fuzzy_rewrite String 确定 Easysearch 如何重写查询。有效值为 constant_score、scoring_boolean、constant_score_boolean、top_terms_N、top_terms_boost_N 和 top_terms_blended_freqs_N。如果 fuzziness 参数不是 0，查询默认使用 top_terms_blended_freqs_${max_expansions} 重写方法。默认值为 constant_score。   fuzzy_transpositions Boolean 将 fuzzy_transpositions 设置为 true （默认）会在 fuzziness 选项的插入、删除和替换操作中添加相邻字符的交换。例如，如果 fuzzy_transpositions 为真（交换“n”和“i”），则 wind 和 wnid 之间的距离为 1；如果为假（删除“n”，插入“n”），则距离为 2。如果 fuzzy_transpositions 为假， rewind 和 wnid 与 wind 的距离相同（2），尽管从更以人为中心的观点来看， wnid 是一个明显的拼写错误。对于大多数用例，默认值是一个不错的选择。   lenient Boolean 将 lenient 设置为 true 会忽略查询与文档字段之间的数据类型不匹配。例如， \u0026ldquo;8.2\u0026rdquo; 的查询字符串可以匹配类型为 float 的字段。默认值为 false 。   max_expansions Positive integer 查询可以扩展到的最大词数。模糊查询会扩展到与指定距离（fuzziness）内的匹配词。然后 Easysearch 尝试匹配这些词。默认值为 50。   minimum_should_match 正整数或负整数、正百分比或负百分比、也可以是这些的组合 如果查询字符串包含多个搜索词并且你使用 or 运算符，文档被考虑为匹配所需的匹配词数。例如，如果 minimum_should_match 为 2， wind often rising 不匹配 The Wind Rises. 如果 minimum_should_match 为 1 ，则匹配。详情请参阅 Minimum should match。   operator String 如果查询字符串包含多个搜索词，文档是否需要所有词都匹配（ AND ）或只需一个词匹配（ OR ）才被视为匹配。有效值为：- OR : 字符串 to be 被解释为 to OR be ；- AND : 字符串 to be 被解释为 to AND be 。默认值为 OR 。   prefix_length Non-negative integer 不考虑模糊性的前导字符数量。默认为 0 。   zero_terms_query String 在某些情况下，分词器会从查询字符串中删除所有词项。例如， stop 分词器会从字符串 an but this 中删除所有词项。在这种情况下， zero_terms_query 指定是否匹配不匹配任何文档（ none ）或所有文档（ all ）。有效值为 none 和 all 。默认为 none 。    ","subcategory":null,"summary":"","tags":null,"title":"Match 查询","url":"/easysearch/main/docs/features/fulltext-search/full-text/match/"},{"category":null,"content":"Match Phrase 查询 #  使用 match_phrase 查询来匹配包含指定顺序中确切的短语的文档。您可以通过提供 slop 参数来增加短语匹配的灵活性。\nmatch_phrase 查询创建一个匹配词项序列的短语查询。\n相关指南（先读这些） #    邻近匹配  全文搜索  以下示例展示了一个基本的 match_phrase 查询：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;the wind\u0026#34; } } } 要传递额外的参数，您可以使用扩展语法：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the wind\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;stop\u0026#34; } } } } 参考用例 #  例如，创建一个包含以下文档的索引：\nPUT testindex/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } PUT testindex/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Gone with the wind\u0026quot; } 以下 match_phrase 查询搜索短语 wind rises ，其中单词 wind 后面跟着单词 rises ：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;wind rises\u0026#34; } } } 返回内容包含匹配的文档：\n{ \u0026#34;took\u0026#34;: 30, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.92980814, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.92980814, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } } ] } } 分词器 #  默认情况下，当你对 text 字段运行查询时，搜索文本会使用与该字段关联的索引分词器进行分析。你可以在 analyzer 参数中指定不同的搜索分词器。例如，以下查询使用了 english 分词器：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the winds\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;english\u0026#34; } } } } english 分词器移除了停用词 the 并执行词干提取，生成了词元 wind 。两个文档都匹配这个词元，并在结果中返回：\n{ \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.19363807, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.19363807, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.17225474, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gone with the wind\u0026#34; } } ] } } Slop #  如果你提供一个 slop 参数，查询会容忍搜索词的重新排序。Slop 指定了查询短语中允许在两个词之间插入的其他词的数量。例如，在以下查询中，搜索文本与文档文本相比进行了重新排序：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wind rises the\u0026#34;, \u0026#34;slop\u0026#34;: 3 } } } } 查询仍然返回匹配的文档：\n{ \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.44026947, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.44026947, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } } ] } } 参数说明 #  该查询将字段名称（ \u0026lt;field\u0026gt; ）作为顶级参数接受：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;text to search for\u0026#34;, ... } } } } \u0026lt;field\u0026gt; 接受以下参数。除 query 外，所有参数都是可选的。\n   参数 数据类型 描述     query String 用于搜索的查询字符串。必填。   analyzer String 用于对查询字符串文本进行分词的分词器。默认值为为 default_field 指定的索引时分词器。如果未为 default_field 指定分词器，则 analyzer 为索引的默认分词器。有关 index.query.default_field 的更多信息，请参阅动态索引级索引设置。   slop 0 （默认）或一个正整数 控制查询中的词语可以错乱到何种程度仍被视为匹配。根据 Lucene 文档的说明：“查询短语中允许插入的其他词语数量。例如，要交换两个词语的顺序需要两次移动（第一次移动将词语叠放在一起），因此为了允许短语的重排序，slop 值必须至少为 2。值为零则要求完全匹配。”   zero_terms_query String 在某些情况下，分词器会从查询字符串中删除所有词项。例如， stop 分词器会从字符串 an but this 中删除所有词项。在这种情况下， zero_terms_query 指定是否匹配不匹配任何文档（ none ）或所有文档（ all ）。有效值为 none 和 all 。默认为 none 。   boost Float 用于调整该查询的相关性得分权重。默认值为 1.0。    ","subcategory":null,"summary":"","tags":null,"title":"Match Phrase 查询","url":"/easysearch/main/docs/features/fulltext-search/full-text/match-phrase/"},{"category":null,"content":"Match Phrase Prefix 查询 #  使用 match_phrase_prefix 查询来指定要匹配的短语。包含您指定短语的文档将被返回。短语中的最后一个部分词被解释为前缀，因此任何包含以该短语和最后一个词的前缀开头的短语的文档都将被返回。\n与 match_phrase 类似，但会从查询字符串中的最后一个词创建一个前缀查询。\n相关指南（先读这些） #    部分匹配  邻近匹配  对于 match_phrase_prefix 和 match_bool_prefix 查询之间的差异，请参阅 match_bool_prefix 和 match_phrase_prefix 查询。\n以下示例展示了一个基本的 match_phrase_prefix 查询：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;the wind\u0026#34; } } } 要传递附加参数，您可以使用扩展语法：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the wind\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;stop\u0026#34; } } } } 参考用例 #\n 例如，创建一个包含以下文档的索引：\nPUT testindex/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } PUT testindex/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Gone with the wind\u0026quot;\n} 以下 match_phrase_prefix 查询会搜索完整单词 wind ，后跟一个以 ri 开头的单词：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;wind ri\u0026#34; } } } 返回包含匹配的文档：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.92980814, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.92980814, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } } ] } } 参数说明 #  该查询将字段名称（ \u0026lt;field\u0026gt;）作为顶级参数接受：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;text to search for\u0026#34;, ... } } } } \u0026lt;field\u0026gt; 接受以下参数。除 query 外，所有参数都是可选的。\n   参数 数据类型 描述     query String 用于搜索的查询字符串。必填。   analyzer String 用于分词查询的分词器。   max_expansions Positive integer 最后一个词项（前缀）可以扩展匹配的最大词项数。值越大召回越多但性能越低。默认值为 50。   slop 0（默认）或正整数 允许匹配词项之间插入的其他词项数量。例如，交换两个词项的顺序需要 slop 为 2。值为 0 要求完全匹配短语顺序。   zero_terms_query String 当分析器移除查询字符串中所有词项时（如停用词过滤），指定匹配行为。none 不匹配任何文档，all 匹配所有文档。默认 none。   boost Float 用于调整该查询的相关性得分权重。默认值为 1.0。    ","subcategory":null,"summary":"","tags":null,"title":"Match Phrase Prefix 查询","url":"/easysearch/main/docs/features/fulltext-search/full-text/match-phrase-prefix/"},{"category":null,"content":"Match Bool Prefix 查询 #  match_bool_prefix 查询分析提供的搜索字符串，并从字符串的词项中创建一个布尔查询。它将除最后一个词项外的每个词项作为完整单词进行匹配。最后一个词项用作前缀。match_bool_prefix 查询返回包含完整单词词项或以前缀词项开头的词项的文档，顺序不限。\n相关指南（先读这些） #    部分匹配  全文搜索  以下示例展示了一个基本的 match_bool_prefix 查询：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_bool_prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;the wind\u0026#34; } } } 要传递额外参数，您可以使用扩展语法：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_bool_prefix\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the wind\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;stop\u0026#34; } } } } 参考样例 #\n 例如，考虑一个包含以下文档的索引：\nPUT testindex/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } PUT testindex/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Gone with the wind\u0026quot;\n}\n以下 match_bool_prefix 查询会搜索整个词 rises 以及以 wi 开头的词，顺序不限：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_bool_prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;rises wi\u0026#34; } } } 前面的查询等同于以下布尔查询：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34; : { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;rises\u0026#34; }}, { \u0026#34;prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;wi\u0026#34;}} ] } } } 响应包含两个文档：\n{ \u0026#34;took\u0026#34;: 15, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.73617, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.73617, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gone with the wind\u0026#34; } } ] } } match_bool_prefix 和 match_phrase_prefix 查询 #  match_bool_prefix 查询匹配任何位置的词，而 match_phrase_prefix 查询匹配整个短语。为了说明区别，再次考虑上一节的 match_bool_prefix 查询：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_bool_prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;rises wi\u0026#34; } } } The wind rises 和 Gone with the wind 都匹配了搜索词，因此查询返回了这两份文档。\n现在在同一索引上运行一个 match_phrase_prefix 查询：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;rises wi\u0026#34; } } } 响应返回没有文档，因为没有任何文档包含按指定顺序出现的 rises wi 短语。\n分词器 #  默认情况下，当你在一个 text 字段上运行查询时，搜索文本会使用与该字段关联的索引分词器进行分析。你可以在 analyzer 参数中指定不同的搜索分词器：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_bool_prefix\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;rise the wi\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;stop\u0026#34; } } } } 参数说明 #\n 该查询将字段名称（ \u0026lt;field\u0026gt; ）作为顶级参数接受：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_bool_prefix\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;text to search for\u0026#34;, ... } } } } \u0026lt;field\u0026gt; 接受以下参数。除 query 外，所有参数都是可选的。\n   参数 数据类型 描述     query String 用于搜索的文本、数字、布尔值或日期。必填。   analyzer String 用于对查询字符串文本进行分词的分词器。默认值为为 default_field 指定的索引时分词器。如果未为 default_field 指定分词器，则 analyzer 为索引的默认分词器。有关 index.query.default_field 的更多信息，请参阅动态索引级索引设置。   fuzziness AUTO 、 0 或正整数 在确定一个词是否匹配一个值时，将一个词改为另一个词所需的字符编辑次数（插入、删除、替换）。例如， wined 和 wind 之间的距离是 1。默认值 AUTO 根据每个词的长度选择值，对于大多数用例是一个不错的选择。   fuzzy_rewrite String 确定 Easysearch 如何重写查询。有效值为 constant_score、scoring_boolean、constant_score_boolean、top_terms_N、top_terms_boost_N 和 top_terms_blended_freqs_N。如果 fuzziness 参数不是 0，查询默认使用 top_terms_blended_freqs_${max_expansions} 重写方法。默认值为 constant_score。   fuzzy_transpositions Boolean 将 fuzzy_transpositions 设置为 true（默认）会在 fuzziness 选项的插入、删除和替换操作中添加相邻字符的交换。例如，如果为 true（交换 \u0026ldquo;n\u0026rdquo; 和 \u0026ldquo;i\u0026rdquo;），则 wind 和 wnid 之间的距离为 1；如果为 false（删除 \u0026ldquo;n\u0026rdquo;，插入 \u0026ldquo;n\u0026rdquo;），则距离为 2。默认值为 true。   max_expansions 正整数 查询可以扩展到的最大词数。模糊查询会扩展到与指定距离（ fuzziness ）内的匹配词。然后 Easysearch 尝试匹配这些词。默认值为 50 。   minimum_should_match 正整数或负整数、正百分比或负百分比、或者这些类型组合 如果查询字符串包含多个搜索词并且你使用 or 运算符，文档被考虑为匹配所需的匹配词数。例如，如果 minimum_should_match 为 2， wind often rising 不匹配 The Wind Rises. 如果 minimum_should_match 为 1 ，则匹配。详情请参阅 Minimum should match。   operator String 如果查询字符串包含多个搜索词，是否所有词都需要匹配（ and ）或只需要一个词匹配（ or ）才能认为文档匹配。有效值为 or 和 and 。默认值是 or 。   prefix_length 非负整数 不考虑模糊性的前导字符数量。默认为 0 。   boost Float 用于调整该查询的相关性得分权重。默认值为 1.0。    ","subcategory":null,"summary":"","tags":null,"title":"Match Bool Prefix 查询","url":"/easysearch/main/docs/features/fulltext-search/full-text/match-bool-prefix/"},{"category":null,"content":"IP 范围聚合 #  ip_range 聚合用于 IP 地址。它适用于 ip 类型字段。您可以在 CIDR 表示法中定义 IP 范围和掩码。\n相关指南（先读这些） #    聚合基础  聚合场景实践  GET sample_data_logs/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;access\u0026#34;: { \u0026#34;ip_range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;ip\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;126.158.155.183\u0026#34; }, { \u0026#34;mask\u0026#34;: \u0026#34;1.0.0.0/8\u0026#34; } ] } } } } 返回内容\n... \u0026#34;aggregations\u0026#34; : { \u0026#34;access\u0026#34; : { \u0026#34;buckets\u0026#34; : [ { \u0026#34;key\u0026#34; : \u0026#34;1.0.0.0/8\u0026#34;, \u0026#34;from\u0026#34; : \u0026#34;1.0.0.0\u0026#34;, \u0026#34;to\u0026#34; : \u0026#34;2.0.0.0\u0026#34;, \u0026#34;doc_count\u0026#34; : 98 }, { \u0026#34;key\u0026#34; : \u0026#34;1.0.0.0-126.158.155.183\u0026#34;, \u0026#34;from\u0026#34; : \u0026#34;1.0.0.0\u0026#34;, \u0026#34;to\u0026#34; : \u0026#34;126.158.155.183\u0026#34;, \u0026#34;doc_count\u0026#34; : 7184 } ] } } } 如果您向映射中将 ip_range 设置为 false 的索引添加了字段格式不正确的文档，Easysearch 会拒绝整个文档。您可以将 ignore_malformed 设置为 true 以指定 Easysearch 应忽略格式不正确的字段。默认值为 false 。\n... \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;ips\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ip_range\u0026#34;, \u0026#34;ignore_malformed\u0026#34;: true } } } 参数说明 #     参数 必需/可选 数据类型 描述     field 必填 String IP 类型的字段名称。   ranges 必填 Array IP 范围数组。每个元素可含 from/to 或 mask（CIDR 表示法）。   keyed 可选 Boolean 若为 true，以键值对形式返回桶。默认 false。    CIDR 表示法 #  使用 mask 参数可以用 CIDR 表示法定义 IP 范围，常用的网段掩码：\n   CIDR 范围 IP 数量     /8 x.0.0.0 - x.255.255.255 16,777,216   /16 x.y.0.0 - x.y.255.255 65,536   /24 x.y.z.0 - x.y.z.255 256   /32 单个 IP 1     提示：ip_range 聚合同时支持 IPv4 和 IPv6 地址。\n ","subcategory":null,"summary":"","tags":null,"title":"IP 范围聚合（IP Range）","url":"/easysearch/main/docs/features/aggregations/bucket-aggregations/ip-range/"},{"category":null,"content":"Intervals 查询 #  intervals 查询根据匹配词的邻近度和顺序来匹配文档。它将一组匹配规则应用于指定字段中的词。该查询生成跨越文本中词的最小间隔序列。你可以组合间隔并按父源进行过滤。\n相关指南（先读这些） #    邻近匹配  全文搜索  考虑一个包含以下文档的索引：\nPUT testindex/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;key-value pairs are efficiently stored in a hash table\u0026#34; } PUT /testindex/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;store key-value pairs in a hash map\u0026quot; } 例如，以下查询搜索包含短语 key-value pairs （词之间没有间隔）后跟 hash table 或 hash map 的文档：\nGET /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;intervals\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;all_of\u0026#34;: { \u0026#34;ordered\u0026#34;: true, \u0026#34;intervals\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;key-value pairs\u0026#34;, \u0026#34;max_gaps\u0026#34;: 0, \u0026#34;ordered\u0026#34;: true } }, { \u0026#34;any_of\u0026#34;: { \u0026#34;intervals\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;hash table\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;hash map\u0026#34; } } ] } } ] } } } } } 该查询返回两个文档：\n{ \u0026#34;took\u0026#34;: 1011, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.25, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.25, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;store key-value pairs in a hash map\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.14285713, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;key-value pairs are efficiently stored in a hash table\u0026#34; } } ] } } 参数说明 #  该查询将字段名称（ \u0026lt;field\u0026gt; ）作为顶级参数接受：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;intervals\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { ... } } } } \u0026lt;field\u0026gt; 接受以下规则对象，这些规则对象用于根据词项、顺序和邻近度来匹配文档。\n   规则 描述     match 匹配分析文本。   prefix 匹配以指定字符集开头的词项。   wildcard 使用通配符模式匹配词项。   regexp 使用正则表达式模式匹配词项。   fuzzy 在指定的编辑距离内匹配与提供词项相似的词项。   all_of 使用合取（ AND ）组合多个规则。   any_of 使用析取（ OR ）组合多个规则。    match 规则 #  match 规则匹配分析后的文本。下表列出了 match 规则支持的所有参数。\n   参数 必需/可选 数据类型 描述     query 必需 String 要搜索的文本。   analyzer 可选 String 用于分析 query 文本的解析器。默认为为  指定的解析器。   filter 可选 间隔过滤器规则对象 用于过滤返回间隔的规则。   max_gaps 可选 Integer 匹配词项之间允许的最大位置数。距离超过 max_gaps 的词项不被视为匹配。如果 max_gaps 未指定或设置为 -1 ，无论位置如何，词项都被视为匹配。如果 max_gaps 设置为 0 ，匹配的词项必须相邻出现。默认为 -1 。   ordered 可选 Boolean 指定匹配的词项是否必须按指定顺序出现。默认值为 false 。   use_field 可选 String 指定在此字段中搜索，而不是在顶层字段中搜索。词项使用为此字段指定的搜索分词器进行分析。通过指定 use_field，您可以像它们是同一个字段一样跨多个字段进行搜索。例如，如果您将相同的文本索引到词干化和未词干化的字段中，您可以搜索与未词干化词项相邻的词干化词项。    prefix 规则 #  prefix 规则匹配以指定字符集（前缀）开头的词项。前缀可以扩展以匹配最多 128 个词项。如果前缀匹配的词项超过 128 个，则返回错误。下表列出了 prefix 规则支持的所有参数。\n   参数 必需/可选 数据类型 描述     prefix 必需 String 用于匹配词的前缀。   analyzer 可选 String 用于分析 prefix 文本的解析器。默认为为  指定的解析器。   use_field 可选 String 指定搜索此字段而不是顶层字段。prefix使用此字段指定的搜索分词器进行规范化，除非您指定了analyzer。    wildcard 规则 #  wildcard 规则使用通配符模式匹配词项。通配符模式最多可以扩展匹配 128 个词项。如果模式匹配的词项超过 128 个，将返回错误。下表列出了 wildcard 规则支持的所有参数。\n   参数 必需/可选 数据类型 描述     pattern 必需 String 用于匹配词项的通配符模式。指定 ? 匹配任意单个字符，或指定 * 匹配零个或多个字符。   analyzer 可选 String 用于分析 pattern 文本的解析器。默认为为  指定的解析器。   use_field 可选 String 指定搜索此字段而不是顶层字段。pattern使用此字段指定的搜索分词器进行规范化，除非您指定了analyzer。     指定以 * 或 ? 开头的模式可能会降低搜索性能，因为它增加了匹配词项所需的迭代次数。\n fuzzy 规则 #  fuzzy 规则匹配与提供的词项在指定编辑距离内相似的词项。模糊模式最多可以扩展以匹配 128 个词项。如果模式匹配的词项超过 128 个，则会返回错误。下表列出了 fuzzy 规则支持的所有参数。\n   参数 必需/可选 数据类型 描述     term 必需 String 要匹配的词项。   analyzer 可选 String 用于分析 term 文本的解析器。默认为为  指定的解析器。   fuzziness 可选 String 在确定词项是否与值匹配时，将一个词转换为另一个词所需的字符编辑次数（插入、删除、替换）。例如， wined 和 wind 之间的距离是 1。有效值是非负整数或 AUTO 。默认值 AUTO 根据每个词的长度选择值，对于大多数用例是一个不错的选择。   transpositions 可选 Boolean 将 transpositions 设置为 true （默认值）会将相邻字符的交换添加到 fuzziness 选项的插入、删除和替换操作中。例如，如果 transpositions 为真（交换“n”和“i”），则 wind 和 wnid 之间的距离是 1；如果为假（删除“n”，插入“n”），则距离是 2。如果 transpositions 是 false ，则 rewind 和 wnid 与 wind 的距离相同（2），尽管从更以人为中心的观点来看， wnid 是一个明显的拼写错误。对于大多数用例，默认值是一个不错的选择。   prefix_length 可选 Integer 模糊匹配时保持不变的开头字符数。默认为 0。   use_field 可选 String 指定搜索此字段而不是顶层字段。term使用此字段指定的搜索分词器进行规范化，除非您指定了analyzer。    regexp 规则 #  regexp 规则使用正则表达式模式匹配词项。正则模式最多可以扩展以匹配 128 个词项。如果模式匹配的词项超过 128 个，则会返回错误。下表列出了 regexp 规则支持的所有参数。\n   参数 必需/可选 数据类型 描述     pattern 必需 String 用于匹配词项的正则表达式模式。使用 Lucene 正则表达式语法。   flags_value 可选 Integer Lucene 正则表达式 flag 的位掩码整数值。可替代字符串形式的 flags。默认启用所有 flag（ALL）。   use_field 可选 String 指定搜索此字段而不是顶层字段。pattern 使用此字段指定的搜索分词器进行规范化，除非您指定了其他分词器。   max_expansions 可选 Integer 正则表达式模式可以扩展到的最大词项数。默认为 128。    all_of 规则 #  all_of 规则使用合取（ AND ）组合多个规则。下表列出了 all_of 规则支持的所有参数。\n   参数 必需/可选 数据类型 描述     intervals 必需 规则对象数组 一组用于组合的规则。文档必须匹配所有规则才能在结果中返回。   filter 可选 间隔过滤器规则对象 用于过滤返回间隔的规则。   max_gaps 可选 Integer 匹配词项之间允许的最大位置数。距离超过 max_gaps 的词项不被视为匹配。如果 max_gaps 未指定或设置为 -1 ，无论位置如何，词项都被视为匹配。如果 max_gaps 设置为 0 ，匹配的词项必须相邻出现。默认为 -1 。   ordered 可选 Boolean 如果 true ，则根据规则生成的间隔应按指定顺序出现。默认为 false 。    any_of 规则 #  any_of 规则使用析取（ OR ）组合多个规则。文档只需匹配任意一条规则即可返回。下表列出了 any_of 规则支持的所有参数。\n   参数 必需/可选 数据类型 描述     intervals 必需 规则对象数组 一组用于组合的规则。文档只需匹配任意一条规则即可返回。   filter 可选 间隔过滤器规则对象 用于过滤返回间隔的规则。    filter 规则 #  filter 规则用于限制结果。下表列出了 filter 规则支持的所有参数。\n   参数 必需/可选 数据类型 描述     after 可选 查询对象 返回出现在指定间隔之后的间隔。   before 可选 查询对象 返回出现在指定间隔之前的间隔。   contained_by 可选 查询对象 返回被指定间隔所包含的间隔。   containing 可选 查询对象 返回包含指定间隔的间隔。   not_contained_by 可选 查询对象 返回不被指定间隔所包含的间隔。   not_containing 可选 查询对象 返回不包含指定间隔的间隔。   not_overlapping 可选 查询对象 返回与指定间隔不重叠的间隔。   overlapping 可选 查询对象 返回与指定间隔重叠的间隔。   script 可选 查询对象 用于匹配文档的脚本。此脚本必须返回 true 或 false 。    示例：过滤器 #  以下查询搜索包含 pairs 和 hash 的文档，这两个词彼此相距不超过五个位置，并且它们之间不包含 efficiently 这个词：\nPOST /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;intervals\u0026#34; : { \u0026#34;title\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;query\u0026#34; : \u0026#34;pairs hash\u0026#34;, \u0026#34;max_gaps\u0026#34; : 5, \u0026#34;filter\u0026#34; : { \u0026#34;not_containing\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;query\u0026#34; : \u0026#34;efficiently\u0026#34; } } } } } } } } 返回内容中只包含文档 2：\n{ \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.25, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.25, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;store key-value pairs in a hash map\u0026#34; } } ] } } 示例：脚本过滤器 #  或者，您可以使用以下变量编写自己的脚本过滤器，并将其与 intervals 查询一起使用：\n interval.start : 间隔开始的位置（词编号）。 interval.end : 间隔结束的位置（词编号）。 interval.gap : 两个词之间的词数。  例如，以下查询在指定间隔内搜索相邻的 map 和 hash 这两个词。词的编号从 0 开始，所以在文本 store key-value pairs in a hash map 中， store 位于位置 0， key 位于位置 1 ，以此类推。指定的间隔应从 a 之后开始，并在字符串末尾之前结束：\nPOST /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;intervals\u0026#34; : { \u0026#34;title\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;query\u0026#34; : \u0026#34;map hash\u0026#34;, \u0026#34;filter\u0026#34; : { \u0026#34;script\u0026#34; : { \u0026#34;source\u0026#34; : \u0026#34;interval.start \u0026gt; 5 \u0026amp;\u0026amp; interval.end \u0026lt; 8 \u0026amp;\u0026amp; interval.gaps == 0\u0026#34; } } } } } } } 返回内容包含文档 2：\n{ \u0026#34;took\u0026#34;: 1, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.5, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.5, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;store key-value pairs in a hash map\u0026#34; } } ] } } 间隔最小化 #  为确保查询在线性时间内运行， intervals 查询会最小化间隔。例如，考虑一个包含文本a b c d c的文档。您可以使用以下查询来搜索被 a 和 c 包含的 d ：\nPOST /testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;intervals\u0026#34; : { \u0026#34;my_text\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;query\u0026#34; : \u0026#34;d\u0026#34;, \u0026#34;filter\u0026#34; : { \u0026#34;contained_by\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;query\u0026#34; : \u0026#34;a c\u0026#34; } } } } } } } } 该查询没有返回结果，因为它匹配了前两个词 a c ，并且在这两个词之间没有找到 d 。\n","subcategory":null,"summary":"","tags":null,"title":"Intervals 查询","url":"/easysearch/main/docs/features/fulltext-search/full-text/intervals/"},{"category":null,"content":"IDs 查询 #  使用 ids 查询在 _id 字段中搜索具有一个或多个特定文档 ID 值的文档。例如，以下查询请求 ID 为 34229 和 91296 的文档：\n相关指南（先读这些） #    结构化搜索  Query DSL 基础  GET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;ids\u0026#34;: { \u0026#34;values\u0026#34;: [ 34229, 91296 ] } } } 参数说明 #\n 查询接受以下参数。\n   参数 数据类型 描述     values Array of strings 要搜索的文档 ID。必填。   boost Float 一个浮点值，用于指定此字段相对于相关性分数的权重。值高于 1.0 会增加字段的相关性。值介于 0.0 和 1.0 之间会降低字段的相关性。默认值为 1.0。    典型使用场景 #  批量获取已知文档 #  当你已经知道一组文档的 ID（例如从其他查询结果或外部系统获得），可以直接用 ids 查询来批量检索：\nGET orders/_search { \u0026#34;query\u0026#34;: { \u0026#34;ids\u0026#34;: { \u0026#34;values\u0026#34;: [\u0026#34;order-001\u0026#34;, \u0026#34;order-002\u0026#34;, \u0026#34;order-003\u0026#34;] } } } 与 Bool 查询组合 #  ids 查询可以与其他查询条件组合使用。例如，在一批已知文档中过滤特定状态的订单：\nGET orders/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;ids\u0026#34;: { \u0026#34;values\u0026#34;: [\u0026#34;order-001\u0026#34;, \u0026#34;order-002\u0026#34;, \u0026#34;order-003\u0026#34;, \u0026#34;order-004\u0026#34;] } }, \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;shipped\u0026#34; } } } } } ids 查询 vs. mget API #     对比项 ids 查询 _mget API     返回格式 标准搜索响应（hits） 文档数组   评分 支持 boost 调整相关性 无评分，直接获取文档   可组合性 可嵌入 bool 等复合查询 独立 API，不可组合   适用场景 需要搜索上下文（排序、过滤等） 简单批量获取，性能更优     提示：如果只需要按 ID 获取文档，不需要评分或额外过滤，使用 _mget API 性能更好。\n ","subcategory":null,"summary":"","tags":null,"title":"IDs 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/ids/"},{"category":null,"content":"HTML Strip 字符过滤器 #  html_strip 字符过滤器会从输入文本中移除 HTML 标签（例如 \u0026lt;div\u0026gt;、\u0026lt;p\u0026gt; 和 \u0026lt;a\u0026gt; 等）并输出纯文本。该过滤器可以配置保留某些标签，或者配置把特定的 HTML 标签实体（如 \u0026amp;nbsp;）解码为空格。\n相关指南（先读这些） #    文本分析基础  文本分析：识别词元  参考样例 #  以下请求展示将 html_strip 字符过滤器应用于文本：\nGET /_analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;char_filter\u0026#34;: [ \u0026#34;html_strip\u0026#34; ], \u0026#34;text\u0026#34;: \u0026#34;\u0026lt;p\u0026gt;Commonly used calculus symbols include \u0026amp;alpha;, \u0026amp;beta; and \u0026amp;theta; \u0026lt;/p\u0026gt;\u0026#34; } 返回内容中包含的词元里，可以看到 HTML 字符已被转换为它们的解码后的值：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;\\nCommonly used calculus symbols include α, β and θ \\n\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 74, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } 参数说明 #  html_strip 字符过滤器可以使用以下参数进行配置。\n   参数 必填/可选 数据类型 描述     escaped_tags 可选 字符串数组 一个 HTML 元素名称列表，指定时不带包围的尖括号（\u0026lt; \u0026gt;）。当从文本中去除 HTML 标签时，过滤器不会移除该列表中的元素。例如，将该配置设置为 [\u0026quot;b\u0026quot;, \u0026quot;i\u0026quot;]时, 将防止 \u0026lt;b\u0026gt; 和 \u0026lt;i\u0026gt; 元素被去除。    示例：带有小写过滤器的自定义分词器 #  以下请求创建了一个自定义分词器，该分词器通过使用 html_strip 字符过滤器来去除 HTML 标签，并通过 lowercase 词元过滤器将纯文本转换为小写形式：\nPUT /html_strip_and_lowercase_analyzer { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;html_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;html_strip\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;html_strip_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;html_filter\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } } } 使用以下请求来检查使用该分词器生成的词元：\nGET /html_strip_and_lowercase_analyzer/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;html_strip_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;\u0026lt;h1\u0026gt;Welcome to \u0026lt;strong\u0026gt;Easysearch\u0026lt;/strong\u0026gt;!\u0026lt;/h1\u0026gt;\u0026#34; } 在返回内容中，HTML 标签已被移除，并且纯文本已被转换为小写形式：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;welcome\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;to\u0026#34;, \u0026#34;start_offset\u0026#34;: 12, \u0026#34;end_offset\u0026#34;: 14, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 23, \u0026#34;end_offset\u0026#34;: 42, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } 示例：保留 HTML 标签的自定义分词器 #  以下示例请求创建了一个能保留 HTML 标签的自定义分词器：\nPUT /html_strip_preserve_analyzer { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;html_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;html_strip\u0026#34;, \u0026#34;escaped_tags\u0026#34;: [\u0026#34;b\u0026#34;, \u0026#34;i\u0026#34;] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;html_strip_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;html_filter\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } 使用以下请求来检查使用该分词器生成的词元：\nGET /html_strip_preserve_analyzer/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;html_strip_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;\u0026lt;p\u0026gt;This is a \u0026lt;b\u0026gt;bold\u0026lt;/b\u0026gt; and \u0026lt;i\u0026gt;italic\u0026lt;/i\u0026gt; text.\u0026lt;/p\u0026gt;\u0026#34; } 在返回内容中，正如在自定义分词器请求中所指定的那样，斜体 italic 标签和加粗 bold 标签已被保留。\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;\\nThis is a \u0026lt;b\u0026gt;bold\u0026lt;/b\u0026gt; and \u0026lt;i\u0026gt;italic\u0026lt;/i\u0026gt; text.\\n\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 52, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"HTML 标签字符过滤器（HTML Strip）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/character-filters/html-strip/"},{"category":null,"content":"Has Parent 查询 #  has_parent 查询返回匹配特定查询的父文档的子文档。您可以通过使用连接字段类型在相同索引中的文档之间建立父子关系。\n相关指南（先读这些） #    Parent-Child 建模  关联查询（Joining）   性能注意：has_parent 查询比其他查询慢，因为它执行了连接操作。随着匹配的父文档数量的增加，性能会降低。您搜索中的每个 has_parent 查询都可能显著影响查询性能。如果您优先考虑速度，请避免使用此查询或尽可能限制其使用。更多性能考虑，请参考 Parent-Child 建模章节。\n 参考样例 #  在您运行一个 has_parent 查询之前，您的索引必须包含一个连接字段，以便建立父子关系。索引映射请求使用以下格式：\nPUT /example_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;relationship_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;relations\u0026#34;: { \u0026#34;parent_doc\u0026#34;: \u0026#34;child_doc\u0026#34; } } } } } 对于这个示例，首先配置一个包含代表产品和其品牌的文档的索引，这些文档如查询示例 has_child 中所述。\n要搜索父项的子项，请使用 has_parent 查询。以下查询返回与查询 economy 匹配的品牌生产的子文档（产品）：\nGET testindex1/_search { \u0026#34;query\u0026#34; : { \u0026#34;has_parent\u0026#34;: { \u0026#34;parent_type\u0026#34;:\u0026#34;brand\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;name\u0026#34;: \u0026#34;economy\u0026#34; } } } } } 返回由该品牌生产的所有产品：\n{ \u0026#34;took\u0026#34;: 11, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_routing\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Electronic watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 300, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;2\u0026#34; } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_routing\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Digital watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 100, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;2\u0026#34; } } } ] } } 检索内部命中项 #  返回与查询匹配的父文档，请提供 inner_hits 参数：\nGET testindex1/_search { \u0026#34;query\u0026#34; : { \u0026#34;has_parent\u0026#34;: { \u0026#34;parent_type\u0026#34;:\u0026#34;brand\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;name\u0026#34;: \u0026#34;economy\u0026#34; } }, \u0026#34;inner_hits\u0026#34;: {} } } } 包含父文档在 inner_hits 字段中：\n{ \u0026#34;took\u0026#34;: 11, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_routing\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Electronic watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 300, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;2\u0026#34; } }, \u0026#34;inner_hits\u0026#34;: { \u0026#34;brand\u0026#34;: { \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.3862942, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.3862942, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Economy brand\u0026#34;, \u0026#34;product_to_brand\u0026#34;: \u0026#34;brand\u0026#34; } } ] } } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_routing\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Digital watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 100, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;2\u0026#34; } }, \u0026#34;inner_hits\u0026#34;: { \u0026#34;brand\u0026#34;: { \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.3862942, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.3862942, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Economy brand\u0026#34;, \u0026#34;product_to_brand\u0026#34;: \u0026#34;brand\u0026#34; } } ] } } } } ] } } 参数说明 #  以下表格列出了所有由 has_parent 查询支持的一级参数。\n   参数 必填/可选 描述     parent_type 必填 指定在 join 字段映射中定义的父级关系的名称。   query 必填 在父级文档上运行的查询。如果父级文档与查询匹配，则返回子文档。   ignore_unmapped\t 可选 指示是否忽略未映射的 parent_type 字段，而不是抛出错误而不返回文档。您可以在查询多个索引时提供此参数，其中一些索引可能不包含 parent_type 字段。默认为 false 。   score 可选 Boolean。是否将匹配的父文档的相关性分数汇总到子文档中。false（默认）——忽略父文档分数，子文档分数等于 boost（默认 1）。true——父文档的相关性分数会汇总到子文档的分数中。   inner_hits 可选 如果提供，则返回与查询匹配的底层命中（父文档）。    排序限制 #  has_parent 查询不支持使用标准排序选项对结果进行排序。如果您需要按父文档中的字段对子文档进行排序，可以使用 function_score 查询并按子文档的得分进行排序。\n对于前面的示例，首先添加一个 customer_satisfaction 字段，通过该字段对属于父（品牌）文档的子文档进行排序：\nPUT testindex1/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Luxury watch brand\u0026#34;, \u0026#34;product_to_brand\u0026#34; : \u0026#34;brand\u0026#34;, \u0026#34;customer_satisfaction\u0026#34;: 4.5 } PUT testindex1/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Economy watch brand\u0026quot;, \u0026quot;product_to_brand\u0026quot; : \u0026quot;brand\u0026quot;, \u0026quot;customer_satisfaction\u0026quot;: 3.9 } 现在您可以根据父品牌的 customer_satisfaction 字段对子文档（产品）进行排序。此查询将分数乘以父文档的 customer_satisfaction 字段：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;has_parent\u0026#34;: { \u0026#34;parent_type\u0026#34;: \u0026#34;brand\u0026#34;, \u0026#34;score\u0026#34;: true, \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: \u0026#34;_score * doc[\u0026#39;customer_satisfaction\u0026#39;].value\u0026#34; } } } } } } 包含按最高父级 customer_satisfaction 排序的产品：\n{ \u0026#34;took\u0026#34;: 11, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 4.5, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 4.5, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Mechanical watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 150, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 3.9, \u0026#34;_routing\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Electronic watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 300, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;2\u0026#34; } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34;: 3.9, \u0026#34;_routing\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Digital watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 100, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;2\u0026#34; } } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"Has Parent 查询","url":"/easysearch/main/docs/features/query-dsl/joining/has-parent/"},{"category":null,"content":"Has Child 查询 #  has_child 查询返回匹配特定查询的子文档的父文档。您可以通过使用连接字段类型在相同索引中的文档之间建立父子关系。\n相关指南（先读这些） #    Parent-Child 建模  关联查询（Joining）   性能注意：has_child 查询比其他查询慢，因为它执行了连接操作。随着指向不同父文档的匹配子文档数量的增加，性能会降低。您搜索中的每个 has_child 查询都可能显著影响查询性能。如果您优先考虑速度，请避免使用此查询或尽可能限制其使用。更多性能考虑，请参考 Parent-Child 建模章节。\n 参考样例 #  在您运行一个 has_child 查询之前，您的索引必须包含一个连接字段，以便建立父子关系。索引映射请求使用以下格式：\nPUT /example_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;relationship_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;relations\u0026#34;: { \u0026#34;parent_doc\u0026#34;: \u0026#34;child_doc\u0026#34; } } } } } 在这个例子中，您将配置一个包含代表产品和其品牌的文档的索引。\n首先，创建索引并建立 brand 和 product 之间的父子关系：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;product_to_brand\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;relations\u0026#34;: { \u0026#34;brand\u0026#34;: \u0026#34;product\u0026#34; } } } } } 创建两个父（品牌）文档：\nPUT testindex1/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Luxury brand\u0026#34;, \u0026#34;product_to_brand\u0026#34; : \u0026#34;brand\u0026#34; } PUT testindex1/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Economy brand\u0026quot;, \u0026quot;product_to_brand\u0026quot; : \u0026quot;brand\u0026quot; } 索引三个子（产品）文档：\nPUT testindex1/_doc/3?routing=1 { \u0026#34;name\u0026#34;: \u0026#34;Mechanical watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 150, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } PUT testindex1/_doc/4?routing=2 { \u0026quot;name\u0026quot;: \u0026quot;Electronic watch\u0026quot;, \u0026quot;sales_count\u0026quot;: 300, \u0026quot;product_to_brand\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;product\u0026quot;, \u0026quot;parent\u0026quot;: \u0026quot;2\u0026quot; } }\nPUT testindex1/_doc/5?routing=2 { \u0026quot;name\u0026quot;: \u0026quot;Digital watch\u0026quot;, \u0026quot;sales_count\u0026quot;: 100, \u0026quot;product_to_brand\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;product\u0026quot;, \u0026quot;parent\u0026quot;: \u0026quot;2\u0026quot; } } 要搜索子文档的父文档，请使用一个 has_child 查询。以下查询返回制造手表的父文档（品牌）：\nGET testindex1/_search { \u0026#34;query\u0026#34; : { \u0026#34;has_child\u0026#34;: { \u0026#34;type\u0026#34;:\u0026#34;product\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;name\u0026#34;: \u0026#34;watch\u0026#34; } } } } } 返回了两个品牌：\n{ \u0026#34;took\u0026#34;: 15, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Luxury brand\u0026#34;, \u0026#34;product_to_brand\u0026#34;: \u0026#34;brand\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Economy brand\u0026#34;, \u0026#34;product_to_brand\u0026#34;: \u0026#34;brand\u0026#34; } } ] } } 检索内部命中项 #  返回与查询匹配的子文档，请提供 inner_hits 参数：\nGET testindex1/_search { \u0026#34;query\u0026#34; : { \u0026#34;has_child\u0026#34;: { \u0026#34;type\u0026#34;:\u0026#34;product\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;name\u0026#34;: \u0026#34;watch\u0026#34; } }, \u0026#34;inner_hits\u0026#34;: {} } } } 包含子文档在 inner_hits 字段中：\n{ \u0026#34;took\u0026#34;: 52, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Luxury brand\u0026#34;, \u0026#34;product_to_brand\u0026#34;: \u0026#34;brand\u0026#34; }, \u0026#34;inner_hits\u0026#34;: { \u0026#34;product\u0026#34;: { \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.53899646, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 0.53899646, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Mechanical watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 150, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } } ] } } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Economy brand\u0026#34;, \u0026#34;product_to_brand\u0026#34;: \u0026#34;brand\u0026#34; }, \u0026#34;inner_hits\u0026#34;: { \u0026#34;product\u0026#34;: { \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.53899646, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 0.53899646, \u0026#34;_routing\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Electronic watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 300, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;2\u0026#34; } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34;: 0.53899646, \u0026#34;_routing\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Digital watch\u0026#34;, \u0026#34;sales_count\u0026#34;: 100, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;2\u0026#34; } } } ] } } } } ] } } 参数说明 #  以下表格列出了所有由 has_child 查询支持的一级参数。\n   参数 必填/可选 描述     type 必填 指定在 join 字段映射中定义的子关系名称。   query 必填 在子文档上运行的查询。如果子文档与查询匹配，则返回父文档。   ignore_unmapped\t 可选 指示是否忽略未映射的 type 字段，而不是抛出错误而不返回文档。您可以在查询多个索引时提供此参数，其中一些索引可能不包含 type 字段。默认为 false 。   max_children 可选 父文档匹配子文档的最大数量。如果超过此数量，父文档将不会出现在搜索结果中。   min_children 可选 父文档要包含在结果中所需的最小子文档匹配数量。如果未达到此要求，父文档将被排除。默认值为 1 。   score_mode 可选 定义匹配子文档的分数如何影响父文档的分数。有效值有：\n- none : 忽略子文档的相关性分数，并将分数 0 分配给父文档。\n- avg : 使用所有匹配子文档的平均相关性分数。\n- max ：将匹配的子文档中的最高相关性得分分配给父文档。\n- min ：将匹配的子文档中的最低相关性得分分配给父文档。\n- sum ：将所有匹配的子文档的相关性得分相加。\n默认为 none 。   inner_hits 可选 如果提供，则返回与查询匹配的基础命中（子文档）。    排序限制 #  has_child 查询不支持使用标准排序选项对结果进行排序。如果您需要按子文档中的字段对父文档进行排序，可以使用 function_score 查询并按父文档的得分进行排序。\n在前面示例中，您可以根据子产品（品牌）的 sales_count 对父文档进行排序。此查询将分数乘以子文档的 sales_count 字段，并将匹配的子文档中的最高相关性分数分配给父文档：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;has_child\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: \u0026#34;_score * doc[\u0026#39;sales_count\u0026#39;].value\u0026#34; } } }, \u0026#34;score_mode\u0026#34;: \u0026#34;max\u0026#34; } } } 响应包含按最高子 sales_count 排序的品牌：\n{ \u0026#34;took\u0026#34;: 6, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 300, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 300, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Economy brand\u0026#34;, \u0026#34;product_to_brand\u0026#34;: \u0026#34;brand\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 150, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Luxury brand\u0026#34;, \u0026#34;product_to_brand\u0026#34;: \u0026#34;brand\u0026#34; } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"Has Child 查询","url":"/easysearch/main/docs/features/query-dsl/joining/has-child/"},{"category":null,"content":"Gsub 处理器 #  gsub 处理器在传入的文档中对字符串字段执行正则表达式搜索和替换操作。如果字段包含字符串数组，则操作应用于数组中的所有元素。然而，如果字段包含非字符串值，处理器将抛出异常。gsub 处理器的用例包括从日志消息或用户生成的内容中删除敏感信息、规范化数据格式或约定（例如，转换日期格式、删除特殊字符），以及从字段值中提取或转换子字符串以进行进一步处理或分析。\n以下是为 gsub 处理器提供的语法：\n\u0026#34;gsub\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_name\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;regex_pattern\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;replacement_string\u0026#34; } 配置参数 #  下表列出了 gsub 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 要应用替换的字段。   pattern 必填 要替换的模式。   replacement 必填 将替换匹配模式的字符串。   target_field 可选 要存储解析数据的字段名称。如果未指定 target_field ，则解析数据将替换 field 字段中的原始数据。默认为 field 。   if 可选 处理器运行的条件。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。默认为 false 。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 gsub_pipeline 的管道，该管道使用 gsub 处理器将 message 字段中所有出现的单词 error 替换为单词 warning ：\nPUT _ingest/pipeline/gsub_pipeline { \u0026#34;description\u0026#34;: \u0026#34;Replaces \u0026#39;error\u0026#39; with \u0026#39;warning\u0026#39; in the \u0026#39;message\u0026#39; field\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;gsub\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;warning\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/gsub_pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;This is an error message\u0026#34; } } ] } 返回内容确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;This is an warning message\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-05-22T19:47:00.645687211Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 logs 的索引中：\nPUT logs/_doc/1?pipeline=gsub_pipeline { \u0026#34;message\u0026#34;: \u0026#34;This is an error message\u0026#34; } 以下响应显示，请求将文档索引到名为 logs 的索引中，并且 gsub 处理器将 message 字段中所有出现的单词 error 替换为单词 warning ：\n{ \u0026#34;_index\u0026#34;: \u0026#34;logs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET logs/_doc/1 以下响应显示了具有修改后的 message 字段值的文档：\n{ \u0026#34;_index\u0026#34;: \u0026#34;logs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;This is an warning message\u0026#34; } } ","subcategory":null,"summary":"","tags":null,"title":"Gsub 处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/gsub/"},{"category":null,"content":"Fuzzy 查询 #  fuzzy 查询用于搜索包含与搜索词相似的词条的文档，相似度在允许的最大 Damerau-Levenshtein 距离范围内。Damerau-Levenshtein 距离衡量将一个词条变为另一个词条所需的一字符变化的数量。这些变化包括：\n相关指南（先读这些） #     文本分析：模糊匹配\n   全文搜索\n  Replacements: 替换，cat 变为 bat\n  Insertions: 插入，cat 变为 cats\n  Deletions: 删除，cat 变为 at\n  Transpositions: 转换，cat 变为 act\n  模糊查询会生成一个包含所有可能扩展的搜索词列表，这些扩展在 Damerau-Levenshtein 距离内。你可以在 max_expansions 字段中指定此类扩展的最大数量。查询然后会搜索匹配任何扩展的文档。如果你将 transpositions 参数设置为 false ，则搜索将使用经典的 Levenshtein 距离。\n以下示例查询搜索发言者 HALET （误写为 HAMLET ）。未指定最大编辑距离，因此使用默认的 AUTO 编辑距离：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;fuzzy\u0026#34;: { \u0026#34;speaker\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;HALET\u0026#34; } } } } 返回内容包含所有发言者为 HAMLET 的文档。\n以下示例查询使用高级参数搜索单词 HALET ：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;fuzzy\u0026#34;: { \u0026#34;speaker\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;HALET\u0026#34;, \u0026#34;fuzziness\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;max_expansions\u0026#34;: 40, \u0026#34;prefix_length\u0026#34;: 0, \u0026#34;transpositions\u0026#34;: true, \u0026#34;rewrite\u0026#34;: \u0026#34;constant_score\u0026#34; } } } } 参数说明 #\n 查询接受字段名称（  ）作为顶级参数：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;fuzzy\u0026#34;: { \u0026#34;\u0026lt;field\u0026gt;\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;sample\u0026#34;, ... } } } } \u0026lt;field\u0026gt; 接受以下参数。除了 value 之外，所有参数都是可选的。\n   参数 数据类型 描述     value String 在 \u0026lt;field\u0026gt; 指定的字段中搜索的词项。   boost Float 一个浮点数，用于指定该字段对相关性评分的权重。值大于 1.0 会增加字段的相关性。值在 0.0 到 1.0 之间会降低字段的相关性。默认值为 1.0。   fuzziness AUTO 、 0 或正整数 在确定一个词项是否匹配某个值时，将一个词变为另一个词所需的字符编辑次数（插入、删除、替换）。例如， wined 和 wind 之间的距离为 1。默认值 AUTO 会根据每个词项的长度选择一个值，并且对于大多数用例来说是一个不错的选择。   max_expansions Positive integer 查询可以扩展的最大项数。模糊查询会扩展到在指定距离 fuzziness 内的匹配项。然后 Easysearch 尝试匹配这些项。默认值为 50 。   prefix_length Positive integer 不考虑在模糊匹配中计算的前导字符数。默认值为 0 。   transpositions Boolean 是否将相邻字符互换（如 ab → ba）计为单次编辑。true 使用 Damerau-Levenshtein 距离，false 使用经典 Levenshtein 距离。默认值为 true。   rewrite String 确定 Easysearch 如何重写和评分多词查询。有效值为 constant_score 、 scoring_boolean 、 constant_score_boolean 、 top_terms_N 、 top_terms_boost_N 和 top_terms_blended_freqs_N 。默认值为 constant_score 。     在 max_expansions 中指定较大的值可能会导致性能下降，特别是在 prefix_length 设置为 0 时，因为 Easysearch 尝试匹配的单词变体数量会很大。\n  如果将 search.allow_expensive_queries 设置为 false，则不会执行模糊查询。\n ","subcategory":null,"summary":"","tags":null,"title":"Fuzzy 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/fuzzy/"},{"category":null,"content":"Function Score 查询 #  如果您需要更改结果中返回的文档的相关性评分，请使用 function_score 查询。function_score 查询定义了一个查询和一个或多个函数，这些函数可以应用于所有结果或结果的一部分，以重新计算它们的相关性评分。\n相关指南（先读这些） #    查询 DSL 基础  相关性：加权与调参  使用一个评分函数 #  最基础的 function_score 查询示例使用一个函数来重新计算分数。以下查询使用一个 weight 函数将所有相关性分数加倍。此函数适用于所有结果文档，因为没有在 function_score 中指定 query 参数：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;weight\u0026#34;: \u0026#34;2\u0026#34; } } } 将评分函数应用于文档子集 #  要将评分函数应用于文档子集，在函数中提供一个查询：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34; } }, \u0026#34;weight\u0026#34;: \u0026#34;2\u0026#34; } } } 支持的功能 #  function_score 查询类型支持以下功能：\n 内置:  weight ：将文档得分乘以一个预定义的增强因子。 random_score ：提供一个对单个用户一致的随机得分，但不同用户之间得分不同。 field_value_factor : 使用指定文档字段的值来重新计算分数。 衰减函数（ gauss 、 exp 和 linear ）：使用指定的衰减函数重新计算分数。   自定义： script_score : 使用脚本对文档进行评分。  权重函数 #  当您使用 weight 函数时，原始的相关性分数会乘以 weight 的浮点值：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;weight\u0026#34;: \u0026#34;2\u0026#34; } } } 与 boost 值不同， weight 函数没有进行归一化。\n随机分数函数 #  random_score 函数提供一致的随机分数，但不同用户之间分数不同。该分数是一个位于 [0, 1) 范围内的浮点数。默认情况下， random_score 函数使用内部 Lucene 文档 ID 作为种子值，由于合并后文档可以重新编号，因此随机值不可重现。为了在生成随机值时保持一致性，您可以提供 seed 和 field 参数。 field 必须是已启用 fielddata 的字段（通常是一个数值字段）。分数是使用 seed ， fielddata 的值，以及使用索引名称和分片 ID 计算的盐值来计算的。由于索引名称和分片 ID 对于位于同一分片中的文档是相同的，因此具有相同 field 值的文档将被分配相同的分数。为了确保同一分片中的所有文档都有不同的分数，请使用具有所有文档唯一值的 field 。一个选项是使用 _seq_no 字段。但是，如果您选择此字段，由于相应的 _seq_no 更新，分数可能会发生变化。\n以下查询使用 random_score 函数与 seed 和 field ：\nGET blogs/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;random_score\u0026#34;: { \u0026#34;seed\u0026#34;: 20, \u0026#34;field\u0026#34;: \u0026#34;_seq_no\u0026#34; } } } } 字段值因子函数 #  field_value_factor 函数使用指定文档字段的值重新计算分数。如果该字段是多值字段，则仅使用其第一个值进行计算，其他值不考虑。\nfield_value_factor 函数支持以下选项：\n field : 用于评分计算的字段。 factor ：一个可选的系数，用于乘以字段值。默认为 1。 modifier : 应用于字段值 v 的修饰符之一。下表列出了所有支持的修饰符。     修改符 公式 描述     log $ \\log v $ 取值的 10 为底的对数。对非正数取对数是非法操作，将导致错误。对于介于 0（不含）和 1（含）之间的值，此函数返回非负值，将导致错误。我们建议使用 log1p 或 log2p 代替 log 。   log1p $ \\log （1+v） $ 取 1 与值的和的 10 为底的对数。   log2p $ \\log （2+v） $ 取 2 和值的和的 10 为底的对数。   ln $ \\ln v $ 取值的自然对数。对非正数取对数是非法操作，将导致错误。对于介于 0（不含）和 1（含）之间的值，此函数返回非负值，将导致错误。我们建议使用 ln1p 或 ln2p 代替 ln 。   ln1p $ \\ln (1+v) $ 取 1 和值的和的自然对数。   reciprocal $\\frac{1}{v}$ 取值的倒数。   square $v^2$ 值的平方。   sqrt $\\sqrt{v}$ 取值的平方根。对负数取平方根是非法操作，会导致错误。确保 v 是非负数。   none N/A 不应用任何修饰符。     missing ：如果字段在文档中缺失时使用的值。 factor 和 modifier 将应用于此值，而不是缺失字段的值。  例如，以下查询使用 field_value_factor 函数给 views 字段赋予更高的权重：\nGET blogs/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;views\u0026#34;, \u0026#34;factor\u0026#34;: 1.5, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34;, \u0026#34;missing\u0026#34;: 1 } } } } 前一个查询使用以下公式计算相关性得分：\n$\\text{score} = \\text{original score} \\cdot \\log(1 + 1.5 \\cdot \\text{views})$\n脚本得分函数 #  使用 script_score 函数，您可以编写一个用于评分文档的自定义脚本，可选地结合文档中字段的值。原始的相关性得分可以在 _score 变量中访问。\n 计算出的得分不能为负。负得分将导致错误。文档得分具有正的 32 位浮点值。精度更高的得分将转换为最接近的 32 位浮点数。\n 例如，以下查询使用 script_score 函数根据原始分数以及博客文章的观看次数和点赞数来计算分数。为了给观看次数和点赞数赋予较小的权重，此公式取观看次数和点赞数之和的对数。为了使对数在观看次数和点赞数为 0 时仍然有效，将 1 加到它们的和上：\nGET blogs/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: {\u0026#34;match\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;easysearch\u0026#34;}}, \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: \u0026#34;_score * Math.log(1 + doc[\u0026#39;likes\u0026#39;].value + doc[\u0026#39;views\u0026#39;].value)\u0026#34; } } } } 脚本被编译并缓存以提高性能。因此，最好重用相同的脚本并传递脚本所需的任何参数：\nGET blogs/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;easysearch\u0026#34; } }, \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;params\u0026#34;: { \u0026#34;add\u0026#34;: 1 }, \u0026#34;source\u0026#34;: \u0026#34;_score * Math.log(params.add + doc[\u0026#39;likes\u0026#39;].value + doc[\u0026#39;views\u0026#39;].value)\u0026#34; } } } } } 衰减函数 #  对于许多应用，您需要根据邻近度或时间顺序对结果进行排序。您可以使用衰减函数来实现这一点。衰减函数通过三种衰减曲线之一（高斯、指数或线性）计算文档得分。\n 衰减函数仅对数值、日期和地理点字段起作用。\n 示例：地理点字段 #  假设您正在寻找一家靠近办公室的酒店。您创建一个将 location 字段映射为地理点的 hotels 索引：\nPUT hotels { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; } } } } 您索引了两个与附近酒店对应的文档：\nPUT hotels/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Hotel Within 200\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.7105, \u0026#34;lon\u0026#34;: 74.00 } } PUT hotels/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Hotel Outside 500\u0026quot;, \u0026quot;location\u0026quot;: { \u0026quot;lat\u0026quot;: 40.7115, \u0026quot;lon\u0026quot;: 74.00 } } origin 定义了计算距离的起点（办公室位置）。 offset 指定了距离原点一定范围内文档获得满分的距离。您可以给距离办公室 200 英尺内的酒店相同的最高分。 scale 定义了图形的衰减率， decay 定义了从原点 scale + offset 距离处分配给文档的分数。一旦超出 200 英尺的半径，您可能决定如果再走 300 英尺才能到达酒店（ scale = 300 英尺），则将其分配的分数减为原来的四分之一（ decay = 0.25）。\n您使用 origin 在（74.00，40.71）处创建以下查询：\nGET hotels/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;functions\u0026#34;: [ { \u0026#34;exp\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;origin\u0026#34;: \u0026#34;40.71,74.00\u0026#34;, \u0026#34;offset\u0026#34;: \u0026#34;200ft\u0026#34;, \u0026#34;scale\u0026#34;: \u0026#34;300ft\u0026#34;, \u0026#34;decay\u0026#34;: 0.25 } } } ] } } } 返回内容包含两家酒店。距离办公室 200 英尺内的酒店得分为 1，而距离 500 英尺半径外的酒店得分为 0.20，这低于 decay 参数的 0.25：\n{ \u0026#34;took\u0026#34;: 854, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;hotels\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Hotel Within 200\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.7105, \u0026#34;lon\u0026#34;: 74 } } }, { \u0026#34;_index\u0026#34;: \u0026#34;hotels\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.20099315, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Hotel Outside 500\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.7115, \u0026#34;lon\u0026#34;: 74 } } } ] } } 参数说明 #  以下表格列出了 gauss 、 exp 和 linear 函数支持的所有参数。\n   参数 描述     origin 计算距离的起点。对于数值字段，必须提供数字；对于日期字段，必须提供日期；对于地理点字段，必须提供地理点。地理点和数值字段是必需的。对于日期字段是可选的（默认为 now ）。对于日期字段，支持日期计算（例如， now-2d ）。   offset 定义了给予文档 1 分的起始距离。可选。默认为 0。   scale 距离 scale + offset 的 origin 文档被分配 decay 分。必需。\n对于数值字段， scale 可以是任何数字。\n对于日期字段， scale 可以定义为带有单位的数字（ 5h ， 1d ）。如果没有提供单位， scale 默认为毫秒。\n对于地理点字段， scale 可以定义为带有单位的数字（ 1mi ， 5km ）。如果没有提供单位， scale 默认为米。   decay 定义距离 origin 的 scale + offset 处的文档得分。可选。默认值为 0.5。     对于文档中缺失的字段，衰减函数返回得分为 1。\n 示例：数值字段 #  以下查询使用指数衰减函数根据评论数量优先排序博客文章：\nGET blogs/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;functions\u0026#34;: [ { \u0026#34;exp\u0026#34;: { \u0026#34;comments\u0026#34;: { \u0026#34;origin\u0026#34;: \u0026#34;20\u0026#34;, \u0026#34;offset\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;scale\u0026#34;: \u0026#34;10\u0026#34; } } } ] } } } 结果中的前两篇博客文章得分为 1，因为一篇位于原点（20），另一篇距离为 16，这处于偏移量内（计算得到满分文档的范围为 20 5，即[15, 25]）。第三篇博客文章距离 origin （20 − （5 + 10）= 15）为 scale + offset ，因此它被赋予默认的 decay 得分（0.5）：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Semantic search in Easysearch\u0026#34;, \u0026#34;views\u0026#34;: 1200, \u0026#34;likes\u0026#34;: 150, \u0026#34;comments\u0026#34;: 16, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-04-17\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Get started with Easysearch 2.7\u0026#34;, \u0026#34;views\u0026#34;: 1400, \u0026#34;likes\u0026#34;: 100, \u0026#34;comments\u0026#34;: 20, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-05-02\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 0.5, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Distributed tracing with Data Prepper\u0026#34;, \u0026#34;views\u0026#34;: 800, \u0026#34;likes\u0026#34;: 50, \u0026#34;comments\u0026#34;: 5, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-04-25\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 0.4352753, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;A very old blog\u0026#34;, \u0026#34;views\u0026#34;: 100, \u0026#34;likes\u0026#34;: 20, \u0026#34;comments\u0026#34;: 3, \u0026#34;date_posted\u0026#34;: \u0026#34;2000-04-25\u0026#34; } } ] } } 示例：日期字段 #  以下查询使用高斯衰减函数来优先显示发布于 2002 年 4 月 24 日左右的博客文章：\nGET blogs/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;functions\u0026#34;: [ { \u0026#34;gauss\u0026#34;: { \u0026#34;date_posted\u0026#34;: { \u0026#34;origin\u0026#34;: \u0026#34;2022-04-24\u0026#34;, \u0026#34;offset\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;scale\u0026#34;: \u0026#34;6d\u0026#34;, \u0026#34;decay\u0026#34;: 0.25 } } } ] } } } 在结果中，第一篇博客文章是在 4 月 24 日前后一天发布的，因此它具有最高的分数 1。第二篇博客文章是在 4 月 17 日发布的，它在 offset + scale （ 1d + 6d ）的范围内，因此得分等于 decay （0.25）。第三篇博客文章是在 4 月 24 日之后 7 天以上发布的，因此得分较低。最后一篇博客文章的得分为 0，因为它是在多年前发布的：\n{ \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Distributed tracing with Data Prepper\u0026#34;, \u0026#34;views\u0026#34;: 800, \u0026#34;likes\u0026#34;: 50, \u0026#34;comments\u0026#34;: 5, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-04-25\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.25, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Semantic search in Easysearch\u0026#34;, \u0026#34;views\u0026#34;: 1200, \u0026#34;likes\u0026#34;: 150, \u0026#34;comments\u0026#34;: 16, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-04-17\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.15154076, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Get started with Easysearch 2.7\u0026#34;, \u0026#34;views\u0026#34;: 1400, \u0026#34;likes\u0026#34;: 100, \u0026#34;comments\u0026#34;: 20, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-05-02\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 0, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;A very old blog\u0026#34;, \u0026#34;views\u0026#34;: 100, \u0026#34;likes\u0026#34;: 20, \u0026#34;comments\u0026#34;: 3, \u0026#34;date_posted\u0026#34;: \u0026#34;2000-04-25\u0026#34; } } ] } } 多值字段 #  如果指定的衰减计算字段包含多个值，可以使用 multi_value_mode 参数。此参数指定以下函数之一，以确定用于计算的字段值：\n min ：（默认）距离 origin 的最小距离。 max ：距离 origin 的最大距离。 avg ：距离 origin 的平均值。 sum ：从 origin 计算出的所有距离之和。  例如，您使用一个距离数组索引文档：\nPUT testindex/_doc/1 { \u0026#34;distances\u0026#34;: [1, 2, 3, 4, 5] } 以下查询使用多值字段 distances 的 max 距离来计算衰减：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;functions\u0026#34;: [ { \u0026#34;exp\u0026#34;: { \u0026#34;distances\u0026#34;: { \u0026#34;origin\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;offset\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;scale\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;multi_value_mode\u0026#34;: \u0026#34;max\u0026#34; } } ] } } } 由于从原点（1）的最大距离在 offset 内，该文档被赋予分数 1：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;distances\u0026#34;: [ 1, 2, 3, 4, 5 ] } } ] } } 衰减曲线计算 #\n 以下公式定义了各种衰减函数的得分计算（ v 表示文档字段值）。\n高斯：\n$\\text{score} = \\exp\\left(-\\frac{(\\max(0, |v - \\text{origin}| - \\text{offset}))^2}{2\\sigma^2}\\right)$\n其中 sigma 计算以确保得分在距离 origin 的 offset + scale 处等于 decay ：\n$\\sigma^2 = -\\frac{\\text{scale}^2}{2\\ln(\\text{decay})}$\n指数：\n$\\text{score} = \\exp(\\lambda \\cdot \\max(0, |v - \\text{origin}| - \\text{offset}))$\nlambda 是计算以确保在距离 origin 的 offset + scale 处的分数等于 decay ：\n$\\lambda = \\frac{\\ln(\\text{decay})}{\\text{scale}}$\n线性:\n$\\text{score} = \\max\\left( \\frac{s - \\max(0, |v - \\text{origin}| - \\text{offset})}{s} \\right)$\ns 是计算以确保在距离 origin 的 offset + scale 处的分数等于 decay ：\n$s = \\frac{\\text{scale}}{1 - \\text{decay}}$\n使用多个评分函数 #  您可以在函数得分查询中通过在 functions 数组中列出它们来指定多个得分函数。\n组合多个函数的得分 #  不同的函数可以使用不同的评分尺度。例如， random_score 函数提供一个介于 0 和 1 之间的得分，但 field_value_factor 没有具体的评分尺度。此外，您可能希望对不同函数给出的得分进行不同的加权。为了调整不同函数的得分，您可以指定每个函数的 weight 参数。然后，每个函数给出的得分乘以 weight 以产生该函数的最终得分。 weight 参数必须在 functions 数组中提供，以便与加权函数区分，\n每个函数给出的得分通过 score_mode 参数进行组合，该参数可以取以下值：\n multiply : （默认）分数相乘。 sum : 分数相加。 avg : 分数取平均值。如果指定了 weight ，则这是一个加权平均值。例如，如果第一个函数的权重 1 返回分数 10 ，第二个函数的权重 4 返回分数 20 ，则平均值为 18 first : 取第一个匹配过滤器的函数的分数。 max : 取最大分值。 min : 取最小分值。  指定分值的上限 #  您可以在 max_boost 参数中指定函数分值的上限。默认上限是 float 值的最大幅度：(2 − 2 ^−23 ) · 2 ^127 。\n将所有函数的得分与查询得分结合 #  您可以在 boost_mode 参数中指定使用所有函数计算得出的得分如何与查询得分结合，该参数可以取以下值之一：\n multiply ：(默认) 将查询得分乘以函数得分。 replace ：忽略查询得分，使用函数得分。 sum : 将查询得分和函数得分相加。 avg : 计算查询得分和函数得分的平均值。 max : 取查询得分和函数得分中的较大值。 min : 取查询得分和函数得分中的较小值。  过滤未达到阈值的文档 #  更改相关性分数不会改变匹配文档的列表。要排除一些未达到阈值的文档，请在 min_score 参数中指定阈值值。然后使用该阈值值对所有查询返回的文档进行评分和过滤。\n参考样例 #  以下请求搜索包含“Easysearch Data Prepper”一词的博客文章，优先考虑发布于 2022 年 4 月 24 日左右的帖子。此外，还会考虑查看次数和点赞数。最后，将截止阈值设置为 10 分：\nGET blogs/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;boost\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;functions\u0026#34;: [ { \u0026#34;gauss\u0026#34;: { \u0026#34;date_posted\u0026#34;: { \u0026#34;origin\u0026#34;: \u0026#34;2022-04-24\u0026#34;, \u0026#34;offset\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;scale\u0026#34;: \u0026#34;6d\u0026#34; } }, \u0026#34;weight\u0026#34;: 1 }, { \u0026#34;gauss\u0026#34;: { \u0026#34;likes\u0026#34;: { \u0026#34;origin\u0026#34;: 200, \u0026#34;scale\u0026#34;: 200 } }, \u0026#34;weight\u0026#34;: 4 }, { \u0026#34;gauss\u0026#34;: { \u0026#34;views\u0026#34;: { \u0026#34;origin\u0026#34;: 1000, \u0026#34;scale\u0026#34;: 800 } }, \u0026#34;weight\u0026#34;: 2 } ], \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;easysearch data prepper\u0026#34; } }, \u0026#34;max_boost\u0026#34;: 10, \u0026#34;score_mode\u0026#34;: \u0026#34;max\u0026#34;, \u0026#34;boost_mode\u0026#34;: \u0026#34;multiply\u0026#34;, \u0026#34;min_score\u0026#34;: 10 } } } 结果包含三篇匹配的博客文章：\n{ \u0026#34;took\u0026#34;: 14, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 31.191923, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 31.191923, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Distributed tracing with Data Prepper\u0026#34;, \u0026#34;views\u0026#34;: 800, \u0026#34;likes\u0026#34;: 50, \u0026#34;comments\u0026#34;: 5, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-04-25\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 13.907352, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Semantic search in Easysearch\u0026#34;, \u0026#34;views\u0026#34;: 1200, \u0026#34;likes\u0026#34;: 150, \u0026#34;comments\u0026#34;: 16, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-04-17\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;blogs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 11.150461, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Get started with Easysearch 2.7\u0026#34;, \u0026#34;views\u0026#34;: 1400, \u0026#34;likes\u0026#34;: 100, \u0026#34;comments\u0026#34;: 20, \u0026#34;date_posted\u0026#34;: \u0026#34;2022-05-02\u0026#34; } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"Function Score 查询","url":"/easysearch/main/docs/features/query-dsl/compound-query/function-score/"},{"category":null,"content":"Exists 查询 #  使用 exists 查询来搜索包含特定字段的文档。\n相关指南（先读这些） #    结构化搜索  Query DSL 基础  如果出现以下任一情况，索引值将不会存在于文档字段中：\n 该字段在映射中指定了 \u0026quot;index\u0026quot; : false 。 源 JSON 中的字段为 null 或 [] 。 字段值的长度超过了映射中 ignore_above 的设置。 字段值格式错误，并且映射中定义了 ignore_malformed 。  索引值将在以下情况下存在于文档字段中：\n 该值是一个包含一个或多个 null 元素和一个或多个非 null 元素的数组（例如， [\u0026quot;one\u0026quot;, null] ）。 该值是一个空字符串（ \u0026quot;\u0026quot; 或 \u0026quot;-\u0026quot; ）。 该值是一个自定义的 null_value ，如字段映射中所定义。  参考样例 #  例如，假设索引包含以下两个文档：\nPUT testindex/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } PUT testindex/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Gone with the wind\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;A 1939 American epic historical film\u0026quot; } 以下查询搜索包含 description 字段的文档：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;description\u0026#34; } } } 返回内容包含匹配的文档：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Gone with the wind\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A 1939 American epic historical film\u0026#34; } } ] } } 查找索引值缺失的文档 #  要查找缺少索引值的文档，可以使用 must_not 布尔查询结合内部 exists 查询。例如，以下请求将查找 description 字段缺失的文档：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must_not\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;description\u0026#34; } } } } } 返回内容包含匹配的文档：\n{ \u0026#34;took\u0026#34;: 19, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;The wind rises\u0026#34; } } ] } } 参数说明 #     参数 数据类型 描述     field String 要检查是否存在的字段名称。必填。   boost Float 一个浮点数，用于指定该字段对相关性评分的权重。值大于 1.0 会增加字段的相关性。值在 0.0 到 1.0 之间会降低字段的相关性。默认值为 1.0。    ","subcategory":null,"summary":"","tags":null,"title":"Exists 查询","url":"/easysearch/main/docs/features/query-dsl/term-based-query/exists/"},{"category":null,"content":"Distance Feature 查询 #  使用 distance_feature 查询来提升与特定日期或地理位置更近的文档的相关性。这可以帮助你在搜索结果中优先显示更近期的或附近的内容。例如，你可以为近期生产的产品分配更高的权重，或提升最接近用户指定位置的项目。\n你可以将此查询应用于包含日期或位置数据的字段。它通常用于 bool 查询的 should 子句中，以改进相关性评分而不过滤掉结果。\n相关指南（先读这些） #    相关性与打分策略  地理位置搜索  Query DSL 基础  配置索引 #  在使用 distance_feature 查询之前，请确保您的索引至少包含以下字段类型之一：date,date_nanos,geo_point\n在此示例中，您将配置 opening_date 和 coordinates 字段，用于运行距离特征查询：\nPUT /stores { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;opening_date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;coordinates\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; } } } } 向索引中添加示例文档：\nPUT /stores/_doc/1 { \u0026#34;store_name\u0026#34;: \u0026#34;Green Market\u0026#34;, \u0026#34;opening_date\u0026#34;: \u0026#34;2025-03-10\u0026#34;, \u0026#34;coordinates\u0026#34;: [74.00, 40.70] } PUT /stores/_doc/2 { \u0026quot;store_name\u0026quot;: \u0026quot;Fresh Foods\u0026quot;, \u0026quot;opening_date\u0026quot;: \u0026quot;2025-04-01\u0026quot;, \u0026quot;coordinates\u0026quot;: [73.98, 40.75] }\nPUT /stores/_doc/3 { \u0026quot;store_name\u0026quot;: \u0026quot;City Organics\u0026quot;, \u0026quot;opening_date\u0026quot;: \u0026quot;2021-04-20\u0026quot;, \u0026quot;coordinates\u0026quot;: [74.02, 40.68] } 示例：根据时效性提升分数 #\n 以下查询搜索匹配 store_name 的文档，并提升最近打开的商店的分数：\nGET /stores/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;store_name\u0026#34;: \u0026#34;market\u0026#34; } }, \u0026#34;should\u0026#34;: { \u0026#34;distance_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;opening_date\u0026#34;, \u0026#34;origin\u0026#34;: \u0026#34;2025-04-07\u0026#34;, \u0026#34;pivot\u0026#34;: \u0026#34;10d\u0026#34; } } } } } 结果包含匹配的文档：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.2372394, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;stores\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.2372394, \u0026#34;_source\u0026#34;: { \u0026#34;store_name\u0026#34;: \u0026#34;Green Market\u0026#34;, \u0026#34;opening_date\u0026#34;: \u0026#34;2025-03-10\u0026#34;, \u0026#34;coordinates\u0026#34;: [ 74, 40.7 ] } } ] } } 根据地理邻近度提升分数 #\n 以下查询搜索匹配 store_name 的文档，并提升更接近给定原点的结果：\nGET /stores/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;store_name\u0026#34;: \u0026#34;market\u0026#34; } }, \u0026#34;should\u0026#34;: { \u0026#34;distance_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;coordinates\u0026#34;, \u0026#34;origin\u0026#34;: [74.00, 40.71], \u0026#34;pivot\u0026#34;: \u0026#34;500m\u0026#34; } } } } } 结果包含匹配的文档：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.2910118, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;stores\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.2910118, \u0026#34;_source\u0026#34;: { \u0026#34;store_name\u0026#34;: \u0026#34;Green Market\u0026#34;, \u0026#34;opening_date\u0026#34;: \u0026#34;2025-03-10\u0026#34;, \u0026#34;coordinates\u0026#34;: [ 74, 40.7 ] } } ] } } 参数说明 #  下表列出了 distance_feature 查询支持的所有顶层参数。\n   参数 必需/可选 描述     field 必需 计算距离所使用的字段的名称。必须是 date 、 date_nanos 或 geo_point 字段，具有 index: true （默认值）和 doc_values: true （默认值）。   origin 必需 用于计算距离的起点。对于 date 字段，使用日期或日期数学表达式（例如， now-1h ）；对于 geo_point 字段，使用地理点。   pivot 必需 在距离 origin 处，匹配文档的分数获得 boost 值的一半。对于日期字段，使用时间单位（例如， 10d ）；对于地理字段，使用距离单位（例如， 1km ）。更多信息，请参阅单位。   boost 可选 匹配文档的相关性分数的乘数。必须是非负浮点数。默认值是 1.0 。    跳过非竞争性命中 #  与 function_score 查询等其他修改得分的查询不同， distance_feature 查询在禁用总命中跟踪（ track_total_hits ）时经过优化，能够高效地跳过非竞争性命中。\n","subcategory":null,"summary":"","tags":null,"title":"Distance Feature 查询","url":"/easysearch/main/docs/features/query-dsl/specialized/distance-feature/"},{"category":null,"content":"Dis Max 查询 #  dis_max 查询返回与一个或多个查询子句匹配的任何文档。对于与多个查询子句匹配的文档，相关性得分设置为所有匹配查询子句中的最高相关性得分。\n当返回的文档的相关性分数相同时，您可以使用 tie_breaker 参数来增加匹配多个查询子句的文档的权重。\n相关指南（先读这些） #    多字段搜索  Query DSL 基础  参考样例 #  考虑一个包含两个文档的索引，您按照以下方式索引这些文档：\nPUT testindex1/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34; The Top 10 Shakespeare Poems\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Top 10 sonnets of England\u0026#39;s national poet and the Bard of Avon\u0026#34; } PUT testindex1/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Sonnets of the 16th Century\u0026quot;, \u0026quot;body\u0026quot;: \u0026quot;The poems written by various 16-th century poets\u0026quot; } 使用 dis_max 查询来搜索包含单词“莎士比亚诗歌”的文档\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;dis_max\u0026#34;: { \u0026#34;queries\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Shakespeare poems\u0026#34; }}, { \u0026#34;match\u0026#34;: { \u0026#34;body\u0026#34;: \u0026#34;Shakespeare poems\u0026#34; }} ] } } } 返回内容包含两个文档：\n{ \u0026#34;took\u0026#34;: 8, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.3862942, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.3862942, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34; The Top 10 Shakespeare Poems\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Top 10 sonnets of England\u0026#39;s national poet and the Bard of Avon\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.2876821, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Sonnets of the 16th Century\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;The poems written by various 16-th century poets\u0026#34; } } ] } } 参数说明 #\n 以下表格列出了所有由 dis_max 查询支持的一级参数。\n   参数 描述     queries 一个或多个查询子句的数组，用于匹配文档。文档必须至少匹配一个查询子句才能在结果中返回。如果文档匹配多个查询子句，则相关度分数设置为所有匹配查询子句中的最高相关度分数。必需的。   tie_breaker 一个介于 0 和 1.0 之间的浮点因子，用于给匹配多个查询子句的文档赋予更多权重。在这种情况下，文档的相关性得分使用以下算法计算：从所有匹配的查询子句中取最高相关性得分，将所有其他匹配子句的得分乘以 tie_breaker 值，然后将相关性得分相加，并进行归一化。可选。默认值为 0（表示只计算最高得分）。    ","subcategory":null,"summary":"","tags":null,"title":"Dis Max 查询","url":"/easysearch/main/docs/features/query-dsl/compound-query/disjunction-max/"},{"category":null,"content":"Constant Score 查询 #  如果您需要返回包含某个词的文档，而不管该词出现多少次，您可以使用 constant_score 查询。constant_score 查询包装一个过滤器查询，并将结果中的所有文档的关联分数设置为 boost 参数的值。因此，所有返回的文档具有相同的关联分数，并且不考虑词频/逆文档频率（TF/IDF）。过滤器查询不会计算关联分数。此外，Easysearch 会缓存常用的过滤器查询以提高性能。\n相关指南（先读这些） #    查询 DSL 基础  结构化搜索  参考样例 #  使用以下查询返回 shakespeare 索引中包含单词“Hamlet”的文档：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;constant_score\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;Hamlet\u0026#34; } }, \u0026#34;boost\u0026#34;: 1.2 } } } 结果中的所有文档都被分配了 1.2 的相关性分数：\n{ \u0026#34;took\u0026#34;: 8, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 96, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.2, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32535\u0026#34;, \u0026#34;_score\u0026#34;: 1.2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32536, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 48, \u0026#34;line_number\u0026#34;: \u0026#34;1.1.97\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;HORATIO\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;Dared to the combat; in which our valiant Hamlet--\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32546\u0026#34;, \u0026#34;_score\u0026#34;: 1.2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32547, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 48, \u0026#34;line_number\u0026#34;: \u0026#34;1.1.108\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;HORATIO\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;His fell to Hamlet. Now, sir, young Fortinbras,\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32625\u0026#34;, \u0026#34;_score\u0026#34;: 1.2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32626, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 59, \u0026#34;line_number\u0026#34;: \u0026#34;1.1.184\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;HORATIO\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;Unto young Hamlet; for, upon my life,\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32633\u0026#34;, \u0026#34;_score\u0026#34;: 1.2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32634, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 60, \u0026#34;line_number\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;MARCELLUS\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;Enter KING CLAUDIUS, QUEEN GERTRUDE, HAMLET, POLONIUS, LAERTES, VOLTIMAND, CORNELIUS, Lords, and Attendants\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32634\u0026#34;, \u0026#34;_score\u0026#34;: 1.2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32635, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 1, \u0026#34;line_number\u0026#34;: \u0026#34;1.2.1\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;KING CLAUDIUS\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;Though yet of Hamlet our dear brothers death\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32699\u0026#34;, \u0026#34;_score\u0026#34;: 1.2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32700, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 8, \u0026#34;line_number\u0026#34;: \u0026#34;1.2.65\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;KING CLAUDIUS\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;But now, my cousin Hamlet, and my son,--\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32703\u0026#34;, \u0026#34;_score\u0026#34;: 1.2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32704, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 12, \u0026#34;line_number\u0026#34;: \u0026#34;1.2.69\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;QUEEN GERTRUDE\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;Good Hamlet, cast thy nighted colour off,\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32723\u0026#34;, \u0026#34;_score\u0026#34;: 1.2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32724, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 16, \u0026#34;line_number\u0026#34;: \u0026#34;1.2.89\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;KING CLAUDIUS\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;Tis sweet and commendable in your nature, Hamlet,\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32754\u0026#34;, \u0026#34;_score\u0026#34;: 1.2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32755, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 17, \u0026#34;line_number\u0026#34;: \u0026#34;1.2.120\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;QUEEN GERTRUDE\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;Let not thy mother lose her prayers, Hamlet:\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;32759\u0026#34;, \u0026#34;_score\u0026#34;: 1.2, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 32760, \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34;: 19, \u0026#34;line_number\u0026#34;: \u0026#34;1.2.125\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;KING CLAUDIUS\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;This gentle and unforced accord of Hamlet\u0026#34; } } ] } } 参数说明 #  下表列出了 constant_score 查询支持的所有顶层参数。\n   参数 描述     filter 文档必须匹配的过滤器查询才能在结果中返回。必填。   boost 分配给所有返回文档的相关性分数的浮点值。可选。默认值为 1.0。    ","subcategory":null,"summary":"","tags":null,"title":"Constant Score 查询","url":"/easysearch/main/docs/features/query-dsl/compound-query/constant-score/"},{"category":null,"content":"Boosting 查询 #  如果你搜索\u0026quot;pitcher\u0026quot;这个词，你的结果可能既与棒球运动员有关，也与盛液体的容器有关。在棒球语境下搜索时，你可能想通过使用 must_not 子句完全排除包含\u0026quot;glass\u0026quot;或\u0026quot;water\u0026quot;的搜索结果。然而，如果你想保留这些结果但降低它们的关联度，可以使用 boosting 查询。\n一个 boosting 查询返回与 positive 查询匹配的文档。在这些文档中，与 negative 查询也匹配的文档的关联度得分会降低（它们的关联度得分会乘以负的提升因子）。\n相关指南（先读这些） #    查询 DSL 基础  相关性：加权与调参  参考用例 #  考虑一个包含两个文档的索引，你以如下方式索引：\nPUT testindex/_doc/1 { \u0026#34;article_name\u0026#34;: \u0026#34;The greatest pitcher in baseball history\u0026#34; } PUT testindex/_doc/2 { \u0026quot;article_name\u0026quot;: \u0026quot;The making of a glass pitcher\u0026quot; } 使用以下匹配查询来搜索包含单词“pitcher”的文档：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;pitcher\u0026#34; } } } 返回的两个文档具有相同的相关性分数：\n{ \u0026#34;took\u0026#34;: 5, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.18232156, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.18232156, \u0026#34;_source\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;The greatest pitcher in baseball history\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.18232156, \u0026#34;_source\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;The making of a glass pitcher\u0026#34; } } ] } } 现在使用以下 boosting 查询来搜索包含单词“pitcher”的文档，但降低包含单词“glass”、“crystal”或“water”的文档的分数：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;boosting\u0026#34;: { \u0026#34;positive\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;pitcher\u0026#34; } }, \u0026#34;negative\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;glass crystal water\u0026#34; } }, \u0026#34;negative_boost\u0026#34;: 0.1 } } } 两个文档仍然会返回，但包含“glass”这个词的文档的相关性得分比之前的情况低 10 倍：\n{ \u0026#34;took\u0026#34;: 13, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.18232156, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.18232156, \u0026#34;_source\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;The greatest pitcher in baseball history\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.018232157, \u0026#34;_source\u0026#34;: { \u0026#34;article_name\u0026#34;: \u0026#34;The making of a glass pitcher\u0026#34; } } ] } } 参数说明 #\n 下表列出了 boosting 查询支持的所有顶层参数。\n   参数 描述     positive 文档必须匹配的查询，才能在结果中返回。必填。   negative 如果结果中的文档匹配此查询，其相关性得分会通过将其原始相关性得分（由 positive 查询产生）乘以 negative_boost 参数来降低。必填。   negative_boost 一个介于 0 和 1.0 之间的浮点数因子，用于将原始相关性分数乘以，以降低与 negative 查询匹配的文档的相关性。必填。    ","subcategory":null,"summary":"","tags":null,"title":"Boosting 查询","url":"/easysearch/main/docs/features/query-dsl/compound-query/boosting/"},{"category":null,"content":"Bool 查询 #  bool 查询可以将多个查询子句组合成一个高级查询。这些子句通过布尔逻辑组合起来，以在结果中找到匹配的文档。\n相关指南（先读这些） #    Query DSL 基础  结构化搜索  查询子句 #  在布尔（ bool ）查询中使用以下查询子句：\n   子句 行为     must 逻辑 and 运算符。结果必须匹配此子句中的所有查询。   must_not 逻辑 not 运算符。所有匹配项都将被排除在结果之外。如果 must_not 包含多个子句，则只返回不匹配任何这些子句的文档。例如， \u0026quot;must_not\u0026quot;:[{clause_A}, {clause_B}] 等同于 NOT(A OR B) 。   should 逻辑 or 运算符。结果必须匹配至少一个查询。匹配更多 should 子句会增加文档的相关性分数。您可以使用 minimum_should_match 参数设置必须匹配的最小查询数量。如果一个查询包含 must 或 filter 子句，默认 minimum_should_match 值为 0。否则，默认 minimum_should_match 值为 1。   filter 逻辑 and 运算符，在应用查询之前首先应用于减少您的数据集。过滤器子句中的查询是一个是或否选项。如果文档匹配查询，则它将出现在结果中；否则，它将不会出现。过滤器查询的结果通常会被缓存以允许更快的返回。使用过滤器查询根据精确匹配、范围、日期或数字来过滤结果。    一个布尔查询具有以下结构：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ {} ], \u0026#34;must_not\u0026#34;: [ {} ], \u0026#34;should\u0026#34;: [ {} ], \u0026#34;filter\u0026#34;: {} } } } 例如，假设您在 Easysearch 集群中索引了莎士比亚的全部作品。您希望构建一个满足以下要求的单个查询：\n text_entry 字段必须包含单词 love ，并且应该包含 life 或 grace 。 speaker 字段不能包含 ROMEO 。 将结果过滤到排位 Romeo and Juliet ，而不影响相关性分数。  这些要求可以在以下查询中组合：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;love\u0026#34; } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;life\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;grace\u0026#34; } } ], \u0026#34;minimum_should_match\u0026#34;: 1, \u0026#34;must_not\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;speaker\u0026#34;: \u0026#34;ROMEO\u0026#34; } } ], \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Romeo and Juliet\u0026#34; } } } } } 返回内容包含匹配文档：\n{ \u0026#34;took\u0026#34;: 12, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 4, \u0026#34;successful\u0026#34;: 4, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 11.356054, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;88020\u0026#34;, \u0026#34;_score\u0026#34;: 11.356054, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 88021, \u0026#34;play_name\u0026#34;: \u0026#34;Romeo and Juliet\u0026#34;, \u0026#34;speech_number\u0026#34;: 19, \u0026#34;line_number\u0026#34;: \u0026#34;4.5.61\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;PARIS\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;O love! O life! not life, but love in death!\u0026#34; } } ] } } 如果你想要识别是这些子句中的哪一个实际导致了匹配结果，请使用 _name 参数为每个查询命名。要添加 _name 参数，请将 match 查询中的字段名更改为对象：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;love\u0026#34;, \u0026#34;_name\u0026#34;: \u0026#34;love-must\u0026#34; } } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;life\u0026#34;, \u0026#34;_name\u0026#34;: \u0026#34;life-should\u0026#34; } } }, { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;grace\u0026#34;, \u0026#34;_name\u0026#34;: \u0026#34;grace-should\u0026#34; } } } ], \u0026#34;minimum_should_match\u0026#34;: 1, \u0026#34;must_not\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;speaker\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;ROMEO\u0026#34;, \u0026#34;_name\u0026#34;: \u0026#34;ROMEO-must-not\u0026#34; } } } ], \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Romeo and Juliet\u0026#34; } } } } } Easysearch 返回一个 matched_queries 数组，列出了匹配这些结果的查询：\n\u0026#34;matched_queries\u0026#34;: [ \u0026#34;love-must\u0026#34;, \u0026#34;life-should\u0026#34; ] 如果你移除不在该列表中的查询，你仍然会看到完全相同的结果。通过检查哪个 should 子句匹配了，你可以更好地理解结果的相关性分数。\n您还可以通过嵌套 bool 查询来构建复杂的布尔表达式。例如，使用以下查询在 play Romeo and Juliet 中查找匹配 ( love OR hate ) AND ( life OR grace ) 的 text_entry 字段：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;love\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;hate\u0026#34; } } ] } }, { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;life\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;grace\u0026#34; } } ] } } ], \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Romeo and Juliet\u0026#34; } } } } } 返回内容包含匹配文档：\n{ \u0026#34;took\u0026#34;: 10, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 2, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;max_score\u0026#34;: 11.37006, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;88020\u0026#34;, \u0026#34;_score\u0026#34;: 11.37006, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 88021, \u0026#34;play_name\u0026#34;: \u0026#34;Romeo and Juliet\u0026#34;, \u0026#34;speech_number\u0026#34;: 19, \u0026#34;line_number\u0026#34;: \u0026#34;4.5.61\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;PARIS\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;O love! O life! not life, but love in death!\u0026#34; } } ] } } 完整参数说明 #     参数 类型 说明     must Array of Query 文档必须匹配的查询。参与评分。   filter Array of Query 文档必须匹配的查询。不参与评分，结果可缓存。   should Array of Query 文档应该匹配的查询。增加 _score。   must_not Array of Query 文档必须不匹配的查询。不参与评分。   minimum_should_match Integer/String should 子句的最少匹配数。详见 minimum_should_match。   boost Float 整个 bool 查询的权重。默认 1.0。   adjust_pure_negative Boolean 当 bool 查询只包含 must_not（纯否定查询）时，是否自动添加 match_all 使查询正常工作。默认 true。通常不需要修改。   _name String 查询标签名称，用于 matched_queries 响应。    ","subcategory":null,"summary":"","tags":null,"title":"Bool 查询","url":"/easysearch/main/docs/features/query-dsl/compound-query/bool/"},{"category":null,"content":"字段级权限 #  字段级权限允许您控制用户可以查看文档中的哪些字段。就像 文档级权限，可以通过角色配置中的索引块来控制访问。\n相关指南（先读这些） #    权限控制总览  安全与多租户最佳实践  包括或排除字段 #  配置字段级权限时，有两个选项：包括或排除字段。如果包含字段，则用户在检索文档时 只能看到 这些字段。例如，如果您包含 actors、title 和 year 字段，则搜索结果可能如下所示：\nPOST movies/_doc/1 { \u0026#34;year\u0026#34;: 2013, \u0026#34;title\u0026#34;: \u0026#34;Rush\u0026#34;, \u0026#34;actors\u0026#34;: [ \u0026#34;Daniel Brühl\u0026#34;, \u0026#34;Chris Hemsworth\u0026#34;, \u0026#34;Olivia Wilde\u0026#34; ] } 如果是排除字段，则用户在检索文档时会看到除这些字段之外的所有内容。例如，如果排除这些相同的字段，则相同的搜索结果可能如下所示：\nPOST movies/_doc/2 { \u0026#34;directors\u0026#34;: [ \u0026#34;Ron Howard\u0026#34; ], \u0026#34;plot\u0026#34;: \u0026#34;A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\u0026#34;, \u0026#34;genres\u0026#34;: [ \u0026#34;Action\u0026#34;, \u0026#34;Biography\u0026#34;, \u0026#34;Drama\u0026#34;, \u0026#34;Sport\u0026#34; ] } 您可以使用配置文件 role.yml 和 REST API 来指定字段级安全设置。\n 要排除字段，可以在配置 role.yml 或通过 REST API，在字段名称前添加 ~ 。 字段名称支持通配符 (*).  role.yml #  limited_movie: cluster: [] indices: - names: - movies privileges: - \u0026#34;read\u0026#34; field_security: - \u0026#34;~actors\u0026#34; - \u0026#34;~title\u0026#34; - \u0026#34;~year\u0026#34; REST API #  参照 创建角色.\n注意多个角色 #  如果将用户映射到多个角色，我们建议这些角色对每个索引使用 include or exclude 语句。安全模块使用 AND 运算符来评估字段级权限的设置，因此组合包含和排除语句可能会导致这两种行为都无法正常工作。\n例如，在 movies 索引中，如果您在一个角色中包含 actors 、 title 和 year，在另一个角色中排除 actors 、 title 和 genres，然后将这两个角色映射到同一用户，则搜索结果可能如下所示：\n{ \u0026#34;_index\u0026#34;: \u0026#34;movies\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;year\u0026#34;: 2013, \u0026#34;directors\u0026#34;: [\u0026#34;Ron Howard\u0026#34;], \u0026#34;plot\u0026#34;: \u0026#34;A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\u0026#34; } } 与文档级权限的交互 #   文档级权限 依赖于 Easysearch 查询，这意味着查询中的所有相关字段都必须可见才能正常工作。如果将字段级权限与文档级权限结合使用，请确保不要限制对文档级权限字段的访问。\n","subcategory":null,"summary":"","tags":null,"title":"字段级权限","url":"/easysearch/main/docs/operations/security/access-control/field-level-security/"},{"category":null,"content":"Synonym Graph 分词过滤器 #  synonym_graph 分词过滤器是同义词分词过滤器的更高级版本。它支持多词同义词，并能跨多个词元处理同义词，这使得它非常适合处理短语或者那些词元之间关系很重要的场景。\n相关指南（先读这些） #    文本分析：同义词  文本分析：规范化  参数说明 #  同义词图分词过滤器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。   synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。   lenient 可选 布尔值 在加载规则配置时是否忽略异常。默认值为 false。   format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：\n- solr\n- wordnet\n默认值为 solr。   expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。\n例如：\n若同义词定义为 “quick, fast” 且 expand 设置为 true，则同义词规则配置如下：\n- quick 映射为 quick\n- quick 映射为 fast\n- fast 映射为 quick\n- fast 映射为 fast\n若 expand 设置为 false，则同义词规则配置如下：\n- quick 映射为 quick\n- fast 映射为 quick    参考样例： Solr 格式 #  以下示例请求创建了一个名为 my-index 的新索引，并配置了一个带有同义词图过滤器的分词器。该过滤器采用默认的 Solr 规则格式进行配置。\nPUT /my-car-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_synonym_graph_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;synonym_graph\u0026#34;, \u0026#34;synonyms\u0026#34;: [ \u0026#34;sports car, race car\u0026#34;, \u0026#34;fast car, speedy vehicle\u0026#34;, \u0026#34;luxury car, premium vehicle\u0026#34;, \u0026#34;electric car, EV\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_synonym_graph_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_synonym_graph_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-car-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_synonym_graph_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;I just bought a sports car and it is a fast car.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ {\u0026#34;token\u0026#34;: \u0026#34;i\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 1,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;just\u0026#34;,\u0026#34;start_offset\u0026#34;: 2,\u0026#34;end_offset\u0026#34;: 6,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 1}, {\u0026#34;token\u0026#34;: \u0026#34;bought\u0026#34;,\u0026#34;start_offset\u0026#34;: 7,\u0026#34;end_offset\u0026#34;: 13,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;a\u0026#34;,\u0026#34;start_offset\u0026#34;: 14,\u0026#34;end_offset\u0026#34;: 15,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;race\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;sports\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 22,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 4,\u0026#34;positionLength\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 5,\u0026#34;positionLength\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;,\u0026#34;start_offset\u0026#34;: 23,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 6}, {\u0026#34;token\u0026#34;: \u0026#34;and\u0026#34;,\u0026#34;start_offset\u0026#34;: 27,\u0026#34;end_offset\u0026#34;: 30,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 7}, {\u0026#34;token\u0026#34;: \u0026#34;it\u0026#34;,\u0026#34;start_offset\u0026#34;: 31,\u0026#34;end_offset\u0026#34;: 33,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 8}, {\u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;,\u0026#34;start_offset\u0026#34;: 34,\u0026#34;end_offset\u0026#34;: 36,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 9}, {\u0026#34;token\u0026#34;: \u0026#34;a\u0026#34;,\u0026#34;start_offset\u0026#34;: 37,\u0026#34;end_offset\u0026#34;: 38,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 10}, {\u0026#34;token\u0026#34;: \u0026#34;speedy\u0026#34;,\u0026#34;start_offset\u0026#34;: 39,\u0026#34;end_offset\u0026#34;: 47,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 11}, {\u0026#34;token\u0026#34;: \u0026#34;fast\u0026#34;,\u0026#34;start_offset\u0026#34;: 39,\u0026#34;end_offset\u0026#34;: 43,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 11,\u0026#34;positionLength\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;vehicle\u0026#34;,\u0026#34;start_offset\u0026#34;: 39,\u0026#34;end_offset\u0026#34;: 47,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 12,\u0026#34;positionLength\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;,\u0026#34;start_offset\u0026#34;: 44,\u0026#34;end_offset\u0026#34;: 47,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 13} ] } 参考样例： WordNet 格式 #  以下示例请求创建了一个名为 my-wordnet-index 的新索引，并配置了一个带有同义词图过滤器的分词器。该过滤器采用 WordNet 规则格式进行配置。\nPUT /my-wordnet-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_synonym_graph_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;synonym_graph\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;wordnet\u0026#34;, \u0026#34;synonyms\u0026#34;: [ \u0026#34;s(100000001, 1, \u0026#39;sports car\u0026#39;, n, 1, 0).\u0026#34;, \u0026#34;s(100000001, 2, \u0026#39;race car\u0026#39;, n, 1, 0).\u0026#34;, \u0026#34;s(100000001, 3, \u0026#39;fast car\u0026#39;, n, 1, 0).\u0026#34;, \u0026#34;s(100000001, 4, \u0026#39;speedy vehicle\u0026#39;, n, 1, 0).\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_synonym_graph_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_synonym_graph_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-wordnet-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_synonym_graph_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;I just bought a sports car and it is a fast car.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ {\u0026#34;token\u0026#34;: \u0026#34;i\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 1,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;just\u0026#34;,\u0026#34;start_offset\u0026#34;: 2,\u0026#34;end_offset\u0026#34;: 6,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 1}, {\u0026#34;token\u0026#34;: \u0026#34;bought\u0026#34;,\u0026#34;start_offset\u0026#34;: 7,\u0026#34;end_offset\u0026#34;: 13,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;a\u0026#34;,\u0026#34;start_offset\u0026#34;: 14,\u0026#34;end_offset\u0026#34;: 15,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;race\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;fast\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 4,\u0026#34;positionLength\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;speedy\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 4,\u0026#34;positionLength\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;sports\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 22,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 4,\u0026#34;positionLength\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 5,\u0026#34;positionLength\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 6,\u0026#34;positionLength\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;vehicle\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 7,\u0026#34;positionLength\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;,\u0026#34;start_offset\u0026#34;: 23,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 8}, {\u0026#34;token\u0026#34;: \u0026#34;and\u0026#34;,\u0026#34;start_offset\u0026#34;: 27,\u0026#34;end_offset\u0026#34;: 30,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 9}, {\u0026#34;token\u0026#34;: \u0026#34;it\u0026#34;,\u0026#34;start_offset\u0026#34;: 31,\u0026#34;end_offset\u0026#34;: 33,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 10}, {\u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;,\u0026#34;start_offset\u0026#34;: 34,\u0026#34;end_offset\u0026#34;: 36,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 11}, {\u0026#34;token\u0026#34;: \u0026#34;a\u0026#34;,\u0026#34;start_offset\u0026#34;: 37,\u0026#34;end_offset\u0026#34;: 38,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 12}, {\u0026#34;token\u0026#34;: \u0026#34;sports\u0026#34;,\u0026#34;start_offset\u0026#34;: 39,\u0026#34;end_offset\u0026#34;: 47,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 13}, {\u0026#34;token\u0026#34;: \u0026#34;race\u0026#34;,\u0026#34;start_offset\u0026#34;: 39,\u0026#34;end_offset\u0026#34;: 47,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 13,\u0026#34;positionLength\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;speedy\u0026#34;,\u0026#34;start_offset\u0026#34;: 39,\u0026#34;end_offset\u0026#34;: 47,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 13,\u0026#34;positionLength\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;fast\u0026#34;,\u0026#34;start_offset\u0026#34;: 39,\u0026#34;end_offset\u0026#34;: 43,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 13,\u0026#34;positionLength\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;,\u0026#34;start_offset\u0026#34;: 39,\u0026#34;end_offset\u0026#34;: 47,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 14,\u0026#34;positionLength\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;,\u0026#34;start_offset\u0026#34;: 39,\u0026#34;end_offset\u0026#34;: 47,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 15,\u0026#34;positionLength\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;vehicle\u0026#34;,\u0026#34;start_offset\u0026#34;: 39,\u0026#34;end_offset\u0026#34;: 47,\u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;,\u0026#34;position\u0026#34;: 16,\u0026#34;positionLength\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;,\u0026#34;start_offset\u0026#34;: 44,\u0026#34;end_offset\u0026#34;: 47,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 17} ] } ","subcategory":null,"summary":"","tags":null,"title":"同义词图分词过滤器（Synonym Graph）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/synonym-graph/"},{"category":null,"content":"UAX URL Email 分词器 #  除了常规文本之外，uax_url_email 分词器还专门用于处理网址、电子邮件地址和域名。它基于 Unicode 文本分割算法（ UAX #29），这使得它能够正确地对复杂文本（包括网址和电子邮件地址）进行分词处理。\n相关指南（先读这些） #    文本分析：识别词元  文本分析基础  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 UAX URL 邮件词元生成器的分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;uax_url_email_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;uax_url_email\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_uax_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;uax_url_email_tokenizer\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_uax_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_uax_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Contact us at support@example.com or visit https://example.com for details.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ {\u0026#34;token\u0026#34;: \u0026#34;Contact\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 7,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;us\u0026#34;,\u0026#34;start_offset\u0026#34;: 8,\u0026#34;end_offset\u0026#34;: 10,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 1}, {\u0026#34;token\u0026#34;: \u0026#34;at\u0026#34;,\u0026#34;start_offset\u0026#34;: 11,\u0026#34;end_offset\u0026#34;: 13,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;support@example.com\u0026#34;,\u0026#34;start_offset\u0026#34;: 14,\u0026#34;end_offset\u0026#34;: 33,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;EMAIL\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;or\u0026#34;,\u0026#34;start_offset\u0026#34;: 34,\u0026#34;end_offset\u0026#34;: 36,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;visit\u0026#34;,\u0026#34;start_offset\u0026#34;: 37,\u0026#34;end_offset\u0026#34;: 42,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 5}, {\u0026#34;token\u0026#34;: \u0026#34;https://example.com\u0026#34;,\u0026#34;start_offset\u0026#34;: 43,\u0026#34;end_offset\u0026#34;: 62,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;URL\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 6}, {\u0026#34;token\u0026#34;: \u0026#34;for\u0026#34;,\u0026#34;start_offset\u0026#34;: 63,\u0026#34;end_offset\u0026#34;: 66,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 7}, {\u0026#34;token\u0026#34;: \u0026#34;details\u0026#34;,\u0026#34;start_offset\u0026#34;: 67,\u0026#34;end_offset\u0026#34;: 74,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 8} ] } 参数说明 #  UAX URL 邮件词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     max_token_length 可选 整数 设置生成词元的最大长度。若超过该长度，词元将按照 max_token_length 所配置的长度拆分为多个词元。默认值为 255。    ","subcategory":null,"summary":"","tags":null,"title":"URL 邮箱分词器（UAX URL Email）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/uax-url-email/"},{"category":null,"content":"Grok 处理器 #  grok 处理器用于通过模式匹配解析和结构化非结构化数据。您可以使用 grok 处理器从日志消息、Web 服务器访问日志、应用程序日志和其他遵循一致格式的日志数据中提取字段。\nGrok 基础 #  grok 处理器使用一组预定义的模式来匹配输入文本的部分。每个模式由一个名称和一个正则表达式组成。例如，模式 %{IP:ip_address} 匹配 IP 地址并将其分配给字段 ip_address 。您可以将多个模式组合起来创建更复杂的表达式。例如，模式 %{IP:client} %{WORD:method} %{URIPATHPARM:request} %{NUMBER:bytes %NUMBER:duration} 匹配来自 Web 服务器访问日志的一行，并提取客户端 IP 地址、HTTP 方法、请求 URI、发送的字节数和请求持续时间。\n 有关可用预定义模式的列表，请参阅 Grok 模式。\n grok 处理器基于 Oniguruma 正则表达式库构建，并支持该库中的所有模式。您可以使用 Grok 调试器工具测试和调试您的 grok 表达式。\n 请注意，模式不是锚定的。为了性能和可靠性，请在您的模式中包含行首锚点（ ^ ）。\n 语法 #  以下是为 grok 处理器的基本语法：\n{ \u0026#34;grok\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;your_message\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;your_patterns\u0026#34;] } } 配置参数 #  要配置 grok 处理器，您有多种选项，允许您定义模式、匹配特定键和控制处理器的行为。下表列出了 grok 处理器的必选和可选参数。\n   参数 是否必填 描述     field 必填 包含要解析的文本的字段名称。   patterns 必填 用于匹配和提取命名捕获的 grok 表达式的列表。列表中第一个匹配的表达式被返回。   pattern_definitions 可选 一个用于定义当前处理器自定义模式的模式名称和模式元组的字典。如果模式与现有名称匹配，它将覆盖现有定义。   trace_match 可选 当参数设置为 true 时，处理器向处理过的文档添加一个名为 _grok_match_index 的字段。该字段包含在 patterns 数组中成功匹配文档的模式索引。此信息对于调试和理解应用于文档的模式非常有用。默认为 false 。   capture_all_matches 可选 当设置为 true 时，捕获重复 grok 模式的所有匹配项，而不仅仅是第一个匹配项。例如，给定文本 192.168.1.1 10.0.0.1 172.16.0.1 和模式 %{IP:ipAddress} %{IP:ipAddress} %{IP:ipAddress} ，所有三个 IP 地址都收集到 ipAddress 字段中的数组中。仅与显式重复的模式一起工作，不与量化模式（如 (%{IP:ipAddress})+ ）一起工作。默认为 false 。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。如果设置为 true ，则处理器在字段不存在或为 null 时不会修改文档。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  以下步骤指导您使用 grok 处理器创建一个摄取管道。\n步骤 1：创建管道 #  以下查询创建了一个名为 log_line 的管道。它使用指定的模式从文档的 message 字段中提取字段。在这种情况下，它提取了 clientip 、 timestamp 和 response_status 字段：\nPUT _ingest/pipeline/log_line { \u0026#34;description\u0026#34;: \u0026#34;Extract fields from a log line\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;grok\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;^%{IPORHOST:clientip} %{HTTPDATE:timestamp} %{NUMBER:response_status:int}\u0026#34;] } } ] } 步骤 2（可选）：测试管道。 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/log_line/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;198.126.12 10/Oct/2000:13:55:36 -0700 200\u0026#34; } } ] } 以下响应确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;response_status\u0026#34;: 200, \u0026#34;clientip\u0026#34;: \u0026#34;198.126.12\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;198.126.12 10/Oct/2000:13:55:36 -0700 200\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;10/Oct/2000:13:55:36 -0700\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-10-21T09:47:38.405544084Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=log_line { \u0026#34;message\u0026#34;: \u0026#34;127.0.0.1 198.126.12 10/Oct/2000:13:55:36 -0700 200\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 自定义模式 #  您可以使用默认模式，或者使用 patterns_definitions 参数将自定义模式添加到您的管道中。自定义 Grok 模式可以在管道中使用，以从不符合内置 Grok 模式的日志消息中提取结构化数据。这对于解析来自自定义应用程序的日志消息或解析以某种方式修改过的日志消息非常有用。自定义模式遵循简单的结构：每个模式都有一个唯一的名称以及定义其匹配行为的相应正则表达式。\n以下是一个如何在配置中包含自定义模式的示例。在这个示例中，问题编号介于 3 到 4 位数字之间，并解析到 issue_number 字段，状态解析到 status 字段：\nPUT _ingest/pipeline/log_line { \u0026#34;processors\u0026#34;: [ { \u0026#34;grok\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;^The issue number %{NUMBER:issue_number} is %{STATUS:status}\u0026#34;], \u0026#34;pattern_definitions\u0026#34; : { \u0026#34;NUMBER\u0026#34; : \u0026#34;\\\\d{3,4}\u0026#34;, \u0026#34;STATUS\u0026#34; : \u0026#34;open|closed\u0026#34; } } } ] } 如何追踪匹配模式 #  要追踪哪些模式匹配并填充了字段，您可以使用 trace_match 参数。以下是如何在您的配置中包含此参数的示例：\nPUT _ingest/pipeline/log_line { \u0026#34;description\u0026#34;: \u0026#34;Extract fields from a log line\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;grok\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;^%{HTTPDATE:timestamp} %{IPORHOST:clientip}\u0026#34;, \u0026#34;%{IPORHOST:clientip} %{HTTPDATE:timestamp} %{NUMBER:response_status:int}\u0026#34;], \u0026#34;trace_match\u0026#34;: true } } ] } 当您模拟管道时，Easysearch 会返回包含 grok_match_index 的 _ingest 元数据，如下面的输出所示：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;127.0.0.1 198.126.12 10/Oct/2000:13:55:36 -0700 200\u0026#34;, \u0026#34;response_status\u0026#34;: 200, \u0026#34;clientip\u0026#34;: \u0026#34;198.126.12\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;10/Oct/2000:13:55:36 -0700\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;_grok_match_index\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2023-11-02T18:48:40.455619084Z\u0026#34; } } } ] } ","subcategory":null,"summary":"","tags":null,"title":"Grok 处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/grok/"},{"category":null,"content":"遍历处理器 #  foreach 处理器用于遍历输入文档中的值列表并对每个值应用转换。这可以用于处理数组中的所有元素，例如将字符串中的所有元素转换为小写或大写等任务。\n以下是为 foreach 处理器提供的语法：\n{ \u0026#34;foreach\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;\u0026lt;field_name\u0026gt;\u0026#34;, \u0026#34;processor\u0026#34;: { \u0026#34;\u0026lt;processor_type\u0026gt;\u0026#34;: { \u0026#34;\u0026lt;processor_config\u0026gt;\u0026#34;: \u0026#34;\u0026lt;processor_value\u0026gt;\u0026#34; } } } } 配置参数 #  下表列出了 foreach 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 要遍历的数组字段。   processor 必填 处理器用于对每个字段执行操作。   ignore_missing 可选 如果 true 指定的字段不存在或为 null，则处理器将静默退出，不会修改文档。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 test-foreach 的管道，该管道使用 foreach 处理器遍历 protocols 字段中的每个元素：\nPUT _ingest/pipeline/test-foreach { \u0026#34;description\u0026#34;: \u0026#34;Lowercase all the elements in an array\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;foreach\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;protocols\u0026#34;, \u0026#34;processor\u0026#34;: { \u0026#34;lowercase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;_ingest._value\u0026#34; } } } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/test-foreach/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;protocols\u0026#34;: [\u0026#34;HTTP\u0026#34;,\u0026#34;HTTPS\u0026#34;,\u0026#34;TCP\u0026#34;,\u0026#34;UDP\u0026#34;] } } ] } 以下示例响应确认管道按预期工作，显示四个元素已被转换为小写：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;protocols\u0026#34;: [ \u0026#34;http\u0026#34;, \u0026#34;https\u0026#34;, \u0026#34;tcp\u0026#34;, \u0026#34;udp\u0026#34; ] }, \u0026#34;_ingest\u0026#34;: { \u0026#34;_value\u0026#34;: null, \u0026#34;timestamp\u0026#34;: \u0026#34;2024-05-23T02:44:10.8201Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPOST testindex1/_doc/1?pipeline=test-foreach { \u0026#34;protocols\u0026#34;: [\u0026#34;HTTP\u0026#34;,\u0026#34;HTTPS\u0026#34;,\u0026#34;TCP\u0026#34;,\u0026#34;UDP\u0026#34;] } 请求将文档索引到索引 testindex1 ，并在索引前应用管道：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 6, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 5, \u0026#34;_primary_term\u0026#34;: 67 } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 显示从 users 字段提取的 JSON 数据文档：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 8, \u0026#34;_seq_no\u0026#34;: 7, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;protocols\u0026#34;: [ \u0026#34;http\u0026#34;, \u0026#34;https\u0026#34;, \u0026#34;tcp\u0026#34;, \u0026#34;udp\u0026#34; ] } } ","subcategory":null,"summary":"","tags":null,"title":"遍历处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/foreach/"},{"category":null,"content":"文档级权限 #  文档级权限允许您将角色限制为索引中文档的一部分子集。\n相关指南（先读这些） #    权限控制总览  安全与多租户最佳实践  参考设置 #  文档级权限使用 Easysearch 查询 DSL 来定义角色授予对哪些文档的访问权限。\n{ \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;public\u0026#34;: \u0026#34;true\u0026#34; } } } } 上面的查询指定了该角色访问的文档里面，其字段 public 必须匹配 true。\n指定字段 query 并设置为将上面的查询，并对查询字符串进行转义，最后的角色设置如下：\nPUT _security/role/public_data { \u0026#34;cluster\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;indices\u0026#34;: [{ \u0026#34;names\u0026#34;: [ \u0026#34;pub*\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;{\\\u0026#34;term\\\u0026#34;: { \\\u0026#34;public\\\u0026#34;: true}}\u0026#34;, \u0026#34;privileges\u0026#34;: [ \u0026#34;read\u0026#34; ] }] } 上面的查询也可以根据需要写的很复杂，但是我们建议保持简单，以最大程度地减少文档级安全功能对集群的性能影响。\n参数替换 #  查询过程中可以利用上下文变量，可根据当前用户的属性来强制实施规则替换。例如 ${user.name} 将替换为当前用户的名称。\n如下规则允许用户读取字段 readable_by 为其用户名值的任何文档：\nPUT _security/role/user_data { \u0026#34;cluster\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;indices\u0026#34;: [{ \u0026#34;names\u0026#34;: [ \u0026#34;pub*\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;{\\\u0026#34;term\\\u0026#34;: { \\\u0026#34;readable_by\\\u0026#34;: \\\u0026#34;${user.name}\\\u0026#34;}}\u0026#34;, \u0026#34;privileges\u0026#34;: [ \u0026#34;read\u0026#34; ] }] } 支持以下变量：\n   名称 描述     ${user.name} 用户名.   ${user.roles} 用户的角色列表，使用逗号分割。   ${attr.\u0026lt;TYPE\u0026gt;.\u0026lt;NAME\u0026gt;} 用户的自定义属性， \u0026lt;TYPE\u0026gt; 支持 internal 和 ldap。    基于属性的权限控制 #  可以将角色和参数替换与 terms_set 查询一起使用，以启用基于属性的权限控制。\n定义角色 #  PUT _security/role/abac { \u0026#34;indices\u0026#34;: [{ \u0026#34;names\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;{\\\u0026#34;terms_set\\\u0026#34;: {\\\u0026#34;security_attributes\\\u0026#34;: {\\\u0026#34;terms\\\u0026#34;: [${attr.internal.permissions}], \\\u0026#34;minimum_should_match_script\\\u0026#34;: {\\\u0026#34;source\\\u0026#34;: \\\u0026#34;doc[\u0026#39;security_attributes\u0026#39;].length\\\u0026#34;}}}}\u0026#34;, \u0026#34;privileges\u0026#34;: [ \u0026#34;read\u0026#34; ] }] } 定义用户 #  PUT _security/user/user1 { \u0026#34;password\u0026#34;: \u0026#34;asdf\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;abac\u0026#34;], \u0026#34;attributes\u0026#34;: { \u0026#34;permissions\u0026#34;: \u0026#34;\\\u0026#34;att1\\\u0026#34;, \\\u0026#34;att2\\\u0026#34;, \\\u0026#34;att3\\\u0026#34;\u0026#34; } } 定义文档 #  POST easysearch/_doc/1 { \u0026#34;security_attributes\u0026#34;:\u0026#34;att1\u0026#34; }  注意，索引的 security_attributes 必须是 keyword 类型。\n ","subcategory":null,"summary":"","tags":null,"title":"文档级权限","url":"/easysearch/main/docs/operations/security/access-control/document-level-security/"},{"category":null,"content":"Character Group 分词器 #  char_group 分词器使用特定字符作为分隔符将文本拆分为词元。它适用于需要简单直接进行分词的场景，为基于分词器的匹配模式提供了一种更简单的替代方案，避免了额外的复杂性。\n相关指南（先读这些） #    文本分析：识别词元  文本分析基础  参考样例 #  以下示例请求创建了一个名为my_index的新索引，并配置了一个带有 char_group 字符组词元生成器的分词器。该词元生成器会依据空格、连字符 - 和冒号 : 来分割文本。\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_char_group_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;char_group\u0026#34;, \u0026#34;tokenize_on_chars\u0026#34;: [ \u0026#34;whitespace\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;:\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_char_group_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_char_group_tokenizer\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_char_group_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_char_group_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Fast-driving cars: they drive fast!\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Fast\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;driving\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 12, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;cars\u0026#34;, \u0026#34;start_offset\u0026#34;: 13, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;they\u0026#34;, \u0026#34;start_offset\u0026#34;: 19, \u0026#34;end_offset\u0026#34;: 23, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;drive\u0026#34;, \u0026#34;start_offset\u0026#34;: 24, \u0026#34;end_offset\u0026#34;: 29, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;fast!\u0026#34;, \u0026#34;start_offset\u0026#34;: 30, \u0026#34;end_offset\u0026#34;: 35, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 5 } ] } 参数说明 #  char_group 词元生成器可使用以下参数进行配置。\n   参数 必填/选填 数据类型 描述     tokenize_on_chars 必填 数组 指定用于对文本进行分词的一组字符。可以指定单个字符（例如 - 或 @），包括转义字符（例如 \\n ），或者字符类，如空白字符（whitespace）、字母（letter）、数字（digit）、标点符号（punctuation）或符号（symbol）。   max_token_length 选填 整数 设置生成词元的最大长度。如果超过此长度，词元将在max_token_length配置的长度处拆分为多个词元。默认值为 255。    ","subcategory":null,"summary":"","tags":null,"title":"字符组分词器（Character Group）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/character-group/"},{"category":null,"content":"监控告警 #  为 Easysearch 集群构建完整的告警体系，及时发现和处理问题，确保集群的稳定运行。\n 告警体系概览 #  Easysearch 的告警体系由以下核心组件组成：\n┌──────────────────────────────────────────────────────────────┐ │ 告警生命周期 │ ├──────────────────────────────────────────────────────────────┤ │ │ │ 数据采集 → 指标评估 → 告警触发 → 告警发送 → 告警处理 │ │ │ │ │ │ │ │ │ └───┬───────┴───────────┴─────────┴────────┘ │ │ │ │ │ └─→ INFINI Console 告警管理平台 │ │ │ └──────────────────────────────────────────────────────────────┘  告警规则体系 #  1. 可用性告警 #  集群的基础可用性监控。\n   告警规则 触发条件 建议阈值 处理建议     集群不健康 _cluster/health 返回 yellow 或 red 立即告警 检查节点是否掉线，运行 GET _cluster/allocation/explain 诊断未分配分片   节点离线 节点掉线或无响应 立即告警（超 1 分钟） 检查节点进程、网络连通性、磁盘空间   分片未分配 存在未分配的主分片或副本分片 立即告警 检查集群容量、节点约束、路由配置   集群脑裂 多个 master 节点并存 立即告警 紧急处理，可能需要恢复集群    2. 资源告警 #  服务器资源耗尽导致的问题。\n   告警规则 触发条件 建议阈值 处理建议     磁盘空间即将耗尽 磁盘使用率过高 75% 警告，85% 严重 及时扩容、删除过期数据、启用冷热分层   磁盘空间耗尽 磁盘写满 85%+ 紧急扩容，集群会变为 read-only   JVM 堆内存告警 JVM 堆使用率过高 75% 警告，85% 严重 增大堆内存、优化查询、添加节点分摊负载   CPU 使用率过高 CPU 长期处于高位 80% 警告，90%+ 严重 检查慢查询、限流索引、添加计算节点   内存使用率过高 系统内存耗尽 85% 警告 增加系统内存、减少后台任务    3. 性能告警 #  集群查询和写入性能相关的告警。\n   告警规则 触发条件 建议阈值 处理建议     搜索延迟过高 查询平均响应时间升高 500ms 警告，1s+ 严重 检查慢查询日志、优化查询语句、增加搜索节点   写入延迟过高 索引平均响应时间升高 100ms 警告，500ms+ 严重 减少副本数（临时）、优化 mapping、增加数据节点   Bulk 拒绝 写入队列满，请求被拒绝 任何拒绝都应告警 降低写入速率、增加数据节点、增加 bulk 队列   查询超时 查询无法在超时时间内完成 连续 5 个以上 优化查询、增加超时时间或查询并发数调优    4. 数据一致性告警 #  保证数据完整性和一致性的告警。\n   告警规则 触发条件 建议阈值 处理建议     副本不一致 分片副本与主分片数据不一致 立即告警 检查分片 recovery 过程、考虑重新分配   快照失败 备份任务失败 立即告警 检查备份仓库连通性、存储空间   索引损坏 索引检查失败，数据可能损坏 立即告警 尝试恢复备份或重建索引    5. 安全告警 #  异常访问和安全事件的告警。\n   告警规则 触发条件 建议阈值 处理建议     登录失败频繁 短时间内大量登录失败 5 分钟内 10+ 次失败 检查应用配置、防止暴力破解   权限操作异常 异常的权限修改或删除操作 立即告警 审计日志、检查操作人身份   证书即将过期 TLS 证书到期时间不足 30 天前告警 更新证书，避免服务中断     告警通知 #  通知渠道 #  使用 INFINI Console 配置多种通知渠道：\n 邮件：适合所有级别告警 钉钉：实时群组通知，支持 @群主 企业微信：内部工作群通知，消息已读追踪 短信：紧急告警（收费） Webhook：集成自定义系统（如 PagerDuty、Slack）  通知分级 #  根据告警严重程度，设置不同的通知规则：\n   严重级别 阈值示例 通知方式 应对时间     P0 - 紧急 集群 red、磁盘满、关键节点离线 电话/短信/Webhook 5 分钟内   P1 - 重要 集群 yellow、性能下降 50%、副本告警 邮件 + 钉钉 15 分钟内   P2 - 普通 资源接近限制、无分配分片 钉钉/企业微信 1 小时内   P3 - 信息 定期健康检查、维护窗口提醒 邮件 次日查看     使用 INFINI Console 配置告警 #  快速开始 #  INFINI Console 提供了直观的告警配置界面。\n1. 访问告警管理 #   登录 INFINI Console 导航到 告警管理 → 告警规则  2. 创建告警规则 #  规则名称：集群状态异常 规则类型：Easysearch 集群健康 触发条件：status != \u0026#39;green\u0026#39; for 1m 告警等级：P1 通知渠道：钉钉 + 邮件 描述：当集群状态为 yellow 或 red 超过 1 分钟时触发 3. 配置通知方式 #  在规则中关联通知渠道：\n通知规则： - P0 告警：立即发送短信和 Webhook - P1 告警：发送钉钉和邮件 - P2 告警：仅发送企业微信 常用告警模板 #  INFINI Console 内置了多个告警模板，可直接使用：\n集群监控模板 #  - 集群变黄告警 - 集群变红告警 - 集群脑裂检测 - 主节点无响应 - 节点离线告警 性能监控模板 #  - 搜索延迟过高 - 写入延迟过高 - Bulk 拒绝告警 - 查询超时告警 - GC 时间过长 资源监控模板 #  - 磁盘使用率告警 - 内存使用率告警 - CPU 使用率告警 - 堆内存告警 - 热点线程检测 数据一致性模板 #  - 副本不一致告警 - 快照失败告警 - 索引损坏告警 告警聚合和去重 #  在 INFINI Console 中配置告警聚合规则，避免告警轰炸：\n   聚合策略 使用场景 配置示例     按规则去重 同一规则在短时间内重复触发 10 分钟内同一规则只发送 1 条   按标签分组 相关告警统一发送 将磁盘、内存、CPU 告警分为一组   告警压制 维护期间暂停非关键告警 定时维护期间关闭 P2、P3 告警   告警关联 多个告警可能来自同一根因 GC 过长 + 堆内存告警同时出现     告警处理最佳实践 #  1. 告警阈值设置 #  原则：设置合理的阈值，避免频繁告警或漏掉真实问题。\n# 示例：磁盘告警 - 使用率 \u0026gt; 75%：P2 警告，提醒关注 - 使用率 \u0026gt; 85%：P1 重要，需要立即扩容 - 使用率 \u0026gt; 95%：P0 紧急，可能导致集群变为 read-only 示例：JVM 堆内存告警  使用率 \u0026gt; 75%：P2 警告 使用率 \u0026gt; 85%：P1 重要，可能导致 GC 风暴 使用率 \u0026gt; 95%：P0 紧急，立即处理 2. 告警响应流程 #   告警触发 ↓ 告警通知（邮件/短信/Webhook） ↓ 收到通知 → 查看告警详情 ↓ 确认告警真实性 → 检查历史趋势 ↓ 执行处理动作 ├─ 临时措施（如降低写入） ├─ 根本原因诊断 └─ 长期解决方案（如扩容） ↓ 更新告警状态 → 关闭告警 3. 常见问题处理 #  问题 1：频繁告警（告警轰炸） #  原因：\n 阈值设置过低 集群确实存在持续问题 网络抖动导致频繁变化  处理：\n 临时调高阈值，避免轰炸 分析告警趋势，找出根本原因 采取长期解决方案（扩容、优化等）  问题 2：告警延迟 #  原因：\n 指标采集间隔过大 通知渠道堵塞  处理：\n 缩短采集间隔（通常 30 秒） 测试通知渠道连通性 优先使用低延迟通知方式（Webhook）  问题 3：误告警 #  原因：\n 瞬间的资源尖峰 维护期间未关闭告警  处理：\n 使用滑动窗口平均值，避免单点尖峰 在维护前关闭相关告警 设置告警压制规则   监控与告警的联动 #  INFINI Console 集成监控和告警 #  INFINI Console 将监控和告警完整结合：\n  实时监控面板\n 集群、节点、索引多维度监控 实时告警在面板上高亮显示 一键查看告警详情    告警关联分析\n 自动关联相关告警 展示告警的历史趋势 建议可能的根本原因    告警历史和统计\n 告警发生时间、持续时间 告警解决时间（MTTR） 告警频率统计     其他监控工具的告警集成 #  Prometheus + Alertmanager #  如果使用 Prometheus 采集 Easysearch 指标：\n# prometheus.yml - 配置 Easysearch exporter scrape_configs: - job_name: \u0026#39;easysearch\u0026#39; static_configs: - targets: [\u0026#39;localhost:9114\u0026#39;] # Elasticsearch Exporter # alerting_rules.yml groups:\n name: easysearch rules:  alert: EasysearchClusterRed expr: elasticsearch_cluster_health_status{color=\u0026quot;red\u0026quot;} == 1 for: 1m annotations: summary: \u0026quot;Easysearch 集群变红\u0026quot; Elasticsearch 原生告警（Agent） #     Easysearch 兼容 Elasticsearch Stack，可使用 Elastic Stack 的告警功能。\n 告警检查清单 #  在完成告警体系建设时，确保涵盖以下内容：\n基础告警 #   集群健康状态告警（green/yellow/red） 节点离线告警 磁盘使用率告警（75%、85%） JVM 堆内存告警（75%、85%） CPU 使用率告警  性能告警 #   搜索延迟告警 写入延迟告警 Bulk 拒绝告警 慢查询监控  可靠性告警 #   快照失败告警 副本同步失败告警 索引损坏告警  安全告警 #   登录失败告警 权限操作告警 证书过期提醒  通知配置 #   邮件配置完成 钉钉/企业微信配置完成 通知渠道测试通过 告警分级和分配完成   相关文档 #    监控指标详解：详细的监控指标和采集方法  故障排查：告警后的诊断和处理  INFINI Console 文档：告警配置详细指南  集群 API 参考：健康检查等相关 API   总结 #  一个完整的告警体系应该包括：\n 合理的告警规则：覆盖可用性、性能、资源、数据一致性和安全 多渠道通知：邮件、短信、即时通讯、Webhook 等 智能聚合和去重：避免告警轰炸 规范的处理流程：快速响应和根因分析 持续改进：根据告警效果调整阈值和规则  建议使用 INFINI Console 作为告警管理平台，它提供了完整的告警生命周期管理功能，帮助您构建一个高效的告警体系。\n","subcategory":null,"summary":"","tags":null,"title":"告警配置","url":"/easysearch/main/docs/operations/alerting/"},{"category":null,"content":"Synonym 分词过滤器 #  synonym 分词过滤器允许将多个术语映射到单个术语，或在单词之间创建等价组，从而提高搜索的灵活性。\n相关指南（先读这些） #    同义词  文本分析基础  参数说明 #  同义词分词过滤器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。   synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。   lenient 可选 布尔值 加载规则配置时是否忽略异常。默认值为 false。   format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：\n- solr\n- wordnet\n默认值为 solr。   expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。\n例如：\n如果同义词定义为 \u0026quot;quick, fast\u0026quot; 且 expand 设置为 true，则同义词规则配置如下：\n- quick =\u0026gt; quick\n- quick =\u0026gt; fast\n- fast =\u0026gt; quick\n- fast =\u0026gt; fast\n如果 expand 设置为 false，则同义词规则配置如下：\n- quick =\u0026gt; quick\n- fast =\u0026gt; quick    参考样例： Solr 格式 #  以下示例请求创建了一个名为 my-synonym-index 的新索引，并配置了一个带有同义词过滤器的分词器。该过滤器使用默认的 Solr 规则格式进行配置。\nPUT /my-synonym-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_synonym_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;synonym\u0026#34;, \u0026#34;synonyms\u0026#34;: [ \u0026#34;car, automobile\u0026#34;, \u0026#34;quick, fast, speedy\u0026#34;, \u0026#34;laptop =\u0026gt; computer\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_synonym_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_synonym_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-synonym-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_synonym_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The quick dog jumps into the car with a laptop\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;the\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;quick\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 9, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;fast\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 9, \u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;speedy\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 9, \u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;dog\u0026#34;, \u0026#34;start_offset\u0026#34;: 10, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;jumps\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;into\u0026#34;, \u0026#34;start_offset\u0026#34;: 20, \u0026#34;end_offset\u0026#34;: 24, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;the\u0026#34;, \u0026#34;start_offset\u0026#34;: 25, \u0026#34;end_offset\u0026#34;: 28, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 }, { \u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;, \u0026#34;start_offset\u0026#34;: 29, \u0026#34;end_offset\u0026#34;: 32, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 6 }, { \u0026#34;token\u0026#34;: \u0026#34;automobile\u0026#34;, \u0026#34;start_offset\u0026#34;: 29, \u0026#34;end_offset\u0026#34;: 32, \u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;, \u0026#34;position\u0026#34;: 6 }, { \u0026#34;token\u0026#34;: \u0026#34;with\u0026#34;, \u0026#34;start_offset\u0026#34;: 33, \u0026#34;end_offset\u0026#34;: 37, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 7 }, { \u0026#34;token\u0026#34;: \u0026#34;a\u0026#34;, \u0026#34;start_offset\u0026#34;: 38, \u0026#34;end_offset\u0026#34;: 39, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 8 }, { \u0026#34;token\u0026#34;: \u0026#34;computer\u0026#34;, \u0026#34;start_offset\u0026#34;: 40, \u0026#34;end_offset\u0026#34;: 46, \u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;, \u0026#34;position\u0026#34;: 9 } ] } 参考样例：WordNet 格式 #  以下示例请求创建了一个名为 my-wordnet-index 的新索引，并配置了一个带有同义词过滤器的分词器。该过滤器是按照 WordNet 规则格式配置的。\nPUT /my-wordnet-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_wordnet_synonym_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;synonym\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;wordnet\u0026#34;, \u0026#34;synonyms\u0026#34;: [ \u0026#34;s(100000001,1,\u0026#39;fast\u0026#39;,v,1,0).\u0026#34;, \u0026#34;s(100000001,2,\u0026#39;quick\u0026#39;,v,1,0).\u0026#34;, \u0026#34;s(100000001,3,\u0026#39;swift\u0026#39;,v,1,0).\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_wordnet_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_wordnet_synonym_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-wordnet-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_wordnet_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;I have a fast car\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;i\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;have\u0026#34;, \u0026#34;start_offset\u0026#34;: 2, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;a\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;fast\u0026#34;, \u0026#34;start_offset\u0026#34;: 9, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;quick\u0026#34;, \u0026#34;start_offset\u0026#34;: 9, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;swift\u0026#34;, \u0026#34;start_offset\u0026#34;: 9, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;SYNONYM\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;car\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"同义词分词过滤器（Synonym）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/synonym/"},{"category":null,"content":"分布式读取写入过程 #  Easysearch 隐藏了分布式系统的大部分底层细节，让你可以专注在业务开发上。但线上出问题时你真正需要的是：这条请求在集群里到底走了哪几步。本页把分布式 CRUD 的关键流程串起来，读完你会更容易理解：\n 为什么同一条数据总能\u0026quot;找到回家的路\u0026quot;（routing） 为什么写入一定要先到主分片（primary） 为什么副本不吃\u0026quot;补丁\u0026quot;，只吃\u0026quot;整份文档\u0026quot;（复制语义） 为什么 bulk 要用看起来奇怪的 NDJSON（性能与内存）   术语提示：你可以把请求发到集群中的任意节点。接收请求并负责\u0026quot;拆分、转发、汇总\u0026quot;的那个节点，称为协调节点（coordinating node）。为了均衡负载，更好的做法是轮询集群中所有节点发送请求。\n 路由：文档如何找到分片 #  当索引一个文档时，Easysearch 需要确定它属于哪个主分片。这个过程是确定性的，基于以下公式：\nshard = hash(routing) % number_of_primary_shards  routing 是一个可变值，默认是文档的 _id，也可以设置成一个自定义的值 routing 通过 Murmur3 x86 32-bit 哈希算法（种子为 0）生成一个数字，然后除以主分片数量取余 余数就是文档所在分片的编号（范围 0 到 number_of_primary_shards - 1）  这就解释了为什么主分片数量在索引创建后不能改变：分片数变了，取模结果就变了，所有之前路由的值都会无效，老文档的\u0026quot;地址\u0026quot;全得重算。工程上一般通过新索引 + 重建索引 + 别名切换来实现扩容（见 别名）。\n 所有的文档 API（get、index、delete、bulk、update、mget）都接受一个 routing 参数。自定义路由常用于\u0026quot;把相关数据放在一起\u0026quot;——例如把同一租户/同一用户的数据路由到同一分片，减少查询时的 fan-out（参见 多租户建模）。\n 写入流程：新建、索引和删除 #  新建、索引和删除请求都是写操作，必须在主分片上完成之后才能被复制到副本分片。\n执行步骤：\n 客户端向 Node 1（协调节点）发送写入请求 Node 1 使用文档的 _id（或自定义 routing）计算出文档属于分片 0，请求被转发到分片 0 的主分片所在节点（例如 Node 3） Node 3 在主分片上执行请求。成功后，将新版本的完整文档（或删除标记）并行转发到所有副本分片节点 一旦所有副本分片都报告成功，Node 3 向协调节点报告成功，协调节点再向客户端返回成功  在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成。\n关键细节：复制的是\u0026quot;新文档版本\u0026quot;，不是\u0026quot;更新指令\u0026quot; #  主分片向副本转发的是文档的新版本，而不是\u0026quot;请把字段 A +1\u0026quot;这样的增量变更。\n原因很直白：复制是并行的、网络到达顺序不保证一致。如果副本只收到\u0026quot;补丁指令\u0026quot;，指令乱序就可能把文档\u0026quot;补坏\u0026quot;。转发整份新版本可以避免这个问题。\n一致性保证 #  wait_for_active_shards 参数控制写入前至少需要多少个活跃分片副本（包括主分片）：\n   值 行为     1（默认） 只要主分片可用就执行   2 主分片 + 至少 1 个副本可用   all 必须主分片和所有副本都可用   自定义数值 指定需要多少个活跃分片副本     早期版本曾使用 quorum（多数）确认作为默认值，但在当前版本中，默认值为 1（仅需主分片可用）。如需更强的数据安全保证，可以显式设置为更大的值（如 2 或 all）。\n 超时（timeout）：如果没有足够的副本分片可用，Easysearch 会等待（默认 1 分钟）。可通过 timeout 参数调整。\n并发冲突与重试语义见 并发控制与版本。\n读取流程：取回一个文档 #  文档可以从主分片或任意副本分片检索，通过轮询实现负载均衡：\n 客户端向协调节点发送 GET /{index}/_doc/{id} 请求 协调节点使用 _id（或指定的 routing）算出目标主分片编号 它在该分片的主/副本里挑一个可用的来读（通常会做轮询） 持有文档的节点将文档返回给协调节点，再返回给客户端  \u0026ldquo;刚写入就读不到\u0026quot;怎么办？ #  你可能遇到两类\u0026quot;刚写入就读不到\u0026quot;的错觉：\n 近实时刷新（NRT）：影响的是 _search，不是 get。get 直接查事务日志 / 存储层路径，通常更\u0026quot;实时\u0026rdquo;。NRT 细节见 写入与存储机制 路由不一致：如果写入用了自定义 routing，读取也必须带同样的 routing，否则会去错分片，自然找不到   在文档被索引但尚未复制到副本分片时，副本可能报告文档不存在，但主分片可以成功返回。一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。\n 更新流程：局部更新 #  update API 结合了读取和写入模式。本质上仍然是\u0026quot;读取旧 _source → 合并修改 → 重新索引整份文档\u0026quot;：\n 客户端向协调节点发送更新请求 协调节点将请求转发到主分片所在节点 主分片节点从本地检索文档，修改 _source 字段中的 JSON，然后重新索引整个文档。如果文档已被其他进程修改（版本冲突），会重试，超过 retry_on_conflict 次后放弃 更新成功后，将新版本的完整文档并行转发到所有副本分片重新索引。所有副本完成后向客户端返回成功  update 的更多细节（脚本更新、upsert、冲突处理）见 Update API。\n批量操作：mget 和 bulk #  mget：批量读取 #  mget 和单条 get 的模式类似，区别是协调节点会按分片把请求拆成多份并行转发：\n 客户端向协调节点发送 mget 请求 协调节点将请求按分片拆分，并行转发到各目标节点 收到所有响应后，汇总为单个响应返回给客户端  这也是 mget 比在客户端循环 get 更\u0026quot;省网络、更省延迟\u0026quot;的原因——docs 数组里还可以为每条文档指定不同的 routing。\nbulk：批量写入 #   客户端向协调节点发送 bulk 请求 协调节点为每个目标节点构建批量请求，并行转发到各主分片所在节点 各主分片按顺序执行每个操作，每个操作完成后并行转发到副本分片 所有节点完成后，协调节点汇总响应返回给客户端  为什么 bulk 用 NDJSON 而不是 JSON 数组？ #  bulk 请求体是\u0026quot;一行 action/metadata，一行可选 body\u0026quot;的 NDJSON 格式。这不是为了折磨人，而是一个有意为之的性能优化。\n如果用 JSON 数组：\n 需要先将整个请求完整解析到内存中（包括很大的文档内容） 再遍历每个元素计算路由、按分片分组 为每个分片构建新的数组/结构，再序列化转发  NDJSON 的做法更\u0026quot;流式\u0026quot;：\n 先只解析很小的 action/metadata 行，立刻知道该把后续 body 转发去哪 原始数据可以更直接地从网络缓冲区被切分、转发 避免在 JVM 里制造大量短命对象，减轻 GC 压力 没有冗余的数据复制，整个请求在最小内存中完成处理  bulk 的格式细节与错误处理见 Bulk API。\n小结 #     操作 执行节点 关键特点     写入（index/create/delete） 主分片 → 副本分片 先主后副，需 quorum 确认   读取（get） 任意分片（轮询） 主副均可，负载均衡   更新（update） 主分片读+写 → 副本 转发完整文档，非增量   批量读（mget） 按分片拆分并行 协调节点汇总   批量写（bulk） 按分片拆分并行 NDJSON 格式，流式拆分    核心规则：\n routing 决定分片：自定义 routing 用于\u0026quot;相关数据放一起\u0026quot;，但读写必须一致 写入先主后副本：主分片裁决，副本复制结果 副本复制整份新版本：避免乱序补丁把文档搞坏 读取可走副本：get 会在主/副本间做负载均衡  下一步 #    分布式查询过程：搜索请求如何在分片间执行  写入与存储机制：近实时搜索、refresh、flush、merge  分布式基础：集群、节点和分片的整体架构  文档操作：文档写入 API、批量操作的格式与错误处理  并发控制与版本：乐观锁与安全更新模式  最佳实践 #    索引与分片设计：分片数量规划、路由策略  数据建模：文档设计与更新模式  ","subcategory":null,"summary":"","tags":null,"title":"分布式读取写入过程","url":"/easysearch/main/docs/fundamentals/distributed-write/"},{"category":null,"content":"监控 #  本文从\u0026quot;如何判断集群是否健康\u0026quot;出发，给出最小可行的监控指标与告警建议，并提供具体的 API 示例。\n 监控三层模型 #  Easysearch 集群的监控可以分为三个层次：\n┌─────────────────────────────────────────────────────────────┐ │ Layer 1: 服务可用性 │ │ • 集群是否整体可用？节点是否存活？ │ ├─────────────────────────────────────────────────────────────┤ │ Layer 2: 资源与性能 │ │ • CPU/内存/磁盘/网络是否有瓶颈？延迟是否异常？ │ ├─────────────────────────────────────────────────────────────┤ │ Layer 3: 业务行为 │ │ • 慢查询、错误率、索引状态是否正常？ │ └─────────────────────────────────────────────────────────────┘ 监控目标：\n 出问题前能看到\u0026quot;趋势变坏\u0026quot;（而不是只在宕机后才收到告警） 告警有明确的处理指引，而不是一堆噪音   集群健康检查 #  快速健康检查 #  GET _cluster/health 响应示例：\n{ \u0026#34;cluster_name\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;timed_out\u0026#34;: false, \u0026#34;number_of_nodes\u0026#34;: 5, \u0026#34;number_of_data_nodes\u0026#34;: 3, \u0026#34;active_primary_shards\u0026#34;: 50, \u0026#34;active_shards\u0026#34;: 100, \u0026#34;relocating_shards\u0026#34;: 0, \u0026#34;initializing_shards\u0026#34;: 0, \u0026#34;unassigned_shards\u0026#34;: 0, \u0026#34;delayed_unassigned_shards\u0026#34;: 0, \u0026#34;number_of_pending_tasks\u0026#34;: 0, \u0026#34;number_of_in_flight_fetch\u0026#34;: 0, \u0026#34;task_max_waiting_in_queue_millis\u0026#34;: 0, \u0026#34;active_shards_percent_as_number\u0026#34;: 100.0 } 状态含义：\n   状态 含义 处理建议     green 所有分片（主分片+副本）均已分配 正常状态   yellow 所有主分片已分配，但部分副本未分配 检查节点数量是否满足副本需求   red 部分主分片未分配，数据可能丢失 立即处理，检查未分配原因    等待集群健康 #  在自动化脚本中，等待集群达到特定状态：\nGET _cluster/health?wait_for_status=green\u0026amp;timeout=60s 索引级健康检查 #  GET _cluster/health/logs-*?level=indices  关键监控指标 #  集群级指标 #  GET _cluster/stats 核心关注点：\n   指标 路径 告警阈值建议     节点数 nodes.count.total 低于预期节点数   分片总数 indices.shards.total 单节点 \u0026gt;1000 个分片   文档数 indices.docs.count 趋势监控   存储大小 indices.store.size_in_bytes 趋势监控    节点级指标 #  GET _nodes/stats 简化视图（推荐日常使用）：\nGET _cat/nodes?v\u0026amp;h=name,heap.percent,ram.percent,cpu,load_1m,disk.used_percent,node.role 输出示例：\nname heap.percent ram.percent cpu load_1m disk.used_percent node.role node-1 45 78 12 1.25 62.3 dim node-2 52 82 15 1.45 58.7 dim node-master 23 45 5 0.32 25.1 m 核心指标详解：\n   指标 说明 告警阈值     heap.percent JVM 堆使用率 \u0026gt; 75% 警告，\u0026gt; 85% 严重   ram.percent 系统内存使用率 \u0026gt; 90% 警告   cpu CPU 使用率 \u0026gt; 80% 持续 5 分钟   load_1m 1 分钟负载 \u0026gt; CPU 核心数 × 2   disk.used_percent 磁盘使用率 \u0026gt; 75% 警告，\u0026gt; 85% 严重    磁盘水位监控 #  Easysearch 内置磁盘保护机制：\n   水位 默认值 行为     low 85% 不再向该节点分配新分片   high 90% 尝试将分片迁移到其他节点   flood_stage 95% 强制将受影响索引设为只读    查看当前设置：\nGET _cluster/settings?include_defaults=true\u0026amp;filter_path=*.cluster.routing.allocation.disk* 分片级指标 #  # 分片分布概览 GET _cat/shards?v\u0026amp;s=state,index # 只看问题分片 GET _cat/shards?v\u0026amp;h=index,shard,prirep,state,unassigned.reason\u0026amp;s=state:desc 未分配分片诊断：\nGET _cluster/allocation/explain 响应示例（磁盘不足）：\n{ \u0026#34;index\u0026#34;: \u0026#34;logs-2026.02.11\u0026#34;, \u0026#34;shard\u0026#34;: 0, \u0026#34;primary\u0026#34;: true, \u0026#34;current_state\u0026#34;: \u0026#34;unassigned\u0026#34;, \u0026#34;unassigned_info\u0026#34;: { \u0026#34;reason\u0026#34;: \u0026#34;INDEX_CREATED\u0026#34;, \u0026#34;at\u0026#34;: \u0026#34;2026-02-11T10:00:00.000Z\u0026#34; }, \u0026#34;can_allocate\u0026#34;: \u0026#34;no\u0026#34;, \u0026#34;allocate_explanation\u0026#34;: \u0026#34;cannot allocate because allocation is not permitted to any of the nodes\u0026#34;, \u0026#34;node_allocation_decisions\u0026#34;: [ { \u0026#34;node_name\u0026#34;: \u0026#34;node-1\u0026#34;, \u0026#34;node_decision\u0026#34;: \u0026#34;no\u0026#34;, \u0026#34;deciders\u0026#34;: [ { \u0026#34;decider\u0026#34;: \u0026#34;disk_threshold\u0026#34;, \u0026#34;decision\u0026#34;: \u0026#34;NO\u0026#34;, \u0026#34;explanation\u0026#34;: \u0026#34;the node is above the high watermark cluster setting\u0026#34; } ] } ] }  性能指标监控 #  搜索性能 #  GET _nodes/stats/indices/search 关键指标：\n   指标 路径 说明     搜索总数 indices.search.query_total QPS = 差值 / 时间间隔   搜索耗时 indices.search.query_time_in_millis 平均延迟 = 耗时 / 总数   当前搜索 indices.search.query_current 并发搜索数   Fetch 耗时 indices.search.fetch_time_in_millis 结果获取耗时    写入性能 #  GET _nodes/stats/indices/indexing 关键指标：\n   指标 路径 说明     写入总数 indices.indexing.index_total 写入 QPS   写入耗时 indices.indexing.index_time_in_millis 写入延迟   写入失败 indices.indexing.index_failed 应为 0    Bulk 拒绝监控 #  GET _nodes/stats/thread_pool/write 关键指标：\n   指标 说明 告警条件     rejected 被拒绝的请求数 \u0026gt; 0 且持续增长   queue 当前队列大小 接近最大值    段合并监控 #  GET _nodes/stats/indices/merges 关注点：\n current：当前合并数 total_time_in_millis：合并总耗时 合并压力过大会影响写入和查询性能   慢查询日志 #  配置慢查询阈值 #  PUT /my-index/_settings { \u0026#34;index.search.slowlog.threshold.query.warn\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.info\u0026#34;: \u0026#34;5s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.debug\u0026#34;: \u0026#34;2s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.trace\u0026#34;: \u0026#34;500ms\u0026#34;, \u0026#34;index.search.slowlog.threshold.fetch.warn\u0026#34;: \u0026#34;1s\u0026#34;, \u0026#34;index.search.slowlog.threshold.fetch.info\u0026#34;: \u0026#34;800ms\u0026#34; } 配置慢写入阈值 #  PUT /my-index/_settings { \u0026#34;index.indexing.slowlog.threshold.index.warn\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;index.indexing.slowlog.threshold.index.info\u0026#34;: \u0026#34;5s\u0026#34; } 查看慢查询日志 #  日志位置：logs/{cluster_name}_index_search_slowlog.log\n日志格式示例：\n[2026-02-11T10:30:00,123][WARN][index.search.slowlog.query] [node-1] [logs-2026.02.11][0] took[12.3s], took_millis[12345], total_hits[10000], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[5], source[{\u0026#34;query\u0026#34;:{\u0026#34;match_all\u0026#34;:{}}}]  常见告警规则 #  可用性告警 #     告警名称 条件 级别 处理建议     集群状态异常 status != green 持续 5 分钟 P1 检查分片分配   节点离线 节点数 \u0026lt; 预期 P1 检查节点进程   未分配分片 unassigned_shards \u0026gt; 0 持续 10 分钟 P2 诊断分配原因    资源告警 #     告警名称 条件 级别 处理建议     磁盘空间不足 disk.used_percent \u0026gt; 85% P2 清理数据或扩容   磁盘空间严重不足 disk.used_percent \u0026gt; 90% P1 立即释放空间   JVM 堆压力 heap.percent \u0026gt; 85% 持续 10 分钟 P2 检查查询或扩容   CPU 过载 cpu \u0026gt; 90% 持续 5 分钟 P2 检查热点查询    性能告警 #     告警名称 条件 级别 处理建议     搜索延迟升高 P95 \u0026gt; 基线 × 2 P3 检查慢查询   Bulk 拒绝 rejected \u0026gt; 0 且持续增长 P2 降低写入并发   段合并积压 current_merges \u0026gt; 5 持续 P3 检查写入模式     监控系统集成 #  Prometheus Exporter 方式 #  使用社区 Exporter 将指标暴露给 Prometheus：\n# prometheus.yml scrape_configs: - job_name: \u0026#39;easysearch\u0026#39; static_configs: - targets: [\u0026#39;es-exporter:9114\u0026#39;] 定时拉取脚本方式 #  #!/bin/bash # 每分钟采集关键指标 CLUSTER_HEALTH=$(curl -s \u0026quot;http://localhost:9200/_cluster/health\u0026quot;) NODE_STATS=$(curl -s \u0026quot;http://localhost:9200/_cat/nodes?format=json\u0026quot;)\n# 解析并发送到监控系统 STATUS=$(echo $CLUSTER_HEALTH | jq -r '.status') UNASSIGNED=$(echo $CLUSTER_HEALTH | jq -r '.unassigned_shards')\n# 示例：发送到 InfluxDB curl -XPOST 'http://influxdb:8086/write?db=easysearch'  \u0026ndash;data-binary \u0026quot;cluster_health,cluster=prod status=\u0026amp;#34;$STATUS\u0026amp;#34;,unassigned=$UNASSIGNED\u0026quot; 日志采集 #\n 配置 Filebeat 采集 Easysearch 日志：\nfilebeat.inputs: - type: log paths: - /var/log/easysearch/*.log multiline.pattern: \u0026#39;^\\[\u0026#39; multiline.negate: true multiline.match: after output.elasticsearch: hosts: [\u0026quot;monitoring-cluster:9200\u0026quot;] index: \u0026quot;easysearch-logs-%{+yyyy.MM.dd}\u0026quot; \n监控仪表盘建议 #  概览面板 #   集群状态（green/yellow/red） 节点数量与角色分布 分片总数与未分配数 存储总量与可用空间  性能面板 #   搜索 QPS 与延迟（P50/P95/P99） 写入 QPS 与延迟 Bulk 队列深度与拒绝数 段合并数与耗时  资源面板 #   各节点 CPU 使用率 各节点 JVM 堆使用率 各节点磁盘使用率 网络流量（入/出）  热点面板 #   Top 10 慢查询索引 Top 10 大索引（按存储） Top 10 热点分片   使用 INFINI Console 进行可视化监控 #  除了手动通过 API 查询监控数据，推荐使用 INFINI Console 来实现集群的可视化管理和监控。INFINI Console 是一款轻量级的多集群、跨版本搜索基础设施统一纳管平台。\nINFINI Console 简介 #  INFINI Console 提供了以下核心功能：\n 多集群管理：统一管理多个 Easysearch/Elasticsearch 集群（支持跨版本） 可视化监控：实时监控集群、节点、索引的关键指标 性能分析：TopN 功能快速识别排名前 N 的关键指标数据点 数据管理：索引管理、快照备份、数据迁移等操作 告警管理：配置告警规则并接收通知  快速开始 #  在线体验：\n访问 INFINI Console Demo（用户名/密码：readonly/readonly）\n本地部署：\nINFINI Console 支持 Docker、Kubernetes 等多种部署方式。具体部署指南请参考 INFINI Console 官方文档。\n推荐的监控仪表盘 #  在 INFINI Console 中建议配置以下仪表盘：\n 集群概览面板：集群状态、节点数、分片分布、存储情况 性能监控面板：搜索/写入 QPS、延迟、Bulk 拒绝情况 资源监控面板：CPU、内存、JVM 堆、磁盘使用率 告警面板：实时告警、告警历史、处理记录 慢查询分析面板：Top 慢查询、错误率分析   使用 Easysearch-UI 进行集群管理 #  Easysearch-UI 是 Easysearch v1.15.0+ 内置的官方轻量级管理界面，无需额外安装，开箱即用。\n快速开始 #  启动 Easysearch 集群后，访问以下地址即可使用：\nhttps://localhost:9200/_ui/ 核心功能 #  Easysearch-UI 提供了以下功能：\n1. 节点管理 #   查看集群中所有节点的信息 监控节点的状态和健康度 节点角色配置查看  2. 索引管理（功能丰富） #   查看所有索引的详细信息 创建、删除、编辑索引 查看索引配置和映射 索引限流设置：灵活配置索引资源配额，让硬件资源向高价值索引倾斜  3. 分片管理 #   查看分片分布情况 分片移动：轻松实现分片重新分配和负载均衡 支持手动迁移分片到指定节点  4. 模板管理 #   UI 界面引导创建和编辑索引模板 管理分量级别的模板配置  5. 热点线程分析 #   实时捕捉热点线程 可视化展示 CPU 消耗最高的线程 快速定位性能瓶颈  6. 生命周期管理 #   可视化配置 ILM（索引生命周期管理）策略 轻松管理索引的热温冷归档  7. 备份管理 #   配置和管理备份仓库 创建、查看、恢复快照 一键快照操作  8. Dev Tools #   类似 Kibana 的 Console，支持 REST API 调用 支持 DSL 查询编写和测试 实时查询结果展示  使用示例 #  查看集群概览 #  登录后首页展示：\n 集群整体健康状态 节点数量和分布 主分片和副本分片统计 索引总数和文档总数  快速索引操作 #   进入\u0026quot;索引管理\u0026quot; 查看索引统计：文档数、存储大小、分片配置 支持批量操作：刷新、关闭、打开索引  性能诊断 #   进入\u0026quot;热点线程\u0026quot; 选择时间窗口（如 30 秒） 快速发现 CPU 消耗最高的操作  容灾配置 #   进入\u0026quot;备份管理\u0026quot; → \u0026ldquo;仓库配置\u0026rdquo; 配置备份仓库（支持 HDFS、NFS、S3 等） 创建快照实现数据备份和灾难恢复   INFINI Console vs Easysearch-UI 对比 #     功能 INFINI Console Easysearch-UI     访问方式 独立应用，多集群 内置在 Easysearch，无需额外部署   多集群管理 ✅ 支持多集群中央管理 ❌ 单集群   仪表盘 ✅ 丰富的自定义仪表盘 ✅ 内置集群概览   告警与通知 ✅ 完整的告警系统 ❌ 不支持   性能分析 ✅ TopN 性能分析、实时监控 ✅ 热点线程捕捉   索引管理 ✅ 支持 ✅ 完整的索引管理（含限流、分片移动）   生命周期管理 ✅ ILM 管理 ✅ 可视化 ILM 配置   备份恢复 ✅ 支持 ✅ 快照管理   数据迁移 ✅ 高级数据迁移工具 ❌ 不支持   Dev Tools ✅ 支持 ✅ 支持（REST API + DSL）   部署成本 需要额外部署（Docker/K8s/二进制） 零部署成本（内置）    选择建议：\n 快速开始、集群基础管理：使用 Easysearch-UI（v1.15.0+） 专业监控、告警、多集群管理：使用 INFINI Console 最佳实践：Easysearch-UI 作为集群内置管理工具，INFINI Console 作为企业级监控平台，两者结合使用可实现完整的可观测性   小结 #     层次 关键指标 主要 API     可用性 集群状态、节点数、未分配分片 _cluster/health   资源 CPU、内存、磁盘、JVM 堆 _nodes/stats、_cat/nodes   性能 搜索/写入延迟、Bulk 拒绝 _nodes/stats/indices   行为 慢查询、错误率 慢查询日志    下一步阅读：\n  故障排查：面对告警时的诊断路径  容量规划：在问题发生前预留足够余量  备份还原：数据安全的最后一道防线  ","subcategory":null,"summary":"","tags":null,"title":"集群监控","url":"/easysearch/main/docs/operations/monitoring/"},{"category":null,"content":"Arabic 分析器 #  arabic 分析器是为阿拉伯语文本特别设计的语言分析器，包含了阿拉伯语特有的归一化和词干提取规则。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 stop 分词过滤器：过滤阿拉伯语停用词 arabic_normalization 分词过滤器：阿拉伯语字符归一化 arabic_stem 分词过滤器：阿拉伯语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;arabic\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;الكلاب تركض في الحديقة\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _arabic_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"阿拉伯语分析器（Arabic）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/arabic-analyzer/"},{"category":null,"content":"Path Hierarchy 分词器 #  path_hierarchy 分词器可以把文件路径（或类似的层次结构）的文本按照各个层级分解为词元。当处理诸如文件路径、网址或其他有分隔符的层次结构数据时，这个分词器特别有用。\n相关指南（先读这些） #    文本分析：识别词元  文本分析基础  参考样例 #  以下示例请求创建一个名为 my_index 的新索引，并配置一个使用路径词元生成器的分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_path_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;path_hierarchy\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_path_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_path_tokenizer\u0026#34; } } } } } 生成的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_path_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;/users/john/documents/report.txt\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;/users\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;/users/john\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;/users/john/documents\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 21, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;/users/john/documents/report.txt\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 32, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } 参数说明 #  路径词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     delimiter 可选 字符串 指定用于分隔路径组件的字符。默认值为 /。   replacement 可选 字符串 配置用于替换词元中分隔符的字符。默认值为 /。   buffer_size 可选 整数 指定缓冲区大小。默认值为 1024。   reverse 可选 布尔值 若为 true，则按逆序生成词元。默认值为 false。   skip 可选 整数 指定分词时要跳过的初始词元（层级）数量。默认值为 0。    使用分隔符和替换参数 #  以下示例请求配置了自定义的分隔符（delimiter）和替换(replacement)参数：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_path_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;path_hierarchy\u0026#34;, \u0026#34;delimiter\u0026#34;: \u0026#34;\\\\\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;/\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_path_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_path_tokenizer\u0026#34; } } } } } 使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_path_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;C:\\\\users\\\\john\\\\documents\\\\report.txt\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;C:\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;C:/users\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;C:/users/john\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;C:/users/john/documents\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 23, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;C:/users/john/documents/report.txt\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 34, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"路径层次分词器（Path Hierarchy）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/path-hierarchy/"},{"category":null,"content":"Spanish 分析器 #  spanish 分析器是为西班牙语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤西班牙语停用词 spanish_light_stem 分词过滤器：西班牙语轻量词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;spanish\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Los perros corren en el parque\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _spanish_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"西班牙语分析器（Spanish）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/spanish-analyzer/"},{"category":null,"content":"Portuguese 分析器 #  portuguese 分析器是为葡萄牙语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤葡萄牙语停用词 portuguese_light_stem 分词过滤器：葡萄牙语轻量词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;portuguese\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Os cães correm no parque\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _portuguese_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"葡萄牙语分析器（Portuguese）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/portuguese-analyzer/"},{"category":null,"content":"Dutch 分析器 #  dutch 分析器是为荷兰语文本特别设计的语言分析器，包含词干覆盖字典。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤荷兰语停用词 stemmer_override 分词过滤器：应用荷兰语词干覆盖字典 snowball(Dutch) 分词过滤器：荷兰语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;dutch\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;De honden rennen in het park\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _dutch_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"荷兰语分析器（Dutch）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/dutch-analyzer/"},{"category":null,"content":"English 分析器 #  english 分析器是为英语文本特别设计的语言分析器，包含了英语特定的处理规则和停用词。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤英语停用词（the, a, an, and 等） porter_stem 分词过滤器：英语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;english\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The quick brown foxes jumping over the lazy dogs\u0026#34; } 分析结果 #  停用词（the, over 等）被移除，词根被提取：\n[ \u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;fox\u0026#34;, \u0026#34;jump\u0026#34;, \u0026#34;lazi\u0026#34;, \u0026#34;dog\u0026#34; ] 相关指南 #    文本分析：词干提取  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"英语分析器（English）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/english-analyzer/"},{"category":null,"content":"Finnish 分析器 #  finnish 分析器是为芬兰语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤芬兰语停用词 snowball(Finnish) 分词过滤器：芬兰语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;finnish\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Koirat juoksevat puistossa\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _finnish_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"芬兰语分析器（Finnish）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/finnish-analyzer/"},{"category":null,"content":"Romanian 分析器 #  romanian 分析器是为罗马尼亚语文本特别设计的语言分析器，包含罗马尼亚语字符归一化。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤罗马尼亚语停用词 romanian_normalization 分词过滤器：罗马尼亚语字符归一化 snowball(Romanian) 分词过滤器：罗马尼亚语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;romanian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Câinii aleargă în parc\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _romanian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"罗马尼亚语分析器（Romanian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/romanian-analyzer/"},{"category":null,"content":"Sorani 分析器 #  sorani 分析器是为索拉尼库尔德语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 sorani_normalization 分词过滤器：索拉尼语字符归一化 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 stop 分词过滤器：过滤索拉尼语停用词 sorani_stem 分词过滤器：索拉尼语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;sorani\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;سەگەکان لە پارکەکەدا ڕادەکەن\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _sorani_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"索拉尼语分析器（Sorani）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/sorani-analyzer/"},{"category":null,"content":"Lithuanian 分析器 #  lithuanian 分析器是为立陶宛语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤立陶宛语停用词 snowball(Lithuanian) 分词过滤器：立陶宛语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;lithuanian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Šunys bėga parke\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _lithuanian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"立陶宛语分析器（Lithuanian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/lithuanian-analyzer/"},{"category":null,"content":"用户与角色 #  安全模块包括一个内部用户数据库。可以使用此数据库代替外部身份验证系统（如 LDAP 或 Active Directory），或作为外部身份验证系统的补充。\n角色是控制对集群访问的核心方式。角色包含集群范围权限、特定于索引的权限、文档和字段级安全性以及租户的任意组合。然后，将用户映射到这些角色，以便用户获得这些权限。\n除非您需要创建新的 只读或隐藏用户，我们强烈建议使用 REST API 来创建新的用户、角色和角色映射。.yml 文件更适合初始设置，而不是持续维护。\n相关指南（先读这些） #    安全与多租户最佳实践  权限控制总览  创建用户 #  user.yml #  参照 本地文件配置。\nREST API #  参照 创建用户。\n创建角色 #  role.yml #  参照 本地文件配置。\nREST API #  参照 创建角色。\n映射用户到角色 #  role_mapping.yml #  参照 本地文件配置。\nREST API #  参照 创建角色映射。\n","subcategory":null,"summary":"","tags":null,"title":"用户与角色","url":"/easysearch/main/docs/operations/security/access-control/users-roles/"},{"category":null,"content":"Swedish 分析器 #  swedish 分析器是为瑞典语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤瑞典语停用词 snowball(Swedish) 分词过滤器：瑞典语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;swedish\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Hundarna springer i parken\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _swedish_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"瑞典语分析器（Swedish）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/swedish-analyzer/"},{"category":null,"content":"Estonian 分析器 #  estonian 分析器是为爱沙尼亚语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤爱沙尼亚语停用词 snowball(Estonian) 分词过滤器：爱沙尼亚语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;estonian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Koerad jooksevad pargis\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _estonian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"爱沙尼亚语分析器（Estonian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/estonian-analyzer/"},{"category":null,"content":"Irish 分析器 #  irish 分析器是为爱尔兰语文本特别设计的语言分析器，使用爱尔兰语专用的小写转换和双重停用词过滤。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 stop(hyphenations) 分词过滤器：移除连字符 elision 分词过滤器：移除爱尔兰语省音符号 irish_lowercase 分词过滤器：爱尔兰语专用小写转换 stop 分词过滤器：过滤爱尔兰语停用词 snowball(Irish) 分词过滤器：爱尔兰语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;irish\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Rithfidh na madraí sa pháirc\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _irish_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"爱尔兰语分析器（Irish）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/irish-analyzer/"},{"category":null,"content":"Thai 分析器 #  thai 分析器是为泰语文本特别设计的语言分析器，使用 Java 内置的泰语分词算法。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n thai 分词器：使用 Java BreakIterator 进行泰语分词 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 stop 分词过滤器：过滤泰语停用词  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;thai\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;สุนัขวิ่งในสวน\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _thai_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"泰语分析器（Thai）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/thai-analyzer/"},{"category":null,"content":"Persian 分析器 #  persian 分析器是为波斯语文本特别设计的语言分析器，包含阿拉伯语归一化和波斯语字符过滤。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n persian_char_filter 字符过滤器：将零宽非连接符替换为空格 standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 arabic_normalization 分词过滤器：阿拉伯语字符归一化 persian_normalization 分词过滤器：波斯语字符归一化 stop 分词过滤器：过滤波斯语停用词  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;persian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;سگ‌ها در پارک می‌دوند\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _persian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"波斯语分析器（Persian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/persian-analyzer/"},{"category":null,"content":"Polish 分析器 #  polish 分析器是为波兰语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤波兰语停用词 polish_stem 分词过滤器：波兰语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;polish\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Psy biegają w parku\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _polish_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"波兰语分析器（Polish）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/polish-analyzer/"},{"category":null,"content":"French 分析器 #  french 分析器是为法语文本特别设计的语言分析器，包含省音处理。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 elision 分词过滤器：移除法语省音符号（l', d', qu' 等） lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤法语停用词 french_light_stem 分词过滤器：法语轻量词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;french\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Les chiens courent dans le parc\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _french_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"法语分析器（French）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/french-analyzer/"},{"category":null,"content":"Japanese 分析器 #  japanese 分析器是为日语文本设计的基础语言分析器，使用 CJK 二元组分词方式。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n cjk 分词器：将 CJK（中日韩）字符分解为二元组（bigrams） lowercase 分词过滤器：转换为小写 cjk_width 分词过滤器：全角/半角字符归一化  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;japanese\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;東京都の天気は晴れです\u0026#34; } 相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"日语分析器（Japanese）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/japanese-analyzer/"},{"category":null,"content":"Czech 分析器 #  czech 分析器是为捷克语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤捷克语停用词 czech_stem 分词过滤器：捷克语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;czech\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Psi běží v parku\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _czech_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"捷克语分析器（Czech）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/czech-analyzer/"},{"category":null,"content":"Norwegian 分析器 #  norwegian 分析器是为挪威语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤挪威语停用词 snowball(Norwegian) 分词过滤器：挪威语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;norwegian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Hundene løper i parken\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _norwegian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"挪威语分析器（Norwegian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/norwegian-analyzer/"},{"category":null,"content":"Latvian 分析器 #  latvian 分析器是为拉脱维亚语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤拉脱维亚语停用词 latvian_stem 分词过滤器：拉脱维亚语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;latvian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Suņi skrien parkā\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _latvian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"拉脱维亚语分析器（Latvian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/latvian-analyzer/"},{"category":null,"content":"Italian 分析器 #  italian 分析器是为意大利语文本特别设计的语言分析器，包含省音处理。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 elision 分词过滤器：移除意大利语省音符号 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤意大利语停用词 italian_light_stem 分词过滤器：意大利语轻量词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;italian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;I cani corrono nel parco\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _italian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"意大利语分析器（Italian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/italian-analyzer/"},{"category":null,"content":"German 分析器 #  german 分析器是为德语文本特别设计的语言分析器，包含德语归一化和轻量词干提取。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤德语停用词 german_normalization 分词过滤器：德语字符归一化（ä→a, ö→o, ü→u, ß→ss） german_light_stem 分词过滤器：德语轻量词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;german\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Die Hunde laufen im Park\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _german_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"德语分析器（German）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/german-analyzer/"},{"category":null,"content":"Greek 分析器 #  greek 分析器是为希腊语文本特别设计的语言分析器，使用专用的希腊语小写转换。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 greek_lowercase 分词过滤器：希腊语专用小写转换 stop 分词过滤器：过滤希腊语停用词 greek_stem 分词过滤器：希腊语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;greek\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Τα σκυλιά τρέχουν στο πάρκο\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _greek_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"希腊语分析器（Greek）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/greek-analyzer/"},{"category":null,"content":"Brazilian 分析器 #  brazilian 分析器是为巴西葡萄牙语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤巴西葡萄牙语停用词 brazilian_stem 分词过滤器：巴西葡萄牙语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;brazilian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Os cachorros correm no parque\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _brazilian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"巴西葡萄牙语分析器（Brazilian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/brazilian-analyzer/"},{"category":null,"content":"Basque 分析器 #  basque 分析器是为巴斯克语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤巴斯克语停用词 snowball(Basque) 分词过滤器：巴斯克语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;basque\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Euskara hizkuntza da\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _basque_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"巴斯克语分析器（Basque）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/basque-analyzer/"},{"category":null,"content":"Bengali 分析器 #  bengali 分析器是为孟加拉语文本特别设计的语言分析器，包含印度语系归一化处理。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 indic_normalization 分词过滤器：印度语系字符归一化 bengali_normalization 分词过滤器：孟加拉语字符归一化 stop 分词过滤器：过滤孟加拉语停用词 bengali_stem 分词过滤器：孟加拉语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;bengali\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;বাংলা ভাষার বিশ্লেষণ\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _bengali_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"孟加拉语分析器（Bengali）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/bengali-analyzer/"},{"category":null,"content":"Turkish 分析器 #  turkish 分析器是为土耳其语文本特别设计的语言分析器，使用土耳其语专用的小写转换。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 apostrophe 分词过滤器：移除撇号及其后字符 turkish_lowercase 分词过滤器：土耳其语专用小写转换（正确处理 İ/I） stop 分词过滤器：过滤土耳其语停用词 snowball(Turkish) 分词过滤器：土耳其语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;turkish\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Köpekler parkta koşuyor\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _turkish_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"土耳其语分析器（Turkish）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/turkish-analyzer/"},{"category":null,"content":"Indonesian 分析器 #  indonesian 分析器是为印度尼西亚语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤印度尼西亚语停用词 indonesian_stem 分词过滤器：印度尼西亚语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;indonesian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Anjing-anjing berlari di taman\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _indonesian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"印度尼西亚语分析器（Indonesian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/indonesian-analyzer/"},{"category":null,"content":"Hindi 分析器 #  hindi 分析器是为印地语文本特别设计的语言分析器，包含印度语系归一化处理。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 indic_normalization 分词过滤器：印度语系字符归一化 hindi_normalization 分词过滤器：印地语字符归一化 stop 分词过滤器：过滤印地语停用词 hindi_stem 分词过滤器：印地语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;hindi\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;कुत्ते पार्क में दौड़ रहे हैं\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _hindi_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"印地语分析器（Hindi）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hindi-analyzer/"},{"category":null,"content":"Hungarian 分析器 #  hungarian 分析器是为匈牙利语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤匈牙利语停用词 snowball(Hungarian) 分词过滤器：匈牙利语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;hungarian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;A kutyák futnak a parkban\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _hungarian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"匈牙利语分析器（Hungarian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hungarian-analyzer/"},{"category":null,"content":"Catalan 分析器 #  catalan 分析器是为加泰罗尼亚语文本特别设计的语言分析器，包含省音处理。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 elision 分词过滤器：移除加泰罗尼亚语省音符号 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤加泰罗尼亚语停用词 snowball(Catalan) 分词过滤器：加泰罗尼亚语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;catalan\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Els gossos corren al parc\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _catalan_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"加泰罗尼亚语分析器（Catalan）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/catalan-analyzer/"},{"category":null,"content":"Galician 分析器 #  galician 分析器是为加利西亚语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤加利西亚语停用词 galician_stem 分词过滤器：加利西亚语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;galician\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Os cans corren no parque\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _galician_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"加利西亚语分析器（Galician）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/galician-analyzer/"},{"category":null,"content":"Stop 分词过滤器 #  stop 分词过滤器用于在分词过程中从词元流中去除常见词汇（也称为停用词）。停用词通常是冠词和介词，比如 \u0026ldquo;a\u0026rdquo; 或 \u0026ldquo;for\u0026rdquo;。这些词在搜索查询中意义不大，并且常常被排除在外，以提高搜索效率和相关性。\n默认的英语停用词列表包含以下单词：a、an、and、are、as、at、be、but、by、for、if、in、into、is、it、no、not、of、on、or、such、that、the、their、then、there、these、they、this、to、was、will 以及 with。\n相关指南（先读这些） #    停用词  文本分析基础  参数说明 #  停用词分词过滤器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     stopwords 可选 字符串 既可以指定自定义的停用词数组，也可以指定一种语言以获取预定义的 Lucene 停用词列表。可指定的语言如下：\n- _arabic_\n- _armenian_\n- _basque_\n- _bengali_\n- _brazilian_（巴西葡萄牙语）\n- _bulgarian_\n- _catalan_\n- _cjk_（中文、日语和韩语）\n- _czech_\n- _danish_\n- _dutch_\n- _english_（默认值）\n- _estonian_\n- _finnish_\n- _french_\n- _galician_\n- _german_\n- _greek_\n- _hindi_\n- _hungarian_\n- _indonesian_\n- _irish_\n- _italian_\n- _latvian_\n- _lithuanian_\n- _norwegian_\n- _persian_\n- _portuguese_\n- _romanian_\n- _russian_\n- _sorani_\n- _spanish_\n- _swedish_\n- _thai_\n- _turkish_   stopwords_path 可选 字符串 指定包含自定义停用词的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。   ignore_case 可选 布尔值 如果设置为 true，则在匹配停用词时不区分大小写。默认值为 false。   remove_trailing 可选 布尔值 如果设置为 true，则在分析过程中会移除末尾的停用词。默认值为 true。    参考样例 #  以下示例请求创建了一个名为 my-stopword-index 的新索引，并配置了一个带有停用词过滤器的分词器，该过滤器使用预定义的英语停用词列表。\nPUT /my-stopword-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_stop_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;stopwords\u0026#34;: \u0026#34;_english_\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_stop_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_stop_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-stopword-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_stop_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;A quick dog jumps over the turtle\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;quick\u0026#34;, \u0026#34;start_offset\u0026#34;: 2, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;dog\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;jumps\u0026#34;, \u0026#34;start_offset\u0026#34;: 12, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;over\u0026#34;, \u0026#34;start_offset\u0026#34;: 18, \u0026#34;end_offset\u0026#34;: 22, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;, \u0026#34;start_offset\u0026#34;: 27, \u0026#34;end_offset\u0026#34;: 33, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 6 } ] } ","subcategory":null,"summary":"","tags":null,"title":"停用词分词过滤器（Stop）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stop/"},{"category":null,"content":"Bulgarian 分析器 #  bulgarian 分析器是为保加利亚语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤保加利亚语停用词 bulgarian_stem 分词过滤器：保加利亚语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;bulgarian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Кучетата тичат в парка\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _bulgarian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"保加利亚语分析器（Bulgarian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/bulgarian-analyzer/"},{"category":null,"content":"Russian 分析器 #  russian 分析器是为俄语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤俄语停用词 snowball(Russian) 分词过滤器：俄语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;russian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Собаки бегают в парке\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _russian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"俄语分析器（Russian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/russian-analyzer/"},{"category":null,"content":"Armenian 分析器 #  armenian 分析器是为亚美尼亚语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤亚美尼亚语停用词 snowball(Armenian) 分词过滤器：亚美尼亚语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;armenian\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Վիdelays հայերենի տեքdelays\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _armenian_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"亚美尼亚语分析器（Armenian）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/armenian-analyzer/"},{"category":null,"content":"Danish 分析器 #  danish 分析器是为丹麦语文本特别设计的语言分析器。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤丹麦语停用词 snowball(Danish) 分词过滤器：丹麦语词干提取  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;danish\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Hundene løber i parken\u0026#34; } 自定义配置 #  可通过以下参数自定义该分析器：\n   参数 说明     stopwords 自定义停用词列表，默认 _danish_   stem_exclusion 不进行词干提取的词语列表    相关指南 #    语言分析器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"丹麦语分析器（Danish）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/danish-analyzer/"},{"category":null,"content":"Fail 处理器 #  fail 处理器在索引过程中执行数据转换和丰富化非常有用。fail 处理器的主要用例是在满足某些条件时失败索引操作。\n以下是为 fail 处理器提供的语法：\n\u0026#34;fail\u0026#34;: { \u0026#34;if\u0026#34;: \u0026#34;ctx.foo == \u0026#39;bar\u0026#39;\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Custom error message\u0026#34; } 配置参数 #  下表列出了 fail 处理器所需的和可选参数。\n   参数 是否必填 描述     message 必填 要包含在失败响应中的自定义错误消息。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个名为 fail-log-pipeline 的管道，该管道使用 fail 处理器故意使管道执行失败，以处理日志事件：\nPUT _ingest/pipeline/fail-log-pipeline { \u0026#34;description\u0026#34;: \u0026#34;A pipeline to test the fail processor for log events\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;fail\u0026#34;: { \u0026#34;if\u0026#34;: \u0026#34;ctx.user_info.contains(\u0026#39;password\u0026#39;) || ctx.user_info.contains(\u0026#39;credit card\u0026#39;)\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Document containing personally identifiable information (PII) cannot be indexed!\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/fail-log-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;user_info\u0026#34;: \u0026#34;Sensitive information including credit card\u0026#34; } } ] } 返回内容确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;error\u0026#34;: { \u0026#34;root_cause\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;fail_processor_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Document containing personally identifiable information (PII) cannot be indexed!\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;fail_processor_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Document containing personally identifiable information (PII) cannot be indexed!\u0026#34; } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=fail-log-pipeline { \u0026#34;user_info\u0026#34;: \u0026#34;Sensitive information including credit card\u0026#34; } 请求因字符串 credit card 出现在 user_info 中而未能将日志事件索引到索引 testindex1 。以下响应包括在失败处理器中指定的自定义错误消息：\n\u0026#34;error\u0026#34;: { \u0026#34;root_cause\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;fail_processor_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Document containing personally identifiable information (PII) cannot be indexed!\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;fail_processor_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Document containing personally identifiable information (PII) cannot be indexed!\u0026#34; }, \u0026#34;status\u0026#34;: 500 } 步骤 4（可选）：检索文档 #  由于日志事件因管道失败而未索引，尝试检索它会导致文档未找到错误 \u0026quot;found\u0026quot;: false :\nGET testindex1/_doc/1 返回内容\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;found\u0026#34;: false } ","subcategory":null,"summary":"","tags":null,"title":"Fail 处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/fail/"},{"category":null,"content":"部署Easysearch Operator #  这里我们准备部署一个 3 节点的Easysearch 集群，准备 three-nodes-easysearch-cluster.yaml 文件，文件内容如下所示，并对关键字段都进行了注释。\napiVersion: infinilabs.infinilabs.com/v1 kind: SearchCluster # 自定义的资源类型 metadata: name: threenodes # Easysearch 集群的名称 namespace: default # Easysearch 集群所在的命名空间 spec: # 规格 security: # 安全相关 config: adminSecret: # admin证书配置 name: easysearch-admin-certs adminCredentialsSecret: # 账户密码配置 name: threenodes-admin-password tls: # tls 协议配置，包括节点间的transport，以及访问集群的http http: # 访问集群http配置 generate: false # 是否需要集群自动生成证书 secret: # 自定义证书配置 name: easysearch-certs transport: # 集群间访问配置 generate: false perNode: false # 是否给每一个节点配置证书 secret: # 自定义证书配置 name: easysearch-certs nodesDn: [\u0026#34;CN=Easysearch_Node\u0026#34;] adminDn: [\u0026#34;CN=Easysearch_Admin\u0026#34;] general: # 通用配置 snapshotRepositories: # s3 快照配置 - name: s3_repository # 配置的s3快照的名称 type: s3 # 快照类型 settings: # 快照配置 bucket: es-operator-bucket # s3中的桶，需提前建好 access_key: minioadmin # 访问s3密钥 secret_key: minioadmin endpoint: http://192.168.3.185:19000 # s3 访问地址 # compress: true version: \u0026#34;1.7.0-223\u0026#34; # Easysearch 版本 httpPort: 9200 # Easysearch 监听http端口 vendor: Easysearch serviceAccount: controller-manager # 访问k8s的service account，需要提前配置好，主要用于访问k8s资源 serviceName: threenodes monitoring: enable: false pluginsList: [] drainDataNodes: false # 在删除节点之前，是否需要先将节点上的数据迁移出去 nodePools: # Easysearch 节点池 - component: masters # 组件名称，作为标识，比如可以是masters，snapshotconfig securityconfig replicas: 3 # 配置集群节点个数 diskSize: \u0026#34;30Gi\u0026#34; # 每个节点占多少磁盘空间 jvm: -Xmx4G -Xms4G # jvm 参数 nodeSelector: # 调度时选择node resources: # 节点所需资源配置，包括申请值和最大值 requests: memory: \u0026#34;4Gi\u0026#34; cpu: \u0026#34;1\u0026#34; limits: memory: \u0026#34;5Gi\u0026#34; cpu: \u0026#34;2\u0026#34; roles: # 节点的角色，一般有master主节点，data数据节点，以及两者可 - \u0026#34;master\u0026#34; - \u0026#34;data\u0026#34; 编写好上述的 Operator yaml 文件，执行命令：\nkubectl create -f three-nodes-easysearch-cluster.yaml 为了快速组建集群，首先会创建一个 bootstrap 节点，然后各个节点同时并发启动，等待一段时间后集群创建成功，分别创建 3 个节点：\nthreenodes-masters-0 threenodes-masters-1 threenodes-masters-2 查看 pvc，对应到 3 个节点分别创建了 3 个 pvc:\ndata-threenodes-masters-0 data-threenodes-masters-1 data-threenodes-masters-2 如下图：\n查看 secret，已经创建了 threenodes-admin-password，用于保存账号密码\n进入 console-0，查看集群状态\ncurl -ku admin:xxxxxxxxxxxx https://threenodes.default.svc.cluster.local:9200 { \u0026quot;name\u0026quot; : \u0026quot;threenodes-masters-0\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;threenodes\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;WIuPcQ2gR4aG2hFrTLF4NQ\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;distribution\u0026quot; : \u0026quot;easysearch\u0026quot;, \u0026quot;number\u0026quot; : \u0026quot;1.7.0\u0026quot;, \u0026quot;distributor\u0026quot; : \u0026quot;INFINI Labs\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;b8a4c52487d245a65679fa4dc45db166cb919c4d\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2023-12-15T01:52:39.584409Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;8.11.2\u0026quot;, \u0026quot;minimum_wire_lucene_version\u0026quot; : \u0026quot;7.7.0\u0026quot;, \u0026quot;minimum_lucene_index_compatibility_version\u0026quot; : \u0026quot;7.7.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, For Easy Search!\u0026quot; } 可继续通过 console web 端操作集群\n至此，说明整个集群成功运行起来。\n  autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  https://asciinema.org/a/cvWISVzr23CPiHYeZwexF4DnP\n","subcategory":null,"summary":"","tags":null,"title":"部署 Easysearch","url":"/easysearch/main/docs/deployment/install-guide/operator/deploy_easysearch/"},{"category":null,"content":"通过 SQL 访问 Easysearch #  Easysearch 提供 SQL 查询接口，允许你用熟悉的 SQL 语法查询索引中的数据。这对于 BI 分析师、报表开发者和需要快速进行数据探索的场景非常方便。\n相关指南（先读这些） #    SQL 查询功能  REST Client API  能力概览 #     功能 支持情况 说明     SELECT ✅ 支持列选择、别名   WHERE ✅ 支持 AND/OR/NOT、比较运算、LIKE、IN   ORDER BY ✅ 支持多列排序   GROUP BY ✅ 支持分组聚合   聚合函数 ✅ COUNT、SUM、AVG、MIN、MAX 等   LIMIT ✅ 支持分页   HAVING ✅ 聚合后过滤   JOIN ⚠️ 有限支持   子查询 ⚠️ 部分场景支持   DDL (CREATE/DROP) ❌ 不支持，使用 REST API 管理索引    使用方式 #  REST API 直接查询 #  POST _sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT speaker, COUNT(*) as cnt FROM shakespeare GROUP BY speaker ORDER BY cnt DESC LIMIT 5\u0026#34; } 返回表格格式的结果：\n{ \u0026#34;schema\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;speaker\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;cnt\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; } ], \u0026#34;datarows\u0026#34;: [ [\u0026#34;GLOUCESTER\u0026#34;, 556], [\u0026#34;HAMLET\u0026#34;, 532], [\u0026#34;KING HENRY V\u0026#34;, 428], [\u0026#34;KING RICHARD II\u0026#34;, 405], [\u0026#34;DUKE OF YORK\u0026#34;, 394] ], \u0026#34;total\u0026#34;: 5, \u0026#34;size\u0026#34;: 5 } 查看执行计划 #  使用 explain API 查看 SQL 语句转换后的 DSL：\nPOST _sql/_explain { \u0026#34;query\u0026#34;: \u0026#34;SELECT * FROM shakespeare WHERE speaker = \u0026#39;HAMLET\u0026#39; LIMIT 10\u0026#34; } 这会返回等价的 Query DSL JSON，帮助你理解和优化查询。\n常用查询示例 #  条件过滤 #  SELECT play_name, speaker, text_entry FROM shakespeare WHERE play_name = \u0026#39;Hamlet\u0026#39; AND speaker IN (\u0026#39;HAMLET\u0026#39;, \u0026#39;HORATIO\u0026#39;) LIMIT 20 聚合统计 #  SELECT play_name, COUNT(*) as lines, COUNT(DISTINCT speaker) as characters FROM shakespeare GROUP BY play_name ORDER BY lines DESC LIMIT 10 日期范围过滤 #  SELECT * FROM logs WHERE @timestamp \u0026gt;= \u0026#39;2024-01-01\u0026#39; AND @timestamp \u0026lt; \u0026#39;2024-02-01\u0026#39; ORDER BY @timestamp DESC LIMIT 100 JDBC 驱动 #  Easysearch 提供 JDBC 驱动，可以让标准的数据库工具（如 DBeaver、DataGrip）直接连接：\njdbc:easysearch://https://localhost:9200 配合 BI 工具 #     工具 连接方式 说明     INFINI Console 内置 SQL 编辑器 开箱即用   DBeaver JDBC 驱动 通用数据库管理工具   Grafana Elasticsearch 数据源 兼容 Elasticsearch 数据源插件   Superset 见 Superset 集成 需要配置数据源    注意事项 #   SQL 查询最终会被转换为 Easysearch 的 Query DSL 执行，部分复杂的 SQL 语法可能无法完全支持 对于高频生产查询，建议直接使用 Query DSL 以获得更精细的控制 全文搜索功能（如 MATCH）可以在 WHERE 子句中使用：WHERE MATCH(field, 'keyword')  ","subcategory":null,"summary":"","tags":null,"title":"通过 SQL 访问 Easysearch","url":"/easysearch/main/docs/integrations/clients/sql/"},{"category":null,"content":"轻量 Agent 接入：Filebeat / Fluent Bit #  Filebeat 和 Fluent Bit 是两款常用的轻量级日志采集 Agent，均可将日志数据直接发送到 Easysearch。\n相关指南（先读这些） #    Logstash 接入  摄取管道  Agent 对比 #     特性 Filebeat Fluent Bit     语言 Go C   内存占用 ~30-50 MB ~5-10 MB   配置方式 YAML INI / YAML   输出到 ES ✅ 原生支持 ✅ 原生支持（es output）   Ingest Pipeline ✅ 支持指定 ✅ 支持指定   多行日志 ✅ multiline 模块 ✅ multiline parser   生态模块 丰富（modules for nginx等） 较少，但灵活的 parser 体系   适用场景 通用日志采集 边缘设备、资源受限环境    Filebeat 配置示例 #  基础配置 #  # filebeat.yml filebeat.inputs: - type: log enabled: true paths: - /var/log/app/*.log # 多行日志合并（如 Java 异常堆栈） multiline.pattern: \u0026#39;^\\d{4}-\\d{2}-\\d{2}\u0026#39; multiline.negate: true multiline.match: after output.elasticsearch: hosts: [\u0026quot;https://easysearch-node1:9200\u0026quot;] username: \u0026quot;admin\u0026quot; password: \u0026quot;your_password\u0026quot; ssl.verification_mode: \u0026quot;none\u0026quot; # 生产环境应配置 CA 证书 index: \u0026quot;app-logs-%{+yyyy.MM.dd}\u0026quot; pipeline: \u0026quot;app-log-pipeline\u0026quot; # 可选：指定 Ingest Pipeline\n# 索引模板设置 setup.template.name: \u0026quot;app-logs\u0026quot; setup.template.pattern: \u0026quot;app-logs-*\u0026quot; setup.ilm.enabled: false # 按需开启 ILM 使用 Filebeat 模块 #\n Filebeat 提供开箱即用的模块（如 nginx、system、mysql 等），可以自动解析常见日志格式：\nfilebeat.modules: - module: nginx access: enabled: true var.paths: [\u0026#34;/var/log/nginx/access.log\u0026#34;] error: enabled: true output.elasticsearch: hosts: [\u0026quot;https://easysearch-node1:9200\u0026quot;] username: \u0026quot;admin\u0026quot; password: \u0026quot;your_password\u0026quot; Fluent Bit 配置示例 #\n # fluent-bit.conf [SERVICE] Flush 5 Log_Level info [INPUT] Name tail Path /var/log/app/.log Tag app. Multiline On Parser_Firstline multiline_java\n[FILTER] Name record_modifier Match app.* Record hostname ${HOSTNAME}\n[OUTPUT] Name es Match app.* Host easysearch-node1 Port 9200 HTTP_User admin HTTP_Passwd your_password tls On tls.verify Off Index app-logs Type _doc Pipeline app-log-pipeline Suppress_Type_Name On 架构选型 #\n 根据数据量和可靠性要求，可以选择不同的接入架构：\n方案 A：直写（简单，适合中小规模） Agent → Easysearch 方案 B：加缓冲（推荐，适合大规模） Agent → Kafka/Redis → Logstash → Easysearch\n方案 C：网关模式 Agent → INFINI Gateway → Easysearch \n  方案 优势 劣势     直写 架构简单，延迟低 流量突增可能压垮 Easysearch   加缓冲 流量削峰、数据不丢失 架构复杂，延迟增加   网关 限流、路由、格式转换 需要部署 Gateway 组件    生产建议 #   始终配置 死信队列（DLQ），避免解析失败的日志丢失 设置合理的 batch size 和 flush interval，平衡延迟与吞吐 使用 Ingest Pipeline 在 Easysearch 端做字段提取、时间解析等预处理 为日志索引配置 索引生命周期管理（ILM） 或定期清理策略 监控 Agent 的内存和 CPU 使用，避免采集端成为瓶颈  ","subcategory":null,"summary":"","tags":null,"title":"轻量 Agent 接入：Filebeat / Fluent Bit","url":"/easysearch/main/docs/integrations/ingest/filebeat-fluentbit/"},{"category":null,"content":"路径配置 #  本页介绍 easysearch.yml 中与文件存储路径相关的配置项。这些都是静态设置，修改后需要重启节点生效。\n path.data #  path.data: /data/easysearch/data    项目 说明     参数 path.data   默认值 $ES_HOME/data   属性 静态   说明 索引数据的存储目录。Easysearch 的所有分片数据（Lucene 段文件、translog）都存储在此目录下。这是磁盘 I/O 最密集的路径    注意事项 #   不要使用默认路径：默认路径在安装目录内（$ES_HOME/data），升级时可能被覆盖。生产环境必须设置独立路径。 目录权限：运行 Easysearch 的用户必须对该目录拥有读写权限。 磁盘选择：建议使用 SSD 存储，可显著提升索引和查询性能。 不要在多个节点之间共享：每个节点的 path.data 必须是独立的目录。  多路径（JBOD） #  支持配置多个数据路径，实现 JBOD（Just a Bunch of Disks）条带化，将分片分散到多块磁盘上：\n# YAML 列表形式 path.data: - /data1/easysearch - /data2/easysearch - /data3/easysearch 多路径行为说明：\n Easysearch 会将不同的分片分配到不同的路径上。 单个分片的所有文件始终存储在同一路径下，不会跨磁盘。 磁盘空间的均衡取决于分片的大小和数量。 如果其中一块磁盘故障，该磁盘上的所有分片都会不可用，但由于有副本机制，数据不会丢失。   如果需要磁盘级别的冗余，建议使用 RAID 或云盘。参见 RAID 配置。\n  path.logs #  path.logs: /data/easysearch/logs    项目 说明     参数 path.logs   默认值 $ES_HOME/logs   属性 静态   说明 日志文件的存储目录。包括主日志、慢日志、GC 日志等    注意事项 #   同样不建议使用默认路径。 日志目录应与数据目录分开，避免日志写入影响数据磁盘 I/O。 确保有足够的磁盘空间存储日志，尤其在开启 DEBUG 日志时。  日志文件说明 #     文件 说明     \u0026lt;cluster.name\u0026gt;.log 主日志文件   \u0026lt;cluster.name\u0026gt;_server.json JSON 格式日志（便于日志采集）   \u0026lt;cluster.name\u0026gt;_deprecation.log API 弃用警告日志   \u0026lt;cluster.name\u0026gt;_slowlog.log 慢查询/慢索引日志   gc.log JVM GC 日志     path.repo #  path.repo: [\u0026#34;/mnt/backup/easysearch\u0026#34;]    项目 说明     参数 path.repo   默认值 未设置   属性 静态   说明 快照仓库的共享文件系统路径。使用 fs 类型的快照仓库时必须配置此项    注意事项 #   该路径必须在所有 master 和 data 节点上都可访问（通常通过 NFS 共享挂载）。 支持多个路径（YAML 列表形式）。 创建快照仓库时指定的 location 必须是 path.repo 下的子目录。  使用示例 #  步骤 1：配置 path.repo（所有节点）\npath.repo: - /mnt/nfs/backup - /mnt/nfs/archive 步骤 2：创建快照仓库\nPUT /_snapshot/my_backup { \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;/mnt/nfs/backup/my_repo\u0026#34; } }  path.shared_data #  path.shared_data: /data/easysearch/shared_data    项目 说明     参数 path.shared_data   默认值 未设置   属性 静态   说明 集群内所有节点共享的数据目录（通常用于共享库、字典等）。可选配置    注意事项 #   该路径应该在所有节点上都可访问，通常通过 NFS 挂载。 主要用于存储需要在集群间共享的资源（如分析器字典、插件数据等）。   完整配置示例 #  生产环境（单磁盘） #  path.data: /data/easysearch/data path.logs: /var/log/easysearch path.repo: [\u0026#34;/mnt/nfs/easysearch-backup\u0026#34;] 生产环境（多磁盘 JBOD） #  path.data: - /data1/easysearch - /data2/easysearch - /data3/easysearch - /data4/easysearch path.logs: /var/log/easysearch path.repo: [\u0026#34;/mnt/nfs/backup\u0026#34;] 开发/测试环境 #  # 使用默认路径即可，无需显式配置 # path.data: data # path.logs: logs  延伸阅读 #    硬件配置 — 磁盘选型建议（SSD vs HDD、JBOD vs RAID）  RAID 配置 — JBOD 多路径 vs. RAID 方案对比  日志配置 — log4j2.properties 日志格式与级别  ","subcategory":null,"summary":"","tags":null,"title":"路径配置","url":"/easysearch/main/docs/deployment/config/node-settings/path/"},{"category":null,"content":"Language 分析器 #  Easysearch 内置了 34 种语言专用分析器，每种都针对该语言的停用词、词干提取和字符归一化进行了优化。使用时只需将 analyzer 设置为对应的语言名称即可。\n支持的语言列表 #     语言 分析器名称 特点      阿拉伯语 arabic 阿拉伯语归一化 + 词干提取    亚美尼亚语 armenian Snowball 词干提取    巴斯克语 basque Snowball 词干提取    孟加拉语 bengali 印度语系归一化 + 词干提取    巴西葡萄牙语 brazilian 巴西葡萄牙语词干提取    保加利亚语 bulgarian 保加利亚语词干提取    加泰罗尼亚语 catalan 省音处理 + Snowball 词干提取    捷克语 czech 捷克语词干提取    丹麦语 danish Snowball 词干提取    荷兰语 dutch 词干覆盖字典 + Snowball    英语 english 所有格处理 + Porter 词干提取    爱沙尼亚语 estonian Snowball 词干提取    芬兰语 finnish Snowball 词干提取    法语 french 省音处理 + 轻量词干提取    加利西亚语 galician 加利西亚语词干提取    德语 german 字符归一化 + 轻量词干提取    希腊语 greek 希腊语专用小写 + 词干提取    印地语 hindi 印度语系归一化 + 词干提取    匈牙利语 hungarian Snowball 词干提取    印度尼西亚语 indonesian 印度尼西亚语词干提取    爱尔兰语 irish 专用小写 + 双重停用词过滤    意大利语 italian 省音处理 + 轻量词干提取    拉脱维亚语 latvian 拉脱维亚语词干提取    立陶宛语 lithuanian Snowball 词干提取    挪威语 norwegian Snowball 词干提取    波斯语 persian 字符过滤 + 阿拉伯语/波斯语归一化    波兰语 polish 波兰语词干提取    葡萄牙语 portuguese 轻量词干提取    罗马尼亚语 romanian 字符归一化 + Snowball 词干提取    俄语 russian Snowball 词干提取    索拉尼语 sorani 索拉尼语归一化 + 词干提取    西班牙语 spanish 轻量词干提取    瑞典语 swedish Snowball 词干提取    泰语 thai Java BreakIterator 泰语分词    土耳其语 turkish 专用小写（İ/I）+ Snowball    使用方式 #  在映射中指定对应语言名称即可：\nPUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;french\u0026#34; } } } } 也可以设置为索引的默认分析器：\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;french\u0026#34; } } } } } 通用参数 #  所有语言分析器都支持以下参数（部分语言除外）：\n   参数 说明     stopwords 自定义停用词列表，默认为对应语言的内置停用词   stem_exclusion 不进行词干提取的词语列表    相关指南 #    文本分析基础  文本分析：识别词元  ","subcategory":null,"summary":"","tags":null,"title":"语言分析器（Language）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/language-analyzer/"},{"category":null,"content":"Stemmer Override 分词过滤器 #  stemmer_override 分词过滤器允许你定义自定义的词干提取规则，这些规则可以覆盖像波特（Porter）或雪球（Snowball）等默认词干提取器的行为。当你希望对某些特定的单词应用特殊的词干提取方式，而这些单词可能无法被标准的词干提取算法正确处理时，这个过滤器就会非常有用。\n相关指南（先读这些） #    文本分析：词干提取  文本分析：规范化  参数说明 #  词干覆盖分词过滤器必须使用以下参数中的一个进行配置。\n   参数 数据类型 描述     rules 字符串 直接在设置中定义覆盖规则。   rules_path 字符串 指定包含自定义规则（映射）的文件的路径。该路径可以是绝对路径，也可以是相对于配置目录的相对路径。    参考样例 #  以下示例请求创建了一个名为 my-index 的新索引，并配置了一个带有词干覆盖过滤器的分词器。\nPUT /my-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_stemmer_override_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer_override\u0026#34;, \u0026#34;rules\u0026#34;: [ \u0026#34;running, runner =\u0026gt; run\u0026#34;, \u0026#34;bought =\u0026gt; buy\u0026#34;, \u0026#34;best =\u0026gt; good\u0026#34; ] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_stemmer_override_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;I am a runner and bought the best shoes\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;i\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;am\u0026#34;, \u0026#34;start_offset\u0026#34;: 2, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;a\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;and\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;buy\u0026#34;, \u0026#34;start_offset\u0026#34;: 18, \u0026#34;end_offset\u0026#34;: 24, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 }, { \u0026#34;token\u0026#34;: \u0026#34;the\u0026#34;, \u0026#34;start_offset\u0026#34;: 25, \u0026#34;end_offset\u0026#34;: 28, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 6 }, { \u0026#34;token\u0026#34;: \u0026#34;good\u0026#34;, \u0026#34;start_offset\u0026#34;: 29, \u0026#34;end_offset\u0026#34;: 33, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 7 }, { \u0026#34;token\u0026#34;: \u0026#34;shoes\u0026#34;, \u0026#34;start_offset\u0026#34;: 34, \u0026#34;end_offset\u0026#34;: 39, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 8 } ] } ","subcategory":null,"summary":"","tags":null,"title":"词干覆盖分词过滤器（Stemmer Override）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stemmer-override/"},{"category":null,"content":"自定义规范化器 #  通过组合字符过滤器和词元过滤器，可以创建满足特定需求的自定义规范化器（normalizer）。\n基本结构 #  PUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;normalizer\u0026#34;: { \u0026#34;\u0026lt;normalizer_name\u0026gt;\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [], \u0026#34;filter\u0026#34;: [] } } } } } 参数 #     参数 类型 必选 说明     type string 是 固定为 custom   char_filter array 否 字符过滤器列表，在词元过滤器之前执行   filter array 否 词元过滤器列表     示例 #  大小写 + ASCII 折叠 #  最常用的组合——同时忽略大小写和变音符号：\nPUT sample-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;normalizer\u0026#34;: { \u0026#34;normalized_keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [], \u0026#34;filter\u0026#34;: [\u0026#34;asciifolding\u0026#34;, \u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;approach\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;normalized_keyword\u0026#34; } } } } 索引一个文档：\nPOST /sample-index/_doc/ { \u0026#34;approach\u0026#34;: \u0026#34;naive\u0026#34; } 以下查询可以匹配该文档：\nGET /sample-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;approach\u0026#34;: \u0026#34;Naïve\u0026#34; } } } 因为 normalizer 将 \u0026quot;Naïve\u0026quot; 和 \u0026quot;naive\u0026quot; 都归一化为 \u0026quot;naive\u0026quot;。\n验证归一化效果：\nGET /sample-index/_analyze { \u0026#34;normalizer\u0026#34;: \u0026#34;normalized_keyword\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Naïve\u0026#34; } 字符映射归一化 #  使用 mapping 字符过滤器替换特定字符，适合产品编码等场景：\nPUT product-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;dash_to_underscore\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mapping\u0026#34;, \u0026#34;mappings\u0026#34;: [\u0026#34;- =\u0026gt; _\u0026#34;] } }, \u0026#34;normalizer\u0026#34;: { \u0026#34;product_code_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;dash_to_underscore\u0026#34;], \u0026#34;filter\u0026#34;: [\u0026#34;uppercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;product_code\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;product_code_normalizer\u0026#34; } } } } 这样 \u0026quot;abc-123\u0026quot;、\u0026quot;ABC-123\u0026quot;、\u0026quot;abc_123\u0026quot; 都会被归一化为 \u0026quot;ABC_123\u0026quot;。\nCJK 全半角归一化 #  处理中日韩文本中的全角/半角字符差异：\nPUT cjk-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;normalizer\u0026#34;: { \u0026#34;cjk_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;cjk_width\u0026#34;, \u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;code\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;cjk_normalizer\u0026#34; } } } } 全角字母 \u0026quot;Ａｐｐｌｅ\u0026quot; 和半角 \u0026quot;Apple\u0026quot; 都会归一化为 \u0026quot;apple\u0026quot;。\n正则替换 #  使用 pattern_replace 字符过滤器去除空白或格式化字符：\nPUT clean-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;strip_spaces\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern_replace\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;\\\\s+\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;normalizer\u0026#34;: { \u0026#34;compact_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;strip_spaces\u0026#34;], \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;serial\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;compact_normalizer\u0026#34; } } } } 这样 \u0026quot;AB CD 123\u0026quot; 和 \u0026quot;abcd123\u0026quot; 都会归一化为 \u0026quot;abcd123\u0026quot;。\n多语言归一化 #  对阿拉伯语关键字进行字符归一化：\nPUT arabic-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;normalizer\u0026#34;: { \u0026#34;arabic_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;arabic_normalization\u0026#34;, \u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;tag\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;arabic_normalizer\u0026#34; } } } }  在映射中使用 #  定义后，在字段映射中通过 normalizer 参数引用：\n{ \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;my_normalizer\u0026#34; } } } } 多字段配合 #  如果需要同时保留精确值和归一化值，使用多字段：\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;normalizer\u0026#34;: { \u0026#34;folding_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;asciifolding\u0026#34;, \u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;brand\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;normalized\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;folding_normalizer\u0026#34; } } } } } }  brand — 精确匹配，区分大小写 brand.normalized — 归一化匹配，忽略大小写和变音符号   可用的过滤器 #  自定义 normalizer 只能使用字符级操作的过滤器，完整列表请参阅 规范化器概述。\n注意事项 #   过滤器顺序有影响：[\u0026quot;asciifolding\u0026quot;, \u0026quot;lowercase\u0026quot;] 先移除变音符号再转小写，通常这个顺序最佳 char_filter 先于 filter 执行：字符过滤器在词元过滤器之前处理 不支持分词器：normalizer 永远不会把一个值拆成多个词元 索引内定义：每个 normalizer 定义在具体索引的 settings 中，不能跨索引共享  ","subcategory":null,"summary":"","tags":null,"title":"自定义规范化器","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/normalizers/custom/"},{"category":null,"content":"结构化搜索 #  结构化搜索针对的是“精确可判定”的字段：ID、枚举、时间、数值、状态位等。本页重点讲如何用好 term/terms/range/exists 这几类基础查询，并配合 filter 上下文使用。\n适用字段与“不适用”的情况 #  适合用结构化查询的典型字段：\n 关键词/编码：订单号、用户 ID、设备 ID、状态码 数值：价格、年龄、计数、评分 时间：创建时间、更新时间、业务时间 标志位/枚举：is_deleted、status、type 等  不适合用结构化查询的字段：\n 需要做全文检索的长文本（标题、描述、评论等）\n→ 应使用 match/match_phrase 等全文查询，详见“全文检索”章节。  term：精确匹配 #  term 查询不会做分词，也不会自动大小写转换，适合：\n keyword 字段 数值字段 布尔/枚举字段  典型用法（以伪 JSON 示意）：\n{ \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } } } 建议：\n 对 text 字段做 term 查询通常是错误的（除非你非常明确知道底层分析行为） 业务 ID、编码类字段建议使用 keyword 类型并用 term 查询  terms：在一组值中匹配 #  terms 用于“IN 列表”场景：\n{ \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;status\u0026#34;: [\u0026#34;active\u0026#34;, \u0026#34;pending\u0026#34;] } } } 典型场景：\n 过滤一组允许状态 从一批 ID 中筛选（注意列表长度与性能）   经验：非常长的 ID 列表（如几万、几十万）建议通过临时索引/标记字段等方式重构，而不是直接塞进 terms。\n range：范围查询 #  range 用于数值或时间范围：\n{ \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;created_at\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2024-01-01T00:00:00Z\u0026#34;, \u0026#34;lt\u0026#34;: \u0026#34;2024-02-01T00:00:00Z\u0026#34; } } } } 常用比较符号：\n gt / gte：大于（等于） lt / lte：小于（等于）  建议：\n 时间范围过滤几乎总是应该放在 filter 中（不参与打分，可缓存） 组合多个 range 时注意边界是否重叠/有缝  exists：字段存在性判断 #  exists 用于判断字段是否存在：\n{ \u0026#34;query\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;email\u0026#34; } } } 适合：\n 筛选“有值/缺失”的数据 区分“null/缺失” vs “有有效值”   小贴士：很多日志/埋点场景里，一部分字段是“有就有，没有就压根不出现在 JSON 里”，这时候 exists 比“用特殊值占位再 term 查询”要自然得多。\n 和 bool/filter 结合使用 #  在实际查询中，这些结构化条件通常放到 bool.filter 里：\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;搜索\u0026#34; } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } }, { \u0026#34;range\u0026#34;: { \u0026#34;created_at\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-7d/d\u0026#34; } } } ] } } } 好处：\n 不参与 _score 计算，语义更清晰（它们本来就是“硬过滤条件”） 更容易被缓存，提高重复查询性能  组合示例：订单列表的典型过滤 #  一个稍微贴近业务的例子：电商后台的“订单列表”过滤：\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;buyer_keywords\u0026#34;: \u0026#34;北京 手机\u0026#34; } } // 主搜索词：全文检索 ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;paid\u0026#34; }}, // 已支付 { \u0026#34;terms\u0026#34;: { \u0026#34;channel\u0026#34;: [\u0026#34;app\u0026#34;, \u0026#34;web\u0026#34;] }}, // 下单渠道 { \u0026#34;range\u0026#34;: { \u0026#34;order_time\u0026#34;: { // 最近 30 天 \u0026#34;gte\u0026#34;: \u0026#34;now-30d/d\u0026#34; }} }, { \u0026#34;term\u0026#34;: { \u0026#34;tenant_id\u0026#34;: \u0026#34;t_123\u0026#34; }} // 多租户隔离 ] } } } 可以看到：\n 所有“业务约束/权限/时间范围”都在 filter 里 只有真正影响排序/相关性的条件放在 must  典型实践小结 #   所有“业务约束/权限/状态/时间范围”优先用结构化查询表达，并放进 filter 尽量避免对 text 字段做 term/terms/range 查询，除非你确实要在分词后的 token 上做精确匹配 大批量 ID 过滤要关注 terms 的规模与性能，必要时用索引设计/标记字段重构问题  下一步可以继续阅读：\n  全文搜索  分页与排序  相关性  参考手册（API 与参数） #    搜索操作概览（功能手册）  精确/Term 查询（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"结构化搜索","url":"/easysearch/main/docs/features/query-dsl/structured-search/"},{"category":null,"content":"Simple Pattern Split 分词器 #  simple_pattern_split 分词器使用正则表达式将文本分割成词元。该正则表达式定义了用于确定文本分割位置的模式。文本中任何匹配该模式的部分都会被用作分隔符，分隔符之间的文本则成为一个词元。当你想定义分隔符，并根据某个模式对文本的其余部分进行分词时，可以使用此分词器。\n该分词器仅将输入文本中匹配到的部分（基于正则表达式）用作分隔符或边界，以将文本分割成词条。匹配到的部分不会包含在生成的词元中。例如，如果将分词器配置为在点号（.）处分割文本，输入文本为 one.two.three，那么生成的词元就是 one、two 和 three。点号本身不会包含在生成的词条中。\n相关指南（先读这些） #    文本分析：识别词元  文本分析基础  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用简单分割匹配词元生成器的分词器。该分词器被配置为在连字符（-）处分割文本：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_pattern_split_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;simple_pattern_split\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;-\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_pattern_split_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_pattern_split_tokenizer\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_pattern_split_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_pattern_split_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;2024-10-09\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;2024\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;09\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } 参数说明 #  简单分割匹配词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     max_token_length 可选 整数 设置生成词元的最大长度。若超过该长度，词元将按照 max_token_length 所配置的长度拆分为多个词元。默认值为 255。    ","subcategory":null,"summary":"","tags":null,"title":"简单模式分割分词器（Simple Pattern Split）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/simple-pattern-split/"},{"category":null,"content":"查询调优与慢查询排查 #  这篇不是“参数大全”，而是一个从症状出发的调优路线：给你一套在遇到“搜索慢/结果怪”时可以照着走的步骤，并总结几类典型反模式。\n1. 先确认：是“慢”，还是“查不准”？ #  调优之前先判断你面对的是哪类问题：\n 性能问题：延迟高、QPS 上不去、偶发长尾 相关性问题：该上的内容没上来、不该上的跑前面 资源问题：CPU/内存/磁盘/网络被拖垮  很多时候，这三类是纠缠在一起的，但你要先选一个“主目标”：是要先跑得稳，还是先查得准。\n2. 性能调优：从慢查询样本开始 #  2.1 收集慢查询样本 #   打开搜索 slowlog（按索引粒度）：  记录超过某个阈值的查询（P95/P99 级别）   从 slowlog 和客户端日志里挑出：  最常出现的慢查询模式 最耗资源的异常查询（比如 fan-out 到海量分片）    2.2 用 profile 看“时间花在哪” #  对代表性查询加上一层 profile: true，观察：\n 是哪一部分耗时最长：  filter 还是 query？ 排序/聚合 是否拖慢了整体？   是否在某些字段上做了昂贵操作：  对高基数字段做排序/聚合 对海量分片做全量 scan    2.3 常见可落地的改造方向 #   把“硬过滤”移到 filter：  所有业务强约束（租户、状态、时间范围）尽量放到 bool.filter，提升缓存命中率。   减少 fan-out：  合理设置索引边界，避免一个查询打到过多分片。 使用 preference / routing 让请求更集中。   优化排序/聚合字段选择：  避免对高基数字段做深度排序/嵌套聚合。 观察是否可以用预计算/冗余字段替代复杂 script。    3. 相关性调优：从 explain/_score 入手 #  3.1 用 explain 看“一条结果为什么得这个分” #  对单条代表性命中文档使用 explain：\n 哪些子查询命中了？ 每个子查询贡献了多少 _score？ 某些不该影响排序的条件是否参与了打分？  结合 bool 结构：\n must/should：负责“贡献分数” filter：只负责“能不能进候选集”  如果发现很多“硬条件”也在 must 里，就可以考虑迁移到 filter。\n3.2 优先做“结构级别”的调整 #  在调参数前，先检查：\n mapping 是否合理（text/keyword 分工、多字段设计） 是否使用了合适的查询类型：  精确字段用 term/terms 而不是 match 自然语言字段用 match/multi_match 而不是 term    然后再考虑：\n minimum_should_match：控制“至少命中多少词” boost / 字段权重：提升标题、重要字段的影响力 function_score：叠加业务信号（热度、点击、时间衰减等）  4. 调优 Checklist：查询结构与 DSL 写法 #  这一段可以当成写 DSL 时的“自查表”：\n bool 结构：  是否清晰区分了 must / should / filter / must_not？ 是否有过度嵌套的 bool，可以简化为单层？   filter 使用：  所有“硬约束”（租户、权限、状态、时间范围）是否都放在 filter？ 是否有可以被缓存的条件混在 query 里？   字段与查询类型匹配：  text 字段是否主要用 match 系列？ keyword/数值/日期字段是否主要用 term/range/聚合？   分页与 from/size：  是否在做非常深的分页（from 很大）？\n→ 可以考虑 search_after 或滚动扫描（scroll/point-in-time）。    5. 典型反模式清单（看到就该警觉） #    在 text 字段上大量使用通配符/正则（*foo*/.*foo.*）：\n 词典扫描开销巨大，极易拖垮节点。 建议：只在极少量字段上、配合足够长的前缀使用；更推荐索引时用 n-grams 方案。    到处是 script_score，且逻辑复杂：\n script 在评分阶段执行，很难缓存，CPU 压力大。 建议：能用字段 + function_score 的地方尽量不用 script_score，把重逻辑前移到索引/预计算。    深分页（from/size 非常大）还要求排序稳定：\n 代价：每个分片都要维护超大的优先队列，内存/CPU 消耗非常高。 建议：用 search_after，让翻页基于上一页的 sort 值，而不是偏移量。    query 与 filter 乱用：\n 例如把权限条件也放到 must 里，既影响排序又浪费分数计算。 建议：权限/租户/状态/固定标签等，都放在 filter。    单个请求打所有索引/所有分片：\n 常见于 UI 默认为 _all 或 *，且没有任何 filter。 建议：为不同业务提供有边界的索引/alias，避免“全库扫描式”的查询。    6. 和运维侧协同：不要只盯着 DSL #  查询调优不只在 DSL 层面，和运维侧配合也很重要：\n 是否有足够的硬件支撑当前查询模式？ 是否需要拆读写集群，或为重查询业务建专用索引/集群？ 是否在监控中设置了合适的延迟/错误告警阈值？  7. 小结与延伸阅读 #   慢查询调优从 样本 开始：slowlog → profile → explain 先改“结构与 mapping”，再改“参数与权重” bool/filter 结构、字段类型、查询类型是否匹配，是最常见的突破口 谨慎使用 wildcard/regexp/script_score/深分页等高成本特性  建议继续阅读：\n  监控  故障排查  相关性与评分调优  参考手册（API 与参数） #  遇到需要查具体字段与参数时，可以从这些 Reference 页面切入：\n  查询与过滤上下文  精确查询与全文检索的对比  最小匹配（minimum_should_match）  Term 查询家族索引页  全文查询索引页  ","subcategory":null,"summary":"","tags":null,"title":"查询调优与慢查询排查","url":"/easysearch/main/docs/best-practices/query-tuning/"},{"category":null,"content":"支持的单位清单 #  Easysearch 在 REST API 中支持以下几类单位，适用于查询、索引设置和集群配置。\n时间单位 #     缩写 含义 示例     d 天 5d   h 小时 12h   m 分钟 30m   s 秒 60s   ms 毫秒 500ms   micros 微秒 100micros   nanos 纳秒 1000nanos    常见使用场景：timeout、scroll、interval、ILM 策略中的 min_age 等。\n字节大小单位 #     缩写 含义 换算     b 字节 1 B   kb Kibibyte 1024 B   mb Mebibyte 1024 KB   gb Gibibyte 1024 MB   tb Tebibyte 1024 GB   pb Pebibyte 1024 TB     注意：虽然缩写看起来是十进制（kb），但实际按二进制计算（1 kb = 1024 bytes）。\n 距离单位 #     缩写 含义     mi 英里   yd 码   ft 英尺   in 英寸   km 千米   m 米   cm 厘米   mm 毫米   nmi/NM 海里    常见使用场景：geo_distance 查询和聚合、geo_bounding_box 过滤等。\n无单位数量 #     缩写 含义 换算     k 千 1,000   m 百万 1,000,000   g 十亿 1,000,000,000   t 万亿 10^12   p 千万亿 10^15    要将 API 响应中的数值自动转换为人类可读格式，请参阅 常用 REST 参数。\n","subcategory":null,"summary":"","tags":null,"title":"支持的单位清单","url":"/easysearch/main/docs/api-reference/units/"},{"category":null,"content":"存储脚本 #  存储脚本（Stored Scripts）允许您将常用的脚本预先保存在集群中，之后通过 ID 引用执行。使用存储脚本可以减少请求体大小、避免重复编译、并集中管理脚本逻辑。\n创建存储脚本 #  使用 POST _scripts/\u0026lt;script_id\u0026gt; 创建或更新存储脚本：\nPOST _scripts/calculate_discount { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;price\u0026#39;].value * (1 - params.discount_rate)\u0026#34; } } 指定脚本上下文 #  可以在创建时指定脚本的使用上下文，Easysearch 会在存储时进行上下文相关的语法校验：\nPOST _scripts/my_score_script/score { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;_score * doc[\u0026#39;boost\u0026#39;].value\u0026#34; } } 上下文名称放在 URL 路径的第二段（/score），可选值包括：score、filter、update、ingest 等。\n获取存储脚本 #  GET _scripts/calculate_discount 响应：\n{ \u0026#34;_id\u0026#34;: \u0026#34;calculate_discount\u0026#34;, \u0026#34;found\u0026#34;: true, \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;price\u0026#39;].value * (1 - params.discount_rate)\u0026#34; } } 使用存储脚本 #  在查询、更新、聚合等操作中通过 id 引用存储脚本：\n在搜索中使用 #  GET products/_search { \u0026#34;script_fields\u0026#34;: { \u0026#34;final_price\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;calculate_discount\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;discount_rate\u0026#34;: 0.15 } } } } } 在 Script Score 中使用 #  GET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;手机\u0026#34; } }, \u0026#34;script\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;my_score_script\u0026#34; } } } } 在 Update 中使用 #  POST _scripts/increment_counter { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;ctx._source[params.field] += params.amount\u0026#34; } } POST my_index/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;increment_counter\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;view_count\u0026#34;, \u0026#34;amount\u0026#34;: 1 } } } 在 Search Template 中使用 #  存储脚本也可以作为 Mustache 搜索模板使用：\nPOST _scripts/my_search_template { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;mustache\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;{{field}}\u0026#34;: \u0026#34;{{query}}\u0026#34; } }, \u0026#34;from\u0026#34;: \u0026#34;{{from}}\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;{{size}}\u0026#34; } } } GET my_index/_search/template { \u0026#34;id\u0026#34;: \u0026#34;my_search_template\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;title\u0026#34;, \u0026#34;query\u0026#34;: \u0026#34;Easysearch\u0026#34;, \u0026#34;from\u0026#34;: 0, \u0026#34;size\u0026#34;: 10 } } 删除存储脚本 #  DELETE _scripts/calculate_discount 查看可用脚本语言 #  GET _script_language 查看可用脚本上下文 #  GET _script_context API 参考 #     操作 API     创建/更新脚本 POST _scripts/\u0026lt;id\u0026gt; 或 PUT _scripts/\u0026lt;id\u0026gt;   创建并校验上下文 POST _scripts/\u0026lt;id\u0026gt;/\u0026lt;context\u0026gt;   获取脚本 GET _scripts/\u0026lt;id\u0026gt;   删除脚本 DELETE _scripts/\u0026lt;id\u0026gt;   查看脚本语言 GET _script_language   查看脚本上下文 GET _script_context    最佳实践 #   命名规范：使用有意义的名称，如 calculate_discount、normalize_score，便于管理 参数化设计：将可变部分通过 params 传入，使同一脚本适用于不同场景 版本管理：存储脚本没有内置版本控制，建议在外部（如 Git）管理脚本源码，通过 CI/CD 部署 上下文校验：创建时指定上下文（如 /score），可以提前发现语法错误 安全控制：通过 script.allowed_types 设置可以限制只允许使用 stored 类型的脚本，禁止内联脚本  ","subcategory":null,"summary":"","tags":null,"title":"存储脚本","url":"/easysearch/main/docs/features/scripting/stored-scripts/"},{"category":null,"content":"多租户与权限模型实践 #  在 SaaS 平台或企业多部门共享 Easysearch 集群的场景下，需要一套完善的多租户隔离与权限控制方案。Easysearch 提供了索引级、文档级和字段级三层安全能力来支撑这一需求。\n相关指南 #    用户与角色管理  接入企业认证体系  多租户隔离模式 #  模式一：按索引隔离 #  每个租户使用独立的索引（或索引前缀），通过角色控制访问范围。\ntenant_a_orders tenant_a_products tenant_b_orders tenant_b_products 角色定义示例：\nPUT _security/role/tenant_a_role { \u0026#34;cluster_permissions\u0026#34;: [\u0026#34;cluster_composite_ops_ro\u0026#34;], \u0026#34;index_permissions\u0026#34;: [ { \u0026#34;index_patterns\u0026#34;: [\u0026#34;tenant_a_*\u0026#34;], \u0026#34;allowed_actions\u0026#34;: [\u0026#34;crud\u0026#34;, \u0026#34;create_index\u0026#34;] } ] }    优点 缺点     隔离彻底，互不影响 索引数量随租户增长   性能可独立调优 跨租户查询需要额外聚合   易于理解和调试 集群管理复杂度较高    模式二：按字段标记隔离 #  所有租户共享索引，通过 tenant_id 字段区分，利用文档级安全（DLS）实现隔离。\nPUT _security/role/tenant_a_dls_role { \u0026#34;cluster_permissions\u0026#34;: [\u0026#34;cluster_composite_ops_ro\u0026#34;], \u0026#34;index_permissions\u0026#34;: [ { \u0026#34;index_patterns\u0026#34;: [\u0026#34;shared_orders\u0026#34;], \u0026#34;dls\u0026#34;: \u0026#34;{\\\u0026#34;term\\\u0026#34;: {\\\u0026#34;tenant_id\\\u0026#34;: \\\u0026#34;tenant_a\\\u0026#34;}}\u0026#34;, \u0026#34;allowed_actions\u0026#34;: [\u0026#34;read\u0026#34;] } ] }    优点 缺点     索引数量少，管理简单 DLS 对查询性能有一定影响   跨租户聚合方便 隔离不如独立索引彻底   适合租户数量多的场景 需要确保写入时带 tenant_id    模式对比 #     维度 按索引隔离 按字段隔离     隔离强度 强 中   管理复杂度 高 低   性能影响 无额外开销 DLS 过滤开销   适用租户数 \u0026lt;100 100+   推荐场景 合规要求高 SaaS 平台    文档级安全（DLS） #  DLS 允许在角色级别定义查询过滤条件，用户只能看到满足条件的文档。\n{ \u0026#34;index_permissions\u0026#34;: [ { \u0026#34;index_patterns\u0026#34;: [\u0026#34;logs-*\u0026#34;], \u0026#34;dls\u0026#34;: \u0026#34;{\\\u0026#34;bool\\\u0026#34;: {\\\u0026#34;must\\\u0026#34;: [{\\\u0026#34;term\\\u0026#34;: {\\\u0026#34;department\\\u0026#34;: \\\u0026#34;engineering\\\u0026#34;}}]}}\u0026#34;, \u0026#34;allowed_actions\u0026#34;: [\u0026#34;read\u0026#34;] } ] } 字段级安全（FLS） #  FLS 控制用户能看到文档中的哪些字段，适用于敏感数据保护。\n{ \u0026#34;index_permissions\u0026#34;: [ { \u0026#34;index_patterns\u0026#34;: [\u0026#34;employees\u0026#34;], \u0026#34;fls\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;department\u0026#34;, \u0026#34;title\u0026#34;], \u0026#34;allowed_actions\u0026#34;: [\u0026#34;read\u0026#34;] } ] } 上例中，用户只能看到 name、department、title 三个字段，salary、ssn 等敏感字段被隐藏。\n也可以使用排除模式：\n{ \u0026#34;fls\u0026#34;: [\u0026#34;~salary\u0026#34;, \u0026#34;~ssn\u0026#34;] } 权限设计最佳实践 #     实践 说明     角色分层 定义基础角色（只读、读写）+ 业务角色（订单管理、日志查看）   最小权限 每个角色只授予完成工作所需的最小索引和操作权限   索引模式通配符 使用 tenant_*_logs 等模式批量授权，避免逐索引配置   分离管理员与业务用户 集群管理操作（cluster_all）只授予运维角色   审计日志 开启审计日志，记录敏感操作便于事后追溯   定期审查 定期审查角色映射，移除不再需要的权限    ","subcategory":null,"summary":"","tags":null,"title":"多租户与权限模型实践","url":"/easysearch/main/docs/integrations/security/multi-tenant-access-control/"},{"category":null,"content":"向量工作流与 Hybrid 检索 #  本文完整介绍在 Easysearch 中使用向量检索的全流程——从索引设计、向量写入到混合检索策略。\n相关指南（先读这些） #    Embedding 服务接入  向量检索功能  Hybrid Search API  索引设计 #  创建向量索引 #   从 1.11.1 版本开始，index.knn 参数已弃用，创建 knn 索引时无需配置。\n PUT knowledge_base { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 2, \u0026#34;number_of_replicas\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34; }, \u0026#34;content_vector\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 768, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;L\u0026#34;: 99, \u0026#34;k\u0026#34;: 1 } }, \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } } } } 向量字段参数 #     参数 说明     dims 向量维度，必须与 Embedding 模型输出维度一致   model 索引模型：lsh（近似搜索）或 exact（精确搜索）   similarity 相似度度量：cosine、l1、l2   L LSH 模型参数：哈希表数量，增大可提高召回率   k LSH 模型参数：哈希函数数量，增大可提高精度    向量写入 #  方式一：Ingest Pipeline 自动向量化 #  配置 Ingest Pipeline 在写入时自动调用 Embedding 服务：\nPUT _ingest/pipeline/vectorize { \u0026#34;processors\u0026#34;: [ { \u0026#34;text_embedding\u0026#34;: { \u0026#34;model_id\u0026#34;: \u0026#34;my-embedding-model\u0026#34;, \u0026#34;field_map\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;content_vector\u0026#34; } } } ] } 写入时指定 pipeline：\nPOST knowledge_base/_doc?pipeline=vectorize { \u0026#34;title\u0026#34;: \u0026#34;Easysearch 集群配置\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;本文介绍 Easysearch 集群的配置方法和最佳实践...\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;tutorial\u0026#34; } 方式二：应用侧预计算 #  在应用中先调用 Embedding 服务获取向量，再连同向量一起写入：\nPOST knowledge_base/_doc { \u0026#34;title\u0026#34;: \u0026#34;Easysearch 集群配置\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;本文介绍 Easysearch 集群的配置方法和最佳实践...\u0026#34;, \u0026#34;content_vector\u0026#34;: [0.12, -0.34, 0.56, ...], \u0026#34;category\u0026#34;: \u0026#34;tutorial\u0026#34; } 批量回填历史数据 #  对于已有的大量文本数据，可以用批处理脚本分批向量化并写入：\nfrom elasticsearch import Elasticsearch, helpers es = Elasticsearch([\u0026quot;https://localhost:9200\u0026quot;], basic_auth=(\u0026quot;admin\u0026quot;, \u0026quot;pwd\u0026quot;), verify_certs=False)\n# 分批读取现有文档 docs = helpers.scan(es, index=\u0026quot;knowledge_base\u0026quot;, query={\u0026quot;query\u0026quot;: {\u0026quot;match_all\u0026quot;: {}}})\nactions = [] for doc in docs: vector = embedding_model.encode(doc[\u0026quot;_source\u0026quot;][\u0026quot;content\u0026quot;]) actions.append({ \u0026quot;_op_type\u0026quot;: \u0026quot;update\u0026quot;, \u0026quot;_index\u0026quot;: \u0026quot;knowledge_base\u0026quot;, \u0026quot;_id\u0026quot;: doc[\u0026quot;_id\u0026quot;], \u0026quot;doc\u0026quot;: {\u0026quot;content_vector\u0026quot;: vector.tolist()} }) if len(actions) \u0026gt;= 100: helpers.bulk(es, actions) actions = [] Hybrid 检索策略 #\n 混合检索将 BM25 全文搜索和向量 kNN 搜索的结果融合，兼顾精确匹配和语义理解：\n基础 Hybrid 查询 #  POST knowledge_base/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;content_vector\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.34, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 50 } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;集群健康检查\u0026#34;, \u0026#34;boost\u0026#34;: 0.3 } } } ] } } } 权重调优 #     BM25 权重 kNN 权重 效果     0.7 0.3 偏向精确匹配，适合关键词明确的查询   0.3 0.7 偏向语义理解，适合自然语言问句   0.5 0.5 平衡模式，通用场景推荐起点     调参建议：使用一组标注好的查询-文档对来评估不同权重组合的检索效果（Precision@K、NDCG 等），找到最适合业务的权重配比。\n ","subcategory":null,"summary":"","tags":null,"title":"向量工作流与 Hybrid 检索","url":"/easysearch/main/docs/integrations/ai/vector-workflow/"},{"category":null,"content":"向量字段建模 #  本页聚焦向量字段的设计策略——在已经理解字段类型和映射参数的基础上，如何为实际业务做出合理的字段规划。\n 关于两种向量字段类型（knn_dense_float_vector / knn_sparse_bool_vector）的映射参数、数据格式、各索引模型的详细说明，请参阅 向量字段类型参考。\n 文本字段 + 向量字段 #  一个典型的混合搜索文档同时包含文本字段和向量字段：\nPUT /hybrid-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;embedding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 768, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;L\u0026#34;: 99, \u0026#34;k\u0026#34;: 1 } } } } } 这样同一份文档既支持 BM25 全文搜索，也支持 kNN 向量搜索，为 Hybrid 检索打基础。\n多向量字段 #  为同一文档存储多个向量表示，例如标题和正文各一个 Embedding：\nPUT /multi-vec-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;title_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 768, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;L\u0026#34;: 99, \u0026#34;k\u0026#34;: 1 } }, \u0026#34;content_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 768, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;L\u0026#34;: 99, \u0026#34;k\u0026#34;: 1 } } } } }  注意：每个向量字段都消耗存储和索引资源。只为线上查询真正需要的维度建向量字段，避免无节制堆积。\n 维度选择 #   维度由上游 Embedding 模型决定（常见 128、256、384、768、1024 等） 维度越高，存储开销越大、索引构建和查询越慢 在效果满足的前提下，优先选择低维度模型（如 256～512） 大维度向量多的索引建议按业务拆分  模型选择建议 #     场景 推荐字段类型 推荐模型 推荐相似度     文本语义搜索 knn_dense_float_vector lsh cosine   图像特征检索 knn_dense_float_vector lsh cosine 或 l2   小数据集精确匹配 knn_dense_float_vector exact cosine   特征标签相似度 knn_sparse_bool_vector lsh 或 sparse_indexed jaccard   位串比较 knn_sparse_bool_vector lsh hamming    写入与更新策略 #  向量字段通常由外部模型计算得出，写入时要考虑：\n 同步写入：写文档的同时调用 Embedding 模型生成向量，保证检索一致性 异步补齐：先写文本文档，异步任务批量生成向量后 update，适合历史数据迁移  下一步 #    向量字段类型参考：字段类型、映射参数、各索引模型详解  向量搜索指南：完整的查询用法、Hybrid 检索、性能调优  k-NN 查询 API：查询参数完整参考  ","subcategory":null,"summary":"","tags":null,"title":"向量字段建模","url":"/easysearch/main/docs/features/vector-search/vector-fields/"},{"category":null,"content":"反范式与权衡 #  在关系型数据库中，规范化（范式）是黄金法则。但在 Easysearch 这样的搜索系统中，反范式（denormalization）往往是更好的选择。本页解释为什么，以及如何在一致性和性能之间做权衡。\n搜索系统中的规范化与冗余 #  与传统数据库\u0026quot;强规范化\u0026quot;不同，面向搜索的文档往往会有适度冗余：\n 预先把常用的派生信息存进文档（如标准化后的地区名、拼音、缩写） 把查询高频的外键信息\u0026quot;带过来\u0026quot;，减少查询时的 join 需求  但冗余也要有边界：\n 冗余会放大存储与更新成本 冗余字段过多，会让 mapping 变得臃肿、难以维护  经验做法：\n 只冗余\u0026quot;确实会被高频查询/排序/聚合\u0026quot;的字段 对变动频率极高的冗余信息，要慎重评估更新成本  为什么需要反范式？ #  使用 Easysearch 得到最好搜索性能的方法是有目的地在索引时进行反范式。对每个文档保持一定数量的冗余副本可以在需要访问时避免进行关联操作。\n示例：博客文章与用户 #  如果我们希望通过用户姓名找到他写的博客文章，可以在博客文档中包含这个用户的姓名：\nPUT /my_index/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john@smith.com\u0026#34;, \u0026#34;dob\u0026#34;: \u0026#34;1970/10/24\u0026#34; } PUT /my_index/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;Relationships\u0026quot;, \u0026quot;body\u0026quot;: \u0026quot;It's complicated\u0026hellip;\u0026quot;, \u0026quot;user\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;John Smith\u0026quot; } } 通过单次查询就能找到用户 John 的博客文章：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;relationships\u0026#34; }}, { \u0026#34;match\u0026#34;: { \u0026#34;user.name\u0026#34;: \u0026#34;John\u0026#34; }} ] } } } 数据反范式的优点是速度快——每个文档都包含了所需的所有信息，查询匹配时不需要进行昂贵的联接操作。\n反范式的代价 #  1. 数据冗余 #   用户信息被复制到每篇博客文章中 如果用户信息更新，需要更新所有相关的博客文章 存储空间会增加  2. 更新成本 #   更新用户信息时，需要重新索引所有相关的博客文章 如果用户信息变化频繁，更新成本会很高  3. 一致性风险 #   如果更新失败或部分失败，可能导致数据不一致 需要设计好更新策略和错误处理机制  何时使用反范式？ #  反范式适合以下场景：\n 读多写少：文档被频繁查询，但很少更新 关联数据变化不频繁：例如用户姓名、产品类别等相对稳定的信息 查询性能优先：需要快速响应，不能接受 JOIN 操作的开销  不适合反范式的场景：\n 关联数据变化频繁：例如库存数量、价格等经常变化的数据 一致性要求极高：不能容忍任何数据不一致的情况 存储成本敏感：数据量巨大，冗余成本过高  实践建议 #   识别稳定的关联数据：用户姓名、产品类别、地区信息等相对稳定的数据适合反范式 识别变化频繁的数据：库存、价格、评分等变化频繁的数据不适合反范式 设计更新策略：  对于变化不频繁的数据，可以接受批量更新 对于变化频繁的数据，考虑使用 Nested 或 Parent-Child 关系   权衡存储和性能：在存储成本和查询性能之间找到平衡点  与 Nested 和 Parent-Child 的对比 #     方案 适用场景 查询性能 更新成本     反范式 关联数据稳定、读多写少 最快 更新需重建所有相关文档   Nested 数组元素需要独立查询，但数量有限 较快 更新父文档时重建所有子文档   Parent-Child 父子生命周期不同步、子文档数量很多 较慢 可以独立更新父/子    选择依据：\n 如果关联数据很少变 → 反范式 如果需要精确匹配数组内的对象组合 → Nested 如果子元素很多、独立更新频繁 → Parent-Child  小结 #   反范式是搜索系统中常用的优化手段，可以显著提升查询性能 需要在一致性、更新成本和查询性能之间做权衡 适合读多写少、关联数据变化不频繁的场景 不适合变化频繁、一致性要求极高的场景  下一步可以继续阅读：\n  Nested 建模  Parent-Child 建模  时间序列建模  ","subcategory":null,"summary":"","tags":["最佳实践","数据建模"],"title":"反范式与权衡","url":"/easysearch/main/docs/best-practices/data-modeling/denormalization/"},{"category":null,"content":"加权与调参 #  当默认 _score 排序不符合业务预期时，就需要通过\u0026quot;加权\u0026quot;来让某些字段或条件更重要。本页给出常见手段和使用建议。\n查询时权重提升 #  查询时的权重提升是可以用来影响相关度的主要工具，任意类型的查询都能接受 boost 参数。将 boost 设置为 2，并不代表最终的评分 _score 是原值的两倍；实际的权重值会经过归一化和一些其他内部优化过程。尽管如此，它确实想要表明一个提升值为 2 的句子的重要性是提升值为 1 语句的两倍。\n在实际应用中，无法通过简单的公式得出某个特定查询语句的\u0026quot;正确\u0026quot;权重提升值，只能通过不断尝试获得。需要记住的是 boost 只是影响相关度评分的其中一个因子；它还需要与其他因子相互竞争。在前例中，title 字段相对 content 字段可能已经有一个\u0026quot;缺省的\u0026quot;权重提升值，这因为在字段长度归一值中，标题往往比相关内容要短，所以不要想当然的去盲目提升一些字段的权重。选择权重，检查结果，如此反复。\n字段级别 Boost：标题 \u0026gt; 标签 \u0026gt; 正文 #  最常见的需求：\n 标题匹配比正文匹配更重要 关键字段（品牌、类目）权重大于辅助字段  常见做法是在 multi_match 中给字段加权，例如：\n{ \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;搜索 引擎\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title^3\u0026#34;, \u0026#34;tags^2\u0026#34;, \u0026#34;content\u0026#34; ] } } } 或者在 bool 查询中：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;quick brown fox\u0026#34;, \u0026#34;boost\u0026#34;: 2 } } }, { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;quick brown fox\u0026#34; } } ] } } } title 查询语句的重要性是 content 查询的 2 倍，因为它的权重提升值为 2。没有设置 boost 的查询语句的值为 1。\n提升索引权重 #  当在多个索引中搜索时，可以使用参数 indices_boost 来提升整个索引的权重，在下面例子中，当要为最近索引的文档分配更高权重时，可以这么做：\nGET /docs_2014_*/_search { \u0026#34;indices_boost\u0026#34;: { \u0026#34;docs_2014_10\u0026#34;: 3, \u0026#34;docs_2014_09\u0026#34;: 2 }, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;quick brown fox\u0026#34; } } } 这个多索引查询涵盖了所有以字符串 docs_2014_ 开始的索引。其中，索引 docs_2014_10 中的所有文件的权重是 3，索引 docs_2014_09 中是 2，其他所有匹配的索引权重为默认值 1。\n使用查询结构修改相关度 #  Easysearch 的查询表达式相当灵活，可以通过调整查询结构中查询语句的所处层次，从而或多或少改变其重要性。\n例如，设想下面这个查询：\nquick OR brown OR red OR fox 可以将所有词都放在 bool 查询的同一层中：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;quick\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;brown\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;red\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;fox\u0026#34; }} ] } } } 这个查询可能最终给包含 quick、red 和 brown 的文档评分与包含 quick、red、fox 文档的评分相同，这里 red 和 brown 是同义词，可能只需要保留其中一个，而我们真正要表达的意思是想做以下查询：\nquick OR (brown OR red) OR fox 根据标准的布尔逻辑，这与原始的查询是完全一样的，但是我们已经在组合查询中看到，bool 查询不关心文档匹配的程度，只关心是否能匹配。\n上述查询有个更好的方式：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;quick\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;fox\u0026#34; }}, { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;brown\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;red\u0026#34; }} ] } } ] } } } 现在，red 和 brown 处于相互竞争的层次，quick、fox 以及 red OR brown 则是处于顶层且相互竞争的词。\nboosting 查询 #  boosting 查询恰恰能解决某些场景下的问题。它仍然允许我们将某些结果包括到结果中，但是使它们降级——即降低它们原来可能应有的排名：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;boosting\u0026#34;: { \u0026#34;positive\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;apple\u0026#34; } }, \u0026#34;negative\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;pie tart fruit crumble tree\u0026#34; } }, \u0026#34;negative_boost\u0026#34;: 0.5 } } } 它接受 positive 和 negative 查询。只有那些匹配 positive 查询的文档罗列出来，对于那些同时还匹配 negative 查询的文档将通过文档的原始 _score 与 negative_boost 相乘的方式降级后的结果。\n为了达到效果，negative_boost 的值必须小于 1.0。在这个示例中，所有包含负向词的文档评分 _score 都会减半。\nconstant_score 查询 #  有时候我们根本不关心 TF/IDF，只想知道一个词是否在某个字段中出现过。可能搜索一个度假屋并希望它能尽可能有以下设施：\n WiFi Garden（花园） Pool（游泳池）  可以用简单的 match 查询进行匹配：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;wifi garden pool\u0026#34; } } } 但这并不是真正的全文搜索，此种情况下，TF/IDF 并无用处。我们既不关心 wifi 是否为一个普通词，也不关心它在文档中出现是否频繁，关心的只是它是否曾出现过。实际上，我们希望根据房屋不同设施的数量对其排名——设施越多越好。如果设施出现，则记 1 分，不出现记 0 分。\n在 constant_score 查询中，它可以包含查询或过滤，为任意一个匹配的文档指定评分 1，忽略 TF/IDF 信息：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;wifi\u0026#34; }} }}, { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;garden\u0026#34; }} }}, { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;pool\u0026#34; }} }} ] } } } 或许不是所有的设施都同等重要——对某些用户来说有些设施更有价值。如果最重要的设施是游泳池，那我们可以为更重要的设施增加权重：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;wifi\u0026#34; }} }}, { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;garden\u0026#34; }} }}, { \u0026#34;constant_score\u0026#34;: { \u0026#34;boost\u0026#34;: 2, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;pool\u0026#34; }} }} ] } } } pool 语句的权重提升值为 2，而其他的语句为 1。\n 注意：最终的评分并不是所有匹配语句的简单求和，各匹配语句的 BM25 评分会按照 bool 查询的规则进行组合。\n function_score 查询 #  function_score 查询是用来控制评分过程的终极武器，它允许为每个与主查询匹配的文档应用一个函数，以达到改变甚至完全替换原始查询评分 _score 的目的。\n实际上，也能用过滤器对结果的子集应用不同的函数，这样一箭双雕：既能高效评分，又能利用过滤器缓存。\nEasysearch 预定义了一些函数：\n weight：为每个文档应用一个简单而不被规范化的权重提升值：当 weight 为 2 时，最终结果为 2 * _score。 field_value_factor：使用这个值来修改 _score，如将 popularity 或 votes（受欢迎或赞）作为考虑因素。 random_score：为每个用户都使用一个不同的随机评分对结果排序，但对某一具体用户来说，看到的顺序始终是一致的。 衰减函数——linear、exp、gauss：将浮动值结合到评分 _score 中，例如结合 publish_date 获得最近发布的文档，结合 geo_location 获得更接近某个具体经纬度（lat/lon）地点的文档，结合 price 获得更接近某个特定价格的文档。 script_score：如果需求超出以上范围时，用自定义脚本可以完全控制评分计算，实现所需逻辑。  如果没有 function_score 查询，就不能将全文查询与最新发生这种因子结合在一起评分，而不得不根据评分 _score 或时间 date 进行排序；这会相互影响抵消两种排序各自的效果。这个查询可以使两个效果融合：可以仍然根据全文相关度进行排序，但也会同时考虑最新发布文档、流行文档、或接近用户希望价格的产品。\n按受欢迎度提升权重 #  设想有个网站供用户发布博客并且可以让他们为自己喜欢的博客点赞，我们希望将更受欢迎的博客放在搜索结果列表中相对较上的位置，同时全文搜索的评分仍然作为相关度的主要排序依据，可以简单的通过存储每个博客的点赞数来实现它：\nPUT /blogposts/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;About popularity\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In this post we will talk about...\u0026#34;, \u0026#34;votes\u0026#34;: 6 } 在搜索时，可以将 function_score 查询与 field_value_factor 结合使用，即将点赞数与全文相关度评分结合：\nGET /blogposts/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ] } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;votes\u0026#34; } } } } function_score 查询将主查询和函数包括在内。主查询优先执行。field_value_factor 函数会被应用到每个与主 query 匹配的文档。每个文档的 votes 字段都必须有值供 function_score 计算。如果没有文档的 votes 字段有值，那么就必须使用 missing 属性提供的默认值来进行评分计算。\n在前面示例中，每个文档的最终评分 _score 都做了如下修改：\nnew_score = old_score * number_of_votes 然而这并不会带来出人意料的好结果，全文评分 _score 通常处于 0 到 10 之间，有 10 个赞的博客会掩盖掉全文评分，而 0 个赞的博客的评分会被置为 0。\nmodifier #  一种融入受欢迎度更好方式是用 modifier 平滑 votes 的值。换句话说，我们希望最开始的一些赞更重要，但是其重要性会随着数字的增加而降低。0 个赞与 1 个赞的区别应该比 10 个赞与 11 个赞的区别大很多。\n对于上述情况，典型的 modifier 应用是使用 log1p 参数值，公式如下：\nnew_score = old_score * log(1 + number_of_votes) log 对数函数使 votes 赞字段的评分曲线更平滑。\n带 modifier 参数的请求如下：\nGET /blogposts/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ] } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;votes\u0026#34;, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34; } } } } 修饰语 modifier 的值可以为：none（默认状态）、log、log1p、log2p、ln、ln1p、ln2p、square、sqrt 以及 reciprocal。\nfactor #  可以通过将 votes 字段与 factor 的积来调节受欢迎程度效果的高低：\nGET /blogposts/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ] } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;votes\u0026#34;, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34;, \u0026#34;factor\u0026#34;: 2 } } } } 添加了 factor 会使公式变成这样：\nnew_score = old_score * log(1 + factor * number_of_votes) factor 值大于 1 会提升效果，factor 值小于 1 会降低效果。\nboost_mode #  或许将全文评分与 field_value_factor 函数值乘积的效果仍然可能太大，我们可以通过参数 boost_mode 来控制函数与查询评分 _score 合并后的结果，参数接受的值为：\n multiply：评分 _score 与函数值的积（默认） sum：评分 _score 与函数值的和 min：评分 _score 与函数值间的较小值 max：评分 _score 与函数值间的较大值 replace：函数值替代评分 _score  与使用乘积的方式相比，使用评分 _score 与函数值求和的方式可以弱化最终效果，特别是使用一个较小 factor 因子时：\nGET /blogposts/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ] } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;votes\u0026#34;, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34;, \u0026#34;factor\u0026#34;: 0.1 }, \u0026#34;boost_mode\u0026#34;: \u0026#34;sum\u0026#34; } } } 之前请求的公式现在变成下面这样：\nnew_score = old_score + log(1 + 0.1 * number_of_votes) max_boost #  最后，可以使用 max_boost 参数限制一个函数的最大效果：\nGET /blogposts/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ] } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;votes\u0026#34;, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34;, \u0026#34;factor\u0026#34;: 0.1 }, \u0026#34;boost_mode\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;max_boost\u0026#34;: 1.5 } } } 无论 field_value_factor 函数的结果如何，最终结果都不会大于 1.5。\n 注意：max_boost 只对函数的结果进行限制，不会对最终评分 _score 产生直接影响。\n 调参策略建议 #  一个相对安全的调参流程：\n 先保证基础查询（不加权）的命中集合是合理的（能\u0026quot;搜到\u0026quot;需要的内容） 再引入字段/子句级 Boost 做轻量调整 需要更多业务信号时，再引入 function_score，并逐步放量  避免：\n 一次同时修改太多权重，导致难以判断是哪一项引起问题 把大量业务规则直接硬编码到查询里，使查询极难维护  小结 #   查询时的权重提升是可以用来影响相关度的主要工具 可以通过调整查询结构来改变查询语句的重要性 boosting 查询可以降级某些结果而不是完全排除 constant_score 查询忽略 TF/IDF，只关心是否匹配 function_score 查询是控制评分过程的终极武器，可以结合多种因素 优先使用字段/子句级 Boost，保持查询结构简单清晰  下一步可以继续阅读：\n  调试与 Explain  相关性常用策略  ","subcategory":null,"summary":"","tags":null,"title":"加权与调参","url":"/easysearch/main/docs/features/fulltext-search/relevance/boosting/"},{"category":null,"content":"分页与滚动 #  Easysearch 提供多种分页机制，适用于不同场景：\n   方式 适用场景 一致性 深度限制     from/size 小规模翻页（前几页） 无（实时数据） 默认 10,000   search_after + sort 深分页、增量翻页 无（实时数据） 无   search_after + PIT 深分页、一致性翻页 快照一致 无   Scroll 批量导出、离线处理 快照一致 无    相关指南 #    分页与排序   from/size 基础分页 #  最基础的分页依赖 from 和 size 两个参数：\n   参数 说明 默认值     from 起始偏移（0 基） 0   size 本页返回条数 10    GET shakespeare/_search { \u0026#34;from\u0026#34;: 0, \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34; } } } 计算页码对应的 from：from = size × (page_number - 1)\n也可以在 URL 参数中指定：\nGET shakespeare/_search?from=0\u0026amp;size=10 注意事项 #   深度限制：from + size 不能超过 index.max_result_window（默认 10,000），超过会报错 无状态：每次请求看到的是\u0026quot;此刻最新\u0026quot;数据，翻页期间有新写入时可能看到重复或跳过的结果 分布式开销：from=10000, size=10 实际上需要每个分片返回 10,010 条再合并，深页性能差   提示：通过 track_total_hits 参数可以控制是否精确统计总命中数。设置为 false 可提升大结果集的分页性能：\n GET shakespeare/_search { \u0026#34;track_total_hits\u0026#34;: false, \u0026#34;from\u0026#34;: 0, \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 也可以设置为具体数值（如 1000），只精确统计到该值，超过后显示为 \u0026quot;gte\u0026quot;。\n search_after 深分页 #  search_after 配合 sort 使用，通过上一页最后一条结果的排序值定位下一页，避免深分页性能问题。\n基础用法 #  第一次查询需要指定 sort：\nGET shakespeare/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;Henry IV\u0026#34; } } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;line_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } }, { \u0026#34;speech_number\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } 每条结果会带一个 sort 数组，取最后一条的 sort 值作为下一页的起始点：\nGET shakespeare/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;Henry IV\u0026#34; } } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;line_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } }, { \u0026#34;speech_number\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ], \u0026#34;search_after\u0026#34;: [3202, 8] }  注意：search_after 数组的值数量和顺序必须与 sort 完全一致。\n  Point in Time（PIT） #  PIT 提供了一个轻量级的数据视图快照，配合 search_after 可以实现一致性深分页——在翻页过程中数据视图保持稳定，不受新写入影响。\n创建 PIT #  POST /shakespeare/_pit?keep_alive=5m 返回一个 pit_id：\n{ \u0026#34;id\u0026#34;: \u0026#34;46ToAwMDaWR5BXV1aWQyK...\u0026#34; } 使用 PIT + search_after 翻页 #  注意：使用 PIT 时，请求体中不指定索引（索引信息已绑定在 PIT 中）：\nGET _search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34; } }, \u0026#34;pit\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;46ToAwMDaWR5BXV1aWQyK...\u0026#34;, \u0026#34;keep_alive\u0026#34;: \u0026#34;5m\u0026#34; }, \u0026#34;sort\u0026#34;: [ { \u0026#34;line_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34; } } ] } 后续翻页使用 search_after：\nGET _search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Hamlet\u0026#34; } }, \u0026#34;pit\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;46ToAwMDaWR5BXV1aWQyK...\u0026#34;, \u0026#34;keep_alive\u0026#34;: \u0026#34;5m\u0026#34; }, \u0026#34;sort\u0026#34;: [ { \u0026#34;line_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34; } } ], \u0026#34;search_after\u0026#34;: [32700] } 管理 PIT #  # 查看所有 PIT GET _pit/_all 删除特定 PIT DELETE _pit { \u0026quot;pit_id\u0026quot;: [\u0026quot;46ToAwMDaWR5BXV1aWQyK\u0026hellip;\u0026quot;] }\n删除所有 PIT DELETE _pit/_all \n注意：PIT 会占用资源（内存和段文件引用），用完后务必及时关闭。\n  Scroll 搜索 #  scroll 适合\u0026quot;批量拉取大量结果\u0026quot;的场景（导出、离线分析等）。它会维护一个搜索上下文快照，数据视图是静态的，新写入不会影响遍历结果。\n 不推荐用于在线用户翻页——Scroll 会长时间占用资源。在线翻页场景请使用 search_after 或 PIT。\n 发起 Scroll 查询 #  GET shakespeare/_search?scroll=10m { \u0026#34;size\u0026#34;: 10000 } 返回一个 _scroll_id。\n获取下一批 #  GET _search/scroll { \u0026#34;scroll\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;scroll_id\u0026#34;: \u0026#34;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAUWdmpUZDhn...\u0026#34; } 只要搜索上下文有效，就可以持续拉取。每次响应都会返回最新的 scroll_id，建议始终使用最新值。\n并行 Scroll（Sliced Scroll） #  对于超大结果集，可以用 sliced scroll 并行拉取，通过 slice.id 和 slice.max 切分：\n# 消费者 0 GET shakespeare/_search?scroll=10m { \u0026#34;slice\u0026#34;: { \u0026#34;id\u0026#34;: 0, \u0026#34;max\u0026#34;: 5 }, \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 消费者 1 GET shakespeare/_search?scroll=10m { \u0026quot;slice\u0026quot;: { \u0026quot;id\u0026quot;: 1, \u0026quot;max\u0026quot;: 5 }, \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} } } 关闭 Scroll #\n # 关闭特定 scroll DELETE _search/scroll/DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAcW... 关闭所有 scroll DELETE _search/scroll/_all \n务必在用完后关闭，否则直到超时前都会占用资源。\n ","subcategory":null,"summary":"","tags":null,"title":"分页与滚动","url":"/easysearch/main/docs/features/query-dsl/paginate-and-scroll/"},{"category":null,"content":"分布式基础 #  Easysearch 的主旨是随时可用和按需扩容。真正的扩容能力来自于水平扩容——为集群添加更多的节点，并且将负载压力和稳定性分散到这些节点中。Easysearch 天生就是分布式的，它知道如何通过管理多节点来提高扩容性和可用性。\n本页通过从 1 节点到 3 节点的演进，直观展示分片分配、故障转移和水平扩容的过程。\n空集群 #  启动一个单独的节点，里面不包含任何数据和索引——这就是一个空集群。\n此时这个节点既是唯一的数据节点，也是主节点。作为用户，我们可以将请求发送到集群中的任何节点（包括主节点），每个节点都知道任意文档所处的位置，并能将请求直接转发到正确的节点。\n集群健康 #  Easysearch 的集群监控信息中最重要的一项是集群健康，status 字段展示为 green、yellow 或 red：\nGET /_cluster/health    颜色 含义     green 所有的主分片和副本分片都正常运行   yellow 所有的主分片都正常运行，但不是所有的副本分片都正常运行   red 有主分片没能正常运行    添加索引 #  索引是指向一个或者多个物理分片的逻辑命名空间。一个分片是一个底层的工作单元，它本身就是一个完整的搜索引擎（一个 Lucene 实例）。\nEasysearch 利用分片将数据分发到集群内各处。当集群规模扩大或缩小时，Easysearch 会自动在各节点间迁移分片，使数据均匀分布。\n让我们创建一个名为 blogs 的索引，分配 3 个主分片和 1 份副本：\nPUT /blogs { \u0026#34;settings\u0026#34; : { \u0026#34;number_of_shards\u0026#34; : 3, \u0026#34;number_of_replicas\u0026#34; : 1 } } 此时只有一个节点，3 个主分片都分配在该节点上。集群健康状态为 yellow——主分片正常，但 3 个副本分片无处分配（在同一节点上保存原始数据和副本没有意义）。\n添加故障转移 #  启动第二个节点后，3 个副本分片将被分配到新节点上——每个主分片对应一个副本分片。这意味着任何一个节点出现问题时数据都完好无损。\n集群健康变为 green：所有 6 个分片都正常运行。\n所有新索引的文档都会先保存在主分片上，然后被并行复制到对应的副本分片。\n水平扩容 #  启动第三个节点后，分片会被重新分配。Node 1 和 Node 2 上各有一个分片被迁移到 Node 3，现在每个节点上只有 2 个分片（而不是之前的 3 个），每个分片能获得更多的硬件资源，性能得到提升。\n拥有 6 个分片（3 个主 + 3 个副本）的索引可以最大扩容到 6 个节点，每个节点只承载一个分片。\n更多的扩容 #  主分片的数目在索引创建时就已确定。但读操作可以同时被主分片和副本分片处理，所以副本越多、硬件越多，可并行处理的搜索请求也越多。\n在运行中的集群上可以动态调整副本分片数目：\nPUT /blogs/_settings { \u0026#34;number_of_replicas\u0026#34; : 2 } 此时 blogs 索引将拥有 9 个分片：3 个主分片和 6 个副本分片。\n ⚠️ 只在相同节点数目的集群上增加副本并不能提高性能（每个分片分到的资源更少了）。需要增加硬件资源来提升吞吐量。但更多的副本确实提高了数据冗余量。\n 应对故障 #  当某个节点出现故障时：\n 主节点立即将该节点上的主分片对应的副本提升为主分片（瞬间完成） 集群状态变为 yellow（主分片齐全，但副本不足） 如果有其他节点可用，新的副本会被分配出去  如果只剩一个节点且有主分片丢失，集群状态变为 red。\n分布式特性 #  Easysearch 在分布式方面几乎是透明的。以下操作都在后台自动完成：\n 分配文档到不同的分片中，文档可以储存在一个或多个节点中 按集群节点来均衡分配分片，对索引和搜索过程进行负载均衡 复制每个分片以支持数据冗余，防止硬件故障导致的数据丢失 将集群中任一节点的请求路由到存有相关数据的节点 集群扩容时无缝整合新节点，重新分配分片以便从离群节点恢复  小结 #     概念 要点     主分片 负责写入，数量在索引创建时确定   副本分片 提供冗余和读扩展，数量可动态调整   集群健康 green/yellow/red 三色表示集群状态   水平扩容 增加节点 → 分片自动重新分配   故障转移 副本自动提升为主分片，瞬间完成    下一步可以继续阅读：\n  分布式写入过程：文档路由、主副本交互、mget/bulk 执行路径  分布式搜索执行过程：query/fetch 两阶段  写入与存储机制：refresh、flush、merge 详解  ","subcategory":null,"summary":"","tags":null,"title":"分布式基础","url":"/easysearch/main/docs/fundamentals/distributed/"},{"category":null,"content":"全文搜索 #  Easysearch SQL 提供了一组相关性函数，可以在 WHERE 子句中使用，将 SQL 查询与 Easysearch 强大的全文搜索能力相结合。这些函数在底层会翻译为对应的 Easysearch DSL 查询。\n函数一览 #     函数 等价函数 底层 DSL 用途     MATCH_QUERY MATCHQUERY match 单字段全文匹配   MULTI_MATCH MULTIMATCH、MULTIMATCHQUERY multi_match 多字段全文匹配   MATCH_PHRASE MATCHPHRASE、MATCHPHRASEQUERY match_phrase 短语匹配   QUERY — query_string Lucene 查询字符串   SCORE SCOREQUERY、SCORE_QUERY constant_score 相关性评分包装     MATCH_QUERY #  在单个文本字段上执行全文匹配查询。\n语法 #  MATCH_QUERY(field, text) -- 或 field = MATCH_QUERY(text)    参数 说明     field 要搜索的字段名（必须是 text 类型）   text 搜索文本    示例 #  POST /_sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT account_number, address FROM accounts WHERE MATCH_QUERY(address, \u0026#39;Holmes\u0026#39;)\u0026#34; } 等价 DSL：\n{\u0026#34;query\u0026#34;: {\u0026#34;match\u0026#34;: {\u0026#34;address\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;Holmes\u0026#34;}}}} 结果：\n   account_number address     1 880 Holmes Lane    也可以使用等号写法：\nSELECT account_number, address FROM accounts WHERE address = MATCH_QUERY(\u0026#39;Holmes\u0026#39;)  MULTI_MATCH #  在多个字段上同时执行全文匹配查询，支持通配符字段模式。\n语法 #  MULTI_MATCH(\u0026#39;query\u0026#39;=\u0026#39;text\u0026#39;, \u0026#39;fields\u0026#39;=\u0026#39;field_pattern\u0026#39;)    参数 说明 示例     query 搜索文本 'Dale'   fields 字段名或通配符模式 '*name'、'firstname,lastname'    示例 #  搜索 firstname 或 lastname 中包含 \u0026ldquo;Dale\u0026rdquo; 的文档：\nPOST /_sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT firstname, lastname FROM accounts WHERE MULTI_MATCH(\u0026#39;query\u0026#39;=\u0026#39;Dale\u0026#39;, \u0026#39;fields\u0026#39;=\u0026#39;*name\u0026#39;)\u0026#34; } 等价 DSL：\n{\u0026#34;query\u0026#34;: {\u0026#34;multi_match\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;Dale\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;*name^1.0\u0026#34;], \u0026#34;type\u0026#34;: \u0026#34;best_fields\u0026#34;}}} 结果：\n   firstname lastname     Dale Adams     MATCH_PHRASE #  匹配精确的短语序列，词项必须按顺序连续出现。\n语法 #  MATCH_PHRASE(field, phrase)    参数 说明     field 要搜索的字段名   phrase 精确短语文本    示例 #  POST /_sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT account_number, address FROM accounts WHERE MATCH_PHRASE(address, \u0026#39;880 Holmes Lane\u0026#39;)\u0026#34; } 等价 DSL：\n{\u0026#34;query\u0026#34;: {\u0026#34;match_phrase\u0026#34;: {\u0026#34;address\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;880 Holmes Lane\u0026#34;}}}} 结果：\n   account_number address     1 880 Holmes Lane     QUERY #  使用 Lucene 查询字符串语法执行搜索，支持布尔运算、通配符、正则表达式和邻近搜索等高级查询语法。\n语法 #  QUERY(\u0026#39;query_string\u0026#39;) 查询字符串遵循 Lucene 查询语法，常用写法包括：\n   写法 说明     field:value 指定字段搜索   AND / OR / NOT 布尔运算   field:val* 通配符   \u0026quot;exact phrase\u0026quot; 短语搜索   field:[min TO max] 范围查询     如果查询字符串包含语法错误，将抛出解析异常。\n 示例 #  POST /_sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT account_number, address FROM accounts WHERE QUERY(\u0026#39;address:Lane OR address:Street\u0026#39;)\u0026#34; } 等价 DSL：\n{\u0026#34;query\u0026#34;: {\u0026#34;query_string\u0026#34;: {\u0026#34;query\u0026#34;: \u0026#34;address:Lane OR address:Street\u0026#34;}}} 结果：\n   account_number address     1 880 Holmes Lane   6 671 Bristol Street   13 789 Madison Street     SCORE #  将全文查询包装为评分查询，使每条匹配文档带有相关性得分。使用 _score 隐式变量可以在 SELECT 或 ORDER BY 中访问该分数。\n语法 #  SCORE(match_expression [, boost])    参数 说明 默认值     match_expression 全文匹配表达式（如 MATCH_QUERY(...) 等） —   boost 可选的浮点权重因子 1.0    示例 #  使用不同的 boost 权重区分不同匹配条件的优先级：\nPOST /_sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT account_number, address, _score FROM accounts WHERE SCORE(MATCH_QUERY(address, \u0026#39;Lane\u0026#39;), 0.5) OR SCORE(MATCH_QUERY(address, \u0026#39;Street\u0026#39;), 100) ORDER BY _score\u0026#34; } 结果：\n   account_number address _score     1 880 Holmes Lane 0.5   6 671 Bristol Street 100.0   13 789 Madison Street 100.0     _score 字段只有在使用 SCORE 函数后才会有非零值。普通的 MATCH_QUERY 在 WHERE 中作为过滤条件使用，不产生评分。\n  组合使用 #  全文搜索函数可以与普通的 SQL 条件组合使用：\n-- 全文搜索 + 精确过滤 SELECT firstname, lastname, address, _score FROM accounts WHERE SCORE(MATCH_QUERY(address, \u0026#39;Street\u0026#39;), 10) AND age \u0026gt; 30 ORDER BY _score DESC \u0026ndash; 全文搜索 + 聚合 SELECT city, COUNT(*) AS cnt FROM accounts WHERE MATCH_QUERY(address, 'Street') GROUP BY city ORDER BY cnt DESC 相关链接 #\n   SQL 查询总览  全文搜索功能  ","subcategory":null,"summary":"","tags":null,"title":"全文搜索","url":"/easysearch/main/docs/features/sql/fulltext/"},{"category":null,"content":"使用 INFINI Console 管理 #  如果你需要多集群统一管理、告警通知、数据探索等企业级功能，推荐使用 INFINI Console。它是 INFINI Labs 提供的独立可视化管理平台，支持同时管理 Easysearch 和 Elasticsearch 集群。\n 💡 如果你只需要管理单个集群并进行日常开发调试，Easysearch 还提供了 内置 Web UI，零部署，开箱即用。\n 安装 Console #  方式一：一键启动（推荐） #  同时启动 Console + Easysearch，最快体验完整管理功能：\ncurl -fsSL http://get.infini.cloud/start-local | sh -s 支持更多参数：\n# 启动 3 个 Easysearch 节点，自定义密码，开启 Agent 指标采集 curl -fsSL http://get.infini.cloud/start-local | sh -s -- up \\  --nodes 3 --password \u0026#34;MyDevPass123.\u0026#34; --metrics-agent 方式二：单独安装 #  # Linux 一键安装 curl -sSL http://get.infini.cloud | bash -s -- -p console cd /opt/console \u0026amp;\u0026amp; ./console 方式三：Docker #  docker run -d --name console \\  -p 9000:9000 \\  infinilabs/console:latest 启动后，打开浏览器访问 http://localhost:9000。\n连接 Easysearch 集群 #   登录 Console 后，进入 系统管理 → 集群管理 点击 新建集群 填写信息：  集群名称：自定义，如 my-easysearch 地址：https://localhost:9200 认证：输入用户名和密码   点击 测试连接，确认连通后保存  Console 支持跨引擎、跨版本、跨集群统一管理，你可以将多个 Easysearch 和 Elasticsearch 集群注册到同一个 Console 中集中管理。\n仪表盘 #  登录后首页展示全局仪表盘：\n 告警/通知/待办：未处理的告警事件一目了然 集群/节点/主机概览：所有已注册集群的整体状态 存储概览：各集群的磁盘和 JVM 使用情况 快捷入口：集群注册、数据探索、告警管理、安全管理  集群监控 #  在集群管理页面可以查看：\n 集群列表：健康状态、节点数、索引数、文档数、分片数 性能指标：索引吞吐、搜索吞吐、索引延迟、查询延迟 筛选分组：按健康状态、引擎版本、区域、标签筛选  点击具体集群可查看详细监控：\n 节点级别：每个节点的 CPU / 内存 / JVM / 磁盘指标 索引级别：每个索引的读写速率、文档增长趋势 集群动态：实时事件流，如分片迁移、节点加入/离开  数据探索 #  Console 提供了类似 Kibana Discover 的数据探索功能：\n 进入 数据探索 页面 选择目标集群和索引 浏览文档列表，查看字段详情 支持基于字段值的快速过滤和搜索 支持时序数据的时间范围选择  开发工具（DevTools） #  Console 的开发工具在内置 UI 基础上进一步增强：\n 多集群 Tab：同时打开多个集群的查询窗口，快速切换 常用命令收藏：保存常用的查询语句，团队共享 SQL 查询：直接编写 SQL，自动转换为 DSL 执行 语法高亮 + 自动补全：API 路径和 JSON 字段名提示  GET /_cluster/health GET /megacorp/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;last_name\u0026#34;: \u0026#34;Smith\u0026#34; } } } 点击 ▶ 按钮或按 Ctrl+Enter 执行。\n告警中心 #  Console 提供完善的告警能力：\n 预设规则：集群健康状态变化、磁盘使用率、JVM 内存等常用告警规则开箱即用 多级别告警：Info / Warning / Critical 分级，避免告警疲劳 多平台通知：支持飞书、钉钉、企业微信、邮件、Slack、Discord、Webhook 等  安全管理 #   连接凭证管理：集中管理各集群的访问凭证 用户授权：Console 后台用户权限控制 审计日志：记录所有操作行为，满足合规要求  平台监控 #  在平台概览页面可以查看所有已注册集群的汇总信息：\n 引擎分布（Easysearch / Elasticsearch 占比） JDK 版本分布 磁盘使用 Top 10 / JVM 使用 Top 10  功能概览 #     功能模块 说明     仪表盘 告警通知、集群/节点/主机概览、快捷入口   集群管理 多集群列表、健康状态、索引/搜索速率、筛选分组   节点监控 CPU/内存/JVM/磁盘指标、节点级别性能监控   索引监控 索引吞吐、查询延迟、索引级别指标   数据探索 类似 Kibana Discover，浏览文档、字段过滤、时序数据   告警中心 预设规则、多级别告警、多平台通知   开发工具 多集群 Tab、常用命令收藏、SQL 查询   安全管理 凭证管理、用户授权、审计日志    内置 UI vs Console 如何选择 #     场景 推荐方案     单集群、个人开发/学习  内置 Easysearch UI   多集群统一管理 INFINI Console   需要告警通知 INFINI Console   资源受限的边缘节点  内置 Easysearch UI   企业级运维、审计合规 INFINI Console    下一步 #    使用 Easysearch 内置 UI：零部署的轻量管理界面  使用 Curl 访问 Easysearch：命令行方式  Java 客户端  Python 客户端  ","subcategory":null,"summary":"","tags":null,"title":"使用 INFINI Console 管理","url":"/easysearch/main/docs/quick-start/connect/console/"},{"category":null,"content":"使用 Curl 访问 Easysearch #  Easysearch 提供 RESTful API，任何能发送 HTTP 请求的工具都可以与它交互。curl 是最常用的命令行工具，非常适合快速验证和学习。\n请求格式 #  一个典型的 Easysearch 请求由以下部分组成：\ncurl -X\u0026lt;VERB\u0026gt; \u0026#39;\u0026lt;PROTOCOL\u0026gt;://\u0026lt;HOST\u0026gt;:\u0026lt;PORT\u0026gt;/\u0026lt;PATH\u0026gt;?\u0026lt;QUERY_STRING\u0026gt;\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  -d \u0026#39;\u0026lt;BODY\u0026gt;\u0026#39;    部件 说明     VERB HTTP 方法：GET、POST、PUT、DELETE、HEAD   PROTOCOL http 或 https（Easysearch 默认启用 HTTPS）   HOST 节点主机名，本地为 localhost   PORT HTTP 服务端口，默认 9200   PATH API 路径，例如 _search、_count、my-index/_doc/1   QUERY_STRING 可选参数，例如 ?pretty 格式化输出   BODY JSON 格式的请求体（如果需要）     💡 Easysearch 默认启用 HTTPS 和基础认证，所以请求时通常需要加上 -k（跳过证书验证）和 -u user:password 参数。\n 验证集群连通性 #  curl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/?pretty\u0026#39; 返回示例：\n{ \u0026#34;name\u0026#34; : \u0026#34;node-1\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;my-cluster\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;xxx\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;distribution\u0026#34; : \u0026#34;easysearch\u0026#34;, \u0026#34;number\u0026#34; : \u0026#34;1.x.x\u0026#34;, ... }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; } 查看集群健康状态：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/_cluster/health?pretty\u0026#39; 统计集群中的文档数量：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/_count?pretty\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } }\u0026#39; 索引文档 #  Easysearch 中，存储数据的行为叫做 索引（indexing）。向索引 megacorp 写入一个文档：\ncurl -k -u admin:admin -XPUT \u0026#39;https://localhost:9200/megacorp/_doc/1?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;first_name\u0026#34; : \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34; : \u0026#34;Smith\u0026#34;, \u0026#34;age\u0026#34; : 25, \u0026#34;about\u0026#34; : \u0026#34;I love to go rock climbing\u0026#34;, \u0026#34;interests\u0026#34; : [ \u0026#34;sports\u0026#34;, \u0026#34;music\u0026#34; ] }\u0026#39; 路径 /megacorp/_doc/1 表示：索引 megacorp，文档 ID 为 1。\n继续写入更多数据：\ncurl -k -u admin:admin -XPUT \u0026#39;https://localhost:9200/megacorp/_doc/2?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;first_name\u0026#34; : \u0026#34;Jane\u0026#34;, \u0026#34;last_name\u0026#34; : \u0026#34;Smith\u0026#34;, \u0026#34;age\u0026#34; : 32, \u0026#34;about\u0026#34; : \u0026#34;I like to collect rock albums\u0026#34;, \u0026#34;interests\u0026#34; : [ \u0026#34;music\u0026#34; ] }\u0026#39; curl -k -u admin:admin -XPUT 'https://localhost:9200/megacorp/_doc/3?pretty'  -H 'Content-Type: application/json' -d ' { \u0026quot;first_name\u0026quot; : \u0026quot;Douglas\u0026quot;, \u0026quot;last_name\u0026quot; : \u0026quot;Fir\u0026quot;, \u0026quot;age\u0026quot; : 35, \u0026quot;about\u0026quot; : \u0026quot;I like to build cabinets\u0026quot;, \u0026quot;interests\u0026quot; : [ \u0026quot;forestry\u0026quot; ] }' \n无需预先创建索引或定义字段类型，Easysearch 会自动完成。当然，生产环境建议使用显式 Mapping。\n 检索文档 #  通过 GET 请求 + 文档 ID 检索单个文档：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_doc/1?pretty\u0026#39; 返回结果包含文档元数据和 _source 字段（原始 JSON）：\n{ \u0026#34;_index\u0026#34; : \u0026#34;megacorp\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34; : 1, \u0026#34;found\u0026#34; : true, \u0026#34;_source\u0026#34; : { \u0026#34;first_name\u0026#34; : \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34; : \u0026#34;Smith\u0026#34;, \u0026#34;age\u0026#34; : 25, \u0026#34;about\u0026#34; : \u0026#34;I love to go rock climbing\u0026#34;, \u0026#34;interests\u0026#34; : [ \u0026#34;sports\u0026#34;, \u0026#34;music\u0026#34; ] } }  💡 HTTP 方法速记：PUT 创建/覆盖文档，GET 检索文档，DELETE 删除文档，HEAD 检查文档是否存在。\n 搜索 #  搜索所有文档 #  curl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_search?pretty\u0026#39; 查询字符串搜索 #  搜索姓氏为 Smith 的员工：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_search?q=last_name:Smith\u0026amp;pretty\u0026#39; 使用 Query DSL #  使用 JSON 请求体构造更复杂的查询——这是 Easysearch 推荐的搜索方式：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_search?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;last_name\u0026#34; : \u0026#34;Smith\u0026#34; } } }\u0026#39; 组合查询：全文 + 过滤 #  搜索姓 Smith 且年龄大于 30 的员工：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_search?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;bool\u0026#34; : { \u0026#34;must\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;last_name\u0026#34; : \u0026#34;Smith\u0026#34; } }, \u0026#34;filter\u0026#34; : { \u0026#34;range\u0026#34; : { \u0026#34;age\u0026#34; : { \u0026#34;gt\u0026#34; : 30 } } } } } }\u0026#39; 全文搜索 #  搜索 about 字段中包含 \u0026ldquo;rock climbing\u0026rdquo; 的文档——Easysearch 会按相关性得分排序：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_search?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;about\u0026#34; : \u0026#34;rock climbing\u0026#34; } } }\u0026#39; 短语搜索 #  如果要精确匹配整个短语 \u0026ldquo;rock climbing\u0026rdquo;，使用 match_phrase：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_search?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;match_phrase\u0026#34; : { \u0026#34;about\u0026#34; : \u0026#34;rock climbing\u0026#34; } } }\u0026#39; 高亮搜索 #  在结果中高亮匹配的文本片段：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_search?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;match_phrase\u0026#34; : { \u0026#34;about\u0026#34; : \u0026#34;rock climbing\u0026#34; } }, \u0026#34;highlight\u0026#34; : { \u0026#34;fields\u0026#34; : { \u0026#34;about\u0026#34; : {} } } }\u0026#39; 返回结果中会多出 highlight 字段，匹配文本被 \u0026lt;em\u0026gt; 标签包裹。\n聚合分析 #  聚合（Aggregations）类似 SQL 的 GROUP BY，可以在搜索的同时做统计分析。\n统计员工最受欢迎的兴趣爱好：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_search?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;size\u0026#34; : 0, \u0026#34;aggs\u0026#34; : { \u0026#34;all_interests\u0026#34; : { \u0026#34;terms\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;interests.keyword\u0026#34; } } } }\u0026#39; 在查询结果基础上聚合——只统计姓 Smith 的员工：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_search?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;last_name\u0026#34; : \u0026#34;Smith\u0026#34; } }, \u0026#34;aggs\u0026#34; : { \u0026#34;all_interests\u0026#34; : { \u0026#34;terms\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;interests.keyword\u0026#34; } } } }\u0026#39; 嵌套聚合——按兴趣分组并计算每组的平均年龄：\ncurl -k -u admin:admin -XGET \u0026#39;https://localhost:9200/megacorp/_search?pretty\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39; { \u0026#34;size\u0026#34; : 0, \u0026#34;aggs\u0026#34; : { \u0026#34;all_interests\u0026#34; : { \u0026#34;terms\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;interests.keyword\u0026#34; }, \u0026#34;aggs\u0026#34; : { \u0026#34;avg_age\u0026#34; : { \u0026#34;avg\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;age\u0026#34; } } } } } }\u0026#39; 删除与清理 #  删除单个文档：\ncurl -k -u admin:admin -XDELETE \u0026#39;https://localhost:9200/megacorp/_doc/1?pretty\u0026#39; 删除整个索引：\ncurl -k -u admin:admin -XDELETE \u0026#39;https://localhost:9200/megacorp?pretty\u0026#39; 常用技巧 #     技巧 命令     格式化输出 加 ?pretty 参数   查看 HTTP 头 curl -i ...   跳过 TLS 证书验证 curl -k ...   指定认证 curl -u admin:admin ...   查看所有索引 curl -k -u admin:admin 'https://localhost:9200/_cat/indices?v'   查看节点信息 curl -k -u admin:admin 'https://localhost:9200/_cat/nodes?v'   查看分片状态 curl -k -u admin:admin 'https://localhost:9200/_cat/shards?v'    下一步 #    Java 客户端  Python 客户端  入门教程：更完整的上手流程  ","subcategory":null,"summary":"","tags":null,"title":"使用 Curl 访问 Easysearch","url":"/easysearch/main/docs/quick-start/connect/curl/"},{"category":null,"content":"Chinese 分析器 #  chinese 分析器为中文文本设计，提供基础的中文处理能力。对于更复杂的中文分词需求，建议使用专门的中文分词插件如 IK、HanLP 或 Jieba。\n分析器组成 #  该分析器由以下分词器和分词过滤器组成：\n cjk 分词器：将 CJK（中日韩）字符分解为二元组（bigrams） lowercase 分词过滤器：转换为小写  示例 #  POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;chinese\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;快速的棕色狐狸跳过懒惰的狗\u0026#34; } 分析结果 #  [ \u0026#34;快\u0026#34;, \u0026#34;速\u0026#34;, \u0026#34;的\u0026#34;, \u0026#34;棕\u0026#34;, \u0026#34;色\u0026#34;, \u0026#34;狐\u0026#34;, \u0026#34;狸\u0026#34;, \u0026#34;跳\u0026#34;, \u0026#34;过\u0026#34;, \u0026#34;懒\u0026#34;, \u0026#34;惰\u0026#34;, \u0026#34;的\u0026#34;, \u0026#34;狗\u0026#34; ] 推荐用法 #  对于生产环境的中文处理，建议使用专门的中文分词插件：\n IK 分词器 - 中文分词的常用选择 HanLP 分词器 - 功能完整的中文 NLP 分词 Jieba 分词器 - 基于 Python Jieba 的分词  详见相关插件文档。\n相关指南 #    文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"中文分析器（Chinese）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/chinese-analyzer/"},{"category":null,"content":"Update by Query #  Update by Query 对所有匹配查询条件的文档执行更新操作。 常用于批量修改字段值、数据迁移补丁、通过脚本做批量计算等场景。\n内部流程：scroll 遍历匹配文档 → 逐批执行 bulk update → 返回统计结果。\n 请求格式 #  POST /\u0026lt;index\u0026gt;/_update_by_query 路径参数 #     参数 必需 说明     \u0026lt;index\u0026gt; 是 目标索引，支持逗号分隔多索引和通配符    查询参数 #     参数 类型 默认值 说明     refresh boolean false 操作完成后是否刷新受影响的分片   timeout time 1m 超时时间   wait_for_completion boolean true 为 true 时同步等待完成后返回结果；为 false 时立即返回 task ID，可通过 Task API 查询进度   wait_for_active_shards string — 操作前需要的活跃分片数量   requests_per_second float 无限制 每秒请求数限制（流量控制）。-1 = 不限制   slices int/string 1 并行切片数。\u0026quot;auto\u0026quot; = 自动按分片数拆分   scroll time — scroll 上下文存活时间（如 5m）   scroll_size int 1000 每批 scroll 获取的文档数   conflicts string abort 版本冲突处理方式：abort = 遇到冲突即中止；proceed = 跳过冲突继续执行   max_docs int 全部 最多处理的文档数量   pipeline string — 对匹配文档执行的 Ingest Pipeline   search_timeout time — 搜索阶段超时   q string — 简单查询字符串（替代请求体中的 query）   df string — q 参数的默认字段   default_operator string OR q 参数的默认运算符   analyzer string — q 参数使用的分析器   analyze_wildcard boolean false 是否分析通配符   lenient boolean — 宽松解析模式   routing string — 路由值   preference string — 查询偏好   terminate_after int 0 每分片最多处理的文档数（0 = 不限制）   expand_wildcards string open 通配符展开：open、closed、hidden、all、none   ignore_unavailable boolean false 忽略不存在的索引   allow_no_indices boolean true 允许通配符不匹配任何索引    请求体 #  { \u0026#34;query\u0026#34;: { ... }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;params\u0026#34;: { ... } }, \u0026#34;max_docs\u0026#34;: 1000, \u0026#34;conflicts\u0026#34;: \u0026#34;proceed\u0026#34; }    字段 类型 说明     query object 筛选条件，语法同 Query DSL。省略时匹配所有文档   script object 更新脚本。通过 ctx._source 访问和修改文档字段   max_docs int 最多处理的文档数   conflicts string 冲突处理策略     不提供 script 时，Update by Query 仅对匹配文档触发重建索引（re-index in place），常用于 mapping 变更后重新分析已有文档。\n  示例 #  基本用法：给所有匹配文档添加字段 #  POST /website/_update_by_query { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;draft\u0026#34; } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.status = \u0026#39;published\u0026#39;\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34; } } 不带脚本：重建索引 #  mapping 变更后，对所有文档触发重新索引：\nPOST /website/_update_by_query { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 流量控制 #  限制每秒处理 500 条，避免影响线上查询：\nPOST /website/_update_by_query?requests_per_second=500\u0026amp;conflicts=proceed { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.migrated = true\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34; } } 异步执行 #  大批量更新时，使用异步模式：\nPOST /website/_update_by_query?wait_for_completion=false { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.version = 2\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34; } } 响应返回 task ID：\n{ \u0026#34;task\u0026#34;: \u0026#34;node_id:task_id\u0026#34; } 通过 Task API 查询进度：\nGET /_tasks/node_id:task_id 并行切片 #  使用 slices=auto 按分片数自动并行处理：\nPOST /website/_update_by_query?slices=auto { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.processed = true\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34; } } 通过 Pipeline 更新 #  利用已有 Ingest Pipeline 处理匹配文档：\nPOST /website/_update_by_query?pipeline=my_pipeline { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } }  响应字段 #  { \u0026#34;took\u0026#34;: 147, \u0026#34;timed_out\u0026#34;: false, \u0026#34;total\u0026#34;: 5, \u0026#34;updated\u0026#34;: 5, \u0026#34;deleted\u0026#34;: 0, \u0026#34;batches\u0026#34;: 1, \u0026#34;version_conflicts\u0026#34;: 0, \u0026#34;noops\u0026#34;: 0, \u0026#34;retries\u0026#34;: { \u0026#34;bulk\u0026#34;: 0, \u0026#34;search\u0026#34;: 0 }, \u0026#34;failures\u0026#34;: [] }    字段 说明     took 操作耗时（毫秒）   total 匹配的文档总数   updated 成功更新的文档数   deleted 被脚本删除的文档数（脚本设置 ctx.op = \u0026quot;delete\u0026quot; 时）   version_conflicts 版本冲突次数   noops 无操作次数（脚本设置 ctx.op = \u0026quot;noop\u0026quot; 时）   batches scroll 批次数   retries.bulk bulk 重试次数   retries.search search 重试次数   failures 失败详情数组     脚本中的操作控制 #  在脚本中可以通过设置 ctx.op 控制每条文档的处理方式：\n   ctx.op 值 行为     \u0026quot;index\u0026quot; 默认值，更新文档   \u0026quot;noop\u0026quot; 跳过该文档，不做任何写入   \u0026quot;delete\u0026quot; 删除该文档    POST /website/_update_by_query { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;if (ctx._source.views \u0026lt; 10) { ctx.op = \u0026#39;noop\u0026#39; } else { ctx._source.popular = true }\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34; } }  参考导航 #     需求 参见     单条文档更新  Update API   按查询删除文档  Delete by Query   跨索引迁移数据  Reindex   乐观并发控制  并发控制与版本    ","subcategory":null,"summary":"","tags":null,"title":"Update by Query","url":"/easysearch/main/docs/features/document-operations/update-by-query/"},{"category":null,"content":"SkyWalking 存储后端集成 #  Apache SkyWalking 是一款广泛使用的分布式应用性能监控（APM）系统。Easysearch 可以作为其存储后端，存储链路追踪和指标数据。\n相关指南（先读这些） #    OpenTelemetry 集成  集群监控  架构说明 #  应用服务（Agent） ↓ gRPC / HTTP SkyWalking OAP Server ↓ ES 协议 Easysearch（存储后端） ↓ SkyWalking UI / Grafana（可视化） SkyWalking OAP Server 使用 Elasticsearch 协议写入和查询数据。由于 Easysearch 兼容 ES 7.x API，可以直接配置为存储后端。\n配置步骤 #  1. 修改 OAP Server 配置 #  编辑 application.yml，将存储类型设为 elasticsearch：\nstorage: selector: elasticsearch elasticsearch: namespace: sw clusterNodes: https://easysearch-node1:9200 protocol: https user: admin password: your_password trustStorePath: /path/to/truststore.jks trustStorePass: changeit # 索引分片配置 indexShardsNumber: 2 indexReplicasNumber: 1 # 数据保留天数 recordDataTTL: 7 metricsDataTTL: 30 # 批量写入设置 bulkActions: 5000 flushInterval: 15 concurrentRequests: 2 2. 关键参数说明 #     参数 说明 推荐值     clusterNodes Easysearch 节点地址 所有节点地址   indexShardsNumber 每个索引的分片数 2~3   indexReplicasNumber 副本数 1   recordDataTTL Trace 记录保留天数 3~7   metricsDataTTL 指标数据保留天数 7~30   bulkActions 批量写入操作数 5000   flushInterval 刷新间隔（秒） 15    3. 验证连接 #  启动 OAP Server 后，检查日志确认连接成功：\ngrep \u0026#34;Elasticsearch storage\u0026#34; oap.log 在 Easysearch 中可以看到 SkyWalking 创建的索引：\nGET _cat/indices/sw_*?v\u0026amp;s=index 容量规划 #     应用规模 日均 Trace 量 建议节点数 建议磁盘空间/节点     小型（\u0026lt;10 服务） \u0026lt;100 万 3 节点 100 GB   中型（10-50 服务） 100-1000 万 3~5 节点 500 GB   大型（\u0026gt;50 服务） \u0026gt;1000 万 5+ 节点 1 TB+    优化建议 #   采样率：在高流量场景下配置 SkyWalking Agent 的采样率（如 10%），减少数据量 索引生命周期：配合 Easysearch 的 ILM 策略自动清理过期索引 Cold 存储：历史数据可以迁移到低成本存储节点 独立集群：生产环境建议将 APM 数据存储在独立的 Easysearch 集群中，避免影响业务搜索  ","subcategory":null,"summary":"","tags":null,"title":"SkyWalking 存储后端集成","url":"/easysearch/main/docs/integrations/observability/skywalking/"},{"category":null,"content":"Python 客户端 #  本页面帮助你快速跑通 Python 客户端连接 Easysearch 的完整流程。\n推荐：Easysearch 官方 Python 客户端 #  Easysearch 提供了官方 Python 客户端 easysearch-py（Apache 2.0 开源），包名 easysearch。\n 兼容说明：也可继续使用 elasticsearch-py 7.10.x 兼容连接，API 调用方式相同，仅导入路径不同。下文示例以官方客户端为主，兼容用法见 备选方案。\n 安装依赖 #  # 官方客户端（推荐） pip install https://github.com/infinilabs/easysearch-py/releases/download/v0.1.0/easysearch-0.1.0-py2.py3-none-any.whl # 如需 async/await 支持 pip install \u0026quot;easysearch[async] @ https://github.com/infinilabs/easysearch-py/releases/download/v0.1.0/easysearch-0.1.0-py2.py3-none-any.whl\u0026amp;#34; 建立连接 #\n from easysearch import Easysearch es = Easysearch( [\u0026quot;https://localhost:9200\u0026quot;], http_auth=(\u0026quot;admin\u0026quot;, \u0026quot;your_password\u0026quot;), verify_certs=False, # 开发环境；生产环境请配置 CA 证书 timeout=30, max_retries=3, retry_on_timeout=True, )\n# 验证连接 print(es.info()) 生产环境（使用 CA 证书） #\n from ssl import create_default_context context = create_default_context(cafile=\u0026quot;/path/to/root-ca.pem\u0026quot;) es = Easysearch( [\u0026quot;https://easysearch-host:9200\u0026quot;], ssl_context=context, http_auth=(\u0026quot;admin\u0026quot;, \u0026quot;your_password\u0026quot;), timeout=30, ) 索引文档 #\n doc = { \u0026#34;title\u0026#34;: \u0026#34;Easysearch 入门\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;分布式搜索引擎快速上手\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;搜索\u0026#34;, \u0026#34;教程\u0026#34;], \u0026#34;views\u0026#34;: 100, } resp = es.index(index=\u0026quot;articles\u0026quot;, id=1, body=doc) print(resp[\u0026quot;result\u0026quot;]) # created 批量写入 #\n from easysearch.helpers import bulk actions = [ {\u0026quot;_index\u0026quot;: \u0026quot;articles\u0026quot;, \u0026quot;_id\u0026quot;: i, \u0026quot;_source\u0026quot;: {\u0026quot;title\u0026quot;: f\u0026quot;文章 {i}\u0026quot;, \u0026quot;views\u0026quot;: i * 10}} for i in range(2, 102) ]\nsuccess, errors = bulk(es, actions) print(f\u0026quot;成功写入 {success} 条\u0026quot;) 获取文档 #\n resp = es.get(index=\u0026#34;articles\u0026#34;, id=1) print(resp[\u0026#34;_source\u0026#34;]) 搜索 #  全文搜索 #  resp = es.search( index=\u0026#34;articles\u0026#34;, body={ \u0026#34;query\u0026#34;: {\u0026#34;match\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;Easysearch\u0026#34;}}, }, ) for hit in resp[\u0026quot;hits\u0026quot;][\u0026quot;hits\u0026quot;]: print(f'{hit[\u0026quot;_score\u0026quot;]:.2f} {hit[\u0026quot;_source\u0026quot;][\u0026quot;title\u0026quot;]}') Bool 组合查询 #\n resp = es.search( index=\u0026#34;articles\u0026#34;, body={ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [{\u0026#34;match\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;文章\u0026#34;}}], \u0026#34;filter\u0026#34;: [{\u0026#34;range\u0026#34;: {\u0026#34;views\u0026#34;: {\u0026#34;gte\u0026#34;: 500}}}], } }, \u0026#34;sort\u0026#34;: [{\u0026#34;views\u0026#34;: \u0026#34;desc\u0026#34;}], \u0026#34;size\u0026#34;: 5, }, ) 聚合 #  resp = es.search( index=\u0026#34;articles\u0026#34;, body={ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;tag_count\u0026#34;: {\u0026#34;terms\u0026#34;: {\u0026#34;field\u0026#34;: \u0026#34;tags.keyword\u0026#34;}}, \u0026#34;avg_views\u0026#34;: {\u0026#34;avg\u0026#34;: {\u0026#34;field\u0026#34;: \u0026#34;views\u0026#34;}}, }, }, ) for bucket in resp[\u0026quot;aggregations\u0026quot;][\u0026quot;tag_count\u0026quot;][\u0026quot;buckets\u0026quot;]: print(f'{bucket[\u0026quot;key\u0026quot;]}: {bucket[\u0026quot;doc_count\u0026quot;]}') 更新与删除 #\n # 局部更新 es.update(index=\u0026#34;articles\u0026#34;, id=1, body={\u0026#34;doc\u0026#34;: {\u0026#34;views\u0026#34;: 200}}) # 删除文档 es.delete(index=\u0026quot;articles\u0026quot;, id=1)\n# 删除索引 es.indices.delete(index=\u0026quot;articles\u0026quot;) 注意事项 #\n    事项 说明     推荐客户端 easysearch-py（官方客户端，包名 easysearch）   兼容客户端 elasticsearch==7.10.1 也可正常使用   证书 开发环境可 verify_certs=False；生产环境应配置 CA 证书   多节点 传入列表 [\u0026quot;https://node1:9200\u0026quot;, \u0026quot;https://node2:9200\u0026quot;]    备选：使用 elasticsearch-py 兼容连接 #  Easysearch 兼容 Elasticsearch 7.10 API，也可以使用 elasticsearch-py 7.10.x 客户端：\npip install elasticsearch==7.10.1 from elasticsearch import Elasticsearch es = Elasticsearch( [\u0026quot;https://localhost:9200\u0026quot;], http_auth=(\u0026quot;admin\u0026quot;, \u0026quot;your_password\u0026quot;), verify_certs=False, timeout=30, ) \n兼容客户端的 API 调用方式完全相同，仅导入路径不同：from elasticsearch import Elasticsearch、from elasticsearch.helpers import bulk。\n 相关文档 #    easysearch-py GitHub  使用 Curl 访问 Easysearch  Java 客户端  入门教程  ","subcategory":null,"summary":"","tags":null,"title":"Python 客户端","url":"/easysearch/main/docs/integrations/clients/python/"},{"category":null,"content":"ORM 框架集成 #  在使用关系数据库的应用中，常见需求是将数据库数据同步到 Easysearch 以实现搜索功能。本文介绍常见 ORM 框架与 Easysearch 的集成方式。\n架构模式 #  模式一：应用层双写 #  应用程序 → 写入 MySQL (ORM) → 同时写入 Easysearch (ES Client) 优点：实现简单，实时性好 缺点：一致性难保障，应用耦合度高\n模式二：CDC 同步（推荐） #  应用程序 → 写入 MySQL ↓ Canal / Debezium 监听 binlog ↓ 同步到 Easysearch 优点：应用无感知，一致性好 缺点：需要额外组件\n模式三：定时全量/增量同步 #  定时任务 → 查询 MySQL 增量数据 → 批量写入 Easysearch 优点：简单可靠 缺点：有延迟（取决于同步周期）\nSpring Data Elasticsearch #  Spring Data 提供了对 Elasticsearch 的原生支持，可直接连接 Easysearch：\n1. 添加依赖 #  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-elasticsearch\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  使用 Spring Boot 2.x 系列（对应 ES 7.x 客户端）。\n 2. 配置连接 #  # application.yml spring: elasticsearch: uris: https://localhost:9200 username: admin password: your-password SSL 配置（自签名证书）：\n@Configuration public class EsConfig extends AbstractElasticsearchConfiguration { @Override public RestHighLevelClient elasticsearchClient() { RestClientBuilder builder = RestClient.builder( new HttpHost(\u0026#34;localhost\u0026#34;, 9200, \u0026#34;https\u0026#34;) ).setHttpClientConfigCallback(httpClientBuilder -\u0026gt; httpClientBuilder .setDefaultCredentialsProvider(credentialsProvider()) .setSSLContext(sslContext()) // 信任自签名证书  ); return new RestHighLevelClient(builder); } } 3. 定义实体 #  @Document(indexName = \u0026#34;product\u0026#34;) public class Product { @Id private String id; @Field\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;type \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; FieldType\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Text\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; analyzer \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;ik_smart\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; String title\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; @Field\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;type \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; FieldType\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Keyword\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; String category\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; @Field\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;type \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; FieldType\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;Double\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; Double price\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt;  } 4. 定义 Repository #\n public interface ProductRepository extends ElasticsearchRepository\u0026lt;Product, String\u0026gt; { List\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;Product\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;findByTitleContaining\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;String keyword\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; List\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;Product\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;findByCategoryAndPriceLessThan\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;String category\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; Double price\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt;  } MyBatis + ES Client 双写 #\n @Service @Transactional public class ProductService { @Autowired \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; ProductMapper mysqlMapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// MyBatis Mapper   @Autowired private RestHighLevelClient esClient;\n\u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;createProduct\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Product product\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 1. 写入 MySQL   mysqlMapper.insert(product);\n \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 2. 同步到 Easysearch   IndexRequest request = new IndexRequest(\u0026quot;product\u0026quot;) .id(product.getId()) .source(JSON.toJSONString(product), XContentType.JSON); esClient.index(request, RequestOptions.DEFAULT); } } \n双写模式下，建议通过消息队列解耦，避免 ES 写入失败影响主流程。\n Canal 数据同步 #   Canal 可以监听 MySQL binlog，将变更实时同步到 Easysearch：\nMySQL binlog → Canal Server → Canal Client → Easysearch Canal Adapter 内置了 ES 适配器，配置示例：\n# application.yml (canal-adapter) canalAdapters: - instance: example groups: - outAdapters: - name: es7 hosts: https://localhost:9200 properties: security.auth: admin:your-password cluster.name: easysearch 注意事项 #     注意项 说明     API 兼容 开启 elasticsearch.api_compatibility: true   客户端版本 使用 ES 7.10.2 oss 版本的客户端   HTTPS 需配置 SSL 信任   一致性 双写模式考虑最终一致性方案   事务 Easysearch 不支持分布式事务，搜索数据允许短暂延迟    延伸阅读 #    Java 客户端  Easy-ES 查询框架  数据接入  SeaTunnel 集成  ","subcategory":null,"summary":"","tags":null,"title":"ORM 框架集成","url":"/easysearch/main/docs/integrations/third-party/orm/"},{"category":null,"content":"NVMe 配置指南 #  NVMe SSD 提供极高的 IOPS 和带宽，是 Easysearch 生产环境的首选存储。\n为什么选择 NVMe #     指标 SATA SSD NVMe SSD     随机读 IOPS ~90K ~500K+   随机写 IOPS ~50K ~200K+   顺序读带宽 ~550 MB/s ~3,500 MB/s   延迟 ~100us ~20us    Easysearch 的段合并、translog 写入、倒排索引读取都受益于低延迟和高 IOPS。\n磁盘格式化与挂载 #  # 查看 NVMe 设备 lsblk | grep nvme # 格式化（推荐 xfs 或 ext4） mkfs.xfs -f /dev/nvme0n1\n# 挂载（关键：noatime 减少元数据写入） mkdir -p /data/easysearch mount -o noatime,nodiratime /dev/nvme0n1 /data/easysearch\n# 持久化 echo \u0026quot;/dev/nvme0n1 /data/easysearch xfs noatime,nodiratime 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab 文件系统选择 #\n    文件系统 优势 适用场景     xfs 大文件处理优秀、并行 IO 好 大索引、高吞吐   ext4 稳定成熟、小文件性能好 通用场景    IO 调度器 #  NVMe 建议使用 none 调度器：\n# 查看当前调度器 cat /sys/block/nvme0n1/queue/scheduler # 设置为 none echo none \u0026gt; /sys/block/nvme0n1/queue/scheduler\n# 持久化（udev 规则） echo 'ACTION==\u0026quot;add|change\u0026quot;, KERNEL==\u0026quot;nvme*\u0026quot;, ATTR{queue/scheduler}=\u0026quot;none\u0026quot;'  \u0026gt; /etc/udev/rules.d/60-nvme-scheduler.rules 多盘配置 #\n 多块 NVMe 时，配置多个数据路径实现条带化：\n# easysearch.yml path.data: - /data1/easysearch - /data2/easysearch Easysearch 会将分片分布到不同路径，自动利用多盘并行 IO。\n 不建议在多 NVMe 上再做软 RAID-0，直接用多路径更简单。\n 健康监控 #  # 查看 NVMe SMART 信息 nvme smart-log /dev/nvme0n1 # 关注指标： # - percentage_used: 磨损百分比 # - data_units_written: 总写入量 # - media_errors: 介质错误数 云环境 NVMe #\n    云厂商 本地 NVMe 实例 注意事项     阿里云 i 系列 实例释放后数据丢失   AWS i3 / i3en 实例停止后数据丢失   腾讯云 IT5 系列 与实例生命周期绑定     使用本地 NVMe 实例时，必须配置副本以保障数据安全。\n 延伸阅读 #    RAID 配置  生产环境部署  系统调优  ","subcategory":null,"summary":"","tags":null,"title":"NVMe 配置","url":"/easysearch/main/docs/deployment/advanced-config/nvme/"},{"category":null,"content":"Mapping 模式与最佳实践 #  在理解了 Mapping 基础之后，本页给出几种最常用、最实用的 Mapping 设计模式，帮助你避免常见坑。\n复杂数据类型：数组、对象与内部对象 #  多值字段（数组） #  任何字段都可以包含多个值，以数组形式索引：\n{ \u0026#34;tag\u0026#34;: [ \u0026#34;search\u0026#34;, \u0026#34;nosql\u0026#34; ] } 要点：\n 数组中所有值必须是相同数据类型（不能混用日期和字符串） 如果通过索引数组创建新字段，Easysearch 会用第一个值的数据类型作为字段类型 数组在索引时被处理为“多值字段”，可以搜索，但无序 搜索时不能指定“第一个”或“最后一个”元素   注意：从 Easysearch 获取文档时，_source 中的数组顺序与索引时一致；但索引层面是无序的，可以把数组想象成“装在袋子里的值”。\n 空字段 #  以下三种情况被认为是空字段，不会被索引：\n{ \u0026#34;null_value\u0026#34;: null, \u0026#34;empty_array\u0026#34;: [], \u0026#34;array_with_null_value\u0026#34;: [ null ] } 在 Lucene 中不能存储 null 值，所以空字段等同于不存在。\n内部对象（嵌套对象） #  JSON 支持嵌套对象，例如：\n{ \u0026#34;tweet\u0026#34;: \u0026#34;Easysearch is very flexible\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;@johnsmith\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, \u0026#34;age\u0026#34;: 26, \u0026#34;name\u0026#34;: { \u0026#34;full\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;first\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;Smith\u0026#34; } } } Easysearch 会自动将内部对象映射为 object 类型，并在 properties 下列出内部字段：\n{ \u0026#34;user\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;gender\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;age\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;full\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;first\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;last\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } } } 内部对象是如何索引的 #  Lucene 不理解内部对象，它只处理扁平化的键值对。Easysearch 会将文档转换为：\n{ \u0026#34;tweet\u0026#34;: [ \u0026#34;easysearch\u0026#34;, \u0026#34;flexible\u0026#34;, \u0026#34;very\u0026#34; ], \u0026#34;user.id\u0026#34;: [ \u0026#34;@johnsmith\u0026#34; ], \u0026#34;user.gender\u0026#34;: [ \u0026#34;male\u0026#34; ], \u0026#34;user.age\u0026#34;: [ 26 ], \u0026#34;user.name.full\u0026#34;: [ \u0026#34;john\u0026#34;, \u0026#34;smith\u0026#34; ], \u0026#34;user.name.first\u0026#34;: [ \u0026#34;john\u0026#34; ], \u0026#34;user.name.last\u0026#34;: [ \u0026#34;smith\u0026#34; ] } 可以通过字段名（如 first）或完整路径（如 user.name.first）引用内部字段。\n内部对象数组的限制 #  考虑包含内部对象的数组：\n{ \u0026#34;followers\u0026#34;: [ { \u0026#34;age\u0026#34;: 35, \u0026#34;name\u0026#34;: \u0026#34;Mary White\u0026#34; }, { \u0026#34;age\u0026#34;: 26, \u0026#34;name\u0026#34;: \u0026#34;Alex Jones\u0026#34; }, { \u0026#34;age\u0026#34;: 19, \u0026#34;name\u0026#34;: \u0026#34;Lisa Smith\u0026#34; } ] } 这个文档会被扁平化为：\n{ \u0026#34;followers.age\u0026#34;: [ 19, 26, 35 ], \u0026#34;followers.name\u0026#34;: [ \u0026#34;alex\u0026#34;, \u0026#34;jones\u0026#34;, \u0026#34;lisa\u0026#34;, \u0026#34;smith\u0026#34;, \u0026#34;mary\u0026#34;, \u0026#34;white\u0026#34; ] } 问题：{age: 35} 和 {name: Mary White} 之间的关联性丢失了，因为每个多值字段只是一包无序的值。\n 可以问：“有一个26岁的追随者吗？” ✓ 但不能问：“是否有一个26岁且名字叫 Alex Jones 的追随者？” ✗  如果需要保持对象内部字段的关联性，应使用 nested 类型（详见「数据建模」章节）。\n模式一：text + keyword 双字段（multi-fields） #  适用场景：同一个字段既要用于全文检索，又要用于精确过滤/排序/聚合。\n惯用设计：\n{ \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } 这样可以在查询中：\n 用 match 作用在 name（全文检索） 用 term / terms / 聚合作用在 name.keyword（精确匹配）  模式二：动态映射控制 #  默认情况下，Easysearch 会为遇到的新字段自动创建映射（动态映射）。可以通过 dynamic 设置控制：\n true：动态添加新字段（默认） false：忽略新字段（不索引，但会保存在 _source） strict：遇到新字段抛出异常  示例：根对象严格，但允许某个内部对象动态：\n{ \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: \u0026#34;strict\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;stash\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;dynamic\u0026#34;: true } } } } 模式三：动态模板（dynamic_templates） #  使用 dynamic_templates 可以完全控制新字段的映射规则，例如：\n{ \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic_templates\u0026#34;: [ { \u0026#34;es\u0026#34;: { \u0026#34;match\u0026#34;: \u0026#34;*_es\u0026#34;, \u0026#34;match_mapping_type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;mapping\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;spanish\u0026#34; } } }, { \u0026#34;en\u0026#34;: { \u0026#34;match\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;match_mapping_type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;mapping\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;english\u0026#34; } } } ] } }  注意：match_mapping_type: \u0026quot;string\u0026quot; 会匹配动态检测为字符串的字段。在动态映射中，字符串字段会被映射为 text 类型（带 keyword 子字段），所以这个模板会应用到这些字段上。\n 模板按顺序检测，第一个匹配的会被应用。\n模式四：日期检测控制 #  默认情况下，Easysearch 会检测字符串是否像日期（如 2014-01-01），如果是就映射为 date 类型。这可能造成问题：\n 第一个文档：{ \u0026quot;note\u0026quot;: \u0026quot;2014-01-01\u0026quot; } → note 被映射为 date 第二个文档：{ \u0026quot;note\u0026quot;: \u0026quot;Logged out\u0026quot; } → 会报错，因为不是有效日期  可以通过 date_detection: false 关闭日期检测：\n{ \u0026#34;mappings\u0026#34;: { \u0026#34;date_detection\u0026#34;: false } } 模式五：规范化字段 #  很多业务字段需要统一格式后再用于搜索/聚合，例如：\n 地区名（“北京市”“北京”） 手机号（带/不带区号、间隔符号） 邮箱、URL 等  推荐做法：\n city_raw：原始输入（text，用于全文检索） city_normalized：规范化后的形态（keyword，用于过滤和聚合）  这样既保留了原始信息，又便于在查询与统计时避免噪音。\n模式六：为时间序列设计时间字段 #  时间序列场景中，建议：\n 使用统一的 @timestamp 或类似命名 一致的时间格式（通常是 ISO8601 或 epoch_millis） 确保该字段为 date 类型，以便：  范围过滤（range） 时间聚合（date_histogram）    这样可以让可视化/监控/分析工具有统一约定。\n常见反模式与踩坑 #    所有字符串一律 text：\n 结果：无法高效做过滤/聚合/排序，mapping 难以维护。 建议：人类可读字段用 text + keyword，ID/code/枚举直接用 keyword。    在 text 字段上做 term/range 查询：\n 结果：匹配的是 analyzer 产出的 token，而不是原始文本，行为常常“看起来对、其实不对”。 建议：全文统一走 match/match_phrase，精确条件落在 keyword/数值/日期字段。    需要子对象语义，却只用 object + 数组：\n 结果：内部字段关联性丢失，出现“26 岁 Mary White 被配成 19 岁 Lisa Smith”的假匹配。 建议：一旦需要问“同一个子对象里字段是否同时满足”，就应建成 nested，并按数据建模章节建议设计查询。    放任动态映射自由生长：\n 结果：字段爆炸（field1、field_1、field-1 等各成一列），类型漂移，mapping 越来越难管。 建议：对核心索引使用 dynamic: strict 或受控的 dynamic_templates，为“杂物袋”预留专门对象字段。    指望在线修改字段类型或分析器来“纠正历史”：\n 结果：硬改 mapping 不会重写历史数据，容易出现新旧数据行为不一致，甚至直接报错。 建议：遇到类型/分析器设计错误时，优先采用“新索引 + 重建索引 + 别名切换”的套路。    小结 #   数组字段会被处理为多值字段，索引时无序；内部对象数组会丢失对象内部关联性，需要 nested 类型才能保持关联 使用 multi-fields（text + keyword）统一解决“模糊搜索 + 精确过滤/聚合”的组合需求 通过 dynamic 和 dynamic_templates 控制动态映射行为，避免意外字段类型推断 提前为聚合、排序和时间序列准备好类型正确、命名统一的字段 适度引入“规范化字段”来保证查询与聚合时的一致性  下一步可以继续阅读：\n  Query DSL 基础  结构化搜索  Nested  参考手册（API 与参数） #    字段类型总览（功能手册）  映射参数（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"Mapping 模式与最佳实践","url":"/easysearch/main/docs/best-practices/data-modeling/mapping-patterns/"},{"category":null,"content":"Linux 环境下使用 Easysearch #   为了安全起见，Easysearch 不支持通过 root 身份来运行，需要新建普通用户，如 easysearch 用户来快速运行 Easysearch。\n 一键安装 #   通过我们提供的自动安装脚本可自动下载最新版本的 easysearch 进行解压安装，默认解压到 /data/easysearch\n curl -sSL http://get.infini.cloud | bash -s -- -p easysearch  脚本的可选参数如下：\n-v [版本号]（默认采用最新版本号）\n-d [安装目录]（默认安装到/data/easysearch）\n bundle 包运行 #   bundle 是内置 JDK 的安装包，不需要额外下载 JDK，可直接解压运行。\n # 创建 easysearch 用户 groupadd -g 602 easysearch useradd -u 602 -g easysearch -m -d /home/easysearch -c \u0026#39;easysearch\u0026#39; -s /bin/bash easysearch # 创建 easysearch 安装目录 mkdir -p /data/easysearch # 下载 bundle 包并解压到安装目录 wget -O - https://release.infinilabs.com/easysearch/stable/bundle/easysearch-1.14.0-2228-linux-amd64-bundle.tar.gz | tar -zx -C /data/easysearch # 初始化 cd /data/easysearch \u0026amp;\u0026amp; bin/initialize.sh # 调整目录权限 chown -R easysearch:easysearch /data/easysearch # 运行 Easysearch su easysearch -c \u0026#34;/data/easysearch/bin/easysearch -d -p pid\u0026#34; # 停止 Easysearch kill -9 $(cat pid) 手动安装 #  以 root 用户进行下面的操作\n 下载 JDK  #下载JDK并存储到/usr/src目录 wget -N https://cdn.azul.com/zulu/bin/zulu17.54.21-ca-jre17.0.13-linux_x64.tar.gz -P /usr/src 创建 JDK 解压后存储路径  mkdir -p /usr/local/jdk 解压文件到创建好的目录  tar -zxf /usr/src/zulu*.tar.gz -C /usr/local/jdk --strip-components 1 配置环境变量  #下载文件到/etc/profile.d wget -N https://release.infinilabs.com/easysearch/archive/java.sh -P /etc/profile.d 让配置生效  source /etc/profile 检查 java 版本信息  java -version 通过在线脚本进行 Easysearch 安装  curl -sSL http://get.infini.cloud |bash -s -- -p easysearch 创建 easysearch 用户组  groupadd -g 602 easysearch 创建 easysearch 用户，并添加到 easysearch 用户组  useradd -u 602 -g easysearch -m -d /home/easysearch -c \u0026#34;Easysearch user\u0026#34; -s /bin/bash easysearch 将 JDK 放置或通过软链接到 /data/easysearch/jdk  ln -s /usr/local/jdk /data/easysearch/jdk 初始化证书，密码及插件  cd /data/easysearch \u0026amp;\u0026amp; bin/initialize.sh 调整目录属主为 easysearch  chown -R easysearch:easysearch /data/easysearch 切换到 easysearch 用户  su - easysearch 运行 Easysearch  cd /data/easysearch \u0026amp;\u0026amp; bin/easysearch 将 Easysearch 配置为服务 #   如果您想通过服务的方式来运行 Easysearch，可手工配置 Easysearch 服务文件\n  下载服务文件  wget -N https://release.infinilabs.com/easysearch/archive/easysearch.service -P /usr/lib/systemd/system 或者直接编辑和执行下面的脚本:\nsudo tee /usr/lib/systemd/system/easysearch.service \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; [Unit] Description=Easysearch Documentation=https://www.infinilabs.com After=network.target [Service] Type=forking User=easysearch WorkingDirectory=/data/easysearch Environment=\u0026#34;ES_PATH_CONF=/data/easysearch/config\u0026#34; PIDFile=/data/easysearch/pid ExecStart=/data/easysearch/bin/easysearch -d -p pid PrivateTmp=true LimitNOFILE=65536 LimitNPROC=65536 LimitAS=infinity LimitFSIZE=infinity LimitMEMLOCK=infinity TimeoutStopSec=0 KillSignal=SIGTERM KillMode=process SendSIGKILL=no SuccessExitStatus=143 [Install] WantedBy=multi-user.target EOF  如果您的 Easysearch 运行用户及安装目录不同，请修改服务文件中的 User 及 ExecStart。\n 重新加载服务配置文件  systemctl daemon-reload 启动 Easysearch 服务  systemctl start easysearch 检查 Easysearch 服务状态  systemctl status easysearch 后续验证工作，请继续查看 安装指南\n","subcategory":null,"summary":"","tags":null,"title":"Linux","url":"/easysearch/main/docs/deployment/install-guide/linux/"},{"category":null,"content":"Drop 处理器 #  drop 处理器用于丢弃不进行索引的文档。这可以用于根据某些条件防止文档被索引。例如，您可能使用 drop 处理器来防止缺少重要字段或包含敏感信息的文档被索引。\n当 drop 处理器丢弃文档时，它不会引发任何错误，这使得它对于防止索引问题而不会在您的 Easysearch 日志中充斥错误消息非常有用。\n语法 #  以下是为 drop 处理器提供的语法：\n{ \u0026#34;drop\u0026#34;: { \u0026#34;if\u0026#34;: \u0026#34;ctx.foo == \u0026#39;bar\u0026#39;\u0026#34; } } 配置参数 #  下表列出了 drop 处理器所需的和可选参数。\n   参数 是否必填 描述     description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 drop-pii 的管道，该管道使用 drop 处理器防止包含个人身份信息（PII）的文档被索引：\nPUT /_ingest/pipeline/drop-pii { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that prevents PII from being indexed\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;drop\u0026#34;: { \u0026#34;if\u0026#34; : \u0026#34;ctx.user_info.contains(\u0026#39;password\u0026#39;) || ctx.user_info.contains(\u0026#39;credit card\u0026#39;)\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/drop-pii/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user_info\u0026#34;: \u0026#34;Sensitive information including credit card\u0026#34; } } ] } 以下示例响应确认管道按预期工作（文档已被删除）：\n{ \u0026#34;docs\u0026#34;: [ null ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中，可以看到 ID 为 1 的文档未进行索引：：\nPUT testindex1/_doc/1?pipeline=drop-pii { \u0026#34;user_info\u0026#34;: \u0026#34;Sensitive information including credit card\u0026#34; } ","subcategory":null,"summary":"","tags":null,"title":"Drop 处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/drop/"},{"category":null,"content":"Simple Pattern 分词器 #  simple_pattern 分词器使用正则表达式匹配文本，将匹配到的内容作为词元输出。它与 simple_pattern_split 的区别在于：simple_pattern 输出匹配的部分，而 simple_pattern_split 输出被分隔的部分。\n该分词器使用 Lucene 正则表达式，语法是标准正则表达式的子集。\n参数 #     参数 说明 默认值     pattern Lucene 正则表达式模式 空字符串（匹配空串）    示例 #  PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;simple_pattern\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;[0-9]{3}\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;my_tokenizer\u0026#34; } } } } } POST my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;fd]]]-%]afd][ 123 fd-ede 456\u0026quot; } 以上示例将产生 123 和 456 两个词元。\n相关指南 #    Simple Pattern Split 分词器  Pattern 分词器  文本分析基础  ","subcategory":null,"summary":"","tags":null,"title":"简单模式分词器（Simple Pattern）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/simple-pattern/"},{"category":null,"content":"点展开器 #  dot_expander 处理器是一个帮助你处理层次化数据的工具。它将包含点的字段转换为对象字段，使它们能够被管道中的其他处理器访问。如果没有这种转换，包含点的字段将无法被处理。\n以下是为 dot_expander 处理器提供的语法：\n{ \u0026#34;dot_expander\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field.to.expand\u0026#34; } } 配置参数 #  下表列出了 dot_expander 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 要展开成对象字段的字段。支持模板使用。   path 可选 此字段仅在要展开的字段嵌套在其他对象字段内时才需要。这是因为 field 参数仅识别叶字段。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道 #  以下查询创建了一个 dot_expander 处理器，该处理器将展开名为 user.address.city 和 user.address.state 的两个字段到嵌套对象中：\nPUT /_ingest/pipeline/dot-expander-pipeline { \u0026#34;description\u0026#34;: \u0026#34;Dot expander processor\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;dot_expander\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;user.address.city\u0026#34; } }, { \u0026#34;dot_expander\u0026#34;:{ \u0026#34;field\u0026#34;: \u0026#34;user.address.state\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/dot-expander-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user.address.city\u0026#34;: \u0026#34;New York\u0026#34;, \u0026#34;user.address.state\u0026#34;: \u0026#34;NY\u0026#34; } } ] } 返回内容确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user\u0026#34;: { \u0026#34;address\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;New York\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;NY\u0026#34; } } }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-17T01:32:56.501346717Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=dot-expander-pipeline { \u0026#34;user.address.city\u0026#34;: \u0026#34;Denver\u0026#34;, \u0026#34;user.address.state\u0026#34;: \u0026#34;CO\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 返回内容中指定的字段已展开为嵌套字段：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;_seq_no\u0026#34;: 3, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;user\u0026#34;: { \u0026#34;address\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;Denver\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CO\u0026#34; } } } } path 参数 #  您可以使用 path 参数来指定对象中点分隔字段的路径。例如，以下管道指定了位于 user 对象中的 address.city 字段：\nPUT /_ingest/pipeline/dot-expander-pipeline { \u0026#34;description\u0026#34;: \u0026#34;Dot expander processor\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;dot_expander\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;address.city\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;user\u0026#34; } }, { \u0026#34;dot_expander\u0026#34;:{ \u0026#34;field\u0026#34;: \u0026#34;address.state\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;user\u0026#34; } } ] } 您可以按照以下方式模拟管道：\nPOST _ingest/pipeline/dot-expander-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user\u0026#34;: { \u0026#34;address.city\u0026#34;: \u0026#34;New York\u0026#34;, \u0026#34;address.state\u0026#34;: \u0026#34;NY\u0026#34; } } } ] } dot_expander 处理器将文档转换为以下结构：\n{ \u0026#34;user\u0026#34;: { \u0026#34;address\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;New York\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;NY\u0026#34; } } } 字段名称冲突 #  如果已存在与 dot_expander 处理器应展开的值路径相同的字段，处理器将两个值合并为一个数组。\n考虑以下展开字段 user.name 的管道：\nPUT /_ingest/pipeline/dot-expander-pipeline { \u0026#34;description\u0026#34;: \u0026#34;Dot expander processor\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;dot_expander\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;user.name\u0026#34; } } ] } 您可以使用包含两个具有完全相同路径 user.name 的值的文档来模拟该管道：\nPOST _ingest/pipeline/dot-expander-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user.name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Steve\u0026#34; } } } ] } 返回内容中确认值已合并到数组中：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user\u0026#34;: { \u0026#34;name\u0026#34;: [ \u0026#34;Steve\u0026#34;, \u0026#34;John\u0026#34; ] } }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-17T01:44:57.420220551Z\u0026#34; } } } ] } 如果一个字段包含相同的名称但不同的路径，那么该字段需要重命名。例如，以下 _simulate 调用会返回一个解析异常：\nPOST _ingest/pipeline/dot-expander-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;user.name\u0026#34;: \u0026#34;Steve\u0026#34; } } ] } 为了避免解析异常，首先使用 rename 处理器重命名字段：\nPUT /_ingest/pipeline/dot-expander-pipeline { \u0026#34;processors\u0026#34; : [ { \u0026#34;rename\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;user\u0026#34;, \u0026#34;target_field\u0026#34; : \u0026#34;user.name\u0026#34; } }, { \u0026#34;dot_expander\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;user.name\u0026#34; } } ] } 现在您可以模拟管道：\nPOST _ingest/pipeline/dot-expander-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;user.name\u0026#34;: \u0026#34;Steve\u0026#34; } } ] } 返回内容中确认字段已合并：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user\u0026#34;: { \u0026#34;name\u0026#34;: [ \u0026#34;John\u0026#34;, \u0026#34;Steve\u0026#34; ] } }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-17T01:52:12.864432419Z\u0026#34; } } } ] } ","subcategory":null,"summary":"","tags":null,"title":"点展开器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/dot-expander/"},{"category":null,"content":"Snowball 分词过滤器 #  snowball 分词过滤器是一种基于 雪球算法的词干提取过滤器。它支持多种语言，并且比波特词干提取算法更加高效和准确。\n相关指南（先读这些） #    文本分析：词干提取  文本分析：规范化  参数说明 #  雪球分词过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：\n 阿拉伯语（Arabic） 亚美尼亚语（Armenian） 巴斯克语（Basque） 加泰罗尼亚语（Catalan） 丹麦语（Danish） 荷兰语（Dutch） 英语（English，默认值） 爱沙尼亚语（Estonian） 芬兰语（Finnish） 法语（French） 德语（German） 德语 2（German2） 匈牙利语（Hungarian） 意大利语（Italian） 爱尔兰语（Irish） Kp 立陶宛语（Lithuanian） 洛文斯（Lovins） 挪威语（Norwegian） 波特（Porter） 葡萄牙语（Portuguese） 罗马尼亚语（Romanian） 俄语（Russian） 西班牙语（Spanish） 瑞典语（Swedish） 土耳其语（Turkish）  参考样例 #  以下示例请求创建了一个名为 my-snowball-index 的新索引，并配置了一个带有雪球过滤器的分词器。\nPUT /my-snowball-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_snowball_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;snowball\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;English\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_snowball_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_snowball_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-snowball-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_snowball_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;running runners\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;runner\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"Snowball 词干分词过滤器（Snowball）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/snowball/"},{"category":null,"content":"Stemmer 分词过滤器 #  stemmer 分词过滤器会将单词缩减为其词根或基本形式（也称为词干 stem）。\n相关指南（先读这些） #    词干提取  文本分析基础  参数说明 #  词干提取分词过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：\n 阿拉伯语：arabic 亚美尼亚语：armenian 巴斯克语：basque 孟加拉语：bengali 巴西葡萄牙语：brazilian 保加利亚语：bulgarian 加泰罗尼亚语：catalan 捷克语：czech 丹麦语：danish 荷兰语：dutch、dutch_kp 英语：english（默认）、light_english、lovins、minimal_english、porter2、possessive_english 爱沙尼亚语：estonian 芬兰语：finnish、light_finnish 法语：light_french、french、minimal_french 加利西亚语：galician、minimal_galician（仅复数处理步骤） 德语：light_german、german、german2、minimal_german 希腊语：greek 印地语：hindi 匈牙利语：hungarian、light_hungarian 印尼语：indonesian 爱尔兰语：irish 意大利语：light_italian、italian 库尔德语（索拉尼方言）：sorani 拉脱维亚语：latvian 立陶宛语：lithuanian 挪威语（书面挪威语）：norwegian、light_norwegian、minimal_norwegian 挪威语（新挪威语）：light_nynorsk、minimal_nynorsk 葡萄牙语：light_portuguese、minimal_portuguese、portuguese、portuguese_rslp 罗马尼亚语：romanian 俄语：russian、light_russian 西班牙语：light_spanish、spanish 瑞典语：swedish、light_swedish 土耳其语：turkish   你也可以使用 name 参数作为 language 参数的别名。如果两个参数都被设置，name 参数将被忽略。\n 参考样例 #  以下示例请求创建了一个名为 my-stemmer-index 的新索引，并配置了一个带有词干提取过滤器的分词器。\nPUT /my-stemmer-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_english_stemmer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;english\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_stemmer_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_english_stemmer\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-stemmer-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_stemmer_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;running runs\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 12, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"词干分词过滤器（Stemmer）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stemmer/"},{"category":null,"content":"解析处理器 #  dissect 处理器从文档文本字段中提取值，并根据解析模式将它们映射到单个字段。该处理器非常适合从具有已知结构的日志消息中提取字段。与 grok 处理器不同，dissect 不使用正则表达式，语法更简单。\n语法 #  以下是为 dissect 处理器提供的语法：\n{ \u0026#34;dissect\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;source_field\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;%{dissect_pattern}\u0026#34; } } 配置参数 #  下表列出了 dissect 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 要解析的数据所包含的字段名称。支持模板使用。   pattern 必填 用于从指定字段提取数据的 dissect 模式。   append_separator 可选 分隔字符或字符串，用于分隔附加字段。默认为 \u0026quot;\u0026quot; （空字符串）。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。如果设置为 true ，则处理器在字段不存在或为 null 时不会修改文档。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 dissect-test 的管道，该管道使用 dissect 处理器解析日志行：\nPUT /_ingest/pipeline/dissect-test { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that dissects web server logs\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;dissect\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;%{client_ip} - - [%{timestamp}] \\\u0026#34;%{http_method} %{url} %{http_version}\\\u0026#34; %{response_code} %{response_size}\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/dissect-test/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;192.168.1.10 - - [03/Nov/2023:15:20:45 +0000] \\\u0026#34;POST /login HTTP/1.1\\\u0026#34; 200 3456\u0026#34; } } ] } 返回内容确认管道正在按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;response_code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;http_method\u0026#34;: \u0026#34;POST\u0026#34;, \u0026#34;http_version\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;client_ip\u0026#34;: \u0026#34;192.168.1.10\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;\u0026#34;192.168.1.10 - - [03/Nov/2023:15:20:45 +0000] \u0026#34;POST /login HTTP/1.1\u0026#34; 200 3456\u0026#34;\u0026#34;\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;/login\u0026#34;, \u0026#34;response_size\u0026#34;: \u0026#34;3456\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;03/Nov/2023:15:20:45 +0000\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-11-03T22:28:32.830244044Z\u0026#34; } } } ] } 步骤 3：获取文档 #  以下查询将文档提取到名为 @0# 的索引中：\nPUT testindex1/_doc/1?pipeline=dissect-test { \u0026#34;message\u0026#34;: \u0026#34;192.168.1.10 - - [03/Nov/2023:15:20:45 +0000] \\\u0026#34;POST /login HTTP/1.1\\\u0026#34; 200 3456\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 解析匹配模式 #  解析模式是一种告诉 dissect 处理器如何将字符串解析为结构化格式的方法。该模式由要丢弃的字符串部分定义。例如， %{client_ip} - - [%{timestamp}] 解析模式将字符串 \u0026quot;192.168.1.10 - - [03/Nov/2023:15:20:45 +0000] \\\u0026quot;POST /login HTTP/1.1\\\u0026quot; 200 3456\u0026quot; 解析为以下字段：\nclient_ip: \u0026#34;192.168.1.1\u0026#34; @timestamp: \u0026#34;03/Nov/2023:15:20:45 +0000\u0026#34; 解析模式的工作原理是将字符串与一组规则进行匹配。例如，第一条规则会丢弃一个空格。 dissect 处理器会找到这个空格，然后将 client_ip 的值赋给该空格之前的所有字符。下一条规则匹配 [ 和 ] 字符，然后将 @timestamp 的值赋给其间的所有内容。\n建立解析匹配模式 #  构建解析模式时，务必注意要丢弃的字符串部分。如果丢弃过多的字符串， dissect 处理器可能无法成功解析剩余数据。相反，如果丢弃的字符串部分不足，处理器可能会创建不必要的字段。\n如果模式中定义的任何 %{keyname} 没有值，则会引发异常。您可以通过在 on_failure 参数中提供错误处理步骤来处理此异常。\n空值和取名跳过键 #  空键 %{} 或命名跳过键可用于匹配值，但会将该值排除在最终文档之外。如果您想要解析字符串但不需要存储其所有部分，这会很有用。\n将匹配的值转换为非字符串数据类型 #  默认情况下，所有匹配的值都表示为字符串数据类型。如果需要将值转换为其他数据类型，可以使用 convert 处理器 。\n修饰符 #  dissect 处理器支持可更改默认处理器行为的键修饰符。这些修饰符始终位于 %{keyname} 的左侧或右侧，并始终包含在 %{} 中。例如， %{+keyname-\u0026gt;} 修饰符包含追加修饰符和右填充修饰符。键修饰符可用于将多个字段合并为一行输出、创建格式化的数据项列表或聚合来自多个来源的值。\n下表列出了 dissect 处理器的主要修饰符。\n   修改符 名称 位置 用例 描述     -\u0026gt; 跳过右部分 右 %{keyname-\u0026gt;} 告诉 dissect 处理器跳过右侧的任何重复字符。例如，可以使用 %{timestamp-\u0026gt;} 来告诉处理器跳过 timestamp 后面的任何填充字符，例如两个连续的空格或任何不同的字符填充。   + 附加 左边 %{keyname} %{+keyname} 附加两个或多个字段。   + 和 /n 按顺序附加 左右都可以 %{+keyname}/2 %{+keyname/1} 按指定顺序附加两个或多个字段。   ? 命名跳过键 左边 %{?skipme} 跳过输出中匹配的值。行为与 %{} 相同。   * 和 \u0026amp; 参考键 左边 %{*r1} %{\u0026amp;r1} 将输出键设置为 * 的值，将输出值设置为 \u0026amp; 。    以下部分提供了每个键修饰符的详细描述以及使用示例。\n右填充修饰符（ -\u0026gt; ） #  解析算法非常精确，要求模式中的每个字符都与源字符串完全匹配。例如，模式 %{hellokey} %{worldkey} （一个空格）将匹配字符串“Hello world”（一个空格），但不会匹配字符串“Hello world”（两个空格），因为模式中只有一个空格，而源字符串有两个空格。\n可以使用右填充修饰符来解决这个问题。当添加到模式 %{helloworldkey-\u0026gt;} %{worldkey} 时，右填充修饰符将匹配 Hello world （1 个空格）、 Hello world （2 个空格），甚至 Hello world （10 个空格）。\n右填充修饰符用于允许 %{keyname-\u0026gt;} 后面的字符重复。右填充修饰符可以与任何其他修饰符一起应用于任何键。它应始终是最右边的修饰符，例如 %{+keyname/1-\u0026gt;} 或 %{} 。\n以下是如何使用右填充修饰符的示例：\n%{city-\u0026gt;}, %{state} %{zip} 在此模式中，右侧填充修饰符 -\u0026gt; 应用于 %{city} 键。两个地址包含相同的信息，但第二个条目在 city 字段中多了一个单词 City 。右侧填充修饰符允许模式匹配这两个地址条目，即使它们的格式略有不同：\nNew York, NY 10017 New York City, NY 10017 以下示例管道使用带有空键 %{-\u0026gt;} 右填充修饰符：\nPUT /_ingest/pipeline/dissect-test { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that dissects web server logs\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;dissect\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;[%{client_ip}]%{-\u0026gt;}[%{timestamp}]\u0026#34; } } ] } 您可以使用以下示例管道测试管道：\nPOST _ingest/pipeline/dissect-test/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;[192.168.1.10] [03/Nov/2023:15:20:45 +0000]\u0026#34; } } ] } 返回的应类似于以下内容：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;client_ip\u0026#34;: \u0026#34;192.168.1.10\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;[192.168.1.10] [03/Nov/2023:15:20:45 +0000]\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;03/Nov/2023:15:20:45 +0000\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-22T22:55:42.090569297Z\u0026#34; } } } ] } 附加修饰符 ( + ) #  附加修饰符将两个或多个值合并为一个输出值。这些值按从左到右的顺序附加。您还可以指定在值之间插入可选分隔符。\n以下是带有附加修饰符的示例管道：\nPUT /_ingest/pipeline/dissect-test { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that dissects web server logs\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;dissect\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;%{+address}, %{+address} %{+address}\u0026#34;, \u0026#34;append_separator\u0026#34;: \u0026#34;|\u0026#34; } } ] } 您可以使用以下示例管道测试管道：\nPOST _ingest/pipeline/dissect-test/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;New York, NY 10017\u0026#34; } } ] } 子字符串附加到 address 字段，如以下响应所示：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;New York|NY|10017\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;New York, NY 10017\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-22T22:30:54.516284637Z\u0026#34; } } } ] } 附加顺序修饰符（ + 和 /n ） #  带顺序附加修饰符会根据 / 后指定的顺序将两个或多个键的值合并为一个输出值。您可以灵活地自定义用于分隔附加值的分隔符。附加修饰符适用于将多个字段编译成单个格式化的输出行、构建结构化的数据项列表以及合并来自不同来源的值。\n以下示例管道使用附加顺序修饰符来反转前一个管道中定义的模式顺序。此管道指定要在附加字段之间插入的分隔符。如果不指定分隔符，则所有值都将以不带分隔符的方式附加。\nPUT /_ingest/pipeline/dissect-test { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that dissects web server logs\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;dissect\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;%{+address/3}, %{+address/2} %{+address/1}\u0026#34;, \u0026#34;append_separator\u0026#34;: \u0026#34;|\u0026#34; } } ] } 您可以使用以下示例管道测试管道：\nPOST _ingest/pipeline/dissect-test/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;New York, NY 10017\u0026#34; } } ] } 子字符串以相反的顺序附加到 address 字段，如以下响应所示：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10017|NY|New York\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;New York, NY 10017\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-22T22:38:24.305974178Z\u0026#34; } } } ] } 命名跳过键修饰符 #  命名跳过键修饰符通过在模式中使用空键 {} 或 ? 修饰符，从最终输出中排除特定匹配项。例如，以下模式是等效的： %{firstName} %{lastName} %{?ignore} 和 %{firstName} %{lastName} %{} 。命名跳过键修饰符可用于从输出中排除不相关或不必要的字段。\n以下模式使用命名的跳过键从输出中排除某个字段（在本例中为 ignore ）。您可以为空键分配一个描述性名称（例如 %{?ignore} ，以明确说明应从最终输出中排除相应的值：\nPUT /_ingest/pipeline/dissect-test { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that dissects web server logs\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;dissect\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;%{firstName} %{lastName} %{?ignore}\u0026#34; } } ] } 您可以使用以下示例管道测试管道：\nPOST _ingest/pipeline/dissect-test/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;John Doe M.D.\u0026#34; } } ] } 返回的应类似于以下内容：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Doe\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;John Doe M.D.\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-22T22:41:58.161475555Z\u0026#34; } } } ] } 参考键（ * 和 \u0026amp; ） #  引用键使用解析后的值作为结构化内容的键值对。这在处理以键值对形式部分记录数据的系统时非常有用。通过使用引用键，您可以保留键值关系并维护提取信息的完整性。\n以下模式使用引用键将数据提取为结构化格式。在此示例中，提取了 client_ip 和两个键值对以获取以下值：\nPUT /_ingest/pipeline/dissect-test { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that dissects web server logs\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;dissect\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;%{client_ip} %{*a}:%{\u0026amp;a} %{*b}:%{\u0026amp;b}\u0026#34; } } ] } 您可以使用以下示例管道测试管道：\nPOST _ingest/pipeline/dissect-test/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;192.168.1.10 response_code:200 response_size:3456\u0026#34; } } ] } 这两个键值对被提取到字段中，如以下响应所示：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;client_ip\u0026#34;: \u0026#34;192.168.1.10\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;192.168.1.10 response_code:200 response_size:3456\u0026#34;, \u0026#34;response_size\u0026#34;: \u0026#34;3456\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-22T22:48:51.475535635Z\u0026#34; } } } ] } ","subcategory":null,"summary":"","tags":null,"title":"解析处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/dissect/"},{"category":null,"content":"Pattern 分词器 #  pattern 分词器是一种高度灵活的分词器，它允许你根据自定义的 Java 正则表达式将文本拆分为词元。与使用 Lucene 正则表达式的 simple_pattern 分词器和 simple_pattern_split 分词器不同，pattern 分词器能够处理更复杂、更细致的正则表达式模式，从而让你对文本的分词方式拥有更强的掌控力。\n相关指南（先读这些） #    文本分析：识别词元  文本分析基础  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用匹配词元生成器的分词器。该分词器会在 -、_ 或 . 字符处对文本进行分割。\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_pattern_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;[-_.]\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_pattern_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_pattern_tokenizer\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_pattern_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_pattern_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch-2024_v1.2\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;2024\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;start_offset\u0026#34;: 16, \u0026#34;end_offset\u0026#34;: 18, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;start_offset\u0026#34;: 19, \u0026#34;end_offset\u0026#34;: 20, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 } ] } 参数说明 #  匹配词元生成器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     pattern 可选 字符串 用于将文本拆分为词元的模式，需使用 Java 正则表达式指定。默认值是 \\W+。   flags 可选 字符串 配置以竖线分隔的标志，用于应用于正则表达式。例如，\u0026quot;CASE_INSENSITIVE \\| MULTILINE \\| DOTALL\u0026quot;。   group 可选 整数 指定用作词元的捕获组。默认值是 -1（在匹配处进行拆分）。    使用分组参数 #  以下示例请求配置了一个 group 参数，该参数仅捕获第二个捕获组。\nPUT /my_index_group2 { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_pattern_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;([a-zA-Z]+)(\\\\d+)\u0026#34;, \u0026#34;group\u0026#34;: 2 } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_pattern_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_pattern_tokenizer\u0026#34; } } } } } 使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index_group2/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_pattern_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;abc123def456ghi\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;start_offset\u0026#34;: 3, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;456\u0026#34;, \u0026#34;start_offset\u0026#34;: 9, \u0026#34;end_offset\u0026#34;: 12, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"正则表达式分词器（Pattern）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/pattern/"},{"category":null,"content":"Term Vectors API #  返回文档中特定字段的词项信息（词频、位置、偏移量等），用于文本分析和调试。\n请求格式 #  GET /\u0026lt;index\u0026gt;/_termvectors/\u0026lt;_id\u0026gt; POST /\u0026lt;index\u0026gt;/_termvectors/\u0026lt;_id\u0026gt; GET /\u0026lt;index\u0026gt;/_termvectors # 在请求体中提供临时文档 POST /\u0026lt;index\u0026gt;/_termvectors 批量获取：\nGET /_mtermvectors POST /_mtermvectors GET /\u0026lt;index\u0026gt;/_mtermvectors POST /\u0026lt;index\u0026gt;/_mtermvectors 路径参数 #     参数 必需 说明     \u0026lt;index\u0026gt; 是 目标索引   \u0026lt;_id\u0026gt; 否 文档 ID。省略时需在请求体中通过 doc 提供临时文档    查询参数 #     参数 类型 默认值 说明     fields string — 逗号分隔的字段列表   offsets boolean true 返回词项偏移量   positions boolean true 返回词项位置   payloads boolean true 返回词项负载   term_statistics boolean false 返回词项的总词频和文档频率   field_statistics boolean true 返回文档计数、文档频率之和、总词频之和   realtime boolean true 实时读取   routing string — 路由值   preference string — 查询偏好    示例 #  GET /website/_termvectors/1?fields=title 在请求体中提供临时文档（无需事先索引）：\nPOST /website/_termvectors { \u0026#34;doc\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Easysearch is fast\u0026#34; }, \u0026#34;per_field_analyzer\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;standard\u0026#34; } } Multi Term Vectors #  POST /_mtermvectors { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title\u0026#34;] }, { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title\u0026#34;, \u0026#34;text\u0026#34;] } ] } 或简写：\nPOST /website/_mtermvectors { \u0026#34;ids\u0026#34;: [\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;], \u0026#34;parameters\u0026#34;: { \u0026#34;fields\u0026#34;: [\u0026#34;title\u0026#34;], \u0026#34;term_statistics\u0026#34;: true } }  参考导航 #     需求 参见     分析器调试  映射与分析   检索单条文档  Get API    ","subcategory":null,"summary":"","tags":null,"title":"Term Vectors API","url":"/easysearch/main/docs/features/document-operations/term-vectors-api/"},{"category":null,"content":"Edge N-gram 分词器 #  edge_ngram 分词器会从每个单词的开头生成部分单词词元，也就是 n 元组。它会根据指定的字符分割文本，并在定义的最小和最大长度范围内生成词元。这种分词器在实现即输即搜（search-as-you-type）功能时特别有用。\n前缀 n 元组非常适合用于自动补全搜索，在这种搜索中单词的顺序可能会有所不同，例如在搜索产品名称或地址时。有关更多信息，请参阅\u0026quot;自动补全\u0026quot;相关内容。不过，对于像电影或歌曲标题这种顺序固定的文本，完成建议器（completion suggester）可能会更准确。\n默认情况下，edge_ngram 分词器生成的词元最小长度为 1，最大长度为 2。例如，当分析文本 Easysearch 时，默认配置会生成 \u0026ldquo;E\u0026rdquo; 和 \u0026ldquo;Ea\u0026rdquo; 这两个 n 元组。这些短的 n 元组常常会匹配到太多不相关的词条，所以调整 n 元组的长度，对分词器进行优化是很有必要的。\n相关指南（先读这些） #    文本分析：识别词元  部分匹配  文本分析基础  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用前缀 n 元词元生成器的分词器。该词元生成器生成长度为 3 到 6 个字符的词元，且将字母和符号都视为有效的词元字符。\nPUT /edge_n_gram_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;my_custom_tokenizer\u0026#34; } }, \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_custom_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;edge_ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 3, \u0026#34;max_gram\u0026#34;: 6, \u0026#34;token_chars\u0026#34;: [ \u0026#34;letter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /edge_n_gram_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Code 42 rocks!\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Cod\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;Code\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;roc\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;rock\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 12, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;rocks\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } 参数说明 #     参数 必填/选填 数据类型 描述     min_gram 选填 整数 词元的最小长度。默认值为 1。   max_gram 选填 整数 词元的最大长度。默认值为 2。   custom_token_chars 选填 字符串 被视为词元一部分的自定义字符（例如，+-_）。   token_chars 选填 字符串数组 定义要包含在词元中的字符类。词元会字符类规则进行拆分。默认是所有字符。可用的字符类包括：\n- letter：字母字符（例如，a、ç 或 “京”）\n- digit：数字字符（例如，3 或 7）\n- punctuation：标点符号（例如，! 或 ?）\n- symbol：其他符号（例如，$ 或 √）\n- whitespace：空格或换行符\n- custom：允许您在 custom_token_chars 设置中指定自定义字符。    最大词元长度（max_gram）参数的限制 #  最大词元长度（max_gram）参数设置了生成的词元的最大长度。当一个查询词项的长度超过这个设定时，它可能无法匹配索引中的任何词条。\n例如，如果将 max_gram 设置为 4，那么在索引过程中，查询词 explore 会被分词为 expl。这样一来，当搜索完整的词条 explore 时，就无法匹配到已索引的词元 expl 了。\n为了解决这个限制问题，你可以应用一个截断(truncate)词元过滤器，将搜索词缩短到最大词元长度。不过，这种方法也存在权衡取舍。将 explore 截断为 expl 可能会导致与一些不相关的词条匹配，比如 explosion 或 explicit，从而降低搜索精度。\n我们建议仔细权衡 max_gram 的值，以确保在进行高效分词的同时，尽量减少不相关的匹配。如果搜索精度至关重要，可考虑其他策略，比如调整查询分词器或微调过滤器。\n最佳实践 #  我们建议仅在索引创建阶段使用前缀 n 元词元生成器，以确保部分单词词元得以存储。在执行搜索时，应使用基础分词器来匹配所有的查询词条。\n配置“即输即搜”功能 #  要实现即输即搜（search-as-you-type）功能，可在索引创建时使用前缀 n 元词元生成器，在搜索时使用一个执行最少处理操作的分词器。以下示例展示了这种实现方法。\n使用前缀 n 元词元生成器创建一个索引：\nPUT /my-autocomplete-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;autocomplete\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;autocomplete\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34; ] }, \u0026#34;autocomplete_search\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;lowercase\u0026#34; } }, \u0026#34;tokenizer\u0026#34;: { \u0026#34;autocomplete\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;edge_ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 2, \u0026#34;max_gram\u0026#34;: 10, \u0026#34;token_chars\u0026#34;: [ \u0026#34;letter\u0026#34; ] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;autocomplete\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;autocomplete_search\u0026#34; } } } } 索引一个包含 product 字段的文档，并刷新该索引：\nPUT my-autocomplete-index/_doc/1?refresh { \u0026#34;title\u0026#34;: \u0026#34;Laptop Pro\u0026#34; } 这种配置确保了前缀 n 元词元生成器会将诸如 “Laptop” 这样的词条拆分成 “La”、“Lap” 和 “Lapt” 等词元，从而在搜索时能够实现部分匹配。在搜索阶段，standard词元生成器会简化查询操作，并且由于有小写字母过滤器的存在，还能确保匹配操作不区分大小写。\n现在，当搜索 “laptop Pr” 或 “lap pr” 时，就会基于部分匹配检索到相关的文档：\nGET my-autocomplete-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;lap pr\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34; } } } } 了解更多信息，请参阅  Search as you type 相关内容。\n","subcategory":null,"summary":"","tags":null,"title":"边缘 N-gram 分词器（Edge N-gram）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/edge-n-gram/"},{"category":null,"content":"日期处理器 #  date 处理器用于从文档字段中解析日期，并将解析后的数据添加到新字段中。默认情况下，解析后的数据存储在 @timestamp 字段中。\n语法 #  以下是为 date 处理器提供的语法：\n{ \u0026#34;date\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date_field\u0026#34;, \u0026#34;formats\u0026#34;: [\u0026#34;yyyy-MM-dd\u0026#39;T\u0026#39;HH:mm:ss.SSSZZ\u0026#34;] } } 配置参数 #  下表列出了 date 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要转换的数据的字段名称。支持模板使用。   formats 必填 期望的日期格式数组。可以是日期格式或以下格式之一：ISO8601、UNIX、UNIX_MS 或 TAI64N。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   locale 可选 解析日期时使用的区域设置。默认为 ENGLISH 。支持模板片段。   on_failure 可选 在处理器失败时运行的处理器列表。   output_format 可选 用于目标字段的日期格式。默认为 yyyy-MM-dd'T'HH:mm:ss.SSSZZ 。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。   target_field 可选 存储解析数据的字段名称。默认目标字段为 @timestamp 。   timezone 可选 解析日期时使用的时区。默认为 UTC 。支持模板片段。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 date-output-format 的管道，该管道使用 date 处理器将欧洲日期格式转换为美国日期格式，并添加了新的字段 date_us ，包含所需的 output_format ：\nPUT /_ingest/pipeline/date-output-format { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that converts European date format to US date format\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;date\u0026#34;: { \u0026#34;field\u0026#34; : \u0026#34;date_european\u0026#34;, \u0026#34;formats\u0026#34; : [\u0026#34;dd/MM/yyyy\u0026#34;, \u0026#34;UNIX\u0026#34;], \u0026#34;target_field\u0026#34;: \u0026#34;date_us\u0026#34;, \u0026#34;output_format\u0026#34;: \u0026#34;MM/dd/yyy\u0026#34;, \u0026#34;timezone\u0026#34; : \u0026#34;UTC\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/date-output-format/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;date_us\u0026#34;: \u0026#34;06/30/2023\u0026#34;, \u0026#34;date_european\u0026#34;: \u0026#34;30/06/2023\u0026#34; } } ] } 返回内容确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;date_us\u0026#34;: \u0026#34;06/30/2023\u0026#34;, \u0026#34;date_european\u0026#34;: \u0026#34;30/06/2023\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-08-22T17:08:46.275195504Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=date-output-format { \u0026#34;date_european\u0026#34;: \u0026#34;30/06/2023\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"日期处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/date/"},{"category":null,"content":"Fingerprint 分析器 #  fingerprint 分析器会创建一个文本指纹。该分析器对从输入生成的词项（词元）进行排序和去重，然后使用一个分隔符将它们连接起来。它通常用于数据去重，对于包含相同单词的相似输入，无论单词顺序如何，它都可以产生相同的输出结果。\nfingerprint 分析器由以下组件组成：\n standard 分词器 lowercase 分词过滤器 asciifolding 分词过滤器 stop 分词过滤器 fingerprint 分词过滤器  相关指南（先读这些） #    文本分析基础  文本分析：规范化  参数说明 #  指纹分词器可以使用以下参数进行配置。\n   参数 必填/可选 数据类型 描述     separator 可选 字符串 在对词项进行词元生成、排序和去重后，用于连接这些词项的字符。默认值为一个空格（ ）。   max_output_size 可选 整数 定义输出词元的最大大小。如果连接后的指纹超过此大小，将被截断。默认值为 255。   stopwords 可选 字符串或字符串列表 自定义的停用词列表。默认值为 _none_。   stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。    参考样例 #  以下命令创建一个名为 my_custom_fingerprint_index 带有指纹分词器的索引：\nPUT /my_custom_fingerprint_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_fingerprint_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;fingerprint\u0026#34;, \u0026#34;separator\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;max_output_size\u0026#34;: 50, \u0026#34;stopwords\u0026#34;: [\u0026#34;to\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;over\u0026#34;, \u0026#34;and\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_fingerprint_analyzer\u0026#34; } } } } 产生的词元 #  以下请求用来检查使用该分词器生成的词元：\nPOST /my_custom_fingerprint_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_fingerprint_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The slow turtle swims over to the dog\u0026#34; } 返回内容中包含了生成的词元：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;dog-slow-swims-turtle\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 37, \u0026#34;type\u0026#34;: \u0026#34;fingerprint\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } 深度定制 #  如果需要深度定制，你可以定义一个包含指纹分词器组件的分词器：\nPUT /custom_fingerprint_analyzer { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_fingerprint\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;asciifolding\u0026#34;, \u0026#34;fingerprint\u0026#34; ] } } } } } ","subcategory":null,"summary":"","tags":null,"title":"指纹分析器（Fingerprint）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/fingerprint-analyzer/"},{"category":null,"content":"KStem 分词过滤器 #  kstem 分词过滤器是一种词干提取过滤器，用于将单词还原为其词根形式。该过滤器是一种专为英语设计的轻量级算法词干提取器，它执行以下词干提取操作：\n 将复数形式的单词还原为单数形式。 将不同的动词时态转换为其基本形式。 去除常见的派生词尾，例如 \u0026ldquo;-ing\u0026rdquo; 或 \u0026ldquo;-ed\u0026rdquo;。  kstem 分词过滤器等同于配置了 light_english 语言的词干提取器过滤器。与其他词干提取过滤器（如 porter_stem）相比，它提供了更为保守的词干提取方式。\n相关指南（先读这些） #    文本分析：词干提取  文本分析：规范化  KStem 分词过滤器基于 Lucene 的 KStemFilter。如需了解更多信息，请参阅 Lucene 文档。\n参考样例 #  以下示例请求创建了一个名为 my_kstem_index 的新索引，并配置了一个带有 KStem 过滤器的分词器：\nPUT /my_kstem_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;kstem_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;kstem\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_kstem_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;kstem_filter\u0026#34; ] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_kstem_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_kstem_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_kstem_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;stops stopped\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;start_offset\u0026#34;: 6, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"KStem 词干分词过滤器（KStem）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/kstem/"},{"category":null,"content":"Count API #  返回匹配查询的文档数量，不返回文档内容。\n请求格式 #  GET /\u0026lt;index\u0026gt;/_count POST /\u0026lt;index\u0026gt;/_count GET /_count POST /_count 路径参数 #     参数 必需 说明     \u0026lt;index\u0026gt; 否 目标索引，支持逗号分隔多索引和通配符    查询参数 #     参数 类型 默认值 说明     q string — 简单查询字符串   df string — q 参数的默认搜索字段   default_operator string OR q 参数的默认逻辑运算符：AND 或 OR   analyzer string — q 参数使用的分析器   analyze_wildcard boolean false 是否分析通配符   lenient boolean — 宽松解析模式   routing string — 路由值   preference string — 查询偏好   min_score float — 最低分数过滤   terminate_after int 0 每分片最多计数的文档数（0 = 不限制）   expand_wildcards string open 通配符展开方式：open、closed、hidden、all、none   ignore_unavailable boolean false 忽略不存在的索引   allow_no_indices boolean true 允许通配符不匹配任何索引    请求体（可选） #  { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;published\u0026#34; } } } 示例 #  使用查询字符串：\nGET /website/_count?q=title:blog 使用请求体查询：\nPOST /website/_count { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;date\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2024-01-01\u0026#34; } } } } 响应：\n{ \u0026#34;count\u0026#34;: 42, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 5, \u0026#34;successful\u0026#34;: 5, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 } }  参考导航 #     需求 参见     执行完整搜索  全文搜索   检索单条文档  Get API    ","subcategory":null,"summary":"","tags":null,"title":"Count API","url":"/easysearch/main/docs/features/document-operations/count-api/"},{"category":null,"content":"系统索引 #  Easysearch 默认的身份信息存放在一个受保护的系统索引里面，名称为：.security， 将索引设置为系统索引可以对该索引的数据进行额外的保护，因为即使您的用户帐户对所有索引具有读取权限，也无法直接访问此系统索引中的数据。\n您可以在 easysearch.yml 中添加其它您希望需要受到保护的索引。\nsecurity.system_indices.enabled: true security.system_indices.indices: [\u0026#34;.infini-*\u0026#34;] 如果要访问系统索引，必须使用管理员证书的方式来进行： 配置管理证书:\ncurl -k --cert ./admin.crt --key ./admin.key -XGET \u0026#39;https://localhost:9200/.security/_search\u0026#39; 另一种方法是从每个节点上的 security.system_indices.index 列表中删除该索引，然后重新启动 Easysearch 即可正常操作该索引。\n","subcategory":null,"summary":"","tags":null,"title":"系统索引","url":"/easysearch/main/docs/operations/security/configuration/system-indices/"},{"category":null,"content":"Pattern 分析器 #  pattern 分析器允许你定义一个自定义分析器，该分析器使用正则表达式（regex）将输入文本分割成词元。它还提供了应用正则表达式、将词元转换为小写以及过滤掉停用词的功能。\n相关指南（先读这些） #    文本分析基础  文本分析：识别词元  参数说明 #  匹配模式分词器可以使用以下参数进行配置。\n   参数 必填/可选 数据类型 描述     pattern 可选 字符串 用于对输入内容进行分词匹配的 Java 正则表达式。默认值是 \\W+。   flags 可选 字符串 以竖线分隔的 Java 正则表达式的字符串标志，这些标志会修改正则表达式的行为。   lowercase 可选 布尔值 是否将词元转换为小写。默认值为 true。   stopwords 可选 字符串或字符串列表 个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。   stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。    参考样例 #  以下命令创建一个名为 my_pattern_index 并使用匹配模式分词器的索引：\nPUT /my_pattern_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_pattern_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;\\\\W+\u0026#34;, \u0026#34;lowercase\u0026#34;: true, \u0026#34;stopwords\u0026#34;: [\u0026#34;and\u0026#34;, \u0026#34;is\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_pattern_analyzer\u0026#34; } } } } 产生的词元 #  以下请求用来检查分词器生成的词元：\nPOST /my_pattern_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_pattern_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch is fast and scalable\u0026#34; } 返回内容中包含了产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;fast\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 18, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;scalable\u0026#34;, \u0026#34;start_offset\u0026#34;: 23, \u0026#34;end_offset\u0026#34;: 31, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"正则表达式分析器（Pattern）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/pattern-analyzer/"},{"category":null,"content":"日期索引名称处理器 #  date_index_name 处理器用于根据文档中的日期或时间戳字段将文档指向正确的基于时间的索引。处理器将 _index 元数据字段设置为日期数学索引名称表达式。然后处理器从正在处理的文档的 field 字段中获取日期或时间戳并将其格式化为日期数学索引名称表达式。然后提取的日期、index_name_prefix 值和 date_rounding 值被组合以创建日期数学索引表达式。例如，如果 field 字段包含值 2023-10-30T12:43:29.000Z，且 index_name_prefix 设置为 week_index-，date_rounding 设置为 w，则日期数学索引名称表达式为 week_index-2023-10-30。您可以使用 date_formats 字段指定日期数学索引表达式中的日期应如何格式化。\n以下是为 date_index_name 处理器提供的语法：\n{ \u0026#34;date_index_name\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;your_date_field or your_timestamp_field\u0026#34;, \u0026#34;date_rounding\u0026#34;: \u0026#34;rounding_value\u0026#34; } } 配置参数 #  下表列出了 date_index_name 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要附加数据的字段名称。支持模板使用。   date_rounding 必填 索引名称中的日期格式为圆整格式。有效值包括 y （年）、 M （月）、 w （周）、 d （日）、 h （小时）、 m （分钟）和 s （秒）。   date_formats 可选 用于解析日期或时间戳字段的日期格式数组。有效选项包括 Java 时间模式或以下格式之一：ISO8601、UNIX、UNIX_MS 或 TAI64N。默认为 yyyy-MM-dd'T'HH:mm:ss.SSSXX 。   index_name_format 可选 日期格式。默认为 yyyy-MM-dd 。支持模板片段。   index_name_prefix 可选 在日期之前附加的索引名称前缀。支持模板片段。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   locale 可选 解析日期的月份名称和星期几时使用的区域设置。默认为 ENGLISH 。支持模板片段。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。   timezone 可选 使用解析日期时的时间区域。默认为 UTC 。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建一个管道。 #  以下查询创建了一个名为 date-index-name1 的管道，该管道使用 date_index_name 处理器将日志索引到月度索引中：\nPUT /_ingest/pipeline/date-index-name1 { \u0026#34;description\u0026#34;: \u0026#34;Create weekly index pipeline\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;date_index_name\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date_field\u0026#34;, \u0026#34;index_name_prefix\u0026#34;: \u0026#34;week_index-\u0026#34;, \u0026#34;date_rounding\u0026#34;: \u0026#34;w\u0026#34;, \u0026#34;date_formats\u0026#34;: [\u0026#34;YYYY-MM-DD\u0026#34;] } } ] } 步骤 2（可选）：测试管道。 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/date-index-name1/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;date_field\u0026#34;: \u0026#34;2023-10-30\u0026#34; } } ] } 返回内容确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;\u0026lt;week_index-{2023-10-01||/w{yyyy-MM-dd|UTC}}\u0026gt;\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;date_field\u0026#34;: \u0026#34;2023-10-30\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-11-13T18:23:10.408593092Z\u0026#34; } } } ] } 步骤 3：摄取文档。 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=date-index-name1 { \u0026#34;date_field\u0026#34;: \u0026#34;2023-10-30\u0026#34; } 请求将文档索引到索引 week_index-2023-10-23 ，并将同一周内所有带有时标的文档也索引到同一个索引，因为管道按周进行四舍五入。\n{ \u0026#34;_index\u0026#34;: \u0026#34;week_index-2025-09-29\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 4, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 3, \u0026#34;_primary_term\u0026#34;: 1 } 步骤 4（可选）：检索文档。 #  要检索文档，请运行以下查询：\nGET week_index-2025-09-29/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"日期索引名称处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/date-index-name/"},{"category":null,"content":"Porter Stem 分词过滤器 #  porter_stem 分词过滤器会将单词还原为其基本（或词干）形式，并去除单词中常见的后缀，这有助于通过单词的词根来匹配相似的单词。例如，单词\u0026quot;running\u0026quot;会被词干提取为\u0026quot;run\u0026quot;。此分词过滤器主要用于英语，并基于 波特词干提取算法进行词干提取操作。\n相关指南（先读这些） #    文本分析：词干提取  文本分析：规范化  参考样例 #  以下示例请求创建了一个名为 my_stem_index 的新索引，并配置了一个带有波特词干提取过滤器的分词器。\nPUT /my_stem_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_porter_stem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;porter_stem\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;porter_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_porter_stem\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_stem_index/_analyze { \u0026#34;text\u0026#34;: \u0026#34;running runners ran\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;porter_analyzer\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;run\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;runner\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;ran\u0026#34;, \u0026#34;start_offset\u0026#34;: 16, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"Porter 词干分词过滤器（Porter Stem）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/porter-stem/"},{"category":null,"content":"N-gram 分词器 #  ngram 分词器会将文本拆分为指定长度的重叠 n-gram（固定长度为 n 的字符序列）。当你希望实现部分单词匹配或自动补全搜索功能时，这个分词器特别有用，因为它会生成原始输入文本的子字符串（n-gram 字符串）。\n相关指南（先读这些） #    文本分析：识别词元  部分匹配  文本分析基础  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 n-gram 词元生成器的分词器。\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_ngram_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 3, \u0026#34;max_gram\u0026#34;: 4, \u0026#34;token_chars\u0026#34;: [\u0026#34;letter\u0026#34;, \u0026#34;digit\u0026#34;] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_ngram_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_ngram_tokenizer\u0026#34; } } } } } 生成的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_ngram_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Search\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ {\u0026#34;token\u0026#34;: \u0026#34;Sea\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 3,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;Sear\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 4,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 1}, {\u0026#34;token\u0026#34;: \u0026#34;ear\u0026#34;,\u0026#34;start_offset\u0026#34;: 1,\u0026#34;end_offset\u0026#34;: 4,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;earc\u0026#34;,\u0026#34;start_offset\u0026#34;: 1,\u0026#34;end_offset\u0026#34;: 5,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;arc\u0026#34;,\u0026#34;start_offset\u0026#34;: 2,\u0026#34;end_offset\u0026#34;: 5,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;arch\u0026#34;,\u0026#34;start_offset\u0026#34;: 2,\u0026#34;end_offset\u0026#34;: 6,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 5}, {\u0026#34;token\u0026#34;: \u0026#34;rch\u0026#34;,\u0026#34;start_offset\u0026#34;: 3,\u0026#34;end_offset\u0026#34;: 6,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 6} ] } 参数说明 #  N-gram 词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     min_gram 可选 整数 n-gram 的最小长度。默认值为 1。   max_gram 可选 整数 n-gram 的最大长度。默认值为 2。   token_chars 可选 字符串列表 分词时要包含的字符类。有效值为：\n- letter（字母）\n- digit（数字）\n- whitespace（空白字符）\n- punctuation（标点符号）\n- symbol（符号）\n- custom（自定义，你还必须指定 custom_token_chars 参数）\n默认值为空列表 []，即保留所有字符。   custom_token_chars 可选 字符串 要包含在词元中的自定义字符。    关于 min_gram 与 max_gram 的最大差值 #  min_gram 和 max_gram 之间的最大差值可通过索引级别的 index.max_ngram_diff 设置进行配置，其默认值为 1。\n以下示例请求创建了一个带有自定义 index.max_ngram_diff 设置的索引：\nPUT /my-index { \u0026#34;settings\u0026#34;: { \u0026#34;index.max_ngram_diff\u0026#34;: 2, \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_ngram_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 3, \u0026#34;max_gram\u0026#34;: 5, \u0026#34;token_chars\u0026#34;: [\u0026#34;letter\u0026#34;, \u0026#34;digit\u0026#34;] } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_ngram_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_ngram_tokenizer\u0026#34; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"N-gram 分词器（N-gram）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/n-gram/"},{"category":null,"content":"Bulk API #  在单个请求中执行多个 index、create、update、delete 操作。是高吞吐写入的核心能力。\n请求格式 #  POST /_bulk POST /\u0026lt;index\u0026gt;/_bulk 路径参数 #     参数 必需 说明     \u0026lt;index\u0026gt; 否 默认索引。指定后操作行中可省略 _index    查询参数 #     参数 类型 默认值 说明     routing string — 默认路由值   pipeline string — 默认 Ingest Pipeline   refresh string false 所有操作完成后的刷新策略   timeout time 1m 超时时间   wait_for_active_shards string — 活跃分片数量   require_alias boolean — 要求目标必须是别名   _source string/boolean — 默认 _source 过滤   _source_includes string — 默认包含字段   _source_excludes string — 默认排除字段    请求体格式（NDJSON） #  请求体是逐行 JSON（Newline Delimited JSON）格式，每行一个 JSON 对象：\n{ \u0026#34;action\u0026#34;: { \u0026#34;metadata\u0026#34; } }\\n { \u0026#34;request_body\u0026#34; }\\n 格式要求：\n 每行必须以换行符 \\n 结尾，包括最后一行 行内不能包含未转义的换行符 不能使用 pretty 格式 Content-Type 应为 application/x-ndjson  操作类型 #     操作 说明 需要请求体行     index 写入文档，已存在则覆盖 是   create 仅创建，已存在则报 409 是   update 部分更新文档 是（doc 或 script）   delete 删除文档 否    操作行元数据 #     字段 说明     _index 目标索引（URL 已指定时可省略）   _id 文档 ID（index 和 create 可省略以自动生成）   routing 路由值   pipeline Ingest Pipeline   require_alias 要求目标是别名   _retry_on_conflict update 操作的冲突重试次数    示例 #  POST /_bulk { \u0026#34;delete\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;123\u0026#34; } } { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;123\u0026#34; } } { \u0026#34;title\u0026#34;: \u0026#34;My first blog post\u0026#34; } { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34; } } { \u0026#34;title\u0026#34;: \u0026#34;My second blog post\u0026#34; } { \u0026#34;update\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;_retry_on_conflict\u0026#34;: 3 } } { \u0026#34;doc\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;My updated blog post\u0026#34; } } 指定默认索引 #  POST /website/_bulk { \u0026#34;index\u0026#34;: {} } { \u0026#34;title\u0026#34;: \u0026#34;Event logged in\u0026#34; } { \u0026#34;index\u0026#34;: {} } { \u0026#34;title\u0026#34;: \u0026#34;Another event\u0026#34; } 响应 #  { \u0026#34;took\u0026#34;: 4, \u0026#34;errors\u0026#34;: false, \u0026#34;items\u0026#34;: [ { \u0026#34;delete\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;status\u0026#34;: 200, \u0026#34;result\u0026#34;: \u0026#34;deleted\u0026#34; } }, { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;_version\u0026#34;: 3, \u0026#34;status\u0026#34;: 201, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34; } }, { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;EiwfApScQiiy7TIKFxRCTw\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;status\u0026#34;: 201, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34; } }, { \u0026#34;update\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;_version\u0026#34;: 4, \u0026#34;status\u0026#34;: 200, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34; } } ] }  errors: false：所有子操作均成功 errors: true：至少有一条失败，需逐条检查 items 中的 status 与 error  部分失败示例 #  { \u0026#34;took\u0026#34;: 3, \u0026#34;errors\u0026#34;: true, \u0026#34;items\u0026#34;: [ { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;status\u0026#34;: 409, \u0026#34;error\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;version_conflict_engine_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;...\u0026#34; } } }, { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;124\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;status\u0026#34;: 201, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34; } } ] }  Bulk 不是事务。每条子操作独立执行、独立成功或失败。调用方需逐条检查结果并决定是否重试。\n 批次大小建议 #  没有固定最佳值，取决于硬件配置、文档大小、集群负载。推荐：\n 从 1,000–5,000 条文档 / 每批 5–15 MB 开始 逐步增大，观察节点 CPU/内存/IO/延迟 当吞吐量不再上升或开始下降时，即为当前环境的上限  Bulk 请求在协调节点一次性驻留内存，过大的批次会占用过多内存并影响 GC。\n 参考导航 #     需求 参见     单条写入  Index API   批量检索  Multi Get API   写入链路预处理  摄取管道   分布式写入过程  分布式写入过程    ","subcategory":null,"summary":"","tags":null,"title":"Bulk API","url":"/easysearch/main/docs/features/document-operations/bulk-api/"},{"category":null,"content":"数据保留与生命周期管理 #  时间序列数据（日志/指标/审计）有个现实：越新的越值钱，越老的越偶尔被翻。当数据规模持续增长，你面临三个经典问题：\n 性能与成本的平衡：最近数据要快速可查，历史数据如何低成本保留？ 合规与可追溯：历史数据要留多久？ 自动化运维：如何减少手工干预？  本页给你一套完整的数据保留策略体系。建议先了解 时间序列数据建模。\n 快速决策树 #  开始 → 数据还在写入? ├─ 是 → 放在\u0026#34;热\u0026#34;节点，使用好硬件 │ ├─ 准备自动化? → 用 ILM（推荐） │ └─ 手动管理? → 按时间切索引 │ └─ 不再写入 → 迁移到\u0026#34;温/冷\u0026#34;节点 ├─ 还会经常查询? → 温节点，forcemerge 优化 │ ├─ 很少查询? → 冷节点或关闭索引 │ └─ 长期存档? ├─ 快照 → 删索引（完全清除） └─ 或用 Rollup 汇总压缩  核心策略详解 #  1. 按时间切索引：最简单的删除策略 #  时间序列数据治理的第一优先：按时间切索引。\n这样做的好处：\n 删除旧数据变成\u0026quot;删整个索引\u0026quot;，而不是\u0026quot;给 N 个文档标记删除\u0026quot; 删索引是异步的、无锁的，不阻塞查询和写入 可以精确控制数据保留窗口  示例：按天切索引\nlogs-2026-02-20 # 今天的索引，持续写入 logs-2026-02-19 # 昨天，不再写入 logs-2026-02-01 # 1 个月前，可以删除 删除过期索引：\nDELETE /logs-2026-01* 2. 热温冷分层：资源利用的金字塔 #  按照访问频率和时间，将索引分配到不同硬件上：\n┌─ HOT（热） → 最好的 SSD、最多内存缓存 → 最近 1-7 天 ├─ WARM（温）→ 次级 SSD 或 HDD → 1 周-1 个月 └─ COLD（冷）→ 廉价存储、最少计算 → 1 个月以上 手动分层（使用节点标签）\n为节点打标签：\n# easysearch.yml node.attr.box_type: hot # 节点类型标记 为索引指定位置：\n# 热索引：部署到 hot 节点 PUT /logs-2026-02-20/_settings { \u0026#34;index.routing.allocation.include.box_type\u0026#34;: \u0026#34;hot\u0026#34; } 7 天后：迁移到 warm 节点 POST /logs-2026-02-13/_settings { \u0026quot;index.routing.allocation.include.box_type\u0026quot;: \u0026quot;warm\u0026quot; } 3. 索引优化：forcemerge #\n 对不再写入的索引，合并段可以减少文件数量、降低资源占用。\n使用场景：\n 日索引在第二天就不再写入，此时可以 forcemerge 必须先迁移到非热节点，避免影响热数据写入  # 合并为单个段 POST /logs-2026-02-19/_forcemerge?max_num_segments=1  注意：forcemerge 是 CPU 和 IO 密集操作，应在维护窗口执行，或确保 WARM/COLD 节点有充足资源。\n 4. 关闭索引：保留但休眠 #  当索引\u0026quot;基本不查但还要保留\u0026quot;（如合规要求保存 1 年），可以关闭它以释放运行时资源。\n# 先 flush，再关闭 POST /logs-2025-01*/_flush POST /logs-2025-01*/_close 关闭后的索引：\n ✅ 占用磁盘空间 ❌ 不占用 JVM 堆、内存缓存、打开的文件句柄 需要查询时可快速重新打开：POST /logs-2025-01*/_open  5. 数据汇总：Rollup #  对于指标/监控数据，用 Rollup 将细粒度数据聚合为粗粒度数据：\nPUT _rollup/jobs/metrics-daily { \u0026#34;rollup\u0026#34;: { \u0026#34;source_index\u0026#34;: \u0026#34;metrics-*\u0026#34;, \u0026#34;target_index\u0026#34;: \u0026#34;rollup_metrics\u0026#34;, \u0026#34;continuous\u0026#34;: true, \u0026#34;schedule\u0026#34;: { \u0026#34;interval\u0026#34;: { \u0026#34;period\u0026#34;: 1, \u0026#34;unit\u0026#34;: \u0026#34;Days\u0026#34;, \u0026#34;start_time\u0026#34;: 1 } }, \u0026#34;page_size\u0026#34;: 1000, \u0026#34;dimensions\u0026#34;: [ { \u0026#34;date_histogram\u0026#34;: { \u0026#34;source_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;fixed_interval\u0026#34;: \u0026#34;1h\u0026#34; } }, { \u0026#34;terms\u0026#34;: { \u0026#34;source_field\u0026#34;: \u0026#34;host.keyword\u0026#34; } } ], \u0026#34;metrics\u0026#34;: [ { \u0026#34;source_field\u0026#34;: \u0026#34;cpu_usage\u0026#34;, \u0026#34;metrics\u0026#34;: [{ \u0026#34;avg\u0026#34;: {} }, { \u0026#34;max\u0026#34;: {} }] }, { \u0026#34;source_field\u0026#34;: \u0026#34;memory_usage\u0026#34;, \u0026#34;metrics\u0026#34;: [{ \u0026#34;avg\u0026#34;: {} }, { \u0026#34;max\u0026#34;: {} }] } ] } } 优势：\n 原始 1s 精度数据保留 7 天 汇总为 1h 精度的数据保留 1 年 存储减少 99%，趋势数据完整保留  6. 快照归档：长期存储 #  对于必须存档但不需频繁访问的数据（如审计日志），快照是最佳方案：\n# 1. 创建快照仓库 PUT /_snapshot/archive { \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;bucket\u0026#34;: \u0026#34;my-archive-bucket\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-west-2\u0026#34; } } 2. 快照旧索引 PUT /_snapshot/archive/logs-202501-snapshot { \u0026quot;indices\u0026quot;: \u0026quot;logs-202501*\u0026quot; }\n3. 验证快照完成后，删除原索引 DELETE /logs-202501* \n完整用法见 备份与恢复\n  自动化：ILM 与 SLM #  索引生命周期管理（ILM） #  ILM 可以自动执行上述所有策略：定义一个生命周期策略后，所有匹配的索引自动经历 hot → warm → cold → delete 的阶段。\n定义策略：\nPUT _ilm/policy/logs-retention-policy { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;hot\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;0m\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;rollover\u0026#34;: { \u0026#34;max_docs\u0026#34;: 50000000 } # 5000万文档滚动 } }, \u0026#34;warm\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;3d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 50 }, \u0026#34;forcemerge\u0026#34;: { \u0026#34;max_num_segments\u0026#34;: 1 } } }, \u0026#34;cold\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 0 }, \u0026#34;read_only\u0026#34;: {} } }, \u0026#34;delete\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;90d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;delete\u0026#34;: {} } } } } } 通过模板关联策略，新索引自动管理：\nPUT _template/logs-template { \u0026#34;index_patterns\u0026#34;: [\u0026#34;logs-*\u0026#34;], \u0026#34;settings\u0026#34;: { \u0026#34;index.lifecycle.name\u0026#34;: \u0026#34;logs-retention-policy\u0026#34;, \u0026#34;index.lifecycle.rollover_alias\u0026#34;: \u0026#34;logs\u0026#34; }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } } } } 这样，每个新建的 logs-* 索引自动进入生命周期管理，无需手工干预。\n 详细参考： 索引生命周期管理 API\n 快照生命周期管理（SLM） #  SLM 自动执行快照，配合 ILM 使用：\nPUT _slm/policy/archive-snapshots { \u0026#34;description\u0026#34;: \u0026#34;每日快照归档\u0026#34;, \u0026#34;creation\u0026#34;: { \u0026#34;schedule\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;expression\u0026#34;: \u0026#34;0 0 2 * * ?\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Shanghai\u0026#34; } } }, \u0026#34;snapshot_config\u0026#34;: { \u0026#34;date_expression\u0026#34;: \u0026#34;\u0026lt;snapshot-{now/d}\u0026gt;\u0026#34;, \u0026#34;repository\u0026#34;: \u0026#34;archive\u0026#34;, \u0026#34;indices\u0026#34;: \u0026#34;logs-*\u0026#34; }, \u0026#34;deletion\u0026#34;: { \u0026#34;condition\u0026#34;: { \u0026#34;max_age\u0026#34;: \u0026#34;365d\u0026#34;, \u0026#34;min_count\u0026#34;: 7, \u0026#34;max_count\u0026#34;: 50 } } }  完整案例：日志系统的生命周期 #  假设日志系统需求：\n 最近 7 天：快速查询，用热节点 7-30 天：偶尔查询，用温节点 30-90 天：极少查询，用冷节点 90+ 天：归档到 S3，集群删除  配置步骤：\n 创建 ILM 策略  PUT _ilm/policy/logs-lifecycle { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;hot\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;0m\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;rollover\u0026#34;: { \u0026#34;max_primary_shard_size\u0026#34;: \u0026#34;30GB\u0026#34; } } }, \u0026#34;warm\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 50 }, \u0026#34;forcemerge\u0026#34;: { \u0026#34;max_num_segments\u0026#34;: 1 }, \u0026#34;allocation\u0026#34;: { \u0026#34;include\u0026#34;: { \u0026#34;box_type\u0026#34;: \u0026#34;warm\u0026#34; } } } }, \u0026#34;cold\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 0 }, \u0026#34;allocation\u0026#34;: { \u0026#34;include\u0026#34;: { \u0026#34;box_type\u0026#34;: \u0026#34;cold\u0026#34; } } } }, \u0026#34;delete\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;90d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;delete\u0026#34;: {} } } } } } 创建索引模板  PUT _template/logs-template { \u0026#34;index_patterns\u0026#34;: [\u0026#34;logs-*\u0026#34;], \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;index.lifecycle.name\u0026#34;: \u0026#34;logs-lifecycle\u0026#34;, \u0026#34;index.lifecycle.rollover_alias\u0026#34;: \u0026#34;logs\u0026#34; } } 创建初始索引并写入数据  # 写入时使用别名，不直接写索引 POST /logs/_doc { \u0026#34;@timestamp\u0026#34;: \u0026#34;2026-02-20T10:00:00Z\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Application started\u0026#34; } 之后，所有事情自动发生：\n 新索引自动进入 hot 阶段，分配到 hot 节点 7 天后自动迁移到 warm 节点并 forcemerge 30 天后迁移到 cold 节点 90 天后自动删除   最佳实践总结 #     场景 推荐策略 优势     新系统，想简单 按时间切 + 手工删除 无需复杂配置，易理解   中型集群，多节点 热温冷分层 + ILM 成本最优，自动化   大型集群 ILM + SLM + Rollup 完全自动化，存储最优   审计/合规 快照 + 归档 长期存储，成本低   指标数据 Rollup + 短期存储 保留趋势，极度省空间    关键建议：\n ✅ 优先用时间切索引，不要依赖按文档删除 ✅ 使用 ILM 自动化，减少人工运维 ✅ 热温冷分层，充分利用不同硬件 ✅ 定期备份，用 SLM 自动化快照 ✅ 监控磁盘和 JVM，提前预警   相关文档 #    快照与恢复  索引生命周期管理 API  快照生命周期管理 API  Rollup 数据汇总  可搜索快照  写入限流  时间序列索引优化  ","subcategory":null,"summary":"","tags":null,"title":"数据生命周期","url":"/easysearch/main/docs/features/data-retention/lifecycle/"},{"category":null,"content":"处理管道故障 #  摄取管道由一系列按顺序执行的处理器组成。如果某个处理器失败，默认行为是整条管道中止，文档不会被索引。\n你有两种方式应对处理器失败：\n 中止管道（默认）：处理器失败后整条管道停止，文档不被索引 跳过失败继续执行：通过 ignore_failure 或 on_failure 让管道在失败后继续运行  默认情况下，如果管道中的某个处理器失败，则摄取管道会停止。如果您希望在处理器失败时继续运行管道，您可以在创建管道时将该处理器的 ignore_failure 参数设置为 true ：\nPUT _ingest/pipeline/my-pipeline/ { \u0026#34;description\u0026#34;: \u0026#34;Rename \u0026#39;provider\u0026#39; field to \u0026#39;cloud.provider\u0026#39;\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;rename\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;provider\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;cloud.provider\u0026#34;, \u0026#34;ignore_failure\u0026#34;: true } } ] } 您可以将 on_failure 参数指定为在处理器失败后立即运行。如果您已指定 on_failure ，即使 on_failure 配置为空，Easysearch 也会运行管道中的其他处理器：\nPUT _ingest/pipeline/my-pipeline/ { \u0026#34;description\u0026#34;: \u0026#34;Add timestamp to the document\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;date\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;timestamp_field\u0026#34;, \u0026#34;formats\u0026#34;: [\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;], \u0026#34;target_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;on_failure\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;ingest_error\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;failed\u0026#34; } } ] } } ] } 如果处理器失败，Easysearch 会记录失败并继续运行搜索管道中剩余的所有处理器。要检查是否有任何失败，您可以使用摄取管道指标。\n摄取管道的监控指标 #  要查看摄取管道的监控指标，请使用节点统计 API：\nGET /_nodes/stats/ingest?filter_path=nodes.*.ingest 包含所有摄取管道的统计信息，例如：\n{ \u0026#34;nodes\u0026#34;: { \u0026#34;iFPgpdjPQ-uzTdyPLwQVnQ\u0026#34;: { \u0026#34;ingest\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;count\u0026#34;: 28, \u0026#34;time_in_millis\u0026#34;: 82, \u0026#34;current\u0026#34;: 0, \u0026#34;failed\u0026#34;: 9 }, \u0026#34;pipelines\u0026#34;: { \u0026#34;user-behavior\u0026#34;: { \u0026#34;count\u0026#34;: 5, \u0026#34;time_in_millis\u0026#34;: 0, \u0026#34;current\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0, \u0026#34;processors\u0026#34;: [ { \u0026#34;append\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;append\u0026#34;, \u0026#34;stats\u0026#34;: { \u0026#34;count\u0026#34;: 5, \u0026#34;time_in_millis\u0026#34;: 0, \u0026#34;current\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 } } } ] }, \u0026#34;remove_ip\u0026#34;: { \u0026#34;count\u0026#34;: 5, \u0026#34;time_in_millis\u0026#34;: 9, \u0026#34;current\u0026#34;: 0, \u0026#34;failed\u0026#34;: 2, \u0026#34;processors\u0026#34;: [ { \u0026#34;remove\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;stats\u0026#34;: { \u0026#34;count\u0026#34;: 5, \u0026#34;time_in_millis\u0026#34;: 8, \u0026#34;current\u0026#34;: 0, \u0026#34;failed\u0026#34;: 2 } } } ] } } } } } }  故障排除：处理摄取管道失败：您首先应该检查日志，看是否有任何错误或警告可以帮助您确定失败的原因。Easysearch 日志包含有关失败的摄取管道的信息，包括失败的处理器和失败的原因。\n ","subcategory":null,"summary":"","tags":null,"title":"处理管道故障","url":"/easysearch/main/docs/features/ingest-pipelines/pipeline-failures/"},{"category":null,"content":"Keyword 分词器 #  keyword 分词器接收文本并将其原封不动地作为单个词元输出。当你希望输入内容保持完整时，比如在管理像姓名、产品代码或电子邮件地址这类结构化数据时，这个分词器就特别有用。\nkeyword 分词器可以与词元过滤器搭配使用来处理文本。例如，对文本进行规范化处理或去除多余的字符。\n相关指南（先读这些） #    文本分析：识别词元  文本分析基础  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用关键字词元生成器的分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_keyword_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_keyword_analyzer\u0026#34; } } } } 生成的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_keyword_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch Example\u0026#34; } 返回内容会是包含原始内容的单个词元：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Easysearch Example\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 18, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } 参数说明 #  关键字词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     buffer_size 可选 整数 确定字符缓冲区的大小。默认值为 256。通常无需更改此设置。    将关键字词元生成器与词元过滤器结合使用 #  为了增强关键字词元生成器的功能，你可以将它与词元过滤器结合起来。词元过滤器能够对文本进行转换，例如将文本转换为小写形式或者移除不需要的字符。\n示例：使用匹配替换（pattern_replace）词元过滤器和关键字词元生成器 #  在这个示例中，匹配替换（pattern_replace）词元过滤器使用正则表达式，会将所有非字母数字字符替换为空字符串：\nPOST _analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;filter\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;pattern_replace\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;[^a-zA-Z0-9]\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;\u0026#34; } ], \u0026#34;text\u0026#34;: \u0026#34;Product#1234-XYZ\u0026#34; } pattern_replace词元过滤器会移除非字母数字字符，并返回以下词元：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Product1234XYZ\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 16, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"关键字分词器（Keyword）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/keyword/"},{"category":null,"content":"Keyword 分析器 #  keyword 分析器根本不会对文本进行分词。相反，它将整个输入视为单个词元，不会将其拆分成单个的词项。keyword 分析器常用于包含电子邮件地址、网址或产品 ID 的字段，以及其他不需要进行分词的情况。\n相关指南（先读这些） #    文本分析基础  文本分析：识别词元  参考样例 #  以下命令创建一个名为 my_keyword_index 使用关键词分词器的索引：\nPUT /my_keyword_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;keyword\u0026#34; } } } } 配置自定义分词器 #  以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于关键词分词器：\nPUT /my_custom_keyword_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_keyword_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } 产生的词元 #  以下请求来检查使用该分词器生成的词元：\nPOST /my_custom_keyword_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_keyword_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Just one token\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Just one token\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 14, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"关键字分析器（Keyword）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/keyword-analyzer/"},{"category":null,"content":"使用 Easysearch UI 访问 #  Easysearch 1.15.0+ 版本内置了可视化管理界面（Easysearch UI），无需安装任何第三方工具，启动服务后直接在浏览器访问即可。部署成本为零、服务端零资源占用，界面默认支持中文。\n 💡 如果你需要多集群统一管理、告警通知等企业级功能，请参阅 使用 INFINI Console 管理。\n 访问方式 #  启动 Easysearch 后，在浏览器中打开：\nhttps://localhost:9200/_ui/ 输入用户名（admin）和初始化时终端输出的密码即可登录。\n 如果 Easysearch 部署在远程服务器上，将 localhost 替换为服务器 IP 地址即可。\n 集群概览：全局状态一目了然 #  登录后首页直接展示集群全景监控面板，核心指标全覆盖：\n 健康状态：醒目的色块（Green / Yellow / Red），进门就知道系统安危 资源仪表盘：CPU、内存、JVM 堆内存、磁盘使用率实时展示，性能瓶颈一眼便知 数据规模：文档数、索引数、分片数实时更新 拓扑视图：直观展示节点分布，多节点集群管理不再抽象  不像只能看到一行 status: yellow 的 JSON 字符串，Easysearch 的概览页用卡片式设计把关键信息全部呈现在首屏。\n节点与分片可视化 #  当集群状态出现异常时，需要深入细节排查：\n 节点管理：清晰展示每个节点的 IP、角色、存储占用和负载情况，对于多节点集群的负载均衡分析至关重要 分片视图：直观展示主分片和副本分片在不同节点上的分布，故障排查效率提升十倍不止 索引列表：查看每个索引的文档数、分片数、存储大小，支持创建/删除/别名管理  开发工具（Dev Tools）：开发者的瑞士军刀 #  内置的开发工具是开发者最常用的功能，提供了支持语法高亮和自动补全的 Web 编辑器：\n 点击左侧菜单 开发工具 在编辑器中输入请求：  GET /_cluster/health GET /megacorp/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;last_name\u0026#34;: \u0026#34;Smith\u0026#34; } } } 点击 ▶ 按钮或按 Ctrl+Enter 执行，右侧即时显示格式化的 JSON 结果  特色亮点：\n DSL + SQL 混合调试：上一行写 GET _search，下一行写 SELECT * FROM ...，无缝切换 语法提示：自动补全 API 路径和 JSON 字段名，减少拼写错误 实时渲染：结果格式化展示，开发效率较命令行提升明显   💡 提示：DevTools 使用缩写格式（省略主机和端口），和官方文档中的 API 示例格式一致，可以直接复制粘贴。\n 高级运维功能 #  除了基础监控，内置 UI 还提供了多项高级运维能力：\n 热点线程检测：CPU 突然飙高？点击\u0026quot;热点线程\u0026quot;，系统直接列出当前最消耗 CPU 的线程名称和操作类型，一抓一个准 索引生命周期管理（ILM）：图形化配置数据保留策略，设置\u0026quot;数据存多久自动删除\u0026quot;，无需手写复杂 JSON 快照备份：可视化配置备份策略，设置自动备份时间和存储位置 安全管理：支持索引/文档/字段粒度权限管控，可集成 LDAP/AD  配置说明 #  内置 UI 通过 http.ui.enabled 配置控制（默认开启）：\n# easysearch.yml http.ui.enabled: true # 默认为 true，设为 false 可关闭 访问路径为 /_ui/，自带路径遍历防护和 CSP 安全策略。生产环境中，如果不需要 UI，可以设为 false 关闭。\n下一步 #    使用 INFINI Console 管理：多集群统一管理、告警、数据探索  使用 Curl 访问 Easysearch：命令行方式  Java 客户端  Python 客户端  ","subcategory":null,"summary":"","tags":null,"title":"使用 Easysearch UI 访问","url":"/easysearch/main/docs/quick-start/connect/easysearch-ui/"},{"category":null,"content":"Painless 常用示例 #  本页收集了各种场景下常用的 Painless 脚本示例，可直接复制使用或按需修改。\n脚本字段（Script Fields） #  在搜索结果中返回动态计算的字段值。\n计算折扣价格 #  GET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script_fields\u0026#34;: { \u0026#34;discounted_price\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;price\u0026#39;].value * (1 - params.discount)\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;discount\u0026#34;: 0.2 } } } } } 拼接姓名字段 #  GET employees/_search { \u0026#34;script_fields\u0026#34;: { \u0026#34;full_name\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; def first = doc[\u0026#39;first_name.keyword\u0026#39;].size() \u0026gt; 0 ? doc[\u0026#39;first_name.keyword\u0026#39;].value : \u0026#39;\u0026#39;; def last = doc[\u0026#39;last_name.keyword\u0026#39;].size() \u0026gt; 0 ? doc[\u0026#39;last_name.keyword\u0026#39;].value : \u0026#39;\u0026#39;; return last + first; \u0026#34;\u0026#34;\u0026#34; } } } } 计算年龄 #  GET users/_search { \u0026#34;script_fields\u0026#34;: { \u0026#34;age\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (doc[\u0026#39;birthday\u0026#39;].size() == 0) return null; ZonedDateTime birthday = doc[\u0026#39;birthday\u0026#39;].value; ZonedDateTime now = ZonedDateTime.ofInstant( Instant.ofEpochMilli(System.currentTimeMillis()), ZoneId.of(\u0026#39;UTC\u0026#39;) ); return ChronoUnit.YEARS.between(birthday, now); \u0026#34;\u0026#34;\u0026#34; } } } } 文档更新 #  计数器递增 #  POST page_views/_update/homepage { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.view_count += params.count\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;count\u0026#34;: 1 } }, \u0026#34;upsert\u0026#34;: { \u0026#34;view_count\u0026#34;: 1 } } 向数组追加唯一元素 #  POST articles/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (ctx._source.tags == null) { ctx._source.tags = []; } for (tag in params.new_tags) { if (!ctx._source.tags.contains(tag)) { ctx._source.tags.add(tag); } } \u0026#34;\u0026#34;\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;new_tags\u0026#34;: [\u0026#34;featured\u0026#34;, \u0026#34;trending\u0026#34;] } } } 从数组移除元素 #  POST articles/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (ctx._source.tags != null) { ctx._source.tags.removeIf(tag -\u0026gt; tag == params.remove_tag); } \u0026#34;\u0026#34;\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;remove_tag\u0026#34;: \u0026#34;outdated\u0026#34; } } } 条件删除文档 #  POST orders/_update/123 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (ctx._source.status == \u0026#39;cancelled\u0026#39; \u0026amp;\u0026amp; ctx._source.refunded == true) { ctx.op = \u0026#39;delete\u0026#39;; } else { ctx.op = \u0026#39;noop\u0026#39;; } \u0026#34;\u0026#34;\u0026#34; } } 嵌套对象更新 #  POST users/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (ctx._source.profile == null) { ctx._source.profile = [:]; } ctx._source.profile.bio = params.bio; ctx._source.profile.updated_at = params.now; \u0026#34;\u0026#34;\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;bio\u0026#34;: \u0026#34;Easysearch 用户\u0026#34;, \u0026#34;now\u0026#34;: \u0026#34;2025-06-15T10:30:00Z\u0026#34; } } } 批量更新（Update By Query） #  为所有文档添加新字段 #  POST my_index/_update_by_query { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.version = params.version\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;version\u0026#34;: 2 } } } 根据条件批量分类 #  POST products/_update_by_query { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (ctx._source.price \u0026lt; 100) { ctx._source.tier = \u0026#39;budget\u0026#39;; } else if (ctx._source.price \u0026lt; 500) { ctx._source.tier = \u0026#39;mid-range\u0026#39;; } else { ctx._source.tier = \u0026#39;premium\u0026#39;; } \u0026#34;\u0026#34;\u0026#34; } } 批量格式转换 #  POST logs/_update_by_query { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (ctx._source.ip != null) { ctx._source.ip_parts = ctx._source.ip.splitOnToken(\u0026#39;.\u0026#39;); } \u0026#34;\u0026#34;\u0026#34; }, \u0026#34;query\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;ip\u0026#34; } } } 自定义评分 #  热度加权评分 #  GET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;技术\u0026#34; } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; double textScore = _score; double popularity = Math.log(2 + doc[\u0026#39;views\u0026#39;].value); double recency = 1.0; if (doc[\u0026#39;publish_date\u0026#39;].size() \u0026gt; 0) { long ageInDays = (System.currentTimeMillis() - doc[\u0026#39;publish_date\u0026#39;].value.toInstant().toEpochMilli()) / 86400000L; recency = 1.0 / (1 + ageInDays * 0.01); } return textScore * popularity * recency; \u0026#34;\u0026#34;\u0026#34; } } } } 地理位置加权 #  GET shops/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; double distance = doc[\u0026#39;location\u0026#39;].arcDistance(params.lat, params.lon) / 1000.0; return 1.0 / (1 + distance); \u0026#34;\u0026#34;\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;lat\u0026#34;: 39.9042, \u0026#34;lon\u0026#34;: 116.4074 } } } } } 自定义排序 #  按优先级 + 时间综合排序 #  GET tasks/_search { \u0026#34;sort\u0026#34;: [ { \u0026#34;_script\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; def priorityScore = 0; switch (doc[\u0026#39;priority.keyword\u0026#39;].value) { case \u0026#39;critical\u0026#39;: priorityScore = 1000; break; case \u0026#39;high\u0026#39;: priorityScore = 100; break; case \u0026#39;medium\u0026#39;: priorityScore = 10; break; default: priorityScore = 1; } long ageHours = (System.currentTimeMillis() - doc[\u0026#39;created_at\u0026#39;].value.toInstant().toEpochMilli()) / 3600000L; return priorityScore + ageHours; \u0026#34;\u0026#34;\u0026#34; }, \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } 摄入管道脚本 #  自动生成摘要 #  PUT _ingest/pipeline/auto_summary { \u0026#34;processors\u0026#34;: [ { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (ctx.content != null \u0026amp;\u0026amp; ctx.content.length() \u0026gt; 200) { ctx.summary = ctx.content.substring(0, 200) + \u0026#39;...\u0026#39;; } else { ctx.summary = ctx.content; } \u0026#34;\u0026#34;\u0026#34; } } ] } IP 地址分类 #  PUT _ingest/pipeline/ip_classify { \u0026#34;processors\u0026#34;: [ { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (ctx.client_ip != null) { if (ctx.client_ip.startsWith(\u0026#39;10.\u0026#39;) || ctx.client_ip.startsWith(\u0026#39;192.168.\u0026#39;)) { ctx.network_type = \u0026#39;internal\u0026#39;; } else { ctx.network_type = \u0026#39;external\u0026#39;; } } \u0026#34;\u0026#34;\u0026#34; } } ] } 时间戳处理 #  PUT _ingest/pipeline/timestamp_process { \u0026#34;processors\u0026#34;: [ { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (ctx.timestamp != null) { def sdf = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#39;T\u0026#39;HH:mm:ss\u0026#34;); def date = sdf.parse(ctx.timestamp); def cal = Calendar.getInstance(); cal.setTime(date); ctx.hour_of_day = cal.get(Calendar.HOUR_OF_DAY); ctx.day_of_week = cal.get(Calendar.DAY_OF_WEEK); ctx.is_weekend = (ctx.day_of_week == 1 || ctx.day_of_week == 7); } \u0026#34;\u0026#34;\u0026#34; } } ] } Reindex 中使用脚本 #  重建索引时重命名字段 #  POST _reindex { \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;old_index\u0026#34; }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;new_index\u0026#34; }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; ctx._source.user_name = ctx._source.remove(\u0026#39;username\u0026#39;); ctx._source.email_address = ctx._source.remove(\u0026#39;email\u0026#39;); if (ctx._source.status == \u0026#39;inactive\u0026#39;) { ctx.op = \u0026#39;noop\u0026#39;; } \u0026#34;\u0026#34;\u0026#34; } } 重建索引时转换数据格式 #  POST _reindex { \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;raw_logs\u0026#34; }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;processed_logs\u0026#34; }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; // 将字符串时间戳转为统一格式 if (ctx._source.log_time != null) { ctx._source.log_time = ctx._source.log_time.replace(\u0026#39; \u0026#39;, \u0026#39;T\u0026#39;) + \u0026#39;Z\u0026#39;; } // 提取日志级别 if (ctx._source.message != null) { def matcher = /^\\[(\\w+)\\]/.matcher(ctx._source.message); if (matcher.find()) { ctx._source.log_level = matcher.group(1); } } \u0026#34;\u0026#34;\u0026#34; } } 实用技巧 #  空值安全处理模式 #  // 方式 1：检查 size def val = doc[\u0026#39;field\u0026#39;].size() \u0026gt; 0 ? doc[\u0026#39;field\u0026#39;].value : defaultValue; // 方式 2：使用 containsKey（在 ctx._source 中） def val = ctx._source.containsKey('field') ? ctx._source.field : defaultValue;\n// 方式 3：Elvis 运算符（对 params） def val = params.value ?: defaultValue; Debug 输出（仅调试用） #\n // 在 Painless Execute API 中使用 Debug.explain 查看变量值 Debug.explain(doc[\u0026#39;field\u0026#39;].value);  提示：Debug.explain 会抛出一个包含变量类型和值的异常，仅在调试时使用，不要用于生产脚本。\n ","subcategory":null,"summary":"","tags":null,"title":"Painless 常用示例","url":"/easysearch/main/docs/features/scripting/painless-examples/"},{"category":null,"content":"Multi Get API #  一次请求获取多条文档。协调节点按分片拆分请求并行转发，比循环调用 GET 更高效。\n请求格式 #  GET /_mget POST /_mget GET /\u0026lt;index\u0026gt;/_mget POST /\u0026lt;index\u0026gt;/_mget 路径参数 #     参数 必需 说明     \u0026lt;index\u0026gt; 否 默认索引。在 URL 中指定后，请求体中的文档可省略 _index    查询参数 #     参数 类型 默认值 说明     routing string — 默认路由值   preference string — 查询偏好   realtime boolean true 实时读取   refresh boolean false 读取前是否刷新   stored_fields string — 默认返回的 stored 字段   _source string/boolean true 默认 _source 过滤   _source_includes string — 默认包含的字段   _source_excludes string — 默认排除的字段    请求体 #  标准格式 #  { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34; }, { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: [\u0026#34;title\u0026#34;] }, { \u0026#34;_index\u0026#34;: \u0026#34;blog\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;routing\u0026#34;: \u0026#34;user1\u0026#34; } ] } 每条文档可单独指定 _index、_source、routing、stored_fields。\n简写格式 #  当所有文档在同一索引时：\nGET /website/_mget { \u0026#34;ids\u0026#34;: [\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;] } 示例 #  GET /_mget { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34; }, { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: \u0026#34;title\u0026#34; } ] } 响应：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;...\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;My second blog entry\u0026#34; } } ] } 如果某条文档不存在，对应的 found 为 false，不影响其他文档的返回。\n 参考导航 #     需求 参见     获取单条文档  Get API   批量写入操作  Bulk API   分布式检索过程  分布式写入过程    ","subcategory":null,"summary":"","tags":null,"title":"Multi Get API","url":"/easysearch/main/docs/features/document-operations/multi-get-api/"},{"category":null,"content":"Lucene 底层原理 #  Easysearch 的搜索能力建立在 Apache Lucene 之上。理解 Lucene 的核心概念，有助于更好地使用 Easysearch 和进行性能调优。\nLucene 与 Easysearch 的关系 #  用户请求 → Easysearch (分布式协调、REST API、集群管理) ↓ 每个分片 = 一个 Lucene Index ↓ Lucene (倒排索引、评分、段合并)  Easysearch 负责：分布式路由、副本复制、集群管理、REST API、安全 Lucene 负责：文本分析、倒排索引构建、查询执行、评分计算  每个 Easysearch 分片（shard） 对应一个完整的 Lucene Index。\n 术语对齐：Lucene 的\u0026quot;索引\u0026quot;在 Easysearch 里更接近\u0026quot;分片\u0026quot;；而 Easysearch 的\u0026quot;索引\u0026quot;是多个分片的集合。\n 倒排索引：全文检索的核心 #  要理解 Easysearch 的搜索行为，必须先回答一个基础问题：文本是怎么变成\u0026quot;可搜索\u0026quot;的？\n关系型数据库通常把一个字段当作一个整体来索引；而全文检索需要把一个字段里的每个\u0026quot;词\u0026quot;都变成可检索的索引项。这就需要倒排索引（Inverted Index）——一个天然支持\u0026quot;一个字段对应很多词项\u0026quot;的数据结构。\n正排 vs 倒排 #  正排索引（文档 → 词项）：\n   文档 ID 内容     1 \u0026ldquo;搜索引擎 技术\u0026rdquo;   2 \u0026ldquo;分布式 搜索引擎\u0026rdquo;    倒排索引（词项 → 文档）：\n   词项 文档列表     搜索引擎 [1, 2]   技术 [1]   分布式 [2]    搜索 \u0026ldquo;搜索引擎\u0026rdquo; 时，直接查倒排索引，O(1) 定位到文档 1 和 2，无需逐条扫描。\n倒排索引的组成 #  一个真实的倒排索引不仅包含\u0026quot;出现在哪些文档\u0026quot;，还会包含丰富的统计信息：\n   组件 作用 对应文件     Term Dictionary 词项的有序字典 .tim   Term Index 词典的前缀索引（FST，加速查找） .tip   Postings List 每个词项的文档列表、位置、频率 .doc, .pos, .pay   Stored Fields 原始文档内容（_source） .fdt, .fdx   Doc Values 列式存储，用于排序和聚合 .dvd, .dvm   Norms 字段长度归一化因子（评分用） .nvd, .nvm    在 Easysearch 中，每个被索引的字段，都有自己的倒排索引——这是一个关键点。\n示例：倒排索引匹配 #  假设有两个文档：\n The quick brown fox jumped over the lazy dog Quick brown foxes leap over lazy dogs in summer  构建倒排索引后（简化版）：\nTerm Doc_1 Doc_2 ------------------------- Quick | | X The | X | brown | X | X dog | X | dogs | | X fox | X | foxes | | X jumped | X | lazy | X | X leap | | X over | X | X quick | X | summer | | X 搜索 quick brown，两个文档都匹配，Doc_1 命中 2 个词条更相关。\n但这个原始索引存在问题：\n Quick 和 quick 是不同的词条——用户可能认为它们相同 fox 和 foxes、dog 和 dogs 有相同词根但被视为不同词条 jumped 和 leap 是同义词但无法关联  这就是为什么在索引之前需要对文本进行分析（Analysis）：小写化、词干提取、同义词扩展等，使词条被规范为标准形式。分析后的索引会变成：\nTerm Doc_1 Doc_2 ------------------------- brown | X | X dog | X | X fox | X | X jump | X | X lazy | X | X over | X | X quick | X | X summer | | X 现在搜索 Quick fox 就能匹配到两个文档了。这个过程由分析器自动处理，详见 映射与分析入门。\n段（Segment）与不可变性 #  不可变性：为什么倒排索引\u0026quot;写入后不改\u0026quot; #  倒排索引一旦写入磁盘，就被设计为不可变（immutable）：不会原地修改。这带来了几个重要好处：\n 无需锁：不存在\u0026quot;就地更新\u0026quot;，并发读写更容易 更友好的缓存：文件进入操作系统缓存后可以长期有效 缓存更耐用：过滤等缓存不会因为\u0026quot;索引被修改\u0026quot;而频繁失效 更好的压缩：一次写入较大的不可变结构，压缩比更高、IO 更少  不可变性的代价也很直观：你无法直接把一个新文档\u0026quot;插入到旧索引里\u0026quot;让它立刻可搜。\n动态更新：用\u0026quot;段\u0026quot;追加，而不是改写 #  Lucene 引入了\u0026quot;按段搜索\u0026quot;的概念来解决这个问题：\n 一个 段（segment） 本身就是一个完整的倒排索引 一个 Lucene 索引是多个段的集合，并通过 提交点（commit point） 记录\u0026quot;当前有哪些段是已知且可用的\u0026quot;  写入的概念流程：\n 新文档先进入内存缓冲（in-memory buffer） 缓冲不时被 refresh：写入一个新段到文件系统缓存 新段被打开，从而使新文档\u0026quot;可见/可搜\u0026quot; 定期 flush：将段 fsync 到磁盘，写入新的提交点  查询时会按顺序查询所有已知段，并把段级结果合并成全局结果。这样就实现了\u0026quot;动态更新\u0026quot;，而不需要重建整个大索引。\n段的生命周期 #  写入文档 → 内存缓冲区 ↓ refresh (默认 1 秒) 新段（不可变）→ 可被搜索 ↓ flush / translog 持久化到磁盘 ↓ merge 多个小段合并为大段 关键特性：\n 不可变性：段一旦写入就不会被修改。更新 = 标记旧文档删除 + 写入新文档 近实时：refresh 操作创建新段，新文档变为可搜索（默认 1 秒延迟） 段合并：后台自动将小段合并为大段，减少段数量，回收已删除文档的空间   详见 写入与存储机制。\n 段数量与性能 #     段数量 影响     太多 每次搜索需遍历所有段，查询变慢   太少 合并时消耗大量 IO 和 CPU   适中 由 Easysearch merge policy 自动控制    Easysearch 默认使用 TieredMergePolicy（分层合并策略），其核心参数：\n   参数 默认值 说明     floor_segment 2 MB 小于此大小的段会被视为等大，鼓励合并小段   max_merge_at_once 10 单次合并操作最多合并多少个段   max_merged_segment 5 GB 合并后的段最大不超过此值（太大的段不参与常规合并）   segments_per_tier 10.0 每层允许的段数量，控制合并的激进程度   deletes_pct_allowed 33% 段中已删除文档的比例超过此值时，会优先被选中合并    可通过以下 API 查看段信息：\nGET /my-index/_segments 删除与更新：不是原地修改 #  段不可变，因此：\n 删除并不会从旧段里抹掉文档，而是在 .del（live docs）文件里标记删除 被标记删除的文档仍可能被查询匹配到，但会在最终结果返回前被过滤掉 更新也是同样语义：旧版本文档被标记删除，新版本作为新文档写入新段  段文件格式 #  每个段包含一组不可变的文件。以段名 _0 为例：\n   文件扩展名 用途 说明     .si Segment Info 段的元信息（版本、文档数等）   .fnm Field Names 字段名称与属性   .fdx / .fdt Stored Fields _source 的索引和数据   .tim / .tip Term Dictionary \u0026amp; Index 词项字典与 FST 前缀索引   .doc / .pos / .pay Postings 文档频率、位置、附加数据   .dvd / .dvm Doc Values 列式存储（排序/聚合用）   .nvd / .nvm Norms 字段长度归一化因子   .liv Live Docs 删除标记位图   .cfs / .cfe Compound File 合并小文件减少文件句柄    索引级别的文件：\n   文件 用途     segments_N Commit Point，记录当前活跃的段列表   write.lock 写入锁，防止并发写入    倒排索引文件的查找路径 #  搜索 \u0026#34;Easysearch\u0026#34; 的查找路径： .tip (FST前缀索引) ↓ 快速定位到 \u0026quot;E\u0026quot; 开头的块 .tim (Term Dictionary) ↓ 在块内二分查找 \u0026quot;Easysearch\u0026quot; .doc (Postings - 频率) ↓ 获取包含该词的文档列表 [doc1, doc5, doc12] .pos (Postings - 位置) ↓ 获取词在每个文档中的位置（短语查询需要） **FST（Finite State Transducer）**是 .tip 文件的核心数据结构，它是一种高度压缩的前缀树，将词项映射到 .tim 文件中的偏移量。FST 常驻内存，是快速查找的关键。\n存储字段与 Doc Values #   Stored Fields（.fdt / .fdx）：存储文档的原始 JSON（即 _source），使用 LZ4 或 DEFLATE 压缩 Doc Values（.dvd / .dvm）：列式存储，用于排序和聚合  行式 (_source): doc1={name:\u0026#34;A\u0026#34;, age:25} doc2={name:\u0026#34;B\u0026#34;, age:30} 列式 (doc values): name: [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;] age: [25, 30] Doc Values 的关键信息：\n keyword、数值、日期、布尔字段默认启用 Doc Values text 字段不支持 Doc Values（使用 fielddata，不推荐） 不需要排序/聚合的字段可关闭以节省空间：\u0026quot;doc_values\u0026quot;: false  压缩控制 #  PUT /my-index { \u0026#34;settings\u0026#34;: { \u0026#34;index.codec\u0026#34;: \u0026#34;best_compression\u0026#34; } } 使用 DEFLATE 代替默认 LZ4，压缩率提升约 20%，但写入和读取速度会略慢。\n磁盘空间估算 #  各文件类型的典型占比（仅供参考）：\n   文件类型 占比 说明     存储字段 (.fdt) 40~60% _source 通常是最大的部分   倒排索引 (.tim/.doc/.pos) 15~30% 取决于字段数量和文本长度   Doc Values (.dvd) 5~15% 取决于可排序/聚合字段数   其他 (norms/meta等) 5~10%     评分模型 #  Lucene 使用 BM25 作为默认评分算法（从 Lucene 6 开始，之前是 TF-IDF）。\nBM25 核心公式 #  $$score(q,d) = \\sum_{t \\in q} IDF(t) \\cdot \\frac{tf \\cdot (k_1 + 1)}{tf + k_1 \\cdot (1 - b + b \\cdot \\frac{dl}{avgdl})}$$\n默认参数：$k_1 = 1.2$，$b = 0.75$，discount_overlaps = true。\nBM25 核心因子 #     因子 含义     TF (Term Frequency) 词项在文档中出现的频率，越高越相关   IDF (Inverse Document Frequency) 词项在所有文档中的稀有程度，越稀有越重要   Field Length 字段越短，单个词项的权重越高    直觉理解：\n 一个词在文档中出现越多 → 越相关（TF） 一个词在整个索引中越罕见 → 越有区分度（IDF） 文档越短 → 单个匹配词的密度越高（Field Length）  查看评分解释 #  GET /my-index/_search { \u0026#34;explain\u0026#34;: true, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;搜索引擎\u0026#34; } } }  详见 相关性与排序。\n 调优要点 #     优化点 说明     refresh_interval 增大刷新间隔减少段创建频率   merge policy 调整合并策略平衡写入与查询   doc values 不需要排序/聚合的字段可关闭 \u0026quot;doc_values\u0026quot;: false   _source 大字段考虑 _source.excludes 减少存储   norms 不需要评分的字段可关闭 \u0026quot;norms\u0026quot;: false   位置信息 不需要短语查询时设置 \u0026quot;index_options\u0026quot;: \u0026quot;docs\u0026quot;   压缩 使用 best_compression 提升压缩率    小结 #   Lucene 是 Easysearch 的底层搜索引擎，每个分片对应一个 Lucene Index 倒排索引把\u0026quot;词项 → 文档集合\u0026quot;作为核心结构，每个字段各自维护倒排索引 不可变性带来并发、缓存与压缩的优势，但意味着不能原地更新 动态更新依赖段（segment）：追加新段 + 更新提交点；查询合并段级结果 FST 是 Lucene 快速查找词项的关键数据结构 BM25 评分基于词频、逆文档频率和字段长度三个因子  下一步可以继续阅读：\n  写入与存储机制：refresh、flush、merge 详解  映射与分析入门：分析器与字段类型  相关性与排序：TF/IDF、BM25 评分详解  ","subcategory":null,"summary":"","tags":null,"title":"Lucene 底层原理","url":"/easysearch/main/docs/fundamentals/lucene-internals/"},{"category":null,"content":"CSV 处理器 #  csv 处理器用于解析 CSV 文件并将它们作为单独的字段存储在文档中。该处理器会忽略空字段。\n语法 #  以下是为 csv 处理器提供的语法：\n{ \u0026#34;csv\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_name\u0026#34;, \u0026#34;target_fields\u0026#34;: [\u0026#34;field1, field2, ...\u0026#34;] } } 配置参数 #  下表列出了 csv 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要转换的数据的字段名称。支持模板使用。   target_fields 必填 字段名称，用于存储解析后的数据。   description 可选 处理器的一个简要描述。   empty_value 可选 表示可选参数，这些参数不是必需的或不适用的。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。如果设置为 true ，则处理器在字段不存在或为 null 时不会修改文档。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   quote 可选 用于在 CSV 数据中引用字段的字符。默认为 \u0026quot; 。   separator 可选 用于在 CSV 数据中分隔字段的分隔符。默认为 , 。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。   trim 可选 如果设置为 true ，处理器将删除文本开头和结尾的空白字符。默认是 false 。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 csv-processor 的管道，将 resource_usage 拆分为三个新字段，分别命名为 cpu_usage 、 memory_usage 和 disk_usage ：\nPUT _ingest/pipeline/csv-processor { \u0026#34;description\u0026#34;: \u0026#34;Split resource usage into individual fields\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;csv\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;resource_usage\u0026#34;, \u0026#34;target_fields\u0026#34;: [\u0026#34;cpu_usage\u0026#34;, \u0026#34;memory_usage\u0026#34;, \u0026#34;disk_usage\u0026#34;], \u0026#34;separator\u0026#34;: \u0026#34;,\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/csv-processor/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;resource_usage\u0026#34;: \u0026#34;25,4096,10\u0026#34;, \u0026#34;memory_usage\u0026#34;: \u0026#34;4096\u0026#34;, \u0026#34;disk_usage\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;cpu_usage\u0026#34;: \u0026#34;25\u0026#34; } } ] } 返回内容确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;memory_usage\u0026#34;: \u0026#34;4096\u0026#34;, \u0026#34;disk_usage\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;resource_usage\u0026#34;: \u0026#34;25,4096,10\u0026#34;, \u0026#34;cpu_usage\u0026#34;: \u0026#34;25\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-08-22T16:40:45.024796379Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=csv-processor { \u0026#34;resource_usage\u0026#34;: \u0026#34;25,4096,10\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"CSV 处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/csv/"},{"category":null,"content":"配置 TLS 证书 #  Easysearch 可以通过启用 TLS 传输加密来保护您数据的网络传输安全。 TLS 的相关设置在配置文件 easysearch.yml 中进行，主要包括两个部分：传输层和 HTTP 层。\n 传输层（Transport）：节点之间的通信。传输层 TLS 是必需的（默认启用）。 HTTP 层：客户端与节点之间的通信。HTTP 层 TLS 是可选的（默认关闭）。  默认的配置如下：\nsecurity.ssl.transport.cert_file: instance.crt security.ssl.transport.key_file: instance.key security.ssl.transport.ca_file: ca.crt security.ssl.http.enabled: true security.ssl.http.cert_file: instance.crt security.ssl.http.key_file: instance.key security.ssl.http.ca_file: ca.crt 一键生成证书 #  启用 TLS 需要设置证书才能工作，通过执行命令 ./bin/initialize.sh 可以一键生成 TLS 证书，如下：\n➨ ./bin/initialize.sh Generating RSA private key, 2048 bit long modulus .......................+++ ...............................+++ e is 65537 (0x10001) Generating RSA private key, 2048 bit long modulus .......................................................................................................................................................................................+++ ......................................................+++ e is 65537 (0x10001) Signature ok subject=/C=IN/ST=FI/L=NI/O=ORG/OU=UNIT/CN=infini.cloud Getting CA Private Key DNS:infini.cloud, DNS:*.infini.cloud ➨ ls config ca.crt easysearch.yml instance.key log4j2.properties ca.key easysearch.yml.example jvm.options security easysearch.keystore instance.crt jvm.options.d TLS 启用开关 #     名称 描述 默认值     security.ssl.transport.enabled 是否启用传输层 TLS。 true   security.ssl.http.enabled 是否启用 HTTP 层 TLS。 false    X.509 PEM 证书和 PKCS#8 密钥 #  下面是 Easysearch 使用 PEM 格式证书和 PKCS#8 私钥的详细参数说明。所有证书和密钥文件必须位于 config 目录下，使用相对路径指定。\n传输层 TLS（PEM） #     名称 描述     security.ssl.transport.key_file 节点私钥文件（PKCS#8，PEM 格式）的路径。必填。   security.ssl.transport.key_secret 私钥文件的密码。如果密钥没有密码保护，可忽略。选填。   security.ssl.transport.cert_file 节点证书文件（PEM 格式）的路径。必填。   security.ssl.transport.ca_file 根证书（CA）文件（PEM 格式）的路径。必填。    HTTP 层 TLS（PEM） #     名称 描述     security.ssl.http.key_file 节点私钥文件（PKCS#8，PEM 格式）的路径。必填。   security.ssl.http.key_secret 私钥文件的密码。如果密钥没有密码保护，可忽略。选填。   security.ssl.http.cert_file 节点证书文件（PEM 格式）的路径。必填。   security.ssl.http.ca_file 根证书（CA）文件（PEM 格式）的路径。必填。    JKS/PKCS12 Keystore 和 Truststore #  除了 PEM 格式，Easysearch 还支持使用 JKS 或 PKCS12 格式的 Keystore 和 Truststore 来存储证书和密钥。所有 Keystore/Truststore 文件必须位于 config 目录下，使用相对路径指定。\n 注意： PEM 格式和 Keystore 格式是互斥的，每一层只能选择其中一种方式配置证书。\n 传输层 TLS（Keystore） #     名称 描述 默认值     security.ssl.transport.keystore_filepath Keystore 文件的路径。必填（若使用 Keystore 方式）。 -   security.ssl.transport.keystore_password Keystore 的密码。 changeit   security.ssl.transport.keystore_keypassword Keystore 中私钥的密码（如果与 Keystore 密码不同）。 与 keystore_password 相同   security.ssl.transport.keystore_alias Keystore 中要使用的证书别名。如果 Keystore 包含多个证书条目，可指定别名。 使用第一个条目   security.ssl.transport.keystore_type Keystore 的类型。 JKS   security.ssl.transport.truststore_filepath Truststore 文件的路径。必填（若使用 Keystore 方式）。 -   security.ssl.transport.truststore_password Truststore 的密码。 changeit   security.ssl.transport.truststore_alias Truststore 中要使用的 CA 证书别名。 使用第一个条目   security.ssl.transport.truststore_type Truststore 的类型。 JKS    HTTP 层 TLS（Keystore） #     名称 描述 默认值     security.ssl.http.keystore_filepath Keystore 文件的路径。必填（若使用 Keystore 方式）。 -   security.ssl.http.keystore_password Keystore 的密码。 changeit   security.ssl.http.keystore_keypassword Keystore 中私钥的密码（如果与 Keystore 密码不同）。 与 keystore_password 相同   security.ssl.http.keystore_alias Keystore 中要使用的证书别名。 使用第一个条目   security.ssl.http.keystore_type Keystore 的类型。 JKS   security.ssl.http.truststore_filepath Truststore 文件的路径。当 clientauth_mode 为 REQUIRE 时必填。 -   security.ssl.http.truststore_password Truststore 的密码。 changeit   security.ssl.http.truststore_alias Truststore 中要使用的 CA 证书别名。 使用第一个条目   security.ssl.http.truststore_type Truststore 的类型。 JKS    Keystore 配置示例 #  # 传输层使用 JKS Keystore security.ssl.transport.keystore_filepath: node-keystore.jks security.ssl.transport.keystore_password: mypassword security.ssl.transport.truststore_filepath: node-truststore.jks security.ssl.transport.truststore_password: mypassword # HTTP 层使用 PKCS12 Keystore security.ssl.http.enabled: true security.ssl.http.keystore_filepath: node-keystore.p12 security.ssl.http.keystore_password: mypassword security.ssl.http.keystore_type: PKCS12 security.ssl.http.truststore_filepath: node-truststore.p12 security.ssl.http.truststore_password: mypassword security.ssl.http.truststore_type: PKCS12 扩展密钥用途（Extended Key Usage） #\n 当启用扩展密钥用途时，传输层可以为服务端和客户端使用不同的证书。这在需要严格区分节点间入站和出站连接身份的场景中非常有用。\n   名称 描述 默认值     security.ssl.transport.extended_key_usage_enabled 是否启用扩展密钥用途，启用后传输层将分别使用独立的服务端和客户端证书。 false    服务端/客户端独立 PEM 证书 #  当启用扩展密钥用途时，可以为传输层的服务端和客户端分别指定 PEM 证书文件：\n   名称 描述     security.ssl.transport.server.key_file 传输层服务端私钥文件路径。   security.ssl.transport.server.key_secret 服务端私钥密码。选填。   security.ssl.transport.server.cert_file 传输层服务端证书文件路径。   security.ssl.transport.server.ca_file 服务端 CA 证书文件路径。   security.ssl.transport.client.key_file 传输层客户端私钥文件路径。   security.ssl.transport.client.key_secret 客户端私钥密码。选填。   security.ssl.transport.client.cert_file 传输层客户端证书文件路径。   security.ssl.transport.client.ca_file 客户端 CA 证书文件路径。    服务端/客户端独立 Keystore 别名 #  当启用扩展密钥用途时，也可以通过 Keystore 别名来区分服务端和客户端证书。此时需要同时指定所有四个别名：\n   名称 描述     security.ssl.transport.server.keystore_alias Keystore 中服务端证书的别名。必填。   security.ssl.transport.server.keystore_keypassword 服务端私钥的密码。默认使用 keystore_password。   security.ssl.transport.client.keystore_alias Keystore 中客户端证书的别名。必填。   security.ssl.transport.client.keystore_keypassword 客户端私钥的密码。默认使用 keystore_password。   security.ssl.transport.server.truststore_alias Truststore 中服务端 CA 证书的别名。必填。   security.ssl.transport.client.truststore_alias Truststore 中客户端 CA 证书的别名。必填。    扩展密钥用途配置示例 #  # 使用 PEM 格式的独立服务端/客户端证书 security.ssl.transport.extended_key_usage_enabled: true security.ssl.transport.server.key_file: transport-server.key security.ssl.transport.server.cert_file: transport-server.crt security.ssl.transport.server.ca_file: ca.crt security.ssl.transport.client.key_file: transport-client.key security.ssl.transport.client.cert_file: transport-client.crt security.ssl.transport.client.ca_file: ca.crt 配置节点证书 #  安全模块需要识别集群节点间的请求（即节点之间的通信）。配置节点证书的最简单方法是在 easysearch.yml 中列出这些证书的可分辨名称（DN）。所有 DN 都必须包含在所有节点上的 easysearch.yml 中。安全模块支持通配符和正则表达式：\nsecurity.nodes_dn: - \u0026#34;CN=node.other.com,OU=SSL,O=Test,L=Test,C=DE\u0026#34; - \u0026#34;CN=*.example.com,OU=SSL,O=Test,L=Test,C=DE\u0026#34; - \u0026#34;CN=elk-devcluster*\u0026#34; - \u0026#34;/CN=.*regex/\u0026#34; 如果您的节点证书在 SAN 部分中具有 OID 标识符，则可以省略此配置。\n相关设置：\n   名称 描述 默认值     security.nodes_dn_dynamic_config_enabled 是否允许通过 REST API 动态更新 nodes_dn 配置。 false   security.cert.oid 自定义 OID 用于在节点证书中标识节点身份。如果节点证书的 SAN 中包含此 OID 的值，则无需配置 security.nodes_dn。 1.2.3.4.5.5    配置管理证书 #  管理员证书是具有执行管理任务的提升权限的常规客户端证书。您需要管理员证书才能使用 HTTP API 更改安全配置。管理员证书在 easysearch.yml 中通过声明其 DN 进行配置：\nsecurity.authcz.admin_dn: - CN=admin,OU=SSL,O=Test,L=Test,C=DE 出于安全原因，您不能在此处使用通配符或正则表达式。\n证书身份模拟（Impersonation） #  安全模块支持基于证书 DN 的身份模拟功能，允许经过认证的用户或传输请求以另一个用户的身份执行操作。\n   名称 描述     security.authcz.impersonation_dn 映射传输层证书 DN 到允许模拟的用户列表。   security.authcz.rest_impersonation_user 映射 REST 层用户到允许模拟的用户列表。    模拟配置示例 #  security.authcz.impersonation_dn: \u0026#34;CN=spock,OU=client,O=client,L=Test,C=DE\u0026#34;: - \u0026#34;worf\u0026#34; - \u0026#34;naomi\u0026#34; security.authcz.rest_impersonation_user: \u0026quot;admin\u0026quot;: - \u0026quot;monitor_user\u0026quot; - \u0026quot;report_user\u0026quot; 主机名验证和 DNS 查找 #\n 除了根据根 CA 和/或中间 CA 验证 TLS 证书外，安全插件还可以对传输层应用其他检查。\n禁用 skip_domain_verify 后，安全模块将验证通信伙伴的主机名是否与证书中的主机名匹配。\n主机名取自证书的 subject 或 SAN 条目。例如，如果节点的主机名为 node-0.example.com，则 TLS 证书中的主机名也必须设置为 node-0.example.com。否则，将引发错误：\n[ERROR][c.a.o.s.s.t.SecuritySSLNettyTransport] [WX6omJY] SSL Problem No name matching \u0026lt;hostname\u0026gt; found [ERROR][c.a.o.s.s.t.SecuritySSLNettyTransport] [WX6omJY] SSL Problem Received fatal alert: certificate_unknown 此外，启用 resolve_hostname 后，安全模块会根据您的 DNS 解析（经过验证的）主机名。如果主机名未解析，则会引发错误。\n   名称 描述 默认值     security.ssl.transport.skip_domain_verify 是否跳过验证传输层上的主机名。 true   security.ssl.transport.resolve_hostname 是否在传输层上根据 DNS 解析主机名。仅在同时启用主机名验证时（skip_domain_verify 为 false）才有效。 true    客户端认证 #  启用 TLS 客户端身份验证后，HTTP 客户端可以将 TLS 证书与 HTTP 请求一起发送，以向安全模块提供身份信息。TLS 客户端认证主要有以下使用场景：\n 在使用 HTTP 管理 API 时提供管理员证书。 根据客户端证书配置角色和权限。 为 INFINI Console、Logstash 或 Beats 等工具提供身份信息。  TLS 客户端认证有三种模式：\n   模式 描述     NONE 安全模块不接受 TLS 客户端证书。如果发送了一个，则将其丢弃。   OPTIONAL 安全模块接受 TLS 客户端证书（如果已发送），但不强制要求。默认值。   REQUIRE 安全模块仅在发送有效的客户端 TLS 证书时接受 HTTP 请求。    对于 HTTP 管理 API，客户端身份验证模式至少必须是 OPTIONAL。\n   名称 描述 默认值     security.ssl.http.clientauth_mode 要使用的 TLS 客户端身份验证模式。可选值：NONE、OPTIONAL、REQUIRE。 OPTIONAL    样例 #  curl -k --cert config/instance.crt --key config/instance.key \\\\ -XDELETE \u0026#39;https://localhost:9200/.infini-*/\u0026#39; -u admin:xxxxxxxxxxxx 证书吊销列表（CRL） #  Easysearch 支持通过 CRL（证书吊销列表）和 OCSP（在线证书状态协议）来验证 HTTP 层客户端证书的有效性。\n   名称 描述 默认值     security.ssl.http.crl.file_path CRL 文件的路径（PEM 格式），文件必须位于 config 目录下。 -   security.ssl.http.crl.validate 是否启用 CRL 验证。 false   security.ssl.http.crl.prefer_crlfile_over_ocsp 优先使用本地 CRL 文件而非 OCSP 进行证书吊销检查。 false   security.ssl.http.crl.check_only_end_entities 仅验证终端实体证书，不验证 CA 证书链中的中间证书。 true   security.ssl.http.crl.disable_ocsp 禁用 OCSP 检查。 false   security.ssl.http.crl.disable_crldp 禁用证书中 CRL 分发点（CRLDP）的自动下载。 false   security.ssl.http.crl.validation_date 用于验证的日期（主要用于测试目的）。格式为 ISO 8601 日期字符串。 当前日期    加密算法和协议 #  您可以限制 HTTP 层和传输层允许的加密算法和 TLS 协议。例如，您可以只允许强密码算法，并将 TLS 版本限制为最新版本。\n如果未配置此设置，密码和 TLS 版本将在通信双方之间自动协商，这在某些情况下可能会导致使用较弱的密码套件。\n   名称 描述     security.ssl.http.enabled_ciphers 数组，HTTP 层启用的 TLS 密码套件。仅支持 Java 格式。   security.ssl.http.enabled_protocols 数组，HTTP 层启用的 TLS 协议。仅支持 Java 格式。   security.ssl.transport.enabled_ciphers 数组，传输层启用的 TLS 密码套件。仅支持 Java 格式。   security.ssl.transport.enabled_protocols 数组，传输层启用的 TLS 协议。仅支持 Java 格式。    默认启用的协议为 TLSv1.3、TLSv1.2 和 TLSv1.1。不安全的 TLSv1 协议默认禁用。\n样例 #  security.ssl.http.enabled_ciphers: - \u0026#34;TLS_AES_128_GCM_SHA256\u0026#34; - \u0026#34;TLS_AES_256_GCM_SHA384\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\u0026#34; security.ssl.http.enabled_protocols: - \u0026#34;TLSv1.2\u0026#34; - \u0026#34;TLSv1.3\u0026#34; 如果您需要使用 TLSv1 并接受潜在的安全风险，您仍然可以手动启用它：\nsecurity.ssl.http.enabled_protocols: - \u0026#34;TLSv1\u0026#34; - \u0026#34;TLSv1.1\u0026#34; - \u0026#34;TLSv1.2\u0026#34; 高级设置 #  OpenSSL 支持 #  Easysearch 支持使用 OpenSSL 作为 TLS 引擎来替代默认的 JDK SSL 实现。OpenSSL 通常可以提供更好的性能。\n 注意： OpenSSL 支持需要 Java 11 或更低版本，且必须设置系统属性 es.unsafe.use_netty_default_allocator=true。Java 12 及以上版本不支持 OpenSSL，将自动使用 JDK SSL。\n    名称 描述 默认值     security.ssl.http.enable_openssl_if_available 如果 OpenSSL 可用，HTTP 层是否使用 OpenSSL。 true   security.ssl.transport.enable_openssl_if_available 如果 OpenSSL 可用，传输层是否使用 OpenSSL。 true    证书热加载 #     名称 描述 默认值     security.ssl.cert_reload.enabled 是否启用 TLS 证书的热加载。启用后，当证书文件发生变化时，节点将自动加载新证书而无需重启。 false    TLS 客户端重协商 #     名称 描述 默认值     security.ssl.allow_client_initiated_renegotiation 是否允许客户端发起的 TLS 重协商。允许此设置可能会导致 DoS 攻击风险，建议保持默认禁用。 false    传输层主体提取器 #     名称 描述 默认值     security.ssl.transport.principal_extractor_class 自定义主体提取器类的完全限定类名。用于自定义如何从传输层 TLS 证书中提取用户身份信息。 -    SSL-Only 模式 #     名称 描述 默认值     security.ssl_only 仅启用 SSL/TLS 加密，不加载安全插件的其余功能（认证、授权等）。适用于仅需要传输加密而不需要安全控制的场景。 false    双模式（Dual Mode） #     名称 描述 默认值     security.ssl.dual_mode.enabled 启用 SSL 双模式。当启用时，节点同时接受 SSL 和非 SSL 的传输层连接，便于集群从非加密向加密状态的平滑迁移。 false    完整配置示例 #  以下是一个使用 PEM 格式证书的完整配置示例：\n# ===== 传输层 TLS ===== security.ssl.transport.enabled: true security.ssl.transport.key_file: instance.key security.ssl.transport.cert_file: instance.crt security.ssl.transport.ca_file: ca.crt security.ssl.transport.skip_domain_verify: true # ===== HTTP 层 TLS ===== security.ssl.http.enabled: true security.ssl.http.key_file: instance.key security.ssl.http.cert_file: instance.crt security.ssl.http.ca_file: ca.crt security.ssl.http.clientauth_mode: OPTIONAL\n# ===== 节点和管理员 DN ===== security.nodes_dn:\n \u0026quot;CN=*.example.com,OU=SSL,O=Test,L=Test,C=DE\u0026quot; security.authcz.admin_dn: \u0026quot;CN=admin,OU=SSL,O=Test,L=Test,C=DE\u0026quot;  # ===== 加密算法限制 ===== security.ssl.http.enabled_protocols:\n \u0026quot;TLSv1.2\u0026quot; \u0026quot;TLSv1.3\u0026quot; security.ssl.transport.enabled_protocols: \u0026quot;TLSv1.2\u0026quot; \u0026quot;TLSv1.3\u0026quot;  # ===== 证书热加载 ===== security.ssl.cert_reload.enabled: true \n","subcategory":null,"summary":"","tags":null,"title":"证书配置","url":"/easysearch/main/docs/operations/security/configuration/tls/"},{"category":null,"content":"Whitespace 分词器 #  whitespace 分词器会依据空白字符（如空格、制表符和换行符）对文本进行拆分。它将由空白字符分隔开的每个单词都视为一个词元，并且不会执行诸如转换为小写形式或去除标点符号等额外的规范化操作。\n相关指南（先读这些） #    文本分析：识别词元  文本分析基础  参考样例 #  以下示例请求创建一个名为 my_index 的新索引，并配置一个使用空格词元生成器的分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;whitespace_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;whitespace\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_whitespace_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;whitespace_tokenizer\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_whitespace_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_whitespace_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch is fast! Really fast.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;fast!\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 19, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;Really\u0026#34;, \u0026#34;start_offset\u0026#34;: 20, \u0026#34;end_offset\u0026#34;: 26, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;fast.\u0026#34;, \u0026#34;start_offset\u0026#34;: 27, \u0026#34;end_offset\u0026#34;: 32, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } 参数说明 #  空格词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     max_token_length 可选 整数 设置生成词元的最大长度。如果超过此长度，词元将按照 max_token_length 配置的长度拆分为多个词元。默认值为 255。    ","subcategory":null,"summary":"","tags":null,"title":"空白分词器（Whitespace）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/whitespace/"},{"category":null,"content":"在管道中访问数据 #  在摄取管道中，您可以使用 ctx 对象访问文档数据。此对象表示已处理的文档，并允许您读取、修改或丰富文档字段。管道处理器对文档的 _source 字段及其元数据字段都具有读写访问权限。\n访问文档字段 #  ctx 对象公开了所有文档字段。您可以直接使用点符号访问它们。\n示例：访问顶级字段 #  给定以下示例文档：\n{ \u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34; } 您可以通过以下方式访问 user ：\n\u0026#34;field\u0026#34;: \u0026#34;ctx.user\u0026#34; 示例：访问嵌套字段 #\n 给定以下示例文档：\n{ \u0026#34;user\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;alice\u0026#34; } } 您可以通过以下方式访问 user.name ：\n\u0026#34;field\u0026#34;: \u0026#34;ctx.user.name\u0026#34; 访问源中的字段 #\n 要访问文档中的字段 _source ，请通过字段名称进行引用：\n{ \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;production\u0026#34; } } 或者，您可以显式使用 _source ：\n{ \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;_source.environment\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;production\u0026#34; } } 访问元数据字段 #  您可以读取或写入以下元数据字段：\n _index _type _id _routing  示例：动态设置 _routing #  { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;_routing\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{region}}\u0026#34; } } 访问摄取元数据字段 #  _ingest.timestamp 字段表示摄取节点接收文档的时间。要持久化此时间戳，请使用 set 处理器：\n{ \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;received_at\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{_ingest.timestamp}}\u0026#34; } } 使用 ctx 在 Mustache 模板中 #  使用 Mustache 模板将字段值插入处理器设置中。使用三个大括号（ {{{ 和 }}} ）用于未转义的字段值。\n示例：合并源字段 #  以下处理器配置将 app 和 env 字段通过下划线（_）连接，并将结果存储在 log_label 字段中：\n{ \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;log_label\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{{app}}}_{{{env}}}\u0026#34; } } 示例：使用 set 处理器生成动态问候 #  如果文档的 user 字段设置为 alice ，请使用以下语法来生成结果 \u0026quot;greeting\u0026quot;: \u0026quot;Hello, alice!\u0026quot; ：\n{ \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;greeting\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Hello, {{{user}}}!\u0026#34; } } 动态字段名称 #  您可以使用字段的值作为新字段的名称：\n{ \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;{{service}}\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{code}}\u0026#34; } } 示例：根据状态路由到动态索引 #  以下处理器配置通过在 status 字段的值后附加 -events 来动态设置目标索引：\n{ \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{status}}-events\u0026#34; } } 使用 ctx 在 script 处理器中 #  使用 script 处理器进行高级转换。\n示例：如果另一个字段缺失则添加字段 #  以下处理器仅在文档中缺失字段时，添加具有值“none”的 error_message 字段：\n{ \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;if (ctx.error_message == null) { ctx.error_message = \u0026#39;none\u0026#39;; }\u0026#34; } } 示例：将一个字段的值复制到另一个字段 #  以下处理器将 timestamp 字段的值复制到一个名为 event_time 的新字段中：\n{ \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;ctx.event_time = ctx.timestamp;\u0026#34; } } 示例：完整的管道 #  以下示例定义了一个完整的摄取管道，该管道使用 source 字段设置标语，从 date 字段提取 year ，并在 received_at 字段记录文档的摄取时间戳：\nPUT _ingest/pipeline/example-pipeline { \u0026#34;description\u0026#34;: \u0026#34;Sets tags, log label, and defaults error message\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;tagline\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{{user.first}}} from {{{department}}}\u0026#34; } }, { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;ctx.year = ctx.date.substring(0, 4);\u0026#34; } }, { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;received_at\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{_ingest.timestamp}}\u0026#34; } } ] } 为了测试管道，请使用以下请求：\nPOST _ingest/pipeline/example-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;user\u0026#34;: { \u0026#34;first\u0026#34;: \u0026#34;Liam\u0026#34; }, \u0026#34;department\u0026#34;: \u0026#34;Engineering\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2024-12-03T14:05:00Z\u0026#34; } } ] } 返回显示了处理后的富文档，包括新添加的 tagline ，提取的 year ，以及由摄取管道生成的 received_at 时间戳：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;user\u0026#34;: { \u0026#34;first\u0026#34;: \u0026#34;Liam\u0026#34; }, \u0026#34;department\u0026#34;: \u0026#34;Engineering\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2024-12-03T14:05:00Z\u0026#34;, \u0026#34;tagline\u0026#34;: \u0026#34;Liam from Engineering\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;2024\u0026#34;, \u0026#34;received_at\u0026#34;: \u0026#34;2025-04-14T18:40:00.000Z\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-14T18:40:00.000Z\u0026#34; } } } ] } ","subcategory":null,"summary":"","tags":null,"title":"在管道中访问数据","url":"/easysearch/main/docs/features/ingest-pipelines/accessing-data/"},{"category":null,"content":"Decimal Digit 分词过滤器 #  decimal_digit 分词过滤器用于将各种字符集中的十进制数字字符（0 到 9）规范化为它们对应的 ASCII 字符。当你希望在文本分析中确保所有数字都能被统一处理，而不管这些数字是以何种字符集书写时，这个过滤器就非常有用。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有十进制数字过滤器的分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;my_decimal_digit_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;decimal_digit\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;my_decimal_digit_filter\u0026#34;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;123 ١٢٣ १२३\u0026#34; } text分词：\n “123”（ASCII 数字） “١٢٣”（阿拉伯 - 印度数字） “१२३”（梵文数字）  返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;NUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"十进制数字分词过滤器（Decimal Digit）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/decimal-digit/"},{"category":null,"content":"Stop 分析器 #  stop 分析器会在文本中移除预定义的停用词。该分析器由一个小写分词器和一个停用词分词过滤器组成。\n相关指南（先读这些） #    文本分析基础  文本分析：停用词  文本分析：识别词元  参数说明 #  你可以使用以下参数来配置一个停用词分词器。\n   参数 必填/可选 数据类型 描述     stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。   stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。    参考样例 #  以下命令创建一个名为 my_stop_index 并使用停词器分词器的索引：\nPUT /my_stop_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;stop\u0026#34; } } } } 配置自定义分词器 #  以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于停用词分词器：\nPUT /my_custom_stop_analyzer_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_stop_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;lowercase\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;stop\u0026#34; ] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_stop_analyzer\u0026#34; } } } } 产生的词元 #  以下请求用来检查分词器生成的词元：\nPOST /my_custom_stop_analyzer_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_stop_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The large turtle is green and brown\u0026#34; } 返回内容中包含了产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;start_offset\u0026#34;: 4, \u0026#34;end_offset\u0026#34;: 9, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;, \u0026#34;start_offset\u0026#34;: 10, \u0026#34;end_offset\u0026#34;: 16, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;start_offset\u0026#34;: 20, \u0026#34;end_offset\u0026#34;: 25, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;brown\u0026#34;, \u0026#34;start_offset\u0026#34;: 30, \u0026#34;end_offset\u0026#34;: 35, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 6 } ] } 指定停用词 #  下面演示了怎么去指定一个自定义的停用词列表：\nPUT /my_new_custom_stop_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_stop_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;stopwords_path\u0026#34;: \u0026#34;stopwords.txt\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_stop_analyzer\u0026#34; } } } } 在这个例子中，该文件位于配置目录中。你也可以指定该文件的完整路径。\n","subcategory":null,"summary":"","tags":null,"title":"停用词分析器（Stop）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/stop-analyzer/"},{"category":null,"content":"Delete API #  按 ID 删除单条文档。\n请求格式 #  DELETE /\u0026lt;index\u0026gt;/_doc/\u0026lt;_id\u0026gt; 路径参数 #     参数 必需 说明     \u0026lt;index\u0026gt; 是 目标索引   \u0026lt;_id\u0026gt; 是 文档 ID    查询参数 #     参数 类型 默认值 说明     routing string — 自定义路由值   timeout time 1m 等待主分片可用的超时   refresh string false 删除后刷新策略   version long — 期望版本号   version_type string internal 版本类型   if_seq_no long — 乐观并发控制   if_primary_term long — 乐观并发控制   wait_for_active_shards string — 活跃分片数量    示例 #  DELETE /website/_doc/123 响应：\n{ \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;result\u0026#34;: \u0026#34;deleted\u0026#34; }  删除后 _version 递增。即使文档被删除后重新写入同一 _id，_version 仍会继续递增。\n  参考导航 #     需求 参见     按查询批量删除  Delete by Query   写入文档  Index API   乐观并发控制  并发控制与版本    ","subcategory":null,"summary":"","tags":null,"title":"Delete API","url":"/easysearch/main/docs/features/document-operations/delete-api/"},{"category":null,"content":"转换处理器 #  convert 处理器将文档中的字段转换为不同的类型，例如，将字符串转换为整数或将整数转换为字符串。对于数组字段，数组中的所有值都会被转换。\n语法 #  以下是为 convert 处理器提供的语法：\n{ \u0026#34;convert\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;field_name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;type-value\u0026#34; } } 配置参数 #  下表列出了 convert 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要转换的数据的字段名称。支持模板使用。   type 必填 要将字段值转换为的类型。支持的类型有 integer 、 long 、 float 、 double 、 string 、 boolean 、 ip 和 auto 。如果 type 是 boolean ，则如果字段值是字符串 true （忽略大小写），则将其设置为 true ；如果字段值是字符串 false （忽略大小写），则将其设置为 false 。如果类型设置为 ip ，则此处理器将验证字段值是否遵循 IPv4 或 IPv6 地址的正确格式；如果值无效，将引发错误。如果值不是允许的值之一，将发生错误。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。如果设置为 true ，则处理器在字段不存在或为 null 时不会修改文档。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。   target_field 可选 字段名称，用于存储解析后的数据。如果未指定，值将存储在 field 字段中。默认为 field 。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 convert-price 的管道，将 price 转换为浮点数，将转换后的值存储在 price_float 字段中，如果该值小于 0 ，则将其设置为 0 ：\nPUT _ingest/pipeline/convert-price { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that converts price to floating-point number and sets value to zero if price less than zero\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;convert\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;price_float\u0026#34; } }, { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;ctx.price_float \u0026lt; 0\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/convert-price/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;price\u0026#34;: \u0026#34;-10.5\u0026#34; } } ] } 返回内容确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;price_float\u0026#34;: -10.5, \u0026#34;price\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-08-22T15:38:21.180688799Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=convert-price { \u0026#34;price\u0026#34;: \u0026#34;10.5\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"转换处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/convert/"},{"category":null,"content":"Whitespace 分析器 #  whitespace 分析器仅基于空白字符（例如，空格和制表符）将文本拆分为词元。比如转换为小写形式或移除停用词这样的转换操作，它都不会应用，因此文本的原始大小写形式会被保留，并且标点符号也会作为词元的一部分包含在内。\n相关指南（先读这些） #    文本分析基础  文本分析：识别词元  参考样例 #  以下命令创建一个名为 my_whitespace_index 并使用空格分词器的索引：\nPUT /my_whitespace_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;whitespace\u0026#34; } } } } 配置自定义分词器 #  以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于空格分词器：\nPUT /my_custom_whitespace_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_whitespace_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;whitespace\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_whitespace_analyzer\u0026#34; } } } } 产生的词元 #  以下请求用来检查分词器生成的词元：\nPOST /my_custom_whitespace_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_whitespace_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The SLOW turtle swims away! 123\u0026#34; } 返回内容中包含了产生的词元\n{ \u0026#34;tokens\u0026#34;: [ {\u0026#34;token\u0026#34;: \u0026#34;the\u0026#34;,\u0026#34;start_offset\u0026#34;: 0,\u0026#34;end_offset\u0026#34;: 3,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;slow\u0026#34;,\u0026#34;start_offset\u0026#34;: 4,\u0026#34;end_offset\u0026#34;: 8,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 1}, {\u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;,\u0026#34;start_offset\u0026#34;: 9,\u0026#34;end_offset\u0026#34;: 15,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;swims\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 21,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;away!\u0026#34;,\u0026#34;start_offset\u0026#34;: 22,\u0026#34;end_offset\u0026#34;: 27,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;123\u0026#34;,\u0026#34;start_offset\u0026#34;: 28,\u0026#34;end_offset\u0026#34;: 31,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 5} ] } ","subcategory":null,"summary":"","tags":null,"title":"空白分析器（Whitespace）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/whitespace-analyzer/"},{"category":null,"content":"Lowercase 分词器 #  lowercase 分词器会在空白处将文本拆分成词条，然后把所有词条转换为小写形式。从功能上来说，这与配置一个 letter 分词器并搭配一个 lowercase 词元过滤器的效果是一样的。不过，使用 lowercase 分词器效率更高，因为分词操作是在一步之内完成的。\n相关指南（先读这些） #    文本分析：识别词元  文本分析：规范化  文本分析基础  参考样例 #  以下示例请求创建了一个名为 my-lowercase-index 的新索引，并配置了一个使用小写词元生成器的分词器：\nPUT /my-lowercase-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_lowercase_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;lowercase\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_lowercase_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_lowercase_tokenizer\u0026#34; } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my-lowercase-index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_lowercase_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;This is a Test. Easysearch 123!\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;this\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34;: 5, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;a\u0026#34;, \u0026#34;start_offset\u0026#34;: 8, \u0026#34;end_offset\u0026#34;: 9, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;start_offset\u0026#34;: 10, \u0026#34;end_offset\u0026#34;: 14, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 16, \u0026#34;end_offset\u0026#34;: 26, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"小写分词器（Lowercase）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/lowercase/"},{"category":null,"content":"后端配置 #  配置安全模块的第一步是确定如何验证用户身份。尽管 Easysearch 本身可以充当一个内部用户数据库，但许多人更喜欢集成企业现有的身份认证体系，例如 LDAP/Active Directory 服务器，或两者组合。设置身份验证和授权服务端的主要配置文件位于 config/security/config.yml。它定义了安全模块如何检索用户凭据、如何验证这些凭据以及如何从后端系统获取其他角色（可选）。\nconfig.yml 主要包含三大部分：\nconfig: dynamic: http: ... authc: ... authz: ... HTTP #  http 部分具有以下配置项：\nhttp: anonymous_auth_enabled: \u0026lt;true|false\u0026gt; xff: enabled: \u0026lt;true|false\u0026gt; internalProxies: \u0026#39;\u0026lt;regex pattern\u0026gt;\u0026#39; remoteIpHeader: \u0026#39;\u0026lt;header name\u0026gt;\u0026#39; 匿名认证 #  设置 anonymous_auth_enabled 可以选择是否开启匿名访问。如果启用，当请求中未提供任何凭据时，用户将以 anonymous 身份通过认证，并被分配 anonymous_backendrole 角色。如果禁用匿名身份验证，则至少在 authc 里面提供一个认证后端，否则安全模块将不予初始化。默认为 false。\n 提示： 如果启用匿名认证，所有 HTTP 认证器将不会发起 challenge（即不会返回 401 要求客户端提供凭据）。\n XFF（X-Forwarded-For）配置 #  如果 Easysearch 位于一个或多个代理或负载均衡器之后，xff 部分可用于配置从 HTTP 头中解析客户端真实 IP 地址的行为。这对于 IP 速率限制和代理认证尤为重要。\n   参数 描述 默认值     enabled 是否启用 XFF 解析 false   internalProxies 可信内部代理 IP 地址的正则表达式。只有来自匹配 IP 的请求才会解析 XFF 头。例如 192\\.168\\.0\\.10\\|192\\.168\\.0\\.11。设置为 .* 信任所有代理。 无   remoteIpHeader 包含客户端真实 IP 的 HTTP 请求头名称 x-forwarded-for    示例：\nhttp: anonymous_auth_enabled: false xff: enabled: true internalProxies: \u0026#39;192\\.168\\.0\\.10|192\\.168\\.0\\.11\u0026#39; remoteIpHeader: \u0026#39;x-forwarded-for\u0026#39; 认证（authc） #  认证配置 authc 具有以下格式：\nauthc: \u0026lt;domain_name\u0026gt;: description: \u0026#34;\u0026lt;描述信息\u0026gt;\u0026#34; http_enabled: \u0026lt;true|false\u0026gt; transport_enabled: \u0026lt;true|false\u0026gt; order: \u0026lt;integer\u0026gt; http_authenticator: type: \u0026lt;type\u0026gt; challenge: \u0026lt;true|false\u0026gt; config: ... authentication_backend: type: \u0026lt;type\u0026gt; config: ... 配置 authc 里面的每一项被称为 身份验证域（Authentication Domain）。它指定了从何处获取用户凭据以及应针对哪个后端对它们进行身份验证。\n您可以使用多个身份验证域。每个身份验证域都有一个名称（例如 basic_internal_auth_domain）、可选的 description 描述字段、http_enabled/transport_enabled 开关和排序参数 order。安全模块按 order 顺序依次使用它们。如果用户成功通过一个域进行了身份验证，安全模块将跳过剩余的验证域。\n关于 Challenge #  challenge 参数决定当未提供凭据时，HTTP 认证器是否应该向客户端发起认证挑战（如返回 HTTP 401 响应和 WWW-Authenticate 头）。默认值为 true。\n 注意： 如果定义了多个 HTTP 认证器，请将不发起 challenge 的认证器（如 proxy、clientcert、jwt）放在前面，将发起 challenge 的认证器（如 basic）放在最后。因为无法同时用两种不同的认证方法向客户端发起 challenge。\n HTTP 认证器类型 #  http_authenticator 指定了在 HTTP 层上使用的身份验证方法。\n   类型 说明 Challenge 备注     basic HTTP 基本身份验证（用户名/密码） 是 最常用的认证方式   proxy 代理认证，从 HTTP 请求头中提取用户信息 否 需要配合 XFF   extended-proxy 扩展代理认证，支持自定义属性头 否 继承 proxy 的所有功能   clientcert 通过客户端 TLS 证书进行身份验证 否 需要 HTTPS   kerberos 通过 Kerberos/SPNEGO 进行身份验证 是 需要企业功能   jwt 通过 JSON Web Token 进行身份验证 否 需要企业功能   openid 通过 OpenID Connect 进行身份验证 否 需要企业功能   saml 通过 SAML 进行身份验证 否 需要企业功能    Basic 认证 #  HTTP 基本身份验证是最简单也是最常用的认证方式。它从 HTTP Authorization 请求头中提取用户名和密码。\nhttp_authenticator: type: basic challenge: true 无需额外配置。通常与 internal 或 ldap 认证后端配合使用。\n代理认证（Proxy） #  代理认证用于当 Easysearch 位于一个已经完成用户认证的代理之后的场景。代理通过 HTTP 请求头将已认证的用户名和角色传递给 Easysearch。\n 注意： 使用代理认证时，必须先启用 XFF（xff.enabled: true），并正确配置 internalProxies 以限定可信代理的 IP 范围。否则任何人都可以通过伪造请求头来冒充用户。\n proxy_auth_domain: http_enabled: true transport_enabled: true order: 3 http_authenticator: type: proxy challenge: false config: user_header: \u0026#34;x-proxy-user\u0026#34; roles_header: \u0026#34;x-proxy-roles\u0026#34; roles_separator: \u0026#34;,\u0026#34; authentication_backend: type: noop    参数 描述 默认值     user_header 包含已认证用户名的 HTTP 请求头 无   roles_header 包含用户角色（逗号分隔）的 HTTP 请求头 无   roles_separator 角色分隔符 ,    扩展代理认证（Extended Proxy） #  扩展代理认证继承了代理认证的所有功能，并额外支持通过 HTTP 请求头传递自定义用户属性。\nhttp_authenticator: type: extended-proxy challenge: false config: user_header: \u0026#34;x-proxy-user\u0026#34; roles_header: \u0026#34;x-proxy-roles\u0026#34; attr_header_prefix: \u0026#34;x-proxy-ext-\u0026#34;    参数 描述 默认值     attr_header_prefix 自定义属性头的前缀。所有以此前缀开头的请求头都会被提取为用户属性。 无    例如，如果 attr_header_prefix 设置为 x-proxy-ext-，则请求头 x-proxy-ext-department: engineering 将为用户添加属性 attr.proxy.department=engineering。\n客户端证书认证（ClientCert） #  通过客户端 TLS 证书进行身份验证。客户端证书必须受节点信任库中的根 CA 之一的信任。\nclientcert_auth_domain: http_enabled: true transport_enabled: true order: 2 http_authenticator: type: clientcert challenge: false config: username_attribute: cn authentication_backend: type: noop    参数 描述 默认值     username_attribute 从客户端证书 DN 中提取用户名所使用的属性。例如 cn、ou 等。如果不设置，则使用完整的 DN 作为用户名。 无（使用完整 DN）   roles_attribute 从客户端证书 DN 中提取角色所使用的属性。 无    JWT 认证 #  通过 JSON Web Token（JWT）进行身份验证。JWT 通常由外部身份提供商签发，Easysearch 通过验证签名来确认用户身份。\njwt_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: jwt challenge: false config: signing_key: \u0026#34;base64 encoded HMAC key or public RSA/ECDSA pem key\u0026#34; jwt_header: \u0026#34;Authorization\u0026#34; jwt_url_parameter: null roles_key: null subject_key: null authentication_backend: type: noop    参数 描述 默认值     signing_key 用于验证 JWT 签名的密钥。对于 HMAC 使用 Base64 编码的密钥，对于 RSA/ECDSA 使用 PEM 格式的公钥。 无（必填）   jwt_header 包含 JWT 的 HTTP 请求头名称 Authorization   jwt_url_parameter 如果 JWT 通过 URL 参数传递，指定参数名 无   roles_key JWT payload 中包含角色的字段名 无   subject_key JWT payload 中包含用户名的字段名 无（使用 sub）    Kerberos 认证 #  通过 Kerberos/SPNEGO 协议进行身份验证，常用于与 Active Directory 集成的企业环境。\nkerberos_auth_domain: http_enabled: true transport_enabled: true order: 6 http_authenticator: type: kerberos challenge: true config: krb_debug: false strip_realm_from_principal: true authentication_backend: type: noop    参数 描述 默认值     krb_debug 是否输出 Kerberos/安全相关的调试日志到标准输出 false   strip_realm_from_principal 是否从 Kerberos principal 中去除 realm 部分。例如将 user@REALM.COM 简化为 user。 true    认证后端类型 #  authentication_backend 指定了对 HTTP 认证器提取的凭据进行验证的后端系统。\n   类型 说明     noop 不进行进一步的身份验证。适用于 HTTP 认证器已经完全验证了用户身份的情况，例如 JWT、Kerberos、客户端证书或代理认证。   internal 使用 user.yml 中定义的内部用户数据库进行认证。   ldap 使用 LDAP 或 Active Directory 进行认证。    Internal 认证后端 #  使用 Easysearch 内置的用户数据库（config/security/user.yml）进行认证。这是最简单的认证后端，适合用户数量较少或测试环境。\nauthentication_backend: type: internal 无需额外配置。\nLDAP 认证后端 #  使用 LDAP 或 Active Directory 对用户进行身份验证。\nldap_auth_domain: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: basic challenge: true authentication_backend: type: ldap config: enable_ssl: false enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: false hosts: - localhost:389 bind_dn: \u0026#34;cn=admin,dc=example,dc=com\u0026#34; password: \u0026#34;password\u0026#34; userbase: \u0026#39;ou=people,dc=example,dc=com\u0026#39; usersearch: \u0026#39;(uid={0})\u0026#39; username_attribute: null 连接配置：\n   参数 描述 默认值     hosts LDAP 服务器地址列表（host:port 格式） 无（必填）   bind_dn 用于连接 LDAP 服务器的绑定 DN 无   password 绑定 DN 的密码 无   connect_timeout 连接超时时间（毫秒） 无   response_timeout 响应超时时间（毫秒） 无    SSL/TLS 配置：\n   参数 描述 默认值     enable_ssl 是否启用 LDAPS false   enable_start_tls 是否启用 StartTLS（enable_ssl 应为 false） false   enable_ssl_client_auth 是否发送客户端证书 false   verify_hostnames 是否验证 LDAP 服务器主机名 false   trust_all 是否信任所有 LDAP 服务器证书 false   cert_alias 客户端证书别名（JKS） 无   ca_alias CA 证书别名（JKS） 无   key_file 客户端私钥文件路径（PEM） 无   cert_file 客户端证书文件路径（PEM） 无   ca_file CA 证书文件路径（PEM） 无   enabled_ssl_ciphers 允许的 SSL 加密套件 无   enabled_ssl_protocols 允许的 SSL 协议 无    用户搜索配置：\n   参数 描述 默认值     userbase 用户搜索的基础 DN 无（必填）   usersearch 搜索用户的 LDAP 过滤器。{0} 将被替换为用户名。 无（必填）   username_attribute 使用用户条目中的此属性作为用户名。如果不设置则使用 DN。 无   search_all_bases 是否在所有基础 DN 下搜索 false    授权（authz） #  对用户进行身份验证后，安全模块可以选择从后端权限系统收集用户的其他角色。\n授权配置 authz 具有以下格式：\nauthz: \u0026lt;domain_name\u0026gt;: description: \u0026#34;\u0026lt;描述信息\u0026gt;\u0026#34; http_enabled: \u0026lt;true|false\u0026gt; transport_enabled: \u0026lt;true|false\u0026gt; authorization_backend: type: \u0026lt;type\u0026gt; config: ... 您可以在本节中定义多个条目。在授权阶段，执行顺序不相关，因此没有 order 字段。\n授权后端类型 #     类型 说明     noop 跳过额外的角色收集。   ldap 从 LDAP 或 Active Directory 获取用户的后端角色。    LDAP 授权后端 #  从 LDAP 或 Active Directory 获取用户的角色信息。\nauthz: roles_from_ldap: http_enabled: true transport_enabled: true authorization_backend: type: ldap config: enable_ssl: false enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: false hosts: - localhost:389 bind_dn: \u0026#34;cn=admin,dc=example,dc=com\u0026#34; password: \u0026#34;password\u0026#34; rolebase: \u0026#39;ou=groups,dc=example,dc=com\u0026#39; rolesearch: \u0026#39;(member={0})\u0026#39; userroleattribute: null userrolename: disabled rolename: cn resolve_nested_roles: true userbase: \u0026#39;ou=people,dc=example,dc=com\u0026#39; usersearch: \u0026#39;(uid={0})\u0026#39; LDAP 授权的连接和 SSL/TLS 配置与 LDAP 认证后端 相同。\n角色搜索配置：\n   参数 描述 默认值     rolebase 角色搜索的基础 DN 无（必填）   rolesearch 搜索角色的 LDAP 过滤器。{0} 将被替换为用户的 DN，{1} 替换为用户名，{2} 替换为 userroleattribute 指定的属性值。 无（必填）   userroleattribute 用户条目中包含角色信息的属性名。其值将替换 rolesearch 中的 {2}。 无   userrolename 用户条目中直接包含角色名的属性（如 memberOf）。设置为 disabled 禁用。 disabled   rolename 角色条目中包含角色名称的属性。也可以设置为 dn 使用完整 DN 作为角色名。 name   resolve_nested_roles 是否递归解析嵌套角色（角色的角色） true   skip_users 跳过匹配的用户，支持通配符和正则表达式 无   rolesearch_enabled 是否启用角色搜索 true   nested_role_filter 嵌套角色过滤器 无   max_nested_depth 最大嵌套角色解析深度 无   custom_attr_maxval_len 自定义属性最大值长度 无   custom_attr_whitelist 允许的自定义属性白名单 无    认证失败监听器（auth_failure_listeners） #  安全模块支持配置认证失败监听器，用于实现基于 IP 或用户名的速率限制，防止暴力破解攻击。\nconfig: dynamic: auth_failure_listeners: ip_rate_limiting: type: ip allowed_tries: 10 time_window_seconds: 3600 block_expiry_seconds: 600 max_blocked_clients: 100000 max_tracked_clients: 100000 internal_authentication_backend_limiting: type: username authentication_backend: internal allowed_tries: 10 time_window_seconds: 3600 block_expiry_seconds: 600 max_blocked_clients: 100000 max_tracked_clients: 100000 IP 速率限制 #  基于 IP 地址的速率限制，无论使用哪个用户名，只要某个 IP 地址在指定时间窗口内认证失败次数超过阈值，就会被临时封禁。\n   参数 描述 默认值     type 设置为 ip 无（必填）   allowed_tries 在被封禁前允许的失败尝试次数 10   time_window_seconds 计数失败尝试的时间窗口（秒） 3600   block_expiry_seconds 封禁持续时间（秒） 600   max_blocked_clients 最大被封禁客户端数量 100000   max_tracked_clients 最大追踪客户端数量 100000    用户名速率限制 #  基于用户名的速率限制，针对特定认证后端的用户名进行限制。\n   参数 描述 默认值     type 设置为 username 无（必填）   authentication_backend 要限制的认证后端名称（如 internal） 无（必填）   allowed_tries 在被封禁前允许的失败尝试次数 10   time_window_seconds 计数失败尝试的时间窗口（秒） 3600   block_expiry_seconds 封禁持续时间（秒） 600   max_blocked_clients 最大被封禁客户端数量 100000   max_tracked_clients 最大追踪客户端数量 100000    完整配置示例 #  以下是一个包含多种认证方式的完整配置示例：\n_meta: type: \u0026#34;config\u0026#34; config_version: 2 config: dynamic: http: anonymous_auth_enabled: false xff: enabled: true internalProxies: '192.168.0.10|192.168.0.11' remoteIpHeader: 'x-forwarded-for' authc: basic_internal_auth_domain: description: \u0026quot;Authenticate via HTTP Basic against internal users database\u0026quot; http_enabled: true transport_enabled: true order: 4 http_authenticator: type: basic challenge: true authentication_backend: type: internal proxy_auth_domain: description: \u0026quot;Authenticate via proxy\u0026quot; http_enabled: false transport_enabled: false order: 3 http_authenticator: type: proxy challenge: false config: user_header: \u0026quot;x-proxy-user\u0026quot; roles_header: \u0026quot;x-proxy-roles\u0026quot; authentication_backend: type: noop jwt_auth_domain: description: \u0026quot;Authenticate via Json Web Token\u0026quot; http_enabled: false transport_enabled: false order: 0 http_authenticator: type: jwt challenge: false config: signing_key: \u0026quot;base64 encoded HMAC key or public RSA/ECDSA pem key\u0026quot; jwt_header: \u0026quot;Authorization\u0026quot; jwt_url_parameter: null roles_key: null subject_key: null authentication_backend: type: noop clientcert_auth_domain: description: \u0026quot;Authenticate via SSL client certificates\u0026quot; http_enabled: false transport_enabled: false order: 2 http_authenticator: type: clientcert challenge: false config: username_attribute: cn authentication_backend: type: noop ldap_auth_domain: description: \u0026quot;Authenticate via LDAP or Active Directory\u0026quot; http_enabled: false transport_enabled: false order: 1 http_authenticator: type: basic challenge: true authentication_backend: type: ldap config: enable_ssl: false enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: false hosts: - localhost:389 bind_dn: \u0026quot;cn=admin,dc=example,dc=com\u0026quot; password: \u0026quot;password\u0026quot; userbase: 'ou=people,dc=example,dc=com' usersearch: '(uid={0})' username_attribute: null authz: basic_authz: http_enabled: true transport_enabled: true authorization_backend: type: noop roles_from_ldap: description: \u0026quot;Authorize via LDAP or Active Directory\u0026quot; http_enabled: false transport_enabled: false authorization_backend: type: ldap config: enable_ssl: false enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: false hosts: - localhost:389 bind_dn: \u0026quot;cn=admin,dc=example,dc=com\u0026quot; password: \u0026quot;password\u0026quot; rolebase: 'ou=groups,dc=example,dc=com' rolesearch: '(member={0})' userroleattribute: null userrolename: disabled rolename: cn resolve_nested_roles: true userbase: 'ou=people,dc=example,dc=com' usersearch: '(uid={0})' auth_failure_listeners: ip_rate_limiting: type: ip allowed_tries: 10 time_window_seconds: 3600 block_expiry_seconds: 600 max_blocked_clients: 100000 max_tracked_clients: 100000 internal_authentication_backend_limiting: type: username authentication_backend: internal allowed_tries: 10 time_window_seconds: 3600 block_expiry_seconds: 600 max_blocked_clients: 100000 max_tracked_clients: 100000 \n","subcategory":null,"summary":"","tags":null,"title":"后端配置","url":"/easysearch/main/docs/operations/security/configuration/backend/"},{"category":null,"content":"删除管道 #  使用 DELETE _ingest/pipeline API 删除摄取管道。\n请求格式 #  删除指定管道：\nDELETE _ingest/pipeline/\u0026lt;pipeline-id\u0026gt; 使用通配符删除匹配的管道：\nDELETE _ingest/pipeline/log-* 删除所有管道（慎用）：\nDELETE _ingest/pipeline/* 参数 #     参数 必需 类型 默认值 说明     pipeline-id 是 string — 要删除的管道 ID，支持通配符 *   cluster_manager_timeout 否 时长 30s 等待集群管理器节点响应的超时时间   timeout 否 时长 30s 等待整体响应的超时时间    响应示例 #  { \u0026#34;acknowledged\u0026#34;: true } 注意事项 #   删除一个正在被 index.default_pipeline 或 index.final_pipeline 引用的管道，会导致后续写入该索引的操作失败 删除操作不可撤销。如需备份管道配置，先用 GET _ingest/pipeline/\u0026lt;pipeline-id\u0026gt; 获取其定义 使用通配符 * 删除所有管道时需格外谨慎，建议仅在测试环境中使用  ","subcategory":null,"summary":"","tags":null,"title":"删除管道","url":"/easysearch/main/docs/features/ingest-pipelines/delete-ingest/"},{"category":null,"content":"Update API #  基于现有文档做部分字段更新或脚本更新。 内部流程：读取当前 _source → 合并修改 → 写入新文档（不是原地修改）。\n请求格式 #  POST /\u0026lt;index\u0026gt;/_update/\u0026lt;_id\u0026gt; 路径参数 #     参数 必需 说明     \u0026lt;index\u0026gt; 是 目标索引   \u0026lt;_id\u0026gt; 是 文档 ID    查询参数 #     参数 类型 默认值 说明     retry_on_conflict int 0 版本冲突时自动重试次数。适合\u0026quot;计数器累加\u0026quot;等顺序不敏感场景   routing string — 自定义路由   timeout time 1m 等待主分片可用的超时   refresh string false 更新后刷新策略：true、false、wait_for   _source string/boolean true 控制响应中是否返回 _source   _source_includes string — 响应中 _source 要包含的字段   _source_excludes string — 响应中 _source 要排除的字段   if_seq_no long — 乐观并发控制   if_primary_term long — 乐观并发控制   wait_for_active_shards string — 活跃分片数量   require_alias boolean false 要求目标必须是别名     注意：Update API 不支持 version / version_type 参数。如需乐观并发控制，请使用 if_seq_no + if_primary_term。\n 请求体字段 #     字段 类型 说明     doc object 要合并的部分文档。与现有 _source 做浅合并   upsert object 当文档不存在时，按此内容创建文档   doc_as_upsert boolean 为 true 时，若文档不存在则直接将 doc 作为新文档插入   script object/string 使用脚本更新。可以是 inline 脚本字符串或 {\u0026quot;source\u0026quot;:\u0026quot;...\u0026quot;,\u0026quot;lang\u0026quot;:\u0026quot;painless\u0026quot;,\u0026quot;params\u0026quot;:{...}} 对象   scripted_upsert boolean 为 true 时，无论文档是否存在都执行脚本（结合 upsert 使用）   detect_noop boolean 为 true 时（默认），如果 doc 合并后没有实际变化，则不执行写入操作   _source object 请求体内的 _source 过滤配置    示例 #  部分更新（doc merge） #  给文档增加 tags 和 views 字段：\nPOST /website/_update/1 { \u0026#34;doc\u0026#34;: { \u0026#34;tags\u0026#34;: [\u0026#34;testing\u0026#34;], \u0026#34;views\u0026#34;: 0 } } 已有字段保持不变，新字段被添加。\n脚本更新 #  基于旧值做计算——将 views 加 1：\nPOST /website/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.views += params.inc\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;inc\u0026#34;: 1 } } } 向数组追加元素：\nPOST /website/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.tags.add(params.new_tag)\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;new_tag\u0026#34;: \u0026#34;search\u0026#34; } } }  脚本适合\u0026quot;简单算子 + 高价值更新\u0026quot;场景。复杂业务逻辑尽量放在上游服务完成。\n Upsert（不存在则插入） #  计数器场景，第一次更新时文档可能还不存在：\nPOST /website/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.views += 1\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34; }, \u0026#34;upsert\u0026#34;: { \u0026#34;views\u0026#34;: 1 } }  第一次调用：文档不存在，按 upsert 内容创建，views = 1 后续调用：文档已存在，执行脚本，views 递增  doc_as_upsert #  \u0026ldquo;文档存在就合并 doc，不存在就把 doc 当新文档插入\u0026rdquo;：\nPOST /website/_update/1 { \u0026#34;doc\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;New title\u0026#34;, \u0026#34;views\u0026#34;: 0 }, \u0026#34;doc_as_upsert\u0026#34;: true } 冲突重试 #  顺序不敏感的更新（如页面浏览计数器），可以自动重试：\nPOST /website/_update/1?retry_on_conflict=5 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.views += 1\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34; } } 最多自动重试 5 次版本冲突。\n 参考导航 #     需求 参见     按查询批量更新  Update by Query   整文档覆盖写入  Index API   乐观并发控制  并发控制与版本    ","subcategory":null,"summary":"","tags":null,"title":"Update API","url":"/easysearch/main/docs/features/document-operations/update-api/"},{"category":null,"content":"Rewrite 参数 #  像 wildcard、prefix、regexp、fuzzy 和 range 这样的多词查询在内部会重组成一组词。rewrite 参数允许你控制这些词重写的执行和评分。\n当多词查询扩展成很多词（例如 prefix: \u0026quot;error*\u0026quot; 匹配数百个词）时，它们在内部会转换成 term 查询。这个过程可能会有以下缺点：\n 超出 indices.query.bool.max_clause_count 限制（默认是 1024）。 影响匹配文档的评分计算方式。 根据所使用的重写方法，影响内存和延迟。  rewrite 参数让你能够控制多词查询的内部行为。\n   模式 评分规则 性能 注释     constant_score 所有匹配具有相同分数 最佳 默认模式，适合过滤器   scoring_boolean 基于 TF/IDF 中等 完整相关性评分   constant_score_boolean 相同分数但使用布尔结构 中等 与 must_not 或 minimum_should_match 一起使用   top_terms_N 在顶部 N 个词上使用 TF/IDF 高效 截断扩展   top_terms_boost_N 静态提升 快速 较低准确度   top_terms_blended_freqs_N 混合评分 平衡 最佳评分/效率权衡    相关指南（先读这些） #    部分匹配  查询 DSL 基础  可用的重写方法 #  下表总结了可用的重写方法。\n   重写方法 描述     constant_score （默认）所有展开的词项作为一个单一单元一起被评估，为每个匹配分配相同的分数。匹配的文档不会被单独评分，这使得它在过滤用例中非常高效。   scoring_boolean 将查询分解为一个布尔 should 子句，每个匹配都有一个词项查询。每个结果根据相关性单独评分。   constant_score_boolean 类似于 scoring_boolean ，但所有文档都获得一个固定的分数，而不管词项频率如何。保持布尔结构，不进行 TF/IDF 加权。   top_terms_N 限制评分和执行到 N 个最频繁的词项。减少资源使用并防止子句过载。   top_terms_boost_N 与 top_terms_N 类似，但使用静态提升代替完整评分。通过简化相关性提供性能改进。   top_terms_blended_freqs_N 选择最匹配的前 N 个词项，并对其文档频率进行平均以用于评分。产生平衡的评分，无需完整词项爆炸。    基于布尔值的重写限制 #  所有基于布尔值的重写，如 scoring_boolean 、 constant_score_boolean 和 top_terms_* ，都受以下动态集群级别的索引设置约束：\nindices.query.bool.max_clause_count 此设置控制允许的最大布尔子句数量（默认： 1024 ）。如果您的查询扩展到的词项数量超过此限制，则会以 too_many_clauses 错误被拒绝。\n例如，通配符（如“error*”）可能会扩展到数百或数千个匹配词项，其中包括“error”、“errors”、“error_log”、“error404”等。每个词项都会变成一个单独的 term 查询。如果词项数量超过 indices.query.bool.max_clause_count 限制，查询将失败：\nPOST /logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;error*\u0026#34;, \u0026#34;rewrite\u0026#34;: \u0026#34;scoring_boolean\u0026#34; } } } } 查询的内部扩展如下：\n{ \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;errors\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error_log\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error404\u0026#34; } }, ... ] } } 常量评分 #  默认的 constant_score 重写方法将所有展开的词项包装成一个查询，并完全跳过评分阶段。这种方法具有以下特点：\n 将所有词项匹配作为单个位图查询执行。 完全忽略评分；每个文档都得到 _score 的 1.0 。 最快的选项；非常适合过滤。  以下示例使用默认的 constant_score 重写方法运行 wildcard 查询，以高效地过滤在 message 字段中匹配 warning* 模式的文档：\nPOST /logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;warning*\u0026#34; } } } } 评分布尔值 #  scoring_boolean 重写方法将扩展的词项拆分为单独的 term 查询，并在布尔 should 子句下组合。这种方法的工作原理如下：\n 将通配符扩展为布尔 should 子句内的单个 term 查询。 每个文档的评分反映了它与多少词项匹配以及这些词项的频率。  以下示例使用 scoring_boolean 重写配置：\nPOST /logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;warning*\u0026#34;, \u0026#34;rewrite\u0026#34;: \u0026#34;scoring_boolean\u0026#34; } } } } 常量分数布尔值 #  constant_score_boolean 重写方法使用与 scoring_boolean 相同的布尔结构，但禁用评分，因此当需要条件逻辑而不需要相关性排序时很有用。此方法具有以下特点：\n 与 scoring_boolean 结构相似，但文档不会被排序。 所有匹配的文档获得相同的分数。 保留布尔条件逻辑的灵活性，例如使用 must_not ，而不进行排序。  以下示例查询使用了一个 must_not 布尔子句：\nPOST /logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must_not\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;error*\u0026#34;, \u0026#34;rewrite\u0026#34;: \u0026#34;constant_score_boolean\u0026#34; } } } } } } 该查询内部展开如下：\n{ \u0026#34;bool\u0026#34;: { \u0026#34;must_not\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;errors\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error_log\u0026#34; } }, ... ] } } } } 最佳词项 N #  top_terms_N 方法是多种重写选项中的一种，旨在扩展多词查询时平衡评分准确性和性能。其工作原理如下：\n 仅选择并评分 N 个最频繁匹配的词。 当你预期会有大量词扩展且希望限制负载时，此方法很有用。 其他有效词会被忽略以保持性能。  以下查询使用 top_terms_2 重写方法，仅对在 message 字段中匹配 warning* 模式的两个最频繁的词进行评分：\nPOST /logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;warning*\u0026#34;, \u0026#34;rewrite\u0026#34;: \u0026#34;top_terms_2\u0026#34; } } } } 最佳词项评分 N #\n top_terms_boost_N 重写方法选择最匹配的前 N 个词项，并使用静态 boost 值代替计算完整的相关性分数。它的工作原理如下：\n 将扩展限制为最前 N 个词项，如 top_terms_N 。 它不是计算 TF/IDF，而是为每个词项分配预设的提升值。 提供更快的执行速度和可预测的相关性权重。  以下示例使用 top_terms_boost_2 重写参数：\nPOST /logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;warning*\u0026#34;, \u0026#34;rewrite\u0026#34;: \u0026#34;top_terms_boost_2\u0026#34; } } } } 混合频率的最佳词项 N #\n top_terms_blended_freqs_N 重写方法选择顶部 N 个匹配词项，并将它们的文档频率混合起来，以生成更平衡的相关性分数。这种方法具有以下特点：\n 选择顶部 N 个匹配词项，并对所有词项应用混合频率。 混合使不同频率的词项之间的评分更加平滑。 在追求性能与真实评分之间取得良好平衡时。  以下示例使用 top_terms_blended_freqs_2 重写参数：\nPOST /logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;warning*\u0026#34;, \u0026#34;rewrite\u0026#34;: \u0026#34;top_terms_blended_freqs_2\u0026#34; } } } } \n","subcategory":null,"summary":"","tags":null,"title":"Rewrite 参数","url":"/easysearch/main/docs/features/query-dsl/rewrite-parameter/"},{"category":null,"content":"ASCII Folding 分词过滤器 #  asciifolding 分词过滤器将非 ASCII 字符转换为与其最接近的 ASCII 等效字符。例如，\u0026ldquo;é\u0026rdquo; 变为 \u0026ldquo;e\u0026rdquo;，\u0026ldquo;ü\u0026rdquo; 变为 \u0026ldquo;u\u0026rdquo;，\u0026ldquo;ñ\u0026rdquo; 变为 \u0026ldquo;n\u0026rdquo;。这个过程被称为\u0026quot;音译\u0026quot;。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  ASCII 折叠分词过滤器有许多优点：\n 增强搜索灵活性：用户在输入查询内容时常常会省略重音符号或特殊字符。ASCII 折叠分词过滤器可确保即使是这样的查询也仍然能返回相关结果。 规范化：通过确保重音字符始终被转换为其 ASCII 等效字符，使索引编制过程标准化。 国际化：对于包含多种语言和字符集的应用程序特别有用。   尽管 ASCII 折叠分词过滤器可以简化搜索，但它也可能导致特定信息的丢失，尤其是当数据集中重音字符和非重音字符之间的区别很重要的时候。\n 参数说明 #  你可以使用 preserve_original 参数来配置 ASCII 折叠分词过滤器。将此参数设置为 true 时，会在词元流中同时保留原始词元及其 ASCII 折叠后的版本。当你希望在搜索查询中同时匹配一个词项的原始版本（带重音符号）和规范化版本（不带重音符号）时，这一点会特别有用。该参数的默认值为 false。\n参考样例 #  以下示例请求创建了一个名为 example_index 的新索引，并定义了一个分词器，该分词器使用了 ASCII 折叠过滤器，且将 preserve_original 参数设置为 true：\nPUT /example_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;custom_ascii_folding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;asciifolding\u0026#34;, \u0026#34;preserve_original\u0026#34;: true } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;custom_ascii_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;custom_ascii_folding\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /example_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;custom_ascii_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Résumé café naïve coördinate\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;resume\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;résumé\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;cafe\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;café\u0026#34;, \u0026#34;start_offset\u0026#34;: 7, \u0026#34;end_offset\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;naive\u0026#34;, \u0026#34;start_offset\u0026#34;: 12, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;naïve\u0026#34;, \u0026#34;start_offset\u0026#34;: 12, \u0026#34;end_offset\u0026#34;: 17, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;coordinate\u0026#34;, \u0026#34;start_offset\u0026#34;: 18, \u0026#34;end_offset\u0026#34;: 28, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;coördinate\u0026#34;, \u0026#34;start_offset\u0026#34;: 18, \u0026#34;end_offset\u0026#34;: 28, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 } ] } ","subcategory":null,"summary":"","tags":null,"title":"ASCII 折叠分词过滤器（ASCII Folding）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/ascii-folding/"},{"category":null,"content":"获取管道 #  使用 GET _ingest/pipeline API 检索管道的定义信息。\n请求格式 #  GET _ingest/pipeline/\u0026lt;pipeline-id\u0026gt; 获取所有管道 #  GET _ingest/pipeline 获取指定管道 #  GET _ingest/pipeline/my-pipeline 响应示例：\n{ \u0026#34;my-pipeline\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;处理学生数据：设置毕业年份并转大写\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;设置毕业年份\u0026#34;, \u0026#34;field\u0026#34;: \u0026#34;grad_year\u0026#34;, \u0026#34;value\u0026#34;: 2023 } }, { \u0026#34;set\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;标记已毕业\u0026#34;, \u0026#34;field\u0026#34;: \u0026#34;graduated\u0026#34;, \u0026#34;value\u0026#34;: true } }, { \u0026#34;uppercase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;name\u0026#34; } } ] } } 通配符匹配 #  支持通配符查询多个管道：\nGET _ingest/pipeline/log-* 查询参数 #     参数 必需 类型 默认值 说明     cluster_manager_timeout 否 时长 30s 等待集群管理器节点响应的超时时间    ","subcategory":null,"summary":"","tags":null,"title":"获取管道","url":"/easysearch/main/docs/features/ingest-pipelines/get-ingest/"},{"category":null,"content":"Simple 分析器 #  simple 分析器是一种非常基础的分析器，它会将文本中的非字母字符串拆分成词项，并将这些词项转换为小写形式。与 standard 分析器不同的是，simple 分析器将除字母字符之外的所有内容都视为分隔符，这意味着它不会把数字、标点符号或特殊字符识别为词元的一部分。\n相关指南（先读这些） #    文本分析基础  文本分析：识别词元  参考样例 #  以下命令创建一个名为 my_simple_index 并使用简单分词器的索引：\nPUT /my_simple_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;simple\u0026#34; } } } } 配置自定义分词器 #  以下命令配置了一个索引，该索引带有一个自定义分词器，这个自定义分词器等同于添加了 html_strip 字符过滤器的简单分词器：\nPUT /my_custom_simple_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;html_strip\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;html_strip\u0026#34; } }, \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_lowercase_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;lowercase\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_simple_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;html_strip\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;my_lowercase_tokenizer\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_simple_analyzer\u0026#34; } } } } 产生的词元 #  以下请求用来检查分词器生成的词元：\nPOST /my_custom_simple_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_simple_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;\u0026lt;p\u0026gt;The slow turtle swims over to dogs \u0026amp;copy; 2024!\u0026lt;/p\u0026gt;\u0026#34; } 返回内容中包含了产生的词元\n{ \u0026#34;tokens\u0026#34;: [ {\u0026#34;token\u0026#34;: \u0026#34;the\u0026#34;,\u0026#34;start_offset\u0026#34;: 3,\u0026#34;end_offset\u0026#34;: 6,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 0}, {\u0026#34;token\u0026#34;: \u0026#34;slow\u0026#34;,\u0026#34;start_offset\u0026#34;: 7,\u0026#34;end_offset\u0026#34;: 11,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 1}, {\u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;,\u0026#34;start_offset\u0026#34;: 12,\u0026#34;end_offset\u0026#34;: 18,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;swims\u0026#34;,\u0026#34;start_offset\u0026#34;: 19,\u0026#34;end_offset\u0026#34;: 24,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;over\u0026#34;,\u0026#34;start_offset\u0026#34;: 25,\u0026#34;end_offset\u0026#34;: 29,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 4}, {\u0026#34;token\u0026#34;: \u0026#34;to\u0026#34;,\u0026#34;start_offset\u0026#34;: 30,\u0026#34;end_offset\u0026#34;: 32,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 5}, {\u0026#34;token\u0026#34;: \u0026#34;dogs\u0026#34;,\u0026#34;start_offset\u0026#34;: 33,\u0026#34;end_offset\u0026#34;: 37,\u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;,\u0026#34;position\u0026#34;: 6} ] } ","subcategory":null,"summary":"","tags":null,"title":"简单分析器（Simple）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/simple-analyzer/"},{"category":null,"content":"正则表达式语法 #  正则表达式（regex）是一种使用特殊符号和运算符定义搜索模式的方法。这些模式允许你在字符串中匹配字符序列。\n在 Easysearch 中，你可以在以下查询类型中使用正则表达式：\n regexp query_string   注意：Easysearch 使用 Apache Lucene 正则表达式引擎，该引擎有自己的语法和限制。它不使用 Perl 兼容正则表达式（PCRE），因此某些熟悉的正则表达式功能可能会表现不同或不受支持。\n 相关指南（先读这些） #    部分匹配  正则查询  字符串查询  查询字符串语法 — 通配符、模糊、范围、布尔等完整语法参考  在 regexp 和 query_string 查询之间进行选择 #  regexp 和 query_string 查询都支持正则表达式，但它们的行为不同，适用于不同的使用场景。\n   特性 regexp 查询 query_string 查询     模式匹配 正则表达式模式必须匹配整个字段值 正则表达式模式可以匹配字段中的任何部分   flags 支持 flags 启用可选正则表达式运算符 flags 不支持   查询类型 词级查询（未评分） 全文检索（评分和解析）   最佳使用场景 对关键字或精确字段进行严格模式匹配 使用支持正则表达式模式的灵活查询字符串在分析字段中进行搜索   复杂查询组合 仅限于正则表达式模式 支持 AND 、 OR 、通配符、字段、提升值以及其他功能。参见查询字符串查询。    保留字符 #  Lucene 的正则表达式引擎支持所有 Unicode 字符。然而，以下字符被视为特殊运算符：\n. ? + * | { } [ ] ( ) \u0026#34; \\ 根据启用的 flags 指定的可选运算符，以下字符也可能被保留：\n@ \u0026amp; ~ \u0026lt; \u0026gt; 要字面匹配这些字符，可以用反斜杠（ \\ ）转义它们，或者将整个字符串用双引号括起来：\n \\\u0026amp; : 匹配字面量 \u0026amp; \\\\ : 匹配字面量反斜杠 ( \\ ) \u0026quot;hello@world\u0026quot; ：匹配整个字符串 hello@world  标准正则表达式运算符 #  Lucene 支持一组核心的正则表达式运算符：\n . – 匹配任何单个字符。示例： f.n 匹配 f 后跟任何字符然后是 n （例如 fan 或 fin ）。 ? – 匹配零个或一个前面的字符。示例： colou?r 匹配 color 和 colour 。 + – 匹配一个或多个前面的字符。示例： go+ 匹配 g 后面跟着一个或多个 o （ go ， goo ， gooo ，等等）。 * – 匹配零个或多个前面的字符。示例： lo*se 匹配 l 后面跟着零个或多个 o ，然后是 se （ lse ， lose ， loose ， loooose ，等等）。 {min,max} – 匹配特定的重复次数范围。如果 max 被省略，则匹配的字符数量没有上限。示例： x{3} 匹配正好 3 个 x （ xxx ）； x{2,4} 匹配 2 到 4 个 x （ xx ， xxx ，或 xxxx ）； x{3,} 匹配 3 个或更多 x （ xxx ， xxxx ， xxxxx ，等等）。 | – 充当逻辑 OR 。例如： apple|orange 匹配 apple 或 orange 。 ( ) – 将字符分组为子模式。例如： ab(cd)? 匹配 ab 和 abcd 。 [ ] – 匹配集合或范围内的一个字符。例如： [aeiou] 匹配任何元音。  - – 当置于括号内时，表示范围，除非被转义或位于括号内的第一个字符。例如： [a-z] 匹配任何小写字母； [-az] 匹配 - 、 a 或 z ； [a\\\\-z] 匹配 a 、 - 或 z 。 ^ – 当置于方括号内时，充当逻辑 NOT ，否定一个字符范围或集合中的任意字符。示例： [^az] 匹配除 a 或 z 之外的任意字符； [^a-z] 匹配除小写字母之外的任意字符； [^-az] 匹配除 - 、 a 和 z 之外的任意字符； [^a\\\\-z] 匹配除 a 、 - 和 z 之外的任意字符。    可选运算符 #  你可以使用 flags 参数启用额外的正则表达式运算符。多个标志之间用 | 分隔。\n以下是可以使用的标志：\n ALL (默认) – 启用所有可选运算符。 COMPLEMENT – 启用 ~ ，该标志会否定最短匹配的表达式。示例： d~ef 匹配 dgf 、 dxf ，但不匹配 def 。 INTERSECTION – 启用 \u0026amp; 作为 AND 逻辑运算符。示例： ab.+\u0026amp;.+cd 匹配包含 ab 开头和 cd 结尾的字符串。 INTERVAL – 启用 \u0026lt;min-max\u0026gt; 语法来匹配数字范围。示例： id\u0026lt;10-12\u0026gt; 匹配 id10 、 id11 和 id12 。 ANYSTRING – 使 @ 能够匹配任何字符串。你可以将其与 ~ 和 \u0026amp; 结合使用以进行排除。示例： @\u0026amp;.*error.*\u0026amp;.*[0-9]{3}.* 匹配同时包含“error”一词和三个数字序列的字符串。  不支持的特性 #  Lucene 的引擎不支持以下常用的正则表达式方式：\n ^ – 行首 $ – 行尾  示例 #  要尝试正则表达式，请将以下文档索引到 logs 索引中：\nPUT /logs/_doc/1 { \u0026#34;message\u0026#34;: \u0026#34;error404\u0026#34; } PUT /logs/_doc/2 { \u0026quot;message\u0026quot;: \u0026quot;error500\u0026quot; }\nPUT /logs/_doc/3 { \u0026quot;message\u0026quot;: \u0026quot;error1a\u0026quot; }\n示例：包含正则表达式的简单查询 #\n 以下 regexp 查询返回 message 字段的全部值与“error”后跟一个或多个数字的模式匹配的文档。如果值仅包含该模式作为子字符串，则不匹配：\nGET /logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;error[0-9]+\u0026#34; } } } } 此查询匹配 error404 和 error500 :\n{ \u0026#34;took\u0026#34;: 28, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;logs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error404\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;logs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error500\u0026#34; } } ] } } 示例：使用可选运算符 #  以下查询匹配 message 字段完全匹配以“error”开头，后跟 400 到 500（含）之间的数字的字符串。 INTERVAL 标志启用 \u0026lt;min-max\u0026gt; 语法用于数值范围：\nGET /logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;error\u0026lt;400-500\u0026gt;\u0026#34;, \u0026#34;flags\u0026#34;: \u0026#34;INTERVAL\u0026#34; } } } } 此查询匹配 error404 和 error500 :\n{ \u0026#34;took\u0026#34;: 22, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;logs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error404\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;logs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error500\u0026#34; } } ] } } 示例：使用 ANYSTRING #  当 ANYSTRING 标志启用时， @ 运算符匹配整个字符串。这在与交集（ \u0026amp; ）结合使用时很有用，因为它允许你在特定条件下构建匹配完整字符串的查询。\n以下查询匹配包含单词“error”和三个数字序列的消息。使用 ANYSTRING 来断言整个字段必须匹配两个模式的交集：\nGET /logs/_search { \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;message.keyword\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;@\u0026amp;.*error.*\u0026amp;.*[0-9]{3}.*\u0026#34;, \u0026#34;flags\u0026#34;: \u0026#34;ANYSTRING|INTERSECTION\u0026#34; } } } } 此查询匹配 error404 和 error500 :\n{ \u0026#34;took\u0026#34;: 20, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;logs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error404\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;logs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;error500\u0026#34; } } ] } } 请注意，此查询也将匹配 xerror500 、 error500x 和 errorxx500 。\n","subcategory":null,"summary":"","tags":null,"title":"正则表达式语法","url":"/easysearch/main/docs/features/query-dsl/regex-syntax/"},{"category":null,"content":"正则表达式条件 #  摄取管道支持使用正则表达式（regex）和 Painless 脚本语言进行条件逻辑。这允许根据文本字段的结构和内容对哪些文档进行处理进行精细控制。正则表达式可以用于 if 参数中评估字符串模式。这对于匹配 IP 格式、验证电子邮件地址、识别 UUID 或处理包含特定关键字的日志特别有用。\n示例：电子邮件名过滤 #  以下管道使用正则表达式来识别来自 @example.com 电子邮件域的用户，并相应地标记这些文档：\nPUT _ingest/pipeline/tag_example_com_users { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;user_domain\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;example.com\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;ctx.email != null \u0026amp;\u0026amp; ctx.email =~ /@example.com$/\u0026#34; } } ] } 使用以下请求来模拟管道：\nPOST _ingest/pipeline/tag_example_com_users/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;email\u0026#34;: \u0026#34;alice@example.com\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;email\u0026#34;: \u0026#34;bob@another.com\u0026#34; } } ] } 只有第一份文档添加了 user_domain ：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;email\u0026#34;: \u0026#34;alice@example.com\u0026#34;, \u0026#34;user_domain\u0026#34;: \u0026#34;example.com\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;email\u0026#34;: \u0026#34;bob@another.com\u0026#34; } } } ] } 示例：检测 IPv6 地址 #  以下管道使用正则表达式来识别并标记 IPv6 格式的地址：\nPUT _ingest/pipeline/ipv6_flagger { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;ip_type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;IPv6\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;ctx.ip != null \u0026amp;\u0026amp; ctx.ip =~ /^[a-fA-F0-9:]+$/ \u0026amp;\u0026amp; ctx.ip.contains(\u0026#39;:\u0026#39;)\u0026#34; } } ] } 使用以下请求来模拟管道：\nPOST _ingest/pipeline/ipv6_flagger/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;2001:0db8:85a3:0000:0000:8a2e:0370:7334\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;192.168.0.1\u0026#34; } } ] } 第一个文档包含一个添加的 ip_type 字段，设置为 IPv6 ：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;2001:0db8:85a3:0000:0000:8a2e:0370:7334\u0026#34;, \u0026#34;ip_type\u0026#34;: \u0026#34;IPv6\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;192.168.0.1\u0026#34; } } } ] } 示例：验证 UUID 字符串 #  以下管道使用正则表达式来验证 session_id 字段是否包含有效的 UUID：\nPUT _ingest/pipeline/uuid_checker { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;valid_uuid\u0026#34;, \u0026#34;value\u0026#34;: true, \u0026#34;if\u0026#34;: \u0026#34;ctx.session_id != null \u0026amp;\u0026amp; ctx.session_id =~ /^[a-f0-9]{8}-[a-f0-9]{4}-4[a-f0-9]{3}-[89ab][a-f0-9]{3}-[a-f0-9]{12}$/\u0026#34; } } ] } 使用以下请求来模拟管道：\nPOST _ingest/pipeline/uuid_checker/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;session_id\u0026#34;: \u0026#34;550e8400-e29b-41d4-a716-446655440000\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;session_id\u0026#34;: \u0026#34;invalid-uuid-1234\u0026#34; } } ] } 第一份文档被标记为新的 valid_uuid 字段：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;session_id\u0026#34;: \u0026#34;550e8400-e29b-41d4-a716-446655440000\u0026#34;, \u0026#34;valid_uuid\u0026#34;: true } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;session_id\u0026#34;: \u0026#34;invalid-uuid-1234\u0026#34; } } } ] } ","subcategory":null,"summary":"","tags":null,"title":"正则表达式条件","url":"/easysearch/main/docs/features/ingest-pipelines/conditional-execution/regex-conditionals/"},{"category":null,"content":"本地配置（YAML） #  config/security/ 目录下包含 Easysearch 安全模块的本地 YAML 配置文件。这些文件在 bin/initialize.sh 初始化时自动加载到安全索引中，也可以手动编辑后重新加载。\n通过本地 YAML 配置文件可以管理默认的内置用户或 隐藏的保留资源，例如 admin 管理员用户。除内置资源外，通过 INFINI Console 或 REST API 来创建其他用户、角色、映射和权限组通常更方便。\n相关指南（先读这些） #    安全与多租户最佳实践  权限控制总览   配置文件概览 #     文件 用途 编辑方式     user.yml 内置用户定义 手动编辑或 API   role.yml 角色定义 手动编辑或 API   role_mapping.yml 角色映射 手动编辑或 API   privilege.yml 权限组（Action Group） 手动编辑或 API   config.yml 认证后端配置 需重新加载生效   nodes_dn.yml 节点 DN 白名单 手动编辑或 API   whitelist.yml API 白名单 手动编辑或 API   audit.yml 审计日志配置 手动编辑或 API     user.yml #  此文件是内置用户配置文件，用于定义系统的初始用户账户及其相关配置（如密码哈希、角色分配等）。\nadmin 用户是 Easysearch 系统中的默认超级管理员用户。\n配置文件里面的密码不能是明文，必须使用 Hash 之后的密码，通过命令 ./bin/hash_password.sh -p \u0026lt;new-password\u0026gt; 可以生成一个密码哈希。\n默认配置 #  --- _meta: type: \u0026#34;user\u0026#34; config_version: 2 admin: hash: \u0026quot;$2y$12$GNHStWSkQzhP9LnUbCVv2O2aFx67vXcFHcQvqC.vaE4AR66xAk0zG\u0026quot; reserved: true external_roles: - \u0026quot;admin\u0026quot; description: \u0026quot;Default admin user\u0026quot; 字段说明 #\n    字段 说明     hash bcrypt 密码哈希（使用 bin/hash_password.sh 生成）   reserved 是否为内置用户（true 不可通过 API 删除）   hidden 是否在用户列表中隐藏   external_roles 外部角色列表（用于角色映射）   attributes 自定义用户属性（用于细粒度权限控制）   description 用户描述信息   static 是否为静态用户（true 仅通过文件管理）    修改密码 #  使用密码哈希工具生成新的 bcrypt 哈希：\n# 生成密码哈希 $ES_HOME/bin/hash_password.sh -p newpassword 或使用 API 修改（需要已认证）：\n# 修改用户密码 curl -k -u admin:oldpassword -X PUT \u0026#34;https://localhost:9200/_security/account\u0026#34; \\  -H \u0026#34;Content-Type: application/json\u0026#34; \\  -d \u0026#39;{\u0026#34;password\u0026#34;: \u0026#34;newpassword\u0026#34;}\u0026#39;  role.yml #  定义不同用户角色的权限。replication_leader 和 replication_follower 用于跨集群复制，security 用于允许通过 REST API 管理安全设置。\n ⚠️ 警告！ 以下角色已被弃用，将在未来的版本中删除，请勿使用：\n security_rest_api_access cross_cluster_replication_leader_full_access cross_cluster_replication_follower_full_access   默认配置 #  --- _meta: type: \u0026#34;role\u0026#34; config_version: 2 replication_leader: reserved: true description: \u0026quot;Grants read access to leader indices for cross-cluster replication.\u0026quot; indices: - names: - \u0026quot;*\u0026quot; privileges: - \u0026quot;indices:admin/plugins/replication/index/setup/validate\u0026quot; - \u0026quot;indices:data/read/plugins/replication/changes\u0026quot; - \u0026quot;indices:data/read/plugins/replication/file_chunk\u0026quot;\nreplication_follower: reserved: true description: \u0026quot;Grants manage replication permissions on follower indices.\u0026quot; cluster: - \u0026quot;cluster:admin/plugins/replication/autofollow/update\u0026quot; indices: - names: - \u0026quot;*\u0026quot; privileges: - \u0026quot;indices:admin/plugins/replication/index/setup/validate\u0026quot; - \u0026quot;indices:data/write/plugins/replication/changes\u0026quot; - \u0026quot;indices:admin/plugins/replication/index/start\u0026quot; - \u0026quot;indices:admin/plugins/replication/index/pause\u0026quot; - \u0026quot;indices:admin/plugins/replication/index/resume\u0026quot; - \u0026quot;indices:admin/plugins/replication/index/stop\u0026quot; - \u0026quot;indices:admin/plugins/replication/index/update\u0026quot; - \u0026quot;indices:admin/plugins/replication/index/status_check\u0026quot;\nsecurity: reserved: true description: \u0026quot;Grants access to the security REST API.\u0026quot; 字段说明 #\n    字段 说明     cluster 集群级权限列表（如 cluster:monitor/*、cluster:admin/* 等）   indices 索引级权限列表，每项包含 names（索引模式）和 privileges（允许的操作）   reserved 是否为内置角色   hidden 是否在角色列表中隐藏   description 角色描述信息   static 是否为静态角色（仅通过文件管理）    索引权限项字段 #     字段 说明     names 索引名称模式列表，支持通配符 *   privileges 允许的操作列表，可以是具体权限或权限集合名称   query 文档级安全（DLS），限制可见文档的查询条件   field_security 字段级安全（FLS），限制可见的字段列表   field_mask 字段脱敏，对指定字段进行脱敏处理    自定义角色示例 #  logs_reader: reserved: false description: \u0026#34;Custom role for reading log indices.\u0026#34; cluster: - \u0026#34;cluster:monitor/health\u0026#34; indices: - names: - \u0026#34;logs-*\u0026#34; privileges: - \u0026#34;read\u0026#34; - \u0026#34;search\u0026#34; 常见权限 #  集群权限：\n cluster:* — 所有集群操作 cluster:monitor/* — 监控操作 cluster:admin/* — 管理操作  索引权限：\n indices:admin/* — 索引管理 indices:data/write/* — 写入操作 indices:data/read/* — 读取操作  权限集合名称（在 privileges 中可直接使用）：\n read — 所有读取操作 write — 所有写入操作 search — 搜索操作 get — 获取单个文档 index — 索引文档 crud — 增删改查组合 manage — 索引管理 indices_all — 所有索引操作  完整权限列表参见 权限列表。\n role_mapping.yml #  将用户（users）、外部组/角色（external_roles）以及主机（hosts）映射到 Easysearch 的安全角色。\n各角色的权限定义在 role.yml，此文件仅决定\u0026quot;谁拥有哪些角色\u0026quot;。每个角色条目下，只要当前主体命中 users、external_roles 或 hosts 中的任意一项，即会获得该角色。\n 提示 示例文件中包含一条已弃用的映射（all_access）。请勿再使用此条目，未来版本会移除。\n 默认配置 #  --- _meta: type: \u0026#34;role_mapping\u0026#34; config_version: 2 superuser: reserved: false external_roles: - \u0026quot;admin\u0026quot; description: \u0026quot;Maps admin to superuser\u0026quot; 字段说明 #\n    字段 说明     external_roles 外部角色名称列表（LDAP 组、SAML 角色等）   users 直接映射的用户名列表   and_external_roles 多个外部角色的 AND 条件（必须同时属于所有列出的角色）   hosts 客户端主机名/IP 列表   description 映射描述信息    使用场景 #   LDAP：映射 LDAP 组的 DN 到 Easysearch 角色 SAML：映射 SAML 属性中的角色值 本地用户：将 user.yml 中用户的 external_roles 映射到角色  自定义映射示例 #  data_team: reserved: false external_roles: - \u0026#34;cn=data-team,ou=groups,dc=example,dc=com\u0026#34; users: - \u0026#34;analyst@example.com\u0026#34; description: \u0026#34;Maps data team LDAP group and specific users to data_team role\u0026#34;  privilege.yml #  定义自定义权限组（Action Group），在角色中复用。系统已内置多个常用权限组（如 read、write、search 等），此文件用于定义额外的自定义权限组。\n默认配置 #  除了元数据之外，该文件默认为空，因为安全模块已内置了常用权限集合：\n--- _meta: type: \u0026#34;privilege\u0026#34; config_version: 2 自定义权限组示例 #  SEARCH_ROLE: reserved: false allowed_actions: - \u0026#34;indices:data/read/search*\u0026#34; - \u0026#34;indices:data/read/get*\u0026#34; WRITE_ROLE: reserved: false allowed_actions: - \u0026quot;indices:data/write/index*\u0026quot; - \u0026quot;indices:data/write/update*\u0026quot; 在角色中使用 #\n 在 role.yml 的 privileges 中引用权限组名称：\nlogs_writer: indices: - names: - \u0026#34;logs-*\u0026#34; privileges: - \u0026#34;SEARCH_ROLE\u0026#34; - \u0026#34;WRITE_ROLE\u0026#34;  config.yml #  配置认证方式（Basic Auth、LDAP、SAML 等）和授权后端。详细说明请参见 认证后端配置。\n默认配置 #  --- _meta: type: \u0026#34;config\u0026#34; config_version: 2 config: dynamic: http: anonymous_auth_enabled: false # xff: # enabled: false # internalProxies: '192.168.0.10|192.168.0.11' # regex pattern\n\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;authc\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;basic_internal_auth_domain\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;description\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Authenticate via HTTP Basic against internal users database\u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;http_enabled\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;transport_enabled\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;order\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#bd93f9\u0026quot;\u0026gt;4\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;http_authenticator\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;type\u0026lt;/span\u0026gt;: basic \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;challenge\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;authentication_backend\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;type\u0026lt;/span\u0026gt;: internal \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;authz\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;basic_authz\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;http_enabled\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;transport_enabled\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;authorization_backend\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;type\u0026lt;/span\u0026gt;: noop  关键部分 #\n    配置块 说明     http HTTP 层设置，包括匿名认证和 XFF 代理配置   authc 认证域配置，支持 basic、proxy、kerberos、clientcert、jwt 等类型   authz 授权后端配置，支持 noop、ldap 等类型     nodes_dn.yml #  定义允许加入集群的节点证书 DN（Distinguished Name）列表，与 easysearch.yml 中的 security.nodes_dn 作用相同，用于节点间 TLS 认证。\n默认配置 #  --- _meta: type: \u0026#34;nodesdn\u0026#34; config_version: 2 # Define nodesdn mapping name and corresponding values # cluster1: # nodes_dn: # - CN=*.example.com 默认文件为空，表示不做额外的节点 DN 限制。\n whitelist.yml #  限制非超级管理员用户可以访问的 API 端点。\n默认配置 #  --- _meta: type: \u0026#34;whitelist\u0026#34; config_version: 2 config: enabled: false requests: /_cluster/settings: - GET /_cat/nodes: - GET 字段说明 #\n    字段 说明     enabled 是否启用白名单功能。false 表示关闭白名单，所有 API 均可访问   requests 白名单规则，键为 API 路径，值为允许的 HTTP 方法列表     注意：白名单仅对非超级管理员用户生效。超级管理员（通过 admin 证书认证）始终可以访问所有 API。\n  audit.yml #  配置安全审计日志的记录范围和行为。\n默认配置 #  --- _meta: type: \u0026#34;audit\u0026#34; config_version: 2 config: enabled: true audit: enable_rest: true disabled_rest_categories: - AUTHENTICATED - GRANTED_PRIVILEGES enable_transport: true disabled_transport_categories: - AUTHENTICATED - GRANTED_PRIVILEGES ignore_users: [] ignore_requests: [] resolve_bulk_requests: false log_request_body: true resolve_indices: true exclude_sensitive_headers: true compliance: enabled: true internal_config: true external_config: false read_metadata_only: true read_watched_fields: {} read_ignore_users: [] write_metadata_only: true write_log_diffs: false write_watched_indices: [] write_ignore_users: [] 审计配置字段 #\n    字段 说明     enabled 是否启用审计日志   enable_rest 是否审计 REST API 请求   enable_transport 是否审计传输层请求   disabled_rest_categories REST 层排除的审计类别   disabled_transport_categories 传输层排除的审计类别   include_users 仅审计指定用户列表（可选，支持通配符）   ignore_users 排除审计的用户列表（支持通配符）   ignore_requests 排除审计的请求列表（支持通配符）   resolve_bulk_requests 是否记录 bulk 请求中的每个操作   log_request_body 是否记录请求体   resolve_indices 是否解析请求涉及的索引（含别名与通配符）   exclude_sensitive_headers 是否排除敏感 HTTP 头（如 Authorization）    合规性配置字段 #     字段 说明     enabled 是否启用合规审计   internal_config 是否记录安全配置的内部变更   external_config 是否记录外部配置文件的变更   read_metadata_only 读取事件是否仅记录元数据   read_watched_fields 监控读取的索引和字段   read_ignore_users 读取审计忽略用户列表   write_metadata_only 写入事件是否仅记录元数据   write_log_diffs 是否记录文档更新的差异   write_watched_indices 监控写入的索引列表   write_ignore_users 写入审计忽略用户列表     修改配置文件 #  方式 1：手动编辑（需重新加载） #  修改 config/security/ 目录下的 YAML 文件后，需要重新初始化安全索引使配置生效：\n# 编辑配置 vi config/security/config.yml # 重新加载安全配置（以管理员身份删除安全索引后重启） curl -XDELETE -k \u0026ndash;cert admin.crt \u0026ndash;key admin.key 'https://localhost:9200/.security' # 然后重启 Easysearch 服务 也可以使用初始化脚本：\nbin/initialize.sh 方式 2：使用 API（即时生效） #  大多数配置可以通过安全 REST API 修改，即时生效：\n# 获取现有角色 curl -k -u admin:password \u0026#34;https://localhost:9200/_security/role/my_role\u0026#34; # 创建或修改角色 curl -k -u admin:password -X PUT \u0026quot;https://localhost:9200/_security/role/my_role\u0026quot;  -H \u0026quot;Content-Type: application/json\u0026quot;  -d '{ \u0026quot;cluster\u0026quot;: [\u0026quot;cluster:monitor/health\u0026quot;], \u0026quot;indices\u0026quot;: [{ \u0026quot;names\u0026quot;: [\u0026quot;logs-*\u0026quot;], \u0026quot;privileges\u0026quot;: [\u0026quot;read\u0026quot;, \u0026quot;search\u0026quot;] }] }'\n# 创建用户 curl -k -u admin:password -X PUT \u0026quot;https://localhost:9200/_security/user/newuser\u0026quot;  -H \u0026quot;Content-Type: application/json\u0026quot;  -d '{\u0026quot;password\u0026quot;: \u0026quot;StrongPass123!\u0026quot;, \u0026quot;external_roles\u0026quot;: [\u0026quot;admin\u0026quot;]}' \n注意 任何对本地配置文件的修改（如 user.yml、role.yml、role_mapping.yml），修改后必须以管理员身份删除 .security 索引，然后重启服务才能生效：\ncurl -XDELETE -k --cert admin.crt --key admin.key \u0026#39;https://localhost:9200/.security\u0026#39;   文件权限 #  安全配置文件包含敏感信息（如密码哈希），应该限制访问权限：\n# 只有 Easysearch 进程用户可以读取 chmod 600 config/security/*.yml # 配置目录权限 chmod 700 config/security/ \n","subcategory":null,"summary":"","tags":null,"title":"本地配置","url":"/easysearch/main/docs/operations/security/configuration/yaml/"},{"category":null,"content":"Letter 分词器 #  letter 分词器会根据任何非字母字符将文本拆分成单词。它在处理许多欧洲语种时效果良好，但对于一些亚洲语种却不太有效，因为在亚洲语种中，单词之间不是由空格来分隔的。\n相关指南（先读这些） #    文本分析：识别词元  文本分析基础  参考样例 #  下面的示例请求创建了一个名为 my_index 的新索引，并配置了一个使用字母词元生成器的分词器。\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_letter_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;letter\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_letter_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST _analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;letter\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Cats 4EVER love chasing butterflies!\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;Cats\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;EVER\u0026#34;, \u0026#34;start_offset\u0026#34;: 6, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;love\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;chasing\u0026#34;, \u0026#34;start_offset\u0026#34;: 16, \u0026#34;end_offset\u0026#34;: 23, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;butterflies\u0026#34;, \u0026#34;start_offset\u0026#34;: 24, \u0026#34;end_offset\u0026#34;: 35, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"字母分词器（Letter）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/letter/"},{"category":null,"content":"Uppercase 分词过滤器 #  uppercase 分词过滤器用于在分析过程中将所有词元（单词）转换为大写形式。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参考样例 #  以下示例请求创建了一个名为 uppercase_example 的新索引，并配置了一个带有大写过滤器的分词器。\nPUT /uppercase_example { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;uppercase_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;uppercase\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;uppercase_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;uppercase_filter\u0026#34; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /uppercase_example/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;uppercase_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch is powerful\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;EASYSEARCH\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;IS\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;POWERFUL\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 22, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"大写分词过滤器（Uppercase）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/uppercase/"},{"category":null,"content":"Get API #  按 ID 检索单条文档，返回完整 _source 及元数据。\n请求格式 #  GET /\u0026lt;index\u0026gt;/_doc/\u0026lt;_id\u0026gt; HEAD /\u0026lt;index\u0026gt;/_doc/\u0026lt;_id\u0026gt; 仅返回 _source（不含元数据）：\nGET /\u0026lt;index\u0026gt;/_source/\u0026lt;_id\u0026gt; HEAD /\u0026lt;index\u0026gt;/_source/\u0026lt;_id\u0026gt; 路径参数 #     参数 必需 说明     \u0026lt;index\u0026gt; 是 目标索引   \u0026lt;_id\u0026gt; 是 文档 ID    查询参数 #     参数 类型 默认值 说明     _source string/boolean true true/false 启用或禁用 _source 返回，或传逗号分隔字段名仅返回指定字段   _source_includes string — _source 中要包含的字段（逗号分隔，支持通配符）   _source_excludes string — _source 中要排除的字段（逗号分隔，支持通配符）   stored_fields string — 要返回的 stored 字段（逗号分隔）   routing string — 自定义路由值。若文档写入时使用了自定义路由，检索时必须指定相同的值   preference string — 查询偏好。_local = 优先本地分片；_primary = 仅主分片；或自定义字符串   realtime boolean true 实时读取。为 true 时不依赖刷新即可读到最新写入   refresh boolean false 读取前是否强制刷新   version long — 期望的版本号，不匹配时返回 409   version_type string internal 版本类型    示例 #  获取完整文档 #  GET /website/_doc/123 响应：\n{ \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Just trying this out...\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2014/01/01\u0026#34; } } 文档不存在时 found 为 false，HTTP 状态码 404。\n只返回部分字段 #  GET /website/_doc/123?_source_includes=title,date 只返回 _source #  GET /website/_source/123 直接返回 JSON 文档内容，不含 _index、_version 等元数据。\n检查文档是否存在 #  HEAD /website/_doc/123  存在 → 200 OK 不存在 → 404 Not Found  不传输 _source，适合只需判断存在性的场景。\n 参考导航 #     需求 参见     批量获取多条文档  Multi Get API   写入文档  Index API   _source 字段控制  _source 与字段存储    ","subcategory":null,"summary":"","tags":null,"title":"Get API","url":"/easysearch/main/docs/features/document-operations/get-api/"},{"category":null,"content":"配置 #  Easysearch 默认配置已针对多数场景优化。本文聚焦生产环境必须关注的配置项。\n 原则：如果不确定是否需要改，就不要改。过度调优往往适得其反。\n  配置速查表 #     类别 关键配置 修改方式 风险等级     标识 cluster.name、node.name 静态（重启生效） 低   路径 path.data、path.logs 静态（重启生效） 高   网络 network.host、http.port、transport.port 静态（重启生效） 中   发现 discovery.seed_hosts、cluster.initial_master_nodes 静态（重启生效） 高   内存 ES_HEAP_SIZE、jvm.options 静态（重启生效） 高   恢复 gateway.recover_after_* 静态（重启生效） 中   集群级 大部分 cluster.*、indices.* 动态 API 视具体配置     必须配置 #  集群和节点名称 #  # easysearch.yml cluster.name: my-cluster-prod # 避免节点误加入其他集群 node.name: node-01 # 便于日志排查和监控识别  不设置 node.name 时，每次重启会随机生成——凌晨 3 点排查问题时，你不会想面对一堆随机名称。\n 数据目录 #  # easysearch.yml path.data: /data/easysearch # 独立于安装目录 path.logs: /var/log/easysearch # 便于日志收集 为什么重要：默认数据存放在安装目录下，升级或重装时可能被误删。\n网络绑定 #  # easysearch.yml network.host: 10.0.0.1 # 内网 IP http.port: 9200 transport.port: 9300 安全建议：\n 生产环境不要绑定 0.0.0.0 或公网 IP 通过 Nginx/Gateway 代理对外暴露  集群发现 #  # easysearch.yml discovery.seed_hosts: - node-01.internal:9300 - node-02.internal:9300 - node-03.internal:9300 # 仅首次启动集群时需要 cluster.initial_master_nodes:\n node-01 node-02 node-03 关键点：\n   列出所有 Master 候选节点 cluster.initial_master_nodes 在集群建立后可以删除 永远不要使用组播——生产环境只用单播   内存配置 #  JVM 堆设置 #  # 方式 1：环境变量 export ES_HEAP_SIZE=16g # 方式 2：jvm.options -Xms16g -Xmx16g 三条铁律：\n   规则 说明     Xms = Xmx 避免运行时调整堆大小   堆 ≤ 物理内存的 50% 另一半留给 Lucene 文件缓存   堆 ≤ 31 GB 超过 32GB 会禁用指针压缩，得不偿失    典型配置：\n   物理内存 JVM 堆 说明     16 GB 8 GB 开发/测试   64 GB 31 GB 生产推荐   128 GB 31 GB 堆不要超过 31GB    禁用 Swap #  内存交换到磁盘会让性能下降 100 倍以上。\n# 临时禁用 sudo swapoff -a # 或降低 swappiness echo \u0026quot;vm.swappiness = 1\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p 也可以锁定 JVM 内存：\n# easysearch.yml bootstrap.memory_lock: true 验证是否生效：\nGET _nodes?filter_path=**.mlockall // 返回 \u0026#34;mlockall\u0026#34;: true 表示成功  集群恢复配置 #  集群重启时，避免在所有节点上线前触发不必要的分片迁移。\n# easysearch.yml gateway.recover_after_nodes: 8 # 至少 8 个节点在线才开始恢复 gateway.expected_nodes: 10 # 集群预期节点数 gateway.recover_after_time: 5m # 最多等待 5 分钟 行为：\n 等待至少 8 个节点上线 如果 5 分钟内 10 个节点都上线了，立即开始恢复 如果 5 分钟后只有 8-9 个节点，也开始恢复  效果：大集群重启时，分片恢复从数小时缩短到几秒钟。\n 动态配置 #  运行时可通过 API 调整的配置，无需重启。\n查看当前配置 #  GET _cluster/settings?include_defaults=true 修改配置 #  PUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.enable\u0026#34;: \u0026#34;all\u0026#34; } } 配置优先级（从高到低）：\n Transient（临时，重启后失效） Persistent（持久，写入集群状态） easysearch.yml 默认值  常用动态配置 #     配置 用途 示例值     cluster.routing.allocation.enable 控制分片分配 all / none   cluster.routing.rebalance.enable 控制分片再平衡 all / none   indices.recovery.max_bytes_per_sec 恢复带宽限制 100mb   cluster.routing.allocation.disk.watermark.low 磁盘低水位 85%   cluster.routing.allocation.disk.watermark.high 磁盘高水位 90%     日志配置 #  日志通过 config/log4j2.properties 配置。\n调整日志级别 #  # 临时调试时调高级别 logger.action.level = debug 问题解决后恢复 logger.action.level = info 慢查询日志 #\n PUT /my-index/_settings { \u0026#34;index.search.slowlog.threshold.query.warn\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;index.search.slowlog.threshold.query.info\u0026#34;: \u0026#34;5s\u0026#34; } 最佳实践：\n 默认保持 INFO 级别 使用 Filebeat 收集日志到集中平台 不要在节点本地保留过多历史日志   配置验证 #  节点启动后检查 #  # 检查节点信息 GET _nodes?filter_path=nodes.*.name,nodes.*.roles,nodes.*.jvm.mem 检查关键设置 GET _nodes/settings?filter_path=nodes..settings.path,nodes..settings.network 常见问题排查 #\n    问题 可能原因 检查命令     节点无法加入集群 发现配置错误 检查 discovery.seed_hosts   内存不足 堆设置不当 GET _nodes/stats/jvm   分片不分配 磁盘水位超限 GET _cluster/allocation/explain   启动慢 恢复配置缺失 检查 gateway.* 配置     小结 #     配置类别 必须设置 推荐值     集群名称 ✓ 有意义的名称   节点名称 ✓ 有意义的名称   数据路径 ✓ 独立于安装目录   网络绑定 ✓ 内网 IP   发现配置 ✓ 显式 seed 列表   JVM 堆 ✓ 50% 内存，≤ 31GB   禁用 Swap ✓ swapoff -a 或 memory_lock    相关文档：\n  节点配置：完整参数说明  集群配置：动态配置详解  容量规划：硬件选型指南  监控：配置监控告警  ","subcategory":null,"summary":"","tags":null,"title":"集群配置","url":"/easysearch/main/docs/operations/configuration/"},{"category":null,"content":"模拟管道 #  使用 _simulate API 测试管道效果，不会实际索引文档。这是调试管道逻辑的最佳工具。\n请求格式 #  模拟已创建的管道：\nPOST _ingest/pipeline/\u0026lt;pipeline-id\u0026gt;/_simulate 在请求体中内联定义管道进行模拟（无需先创建）：\nPOST _ingest/pipeline/_simulate 请求内容字段 #  下表列出了用于模拟管道的请求正文字段。\n   参数名 是否必需 类型 描述     docs 必需的 数组 用于测试管道的文档内容。   pipeline 可选 对象 要模拟的管道。如果未包含管道标识符，则模拟使用的是最新创建的管道。    docs 字段可以包含下表中列出的子字段。\n   参数名 是否必需 类型 描述     source 必需的 对象 文档的 JSON 正文。   id 可选 字符串 一个独特的文档标识符。该标识符不能在索引的其它地方使用。   index 可选 字符串 文档的转换数据出现的索引。    查询参数 #  下表列出了模拟管道的查询参数。\n   参数名 类型 描述     verbose 布尔值 详细模式。在执行的管道中显示每个处理器的数据输出。    示例：指定路径中的管道 #  POST /_ingest/pipeline/my-pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;grad_year\u0026#34;: 2024, \u0026#34;graduated\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;grad_year\u0026#34;: 2025, \u0026#34;graduated\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;Jane Doe\u0026#34; } } ] } 返回内容如下：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;JOHN DOE\u0026#34;, \u0026#34;grad_year\u0026#34;: 2023, \u0026#34;graduated\u0026#34;: true }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-06-20T23:19:54.635306588Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;JANE DOE\u0026#34;, \u0026#34;grad_year\u0026#34;: 2023, \u0026#34;graduated\u0026#34;: true }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-06-20T23:19:54.635746046Z\u0026#34; } } } ] } 示例：详细模式 #  当使用 verbose 参数设置为 true 运行上一个请求时，响应将显示每个文档的转换序列。例如，对于 ID 为 1 的文档，响应包含按顺序应用管道中每个处理器的结果：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;processor_results\u0026#34;: [ { \u0026#34;processor_type\u0026#34;: \u0026#34;set\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Sets the graduation year to 2023\u0026#34;, \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;grad_year\u0026#34;: 2023, \u0026#34;graduated\u0026#34;: false }, \u0026#34;_ingest\u0026#34;: { \u0026#34;pipeline\u0026#34;: \u0026#34;my-pipeline\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2023-06-20T23:23:26.656564631Z\u0026#34; } } }, { \u0026#34;processor_type\u0026#34;: \u0026#34;set\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Sets \u0026#39;graduated\u0026#39; to true\u0026#34;, \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;grad_year\u0026#34;: 2023, \u0026#34;graduated\u0026#34;: true }, \u0026#34;_ingest\u0026#34;: { \u0026#34;pipeline\u0026#34;: \u0026#34;my-pipeline\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2023-06-20T23:23:26.656564631Z\u0026#34; } } }, { \u0026#34;processor_type\u0026#34;: \u0026#34;uppercase\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;JOHN DOE\u0026#34;, \u0026#34;grad_year\u0026#34;: 2023, \u0026#34;graduated\u0026#34;: true }, \u0026#34;_ingest\u0026#34;: { \u0026#34;pipeline\u0026#34;: \u0026#34;my-pipeline\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2023-06-20T23:23:26.656564631Z\u0026#34; } } } ] } ] } 示例：在请求体中指定管道 #  或者，您可以直接在请求体中指定一个管道，而无需先创建一个管道：\nPOST /_ingest/pipeline/_simulate { \u0026#34;pipeline\u0026#34; : { \u0026#34;description\u0026#34;: \u0026#34;Splits text on white space characters\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;csv\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;name\u0026#34;, \u0026#34;separator\u0026#34;: \u0026#34;,\u0026#34;, \u0026#34;target_fields\u0026#34;: [\u0026#34;last_name\u0026#34;, \u0026#34;first_name\u0026#34;], \u0026#34;trim\u0026#34;: true } }, { \u0026#34;uppercase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;last_name\u0026#34; } } ] }, \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;second-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Doe,John\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;second-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Doe, Jane\u0026#34; } } ] } 返回内容如下：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;second-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Doe,John\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;DOE\u0026#34;, \u0026#34;first_name\u0026#34;: \u0026#34;John\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-08-24T19:20:44.816219673Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;second-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Doe, Jane\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;DOE\u0026#34;, \u0026#34;first_name\u0026#34;: \u0026#34;Jane\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-08-24T19:20:44.816492381Z\u0026#34; } } } ] } ","subcategory":null,"summary":"","tags":null,"title":"模拟管道","url":"/easysearch/main/docs/features/ingest-pipelines/simulate-ingest/"},{"category":null,"content":"Standard 分词器 #  standard 分词器是 Easysearch 中的默认分词器。它基于单词边界，采用一种基于语法的方法对文本进行分词，这种方法能够识别字母、数字以及标点等其他字符。它具有高度的通用性，适用于多种语言，它使用了 Unicode 文本分割规则（ UAX#29）来将文本分割成词元。\n相关指南（先读这些） #    词汇识别  文本分析基础  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用标准词元生成器的分词器：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_standard_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;standard\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_standard_analyzer\u0026#34; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_standard_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Easysearch is powerful, fast, and scalable.\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;easysearch\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34;: 11, \u0026#34;end_offset\u0026#34;: 13, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;powerful\u0026#34;, \u0026#34;start_offset\u0026#34;: 14, \u0026#34;end_offset\u0026#34;: 22, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;fast\u0026#34;, \u0026#34;start_offset\u0026#34;: 24, \u0026#34;end_offset\u0026#34;: 28, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;and\u0026#34;, \u0026#34;start_offset\u0026#34;: 30, \u0026#34;end_offset\u0026#34;: 33, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 4 }, { \u0026#34;token\u0026#34;: \u0026#34;scalable\u0026#34;, \u0026#34;start_offset\u0026#34;: 34, \u0026#34;end_offset\u0026#34;: 42, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 5 } ] } 参数说明 #  标准词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     max_token_length 可选 整数 设置生成词元的最大长度。若超过该长度，词元将按照 max_token_length 所配置的长度拆分为多个词元。默认值为 255。    ","subcategory":null,"summary":"","tags":null,"title":"标准分词器（Standard）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/standard/"},{"category":null,"content":"Standard 分析器 #  standard 分析器是在未指定其他分析器时默认使用的分析器。它旨在为通用文本处理提供一种基础且高效的方法。\n该分析器由以下分词器和分词过滤器组成：\n standard 分词器：去除大部分标点符号，并依据空格和其他常见分隔符对文本进行分割。 lowercase 分词过滤器：将所有词元转换为小写，以确保匹配时不区分大小写。 stop 分词过滤器：从分词后的输出中移除常见的停用词，例如 \u0026ldquo;the\u0026rdquo;、\u0026ldquo;is\u0026rdquo; 和 \u0026ldquo;and\u0026rdquo;。  相关指南（先读这些） #    文本分析基础  文本分析：识别词元  参考样例 #  以下命令创建一个名为 my_standard_index 并使用标准分词器的索引：\nPUT /my_standard_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; } } } } 参数说明 #  你可以使用以下参数来配置标准分词器。\n   参数 必填/可选 数据类型 描述     max_token_length 可选 整数 设置生成的词元的最大长度。如果超过此长度，词元将按照 max_token_length 中配置的长度拆分为多个词元。默认值为 255。   stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。   stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。    配置自定义分词器 #  以下命令配置一个索引，该索引带有一个等同于标准分词器的自定义分词器：\nPUT /my_custom_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;stop\u0026#34; ] } } } } } 产生的词元 #  以下请求用来检查分词器生成的词元：\nPOST /my_custom_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The slow turtle swims away\u0026#34; } 返回内容中包含了产生的词元\n{ \u0026#34;tokens\u0026#34;: [ {\u0026#34;token\u0026#34;: \u0026#34;slow\u0026#34;,\u0026#34;start_offset\u0026#34;: 4,\u0026#34;end_offset\u0026#34;: 8,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 1}, {\u0026#34;token\u0026#34;: \u0026#34;turtle\u0026#34;,\u0026#34;start_offset\u0026#34;: 9,\u0026#34;end_offset\u0026#34;: 15,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 2}, {\u0026#34;token\u0026#34;: \u0026#34;swims\u0026#34;,\u0026#34;start_offset\u0026#34;: 16,\u0026#34;end_offset\u0026#34;: 21,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 3}, {\u0026#34;token\u0026#34;: \u0026#34;away\u0026#34;,\u0026#34;start_offset\u0026#34;: 22,\u0026#34;end_offset\u0026#34;: 26,\u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;,\u0026#34;position\u0026#34;: 4} ] } ","subcategory":null,"summary":"","tags":null,"title":"标准分析器（Standard）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/standard-analyzer/"},{"category":null,"content":"查询字符串语法 #  Easysearch 的 query_string 和 simple_query_string 查询底层使用 Lucene 查询解析器（Query Parser）将查询字符串解析为查询对象。本页详细描述查询解析器支持的完整语法。\n相关指南 #    Query String 查询 — query_string 查询的 API 参数  Simple Query String 查询 — 更宽松的查询字符串语法  正则表达式语法 — regexp 查询和 query_string 中使用的正则语法   词项（Terms） #  查询字符串由词项和操作符组成。词项有两种类型：\n 单个词项：一个单词，如 test、hello 短语：用双引号包围的一组词，如 \u0026quot;hello dolly\u0026quot;  多个词项可以通过布尔操作符组合成更复杂的查询。\n 注意：查询字符串中的词项和短语会经过索引时使用的相同分析器处理。因此，选择不会干扰查询词项的分析器非常重要。\n  字段（Fields） #  Easysearch 支持按字段搜索。搜索时可以指定字段名，也可以使用默认字段。\n语法为字段名后跟冒号 :，然后是要搜索的词项：\ntitle:\u0026#34;The Right Way\u0026#34; AND content:go 上面的查询在 title 字段中查找短语 \u0026ldquo;The Right Way\u0026rdquo;，在 content 字段中查找词 \u0026ldquo;go\u0026rdquo;。\n 注意：字段指示符仅对其直接前面的词项有效。例如：\ntitle:Do it right 只有 \u0026ldquo;Do\u0026rdquo; 在 title 字段中搜索，\u0026ldquo;it\u0026rdquo; 和 \u0026ldquo;right\u0026rdquo; 将在默认字段中搜索。\n  词项修饰符 #  通配符搜索 #  支持单字符和多字符通配符：\n   通配符 含义 示例     ? 匹配任意单个字符 te?t → 匹配 test、text   * 匹配零个或多个字符 test* → 匹配 test、tests、tester    通配符可以出现在词项中间：\nte*t  注意：不能将 * 或 ? 作为搜索词的第一个字符（性能原因）。如果需要左通配符匹配，考虑使用 通配符字段类型（Wildcard）。\n 模糊搜索 #  模糊搜索基于 Damerau-Levenshtein 编辑距离算法，能找到拼写相似的词项。在词项末尾加波浪号 ~ 即可启用：\nroam~ 这会匹配 foam、roams 等拼写相近的词。\n可以指定编辑距离参数（0~2），值越大匹配越宽松：\nroam~1 默认编辑距离为 2。\n邻近搜索 #  邻近搜索查找彼此在指定距离内的词。在短语末尾加 ~ 和距离值：\n\u0026#34;jakarta apache\u0026#34;~10 这会查找 \u0026ldquo;jakarta\u0026rdquo; 和 \u0026ldquo;apache\u0026rdquo; 在 10 个词距离内出现的文档。\n范围搜索 #  范围搜索匹配字段值在指定上下界之间的文档：\n   语法 含义 示例     [min TO max] 包含边界（闭区间） date:[2020-01-01 TO 2020-12-31]   {min TO max} 排除边界（开区间） title:{Aida TO Carmen}   [min TO max} 左闭右开 count:[1 TO 100}   {min TO max] 左开右闭 count:{0 TO 100]    范围搜索不限于日期字段，也适用于数值和字符串字段：\nage:[18 TO 65] title:{Alpha TO Omega} 使用通配符表示无上限或无下限：\nage:[18 TO *] age:{* TO 65] 权重提升（Boosting） #  使用 ^ 符号和提升因子来控制词项的相关性权重：\njakarta^4 apache 这会使包含 \u0026ldquo;jakarta\u0026rdquo; 的文档更相关。短语也可以提升权重：\n\u0026#34;jakarta apache\u0026#34;^4 \u0026#34;Apache Lucene\u0026#34; 默认提升因子为 1。提升因子必须为正数，但可以小于 1（如 0.2）。\n 布尔操作符 #  布尔操作符用于组合多个词项的逻辑关系。\n 注意：布尔操作符关键字必须全部大写。\n OR（默认操作符） #  OR 是默认的连接操作符。如果两个词项之间没有布尔操作符，则使用 OR。可以用 || 替代：\n\u0026#34;jakarta apache\u0026#34; jakarta \u0026#34;jakarta apache\u0026#34; OR jakarta 匹配包含任意一个词项的文档。\nAND #  AND 操作符要求两个词项都存在于文档中。可以用 \u0026amp;\u0026amp; 替代：\n\u0026#34;jakarta apache\u0026#34; AND \u0026#34;Apache Lucene\u0026#34; NOT #  NOT 操作符排除包含指定词项的文档。可以用 ! 替代：\n\u0026#34;jakarta apache\u0026#34; NOT \u0026#34;Apache Lucene\u0026#34;  注意：NOT 不能单独使用。以下查询不会返回任何结果：\nNOT \u0026#34;jakarta apache\u0026#34;  必须出现（+） #  + 操作符要求其后的词项必须存在于文档中：\n+jakarta lucene 匹配必须包含 \u0026ldquo;jakarta\u0026rdquo;、可选包含 \u0026ldquo;lucene\u0026rdquo; 的文档。\n必须不出现（-） #  - 操作符排除包含其后词项的文档：\n\u0026#34;jakarta apache\u0026#34; -\u0026#34;Apache Lucene\u0026#34; 操作符总结 #     操作符 等价符号 含义     OR \\|\\| 任一词项匹配即可（默认）   AND \u0026amp;\u0026amp; 所有词项都必须匹配   NOT ! 排除包含该词项的文档   + — 该词项必须出现   - — 该词项必须不出现     分组（Grouping） #  使用括号 () 对子句进行分组，控制布尔逻辑的优先级：\n(jakarta OR apache) AND website 这确保 \u0026ldquo;website\u0026rdquo; 必须存在，同时 \u0026ldquo;jakarta\u0026rdquo; 或 \u0026ldquo;apache\u0026rdquo; 至少存在一个。\n字段分组 #  对同一字段的多个子句进行分组：\ntitle:(+return +\u0026#34;pink panther\u0026#34;) 这会在 title 字段中查找同时包含 \u0026ldquo;return\u0026rdquo; 和 \u0026ldquo;pink panther\u0026rdquo; 的文档。\n 转义特殊字符 #  以下字符在查询语法中有特殊含义，如果要按字面值匹配，需要用反斜杠 \\ 转义：\n+ - \u0026amp;\u0026amp; || ! ( ) { } [ ] ^ \u0026#34; ~ * ? : \\ 例如，搜索 (1+1):2：\n\\(1\\+1\\)\\:2  在 Easysearch 中使用 #  query_string 查询 #  GET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;default_field\u0026#34;: \u0026#34;content\u0026#34;, \u0026#34;query\u0026#34;: \u0026#34;title:(Easysearch AND 教程) AND date:[2025-01-01 TO 2025-12-31]\u0026#34; } } } simple_query_string 查询 #  simple_query_string 使用简化的语法，不会因语法错误而报错：\n   操作符 query_string simple_query_string     AND AND 或 \u0026amp;\u0026amp; +   OR OR 或 \\|\\| \\|   NOT NOT 或 ! -   短语 \u0026quot;...\u0026quot; \u0026quot;...\u0026quot;   前缀 term* term*   模糊 term~N term~N   邻近 \u0026quot;...\u0026quot;~N \u0026quot;...\u0026quot;~N   分组 (...) (...)   字段指定 field:term ❌ 不支持   范围 [min TO max] ❌ 不支持   正则 /pattern/ ❌ 不支持   提升 term^N ❌ 不支持    常用查询示例 #  # 多字段搜索 title:Easysearch AND content:\u0026#34;搜索引擎\u0026#34; 模糊匹配 + 邻近搜索 serach~1 AND \u0026quot;全文 搜索\u0026quot;~3\n范围 + 通配符 date:[2025-01-01 TO ] AND author:张\n必须/可选/排除 +Easysearch +搜索 -Elasticsearch\n分组 + 权重提升 (title:搜索引擎^3 OR content:搜索引擎) AND status:published \n注意事项 #     注意项 说明     大小写 布尔操作符（AND、OR、NOT）必须全大写，否则被当作普通词项   分析器 查询字符串中的词项会经过字段的分析器处理，确保分析器配置正确   默认操作符 默认使用 OR，可以通过 default_operator 参数改为 AND   错误处理 query_string 对语法错误严格报错，面向终端用户建议使用 simple_query_string   性能 避免前导通配符（*keyword），会触发全量词项扫描   嵌套文档 query_string 不会返回嵌套文档，需使用 nested 查询    ","subcategory":null,"summary":"","tags":null,"title":"查询字符串语法","url":"/easysearch/main/docs/features/query-dsl/query-parser-syntax/"},{"category":null,"content":"控制条件和处理器 #  在摄取管道中， pipeline 处理器允许根据文档内容条件性地执行不同的子管道。当不同类型的文档需要单独的处理逻辑时，这提供了强大的灵活性。您可以使用 if 参数在 pipeline 处理器中根据字段值、数据类型或内容结构将文档路由到不同的管道。然后，每个管道可以独立应用自己的处理器集。这种方法通过仅在相关位置应用逻辑，保持了管道的模块化和可维护性。\n示例：按服务路由日志 #  以下示例演示了如何根据文档中的 service.name 字段将日志路由到不同的子管道。\n创建第一个名为 webapp_logs 的管道：\nPUT _ingest/pipeline/webapp_logs { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;log_type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;webapp\u0026#34; } } ] } 创建第二个名为 api_logs 的管道：\nPUT _ingest/pipeline/api_logs { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;log_type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;api\u0026#34; } } ] } 创建主路由管道名为 service_router ，根据 service.name 将文档路由到相应的管道：\nPUT _ingest/pipeline/service_router { \u0026#34;processors\u0026#34;: [ { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;webapp_logs\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;ctx.service?.name == \u0026#39;webapp\u0026#39;\u0026#34; } }, { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;api_logs\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;ctx.service?.name == \u0026#39;api\u0026#39;\u0026#34; } } ] } 使用以下请求来模拟管道：\nPOST _ingest/pipeline/service_router/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;service\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;webapp\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;Homepage loaded\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;service\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;api\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;GET /v1/users\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;service\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;worker\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;Task started\u0026#34; } } ] } 返回内容中，第一份文档由 webapp_logs 管道处理，第二份文档由 api_logs 管道处理。第三份文档保持不变，因为它不匹配任何条件：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;log_type\u0026#34;: \u0026#34;webapp\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Homepage loaded\u0026#34;, \u0026#34;service\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;webapp\u0026#34; } }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-24T10:54:12.555447087Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;log_type\u0026#34;: \u0026#34;api\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;GET /v1/users\u0026#34;, \u0026#34;service\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;api\u0026#34; } }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-24T10:54:12.55548442Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Task started\u0026#34;, \u0026#34;service\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;worker\u0026#34; } }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-24T10:54:12.555490754Z\u0026#34; } } } ] } 示例：数据类型处理 #\n 您还可以使用管道处理器应用特定类型的管道。以下管道将日志根据 code 字段是否为数字，分别发送到 numeric_handler 和 string_handler 。如果它是类型 String ，则发送到 string_handler 。\n创建第一个名为 numeric_handler 的管道：\nPUT _ingest/pipeline/numeric_handler { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;code_type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;numeric\u0026#34; } } ] } 创建第二个名为 string_handler 的管道：\nPUT _ingest/pipeline/string_handler { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;code_type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;string\u0026#34; } } ] } 创建名为 type_router 的主路由管道，根据 code 字段将文档路由到相应的管道：\nPUT _ingest/pipeline/type_router { \u0026#34;processors\u0026#34;: [ { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;numeric_handler\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;ctx.code instanceof Integer || ctx.code instanceof Long || ctx.code instanceof Double\u0026#34; } }, { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;string_handler\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;ctx.code instanceof String\u0026#34; } } ] } 使用以下请求来模拟管道：\nPOST _ingest/pipeline/type_router/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;code\u0026#34;: 404 } }, { \u0026#34;_source\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;ERR_NOT_FOUND\u0026#34; } } ] } 返回的文档由各个子管道添加了新字段 code_type\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;code\u0026#34;: 404, \u0026#34;code_type\u0026#34;: \u0026#34;numeric\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_source\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;ERR_NOT_FOUND\u0026#34;, \u0026#34;code_type\u0026#34;: \u0026#34;string\u0026#34; } } } ] } ","subcategory":null,"summary":"","tags":null,"title":"控制条件和处理器","url":"/easysearch/main/docs/features/ingest-pipelines/conditional-execution/conditional-pipeline-processor/"},{"category":null,"content":"Lowercase 分词过滤器 #  lowercase 分词过滤器用于将分词流中的所有字符转换为小写，从而使搜索不区分大小写。\n相关指南（先读这些） #    文本分析：规范化  文本分析：识别词元  参数 #  小写分词过滤器可以使用以下参数进行配置。\n   参数 必填/可选 描述     language 可选 指定一个特定语言的分词过滤器。有效值为：\n- 希腊语 greek\n- 爱尔兰语irish\n- 土耳其语turkish。\n默认值是 Lucene 的小写过滤器。    参考样例 #  以下示例请求创建了一个名为 custom_lowercase_example 的新索引。它配置了一个带有小写过滤器的分析器，并指定语言为希腊语。\nPUT /custom_lowercase_example { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;greek_lowercase_example\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;greek_lowercase\u0026#34;] } }, \u0026#34;filter\u0026#34;: { \u0026#34;greek_lowercase\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;lowercase\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;greek\u0026#34; } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /custom_lowercase_example/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;greek_lowercase_example\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Αθήνα ΕΛΛΑΔΑ\u0026#34; } 返回内容包含产生的词元\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;αθηνα\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;ελλαδα\u0026#34;, \u0026#34;start_offset\u0026#34;: 6, \u0026#34;end_offset\u0026#34;: 12, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"小写分词过滤器（Lowercase）","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/lowercase/"},{"category":null,"content":"字节处理器 #  bytes 处理器将可读的字节值转换为等效的字节数值。字段可以是标量或数组。如果字段是标量，则值将被转换并存储在该字段中。如果字段是数组，则转换数组中的所有值。\n语法 #  以下是为 bytes 处理器提供的语法：\n{ \u0026#34;bytes\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;your_field_name\u0026#34; } } 配置参数 #  下表列出了 bytes 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要转换的数据的字段名称。支持模板使用。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   ignore_missing 可选 指定处理器是否应忽略不包含指定字段的文档。如果设置为 true ，则处理器在字段不存在或为 null 时不会修改文档。默认为 false 。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。   target_field 可选 指定存储解析数据的字段名称。如果没有指定，则值将存储在 field 字段中。默认为 field 。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 file_upload 的管道，该管道有一个 bytes 处理器。它将 file_size 转换为字节的等效值，并将其存储在名为 file_size_bytes 的新字段中：\nPUT _ingest/pipeline/file_upload { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that converts file size to bytes\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;bytes\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;file_size\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;file_size_bytes\u0026#34; } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/file_upload/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;file_size_bytes\u0026#34;: \u0026#34;10485760\u0026#34;, \u0026#34;file_size\u0026#34;: \u0026#34;10MB\u0026#34; } } ] } 返回内容确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;file_size_bytes\u0026#34;: \u0026#34;10485760\u0026#34;, \u0026#34;file_size\u0026#34;: \u0026#34;10MB\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-08-22T16:09:42.771569211Z\u0026#34; } } } ] } 步骤 3：摄取文档 #  以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=file_upload { \u0026#34;file_size\u0026#34;: \u0026#34;10MB\u0026#34; } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 ","subcategory":null,"summary":"","tags":null,"title":"字节处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/bytes/"},{"category":null,"content":"Minimum Should Match 参数 #  minimum_should_match 参数可用于全文搜索，并指定文档必须匹配的最小词项数量才能在搜索结果中返回。\n以下示例要求文档至少匹配三个搜索词中的两个才能作为搜索结果返回：\n相关指南（先读这些） #    查询 DSL 基础  全文搜索  GET /shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;prince king star\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: \u0026#34;2\u0026#34; } } } } 在这个示例中，查询有三个可选子句，它们通过 OR 结合，因此文档必须匹配 prince 和 king ，或者 prince 和 star ，或者 king 和 star 。\n参数值说明 #  您可以指定 minimum_should_match 参数为以下值之一。\n   值类型 示例 描述     非负整数 2 一个文档必须匹配这个数量的可选子句。   负整数 -1 一个文档必须匹配可选子句总数减去这个数。   非负百分比 70% 一个文档必须匹配可选子句总数的这个百分比。要匹配的子句数向下取整到最接近的整数。   负百分比 -30% 一个文档可以有这个百分比的不匹配的可选子句。文档允许不匹配的子句数向下取整到最接近的整数。   组合 2\u0026lt;75% n\u0026lt;p% 格式的表达式。如果可选子句的数量小于或等于 n ，文档必须匹配所有可选子句。如果可选子句的数量大于 n ，则文档必须匹配 p 百分比的可选子句。   多种组合 3\u0026lt;-1 5\u0026lt;50% 用空格分隔的多个组合。每个条件适用于 \u0026lt; 符号左侧数字更多的可选子句数量。在这个例子中，如果有三个或更少可选子句，文档必须匹配所有它们。如果有四或五个可选子句，文档必须匹配所有但一个。如果有 6 个或更多可选子句，文档必须匹配 50% 的它们。     设 n 为文档必须匹配的可选子句数量。当 n 计算为百分比时，如果 n 小于 1，则使用 1。如果 n 大于可选子句的数量，则使用可选子句的数量。\n 在布尔查询中使用参数 #  布尔查询在 should 子句中列出可选子句，在 must 子句中列出必需子句。可选地，它还可以包含一个 filter 子句来过滤结果。\n考虑一个包含以下五个文档的示例索引：\nPUT testindex/_doc/1 { \u0026#34;text\u0026#34;: \u0026#34;one Easysearch\u0026#34; } PUT testindex/_doc/2 { \u0026quot;text\u0026quot;: \u0026quot;one two Easysearch\u0026quot; }\nPUT testindex/_doc/3 { \u0026quot;text\u0026quot;: \u0026quot;one two three Easysearch\u0026quot; }\nPUT testindex/_doc/4 { \u0026quot;text\u0026quot;: \u0026quot;one two three four Easysearch\u0026quot; }\nPUT testindex/_doc/5 { \u0026quot;text\u0026quot;: \u0026quot;Easysearch\u0026quot; } 以下查询包含四个可选子句：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Easysearch\u0026#34; } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;two\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;three\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;four\u0026#34; } } ], \u0026#34;minimum_should_match\u0026#34;: \u0026#34;80%\u0026#34; } } } 因为 minimum_should_match 被指定为 80% ，匹配可选子句的数量计算为 4 · 0.8 = 3.2，然后向下取整为 3。因此，结果包含至少匹配三个子句的文档：\n{ \u0026#34;took\u0026#34;: 40, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2.494999, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 2.494999, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one two three four Easysearch\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.5744598, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one two three Easysearch\u0026#34; } } ] } } 现在将 minimum_should_match 指定为 -20% :\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Easysearch\u0026#34; } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;two\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;three\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;four\u0026#34; } } ], \u0026#34;minimum_should_match\u0026#34;: \u0026#34;-20%\u0026#34; } } } 一个文档中可以有的不匹配的可选子句的数量计算为 4 · 0.2 = 0.8，并向下取整为 0。因此，结果只包含一个匹配所有可选子句的文档：\n{ \u0026#34;took\u0026#34;: 41, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2.494999, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 2.494999, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one two three four Easysearch\u0026#34; } } ] } } 请注意，指定一个正百分比（ 80% ）和一个负百分比（ -20% ）并没有导致文档必须匹配的可选子句数量相同，因为在这两种情况下，结果都被向下取整。如果可选子句的数量，例如为 5，那么 80% 和 -20% 都会产生相同数量的文档必须匹配的可选子句（4）。\n默认 minimum_should_match 值 #  如果一个查询包含 must 或 filter 子句，默认 minimum_should_match 值为 0。例如，以下查询搜索匹配 Easysearch 和 0 个可选 should 子句的文档：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Easysearch\u0026#34; } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;two\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;three\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;four\u0026#34; } } ] } } } 这个查询返回索引中的所有五个文档：\n{ \u0026#34;took\u0026#34;: 34, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 5, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2.494999, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 2.494999, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one two three four Easysearch\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.5744598, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one two three Easysearch\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.91368985, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one two Easysearch\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.4338556, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one Easysearch\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34;: 0.11964063, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Easysearch\u0026#34; } } ] } } 然而，如果你省略了 must 子句，那么查询将搜索匹配一个可选的 should 子句的文档：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;two\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;three\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;four\u0026#34; } } ] } } } 结果只包含四个至少匹配一个可选子句的文档：\n{ \u0026#34;took\u0026#34;: 19, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 4, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 2.426633, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 2.426633, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one two three four Easysearch\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.4978898, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one two three Easysearch\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.8266785, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one two Easysearch\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.3331056, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;one Easysearch\u0026#34; } } ] } } \n","subcategory":null,"summary":"","tags":null,"title":"Minimum Should Match 参数","url":"/easysearch/main/docs/features/query-dsl/minimum-should-match/"},{"category":null,"content":"Index API #  将一个 JSON 文档写入指定索引。若文档 ID 已存在，则整文档覆盖并递增 _version。\n请求格式 #  PUT /\u0026lt;index\u0026gt;/_doc/\u0026lt;_id\u0026gt; POST /\u0026lt;index\u0026gt;/_doc/\u0026lt;_id\u0026gt; POST /\u0026lt;index\u0026gt;/_doc # 自动生成 _id 仅创建（文档存在时返回 409 Conflict）：\nPUT /\u0026lt;index\u0026gt;/_create/\u0026lt;_id\u0026gt; POST /\u0026lt;index\u0026gt;/_create/\u0026lt;_id\u0026gt; 路径参数 #     参数 必需 说明     \u0026lt;index\u0026gt; 是 目标索引名称   \u0026lt;_id\u0026gt; 否 文档 ID。使用 POST /\u0026lt;index\u0026gt;/_doc 时可省略，由系统自动生成    查询参数 #     参数 类型 默认值 说明     op_type string index 写入类型。index = 创建或覆盖；create = 仅创建（已存在时返回 409）。使用 _create 端点时强制为 create   routing string — 自定义路由值，决定文档落入哪个分片   pipeline string — 写入前执行的 Ingest Pipeline 名称   refresh string false 写入后是否刷新。true = 立即刷新；wait_for = 等待下一次自动刷新完成后返回；false = 不等待   timeout time 1m 等待主分片可用的超时时间   version long — 用于外部版本控制的版本号   version_type string internal 版本类型：internal、external、external_gte   if_seq_no long — 乐观并发控制：仅当文档的 _seq_no 等于此值时才执行写入   if_primary_term long — 乐观并发控制：仅当文档的 _primary_term 等于此值时才执行写入   wait_for_active_shards string 1 写入前需要的活跃分片数量。1 = 仅主分片；all = 全部分片   require_alias boolean false 为 true 时要求 \u0026lt;index\u0026gt; 必须是别名，否则返回错误    请求体 #  完整的 JSON 文档，即 _source 的内容。\n示例 #  指定 ID 写入 #  PUT /website/_doc/123 { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Just trying this out...\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2014/01/01\u0026#34; } 响应：\n{ \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1 } 再次 PUT 同一 _id 会覆盖旧文档：result 变为 \u0026quot;updated\u0026quot;，_version 递增。\n 实现细节：更新不是原地改写，而是将旧文档标记删除，再写入一条新文档。后台通过段合并（segment merge）彻底清理旧版本。\n 自动生成 ID #  POST /website/_doc { \u0026#34;title\u0026#34;: \u0026#34;My second blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Still trying this out...\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2014/01/01\u0026#34; } 响应中 _id 由系统生成（URL 安全、全局唯一）：\n{ \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;AVFgSgVHUP18jI2wRx0w\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34; }  自动 ID 无法保证幂等写入。如果请求超时后重试，可能会产生两条不同 _id 的重复文档。需要幂等语义时请自行指定 _id。\n 仅创建（禁止覆盖） #  PUT /website/_doc/123?op_type=create { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34; } 或等价写法：\nPUT /website/_doc/123/_create { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34; }  文档不存在 → 201 Created 文档已存在 → 409 Conflict，原文档不受影响  适用场景：日志/事件流、财务流水等\u0026quot;只允许插入不允许覆盖\u0026quot;的业务。\n 参考导航 #     需求 参见     按 ID 检索文档  Get API   部分更新文档  Update API   批量写入  Bulk API   写入的分布式协调  分布式写入过程   乐观并发控制  并发控制与版本    ","subcategory":null,"summary":"","tags":null,"title":"Index API","url":"/easysearch/main/docs/features/document-operations/index-api/"},{"category":null,"content":"集群与节点配置 #  本页介绍 easysearch.yml 中与集群身份和节点角色相关的配置项。这些都是静态设置，修改后需要重启节点生效。\n 集群名称 #  cluster.name: easysearch    项目 说明     参数 cluster.name   默认值 easysearch   属性 静态   说明 集群的唯一名称。所有节点必须配置相同的集群名称才能组成一个集群。不同环境（开发、测试、生产）应使用不同的集群名称以防止节点误加入    注意事项：\n 集群名称不能包含冒号（:），建议仅使用小写字母、数字和连字符。 集群名称一旦投入使用后不建议更改，因为它会影响数据目录结构。 同一网络中如果有多个集群，务必确保集群名称各不相同。  示例：\n# 开发环境 cluster.name: dev-cluster # 生产环境 cluster.name: prod-search \n节点名称 #  node.name: node-1    项目 说明     参数 node.name   默认值 主机名（hostname）   属性 静态   说明 节点在集群中的唯一标识符。如果未显式设置，默认使用机器主机名。用于日志输出、集群状态显示、API 返回信息中。建议使用有意义的名称便于运维定位    命名建议：\n 采用统一的命名规则，例如 \u0026lt;角色\u0026gt;-\u0026lt;编号\u0026gt; 或 \u0026lt;机房\u0026gt;-\u0026lt;角色\u0026gt;-\u0026lt;编号\u0026gt; 名称在集群中必须唯一  示例：\n# 按角色命名 node.name: master-1 node.name: data-hot-1 node.name: data-warm-2 # 按机房命名 node.name: bj-data-01 node.name: sh-master-02\n# 使用环境变量 node.name: ${HOSTNAME} \n节点角色 #  node.roles: [master, data, ingest, remote_cluster_client]    项目 说明     参数 node.roles   默认值 [master, data, ingest, remote_cluster_client]   属性 静态   说明 决定节点在集群中承担的功能。默认情况下节点拥有所有角色    可选角色 #     角色 缩写 功能 适用场景     master m 参与主节点选举，管理集群状态（索引创建/删除、分片分配等） 每个集群建议 3 个专用 master 节点   data d 存储索引数据，执行 CRUD、搜索和聚合操作 主要计算与存储工作负载   ingest i 执行 ingest pipeline（文档预处理） 有复杂预处理需求时可独立部署   remote_cluster_client r 充当跨集群搜索/复制的客户端 需要跨集群查询时启用   search s 执行搜索操作 专用搜索节点场景     缩写字母会在 GET _cat/nodes 输出的 node.role 列中显示。\n 旧版配置兼容 #  以下旧版布尔配置已废弃，建议迁移到 node.roles：\n   旧配置 等价 node.roles 写法     node.master: true node.roles 中包含 master   node.data: true node.roles 中包含 data   node.ingest: true node.roles 中包含 ingest   node.master: false + node.data: false node.roles: []（协调节点）    架构建议 #  小规模集群（3 节点）— 所有节点同时承担所有角色：\n# 所有节点使用相同配置 node.roles: [master, data, ingest] 中大规模集群（10+ 节点）— 角色分离：\n# 专用 Master 节点（3 个，不存储数据，资源占用低） node.roles: [master] # 专用 Data 节点（可水平扩展） node.roles: [data, ingest]\n# 专用协调节点（无数据、无 master 资格，仅做请求路由和结果聚合） node.roles: [] 冷热分层集群 — 搭配 node.attr.temp 使用：\n# 热节点（SSD，接收实时写入） node.roles: [data] node.attr.temp: hot # 温节点（大容量 HDD，存储历史数据） node.roles: [data] node.attr.temp: warm\n# 冷节点（归档） node.roles: [data] node.attr.temp: cold \n自定义节点属性 #  node.attr.zone: cn-hangzhou-a node.attr.rack: rack-01 node.attr.temp: hot    项目 说明     参数 node.attr.\u0026lt;key\u0026gt;   默认值 无   属性 静态   说明 自定义的键值对属性，附加到节点上。可用于分片分配感知（Shard Allocation Awareness）和分片分配过滤    常用场景 #  可用区感知 #  确保主分片和副本分布在不同的可用区，提高容灾能力：\n# 节点配置（easysearch.yml） node.attr.zone: cn-hangzhou-a 搭配集群设置 API 启用分配感知：\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.awareness.attributes\u0026#34;: \u0026#34;zone\u0026#34; } } 机架感知 #  node.attr.rack: rack-01 冷热分层 #  # 热节点 node.attr.temp: hot # 温节点 node.attr.temp: warm 搭配索引生命周期管理或手动分配：\nPUT /old-logs-*/_settings { \u0026#34;index.routing.allocation.require.temp\u0026#34;: \u0026#34;warm\u0026#34; } 查看节点属性 #  GET _cat/nodeattrs?v  系统启动配置 #  bootstrap.system_call_filter #  bootstrap.system_call_filter: false    项目 说明     参数 bootstrap.system_call_filter   默认值 true   属性 静态   说明 是否启用 Linux 系统调用过滤（seccomp）。设为 false 时禁用系统调用过滤。在某些运行环境（如 Docker、虚拟机、某些云平台）中可能需要禁用以避免启动失败    禁用场景：\n 在不支持 seccomp 的容器环境中运行 某些云计算平台有限制 开发和测试环境   生产环境建议保持启用（true）以增强安全性。\n  完整配置示例 #  三节点通用集群 #  # ---- node-1 ---- cluster.name: prod-cluster node.name: node-1 node.roles: [master, data, ingest] node.attr.zone: zone-a # \u0026mdash;- node-2 \u0026mdash;- cluster.name: prod-cluster node.name: node-2 node.roles: [master, data, ingest] node.attr.zone: zone-b\n# \u0026mdash;- node-3 \u0026mdash;- cluster.name: prod-cluster node.name: node-3 node.roles: [master, data, ingest] node.attr.zone: zone-a 角色分离集群 #\n # ---- master-1 ---- cluster.name: prod-cluster node.name: master-1 node.roles: [master] # \u0026mdash;- data-hot-1 \u0026mdash;- cluster.name: prod-cluster node.name: data-hot-1 node.roles: [data, ingest] node.attr.temp: hot\n# \u0026mdash;- data-warm-1 \u0026mdash;- cluster.name: prod-cluster node.name: data-warm-1 node.roles: [data] node.attr.temp: warm \n延伸阅读 #    网络配置 — 绑定地址与端口  集群发现 — 节点如何发现彼此组成集群  集群配置 — 分片分配感知等动态设置  ","subcategory":null,"summary":"","tags":null,"title":"集群与节点","url":"/easysearch/main/docs/deployment/config/node-settings/cluster-node/"},{"category":null,"content":"配置文件 #  可以在每个 Easysearch 节点上找到 easysearch.yml ，通常位于 Easysearch 安装目录下 config/easysearch.yml。\n配置文件一览 #  Easysearch 主要有以下几类配置文件：\n easysearch.yml：节点与集群配置的主文件，包括网络、发现、角色、路径等。  只放“本机相关”和“集群引导”类配置（如 cluster.name、node.name、network.host、discovery.seed_hosts 等）更易维护。   jvm.options：JVM 启动参数，如堆大小（-Xms/-Xmx）、GC 设置等。 log4j2.properties：日志配置，控制日志级别、输出格式与输出位置。 config/security/*.yml：安全模块本地配置，用于内置用户、角色等，详见 安全配置。  配置目录位置 #   默认情况下，配置目录位于 $ES_HOME/config。 你可以通过环境变量 ES_PATH_CONF 指定一个自定义配置目录，例如：  ES_PATH_CONF=/path/to/my/config ./bin/easysearch 在使用 systemd、Docker 或自带脚本运行服务时，通常需要在对应的服务配置里设置 ES_PATH_CONF，而不是只在当前 shell 里导出。\n配置文件格式要点（YAML） #  Easysearch 的配置文件使用 YAML 格式，几个常用规则：\n 缩进代表层级：  path: data: /var/lib/easysearch logs: /var/log/easysearch  同样的内容也可以写成“扁平 key”：  path.data: /var/lib/easysearch path.logs: /var/log/easysearch  列表（sequence） 可以写成多行：  discovery.seed_hosts: - 192.168.1.10:9300 - 192.168.1.11 - seeds.mydomain.com 也可以写成单行数组形式：\ndiscovery.seed_hosts: [\u0026#34;192.168.1.10:9300\u0026#34;, \u0026#34;192.168.1.11\u0026#34;, \u0026#34;seeds.mydomain.com\u0026#34;] 环境变量替换 #  在 easysearch.yml 中，可以使用 ${...} 的形式引用环境变量，Easysearch 启动时会自动替换。例如：\nnode.name: ${HOSTNAME} network.host: ${ES_NETWORK_HOST} 注意事项：\n 环境变量的值会被当作纯字符串读取。 如果需要“列表”效果，可以使用逗号分隔的字符串，然后由 Easysearch 解析为列表，例如：  export SEED_HOSTS=\u0026#34;10.0.0.1:9300,10.0.0.2:9300\u0026#34; discovery.seed_hosts: ${SEED_HOSTS} 配置作用域与推荐用法 #   静态设置：只能在 easysearch.yml 或启动参数里配置，例如路径、网络、节点角色等，修改后需要重启节点。 动态设置：可以通过 _cluster/settings 接口在运行时修改，例如多数集群级别的调优项。  推荐实践：\n easysearch.yml：只放节点本地 / 集群引导配置，保持所有节点上内容简单且一致。 集群级别调优：使用 集群配置 中介绍的 集群设置 API 来做持久（persistent）或临时（transient）修改。  警告 #  切勿将未受保护的节点暴露在公共互联网上！\nJVM 配置 #  详见 JVM 选项，包括堆内存、垃圾回收器等。\n日志配置 #  详见 日志配置，包括日志位置、级别调整、慢日志设置。\n安全配置 #  config/security/ 目录下的配置文件说明详见 本地配置（YAML）\nElasticsearch 兼容性 #  Easysearch 支持与 Elasticsearch 8.x API 的兼容性模式：\nelasticsearch.api_compatibility: true elasticsearch.api_compatibility_version: \u0026#34;8.9.0\u0026#34;    参数 默认值 说明     elasticsearch.api_compatibility false 是否启用 Elasticsearch 兼容性模式。启用后，Easysearch 将支持部分 Elasticsearch 8.x API 接口   elasticsearch.api_compatibility_version 无 指定兼容的 Elasticsearch 版本。例如 \u0026quot;8.9.0\u0026quot; 表示兼容到 Elasticsearch 8.9.0    使用场景：\n 从 Elasticsearch 迁移到 Easysearch 时，保持现有应用无需修改 需要逐步迁移的环境   常用网络设置 #   完整的 easysearch.yml 配置参考（包括索引、分片分配、断路器、安全、线程池等全部配置项）请参见 集群配置。\n Easysearch 默认只绑定到 localhost。对于生产环境的集群，需要配置基本的网络设置：\n   参数名称 属性 功能说明 默认值 生产环境示例     network.host 静态 节点绑定的主机名或 IP 地址。 _local_ 192.168.1.10   network.bind_host 静态 节点监听传入请求的具体地址。可以配置为外网地址（例如 0.0.0.0 监听所有接口）或其他特定的地址。 随 network.host 0.0.0.0   network.publish_host 静态 节点间通信的通告地址。在多网卡或多网络环境中，应显式设置 network.publish_host。 随 network.host 192.168.1.10   http.port 静态 HTTP 请求的绑定端口。支持单个值或范围。指定范围时，节点将绑定到该范围内的第一个可用端口。 9200-9300 9200   transport.port 静态 节点间通信 (TCP) 端口。支持单个值或范围。指定范围时，节点将绑定到该范围内第一个可用端口。在所有具有主节点资格（master-eligible）的节点上，请将此项设置为单个值。 9300-9400 9300   discovery.seed_hosts 静态 提供集群中有资格成为主节点的节点地址列表。也可以是一个包含多个以逗号分隔的地址的字符串。每个地址的格式为 host:port 或 host。端口按顺序检查以下设置来确定： transport.profiles.default.port，transport.port，未设置时默认9300。 [\u0026quot;127.0.0.1\u0026quot;, \u0026quot;[::1]\u0026quot;]。 [\u0026quot;10.0.0.1\u0026quot;, \u0026quot;10.0.0.2\u0026quot;]   discovery.type 静态 指定 Easysearch 是否应形成一个多节点集群。如果将 discovery.type 设置为 single-node，Easysearch 将形成一个单节点集群。 zen (默认形成多节点集群，无需设置) single-node   cluster.initial_master_nodes 静态 设置新集群中初始的主节点候选节点列表。默认情况下，此列表为空，意味着该节点期望加入已经引导好的集群。在生产环境中首次启动新的 Easysearch 集群时，必须配置 cluster.initial_master_nodes，以明确哪些节点有资格参与主节点的选举。 当集群完成了首次主节点选举，集群已经正常运行时，就不再需要此设置了。这时应该从每个节点的配置中移除这项设置。 [] 默认为空 [\u0026quot;node-1\u0026quot;, \u0026quot;node-2\u0026quot;]   node.roles 静态 定义节点的角色。节点默认具有右侧列出的角色。如果设置了 node.roles，则节点只会被分配指定的角色。 [master, data, ingest, remote_cluster_client] [\u0026quot;data\u0026quot;, \u0026quot;ingest\u0026quot;]    分布式集群模式 #  以下配置适用于 Easysearch 的分布式集群模式，确保各节点可以发现彼此并组成一个集群，分布式模式建议 3 个独立的 Master 节点，配置如下：\ncluster.name: easysearch node.roles: [ \u0026#34;master\u0026#34; ] network.host: 0.0.0.0 http.port: 19201 transport.port: 19301 discovery.seed_hosts: [\u0026#34;192.168.101.5:19301\u0026#34;, \u0026#34;192.168.101.6:19302\u0026#34;, \u0026#34;192.168.101.7:19303\u0026#34;] cluster.initial_master_nodes: [\u0026#34;192.168.101.5:19301\u0026#34;, \u0026#34;192.168.101.6:19302\u0026#34;, \u0026#34;192.168.101.7:19303\u0026#34;] 数据节点配置如下：\ncluster.name: easysearch node.roles: [ \u0026#34;data\u0026#34; ] network.host: 0.0.0.0 http.port: 19200 transport.port: 19300 discovery.seed_hosts: [\u0026#34;192.168.101.5:19301\u0026#34;, \u0026#34;192.168.101.6:19302\u0026#34;, \u0026#34;192.168.101.7:19303\u0026#34;]  Master 使用 3 个就够了，数据节点可以根据动态增加。\n 单节点服务模式 #  如果希望单个 Easysearch 节点模拟集群，能监听所有网卡并提供对外服务供其他主机访问，也就是单节点模式，可以使用以下配置：\ncluster.name: easysearch network.host: 0.0.0.0 http.port: 13200 transport.port: 13300 cluster.initial_master_nodes: [\u0026#34;localhost:13300\u0026#34;] 了解更多，请查看 搭建集群\n","subcategory":null,"summary":"","tags":null,"title":"配置说明","url":"/easysearch/main/docs/deployment/config/configuration_file/"},{"category":null,"content":"部署 Easysearch Operator #  Easysearch Operator 只能在 k8s 环境下部署安装，请准备好一套 k8s 环境\n部署前准备 #   k8s 环境\n要求Kubernetes 1.9以上版本，自 1.9 版本以后，StatefulSet成为了在Kubernetes中管理有状态应用的标准方式。 StorageClass\nStorageClass 允许集群管理员定义多种存储方案，如快速的 SSD、标准的硬盘，或者其他的存储系统。无需手动预先创建存储资源，用户只需要在 PersistentVolumeClaim (PVC) 中指定需要的 StorageClass，存储资源就可以根据需求动态地创建。 ServiceAccount\n创建一个 ServiceAccount 用于 Easysearch Operator 获取和操作 k8s 资源 apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/name: serviceaccount app.kubernetes.io/instance: controller-manager-sa app.kubernetes.io/component: rbac app.kubernetes.io/created-by: k8s-operator app.kubernetes.io/part-of: k8s-operator app.kubernetes.io/managed-by: kustomize name: controller-manager # ServiceAccount 的名字是 controller-manager namespace: default  ClusterRole\n创建 ClusterRole，用于定义访问 k8s 集群的角色权限 展开查看完整代码 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: manager-role namespace: default rules: - apiGroups: - apps resources: - deployments verbs: - create - delete - get - list - patch - update - watch - apiGroups: - apps resources: - statefulsets verbs: - create - delete - get - list - patch - update - watch - apiGroups: - batch resources: - jobs verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - update - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - secrets verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - services verbs: - create - delete - get - list - patch - update - watch - apiGroups: - infinilabs.infinilabs.com resources: - events verbs: - create - patch - apiGroups: - infinilabs.infinilabs.com resources: - searchclusters verbs: - create - delete - get - list - patch - update - watch - apiGroups: - infinilabs.infinilabs.com resources: - searchclusters/finalizers verbs: - update - apiGroups: - infinilabs.infinilabs.com resources: - searchclusters/status verbs: - get - patch - update - apiGroups: - policy resources: - poddisruptionbudgets verbs: - create - delete - get - list - patch - update - watch - apiGroups: - monitoring.coreos.com resources: - servicemonitors verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - persistentvolumeclaims verbs: - create - delete - get - list - patch - update - watch  ClusterRoleBinding\n创建 ClusterRoleBinding，将 service account（controller-manager） 和 role （manager-role）绑定起来 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/name: clusterrolebinding app.kubernetes.io/instance: manager-rolebinding app.kubernetes.io/component: rbac app.kubernetes.io/created-by: k8s-operator app.kubernetes.io/part-of: k8s-operator app.kubernetes.io/managed-by: kustomize name: manager-rolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: manager-role subjects: - kind: ServiceAccount name: controller-manager namespace: default      cert-manager\n部署 cert-manager 来管理 Easysearch 整个集群的证书  至此，准备工作完毕，以上的流程都是一劳永逸的，在第一次部署 Operator 的时候才需要，后续不再需要重新部署。\n部署 Easysearch Operator #  下载镜像 #  Easysearch Operator 的镜像发布在 Docker 的官方仓库，地址如下：\nhttps://hub.docker.com/r/infinilabs/operator\n使用下面的命令即可获取最新的容器镜像：\ndocker pull infinilabs/operator:latest 验证镜像 #  将镜像下载到本地之后，可以看到 Easysearch Operator 容器镜像非常小，只有大概 60MB，所以下载的速度应该是非常快的。\n➜ docker images REPOSITORY TAG IMAGE ID CREATED SIZE infinilabs/operator latest 5da94b93e7eb 8 minutes ago 61.2MB 部署 Operator #  在已有的 k8s 环境中部署 Operator，Operator 采用 deployment 方式部署，编辑对应的 yaml 文件，并 apply 即可。\nkind: Deployment apiVersion: apps/v1 metadata: name: k8s-operator namespace: default labels: app: k8s-operator spec: replicas: 1 selector: matchLabels: app: k8s-operator template: metadata: labels: app: k8s-operator spec: containers: - name: k8s-operator image: \u0026#39;infinilabs/operator:latest\u0026#39; imagePullPolicy: IfNotPresent restartPolicy: Always serviceAccount: controller-manager # 指定账号 查看部署情况 #  ik get deployment NAME READY UP-TO-DATE AVAILABLE AGE k8s-operator 1/1 1 1 2m6s 至此，Easysearch Operator 已经部署完成。\n","subcategory":null,"summary":"","tags":null,"title":"部署 Operator","url":"/easysearch/main/docs/deployment/install-guide/operator/deploy_operator/"},{"category":null,"content":"评分基础 #  处理结构化数据（比如：时间、数字、字符串、枚举）的数据库，只需检查文档（或关系数据库里的行）是否与查询匹配。\n布尔的是/非匹配是全文搜索的基础，但不止如此，我们还要知道每个文档与查询的相关度，在全文搜索引擎中不仅需要找到匹配的文档，还需根据它们相关度的高低进行排序。\n全文相关的公式或相似算法（similarity algorithms）会将多个因素合并起来，为每个文档生成一个相关度评分 _score。本章中，我们会验证各种可变部分，然后讨论如何来控制它们。\n当然，相关度不只与全文查询有关，也需要将结构化的数据考虑其中。可能我们正在找一个度假屋，需要一些的详细特征（空调、海景、免费 WiFi），匹配的特征越多相关度越高。可能我们还希望有一些其他的考虑因素，如回头率、价格、受欢迎度或距离，当然也同时考虑全文查询的相关度。\n所有的这些都可以通过 Easysearch 强大的评分基础来实现。\n相关度评分背后的理论 #  Easysearch 使用布尔模型（Boolean Model）查找匹配文档，并用一个名为实用评分函数（practical scoring function）的公式来计算相关度。这个公式借鉴了词频/逆向文档频率（term frequency/inverse document frequency）和向量空间模型（vector space model），同时也加入了一些现代的新特性，如字段长度归一化（field length normalization），以及词或查询语句权重提升。\n 注意：Easysearch 默认使用 BM25 相似度算法，而非经典的 TF/IDF。BM25 基于概率信息检索模型，在大多数场景下效果更好。下面先介绍 TF/IDF 的基本概念（有助于理解评分原理），然后说明 BM25 的改进。\n  不要紧张！这些概念并没有像它们字面看起来那么复杂，尽管本小节提到了算法、公式和数学模型，但内容还是让人容易理解的，与理解算法本身相比，了解这些因素如何影响结果更为重要。\n 布尔模型 #  布尔模型（Boolean Model）只是在查询中使用 AND、OR 和 NOT（与、或和非）这样的条件来查找匹配的文档，以下查询：\nfull AND text AND search AND (elasticsearch OR lucene) 会将所有包括词 full、text 和 search，以及 elasticsearch 或 lucene 的文档作为结果集。\n这个过程简单且快速，它将所有可能不匹配的文档排除在外。\n词频/逆向文档频率（TF/IDF） #  当匹配到一组文档后，需要根据相关度排序这些文档，不是所有的文档都包含所有词，有些词比其他的词更重要。一个文档的相关度评分部分取决于每个查询词在文档中的权重。\n词的权重由三个因素决定：\n词频（Term Frequency） #  词在文档中出现的频度是多少？频度越高，权重越高。5 次提到同一词的字段比只提到 1 次的更相关。词频的计算方式如下：\ntf(t in d) = √frequency 词 t 在文档 d 的词频（tf）是该词在文档中出现次数的平方根。\n如果不在意词在某个字段中出现的频次，而只在意是否出现过，则可以在字段映射中禁用词频统计：\nPUT /my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index_options\u0026#34;: \u0026#34;docs\u0026#34; } } } } 将参数 index_options 设置为 docs 可以禁用词频统计及词频位置，这个映射的字段不会计算词的出现次数，对于短语或近似查询也不可用。\n逆向文档频率（Inverse Document Frequency） #  词在集合所有文档里出现的频率是多少？频次越高，权重越低。常用词如 and 或 the 对相关度贡献很少，因为它们在多数文档中都会出现，一些不常见词如 elasticsearch 或 hippopotamus 可以帮助我们快速缩小范围找到感兴趣的文档。逆向文档频率的计算公式如下：\nidf(t) = 1 + log ( numDocs / (docFreq + 1)) 词 t 的逆向文档频率（idf）是：索引中文档数量除以所有包含该词的文档数，然后求其对数。\n字段长度归一值（Field-Length Norm） #  字段的长度是多少？字段越短，字段的权重越高。如果词出现在类似标题 title 这样的字段，要比它出现在内容 body 这样的字段中的相关度更高。字段长度的归一值公式如下：\nnorm(d) = 1 / √numTerms 字段长度归一值（norm）是字段中词数平方根的倒数。\n字段长度的归一值对全文搜索非常重要，许多其他字段不需要有归一值。无论文档是否包括这个字段，索引中每个文档的每个 text 字段都大约占用 1 个 byte 的空间。对于 keyword 字符串字段的归一值默认是禁用的，而对于 text 字段也可以通过修改字段映射禁用归一值：\nPUT /my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;norms\u0026#34;: { \u0026#34;enabled\u0026#34;: false } } } } } 这个字段不会将字段长度归一值考虑在内，长字段和短字段会以相同长度计算评分。\n对于有些应用场景如日志，归一值不是很有用，要关心的只是字段是否包含特殊的错误码或者特定的浏览器唯一标识符。字段的长度对结果没有影响，禁用归一值可以节省大量内存空间。\n结合使用 #  以下三个因素——词频（term frequency）、逆向文档频率（inverse document frequency）和字段长度归一值（field-length norm）——是在索引时计算并存储的。最后将它们结合在一起计算单个词在特定文档中的权重。\n 提示：前面公式中提到的文档实际上是指文档里的某个字段，每个字段都有它自己的倒排索引，因此字段的 TF/IDF 值就是文档的 TF/IDF 值。\n 当用 explain 查看一个简单的 term 查询时，可以发现与计算相关度评分的因子就是前面章节介绍的这些：\nPUT /my_index/_doc/1 { \u0026#34;text\u0026#34; : \u0026#34;quick brown fox\u0026#34; } GET /my_index/_search?explain { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;text\u0026quot;: \u0026quot;fox\u0026quot; } } } 以上请求（简化）的 explanation 解释如下：\nweight(text:fox in 0) [PerFieldSimilarity]: 0.15342641 result of: fieldWeight in 0 0.15342641 product of: tf(freq=1.0), with freq of 1: 1.0 idf(docFreq=1, maxDocs=1): 0.30685282 fieldNorm(doc=0): 0.5  词 fox 在文档的内部 Lucene doc ID 为 0，字段是 text 里的最终评分 词 fox 在该文档 text 字段中只出现了一次 fox 在所有文档 text 字段索引的逆向文档频率 该字段的字段长度归一值  当然，查询通常不止一个词，所以需要一种合并多词权重的方式——向量空间模型（vector space model）。\n向量空间模型 #  向量空间模型（vector space model）提供一种比较多词查询的方式，单个评分代表文档与查询的匹配程度，为了做到这点，这个模型将文档和查询都以向量的形式表示。\n向量实际上就是包含多个数的一维数组，例如：\n[1,2,5,22,3,8] 在向量空间模型里，向量空间模型里的每个数字都代表一个词的权重，与词频/逆向文档频率（term frequency/inverse document frequency）计算方式类似。\n 提示：尽管 TF/IDF 是向量空间模型计算词权重的默认方式，但不是唯一方式。Easysearch 还有其他模型如 Okapi-BM25。BM25 是 Easysearch 的默认相似度算法，它在 TF/IDF 基础上做了重要改进：词频具有饱和效应（一个词出现 10 次不会比出现 5 次有明显更高的权重），并且通过参数 k1 和 b 可以调节词频饱和度和字段长度归一化的影响。\n 设想如果查询 \u0026ldquo;happy hippopotamus\u0026rdquo;，常见词 happy 的权重较低，不常见词 hippopotamus 权重较高，假设 happy 的权重是 2，hippopotamus 的权重是 5，可以将这个二维向量——[2,5]——在坐标系下作条直线，线的起点是 (0,0) 终点是 (2,5)。\n现在，设想我们有三个文档：\n I am happy in summer。 After Christmas I\u0026rsquo;m a hippopotamus。 The happy hippopotamus helped Harry。  可以为每个文档都创建包括每个查询词——happy 和 hippopotamus——权重的向量，然后将这些向量置入同一个坐标系中：\n 文档 1：(happy,____________) —— [2,0] 文档 2：( ___ ,hippopotamus) —— [0,5] 文档 3：(happy,hippopotamus) —— [2,5]  向量之间是可以比较的，只要测量查询向量和文档向量之间的角度就可以得到每个文档的相关度，文档 1 与查询之间的角度最大，所以相关度低；文档 2 与查询间的角度较小，所以更相关；文档 3 与查询的角度正好吻合，完全匹配。\n 提示：在实际中，只有二维向量（两个词的查询）可以在平面上表示，幸运的是，线性代数——作为数学中处理向量的一个分支——为我们提供了计算两个多维向量间角度工具，这意味着可以使用如上同样的方式来解释多个词的查询。关于比较两个向量的更多信息可以参考余弦近似度（cosine similarity）。\n Lucene 的实用评分函数 #  对于多词查询，Lucene 使用布尔模型（Boolean model）、TF/IDF 以及向量空间模型（vector space model），然后将它们组合到单个高效的包里以收集匹配文档并进行评分计算。\n BM25 vs 经典 TF/IDF：Easysearch 默认使用 BM25 而非经典 TF/IDF。BM25 的实际评分公式为：\nscore(q,d) = ∑ ( idf(t) · tf_bm25(t in d) · t.getBoost() ) (t in q) 其中 BM25 的词频计算引入了饱和函数：\ntf_bm25 = (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * dl / avgdl))  k1：控制词频饱和度，默认 1.2。值越大，词频对评分的影响越大 b：控制字段长度归一化的影响，默认 0.75。值为 0 时完全忽略字段长度，值为 1 时完全按字段长度归一化 dl：当前文档的字段长度（词项数） avgdl：所有文档该字段的平均长度  这比经典 TF/IDF 的 √frequency 更合理——一个词出现 20 次不会比出现 10 次获得显著更高的评分。\n 一个多词查询：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;quick fox\u0026#34; } } } 会在内部被重写为：\nGET /my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ {\u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;quick\u0026#34; }}, {\u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;fox\u0026#34; }} ] } } } bool 查询实现了布尔模型，在这个例子中，它会将包括词 quick 和 fox 或两者兼有的文档作为查询结果。\n只要一个文档与查询匹配，Lucene 就会为查询计算评分，然后合并每个匹配词的评分结果。这里使用的评分计算公式叫做实用评分函数（practical scoring function）。看似很高大上，但是别被吓到——多数的组件都已经介绍过，下一步会讨论它引入的一些新元素。\n在 BM25 下，简化的评分公式为：\nscore(q,d) = ∑ ( idf(t)² · tf_bm25(t in d) · t.getBoost() · norm(t,d) ) (t in q)  score(q,d) 是文档 d 与查询 q 的相关度评分 查询 q 中每个词 t 对于文档 d 的权重和 idf(t) 是词 t 的逆向文档频率 tf_bm25(t in d) 是词 t 在文档 d 中的 BM25 词频（带饱和效应） t.getBoost() 是查询中使用的 boost norm(t,d) 是字段长度归一值（在 BM25 中已集成到 tf_bm25 公式中）   历史说明：经典 TF/IDF 评分公式中还包含 queryNorm（查询归一化因子）和 coord（协调因子），但这两个因子在 Easysearch（基于 Lucene 8+）中已被移除。如果在 explain 输出中看到相关术语，那是旧版本的遗留信息。\n BM25 参数调优 #  Easysearch 默认使用 BM25 相似度，可以在字段映射中自定义其参数：\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;similarity\u0026#34;: { \u0026#34;custom_bm25\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;BM25\u0026#34;, \u0026#34;k1\u0026#34;: 1.2, \u0026#34;b\u0026#34;: 0.75 } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;custom_bm25\u0026#34; } } } }    参数 默认值 说明     k1 1.2 控制词频饱和速度。值越大词频影响越大；设为 0 则完全忽略词频   b 0.75 控制字段长度归一化。0 = 忽略长度；1 = 完全按长度归一化     调优建议：\n 短字段（标题）：可降低 b（如 0.3），减少长度惩罚 长字段（正文）：保持默认值通常效果最好 如果词频很重要：增大 k1（如 2.0）   其他相似度算法 #  除 BM25 外，Easysearch 还支持以下相似度算法：\n   算法 说明     BM25 默认。基于概率模型，带词频饱和   boolean 不计算评分，所有匹配文档得分相同   DFR 基于随机性偏离（Divergence from Randomness）框架   IB 基于信息模型（Information Based）   LMDirichlet Dirichlet 平滑的语言模型   LMJelinekMercer Jelinek-Mercer 平滑的语言模型   DFI 基于独立性偏离（Divergence from Independence）   scripted 自定义脚本评分    小结 #   Easysearch 默认使用 BM25 相似度算法（而非经典 TF/IDF） BM25 考虑词频（带饱和效应）、逆向文档频率和字段长度归一化 可通过 k1 和 b 参数调优 BM25 的行为 布尔模型用于查找匹配文档，BM25 用于计算相关度评分 向量空间模型用于比较多词查询 经典 TF/IDF 中的协调因子和查询归一化因子已被移除  下一步可以继续阅读：\n  加权与调参  调试与 Explain  ","subcategory":null,"summary":"","tags":null,"title":"评分基础","url":"/easysearch/main/docs/features/fulltext-search/relevance/scoring-basics/"},{"category":null,"content":"自动补全 #  自动补全是在用户输入过程中，实时给出可能的搜索词建议。Easysearch 支持三种实现方式，各有适用场景：\n   方式 时机 性能 适用场景     前缀匹配（match_phrase_prefix） 查询时 一般 快速原型，无需特殊 mapping   Edge N-gram 索引时 好 大规模数据的前缀补全   Completion Suggester 索引时 最优 高并发自动补全，支持权重控制    相关指南 #    建议与纠错  部分匹配   前缀匹配（match_phrase_prefix） #  前缀匹配会在查询时，对最后一个 term 做前缀展开。典型做法是使用 match_phrase_prefix 在 text 字段上做查询。\n不需要特殊 mapping，可以直接在现有 text 字段上使用。\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;qui\u0026#34;, \u0026#34;slop\u0026#34;: 3 } } } } 参数说明 #     参数 说明 默认值     query 查询文本 必填   slop 允许的词项位置偏移量 0   max_expansions 最后一个词项的最大前缀展开数量 50   analyzer 覆盖默认分析器 字段默认分析器   zero_terms_query 当分析器移除所有词项时的行为（none 或 all） none     性能注意：前缀匹配属于相对昂贵的查询。例如前缀为 a 时，可能会匹配到几十万 terms。建议通过 max_expansions 限制展开规模：\n GET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;qui\u0026#34;, \u0026#34;slop\u0026#34;: 3, \u0026#34;max_expansions\u0026#34;: 10 } } } }  Edge N-gram 匹配 #  Edge N-gram 在索引时把单词切成一系列前缀片段，加速前缀查询。例如对 \u0026quot;quick\u0026quot; 做 edge N-gram：\n   前缀 Token     1 字符 q   2 字符 qu   3 字符 qui   4 字符 quic   5 字符 quick    配置分析器 #  定义带 edge_ngram 过滤器的自定义分析器：\nPUT shakespeare { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;autocomplete\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;standard\u0026#34; } } }, \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;edge_ngram_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;edge_ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 1, \u0026#34;max_gram\u0026#34;: 20 } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;autocomplete\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;edge_ngram_filter\u0026#34;] } } } } }  重要：建议在 mapping 中指定 search_analyzer 为 standard，避免查询词本身也被切成前缀导致过多无关匹配。这是少数需要\u0026quot;索引时分析器 ≠ 查询时分析器\u0026quot;的场景之一。\n Edge N-gram 过滤器参数 #     参数 说明 默认值     min_gram 最小前缀长度 1   max_gram 最大前缀长度 2   preserve_original 是否保留原始 token false     提示：除了 edge_ngram 过滤器，Easysearch 也支持 edge_ngram 分词器（tokenizer），它按字符级别切分而非基于词级别。过滤器在大多数自动补全场景中更实用。\n 测试分析器 #  POST shakespeare/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;autocomplete\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;quick\u0026#34; } 查询示例 #  GET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;qui\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; } } } }  Completion Suggester #  Completion Suggester 使用基于 FST（有限状态转换器） 的数据结构，常驻内存、专门优化前缀匹配性能，适合高并发自动补全场景。\n创建 Mapping #  使用专门的 completion 字段类型：\nPUT shakespeare { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;completion\u0026#34; } } } } 基础查询 #  GET shakespeare/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;autocomplete\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;To be\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;text_entry\u0026#34;, \u0026#34;size\u0026#34;: 5 } } } } Completion 参数 #     参数 说明 默认值     prefix 前缀文本 -   field completion 字段名 必填   size 返回建议数量 5   skip_duplicates 过滤重复建议 false   fuzzy 启用模糊补全（容错输入） 不启用    模糊补全 #  支持容错输入的自动补全，用户打错字也能返回正确建议：\nGET shakespeare/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;autocomplete\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;To bo\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;text_entry\u0026#34;, \u0026#34;fuzzy\u0026#34;: { \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34; } } } } } fuzzy 参数 #     参数 说明 默认值     fuzziness 编辑距离（AUTO、0、1、2） AUTO   transpositions 是否允许字符交换 true   min_length 开始模糊匹配的最小输入长度 3   prefix_length 不参与模糊的前缀字符数 1   unicode_aware 是否按 Unicode 字符计算编辑距离 false    正则补全 #  支持基于正则表达式的补全匹配：\nGET shakespeare/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;autocomplete\u0026#34;: { \u0026#34;regex\u0026#34;: \u0026#34;To b[aeiou].*\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;text_entry\u0026#34; } } } } 权重控制 #  通过 input 和 weight \u0026ldquo;运营\u0026quot;特定建议词：\nPUT shakespeare/_doc/1 { \u0026#34;text_entry\u0026#34;: { \u0026#34;input\u0026#34;: [\u0026#34;To be, or not to be: that is the question:\u0026#34;], \u0026#34;weight\u0026#34;: 10 } } 权重更高的候选会优先排在补全结果前面。\n Term Suggester（拼写纠正） #  Term Suggester 基于编辑距离对单个词项提供拼写纠正建议：\nGET shakespeare/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;spell-check\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;lief\u0026#34;, \u0026#34;term\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;text_entry\u0026#34; } } } } 返回按 score 排序的纠正候选，freq 表示该词在索引中出现的次数。分数越高建议越好。\n Phrase Suggester（短语纠正） #  Phrase Suggester 使用 N-gram 语言模型对整个短语进行纠正，实现\u0026quot;Did you mean \u0026hellip;?\u0026ldquo;功能。\n需要先配置 shingle 过滤器来构建词级 N-gram：\nPUT shakespeare { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;trigram\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;shingle\u0026#34;] } }, \u0026#34;filter\u0026#34;: { \u0026#34;shingle\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;shingle\u0026#34;, \u0026#34;min_shingle_size\u0026#34;: 2, \u0026#34;max_shingle_size\u0026#34;: 3 } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;trigram\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;trigram\u0026#34; } } } } } } 使用 phrase 建议：\nPOST shakespeare/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;That the qution\u0026#34;, \u0026#34;simple_phrase\u0026#34;: { \u0026#34;phrase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;text_entry.trigram\u0026#34; } } } } 返回纠正后的短语：\u0026quot;that is the question\u0026quot;。\n","subcategory":null,"summary":"","tags":null,"title":"自动补全","url":"/easysearch/main/docs/features/query-dsl/autocomplete/"},{"category":null,"content":"索引与分片设计实践 #  这篇更偏“工程实践”：不讲所有细节，只给你一套在大多数场景下都靠谱的索引设计思路，帮你少走弯路。\n1. 先确定“索引边界”，再谈分片 #  思考顺序建议是：\n 这一坨数据是否应该是一个索引？  典型边界：数据生命周期、访问模式、隔离需求（权限/多租户）、硬件拓扑（可用区）   在它内部，需要多少主分片、多少副本？  常用的索引边界模式：\n 按业务域：orders、products、logs_app、logs_security… 按时间：logs-2026.02.01、logs-2026.02.02（时间序列，见数据建模章节） 按租户/大客户：大客户单独索引，小客户共享索引（见多租户建模章节）   建议：先控制“索引颗粒度”合理，再通过分片做水平扩展；不要指望一个索引撑住所有业务。\n 2. 分片与副本：从“目标分片大小”倒推 #  分片设计没有万能数字，但可以从一个直觉出发：希望单个分片的大小 \u0026amp; 负载落在一个可控区间内。实践中常见的经验值：\n 单分片 10–50GB 对大多数场景比较健康（结合你的磁盘/快照策略微调） 对于热点写入较猛的日志类索引，可以略小一点，便于滚动与恢复  一个简单的倒推流程：\n 用测试环境或历史数据估算：一个分片大概能承载多少数据 \u0026amp; QPS（参考“容量规划”章节） 预估未来一段时间的索引规模（例如：单日日志量、订单增长等） 用 总数据量 / 目标分片大小 ≈ 主分片数 算一个初始值，再按机器数/故障域稍微调整  副本数则更多取决于：\n 可用性（能容忍多少节点故障） 读流量（读多写少时可以多一点副本当读节点）  生产上常见组合：\n 写多读少：number_of_replicas = 0/1 读多写少：number_of_replicas = 1+，甚至为查询集群设独立的 follower  3. 时间序列：按时间切索引，而不是疯狂加分片 #  日志、指标、行为埋点这些“时间序列”场景，最容易踩坑的设计就是：\n 一个索引，分片拉满，想靠加分片吃下所有历史。\n 更推荐的做法：\n 按时间切索引：天/周/月（logs-2026.02.01 之类） 用别名组织“当前/最近 N 天/全部”视图：  logs_current → 指向当天索引 logs_last_7d → 指向最近 7 个索引   数据退役时，按索引维度做：  flush + close（冷数据仍保留） snapshot + delete（非常老的数据）    好处：\n 清理和归档成本都在“索引级别”，而不是“逐文档删除” 不需要在单索引内搞海量分片，恢复和迁移更安全  4. 多租户：一索引一大户，多小户共索引 #  多租户设计常见两极：\n 一租户一索引：隔离性好，适合少数大客户（数据量/查询压力足够大） 多租户共索引：通过字段 + routing + 查询过滤隔离，适合大量小客户  一个折衷方案：\n 为“大租户”单独建索引（甚至单独集群） “长尾小租户”共享一个或少数几个索引，通过 tenant_id 字段 + alias/routing 做隔离  索引与分片设计时需要考虑：\n 是否需要“搬家”能力（从共享索引迁出到独立索引） tenant_id 是否作为 routing key（让同一租户数据尽量落在同一分片）  具体建模细节可参考数据建模章节的“多租户”部分。\n5. 零停机演进：新索引 + 重建 + 别名切换 #  索引设计从来不是“一步到位”，业务演进到一定阶段，你几乎必然会遇到：\n 字段类型要改（text ↔ keyword，数值精度调整） 分析器要换（分词策略、同义词、归一化规则） 分片数要调整（原来的扩展因子不够用了）  这类变更不能在原索引上“硬改”，推荐统一走这条路线：\n 设计并创建新索引（my_index_v2），带上新 Mapping 与 settings 用 _reindex 或 scroll + bulk 从旧索引迁移数据 用别名原子切换：  my_index 从 my_index_v1 移除 my_index 新增指向 my_index_v2    示意（别名切换）：\nPOST /_aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;remove\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;my_index_v1\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;my_index\u0026#34; }}, { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;my_index_v2\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;my_index\u0026#34; }} ] } 要点：\n 应用层只使用别名，不直接写死真实索引名 重建过程中可以双写/只读/灰度切换，具体策略根据业务而定  6. 索引与分片设计的一点 Checklist #   这一坨数据是不是应该拆成多个索引？（按业务域 / 时间 / 租户） 单分片目标大小大致是多少？（结合存储和恢复策略） 主分片数与副本数是如何依据容量 \u0026amp; HA 要求确定的？ 是否为时间序列/多租户场景选用了合适的模式？ 是否已经预留了“新索引 + 重建 + 别名切换”的演进路线？  下一步可以继续阅读：\n  时间序列建模  多租户建模  别名（Aliases）  重建索引与零停机切换  参考手册（API 与参数） #  本篇更多讲的是“怎么设计”，如果你需要查具体的 API 与参数，可以参考：\n  别名（Aliases）  索引模板  重建数据（_reindex）  ","subcategory":null,"summary":"","tags":null,"title":"索引与分片设计实践","url":"/easysearch/main/docs/best-practices/index-design/"},{"category":null,"content":"添加处理器 #  append 处理器用于向字段添加值：\n 如果该字段是数组， append 处理器会将指定的值追加到该数组中。 如果该字段是标量字段， append 处理器将其转换为数组，并将指定的值追加到该数组中。 如果该字段不存在， append 处理器会创建一个包含指定值的数组。  语法 #  以下是为 append 处理器提供的语法：\n{ \u0026#34;append\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;your_target_field\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;your_appended_value\u0026#34;] } } 配置参数 #  下表列出了 append 处理器所需的和可选参数。\n   参数 是否必填 描述     field 必填 包含要附加数据的字段名称。支持模板使用。   value 必填 要附加的值。这可以是一个静态值或从现有字段派生的动态值。支持模板使用。   description 可选 处理器的一个简要描述。   if 可选 处理器运行的条件。   ignore_failure 可选 指定处理器是否在遇到错误时继续执行。如果设置为 true ，则忽略失败。默认为 false 。   allow_duplicates 可选 指定是否将字段中已存在的值附加。如果为 true ，则附加重复值。否则，将跳过。   on_failure 可选 在处理器失败时运行的处理器列表。   tag 可选 处理器的标识标签。在调试中区分同一类型的处理器很有用。    如何使用 #  按照以下步骤在管道中使用处理器。\n步骤 1：创建管道 #  以下查询创建了一个名为 user-behavior 的管道，该管道包含一个追加处理器。它将每个新文档的 page_view 追加到名为 event_types 的数组字段中：\nPUT _ingest/pipeline/user-behavior { \u0026#34;description\u0026#34;: \u0026#34;Pipeline that appends event type\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;append\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;event_types\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;page_view\u0026#34;] } } ] } 步骤 2（可选）：测试管道 #   建议您在摄取文档之前测试您的管道。\n 要测试管道，请运行以下查询：\nPOST _ingest/pipeline/user-behavior/_simulate { \u0026#34;docs\u0026#34;:[ { \u0026#34;_source\u0026#34;:{ } } ] } 返回的内容确认管道按预期工作：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;event_types\u0026#34;: [ \u0026#34;page_view\u0026#34; ] }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2023-08-28T16:55:10.621805166Z\u0026#34; } } } ] } 步骤 3：摄取文档 #\n 以下查询将文档索引到名为 testindex1 的索引中：\nPUT testindex1/_doc/1?pipeline=user-behavior { } 步骤 4（可选）：检索文档 #  要检索文档，请运行以下查询：\nGET testindex1/_doc/1 由于文档不包含 event_types 字段，将创建一个数组字段并将事件追加到该数组中：\n{ \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;_seq_no\u0026#34;: 1, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;event_types\u0026#34;: [ \u0026#34;page_view\u0026#34; ] } } ","subcategory":null,"summary":"","tags":null,"title":"添加处理器","url":"/easysearch/main/docs/features/ingest-pipelines/index-processors/append/"},{"category":null,"content":"测试环境部署指南 #  用最少资源快速搭建一个可运行的 Easysearch 实例，适合功能验证、开发联调和学习使用。\n最低硬件要求 #     项目 最低配置 建议配置     CPU 2 核 4 核   内存 4 GB 8 GB   磁盘 20 GB 50 GB SSD   JDK 11+ 17+     测试环境可使用 HDD，但 SSD 体验更佳。\n 一键安装（推荐） #  # 使用一键安装脚本 curl -sSL http://get.infini.cloud | bash -s -- -p easysearch # 调小 JVM 堆（测试环境 512MB 即可） sed -i 's/1g/512m/g' /data/easysearch/config/jvm.options\n# 初始化 cd /data/easysearch \u0026amp;\u0026amp; bin/initialize.sh -s\n# 调整目录权限 chown -R easysearch:easysearch /data/easysearch\n# 以 easysearch 用户启动 su easysearch -c \u0026quot;/data/easysearch/bin/easysearch -d\u0026quot; 初始化完成后，admin 密码会直接输出在终端中，请务必记住。也可以通过环境变量 EASYSEARCH_INITIAL_ADMIN_PASSWORD 预先指定密码（需要 1.8.2 及以后版本）。\nDocker 快速启动 #  如果已安装 Docker，可以用一条命令启动：\ndocker run -d \\  --name easysearch-test \\  -p 9200:9200 \\  -e \u0026#34;EASYSEARCH_INITIAL_ADMIN_PASSWORD=MyTest@2024\u0026#34; \\  -e \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; \\  infinilabs/easysearch:latest  更多 Docker 选项见 Docker 部署。\n MacOS 快速体验 #  # Homebrew 安装 JDK brew install openjdk@17 # 下载并解压 Easysearch # 具体下载地址见官方页面\ncd easysearch bin/initialize.sh -s bin/easysearch \n详见 MacOS 部署。\n 验证安装 #  # 测试连接（将 YOUR_PASSWORD 替换为初始化时终端输出的密码） curl -ku admin:YOUR_PASSWORD https://localhost:9200 # 预期输出（示例） { \u0026quot;name\u0026quot;: \u0026quot;node-1\u0026quot;, \u0026quot;cluster_name\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;cluster_uuid\u0026quot;: \u0026quot;\u0026hellip;\u0026quot;, \u0026quot;version\u0026quot;: { \u0026quot;distribution\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;number\u0026quot;: \u0026quot;x.y.z\u0026quot;, \u0026hellip; }, \u0026quot;tagline\u0026quot;: \u0026quot;You Know, For Easy Search!\u0026quot; } 快速写入与查询 #\n # 写入一条文档 curl -ku admin:YOUR_PASSWORD -X POST \u0026#34;https://localhost:9200/test/_doc/1\u0026#34; \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;Hello Easysearch\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;测试环境验证成功\u0026#34;}\u0026#39; # 搜索 curl -ku admin:YOUR_PASSWORD \u0026quot;https://localhost:9200/test/_search?q=Hello\u0026quot; 注意事项 #\n  测试环境不建议用于存放重要数据 单节点无高可用保障，集群状态为 yellow（因为副本无法分配到其他节点） 如需模拟分布式集群，可使用 Docker Compose 或 分布式集群部署  延伸阅读 #    生产环境部署  Docker Compose 集群  入门教程  ","subcategory":null,"summary":"","tags":null,"title":"测试环境部署","url":"/easysearch/main/docs/deployment/install-guide/test-env/"},{"category":null,"content":"核心概念 #  Easysearch 是一个分布式搜索分析引擎，建立在 Apache Lucene 基础之上。本页介绍 Easysearch 的核心概念，帮助你快速建立正确的心智模型。\n什么是 Easysearch #  Easysearch 是一个高性能的分布式搜索引擎，建立在全文搜索引擎库 Apache Lucene 基础之上。Lucene 可以说是当下最先进、高性能、全功能的搜索引擎库。\n但 Lucene 仅仅只是一个库。为了充分发挥其功能，你需要使用 Java 并将 Lucene 直接集成到应用程序中。Easysearch 在此基础上提供了一套简单一致的 RESTful API，使全文检索变得简单。\nEasysearch 不仅仅是全文搜索引擎，它可以被准确地形容为：\n 一个分布式的实时文档存储，每个字段可以被索引与搜索 一个分布式实时分析搜索引擎 能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或非结构化数据  面向文档 #  Easysearch 是面向文档（Document-Oriented）的：它存储整个对象或文档，并索引每个文档的内容使之可以被检索。在 Easysearch 中，我们对文档进行索引、检索、排序和过滤——而不是对行列数据。\nJSON #  Easysearch 使用 JSON 作为文档的序列化格式：\n{ \u0026#34;email\u0026#34;: \u0026#34;john@smith.com\u0026#34;, \u0026#34;first_name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Smith\u0026#34;, \u0026#34;info\u0026#34;: { \u0026#34;bio\u0026#34;: \u0026#34;Eco-warrior and defender of the weak\u0026#34;, \u0026#34;age\u0026#34;: 25, \u0026#34;interests\u0026#34;: [ \u0026#34;dolphins\u0026#34;, \u0026#34;whales\u0026#34; ] }, \u0026#34;join_date\u0026#34;: \u0026#34;2014/05/01\u0026#34; } 在 Easysearch 中将对象转化为 JSON 后构建索引，要比在扁平的表结构中简单得多。\n文档元数据 #  一个文档不仅包含数据，还包含元数据：\n   元数据 说明     _index 文档所属的索引，是逻辑命名空间   _id 文档的唯一标识符，可自定义或自动生成   _source 文档的原始 JSON 内容    _index 和 _id 组合可以唯一确定 Easysearch 中的一个文档。\n 更多文档建模内容，详见 文档设计。\n 索引与分片 #  索引（Index） 是逻辑上的数据集合，通常对应一类业务数据（例如 orders、products）。索引由两类要素决定：\n settings：分片数、副本数、刷新间隔、分析器配置等 mappings：字段结构与类型、动态映射规则等  一个索引会被切分为多个分片（shard），分布在不同节点上。分片是索引在物理层面的切分单元，可以理解为\u0026quot;一个独立的 Lucene 索引\u0026quot;。\n 主分片（primary shard）：负责接收写入并生成变更（默认 1 个主分片） 副本分片（replica shard）：用于高可用与提升查询吞吐（默认 1 份副本）  分片的数量与大小直接影响：\n 查询并行度与延迟 写入并行度与热点 资源使用与故障恢复时间   \u0026ldquo;索引\u0026quot;一词的多重含义\n在 Easysearch 语境中，\u0026ldquo;索引\u0026quot;有三层含义：\n 索引（名词）：类似于关系数据库中的\u0026quot;数据库\u0026rdquo;，是存储文档的逻辑容器 索引（动词）：将一个文档存储到索引中以便被检索和查询 倒排索引：Easysearch 底层使用的数据结构，详见 Lucene 底层原理   副本 #  副本是主分片的冗余拷贝：\n 主分片所在节点故障时可提升为主分片，保证服务连续性 可以承载查询流量，提升读吞吐  副本数越多，可用性与查询吞吐通常越好，但成本更高（磁盘/CPU/内存/网络）。\n集群与节点 #  一个运行中的 Easysearch 实例称为一个节点，集群是由一个或多个拥有相同 cluster.name 配置的节点组成，它们共同承担数据和负载的压力。\n 当有节点加入或移除时，集群会自动重新分配数据 被选举为主节点的节点负责管理集群范围内的变更（增删索引、增删节点等），但不需要参与文档级别的操作 每个节点都知道任意文档所处的位置，能够将请求直接转发到正确的节点   集群的扩容、故障转移和分片分配的详细过程，请参阅 分布式基础。\n 路由 #  路由决定一份文档\u0026quot;落到哪个主分片\u0026rdquo;。默认基于 _id 计算哈希并映射到分片号。\n 写入路由：决定写入去哪个主分片 查询路由：提供与写入一致的路由值，可将查询范围从\u0026quot;全分片\u0026quot;缩小到\u0026quot;单分片/少量分片\u0026quot;   路由公式与详细机制，请参阅 分布式写入过程。\n 概念速查 #     概念 说明     Index 逻辑集合，对应一类业务数据   Shard 物理切分单元，每个分片是一个 Lucene 索引   Replica 主分片的冗余拷贝，用于高可用与读扩展   Document 最小数据单元，JSON 格式   Cluster 一组协同工作的节点   Node 一个运行中的 Easysearch 实例   Routing 决定文档归属哪个分片的机制    下一步可以继续阅读：\n  分布式基础：集群扩容、故障恢复、分片分配  Lucene 底层原理：倒排索引与段结构  数据建模：文档设计、Nested、Parent-Child、反范式  ","subcategory":null,"summary":"","tags":null,"title":"核心概念","url":"/easysearch/main/docs/fundamentals/concepts/"},{"category":null,"content":"映射基础 #  映射也叫 Mapping，定义了索引中文档的“字段结构与类型”，是搜索行为的基础。搞清楚 Mapping，很多“为什么搜不到/搜不准”的问题就会迎刃而解。\n核心概念 #  Mapping 负责什么？ #   字段类型：text / keyword / 数值 / 日期 / 布尔 / 对象 / nested 等 字段是否可搜索、是否可排序/聚合（doc_values、fielddata 等） 是否启用 _source、动态映射策略等  可以认为 Mapping 是 Easysearch 的\u0026quot;模式（schema）\u0026quot;，但比传统数据库更灵活。\n精确值 vs 全文 #  在 Easysearch 里，大致可以把字段分成两类：\n 精确值：ID、日期、数值、状态码、枚举、用户名、邮箱等  Foo 与 foo 被视为不同 2014 与 2014-09-15 也被视为不同值   全文：自然语言文本，如标题、正文、评论内容等  我们更关心“匹不匹配、相关度高不高”，而不是完全相同    精确值的查询语义更像 SQL 里的：\nWHERE name = \u0026#34;John Smith\u0026#34; AND user_id = 2 AND date \u0026gt; \u0026#34;2014-09-15\u0026#34; 全文则是“相关性排序”的世界：搜索的是“像什么”“更接近什么”，而不是绝对等号。这也是为什么同样是字符串字段，有的该按精确值建模，有的该按全文建模。\ntext vs keyword：最重要的一对类型 #   text：  写入时会被分析器分词、归一化 适合全文检索（match/match_phrase 等） 不适合直接排序/聚合（需要专门配置）   keyword：  不分词，整体作为一个值 适合精确过滤、排序、聚合（terms、sort） 不适合做自然语言级别的模糊匹配    人类可读字符串字段，通常需要同时提供 text + keyword 两个视角（详见“Mapping 模式与最佳实践”）。\n设计与选择 #  字段类型选择：一个简单决策树 #  可以先按下面几步思考：\n 这是“描述信息”还是“精确属性”？  描述信息（标题、正文、评论等） → text 为主，考虑是否需要额外 keyword 精确属性（ID、状态、枚举、code） → keyword / 数值 / 布尔 / 日期   将来要不要排序 / 聚合？  需要：字段类型必须支持 doc_values（keyword、数值、日期、布尔等） 纯全文相关性，不排序不聚合：text 即可   是否要支持多种视角（全文 + 精确）？  需要：用 multi-fields 同时暴露 field（text）与 field.keyword（keyword）    几个典型场景：\n 日志消息：message → text，必要时再加 message.keyword 方便聚合/精确过滤 订单：order_id → keyword；status → keyword；amount → 数值；created_at → date 商品：title → text + title.keyword；brand → keyword；price → 数值；tags → keyword 数组  动态映射与显式映射 #  默认情况下，索引可以使用动态映射：当新字段第一次出现在文档中时，系统会尝试根据值推断字段类型。\n优点：\n 上手快，不需要一开始写完整的 Mapping  缺点：\n 推断结果不一定符合预期（例如数字字符串被当成数值） 字段名稍有差异就会产生大量字段，影响管理与性能  建议：\n 对核心业务索引，尽量为关键字段写显式 Mapping 可以配合动态模板限制或规范动态映射行为  字段类型速览 #  常见字段类型概览 #   字符串：  text：全文检索 keyword：精确匹配、排序、聚合   数值：  integer、long、float、double 等   日期：  date：支持多种格式，适合时间范围查询与聚合   布尔：  boolean：true/false   结构：  object：嵌套对象（扁平存储，多值字段） nested：真正的嵌套文档，支持精确的子对象查询（见“数据建模”章节）    multi-fields：一份数据，多种索引视角 #  multi-fields 允许你在 Mapping 中为同一源字段定义多个“索引视图”。最常见的模式是：全文 + 精确值：\nPUT /products { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, // 用于全文检索 \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; // 用于过滤、聚合、排序 } } } } } } 查询时的典型用法：\n title：match / multi_match 等全文检索 title.keyword：term/terms + 聚合 + 排序  总结与进阶 #  重点回顾 #   Mapping 是所有查询/聚合行为的基础，设计字段类型时要先想清楚\u0026quot;将来如何用这些字段\u0026quot; 对重要索引，应使用显式 Mapping 管控字段类型与命名 text/keyword 的区别要牢记，这是绝大多数搜索问题的根源之一  下一步可以继续阅读：\n  映射模式与常见坑  Query DSL 基础  结构化搜索  参考手册与 API #    字段类型总览（功能手册）  映射参数（功能手册）  元字段（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"映射基础","url":"/easysearch/main/docs/features/mapping-and-analysis/mapping-basics/"},{"category":null,"content":"文档设计 #  这一页回答两个问题：应该把什么放进一个文档？字段怎么设计才适合搜索？ 这里聚焦单个文档层面的建模，跨文档关系（Nested、Parent-Child、反范式）放在后续章节。\n什么是文档 #  在 Easysearch 中，一个 文档（Document） 是被序列化为 JSON 的最顶层对象，指定了唯一 ID 并存储到 Easysearch 中。例如：\n{ \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;age\u0026#34;: 42, \u0026#34;confirmed\u0026#34;: true, \u0026#34;join_date\u0026#34;: \u0026#34;2014-06-01\u0026#34;, \u0026#34;home\u0026#34;: { \u0026#34;lat\u0026#34;: 51.5, \u0026#34;lon\u0026#34;: 0.1 }, \u0026#34;accounts\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;facebook\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;johnsmith\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;twitter\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;johnsmith\u0026#34; } ] } 文档可以包含字符串、数字、布尔、日期、嵌套对象、数组等多种类型。\n文档元数据 #  每个文档都有三个核心元数据：\n   元数据 说明     _index 文档存放的索引，是逻辑命名空间   _id 文档的唯一标识符，可自定义或自动生成   _source 文档的原始 JSON 内容    此外，每个文档还有 _version 字段——每次对文档修改（包括删除）时版本号递增，用于并发控制。\n一个\u0026quot;好文档\u0026quot;的几个特征 #   字段含义清晰、类型正确（text/keyword/数值/日期等） 能支撑主要的搜索与聚合需求，而不是只反映存储表结构 重要的过滤/排序/聚合维度都有对应字段 更新策略是可控的（全量更新、部分更新、幂等写入等）  可以先从\u0026quot;业务查询需求\u0026quot;倒推字段设计，而不是从数据库表结构直接平铺。\n字段类型与多字段（multi-fields） #  同一份业务信息，往往需要不同的查询方式，例如\u0026quot;商品名称\u0026quot;：\n 既要支持全文检索（模糊匹配） 又要支持精确匹配/去重/聚合（关键词级别）  推荐做法是使用多字段（multi-fields），例如：\n name（text）：用于全文检索 name.keyword（keyword）：用于精确过滤、排序、聚合  实务建议：\n 人类可读文本：通常需要 text + keyword 两个视角 ID、编码、枚举：直接用 keyword/数值/布尔类型即可 金额、计数、比例：用适当的数值类型，并为排序/聚合做好 doc_values 支持  标识符与路由字段 #  _id 与业务字段的考虑：\n _id：内部唯一标识，用于幂等写入、精确读取/删除 业务 ID（如 user_id、order_id）：通常也会单独做成字段，用于过滤、聚合、权限控制等  在多租户/分库分表等场景下，可以考虑额外的路由字段（如 tenant_id），后续在索引设计和路由策略中使用（见 多租户建模、 分布式写入）。\n更新模式与幂等性 #  常见的文档更新模式：\n 全量替换（put index）：一次写入整个文档  简单可靠，适合文档相对较小、更新频率不高的场景   部分更新（update + doc/script）：只更新部分字段  适合字段很多但部分字段频繁变更的场景    从建模角度，需要考虑：\n 是否有字段可以晚一点异步更新（例如离线计算得出的统计值） 是否需要为\u0026quot;最终一致\u0026quot;的字段准备单独的更新通道  文档结构要能支撑你想要的更新模式。\n并发控制：乐观锁 #  Easysearch 使用乐观并发控制：不会阻塞操作，而是利用版本号来避免冲突。\n在更新文档时指定版本号，只有版本匹配时更新才会成功：\nPUT /website/_doc/1?if_seq_no=5\u0026amp;if_primary_term=1 { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Starting to get the hang of this...\u0026#34; } 如果版本冲突（其他进程已更新），Easysearch 返回 409 Conflict，应用程序可以选择重试或通知用户。\n常见场景：\n 不关心冲突：直接写入覆盖（如从主数据库同步数据） 需要避免冲突：使用 if_seq_no + if_primary_term 做乐观锁 外部版本号：如果主数据库已有版本号（如时间戳），可通过 version_type=external 复用  小结 #   文档是 JSON 对象，包含 _index、_id、_source 等元数据 以\u0026quot;搜索/过滤/聚合需求\u0026quot;驱动字段设计，而不是简单照搬表结构 合理利用 multi-fields 同时支持全文检索与精确过滤 为标识符与路由预留好字段，方便后续扩展多租户策略 适度冗余换取查询简化与性能，但要控制冗余的数量与更新成本  下一步可以继续阅读：\n  反范式与权衡  映射基础  文档操作  ","subcategory":null,"summary":"","tags":["最佳实践","数据建模"],"title":"文档设计","url":"/easysearch/main/docs/best-practices/data-modeling/document-design/"},{"category":null,"content":"接入企业认证体系 #  在企业环境中，Easysearch 通常需要与已有的身份认证系统集成，实现统一的用户管理和单点登录。本文介绍常见的集成模式与配置要点。\n相关指南 #    安全 API 概述  用户与角色管理  多租户与权限模型  认证模式概览 #     模式 适用场景 复杂度 说明     内置用户 小型团队、开发测试 低 直接在 Easysearch 中管理用户和密码   LDAP/AD 已有 Active Directory 中 对接企业目录服务，集中管理用户   OIDC/OAuth 2.0 SSO 场景、云原生架构 中 对接 Keycloak、Auth0、Azure AD 等   反向代理 + Header 已有统一认证网关 低 由网关完成认证，Header 传递身份信息   SAML 2.0 企业级 SSO 高 对接 ADFS、Okta 等 SAML IdP    LDAP/Active Directory 集成 #  LDAP 是最常见的企业认证集成方式，将 Easysearch 的用户验证委派给现有的 LDAP 或 Active Directory。\n配置示例 #  在 easysearch-security 配置中添加 LDAP 认证后端：\n# config/security/config.yml _meta: type: \u0026#34;config\u0026#34; config_version: 2 config: dynamic: authc: ldap_auth: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: basic challenge: true authentication_backend: type: ldap config: hosts: - ldap.example.com:636 enable_ssl: true bind_dn: \u0026quot;cn=admin,dc=example,dc=com\u0026quot; password: \u0026quot;admin_password\u0026quot; userbase: \u0026quot;ou=users,dc=example,dc=com\u0026quot; usersearch: \u0026quot;(uid={0})\u0026quot; 角色映射 #\n 将 LDAP 组映射到 Easysearch 角色：\n# config/security/roles_mapping.yml admin_role: backend_roles: - \u0026#34;cn=es-admins,ou=groups,dc=example,dc=com\u0026#34; readonly_role: backend_roles: - \u0026quot;cn=es-viewers,ou=groups,dc=example,dc=com\u0026quot; OIDC 集成 #\n OpenID Connect 适用于与 Keycloak、Azure AD、Okta 等 IdP 对接的 SSO 场景。\n典型流程 #  用户 → 浏览器 → Easysearch Dashboards ↓ 302 重定向 OIDC IdP（登录页面） ↓ Authorization Code Easysearch（Token 验证） ↓ 返回数据 / Dashboards 页面 关键配置项 #     配置项 说明 示例     openid_connect_url IdP 的 Discovery URL https://keycloak/realms/es/.well-known/...   client_id 注册的客户端 ID easysearch-client   client_secret 客户端密钥 xxxxxxxx   subject_key JWT 中的用户标识字段 preferred_username   roles_key JWT 中的角色字段 realm_access.roles    反向代理认证 #  当企业已有统一认证网关（如 Nginx + OAuth2 Proxy、APISIX、Kong）时，可以通过 HTTP Header 传递已认证的用户身份。\n架构 #  用户 → 认证网关（完成认证）→ 注入 X-Proxy-User Header → Easysearch 配置要点 #  proxy_auth: http_enabled: true transport_enabled: false order: 0 http_authenticator: type: proxy challenge: false config: user_header: \u0026#34;X-Proxy-User\u0026#34; roles_header: \u0026#34;X-Proxy-Roles\u0026#34; authentication_backend: type: noop  安全提示：务必确保 Easysearch 仅接受来自可信代理的请求，建议通过网络策略或 IP 白名单限制直接访问。\n 安全实践建议 #     实践 说明     启用 HTTPS 所有认证流程必须走 TLS 加密通道   最小权限原则 角色映射时只授予必要的索引和操作权限   审计日志 开启审计日志记录认证成功/失败事件   密钥轮换 定期更换 LDAP Bind 密码和 OIDC Client Secret   失败锁定 配置连续认证失败后的账户锁定策略    ","subcategory":null,"summary":"","tags":null,"title":"接入企业认证体系","url":"/easysearch/main/docs/integrations/security/auth-integration/"},{"category":null,"content":"快速入门 #  为了让大家对 Easysearch 能实现什么及其上手难易程度有一个基本印象，让我们从一个简单的教程开始并介绍索引、搜索及聚合等基础概念。\n我们将一并介绍一些新的技术术语，即使无法立即全部理解它们也无妨，因为在本书后续内容中，我们将继续深入介绍这里提到的所有概念。\n接下来尽情享受 Easysearch 探索之旅。\n创建一个雇员目录 #  我们受雇于 Megacorp 公司，作为 HR 部门新的\u0026quot;热爱无人机\u0026quot;（\u0026ldquo;We love our drones!\u0026quot;）激励项目的一部分，我们的任务是为此创建一个员工目录。该目录应当能培养员工认同感及支持实时、高效、动态协作，因此有一些业务需求：\n 支持包含多值标签、数值、以及全文本的数据 检索任一员工的完整信息 允许结构化搜索，比如查询 30 岁以上的员工 允许简单的全文搜索以及较复杂的短语搜索 支持在匹配文档内容中高亮显示搜索片段 支持基于数据创建和管理分析仪表盘  索引员工文档 #  第一个业务需求是存储员工数据。这将会以员工文档的形式存储：一个文档代表一个员工。存储数据到 Easysearch 的行为叫做索引，但在索引一个文档之前，需要确定将文档存储在哪里。\n一个 Easysearch 集群可以包含多个索引，相应的每个索引可以包含多个文档，每个文档又有多个属性。\n 注意：索引这个词在 Easysearch 语境中有多种含义：\n 索引（名词）：一个索引类似于传统关系数据库中的一个数据库，是一个存储关系型文档的地方。索引的复数词为 indices 或 indexes。 索引（动词）：索引一个文档就是存储一个文档到一个索引（名词）中以便被检索和查询。这非常类似于 SQL 语句中的 INSERT 关键词，除了文档已存在时，新文档会替换旧文档情况之外。 倒排索引：Easysearch 和 Lucene 使用了一个叫做倒排索引的结构来达到快速检索的目的。默认的，一个文档中的每一个属性都是被索引的（有一个倒排索引）和可搜索的。   对于员工目录，我们将做如下操作：\n 每个员工索引一个文档，文档包含该员工的所有信息 该类型位于索引 megacorp 内 该索引保存在我们的 Easysearch 集群中  实践中这非常简单（尽管看起来有很多步骤），我们可以通过一条命令完成所有这些动作：\nPUT /megacorp/_doc/1 { \u0026#34;first_name\u0026#34; : \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34; : \u0026#34;Smith\u0026#34;, \u0026#34;age\u0026#34; : 25, \u0026#34;about\u0026#34; : \u0026#34;I love to go rock climbing\u0026#34;, \u0026#34;interests\u0026#34;: [ \u0026#34;sports\u0026#34;, \u0026#34;music\u0026#34; ] } 注意，路径 /megacorp/_doc/1 包含了三部分的信息：\n megacorp：索引名称 _doc：文档类型（Easysearch 中类型统一为 _doc） 1：特定雇员的ID  请求体——JSON 文档——包含了这位员工的所有详细信息，他的名字叫 John Smith，今年 25 岁，喜欢攀岩。\n很简单！无需进行执行管理任务，如创建一个索引或指定每个属性的数据类型之类的，可以直接只索引一个文档。Easysearch 默认地完成其他一切，因此所有必需的管理任务都在后台使用默认设置完成。\n进行下一步前，让我们增加更多的员工信息到目录中：\nPUT /megacorp/_doc/2 { \u0026#34;first_name\u0026#34; : \u0026#34;Jane\u0026#34;, \u0026#34;last_name\u0026#34; : \u0026#34;Smith\u0026#34;, \u0026#34;age\u0026#34; : 32, \u0026#34;about\u0026#34; : \u0026#34;I like to collect rock albums\u0026#34;, \u0026#34;interests\u0026#34;: [ \u0026#34;music\u0026#34; ] } PUT /megacorp/_doc/3 { \u0026quot;first_name\u0026quot; : \u0026quot;Douglas\u0026quot;, \u0026quot;last_name\u0026quot; : \u0026quot;Fir\u0026quot;, \u0026quot;age\u0026quot; : 35, \u0026quot;about\u0026quot;: \u0026quot;I like to build cabinets\u0026quot;, \u0026quot;interests\u0026quot;: [ \u0026quot;forestry\u0026quot; ] } 检索文档 #\n 目前我们已经在 Easysearch 中存储了一些数据，接下来就能专注于实现应用的业务需求了。第一个需求是可以检索到单个雇员的数据。\n这在 Easysearch 中很简单。简单地执行一个 HTTP GET 请求并指定文档的地址——索引库和ID。使用这两个信息可以返回原始的 JSON 文档：\nGET /megacorp/_doc/1 返回结果包含了文档的一些元数据，以及 _source 属性，内容是 John Smith 雇员的原始 JSON 文档：\n{ \u0026#34;_index\u0026#34; : \u0026#34;megacorp\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34; : 1, \u0026#34;found\u0026#34; : true, \u0026#34;_source\u0026#34; : { \u0026#34;first_name\u0026#34; : \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34; : \u0026#34;Smith\u0026#34;, \u0026#34;age\u0026#34; : 25, \u0026#34;about\u0026#34; : \u0026#34;I love to go rock climbing\u0026#34;, \u0026#34;interests\u0026#34;: [ \u0026#34;sports\u0026#34;, \u0026#34;music\u0026#34; ] } }  提示：将 HTTP 命令由 PUT 改为 GET 可以用来检索文档，同样的，可以使用 DELETE 命令来删除文档，以及使用 HEAD 指令来检查文档是否存在。如果想更新已存在的文档，只需再次 PUT。\n 轻量搜索 #  一个 GET 是相当简单的，可以直接得到指定的文档。现在尝试点儿稍微高级的功能，比如一个简单的搜索！\n第一个尝试的几乎是最简单的搜索了。我们使用下列请求来搜索所有雇员：\nGET /megacorp/_search 可以看到，我们仍然使用索引库 megacorp，但与指定一个文档 ID 不同，这次使用 _search。返回结果包括了所有三个文档，放在数组 hits 中。一个搜索默认返回十条结果。\n接下来，尝试下搜索姓氏为 Smith 的雇员。为此，我们将使用一个查询字符串搜索，很容易通过命令行完成。这个方法一般涉及到一个查询字符串（query-string）搜索，因为我们通过一个URL参数来传递查询信息给搜索接口：\nGET /megacorp/_search?q=last_name:Smith 我们仍然在请求路径中使用 _search 端点，并将查询本身赋值给参数 q=。返回结果给出了所有的 Smith。\n使用查询表达式搜索 #  Query-string 搜索通过命令非常方便地进行临时性的即席搜索，但它有自身的局限性。Easysearch 提供一个丰富灵活的查询语言叫做查询表达式（Query DSL），它支持构建更加复杂和健壮的查询。\n领域特定语言（DSL），使用 JSON 构造了一个请求。我们可以像这样重写之前的查询所有名为 Smith 的搜索：\nGET /megacorp/_search { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;last_name\u0026#34; : \u0026#34;Smith\u0026#34; } } } 返回结果与之前的查询一样，但还是可以看到有一些变化。其中之一是，不再使用 query-string 参数，而是一个请求体替代。这个请求使用 JSON 构造，并使用了一个 match 查询（属于查询类型之一，后面将继续介绍）。\n更复杂的搜索 #  现在尝试下更复杂的搜索。同样搜索姓氏为 Smith 的员工，但这次我们只需要年龄大于 30 的。查询需要稍作调整，使用过滤器 filter，它支持高效地执行一个结构化查询。\nGET /megacorp/_search { \u0026#34;query\u0026#34; : { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;last_name\u0026#34; : \u0026#34;smith\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;range\u0026#34; : { \u0026#34;age\u0026#34; : { \u0026#34;gt\u0026#34; : 30 } } } } } } 这部分与我们之前使用的 match 查询一样。这部分是一个 range 过滤器，它能找到年龄大于 30 的文档，其中 gt 表示大于（great than）。\n目前无需太多担心语法问题，后续会更详细地介绍。只需明确我们添加了一个过滤器用于执行一个范围查询，并复用之前的 match 查询。现在结果只返回了一名员工，叫 Jane Smith，32 岁。\n全文搜索 #  截止目前的搜索相对都很简单：单个姓名，通过年龄过滤。现在尝试下稍微高级点儿的全文搜索——一项传统数据库确实很难搞定的任务。\n搜索下所有喜欢攀岩（rock climbing）的员工：\nGET /megacorp/_search { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;about\u0026#34; : \u0026#34;rock climbing\u0026#34; } } } 显然我们依旧使用之前的 match 查询在 about 属性上搜索 rock climbing。得到两个匹配的文档。\nEasysearch 默认按照相关性得分排序，即每个文档跟查询的匹配程度。第一个最高得分的结果很明显：John Smith 的 about 属性清楚地写着 rock climbing。\n但为什么 Jane Smith 也作为结果返回了呢？原因是她的 about 属性里提到了 rock。因为只有 rock 而没有 climbing，所以她的相关性得分低于 John 的。\n这是一个很好的案例，阐明了 Easysearch 如何在全文属性上搜索并返回相关性最强的结果。Easysearch 中的相关性概念非常重要，也是完全区别于传统关系型数据库的一个概念，数据库中的一条记录要么匹配要么不匹配。\n短语搜索 #  找出一个属性中的独立单词是没有问题的，但有时候想要精确匹配一系列单词或者短语。比如，我们想执行这样一个查询，仅匹配同时包含 rock 和 climbing，并且二者以短语 rock climbing 的形式紧挨着的雇员记录。\n为此对 match 查询稍作调整，使用一个叫做 match_phrase 的查询：\nGET /megacorp/_search { \u0026#34;query\u0026#34; : { \u0026#34;match_phrase\u0026#34; : { \u0026#34;about\u0026#34; : \u0026#34;rock climbing\u0026#34; } } } 毫无悬念，返回结果仅有 John Smith 的文档。\n高亮搜索 #  许多应用都倾向于在每个搜索结果中高亮部分文本片段，以便让用户知道为何该文档符合查询条件。在 Easysearch 中检索出高亮片段也很容易。\n再次执行前面的查询，并增加一个新的 highlight 参数：\nGET /megacorp/_search { \u0026#34;query\u0026#34; : { \u0026#34;match_phrase\u0026#34; : { \u0026#34;about\u0026#34; : \u0026#34;rock climbing\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34; : { \u0026#34;about\u0026#34; : {} } } } 当执行该查询时，返回结果与之前一样，与此同时结果中还多了一个叫做 highlight 的部分。这个部分包含了 about 属性匹配的文本片段，并以 HTML 标签 \u0026lt;em\u0026gt;\u0026lt;/em\u0026gt; 封装：\n{ ... \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;max_score\u0026#34;: 0.23013961, \u0026#34;hits\u0026#34;: [ { ... \u0026#34;_score\u0026#34;: 0.23013961, \u0026#34;_source\u0026#34;: { \u0026#34;first_name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Smith\u0026#34;, \u0026#34;age\u0026#34;: 25, \u0026#34;about\u0026#34;: \u0026#34;I love to go rock climbing\u0026#34;, \u0026#34;interests\u0026#34;: [ \u0026#34;sports\u0026#34;, \u0026#34;music\u0026#34; ] }, \u0026#34;highlight\u0026#34;: { \u0026#34;about\u0026#34;: [ \u0026#34;I love to go \u0026lt;em\u0026gt;rock\u0026lt;/em\u0026gt; \u0026lt;em\u0026gt;climbing\u0026lt;/em\u0026gt;\u0026#34; ] } } ] } } 分析 #  终于到了最后一个业务需求：支持管理者对员工目录做分析。Easysearch 有一个功能叫聚合（aggregations），允许我们基于数据生成一些精细的分析结果。聚合与 SQL 中的 GROUP BY 类似但更强大。\n举个例子，挖掘出员工中最受欢迎的兴趣爱好：\nGET /megacorp/_search { \u0026#34;aggs\u0026#34;: { \u0026#34;all_interests\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;interests\u0026#34; } } } } 暂时忽略掉语法，直接看看结果：\n{ ... \u0026#34;hits\u0026#34;: { ... }, \u0026#34;aggregations\u0026#34;: { \u0026#34;all_interests\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;music\u0026#34;, \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: \u0026#34;forestry\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: \u0026#34;sports\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 } ] } } } 可以看到，两位员工对音乐感兴趣，一位对林业感兴趣，一位对运动感兴趣。这些聚合的结果数据并非预先统计，而是根据匹配当前查询的文档即时生成的。如果想知道叫 Smith 的员工中最受欢迎的兴趣爱好，可以直接构造一个组合查询：\nGET /megacorp/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;last_name\u0026#34;: \u0026#34;smith\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;all_interests\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;interests\u0026#34; } } } } all_interests 聚合已经变为只包含匹配查询的文档。\n聚合还支持分级汇总。比如，查询特定兴趣爱好员工的平均年龄：\nGET /megacorp/_search { \u0026#34;aggs\u0026#34; : { \u0026#34;all_interests\u0026#34; : { \u0026#34;terms\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;interests\u0026#34; }, \u0026#34;aggs\u0026#34; : { \u0026#34;avg_age\u0026#34; : { \u0026#34;avg\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;age\u0026#34; } } } } } } 得到的聚合结果有点儿复杂，但理解起来还是很简单的：\n... \u0026#34;all_interests\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;music\u0026#34;, \u0026#34;doc_count\u0026#34;: 2, \u0026#34;avg_age\u0026#34;: { \u0026#34;value\u0026#34;: 28.5 } }, { \u0026#34;key\u0026#34;: \u0026#34;forestry\u0026#34;, \u0026#34;doc_count\u0026#34;: 1, \u0026#34;avg_age\u0026#34;: { \u0026#34;value\u0026#34;: 35 } }, { \u0026#34;key\u0026#34;: \u0026#34;sports\u0026#34;, \u0026#34;doc_count\u0026#34;: 1, \u0026#34;avg_age\u0026#34;: { \u0026#34;value\u0026#34;: 25 } } ] } 输出基本是第一次聚合的加强版。依然有一个兴趣及数量的列表，只不过每个兴趣都有了一个附加的 avg_age 属性，代表有这个兴趣爱好的所有员工的平均年龄。\n即使现在不太理解这些语法也没有关系，依然很容易了解到复杂聚合及分组通过 Easysearch 特性实现得很完美，能够提取的数据类型也没有任何限制。\n教程结语 #  欣喜的是，这是一个关于 Easysearch 基础描述的教程，且仅仅是浅尝辄止，更多诸如 suggestions、geolocation、percolation、fuzzy 与 partial matching 等特性均被省略，以便保持教程的简洁。但它确实突显了开始构建高级搜索功能多么容易。不需要配置——只需要添加数据并开始搜索！\n很可能语法会让你在某些地方有所困惑，并且对各个方面如何微调也有一些问题。没关系！本书后续内容将针对每个问题详细解释，让你全方位地理解 Easysearch 的工作原理。\n分布式特性 #  在本章开头，我们提到过 Easysearch 可以横向扩展至数百（甚至数千）的服务器节点，同时可以处理 PB 级数据。我们的教程给出了一些使用 Easysearch 的示例，但并不涉及任何内部机制。Easysearch 天生就是分布式的，并且在设计时屏蔽了分布式的复杂性。\nEasysearch 在分布式方面几乎是透明的。教程中并不要求了解分布式系统、分片、集群发现或其他的各种分布式概念。可以使用笔记本上的单节点轻松地运行教程里的程序，但如果你想要在 100 个节点的集群上运行程序，一切依然顺畅。\nEasysearch 尽可能地屏蔽了分布式系统的复杂性。这里列举了一些在后台自动执行的操作：\n 分配文档到不同的容器或分片中，文档可以储存在一个或多个节点中 按集群节点来均衡分配这些分片，从而对索引和搜索过程进行负载均衡 复制每个分片以支持数据冗余，从而防止硬件故障导致的数据丢失 将集群中任一节点的请求路由到存有相关数据的节点 集群扩容时无缝整合新节点，重新分配分片以便从离群节点恢复  当阅读本书时，将会遇到有关 Easysearch 分布式特性的补充章节。这些章节将介绍有关集群扩容、故障转移、应对文档存储、执行分布式搜索，以及分区（shard）及其工作原理。\n这些章节并非必读，完全可以无需了解内部机制就使用 Easysearch，但是它们将从另一个角度帮助你了解更完整的 Easysearch 知识。可以根据需要跳过它们，或者想更完整地理解时再回头阅读也无妨。\n后续步骤 #  现在大家对于通过 Easysearch 能够实现什么样的功能、以及上手的难易程度应该已经有了初步概念。Easysearch 力图通过最少的知识和配置做到开箱即用。学习 Easysearch 的最好方式是投入实践：尽情开始索引和搜索吧！\n然而，对于 Easysearch 知道得越多，就越有生产效率。告诉 Easysearch 越多的领域知识，就越容易进行结果调优。\n本书的后续内容将帮助你从新手成长为专家，每个章节不仅阐述必要的基础知识，而且包含专家建议。如果刚刚上手，这些建议可能无法立竿见影；但 Easysearch 有着合理的默认设置，在无需干预的情况下通常都能工作得很好。当你开始追求毫秒级的性能提升时，随时可以重温这些章节。\n小结 #   Easysearch 是一个分布式的实时文档存储，每个字段可以被索引与搜索 文档以 JSON 格式存储，支持复杂的数据结构 可以通过 RESTful API 与 Easysearch 交互 索引、搜索、聚合等操作都非常简单，开箱即用 Easysearch 天生就是分布式的，屏蔽了分布式系统的复杂性  下一步可以继续阅读：\n  基础理论：深入了解集群、分片、近实时等核心概念  写入与存储：学习如何高效写入数据  搜索：深入学习 Query DSL 和各种搜索功能  聚合分析：学习如何使用聚合进行数据分析  ","subcategory":null,"summary":"","tags":null,"title":"快速入门","url":"/easysearch/main/docs/quick-start/tutorial/"},{"category":null,"content":"常用 URL 参数 #  Easysearch 的 REST API 支持以下通用参数，适用于大部分接口。\n   参数 说明 示例     pretty 以缩进格式返回 JSON，便于阅读和调试。 GET \u0026lt;index_name\u0026gt;/_search?pretty=true   human 将输出中的数值转换为人类可读格式（如 1h 代替 3600000ms，1kb 代替 1024 bytes）。 GET \u0026lt;index_name\u0026gt;/_search?human=true   error_trace 当请求出错时，在响应中包含完整的异常堆栈信息，便于排查问题。 GET \u0026lt;index_name\u0026gt;/_search?error_trace=true   format 指定返回格式。CAT API 支持 json、yaml、text 等格式。 GET _cat/indices?format=json   filter_path 过滤响应字段，只返回指定路径的内容，减少网络传输量。支持通配符 *。 GET _cluster/health?filter_path=status,number_of_nodes   flat_settings 以扁平化格式返回设置（index.number_of_shards 而非嵌套对象）。 GET \u0026lt;index_name\u0026gt;/_settings?flat_settings=true   Content-Type 请求头，指定请求体的内容类型。支持 application/json、application/yaml、application/cbor。 POST _scripts/\u0026lt;template\u0026gt; -H 'Content-Type: application/json'   source / source_content_type 当客户端不支持在非 POST 请求中发送请求体时，可通过查询参数传递请求体内容及其类型。 GET _search?source_content_type=application/json\u0026amp;source={\u0026quot;query\u0026quot;:{\u0026quot;match_all\u0026quot;:{}}}    使用建议 #   开发调试时建议加上 ?pretty=true，生产环境去掉以减少响应体积 监控脚本中使用 filter_path 只获取需要的字段，降低解析开销 排查异常时加上 error_trace=true 获取完整错误堆栈  ","subcategory":null,"summary":"","tags":null,"title":"常用 URL 参数","url":"/easysearch/main/docs/api-reference/common-parameters/"},{"category":null,"content":"复杂逻辑 #  在摄取管道中，处理器中的 if 参数可以使用 Painless 脚本来评估复杂条件。这些条件有助于微调文档处理，允许进行高级逻辑，例如类型检查、正则表达式和组合多个标准。\n多条件检查 #  您可以将逻辑运算符如 \u0026amp;\u0026amp; （与）、 || （或）和 ! （非）组合起来构建更复杂的条件。以下管道标签将文档标记为 spam ，如果它们包含一个高于 1000 的 error_code ，则将其丢弃：\nPUT _ingest/pipeline/spammy_error_handler { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;tags\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;spam\u0026#34;], \u0026#34;if\u0026#34;: \u0026#34;ctx.message != null \u0026amp;\u0026amp; ctx.message.contains(\u0026#39;OutOfMemoryError\u0026#39;)\u0026#34; } }, { \u0026#34;drop\u0026#34;: { \u0026#34;if\u0026#34;: \u0026#34;ctx.tags != null \u0026amp;\u0026amp; ctx.tags.contains(\u0026#39;spam\u0026#39;) \u0026amp;\u0026amp; ctx.error_code != null \u0026amp;\u0026amp; ctx.error_code \u0026gt; 1000\u0026#34; } } ] } 您可以使用以下 _simulate 请求测试管道：\nPOST _ingest/pipeline/spammy_error_handler/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;OutOfMemoryError occurred\u0026#34;, \u0026#34;error_code\u0026#34;: 1200 } }, { \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;OutOfMemoryError occurred\u0026#34;, \u0026#34;error_code\u0026#34;: 800 } }, { \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;All good\u0026#34;, \u0026#34;error_code\u0026#34;: 200 } } ] } 第一份文档被丢弃，因为它包含一个 OutOfMemoryError 字符串和一个高于 1000 的 error_code ：\n{ \u0026#34;docs\u0026#34;: [ null, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;error_code\u0026#34;: 800, \u0026#34;message\u0026#34;: \u0026#34;OutOfMemoryError occurred\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;spam\u0026#34; ] }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:20:10.704359884Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;error_code\u0026#34;: 200, \u0026#34;message\u0026#34;: \u0026#34;All good\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:20:10.704369801Z\u0026#34; } } } ] } 类型安全的评估 #  使用 instanceof 确保在执行操作之前正在使用正确的数据类型。以下管道配置为仅在 message 为类型 String 且长度超过 10 个字符时，添加设置为 true 的 processed 字段：\nPUT _ingest/pipeline/string_message_check { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;processed\u0026#34;, \u0026#34;value\u0026#34;: true, \u0026#34;if\u0026#34;: \u0026#34;ctx.message != null \u0026amp;\u0026amp; ctx.message instanceof String \u0026amp;\u0026amp; ctx.message.length() \u0026gt; 10\u0026#34; } } ] } 使用以下 _simulate 请求测试管道：\nPOST _ingest/pipeline/string_message_check/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;short\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;This is a longer message\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: 1234567890 } } ] } 只有第二份文档添加了新字段：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;short\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:28:14.040115261Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;processed\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;This is a longer message\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:28:14.040141469Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: 1234567890 }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:28:14.040144844Z\u0026#34; } } } ] } 使用正则表达式 #\n Painless 脚本支持 =~ 运算符来评估正则表达式。以下管道标志以 192.168. 开头的可疑 IP 模式：\nPUT _ingest/pipeline/flag_suspicious_ips { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;alert\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;suspicious_ip\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;\u0026#34;\u0026#34;ctx.ip != null \u0026amp;\u0026amp; ctx.ip =~ /^192\\.168\\.\\d+\\.\\d+$/\u0026#34;\u0026#34;\u0026#34; } } ] } 使用以下 _simulate 请求测试管道：\nPOST _ingest/pipeline/flag_suspicious_ips/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;192.168.0.1\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;10.0.0.1\u0026#34; } } ] } 第一份文档添加了 alert 字段：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;alert\u0026#34;: \u0026#34;suspicious_ip\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;192.168.0.1\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:32:45.367916428Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;10.0.0.1\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:32:45.36793772Z\u0026#34; } } } ] } 组合字段和空值检查 #  以下管道添加一个 priority 字段，如果 level 是 critical 且提供了 timestamp ，则将其设置为 high 。该脚本还确保在继续之前所有字段都存在并满足特定条件：\nPUT _ingest/pipeline/critical_log_handler { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;priority\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;ctx.level != null \u0026amp;\u0026amp; ctx.level == \u0026#39;critical\u0026#39; \u0026amp;\u0026amp; ctx.timestamp != null\u0026#34; } } ] } 使用以下 _simulate 请求测试管道：\nPOST _ingest/pipeline/critical_log_handler/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;critical\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-01T00:00:00Z\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-01T00:00:00Z\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;critical\u0026#34; } } ] } 只有第一份文档添加了 priority 字段：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;priority\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;critical\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-01T00:00:00Z\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:39:25.46840371Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-01T00:00:00Z\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:39:25.46843021Z\u0026#34; } } }, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;critical\u0026#34; }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:39:25.468434835Z\u0026#34; } } } ] } 多条件处理 #\n 以下条件：\n 如果该字段尚未存在，则添加一个设置为 production 的 env 字段。 如果 status 字段中的值大于或等于 500 ，则添加一个设置为 major 的 severity 字段。 如果 env 字段设置为 test 且 message 字段包含 debug ，则删除该文档。  PUT _ingest/pipeline/advanced_log_pipeline { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;env\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;!ctx.containsKey(\u0026#39;env\u0026#39;)\u0026#34; } }, { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;severity\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;major\u0026#34;, \u0026#34;if\u0026#34;: \u0026#34;ctx.status != null \u0026amp;\u0026amp; ctx.status \u0026gt;= 500\u0026#34; } }, { \u0026#34;drop\u0026#34;: { \u0026#34;if\u0026#34;: \u0026#34;ctx.env == \u0026#39;test\u0026#39; \u0026amp;\u0026amp; ctx.message?.contains(\u0026#39;debug\u0026#39;)\u0026#34; } } ] } 使用以下 _simulate 请求来测试管道：\nPOST _ingest/pipeline/advanced_log_pipeline/_simulate { \u0026#34;docs\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;status\u0026#34;: 503, \u0026#34;message\u0026#34;: \u0026#34;Server unavailable\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;env\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;debug log output\u0026#34; } }, { \u0026#34;_source\u0026#34;: { \u0026#34;status\u0026#34;: 200, \u0026#34;message\u0026#34;: \u0026#34;OK\u0026#34; } } ] } 在返回内容中，请注意第一个文档添加了 env: production 和 severity: major 字段。第二个文档被丢弃。第三个文档添加了一个 env: production 字段：\n{ \u0026#34;docs\u0026#34;: [ { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;severity\u0026#34;: \u0026#34;major\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Server unavailable\u0026#34;, \u0026#34;env\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;status\u0026#34;: 503 }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:51:46.795026554Z\u0026#34; } } }, null, { \u0026#34;doc\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;_index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;_id\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;env\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;status\u0026#34;: 200 }, \u0026#34;_ingest\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-04-23T10:51:46.795048304Z\u0026#34; } } } ] } 空值安全 #  使用空安全导航表示法（ ?. ）来检查字段是否为 null 。请注意，这种表示法可能会静默地返回 null ；因此，我们建议首先检查返回的值是否为 null ，然后使用 .contains 或 == 等操作。\n不安全语法：\u0026quot;if\u0026quot;: \u0026quot;ctx.message?.contains('debug')\u0026quot;,如果文档中不存在 message 字段，此请求将返回一个包含消息 Cannot invoke \u0026quot;Object.getClass()\u0026quot; because \u0026quot;value\u0026quot; is null 的 null_pointer_exception 。安全语法：\u0026quot;if\u0026quot;: \u0026quot;ctx.message != null \u0026amp;\u0026amp; ctx.message.contains('debug')\u0026quot;\n","subcategory":null,"summary":"","tags":null,"title":"复杂逻辑","url":"/easysearch/main/docs/features/ingest-pipelines/conditional-execution/complex-conditionals/"},{"category":null,"content":"基础查询 #  概述 #  SELECT 语句是从 Easysearch 索引中检索数据的核心语法。完整语法如下：\nSELECT [ALL | DISTINCT] (* | expression) [[AS] alias] [, ...] FROM index_name [WHERE predicates] [GROUP BY expression [, ...] [HAVING predicates]] [ORDER BY expression [ASC | DESC] [NULLS {FIRST | LAST}] [, ...]] [LIMIT [offset, ] size]  语句末尾允许加分号 ;，这对 BI 工具和 Excel 生成的查询很有用。\n 执行顺序 #  SQL 各子句的实际执行顺序与书写顺序不同：\nFROM index -- ① 确定数据源 → WHERE -- ② 行级过滤 → GROUP BY -- ③ 分组 → HAVING -- ④ 组级过滤 → SELECT -- ⑤ 字段投影 → ORDER BY -- ⑥ 排序 → LIMIT -- ⑦ 分页  SELECT #  SELECT 子句指定要检索的字段。\n查询所有字段 #  POST /_sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT * FROM accounts\u0026#34; } 查询指定字段 #  POST /_sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT firstname, lastname, balance FROM accounts\u0026#34; } 字段别名 #  使用 AS 为字段设置别名，提高可读性：\nPOST /_sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT account_number AS num, balance AS bal FROM accounts\u0026#34; } DISTINCT 去重 #  返回字段的唯一值（底层翻译为 terms 聚合）：\nPOST /_sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT DISTINCT age FROM accounts\u0026#34; } DISTINCT 也可以与表达式结合使用：\nSELECT DISTINCT SUBSTRING(lastname, 1, 1) FROM accounts  FROM #  FROM 子句指定查询的数据源（索引）。\n索引别名 #  SELECT acc.firstname, acc.lastname FROM accounts acc 索引模式匹配 #  通过通配符从多个名称相似的索引查询数据，适合时间序列索引（如 logs-2025.02.*）：\nSELECT * FROM account* 子查询 #  FROM 子句支持子查询（详见 复杂查询）。\n WHERE #  WHERE 子句对文档进行行级过滤。支持以下谓词：\n   运算符 说明 示例     =, \u0026lt;\u0026gt;, != 相等/不等 WHERE age = 30   \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;= 比较 WHERE balance \u0026gt;= 10000   AND, OR, NOT 逻辑组合 WHERE age \u0026gt; 30 AND gender = 'M'   IN 集合匹配 WHERE state IN ('CA', 'NY', 'TX')   BETWEEN ... AND 范围 WHERE age BETWEEN 25 AND 35   LIKE 模式匹配 WHERE lastname LIKE 'D%'   IS NULL / IS NOT NULL 空值检测 WHERE employer IS NULL    基本过滤 #  POST /_sql { \u0026#34;query\u0026#34;: \u0026#34;SELECT account_number, balance FROM accounts WHERE balance \u0026gt; 10000\u0026#34; } 翻译为 DSL：\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [{ \u0026#34;range\u0026#34;: {\u0026#34;balance\u0026#34;: {\u0026#34;gt\u0026#34;: 10000}} }] } } } 组合条件 #  SELECT firstname, age, balance FROM accounts WHERE age \u0026gt; 30 AND balance \u0026gt; 5000 AND state IN (\u0026#39;IL\u0026#39;, \u0026#39;TN\u0026#39;) NULL 检测 #  Easysearch 允许灵活的文档结构，字段可能缺失。使用 IS NULL / IS NOT NULL 检测：\nSELECT account_number, employer FROM accounts WHERE employer IS NULL  当前 SQL 插件不区分\u0026quot;字段缺失\u0026quot;和\u0026quot;字段值为 null\u0026quot;。\n  GROUP BY #  GROUP BY 子句将文档按字段值分组，通常与聚合函数配合使用。详细的聚合语法见 聚合查询。\nGROUP BY 表达式支持三种形式：\n-- 1. 按字段名 GROUP BY gender \u0026ndash; 2. 按 SELECT 列表中的序号 GROUP BY 1\n\u0026ndash; 3. 按表达式 GROUP BY ABS(age) 示例 #\n SELECT gender, COUNT(*), AVG(age) FROM accounts GROUP BY gender +--------+-----------+----------+ | gender | COUNT(*) | AVG(age) | +--------+-----------+----------+ | F | 1 | 28.0 | | M | 3 | 33.67 | +--------+-----------+----------+ 按别名分组 #  SELECT account_number AS num FROM accounts GROUP BY num 按标量函数分组 #  SELECT ABS(age) AS a, COUNT(*) FROM accounts GROUP BY ABS(age)  HAVING #  HAVING 子句在 GROUP BY 之后对聚合结果进行过滤。\nSELECT age, MAX(balance) FROM accounts GROUP BY age HAVING MIN(balance) \u0026gt; 10000 HAVING 中可以使用与 SELECT 不同的聚合函数，也可以引用 SELECT 中的别名：\nSELECT gender, SUM(age) AS s FROM accounts GROUP BY gender HAVING s \u0026gt; 100  ORDER BY #  ORDER BY 子句指定结果排序方式。\n-- 按字段名排序（ASC 升序为默认） SELECT * FROM accounts ORDER BY balance DESC \u0026ndash; 按序号排序 SELECT firstname, age FROM accounts ORDER BY 2 DESC\n\u0026ndash; 按别名排序 SELECT balance AS b FROM accounts ORDER BY b\n\u0026ndash; 空值排序控制 SELECT * FROM accounts ORDER BY employer NULLS LAST\n\u0026ndash; 按聚合函数排序 SELECT gender, MAX(age) FROM accounts GROUP BY gender ORDER BY MAX(age) DESC \nLIMIT #  LIMIT 子句控制返回结果的数量。建议与 ORDER BY 配合使用以获得确定性结果。\n-- 返回前 10 条 SELECT * FROM accounts ORDER BY balance DESC LIMIT 10 \u0026ndash; 跳过 5 条后返回 10 条（分页） SELECT * FROM accounts ORDER BY account_number LIMIT 5, 10\n\u0026ndash; 使用 OFFSET 语法 SELECT * FROM accounts ORDER BY age LIMIT 10 OFFSET 5 \n对于大偏移量的分页，性能较低。大规模分页建议使用 游标分页。\n ","subcategory":null,"summary":"","tags":null,"title":"基础查询","url":"/easysearch/main/docs/features/sql/basics/"},{"category":null,"content":"向量搜索指南 #  本页介绍如何在 Easysearch 中使用向量搜索——从创建索引到执行查询的完整流程。\n前提条件 #  向量搜索需要 k-NN 插件。安装方法参见 插件安装。\n 注意：从 1.11.1 版本起，创建 k-NN 索引时不再需要配置 index.knn 参数。\n 步骤一：创建向量索引 #  稠密浮点向量（最常用） #  适用于文本 Embedding、图像特征等场景：\nPUT /my-vectors { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;embedding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 768, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;L\u0026#34;: 99, \u0026#34;k\u0026#34;: 1 } } } } }  映射参数、各索引模型和相似度函数的完整说明，请参阅 向量字段类型参考。\n 步骤二：索引文档 #  向量通常由外部 Embedding 模型生成，写入时附带向量数据：\nPUT /my-vectors/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;Easysearch 分布式搜索引擎\u0026#34;, \u0026#34;embedding\u0026#34;: [0.12, -0.03, 0.88, 0.45, ...] } PUT /my-vectors/_doc/2 { \u0026quot;title\u0026quot;: \u0026quot;向量相似度搜索原理\u0026quot;, \u0026quot;embedding\u0026quot;: [-0.05, 0.22, 0.67, 0.31, \u0026hellip;] } \n提示：向量维度必须与映射中 dims 参数一致，否则写入会报错。\n 步骤三：执行向量搜索 #  近似搜索（LSH） #  LSH 模型提供高效的近似最近邻搜索，适用于大规模数据集：\nPOST /my-vectors/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.03, 0.88, 0.45, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 100 } } } 查询参数 #     参数 类型 说明     field String 向量字段名   vec.values Float[] 查询向量   model String 必须与创建索引时一致   similarity String 相似度函数   candidates Integer 候选数量，越大越准但越慢。推荐值：size 的 5～10 倍    精确搜索（Exact） #  精确模型计算查询向量与所有文档的精确相似度，适用于小数据集或对精度要求极高的场景：\nPOST /my-vectors/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.03, 0.88, 0.45, ...] }, \u0026#34;model\u0026#34;: \u0026#34;exact\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34; } } }  ⚠️ 精确搜索的时间复杂度为 O(n²)，文档数量较大时性能会显著下降。\n 引用已索引向量 #  可以用索引中已有文档的向量作为查询向量，无需在请求中传递完整向量值：\nPOST /my-vectors/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;my-vectors\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34; }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 100 } } } 带过滤的向量搜索 #  向量搜索可以与布尔查询组合，实现\u0026quot;语义相似 + 结构化过滤\u0026quot;：\nPOST /my-vectors/_search { \u0026#34;size\u0026#34;: 5, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.03, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 50 } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;tech\u0026#34; } }, { \u0026#34;range\u0026#34;: { \u0026#34;created_at\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-30d/d\u0026#34; } } } ] } } }  注意：过滤在向量召回之后执行（post-filtering），先通过 kNN 召回候选集，再用 filter 筛选。\n Hybrid 检索（混合搜索） #  将向量相似度与 BM25 全文搜索融合，同时利用关键词匹配和语义理解：\nPOST /my-vectors/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.03, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 100 } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;分布式搜索引擎\u0026#34;, \u0026#34;boost\u0026#34;: 2.0 } } } ] } } } 工作原理：\n knn_nearest_neighbors 找出语义相似的文档（向量相似度得分） match 找出关键词匹配的文档（BM25 得分） Easysearch 将两路得分融合，输出综合排序   可以通过 boost 参数调节两路得分的权重比例。\n 使用 function_score 集成向量评分 #  向量相似度也可以作为 function_score 中的评分函数，与其他评分逻辑灵活组合：\nPOST /my-vectors/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;搜索引擎\u0026#34; } }, \u0026#34;functions\u0026#34;: [ { \u0026#34;knn_nearest_neighbors\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.12, -0.03, ...] }, \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;candidates\u0026#34;: 100 }, \u0026#34;weight\u0026#34;: 2 } ], \u0026#34;boost_mode\u0026#34;: \u0026#34;multiply\u0026#34; } } } 性能调优 #  LSH 参数调优 #   L（哈希表数量）：增大提高召回率，但增加内存和索引时间。建议从 50～100 开始 k（哈希函数数量）：增大提高精度但可能降低召回率。建议从 1～3 开始 candidates：查询时的候选数量，一般设为 size 的 5～10 倍  容量规划 #   维度越高，单条文档的向量存储开销越大 在满足效果的前提下，尽量使用较低维度的模型（如 256～512 维） 避免一个索引里堆积过多向量字段，必要时按业务拆分索引  写入策略 #   同步写入：写文档的同时调用 Embedding 模型生成向量，保证检索一致性 异步补齐：先写文本文档，异步任务批量生成向量后 update，适合历史数据迁移  下一步 #    向量字段类型参考：字段类型、映射参数、各索引模型详解  向量字段建模：多向量设计、维度选择、模型选型策略  k-NN 查询 API：完整的 API 参数参考  Embedding 服务集成：接入外部 Embedding 模型  AI API 集成：Hybrid Search API 等高级功能  ","subcategory":null,"summary":"","tags":null,"title":"向量搜索指南","url":"/easysearch/main/docs/features/vector-search/vector-search/"},{"category":null,"content":"创建管道 #  使用 PUT _ingest/pipeline API 创建或更新摄取管道。如果指定的 pipeline-id 已存在，则覆盖更新。\n请求格式 #  PUT _ingest/pipeline/\u0026lt;pipeline-id\u0026gt; 示例 #  创建一个包含 set 和 uppercase 处理器的管道：\nPUT _ingest/pipeline/my-pipeline { \u0026#34;description\u0026#34;: \u0026#34;处理学生数据：设置毕业年份并转大写\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;设置毕业年份\u0026#34;, \u0026#34;field\u0026#34;: \u0026#34;grad_year\u0026#34;, \u0026#34;value\u0026#34;: 2023 } }, { \u0026#34;set\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;标记已毕业\u0026#34;, \u0026#34;field\u0026#34;: \u0026#34;graduated\u0026#34;, \u0026#34;value\u0026#34;: true } }, { \u0026#34;uppercase\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;name\u0026#34; } } ] } 成功响应：\n{ \u0026#34;acknowledged\u0026#34;: true } 请求体参数 #     参数 必需 类型 说明     processors 是 array 有序处理器列表，按定义顺序依次执行   description 否 string 管道描述，出现在 GET _ingest/pipeline 的返回中    路径参数 #     参数 必需 类型 说明     pipeline-id 是 string 管道的唯一标识符    查询参数 #     参数 必需 类型 默认值 说明     cluster_manager_timeout 否 时长 30s 等待集群管理器节点响应的超时时间   timeout 否 时长 30s 等待整体响应的超时时间    Mustache 模板 #  处理器参数中可以使用 Mustache 模板引用文档字段。用三个花括号包裹字段名 {{{field_name}}} 可获取未转义的原始值：\nPUT _ingest/pipeline/dynamic-pipeline { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;{{{role}}}\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{{tenure}}}\u0026#34; } } ] } 此管道会用文档中 role 字段的值作为目标字段名，用 tenure 字段的值作为该字段的值。详见 在管道中访问数据。\n","subcategory":null,"summary":"","tags":null,"title":"创建管道","url":"/easysearch/main/docs/features/ingest-pipelines/create-ingest/"},{"category":null,"content":"使用 Logstash 接入数据 #  Logstash 适合做“重量级管道”：支持丰富的输入源、过滤器和输出插件。\n这一页只解决几个核心问题：\n 最小可用的 Logstash → Easysearch pipeline 长什么样？ 和 Easysearch 的索引与 mapping 怎么配合，避免“写进去一团糊”？ 在生产环境里，批量大小、重试、幂等这些常见坑怎么规避？  1. 最小可用 Pipeline 示例 #  下面是一个从文件读取日志、简单解析后写入 Easysearch 的最小示例（概念结构）：\ninput { file { path =\u0026gt; \u0026#34;/var/log/app/app.log\u0026#34; start_position =\u0026gt; \u0026#34;beginning\u0026#34; sincedb_path =\u0026gt; \u0026#34;/var/lib/logstash/.sincedb_app\u0026#34; } } filter {\n示例：按 JSON 日志解析 json { source =\u0026gt; \u0026quot;message\u0026quot; }\n示例：统一时间字段名称 if [timestamp] { mutate { rename =\u0026gt; { \u0026quot;timestamp\u0026quot; =\u0026gt; \u0026quot;@timestamp\u0026quot; } } } }\noutput { elasticsearch { hosts =\u0026gt; [\u0026quot;http://easysearch:9200\u0026quot;] index =\u0026gt; \u0026quot;app-logs-%{+YYYY.MM.dd}\u0026quot; # user / password / ssl 相关配置按实际集群安全设置填写 } } 要点：\n index 命名：直接按日期切索引，方便后续在 Easysearch 里做数据保留与迁移。 @timestamp 统一：便于在查询、聚合和排序时统一时间字段。 Logstash 内部会自动对 Easysearch 使用 _bulk 写入，你只需要关注批量大小等参数。   安全相关配置（TLS、账号密码等）请参考 Easysearch 的安全模块与 Logstash 官方文档，这里不展开具体参数表。\n 2. 和 Easysearch 的索引与 mapping 如何配合？ #  Logstash 的输出只是“把文档发到某个索引”，索引本身如何配置，还是要在 Easysearch 侧做好设计：\n 优先使用索引模板：  在 索引模板中定义好 settings + mappings + aliases。 让 app-logs-* 这类索引在自动创建时，字段类型就正确、默认别名也就位。   控制动态映射：  日志数据字段多、变化快，建议通过动态模板控制新字段类型，而不是完全放开放飞。 避免“同一个业务字段有时是字符串、有时是数值”导致 mapping 冲突。   统一关键字段约定：  时间：统一用 @timestamp（date 类型）。 租户/环境：如 tenant_id、env 字段，后续在 Easysearch 里会经常被用作 filter。    典型做法是：\n 在 Easysearch 侧先写好模板和索引设计； 再在 Logstash 里只负责“把字段映射到约定好的名字”，尽量不在 Logstash 里做复杂业务逻辑。  3. 批量、重试与幂等：生产环境的几个关键参数 #  Logstash 向 Easysearch 写入时，本质是使用 _bulk 接口。\n几个需要重点关注的点：\n 批量大小：  输出插件中的批量相关设置决定单次 bulk 的条数。 批量太小：QPS 高但效率差；太大：单次失败重试成本高，内存压力大。   重试策略：  对于暂时性错误（如 429、部分 5xx）可以允许有限次数自动重试。 对于数据本身有问题的 4xx（mapping 冲突、字段类型错误）不要盲目重试，应通过监控/告警修正上游数据或 mapping。   幂等性：  如果上游可能重放数据（比如消息队列消费失败重试），应在 Logstash 里选择合适的文档 ID（例如业务主键），让多次写入不会制造重复脏数据。 对于日志类场景，可以接受“多一条重复”，则可以用自动生成 ID；对业务数据同步，更建议使用稳定 ID。    这些参数和行为，在权威指南里会有比较通用的建议，迁移到 Easysearch 上时只要统一用 Easysearch 的版本语义来验证即可。\n4. 常见坑与规避建议 #  结合 Easysearch 的特点，Logstash 集成时容易踩的坑主要有：\n 日志行解析失败：  如 json 或 grok 解析失败，导致字段乱掉或全部落在 message。 建议为解析失败的记录设置单独标签和输出（比如写入一个 *-deadletter 索引），避免污染主索引。   动态字段爆炸：  比如把整个 JSON 嵌套全展开到字段，或接入包含高基数字段名的数据源。 建议通过 dynamic_templates 或 Logstash 过滤器提前裁剪/归一字段。   时间与时区问题：  日志中时间字符串没有带时区，或格式不统一，导致 @timestamp 解析偏差。 建议统一在 Logstash filter 里用 date 插件解析成 @timestamp，并确认时区。   安全与权限：  在启用了 Easysearch 安全功能时，为 Logstash 准备专门的角色/账号，只授予必要的索引写入权限。    这些问题大多属于“接入层老生常谈”，但一旦踩了坑，排查成本会很高，建议在首次上线前就按 checklist 自查一遍。\n5. 相关章节 #  想深入了解接入前后两端的更多细节，可以继续看：\n  索引模板与别名设计  摄取管道配置：Easysearch 侧摄取管道，与 Logstash 形成\u0026quot;前后两级流水线\u0026quot;  索引与分片/副本设计：在大流量日志场景下尤为重要  ","subcategory":null,"summary":"","tags":null,"title":"使用 Logstash 接入数据","url":"/easysearch/main/docs/integrations/ingest/logstash/"},{"category":null,"content":"Query DSL 基础 #  这一页的目标是：在不记任何 API 细节的前提下，先把 Query DSL 的“骨架”和核心概念讲清楚，后面看具体查询语法就会轻松很多。\nJSON 结构而不是\u0026quot;SQL 语句\u0026quot; #  Easysearch 的查询是 JSON 结构，而不是一条字符串语句。一个最小的查询大致长这样：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;hello world\u0026#34; } } } 要记住两个层级：\n 最外层的 \u0026quot;query\u0026quot;：说明这是一个\u0026quot;查询上下文\u0026quot; \u0026quot;match\u0026quot;、\u0026quot;term\u0026quot;、\u0026quot;range\u0026quot; 等：都是不同类型的\u0026quot;子查询\u0026quot;  查询语句的结构 #  一个查询语句的典型结构：\n{ QUERY_NAME: { ARGUMENT: VALUE, ARGUMENT: VALUE,... } } 如果是针对某个字段，那么它的结构如下：\n{ QUERY_NAME: { FIELD_NAME: { ARGUMENT: VALUE, ARGUMENT: VALUE,... } } } 例如，使用 match 查询语句来查询 tweet 字段中包含 easysearch 的文档：\n{ \u0026#34;match\u0026#34;: { \u0026#34;tweet\u0026#34;: \u0026#34;easysearch\u0026#34; } } 空查询 #  空查询（{}）在功能上等价于使用 match_all 查询，正如其名字一样，匹配所有文档：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } Query 上下文 vs Filter 上下文 #  Query DSL 里有两种“语义上下文”：\n Query 上下文：用于计算相关性 _score  典型：match、multi_match 等全文检索 结果有打分，适合排序   Filter 上下文：只判断“是不是匹配”，不参与 _score 计算  典型：term、range 作为过滤条件 结果可缓存，通常更快    经验法则：\n “是不是这个用户、是不是这个状态” → filter “相关不相关、哪条更像” → query  更直观一点的例子（同样是精确值条件，用在不同上下文）：\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } } // 参与打分 ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } } // 只过滤，不打分 ] } } } 实际场景里，过滤条件几乎总是放在 filter 里：\n 逻辑语义更清晰（“必须满足，但不影响相关性”） 容易被缓存，加速高并发请求  bool 查询：组合一切的基础 #  大部分复杂查询，最后都会长成一个 bool。查询语句可以分为两类：\n 叶子语句（Leaf clauses）：如 match 语句，用于将查询字符串和一个字段（或多个字段）对比 复合语句（Compound clauses）：主要用于合并其它查询语句，如 bool 语句  bool 查询允许你组合多个查询语句，它接收以下参数：\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;搜索\u0026#34; } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;hot\u0026#34; } } ], \u0026#34;must_not\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;is_deleted\u0026#34;: true } } ] } } } 各个子句的含义：\n must：文档必须匹配这些条件才能被包含进来，同时参与评分（query 上下文） filter：文档必须匹配，但不参与评分（filter 上下文），结果可缓存 should：如果满足这些语句中的任意语句，将增加 _score，否则无任何影响。主要用于修正每个文档的相关性得分 must_not：文档必须不匹配这些条件才能被包含进来（排除条件）   提示：如果没有 must 语句，那么至少需要能够匹配其中的一条 should 语句。但如果存在至少一条 must 语句，则对 should 语句的匹配没有要求。\n 建议的使用习惯：\n 业务强约束（如状态、租户、时间范围）放到 filter 影响排序的条件放到 must / should 不希望命中的文档用 must_not 排掉  minimum_should_match：控制“should 至少命中几个” #  should 默认是“加分项”——命中越多 _score 越高，但全不命中也不会被过滤掉。\n在有多个 should 时，你可以用 minimum_should_match 要求“至少命中 N 个”：\n{ \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;手机\u0026#34; } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;brand\u0026#34;: \u0026#34;apple\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;brand\u0026#34;: \u0026#34;xiaomi\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;brand\u0026#34;: \u0026#34;huawei\u0026#34; } } ], \u0026#34;minimum_should_match\u0026#34;: 1 } } 这类写法常用于“主条件 + 若干偏好条件”，既保证主条件命中，又能避免“一个偏好都不满足”的结果混进来。\n复合语句可以嵌套 #  一条复合语句可以合并任何其它查询语句，包括复合语句。这意味着复合语句之间可以互相嵌套，可以表达非常复杂的逻辑。\n例如，以下查询是为了找出信件正文包含 business opportunity 的星标邮件，或者在收件箱正文包含 business opportunity 的非垃圾邮件：\n{ \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;email\u0026#34;: \u0026#34;business opportunity\u0026#34; }}, \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;starred\u0026#34;: true }}, { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;folder\u0026#34;: \u0026#34;inbox\u0026#34; }}, \u0026#34;must_not\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;spam\u0026#34;: true }} }} ], \u0026#34;minimum_should_match\u0026#34;: 1 } } 常见 Query 类型的脑图 #  按用途大致可以这样分类（后续对应到各自章节）：\n 结构化查询：term、terms、range、exists 等 全文检索：match、match_phrase、multi_match 等 组合/逻辑查询：bool、constant_score 等 特殊用途：function_score、script、nested 等  这一页只关注“壳子”和组合方式，具体语法在后续页面分别展开。\n写 DSL 时常见几个坑 #    把所有条件都塞进 must\n 结果：评分杂乱、缓存利用不好 建议：能 filter 的尽量用 filter    过度嵌套 bool，结构难以维护\n 建议：尽量把逻辑整理成“必选/加分/过滤/排除”四类，用一层 bool 表达清楚    混用查询类型却不了解语义差异\n 例如对 keyword 字段用全文型查询，或对 text 字段用 term 精确匹配 建议：先在 Mapping 章节搞清楚字段类型，再决定用哪类查询    下一步可以继续阅读：\n  结构化搜索  全文搜索  参考手册（API 与参数） #    搜索操作概览（功能手册）  全文查询（功能手册）  精确/Term 查询（功能手册）  复合查询（功能手册）  ","subcategory":null,"summary":"","tags":null,"title":"Query DSL 基础","url":"/easysearch/main/docs/features/query-dsl/query-dsl-basics/"},{"category":null,"content":"Painless 脚本语言 #  Painless 是 Easysearch 内置的高性能脚本语言，也是默认脚本语言。它专为搜索引擎场景设计，具备以下特点：\n 高性能：编译为 Java 字节码，通过 JIT 直接在 JVM 上运行 安全沙盒：只允许访问白名单内的 Java API，无法执行文件、网络等危险操作 Java 风格语法：对 Java 开发者友好，学习成本低 专用优化：内置 doc、_source、ctx 等快捷访问方式  基本语法 #  变量与类型 #  Painless 是强类型语言，支持类型推断：\n// 显式类型声明 int count = 10; double price = 99.5; String name = \u0026#34;Easysearch\u0026#34;; boolean active = true; // 类型推断（使用 def） def value = doc['price'].value; def list = [1, 2, 3]; def map = ['key': 'value', 'count': 42]; 支持的基本类型：\n   类型 说明 示例     byte, short, int, long 整数类型 42, 100L   float, double 浮点类型 3.14, 2.0f   boolean 布尔类型 true, false   char 字符类型 'A'   String 字符串 \u0026quot;hello\u0026quot;   def 动态类型（运行时确定） def x = 1    运算符 #  // 算术运算 int sum = a + b; double avg = total / count; int remainder = x % 2; // 比较运算 boolean eq = (a == b); // 值相等 boolean ref = (a === b); // 引用相等 boolean gt = (a \u0026gt; b);\n// 逻辑运算 boolean result = (a \u0026gt; 0) \u0026amp;\u0026amp; (b \u0026lt; 100); boolean either = (a \u0026gt; 0) || (b \u0026gt; 0); boolean not = !(a \u0026gt; 0);\n// 字符串连接 String full = first + \u0026quot; \u0026quot; + last;\n// 三元运算符 def status = (score \u0026gt; 80) ? \u0026quot;pass\u0026quot; : \u0026quot;fail\u0026quot;;\n// 空值安全运算符（Elvis） def val = params.value ?: 0;\n// 空安全访问 def len = params.name?.length(); 控制流 #\n // if-else if (doc[\u0026#39;status\u0026#39;].value == \u0026#39;active\u0026#39;) { return 1; } else if (doc[\u0026#39;status\u0026#39;].value == \u0026#39;pending\u0026#39;) { return 0.5; } else { return 0; } // for 循环 int total = 0; for (int i = 0; i \u0026lt; 10; i++) { total += i; }\n// for-each 循环 for (def item : params.items) { total += item; }\n// while 循环 int i = 0; while (i \u0026lt; doc['tags'].length) { if (doc['tags'][i] == 'important') { return true; } i++; } 集合操作 #\n // List（列表） def list = [1, 2, 3, 4, 5]; list.add(6); int size = list.size(); def first = list.get(0); list.remove(0); // Map（字典） def map = ['name': 'test', 'score': 95]; map.put('grade', 'A'); def name = map.get('name'); boolean has = map.containsKey('score');\n// 遍历 Map for (def entry : map.entrySet()) { String key = entry.getKey(); def val = entry.getValue(); }\n// Stream 风格操作 def filtered = list.stream() .filter(x -\u0026gt; x \u0026gt; 2) .map(x -\u0026gt; x * 2) .collect(Collectors.toList()); 正则表达式 #\n // 匹配检查 if (doc[\u0026#39;email\u0026#39;].value =~ /.*@example\\.com/) { return true; } // 查找与提取 def matcher = /(\\d{4})-(\\d{2})-(\\d{2})/.matcher(doc['date'].value); if (matcher.find()) { def year = matcher.group(1); }\n// 替换 String cleaned = doc['text'].value.replaceAll(/\\s+/, ' '); 访问文档数据 #\n Painless 提供三种方式访问文档数据，适用于不同场景：\n1. doc['field'] — Doc Values 访问（推荐） #  从列式存储的 doc values 中读取，性能最好：\n// 读取单值字段 def price = doc[\u0026#39;price\u0026#39;].value; def name = doc[\u0026#39;name.keyword\u0026#39;].value; // 读取多值字段 def tags = doc['tags']; int tagCount = tags.size(); def firstTag = tags[0];\n// 检查字段是否有值 if (doc['field'].size() \u0026gt; 0) { return doc['field'].value; }\n// 日期字段 ZonedDateTime date = doc['timestamp'].value; int year = date.getYear(); int month = date.getMonthValue(); \n注意：doc 方式需要字段启用了 doc values（大多数字段默认启用，text 字段除外）。text 字段需要开启 fielddata: true 才能通过 doc 访问。\n 2. params._source — 源文档访问 #  从存储的 _source JSON 中读取，可以访问任意字段但性能较慢：\n// 访问嵌套字段 def address = params._source.address.city; // 访问 text 字段原始值 def description = params._source.description;\n// 访问数组 def firstItem = params._source.items[0].name; \n注意：params._source 不是在所有上下文中都可用。在 score 和 filter 上下文中，推荐使用 doc 方式。\n 3. ctx — 上下文变量 #  在 Update 和 Ingest 场景中使用：\n// Update API 中 ctx._source.counter += 1; ctx._source.tags.add(\u0026#39;new_tag\u0026#39;); ctx._source.last_updated = params.now; // 条件删除 if (ctx._source.status == 'expired') { ctx.op = 'delete'; }\n// Ingest 处理器中 ctx.field_name = 'new_value'; ctx.timestamp = new SimpleDateFormat(\u0026quot;yyyy-MM-dd\u0026quot;).format(new Date()); 访问方式对比 #\n    方式 性能 可用上下文 适用场景     doc['field'] ⚡ 最快 Score、Filter、Agg、Sort 查询时的字段访问   params._source 🐢 较慢 部分查询上下文 访问嵌套结构或 text 字段   ctx._source ⚡ 快 Update、Ingest 文档更新和摄入处理   params ⚡ 快 所有上下文 访问用户传入的参数    脚本上下文 #  Painless 脚本在不同的上下文中运行，每个上下文有不同的可用变量和返回值要求：\nScore 上下文（评分） #  用于 script_score 查询，必须返回 double：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;popularity\u0026#39;].value * Math.log(2 + doc[\u0026#39;votes\u0026#39;].value)\u0026#34; } } } } 可用变量：doc、params、_score（原始查询评分）\nFilter 上下文（过滤） #  用于 script 查询，必须返回 boolean：\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;age\u0026#39;].value \u0026gt; params.min_age \u0026amp;\u0026amp; doc[\u0026#39;age\u0026#39;].value \u0026lt; params.max_age\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;min_age\u0026#34;: 18, \u0026#34;max_age\u0026#34;: 65 } } } } } 可用变量：doc、params\nUpdate 上下文 #  用于 Update API，通过 ctx._source 修改文档：\nPOST my_index/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.counter += params.increment\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;increment\u0026#34;: 5 } } } 可用变量：ctx（含 _source、_id、_version、_routing、op）、params\nIngest 上下文 #  用于摄入管道的 Script Processor：\nPUT _ingest/pipeline/my_pipeline { \u0026#34;processors\u0026#34;: [ { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx.full_name = ctx.first_name + \u0026#39; \u0026#39; + ctx.last_name\u0026#34; } } ] } 可用变量：ctx（文档字段）、params\nAggregation 上下文 #  用于 Scripted Metric Aggregation：\nGET my_index/_search { \u0026#34;aggs\u0026#34;: { \u0026#34;profit\u0026#34;: { \u0026#34;scripted_metric\u0026#34;: { \u0026#34;init_script\u0026#34;: \u0026#34;state.transactions = []\u0026#34;, \u0026#34;map_script\u0026#34;: \u0026#34;state.transactions.add(doc[\u0026#39;profit\u0026#39;].value)\u0026#34;, \u0026#34;combine_script\u0026#34;: \u0026#34;double total = 0; for (t in state.transactions) { total += t; } return total\u0026#34;, \u0026#34;reduce_script\u0026#34;: \u0026#34;double total = 0; for (s in states) { total += s; } return total\u0026#34; } } } } Sort 上下文（排序） #  用于自定义排序，返回排序值：\nGET my_index/_search { \u0026#34;sort\u0026#34;: [ { \u0026#34;_script\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;priority\u0026#39;].value * 100 + doc[\u0026#39;score\u0026#39;].value\u0026#34; }, \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } 可用变量：doc、params\n常用内置 API #  数学函数 #  Math.abs(-5) // 5 Math.max(a, b) // 较大值 Math.min(a, b) // 较小值 Math.pow(2, 10) // 1024.0 Math.sqrt(144) // 12.0 Math.log(100) // 4.605... Math.log10(100) // 2.0 Math.ceil(3.2) // 4.0 Math.floor(3.8) // 3.0 Math.round(3.5) // 4 Math.random() // 0.0-1.0 随机数 字符串方法 #  String s = \u0026#34;Hello World\u0026#34;; s.length() // 11 s.substring(0, 5) // \u0026#34;Hello\u0026#34; s.toLowerCase() // \u0026#34;hello world\u0026#34; s.toUpperCase() // \u0026#34;HELLO WORLD\u0026#34; s.trim() // 去除首尾空白 s.contains(\u0026#34;World\u0026#34;) // true s.startsWith(\u0026#34;Hello\u0026#34;) // true s.endsWith(\u0026#34;World\u0026#34;) // true s.indexOf(\u0026#34;World\u0026#34;) // 6 s.replace(\u0026#34;World\u0026#34;, \u0026#34;ES\u0026#34;) // \u0026#34;Hello ES\u0026#34; s.split(\u0026#34; \u0026#34;) // [\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;] 日期与时间 #  // 从 doc values 获取日期 ZonedDateTime dt = doc[\u0026#39;timestamp\u0026#39;].value; dt.getYear() // 2025 dt.getMonthValue() // 6 dt.getDayOfMonth() // 15 dt.getHour() // 14 dt.getMinute() // 30 // 日期差值（毫秒） long now = System.currentTimeMillis(); long docTime = doc['timestamp'].value.toInstant().toEpochMilli(); long diffDays = (now - docTime) / (1000 * 60 * 60 * 24);\n// 格式化日期 DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\u0026quot;yyyy-MM-dd\u0026quot;); String dateStr = dt.format(formatter); 常见使用示例 #\n 条件赋分 #  GET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;手机\u0026#34; } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; double score = _score; if (doc[\u0026#39;in_stock\u0026#39;].value) { score *= 1.5; } if (doc.containsKey(\u0026#39;promotion\u0026#39;) \u0026amp;\u0026amp; doc[\u0026#39;promotion\u0026#39;].size() \u0026gt; 0) { score *= 2.0; } return score; \u0026#34;\u0026#34;\u0026#34; } } } } 批量字段更新 #  POST my_index/_update_by_query { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; ctx._source.full_name = ctx._source.first_name + \u0026#39; \u0026#39; + ctx._source.last_name; if (ctx._source.age \u0026gt;= 18) { ctx._source.category = \u0026#39;adult\u0026#39;; } else { ctx._source.category = \u0026#39;minor\u0026#39;; } \u0026#34;\u0026#34;\u0026#34; }, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must_not\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;full_name\u0026#34; } } } } } 数组操作 #  POST my_index/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; if (!ctx._source.tags.contains(params.new_tag)) { ctx._source.tags.add(params.new_tag); } \u0026#34;\u0026#34;\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;new_tag\u0026#34;: \u0026#34;featured\u0026#34; } } } 按天衰减评分 #  GET articles/_search { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;搜索引擎\u0026#34; } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; long now = System.currentTimeMillis(); long published = doc[\u0026#39;publish_date\u0026#39;].value.toInstant().toEpochMilli(); long daysDiff = (now - published) / (1000L * 60 * 60 * 24); double decay = Math.exp(-0.01 * daysDiff); return _score * decay; \u0026#34;\u0026#34;\u0026#34; } } } } 调试与排查 #  使用 Painless Execute API 调试 #  POST _scripts/painless/_execute { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; int total = 0; for (int i = 0; i \u0026lt; params.values.size(); i++) { total += params.values[i]; } return total; \u0026#34;\u0026#34;\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;values\u0026#34;: [10, 20, 30] } } } 使用 Explain API 调试评分 #  GET my_index/_explain/1 { \u0026#34;query\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc[\u0026#39;score\u0026#39;].value * 2\u0026#34; } } } } 常见错误与解决 #     错误信息 原因 解决方法     compile error 语法错误 检查语法，使用 Painless Execute API 调试   null_pointer_exception 字段值为空 使用 doc['field'].size() \u0026gt; 0 检查   class_cast_exception 类型不匹配 使用显式类型转换或 def   script_exception: too many compilations 编译频率过高 使用 params 参数化，避免动态拼接脚本    性能最佳实践 #   使用 params 参数化：将变量值通过 params 传入，使脚本可被缓存复用 优先使用 doc 访问：doc['field'].value 比 params._source.field 快很多 避免在热路径使用脚本：高频查询优先考虑非脚本方案（如 function_score 内置函数） 使用存储脚本：反复使用的脚本存储后可避免重复编译 控制脚本复杂度：避免在脚本中做大量循环或复杂计算 使用强类型：明确声明类型（int、double）比 def 性能稍好  ","subcategory":null,"summary":"","tags":null,"title":"Painless 脚本语言","url":"/easysearch/main/docs/features/scripting/painless/"},{"category":null,"content":"OpenTelemetry 集成 #  本页从架构和建模角度，讲清楚一件事：如果你已经用了 OpenTelemetry，怎么把指标/日志/Trace 稳定地落到 Easysearch 上，并跑起来统一分析？\n 数据流：应用 → OTel SDK → OTel Collector → Easysearch 索引与 mapping：metrics / logs / traces 三类数据怎么按索引和字段建模 与现有监控/告警体系的分工：谁做实时告警，谁做长期存储与分析  不涉及具体 Collector 配置语法的全集，只给出 Easysearch 侧的“接入契约”与推荐套路。\n1. OTel → Easysearch 的典型数据流 #  一个常见部署大致长这样：\n 应用侧：接入 OTel SDK（或通过语言/框架集成），上报 metrics / logs / traces OTel Collector：集中收集、处理、转发 OTel 数据 输出到 Easysearch：  通过支持 Easysearch/ES 协议的 exporter 或通过自定义 exporter / gateway，将数据转换为 Easysearch 可接受的 HTTP 请求（通常是 _bulk）    在 Easysearch 看来，本质上就是有一条“写入大量结构化/半结构化数据”的管道，区别只在于字段和索引设计。\n2. 索引设计：按“信号类型 + 时间”切分 #  建议把 OTel 的三类信号拆成至少三块索引前缀：\n 指标（metrics）：如 otel-metrics-YYYY.MM.DD 日志（logs）：如 otel-logs-YYYY.MM.DD Trace / Span：如 otel-traces-YYYY.MM.DD  理由：\n 不同信号有不同的查询模式和字段布局，混在一个索引里会让 mapping 很难控制 按时间切分索引，方便做数据保留策略与冷热分层 后续你也可以根据业务再细分（例如按环境/业务域再拆前缀）  可以结合索引模板，让新创建的 otel-* 索引自动带上合适的 settings / mappings / aliases。\n3. 字段与 mapping 设计要点 #  OpenTelemetry 的语义里已经定义了很多标准字段，建议在 Easysearch 侧尽量复用这些名字，不要“自创一套”：\n 共通字段：  service.name、service.namespace、service.instance.id resource.*：环境、集群、节点、region 等元信息   指标（metrics）：  metric.name、metric.type（counter/gauge/histogram 等） metric.value 或直方图相关字段 标签（dimension）统一放在 attributes.* 下（例如 attributes.http.method）   日志（logs）：  body：原始日志内容（字符串或结构化） severity 或 log.level 关联的 trace_id / span_id（方便从日志跳转到 Trace）   Trace / Span：  trace_id、span_id、parent_span_id span.name、span.kind、status.code duration_ms（自算一个便于聚合的数值字段） 关键 attributes（如 http.url、db.statement 等）仍放在 attributes.*    结合 Easysearch 的 mapping 能力：\n 对于高基数的标识类字段（trace_id / span_id / service.instance.id 等），使用 keyword 类型 对时间戳统一用 @timestamp（date 类型），方便跨类型联合分析 对一部分常用标签，可以单独“抄一份”到顶层字段（如 env、region），减少查询时的嵌套路径开销  4. 典型查询与聚合场景 #  把数据落在 Easysearch 后，常见会有几类查询/分析需求：\n 指标看板：  对 metric.value 按 date_histogram 聚合，按 service.name 等分组 和一般的时间序列指标几乎一致，只是字段命名更贴近 OTel 语义   日志+Trace 联动：  在日志索引里根据 trace_id / span_id 找到相关日志 在 Trace 索引里根据 trace_id 做整条调用链的重建与聚合分析   多维 Drill-down：  利用 attributes.* 里的标签，按照 http.method、status_code、region 等做分组统计 在高维标签场景下，需要关注字段数量和高基数字段对聚合性能的影响    这些查询模式本身和“普通的日志/指标看板”非常接近，只是字段命名和建模遵循了 OTel 的规范。\n5. 与现有监控/告警平台的分工 #  在很多团队里，告警往往由 Prometheus / 专用 APM / SaaS 平台承担，而 Easysearch 更适合作为：\n 统一检索层：日志、Trace、部分指标统一进来做问题排查和历史分析 长周期存储层：将 OTel 数据的长期保留交给 Easysearch，短周期/实时告警仍放在专门的时序系统  这种分工下：\n 实时告警：继续用现有告警系统（如 Prometheus rules / Alertmanager） 事后分析与追溯：通过 Easysearch + 可视化（如 Superset、自研控制台等）做多维查询和分析  Easysearch 侧需要重点做好：\n 数据保留策略（参考“数据生命周期与保留策略”一文） 索引与分片设计（特别是高写入量的 Trace 与日志索引） 安全与多租户（按环境/团队/服务做权限和视图隔离）  6. 相关文档 #  可以搭配阅读以下章节，形成一条完整的 OTel → Easysearch 可观测性链路：\n  索引模板：为 OTel 指标/日志/Trace 定义索引模板  时间序列建模：时序数据的索引设计与查询优化  数据生命周期：指标/日志/Trace 的保留与归档策略  索引设计：高写入场景下的索引/分片规划  ","subcategory":null,"summary":"","tags":null,"title":"OpenTelemetry 集成","url":"/easysearch/main/docs/integrations/observability/opentelemetry/"},{"category":null,"content":"NUMA 配置指南 #  在多路服务器（2 路及以上 CPU）上部署 Easysearch 时，NUMA（Non-Uniform Memory Access）拓扑会显著影响性能。不当的配置可能导致跨节点内存访问，增加延迟 30%–50%。\nNUMA 基础概念 #  ┌──────────────┐ ┌──────────────┐ │ NUMA Node 0│ │ NUMA Node 1│ │ CPU 0-15 │ │ CPU 16-31 │ │ 本地内存 128G│◄──►│ 本地内存 128G│ │ 延迟 ~80ns │ QPI│ 延迟 ~80ns │ └──────────────┘ └──────────────┘ 本地访问快 跨节点访问慢 (~130ns)  每个 CPU 有自己的本地内存，访问本地内存最快 访问远端 NUMA 节点的内存需要通过互联总线（QPI/UPI），延迟更高 JVM 大堆 + 跨 NUMA 节点 = GC 停顿时间增加  查看 NUMA 拓扑 #  # 查看 NUMA 节点信息 numactl --hardware # 输出示例 available: 2 nodes (0-1) node 0 cpus: 0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23 node 0 size: 131072 MB node 1 cpus: 8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31 node 1 size: 131072 MB node distances: node 0 1 0: 10 21 1: 21 10 # 查看当前内存分配策略 numactl \u0026ndash;show 推荐配置方案 #\n 方案一：绑定单个 NUMA 节点（推荐） #  将 Easysearch 进程绑定到一个 NUMA 节点，确保所有内存访问都在本地：\n# 绑定到 NUMA node 0 numactl --cpunodebind=0 --membind=0 /data/easysearch/bin/easysearch 适用场景：\n 单机部署一个 Easysearch 实例 JVM 堆 ≤ 单个 NUMA 节点的内存（通常够用，31G 堆 + 页缓存）  方案二：每个 NUMA 节点一个实例 #  在大型服务器上，每个 NUMA 节点运行一个 Easysearch 实例：\n# 实例 1：绑定 NUMA node 0，HTTP 端口 9200 numactl --cpunodebind=0 --membind=0 /data/easysearch-node1/bin/easysearch # 实例 2：绑定 NUMA node 1，HTTP 端口 9201 numactl \u0026ndash;cpunodebind=1 \u0026ndash;membind=1 /data/easysearch-node2/bin/easysearch 每个实例配置独立的：\n http.port transport.port path.data node.name  方案三：交错分配（不推荐用于搜索场景） #  # 内存在所有 NUMA 节点间交错分配 numactl --interleave=all /data/easysearch/bin/easysearch 交错分配平均化了延迟，但放弃了本地访问的优势，通常不如方案一。\nsystemd 集成 #  如果使用 systemd 管理 Easysearch，在 service 文件中配置：\n[Service] ExecStart=/usr/bin/numactl --cpunodebind=0 --membind=0 /data/easysearch/bin/easysearch JVM 相关配置 #  配合 NUMA 绑定，JVM 参数建议：\n# config/jvm.options # 启用 NUMA 感知的内存分配（G1 GC） -XX:+UseNUMA # 大页内存（可选，需要 OS 配置） -XX:+UseLargePages 配置大页内存（可选） #\n # 计算所需大页数（堆 31G / 大页 2MB = 15872 页，预留额外） echo 16000 \u0026gt; /proc/sys/vm/nr_hugepages # 持久化 echo \u0026quot;vm.nr_hugepages=16000\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf 验证 NUMA 绑定 #\n # 查看进程的 NUMA 分配 numastat -p $(pgrep -f easysearch | head -1) # 查看内存是否集中在绑定的节点 # 期望：绑定节点的内存使用远高于其他节点 性能对比参考 #\n    配置 索引吞吐 查询延迟 (p99)     不做 NUMA 配置 基准 基准   --interleave=all +5~10% -5~10%   --cpunodebind=0 --membind=0 +15~25% -20~30%     实际收益取决于硬件拓扑、数据规模和查询模式。\n 延伸阅读 #    系统调优  生产环境部署  硬件配置  ","subcategory":null,"summary":"","tags":null,"title":"NUMA 配置","url":"/easysearch/main/docs/deployment/advanced-config/numa/"},{"category":null,"content":"Match All 查询 #  match_all 查询返回所有文档。如果需要返回整个文档集，这个查询在测试大量文档集时很有用。\n相关指南（先读这些） #    查询 DSL 基础  结构化搜索  GET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } match_all 查询有一个 match_none 的对应查询，这个对应查询很少有用：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_none\u0026#34;: {} } } 参数说明 #\n 全匹配和全不匹配查询都接受以下参数。所有参数都是可选的。\n   参数 数据类型 描述     boost Float 一个浮点数值，用于指定该字段对相关性分数的权重。大于 1.0 的值会增加字段的权重。介于 0.0 和 1.0 之间的值会降低字段的权重。默认值为 1.0。   _name String 用于查询标签的查询名称。可选。    常见使用场景 #  配合 filter 使用 #  match_all 经常与过滤器结合使用，在不需要相关性评分的场景下获取文档子集：\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;filter\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;gte\u0026#34;: 100 } } } } } } 在聚合中使用 #  当只需要聚合结果而不关心搜索命中时，match_all 是默认的隐含查询。以下两种写法等价：\nGET products/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } } } } 调整 boost 控制评分 #  通过调整 boost 值可以控制 match_all 分配给所有文档的评分（默认为 1.0）：\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: { \u0026#34;boost\u0026#34;: 1.5 } } }  提示：match_all 在没有指定查询条件时是 Easysearch 的默认行为。直接使用 GET index/_search 等价于使用 match_all。\n ","subcategory":null,"summary":"","tags":null,"title":"Match All 查询","url":"/easysearch/main/docs/features/query-dsl/match-all/"},{"category":null,"content":"Lowercase 规范化器 #  lowercase 是 Easysearch 内置的规范化器，将 keyword 字段的整个值转换为小写形式。无需在 settings 中定义即可直接使用。\n使用方式 #  在映射中直接引用：\nPUT my-index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;status\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;lowercase\u0026#34; } } } } 效果演示 #  索引文档：\nPUT my-index/_doc/1 { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34; } PUT my-index/_doc/2 { \u0026quot;status\u0026quot;: \u0026quot;Ok\u0026quot; }\nPUT my-index/_doc/3 { \u0026quot;status\u0026quot;: \u0026quot;ok\u0026quot; } 查询时，无论使用哪种大小写形式都能匹配所有文档：\nGET my-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34; } } } 三个文档都会被返回，因为 \u0026quot;OK\u0026quot;、\u0026quot;Ok\u0026quot;、\u0026quot;ok\u0026quot; 在索引时都被归一化为 \u0026quot;ok\u0026quot;。\n对聚合的影响 #  GET my-index/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;status_counts\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;status\u0026#34; } } } } 聚合结果中只会出现 \u0026quot;ok\u0026quot; 一个桶（而非三个不同的大小写变体），计数为 3。\n等价的自定义配置 #  lowercase 内置规范化器等价于以下自定义配置：\nPUT my-index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;normalizer\u0026#34;: { \u0026#34;my_lowercase\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;] } } } } } 如果只需要小写转换，推荐直接使用内置的 lowercase，更简洁。\n适用场景 #     场景 示例     状态码匹配 \u0026quot;OK\u0026quot; / \u0026quot;ok\u0026quot; / \u0026quot;Ok\u0026quot; 视为相同   标签归一化 \u0026quot;JavaScript\u0026quot; / \u0026quot;javascript\u0026quot; 聚合在一起   枚举值查询 \u0026quot;ACTIVE\u0026quot; / \u0026quot;active\u0026quot; 无差别匹配   用户名不区分大小写 \u0026quot;Admin\u0026quot; / \u0026quot;admin\u0026quot; 指向同一用户    限制 #   仅做小写转换，不处理变音符号（\u0026quot;Naïve\u0026quot; → \u0026quot;naïve\u0026quot; 而非 \u0026quot;naive\u0026quot;） 不做 Unicode 归一化或字符映射 如需更复杂的归一化逻辑，请使用 自定义规范化器  ","subcategory":null,"summary":"","tags":null,"title":"Lowercase 规范化器","url":"/easysearch/main/docs/features/mapping-and-analysis/text-analysis/normalizers/lowercase/"},{"category":null,"content":"Java 客户端集成与示例 #  Easysearch Java API Client 是 Easysearch 的官方 Java 客户端，提供了强类型、流式构建器风格的 API 接口。新项目推荐使用此客户端。\n 全新重构的 2.0.x 版本，更轻量，移除冗余依赖 兼容 Easysearch 各版本 支持阻塞和异步两种调用方式 使用流式构建器和函数式模式，代码简洁易读 通过 Jackson 无缝集成应用类  相关指南 #    官方 Java Client API 文档  如何使用 Curl 操作  依赖引入 #  Maven #  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.infinilabs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;easysearch-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Gradle #  implementation \u0026#39;com.infinilabs:easysearch-client:2.0.2\u0026#39;  已发布到 Maven 中央仓库： mvnrepository.com/artifact/com.infinilabs/easysearch-client，需要 JDK 8 或以上。\n 连接配置 #  基础连接（带认证 + HTTPS） #  Easysearch 默认启用安全认证和 HTTPS，以下代码展示完整的连接初始化：\nimport com.infinilabs.clients.easysearch.EasysearchClient; import com.infinilabs.clients.json.jackson.JacksonJsonpMapper; import com.infinilabs.clients.transport.rest_client.RestClientTransport; import com.infinilabs.clients.transport.EasysearchTransport; import org.apache.http.HttpHost; import org.apache.http.auth.AuthScope; import org.apache.http.auth.UsernamePasswordCredentials; import org.apache.http.impl.client.BasicCredentialsProvider; import org.apache.http.conn.ssl.NoopHostnameVerifier; import org.apache.http.nio.conn.ssl.SSLIOSessionStrategy; import org.elasticsearch.client.RestClient;\nimport javax.net.ssl.SSLContext; import org.apache.http.ssl.SSLContextBuilder;\npublic static EasysearchClient create() throws Exception { HttpHost[] hosts = new HttpHost[]{new HttpHost(\u0026quot;localhost\u0026quot;, 9200, \u0026quot;https\u0026quot;)};\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// SSL 配置（开发环境信任所有证书）   SSLContext sslContext = SSLContextBuilder.create() .loadTrustMaterial(null, (chains, authType) -\u0026gt; true) .build(); SSLIOSessionStrategy sessionStrategy = new SSLIOSessionStrategy(sslContext, NoopHostnameVerifier.INSTANCE);\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 认证   BasicCredentialsProvider credsProv = new BasicCredentialsProvider(); credsProv.setCredentials(AuthScope.ANY, new UsernamePasswordCredentials(\u0026quot;admin\u0026quot;, \u0026quot;your_password\u0026quot;));\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 创建 RestClient   RestClient restClient = RestClient.builder(hosts) .setHttpClientConfigCallback(httpBuilder -\u0026gt; httpBuilder .setDefaultCredentialsProvider(credsProv) .setSSLStrategy(sessionStrategy) .disableAuthCaching() ) .setRequestConfigCallback(reqBuilder -\u0026gt; reqBuilder.setConnectTimeout(30000).setSocketTimeout(300000) ) .build();\n\u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// 创建 EasysearchClient   EasysearchTransport transport = new RestClientTransport( restClient, new JacksonJsonpMapper()); return new EasysearchClient(transport); } \n生产环境建议：使用正规 CA 签发的证书或导入 Easysearch 的 CA 证书到信任库中，而非信任所有证书。\n 基础 CRUD 操作 #  索引文档 #  EasysearchClient client = create(); // 使用 Map Map\u0026lt;String, Object\u0026gt; doc = Map.of( \u0026quot;name\u0026quot;, \u0026quot;Easysearch\u0026quot;, \u0026quot;category\u0026quot;, \u0026quot;database\u0026quot; ); client.index(i -\u0026gt; i.index(\u0026quot;products\u0026quot;).id(\u0026quot;1\u0026quot;).document(doc)); 获取文档 #\n var response = client.get(g -\u0026gt; g.index(\u0026#34;products\u0026#34;).id(\u0026#34;1\u0026#34;), Map.class); if (response.found()) { System.out.println(response.source()); } 删除文档 #  client.delete(d -\u0026gt; d.index(\u0026#34;products\u0026#34;).id(\u0026#34;1\u0026#34;)); 搜索查询 #  var response = client.search(s -\u0026gt; s .index(\u0026#34;products\u0026#34;) .query(q -\u0026gt; q.match(m -\u0026gt; m.field(\u0026#34;name\u0026#34;).query(\u0026#34;Easysearch\u0026#34;))) .from(0) .size(10), Map.class ); response.hits().hits().forEach(hit -\u0026gt; System.out.println(hit.source()) ); 批量操作 #\n import com.infinilabs.clients.easysearch.core.bulk.BulkRequest; var result = client.bulk(b -\u0026gt; b .operations(o -\u0026gt; o.index(i -\u0026gt; i.index(\u0026quot;products\u0026quot;).id(\u0026quot;2\u0026quot;) .document(Map.of(\u0026quot;name\u0026quot;, \u0026quot;Console\u0026quot;, \u0026quot;category\u0026quot;, \u0026quot;tool\u0026quot;)))) .operations(o -\u0026gt; o.index(i -\u0026gt; i.index(\u0026quot;products\u0026quot;).id(\u0026quot;3\u0026quot;) .document(Map.of(\u0026quot;name\u0026quot;, \u0026quot;Gateway\u0026quot;, \u0026quot;category\u0026quot;, \u0026quot;proxy\u0026quot;)))) );\nif (result.errors()) { result.items().stream() .filter(item -\u0026gt; item.error() != null) .forEach(item -\u0026gt; System.err.println(item.error().reason())); } 生产环境配置要点 #\n    配置项 推荐值 说明     连接超时 5000~30000ms setConnectTimeout()   Socket 超时 60000~300000ms setSocketTimeout()   最大连接数 30~100 setMaxConnTotal(100)   多节点负载均衡 配置所有节点 RestClient.builder(host1, host2, host3)   关闭客户端 应用退出时调用 restClient.close()     提示：在微服务架构中，建议将 EasysearchClient 作为单例 Bean 管理（如 Spring @Bean），避免频繁创建和销毁连接。\n 已有项目兼容说明 #  如果你有已有项目使用 Elasticsearch 7.10 的 RestHighLevelClient，也可以直接连接 Easysearch（需在 easysearch.yml 中开启 elasticsearch.api_compatibility: true）。但新项目推荐使用 Easysearch 官方客户端，获得更好的类型安全和 API 体验。\n","subcategory":null,"summary":"","tags":null,"title":"Java 客户端集成与示例","url":"/easysearch/main/docs/integrations/clients/java/"},{"category":null,"content":"Embedding 服务接入 #  要在 Easysearch 中使用向量检索，首先需要将文本（或其他数据）转换为向量表示。这个过程需要 Embedding 模型服务的支持。\n相关指南（先读这些） #    向量检索  Ingest Text Embedding  Search Text Embedding  LangChain 集成  部署模式 #  模式一：写入链路嵌入（推荐） #  在数据写入 Easysearch 时，通过 Ingest Pipeline 自动调用 Embedding 服务：\n应用数据 → Easysearch Ingest Pipeline → 调用 Embedding API → 写入向量字段 优势是写入后即可搜索，无需维护外部向量化流程。参见 Ingest Text Embedding。\n模式二：查询链路嵌入 #  搜索时实时将查询文本转换为向量，在 Easysearch 中做 kNN 检索：\n用户查询 → 调用 Embedding API → 向量化 → Easysearch kNN 搜索 参见 Search Text Embedding。\n模式三：离线批处理 #  在应用侧完成向量化，再将向量字段直接写入 Easysearch：\n原始数据 → 批处理脚本 → 调用 Embedding API → 写入 Easysearch（含向量字段） 适合大批量历史数据回填或对延迟不敏感的场景。\n常用 Embedding 服务 #     服务/模型 维度 语言支持 部署方式     OpenAI text-embedding-3 256~3072 多语言 云 API   Cohere Embed 1024 多语言 云 API   BGE / M3E（BAAI） 768~1024 中英文 自部署   Sentence-Transformers 384~768 多语言 自部署   通义千问 Embedding 1536 中英文 云 API    自部署 Embedding 服务示例 #  使用 Python FastAPI 部署一个简单的 Embedding 服务：\nfrom fastapi import FastAPI from sentence_transformers import SentenceTransformer app = FastAPI() model = SentenceTransformer(\u0026quot;BAAI/bge-base-zh-v1.5\u0026quot;)\n@app.post(\u0026quot;/embed\u0026quot;) async def embed(texts: list[str]): embeddings = model.encode(texts).tolist() return {\u0026quot;embeddings\u0026quot;: embeddings} 启动后即可被 Easysearch 的 Ingest Pipeline 或应用程序调用。\n实践建议 #     要点 建议     维度选择 768~1024 维是精度和性能的平衡点，过高维度会增加存储和检索成本   批量处理 一次请求多条文本，减少网络往返   缓存策略 对相同文本的 Embedding 结果做缓存，避免重复计算   超时设置 Embedding 服务响应时间波动大，设置合理的超时（如 30s）   模型一致性 写入和查询必须使用同一模型，否则向量空间不兼容    ","subcategory":null,"summary":"","tags":null,"title":"Embedding 服务接入","url":"/easysearch/main/docs/integrations/ai/embedding-service/"},{"category":null,"content":"Easy-ES 查询框架 #   Easy-ES 是 Dromara 开源社区下的搜索引擎 ORM 框架，类似于 MyBatis-Plus 之于 MySQL。它在 Java 客户端的基础上只做增强不做改变，提供 Lambda 风格的简洁 API，可减少 50%~80% 的代码量。\nEasy-ES 提供了专门的 Easysearch 原生分支，底层直接使用 Easysearch Java Client，无需 ES API 兼容模式，具备完整的原生性能和功能支持。\n 源码仓库： https://gitee.com/dromara/easy-es/tree/easy-es4easySearch Maven 中央仓库： easy-es-boot-starter 2.1.0-easysearch  核心优势 #     特性 说明     极简开发 一行代码完成查询，相比原生 API 代码量减少 50%~80%   自动索引管理 索引全生命周期由框架自动托管，零停机更新，无需手动管理   SQL 语法兼容 支持 MySQL 风格的 and、or、like、in 等常用语法查询   Lambda 表达式 类型安全的字段访问，避免手写字段名导致的错误   Spring Boot 集成 开箱即用的自动配置，完美融入 Spring Boot 生态   原生 Easysearch 支持 底层使用 Easysearch Java Client，无需兼容层     快速开始 #  1. 添加依赖 #  Maven #  \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;11\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;11\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;spring-boot.version\u0026gt;2.7.0\u0026lt;/spring-boot.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-boot.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt;\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dromara.easy-es\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;easy-es-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.0-easysearch\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Gradle #\n plugins { id \u0026#39;java\u0026#39; id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;2.7.0\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.0.11.RELEASE\u0026#39; } sourceCompatibility = '11'\ndependencies { implementation 'org.dromara.easy-es:easy-es-boot-starter:2.1.0-easysearch' implementation 'org.springframework.boot:spring-boot-starter-web' } 2. 配置连接 #\n # application.yml easy-es: enable: true address: localhost:9200 schema: https username: admin password: your_password_here keep-alive-millis: 18000 global-config: process-index-mode: smoothly async-process-index-blocking: true print-dsl: false db-config: map-underscore-to-camel-case: true id-type: customize field-strategy: not_empty refresh-policy: immediate enable-track-total-hits: true 配置说明：\n   参数 说明     address Easysearch 服务地址（host:port）   schema 协议，Easysearch 默认启用 HTTPS，填 https   username / password Easysearch 用户认证凭据   keep-alive-millis 连接保持时间（毫秒）   process-index-mode 索引处理模式：smoothly（平滑零停机）、manual（手动）   print-dsl 是否打印 DSL 语句（开发调试时设为 true）   refresh-policy 刷新策略：immediate（立即）、wait_until、none    3. 定义实体类 #  import org.dromara.easyes.annotation.*; import org.dromara.easyes.annotation.rely.*; @Settings(shardsNum = 3, replicasNum = 2) @IndexName(value = \u0026quot;easyes_document\u0026quot;, keepGlobalPrefix = true) public class Document {\n@IndexId\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;type \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; IdType\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;CUSTOMIZE\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; String id\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** 标题 - 默认 keyword 类型 */\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; String title\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** 内容 - TEXT 类型，使用 IK 分词器，支持高亮 */\u0026lt;/span\u0026gt; @HighLight\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;mappingField \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;highlightContent\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; @IndexField\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;fieldType \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; FieldType\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;TEXT\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; analyzer \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Analyzer\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;IK_SMART\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; String content\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** 创建者 */\u0026lt;/span\u0026gt; @IndexField\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;strategy \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; FieldStrategy\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;NOT_EMPTY\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; String creator\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** 创建时间 */\u0026lt;/span\u0026gt; @IndexField\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;fieldType \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; FieldType\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;DATE\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; dateFormat \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;yyyy-MM-dd HH:mm:ss\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; LocalDateTime gmtCreate\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** 高亮结果映射字段 */\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; String highlightContent\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** 点赞数 */\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; Integer starNum\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** 地理位置 */\u0026lt;/span\u0026gt; @IndexField\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;fieldType \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; FieldType\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;GEO_POINT\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; String location\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;// getters \u0026amp;amp; setters ...  } 常用注解：\n   注解 说明     @IndexName 指定索引名，keepGlobalPrefix 启用全局前缀   @IndexId 主键标识，IdType.CUSTOMIZE 为自定义 ID   @IndexField 字段映射，指定字段类型、分词器、更新策略等   @HighLight 高亮配置，mappingField 指定高亮结果映射的字段   @Settings 索引设置，分片数、副本数等    4. 定义 Mapper #  import org.dromara.easyes.core.kernel.BaseEsMapper; public interface DocumentMapper extends BaseEsMapper\u0026lt;Document\u0026gt; { } 5. 启动类配置 #\n import org.dromara.easyes.spring.annotation.EsMapperScan; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication @EsMapperScan(\u0026quot;com.example.mapper\u0026quot;) public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 6. 业务使用 #\n import org.dromara.easyes.core.conditions.select.LambdaEsQueryWrapper; import org.easysearch.action.search.SearchResponse; @RestController public class DocumentController {\n@Resource \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;private\u0026lt;/span\u0026gt; DocumentMapper documentMapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** 插入文档 */\u0026lt;/span\u0026gt; @GetMapping\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;/insert\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; Integer \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;insert\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; Document doc \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; doc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setId\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;1\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; doc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setTitle\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;Easy-ES 入门\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; doc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setContent\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;这是一篇关于 Easy-ES 与 Easysearch 集成的教程\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; doc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setCreator\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;admin\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; doc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setGmtCreate\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;LocalDateTime\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;now\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; doc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;setStarNum\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;100\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; documentMapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;insert\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;doc\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** Lambda 条件查询 */\u0026lt;/span\u0026gt; @GetMapping\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;/search\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; List\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;search\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;@RequestParam String keyword\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; LambdaEsQueryWrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; wrapper \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; LambdaEsQueryWrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026amp;gt;();\u0026lt;/span\u0026gt; wrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;match\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;::\u0026lt;/span\u0026gt;getContent\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; keyword\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;ge\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;::\u0026lt;/span\u0026gt;getStarNum\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; 10\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;orderByDesc\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;::\u0026lt;/span\u0026gt;getStarNum\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; documentMapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;selectList\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;wrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** 高亮搜索 */\u0026lt;/span\u0026gt; @GetMapping\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;/highlight\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; List\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;highlight\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;@RequestParam String content\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; LambdaEsQueryWrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; wrapper \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; LambdaEsQueryWrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026amp;gt;();\u0026lt;/span\u0026gt; wrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;match\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;::\u0026lt;/span\u0026gt;getContent\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; content\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; documentMapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;selectList\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;wrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** 聚合查询 */\u0026lt;/span\u0026gt; @GetMapping\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;/agg\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; Aggregations \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;aggregation\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; LambdaEsQueryWrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; wrapper \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; LambdaEsQueryWrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;\u0026amp;lt;\u0026amp;gt;();\u0026lt;/span\u0026gt; wrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;groupBy\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;::\u0026lt;/span\u0026gt;getGmtCreate\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;max\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;::\u0026lt;/span\u0026gt;getStarNum\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;min\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Document\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;::\u0026lt;/span\u0026gt;getStarNum\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; SearchResponse response \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; documentMapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;search\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;wrapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; response\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;getAggregations\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#6272a4\u0026quot;\u0026gt;/** SQL 查询 */\u0026lt;/span\u0026gt; @GetMapping\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;/sql\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#8be9fd;font-style:italic\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; String \u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;queryBySQL\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;@RequestParam String title\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; String sql \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; String\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;format\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f1fa8c\u0026quot;\u0026gt;\u0026amp;#34;SELECT * FROM dev_easyes_document WHERE title = \u0026amp;#39;%s\u0026amp;#39;\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; title\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; documentMapper\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#50fa7b\u0026quot;\u0026gt;executeSQL\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sql\u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#ff79c6\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt;  } \n与 ES 通用版的区别 #     对比项 Easysearch 原生版 ES 通用版     Maven 坐标 org.dromara.easy-es:easy-es-boot-starter:2.1.0-easysearch org.dromara.easy-es:easy-es-boot-starter:2.x.x   底层客户端 Easysearch Java Client Elasticsearch RestHighLevelClient   API 兼容模式 不需要 需要开启 elasticsearch.api_compatibility: true   源码分支 easy-es4easySearch master   import 包名 org.easysearch.* org.elasticsearch.*     如果您从 Easy-ES 的 ES 通用版本迁移到 Easysearch 原生版，只需更换 Maven 依赖并将 import 中的 org.elasticsearch 替换为 org.easysearch。\n  注意事项 #     注意项 说明     Java 版本 需要 JDK 11 及以上   Spring Boot 版本 推荐 2.7.x   HTTPS Easysearch 默认启用 HTTPS，配置 schema: https   认证 配置 username 和 password   索引前缀 通过 db-config.index-prefix 设置全局索引前缀    相关链接 #    Easy-ES 官方网站  Easy-ES Easysearch 分支源码  Easy-ES 2.1.0-easysearch 发布公告  Maven 中央仓库  延伸阅读 #    Java 客户端  Quick Start 教程  ","subcategory":null,"summary":"","tags":null,"title":"Easy-ES 查询框架","url":"/easysearch/main/docs/integrations/third-party/easy-es/"},{"category":null,"content":"查询与过滤上下文 #  查询由查询子句组成，这些子句可以在过滤（filter）上下文或查询上下文中运行。\n在过滤（filter）上下文中的查询子句会询问\u0026quot;文档是否匹配查询子句？\u0026ldquo;并返回匹配的文档。在查询上下文中的查询子句会询问\u0026quot;文档与查询子句匹配程度如何？\u0026quot;，返回匹配的文档，并以相关性分数的形式提供每个文档的相关性。\n相关指南（先读这些） #    查询 DSL 基础  结构化搜索  相关性分数 #  相关性分数衡量文档与查询的匹配程度。它是一个正浮点数，Easysearch 会记录在每个文档的 _score 元数据字段中：\n\u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;shakespeare\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;32437\u0026#34;, \u0026#34;_score\u0026#34; : 18.781435, \u0026#34;_source\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34; : 32438, \u0026#34;play_name\u0026#34; : \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34; : 3, \u0026#34;line_number\u0026#34; : \u0026#34;1.1.3\u0026#34;, \u0026#34;speaker\u0026#34; : \u0026#34;BERNARDO\u0026#34;, \u0026#34;text_entry\u0026#34; : \u0026#34;Long live the king!\u0026#34; } }, ... 一个更高的分数表示文档更相关。虽然不同的查询类型计算相关性分数的方式不同，但所有查询类型都会考虑查询子句是在过滤（filter）上下文还是查询上下文中运行。\n 在查询上下文中使用你想影响相关性分数的查询子句，并在过滤（filter）上下文中使用所有其他查询子句。\n 过滤上下文 #  在过滤上下文中的查询子句会问“文档是否匹配查询子句？”，这个问题有二元答案。例如，如果你有一个包含学生数据的索引，你可以使用过滤上下文来回答以下关于学生的疑问:\n 学生的 honors 状态是否设置为 true ？ 学生的 graduation_year 是否在 2020–2022 范围内？  使用过滤上下文时，Easysearch 会返回匹配的文档，而不会计算相关性分数。因此，您应该使用过滤上下文来处理具有精确值的字段。\n要在过滤上下文中运行查询子句，请将其传递给 filter 参数。例如，以下布尔查询会搜索在 2020–2022 年获得荣誉学位的学生：\nGET students/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;honors\u0026#34;: true }}, { \u0026#34;range\u0026#34;: { \u0026#34;graduation_year\u0026#34;: { \u0026#34;gte\u0026#34;: 2020, \u0026#34;lte\u0026#34;: 2022 }}} ] } } } 为了提高性能，Easysearch 会缓存常用的过滤器。\n查询上下文 #  查询上下文中的查询子句会询问“文档与查询子句的匹配程度如何？”，这个问题没有二元答案。查询上下文适用于全文搜索，你不仅希望获得匹配的文档，还希望确定每个文档的相关性。例如，如果你有一个包含莎士比亚全部作品的索引，你可以使用查询上下文进行以下搜索：\n 查找包含单词 dream 的文档，包括其各种形式（ dreaming 或 dreams ）和同义词（ contemplate ）。 找到匹配 long live king 单词的文档。  使用查询上下文时，每个匹配的文档在 _score 字段中包含一个相关性分数，你可以用它来按相关性排序文档。\n要在查询上下文中运行查询子句，将其传递给 query 参数。例如，以下查询在 shakespeare 索引中搜索匹配 long live king 单词的文档：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;long live king\u0026#34; } } }  相关性分数是单精度浮点数，具有 24 位尾数精度。如果分数计算超过尾数精度，可能会发生精度损失。\n ","subcategory":null,"summary":"","tags":null,"title":"查询与过滤上下文","url":"/easysearch/main/docs/features/query-dsl/query-filter-context/"}]