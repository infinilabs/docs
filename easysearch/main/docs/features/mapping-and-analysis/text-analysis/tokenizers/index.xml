<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>分词器（Tokenizers） on INFINI Easysearch (main)</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/</link><description>Recent content in 分词器（Tokenizers） on INFINI Easysearch (main)</description><generator>Hugo -- gohugo.io</generator><atom:link href="/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/index.xml" rel="self" type="application/rss+xml"/><item><title>标准分词器（Standard）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/standard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/standard/</guid><description>Standard 分词器 # standard 分词器是 Easysearch 中的默认分词器。它基于单词边界，采用一种基于语法的方法对文本进行分词，这种方法能够识别字母、数字以及标点等其他字符。它具有高度的通用性，适用于多种语言，它使用了 Unicode 文本分割规则（ UAX#29）来将文本分割成词元。
相关指南（先读这些） # 词汇识别 文本分析基础 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用标准词元生成器的分词器：
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_standard_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;standard&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_standard_analyzer&amp;#34; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;my_standard_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Easysearch is powerful, fast, and scalable.</description></item><item><title>字母分词器（Letter）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/letter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/letter/</guid><description>Letter 分词器 # letter 分词器会根据任何非字母字符将文本拆分成单词。它在处理许多欧洲语种时效果良好，但对于一些亚洲语种却不太有效，因为在亚洲语种中，单词之间不是由空格来分隔的。
相关指南（先读这些） # 文本分析：识别词元 文本分析基础 参考样例 # 下面的示例请求创建了一个名为 my_index 的新索引，并配置了一个使用字母词元生成器的分词器。
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_letter_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;letter&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_letter_analyzer&amp;#34; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST _analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;letter&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Cats 4EVER love chasing butterflies!&amp;#34; } 返回内容包含产生的词元</description></item><item><title>小写分词器（Lowercase）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/lowercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/lowercase/</guid><description>Lowercase 分词器 # lowercase 分词器会在空白处将文本拆分成词条，然后把所有词条转换为小写形式。从功能上来说，这与配置一个 letter 分词器并搭配一个 lowercase 词元过滤器的效果是一样的。不过，使用 lowercase 分词器效率更高，因为分词操作是在一步之内完成的。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 文本分析基础 参考样例 # 以下示例请求创建了一个名为 my-lowercase-index 的新索引，并配置了一个使用小写词元生成器的分词器：
PUT /my-lowercase-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_lowercase_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;lowercase&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_lowercase_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;my_lowercase_tokenizer&amp;#34; } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my-lowercase-index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;my_lowercase_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;This is a Test.</description></item><item><title>空白分词器（Whitespace）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/whitespace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/whitespace/</guid><description>Whitespace 分词器 # whitespace 分词器会依据空白字符（如空格、制表符和换行符）对文本进行拆分。它将由空白字符分隔开的每个单词都视为一个词元，并且不会执行诸如转换为小写形式或去除标点符号等额外的规范化操作。
相关指南（先读这些） # 文本分析：识别词元 文本分析基础 参考样例 # 以下示例请求创建一个名为 my_index 的新索引，并配置一个使用空格词元生成器的分词器：
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;whitespace_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;whitespace&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_whitespace_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;whitespace_tokenizer&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_whitespace_analyzer&amp;#34; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;my_whitespace_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Easysearch is fast!</description></item><item><title>关键字分词器（Keyword）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/keyword/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/keyword/</guid><description>Keyword 分词器 # keyword 分词器接收文本并将其原封不动地作为单个词元输出。当你希望输入内容保持完整时，比如在管理像姓名、产品代码或电子邮件地址这类结构化数据时，这个分词器就特别有用。
keyword 分词器可以与词元过滤器搭配使用来处理文本。例如，对文本进行规范化处理或去除多余的字符。
相关指南（先读这些） # 文本分析：识别词元 文本分析基础 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用关键字词元生成器的分词器：
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_keyword_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;keyword&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_keyword_analyzer&amp;#34; } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;my_keyword_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Easysearch Example&amp;#34; } 返回内容会是包含原始内容的单个词元：</description></item><item><title>N-gram 分词器（N-gram）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/n-gram/</guid><description>N-gram 分词器 # ngram 分词器会将文本拆分为指定长度的重叠 n-gram（固定长度为 n 的字符序列）。当你希望实现部分单词匹配或自动补全搜索功能时，这个分词器特别有用，因为它会生成原始输入文本的子字符串（n-gram 字符串）。
相关指南（先读这些） # 文本分析：识别词元 部分匹配 文本分析基础 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 n-gram 词元生成器的分词器。
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_ngram_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;ngram&amp;#34;, &amp;#34;min_gram&amp;#34;: 3, &amp;#34;max_gram&amp;#34;: 4, &amp;#34;token_chars&amp;#34;: [&amp;#34;letter&amp;#34;, &amp;#34;digit&amp;#34;] } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_ngram_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;my_ngram_tokenizer&amp;#34; } } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;my_ngram_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Search&amp;#34; } 返回内容包含产生的词元</description></item><item><title>边缘 N-gram 分词器（Edge N-gram）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/edge-n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/edge-n-gram/</guid><description>Edge N-gram 分词器 # edge_ngram 分词器会从每个单词的开头生成部分单词词元，也就是 n 元组。它会根据指定的字符分割文本，并在定义的最小和最大长度范围内生成词元。这种分词器在实现即输即搜（search-as-you-type）功能时特别有用。
前缀 n 元组非常适合用于自动补全搜索，在这种搜索中单词的顺序可能会有所不同，例如在搜索产品名称或地址时。有关更多信息，请参阅&amp;quot;自动补全&amp;quot;相关内容。不过，对于像电影或歌曲标题这种顺序固定的文本，完成建议器（completion suggester）可能会更准确。
默认情况下，edge_ngram 分词器生成的词元最小长度为 1，最大长度为 2。例如，当分析文本 Easysearch 时，默认配置会生成 &amp;ldquo;E&amp;rdquo; 和 &amp;ldquo;Ea&amp;rdquo; 这两个 n 元组。这些短的 n 元组常常会匹配到太多不相关的词条，所以调整 n 元组的长度，对分词器进行优化是很有必要的。
相关指南（先读这些） # 文本分析：识别词元 部分匹配 文本分析基础 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用前缀 n 元词元生成器的分词器。该词元生成器生成长度为 3 到 6 个字符的词元，且将字母和符号都视为有效的词元字符。
PUT /edge_n_gram_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_custom_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;my_custom_tokenizer&amp;#34; } }, &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_custom_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;edge_ngram&amp;#34;, &amp;#34;min_gram&amp;#34;: 3, &amp;#34;max_gram&amp;#34;: 6, &amp;#34;token_chars&amp;#34;: [ &amp;#34;letter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>正则表达式分词器（Pattern）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/pattern/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/pattern/</guid><description>Pattern 分词器 # pattern 分词器是一种高度灵活的分词器，它允许你根据自定义的 Java 正则表达式将文本拆分为词元。与使用 Lucene 正则表达式的 simple_pattern 分词器和 simple_pattern_split 分词器不同，pattern 分词器能够处理更复杂、更细致的正则表达式模式，从而让你对文本的分词方式拥有更强的掌控力。
相关指南（先读这些） # 文本分析：识别词元 文本分析基础 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用匹配词元生成器的分词器。该分词器会在 -、_ 或 . 字符处对文本进行分割。
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_pattern_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;pattern&amp;#34;, &amp;#34;pattern&amp;#34;: &amp;#34;[-_.]&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_pattern_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;my_pattern_tokenizer&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_pattern_analyzer&amp;#34; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>简单模式分词器（Simple Pattern）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/simple-pattern/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/simple-pattern/</guid><description>Simple Pattern 分词器 # simple_pattern 分词器使用正则表达式匹配文本，将匹配到的内容作为词元输出。它与 simple_pattern_split 的区别在于：simple_pattern 输出匹配的部分，而 simple_pattern_split 输出被分隔的部分。
该分词器使用 Lucene 正则表达式，语法是标准正则表达式的子集。
参数 # 参数 说明 默认值 pattern Lucene 正则表达式模式 空字符串（匹配空串） 示例 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;simple_pattern&amp;#34;, &amp;#34;pattern&amp;#34;: &amp;#34;[0-9]{3}&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;my_tokenizer&amp;#34; } } } } } POST my_index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;my_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;fd]]]-%]afd][ 123 fd-ede 456&amp;#34; } 以上示例将产生 123 和 456 两个词元。</description></item><item><title>简单模式分割分词器（Simple Pattern Split）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/simple-pattern-split/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/simple-pattern-split/</guid><description>Simple Pattern Split 分词器 # simple_pattern_split 分词器使用正则表达式将文本分割成词元。该正则表达式定义了用于确定文本分割位置的模式。文本中任何匹配该模式的部分都会被用作分隔符，分隔符之间的文本则成为一个词元。当你想定义分隔符，并根据某个模式对文本的其余部分进行分词时，可以使用此分词器。
该分词器仅将输入文本中匹配到的部分（基于正则表达式）用作分隔符或边界，以将文本分割成词条。匹配到的部分不会包含在生成的词元中。例如，如果将分词器配置为在点号（.）处分割文本，输入文本为 one.two.three，那么生成的词元就是 one、two 和 three。点号本身不会包含在生成的词条中。
相关指南（先读这些） # 文本分析：识别词元 文本分析基础 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用简单分割匹配词元生成器的分词器。该分词器被配置为在连字符（-）处分割文本：
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_pattern_split_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;simple_pattern_split&amp;#34;, &amp;#34;pattern&amp;#34;: &amp;#34;-&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_pattern_split_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;my_pattern_split_tokenizer&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_pattern_split_analyzer&amp;#34; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>路径层次分词器（Path Hierarchy）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/path-hierarchy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/path-hierarchy/</guid><description>Path Hierarchy 分词器 # path_hierarchy 分词器可以把文件路径（或类似的层次结构）的文本按照各个层级分解为词元。当处理诸如文件路径、网址或其他有分隔符的层次结构数据时，这个分词器特别有用。
相关指南（先读这些） # 文本分析：识别词元 文本分析基础 参考样例 # 以下示例请求创建一个名为 my_index 的新索引，并配置一个使用路径词元生成器的分词器：
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_path_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;path_hierarchy&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_path_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;my_path_tokenizer&amp;#34; } } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;my_path_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;/users/john/documents/report.txt&amp;#34; } 返回内容包含产生的词元
{ &amp;#34;tokens&amp;#34;: [ { &amp;#34;token&amp;#34;: &amp;#34;/users&amp;#34;, &amp;#34;start_offset&amp;#34;: 0, &amp;#34;end_offset&amp;#34;: 6, &amp;#34;type&amp;#34;: &amp;#34;word&amp;#34;, &amp;#34;position&amp;#34;: 0 }, { &amp;#34;token&amp;#34;: &amp;#34;/users/john&amp;#34;, &amp;#34;start_offset&amp;#34;: 0, &amp;#34;end_offset&amp;#34;: 11, &amp;#34;type&amp;#34;: &amp;#34;word&amp;#34;, &amp;#34;position&amp;#34;: 0 }, { &amp;#34;token&amp;#34;: &amp;#34;/users/john/documents&amp;#34;, &amp;#34;start_offset&amp;#34;: 0, &amp;#34;end_offset&amp;#34;: 21, &amp;#34;type&amp;#34;: &amp;#34;word&amp;#34;, &amp;#34;position&amp;#34;: 0 }, { &amp;#34;token&amp;#34;: &amp;#34;/users/john/documents/report.</description></item><item><title>字符组分词器（Character Group）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/character-group/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/character-group/</guid><description>Character Group 分词器 # char_group 分词器使用特定字符作为分隔符将文本拆分为词元。它适用于需要简单直接进行分词的场景，为基于分词器的匹配模式提供了一种更简单的替代方案，避免了额外的复杂性。
相关指南（先读这些） # 文本分析：识别词元 文本分析基础 参考样例 # 以下示例请求创建了一个名为my_index的新索引，并配置了一个带有 char_group 字符组词元生成器的分词器。该词元生成器会依据空格、连字符 - 和冒号 : 来分割文本。
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_char_group_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;char_group&amp;#34;, &amp;#34;tokenize_on_chars&amp;#34;: [ &amp;#34;whitespace&amp;#34;, &amp;#34;-&amp;#34;, &amp;#34;:&amp;#34; ] } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_char_group_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;my_char_group_tokenizer&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_char_group_analyzer&amp;#34; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>URL 邮箱分词器（UAX URL Email）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/uax-url-email/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/uax-url-email/</guid><description>UAX URL Email 分词器 # 除了常规文本之外，uax_url_email 分词器还专门用于处理网址、电子邮件地址和域名。它基于 Unicode 文本分割算法（ UAX #29），这使得它能够正确地对复杂文本（包括网址和电子邮件地址）进行分词处理。
相关指南（先读这些） # 文本分析：识别词元 文本分析基础 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 UAX URL 邮件词元生成器的分词器：
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;uax_url_email_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;uax_url_email&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_uax_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;uax_url_email_tokenizer&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_uax_analyzer&amp;#34; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>经典分词器（Classic）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/classic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/classic/</guid><description>Classic 分词器 # classic 分词器会解析文本，并应用英语语法规则将文本拆分为词元。它包含以下特定的逻辑来处理匹配规则：
相关指南（先读这些） # 文本分析：识别词元
文本分析基础
首字母缩写词
电子邮件地址
域名
某些类型的标点符号
这种词元生成器最适合处理英语文本。对于其他语言，尤其是那些具有不同语法结构的语言，它可能无法产生最佳效果。
经典词元生成器按如下方式解析文本：
标点符号：在大多数标点符号处拆分文本，并移除标点字符。后面不跟空格的点号会被视为词元的一部分。 连字符：在连字符处拆分单词，但当词元中存在数字时除外。当词元中存在数字时，该词元不会被拆分，而是被当作产品编号处理。 电子邮件：识别电子邮件地址和主机名，并将它们作为单个词元保留。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用经典词元生成器的分词器：
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_classic_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;classic&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_classic_analyzer&amp;#34; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>泰语分词器（Thai）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/thai/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/thai/</guid><description>Thai 分词器 # thai 分词器使用 Java 内置的泰语分词算法（BreakIterator）对泰语文本进行分词。对于非泰语文本，其行为与 standard 分词器相同。
示例 # POST _analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;thai&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;การทดสอบ&amp;#34; } 相关指南 # Standard 分词器 文本分析基础</description></item><item><title>ICU 分词器（ICU Tokenizer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/icu-tokenizer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/icu-tokenizer/</guid><description>ICU Tokenizer # icu_tokenizer 分词器使用 ICU 的 Unicode 文本分割算法，对多语言文本（尤其是亚洲语言混合文本）提供比 standard 分词器更好的分词效果。
需要安装 analysis-icu 插件。
示例 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_icu&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;icu_tokenizer&amp;#34; } } } } } 参数 # 参数 说明 默认值 rule_files 自定义 ICU 分词规则文件路径 无 相关指南 # ICU 分析器 Standard 分词器 文本分析基础</description></item><item><title>IK智能分词器（IK Smart）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/ik-smart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/ik-smart/</guid><description>IK Smart 分词器 # ik_smart 是 IK 分词器插件提供的智能分词模式，是最常用的中文分词方式。
##分词方式
智能分词（Smart）模式倾向于将文本分成&amp;quot;人类可读&amp;quot;的粗粒度词项，适合大多数应用场景。
示例 # POST _analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;ik_smart&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;北京市朝阳区建国路1号&amp;#34; } 分析结果 # [ &amp;#34;北京市&amp;#34;, &amp;#34;朝阳区&amp;#34;, &amp;#34;建国路&amp;#34;, &amp;#34;1&amp;#34;, &amp;#34;号&amp;#34; ] 相关指南 # IK分词器文档 文本分析基础 依赖插件 # analysis-ik 插件 配置 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_ik_smart&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;ik_smart&amp;#34; } } } } }</description></item><item><title>IK最大词分词器（IK Max Word）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/ik-max-word/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/ik-max-word/</guid><description>IK Max Word 分词器 # ik_max_word 是 IK 分词器插件提供的细粒度分词模式。
分词方式 # 最大词模式（Max Word）倾向于将文本分成最细粒度的词项，适合对召回率要求高的场景。
示例 # POST _analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;ik_max_word&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;北京市朝阳区建国路1号&amp;#34; } 分析结果 # [ &amp;#34;北京市&amp;#34;, &amp;#34;北京&amp;#34;, &amp;#34;市&amp;#34;, &amp;#34;朝阳区&amp;#34;, &amp;#34;朝阳&amp;#34;, &amp;#34;区&amp;#34;, &amp;#34;建国路&amp;#34;, &amp;#34;建国&amp;#34;, &amp;#34;路&amp;#34;, &amp;#34;1&amp;#34;, &amp;#34;号&amp;#34; ] 相关指南 # IK分词器文档 文本分析基础 依赖插件 # analysis-ik 插件 配置 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_ik_max_word&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;ik_max_word&amp;#34; } } } } }</description></item><item><title>Jieba Search 分词器</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/jieba-search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/jieba-search/</guid><description>Jieba Search 分词器 # jieba_search 分词器是 analysis-jieba 插件 提供的搜索模式分词器。它在精确分词的基础上对长词不做额外切分，适合搜索时使用。
前提条件 # bin/easysearch-plugin install analysis-jieba 与 jieba_index 的对比 # 分词器 模式 以&amp;quot;中华人民共和国&amp;quot;为例 适用场景 jieba_search 精确模式 中华人民共和国 搜索时 jieba_index 索引模式 中华、华人、人民、共和、共和国、中华人民共和国 索引时 使用示例 # 索引/搜索搭配 # PUT my-jieba-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;jieba_idx&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;jieba_index&amp;#34; }, &amp;#34;jieba_srch&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;jieba_search&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;jieba_idx&amp;#34;, &amp;#34;search_analyzer&amp;#34;: &amp;#34;jieba_srch&amp;#34; } } } } 测试分词 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;jieba_search&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;中华人民共和国国歌&amp;#34; } 相关链接 # Jieba Index 分词器 — 索引模式 文本分析</description></item><item><title>Jieba Index 分词器</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/jieba-index/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/jieba-index/</guid><description>Jieba Index 分词器 # jieba_index 分词器是 analysis-jieba 插件 提供的索引模式分词器。它在精确分词的基础上对长词进行二次切分，生成更多子词项，适合索引时使用以最大化召回率。
前提条件 # bin/easysearch-plugin install analysis-jieba 分词效果 # 以&amp;quot;中华人民共和国国歌&amp;quot;为例：
分词器 输出 jieba_search 中华人民共和国、国歌 jieba_index 中华、华人、人民、共和、共和国、中华人民共和国、国歌 使用示例 # 基本用法 # PUT my-jieba-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;jieba_idx&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;jieba_index&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;jieba_idx&amp;#34;, &amp;#34;search_analyzer&amp;#34;: &amp;#34;jieba_srch&amp;#34; } } } } 测试分词 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;jieba_index&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;中华人民共和国国歌&amp;#34; } 最佳实践 # 场景 分词器 索引时 jieba_index（最大化召回） 搜索时 jieba_search（精确匹配） 相关链接 # Jieba Search 分词器 — 搜索模式 文本分析</description></item><item><title>拼音分词器（Pinyin）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/pinyin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/pinyin/</guid><description>Pinyin 分词器 # pinyin 分词器来自 analysis-pinyin 插件，将中文文本直接转换为拼音词元。与 pinyin 词元过滤器不同，分词器在分词阶段就完成拼音转换。
前提条件 # bin/easysearch-plugin install analysis-pinyin 分词器 vs 过滤器 # 组件 说明 适用场景 pinyin 分词器 直接将整段文本转拼音并分词 纯拼音索引 pinyin 词元过滤器 在已有分词结果上转拼音 需要保留原始中文词元 使用示例 # 基本用法 # PUT my-pinyin-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;pinyin_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;pinyin&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;] } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;name&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;pinyin_analyzer&amp;#34; } } } } 测试分词 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;pinyin&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;中华人民共和国&amp;#34; } 参数 # 分词器支持与 pinyin 词元过滤器相同的参数：</description></item><item><title>拼音首字母分词器（Pinyin First Letter）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/pinyin-first-letter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/pinyin-first-letter/</guid><description>Pinyin First Letter 分词器 # pinyin_first_letter 分词器将中文字符转换为拼音首字母缩写，例如 &amp;ldquo;中华人民共和国&amp;rdquo; → &amp;ldquo;zhrmghg&amp;rdquo;。适用于拼音首字母搜索场景。
需要安装 analysis-pinyin 插件。
示例 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;pinyin_first_letter&amp;#34; } } } } } 相关指南 # Pinyin 分词器 Pinyin 分析器 文本分析基础</description></item><item><title>HanLP 分词器概述</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp/</guid><description>HanLP 分词器 # HanLP 是一个功能强大的中文自然语言处理库，通过 analysis-hanlp 插件 集成到 Easysearch 中。该插件提供了 7 种分词模式，覆盖从高速到高精度的各种需求。
前提条件 # bin/easysearch-plugin install analysis-hanlp 分词模式一览 # 分词器名称 模式 速度 精度 适用场景 hanlp_standard 标准分词 ★★★ ★★★★ 通用中文分词 hanlp_index 索引分词 ★★★ ★★★ 索引时最大化召回 hanlp_nlp NLP 分词 ★★ ★★★★★ 命名实体识别 hanlp_crf CRF 分词 ★★ ★★★★★ 新词发现 hanlp_n_short N-最短路径 ★★ ★★★★ 歧义消解 hanlp_dijkstra 最短路径 ★★★ ★★★ 快速精确分词 hanlp_speed 极速分词 ★★★★★ ★★ 大数据量高吞吐 索引/搜索推荐搭配 # PUT my-hanlp-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;hanlp_index_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_index&amp;#34; }, &amp;#34;hanlp_search_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_standard&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;hanlp_index_analyzer&amp;#34;, &amp;#34;search_analyzer&amp;#34;: &amp;#34;hanlp_search_analyzer&amp;#34; } } } } 测试分词 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_standard&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;中华人民共和国国歌&amp;#34; } 模式选择建议 # 需求 推荐模式 通用场景 hanlp_standard 索引时最大召回 hanlp_index 人名/地名/机构名识别 hanlp_nlp 识别新词（训练语料外的词） hanlp_crf 追求最大吞吐量 hanlp_speed 相关链接 # 文本分析</description></item><item><title>HanLP 标准分词器（HanLP Standard）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-standard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-standard/</guid><description>HanLP Standard 分词器 # hanlp_standard 分词器使用 HanLP 标准分词模式对中文文本进行分词，适合大多数中文搜索场景。
需要安装 analysis-hanlp 插件。
示例 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_standard&amp;#34; } } } } } 相关指南 # HanLP 分词器 HanLP 标准分析器 文本分析基础</description></item><item><title>HanLP Index 分词器</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-index/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-index/</guid><description>HanLP Index 分词器 # hanlp_index 分词器是 analysis-hanlp 插件 提供的索引模式分词器。它在标准分词的基础上对长词进行二次切分，生成更多子词项，适合索引时使用以提高召回率。
前提条件 # 需要安装 analysis-hanlp 插件：
bin/easysearch-plugin install analysis-hanlp 分词效果对比 # 以&amp;quot;中华人民共和国国歌&amp;quot;为例：
分词器 输出词项 hanlp_standard 中华人民共和国、国歌 hanlp_index 中华、华人、人民、共和、共和国、中华人民共和国、国歌 hanlp_index 输出更细粒度的子词，确保无论用户搜索&amp;quot;中华&amp;quot;、&amp;ldquo;人民&amp;quot;还是&amp;quot;共和国&amp;quot;都能命中。
使用示例 # 在映射中指定 # PUT my-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;hanlp_index_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_index&amp;#34; } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;hanlp_index_analyzer&amp;#34;, &amp;#34;search_analyzer&amp;#34;: &amp;#34;hanlp_standard&amp;#34; } } } } 测试分词效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_index&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;中华人民共和国国歌&amp;#34; } 最佳实践 # 场景 推荐 索引时 使用 hanlp_index（最大化召回） 搜索时 使用 hanlp_standard 或 hanlp_nlp（精确匹配） 相关链接 # HanLP Standard 分词器 — 标准分词模式 HanLP NLP 分词器 — 命名实体识别模式 HanLP 通用 — HanLP 分词器概述 文本分析</description></item><item><title>HanLP NLP 分词器（HanLP NLP）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-nlp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-nlp/</guid><description>HanLP NLP 分词器 # hanlp_nlp 分词器使用 HanLP NLP 分词模式，支持命名实体识别，适合需要高精度语义分析的场景。
需要安装 analysis-hanlp 插件，并确保 perceptron CWS 模型可用。
示例 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_nlp&amp;#34; } } } } } 相关指南 # HanLP 分词器 HanLP NLP 分析器 文本分析基础</description></item><item><title>HanLP CRF 分词器（HanLP CRF）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-crf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-crf/</guid><description>HanLP CRF 分词器 # hanlp_crf 分词器使用 HanLP CRF（条件随机场）分词模式，适合需要高精度分词的学术研究场景。
需要安装 analysis-hanlp 插件，并确保 CRF CWS 模型可用。
示例 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_crf&amp;#34; } } } } } 相关指南 # HanLP 分词器 HanLP CRF 分析器 文本分析基础</description></item><item><title>HanLP N 最短路分词器（HanLP N-Short）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-n-short/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-n-short/</guid><description>HanLP N-Short 分词器 # hanlp_n_short 分词器使用 HanLP N 最短路径分词算法，能找到全局 N 条最短路径，适合需要高精度分词的场景。
需要安装 analysis-hanlp 插件。
示例 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_n_short&amp;#34; } } } } } 相关指南 # HanLP 分词器 文本分析基础</description></item><item><title>HanLP 最短路分词器（HanLP Dijkstra）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-dijkstra/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-dijkstra/</guid><description>HanLP Dijkstra 分词器 # hanlp_dijkstra 分词器使用 HanLP Dijkstra 最短路径分词算法，基于词典的全切分方式。
需要安装 analysis-hanlp 插件。
示例 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_dijkstra&amp;#34; } } } } } 相关指南 # HanLP 分词器 文本分析基础</description></item><item><title>HanLP 极速分词器（HanLP Speed）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-speed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/hanlp-speed/</guid><description>HanLP Speed 分词器 # hanlp_speed 分词器使用 HanLP 极速分词模式，牺牲一定精度换取更快的分词速度，适合对延迟敏感的在线搜索场景。
需要安装 analysis-hanlp 插件。
示例 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;hanlp_speed&amp;#34; } } } } } 相关指南 # HanLP 分词器 文本分析基础</description></item><item><title>简繁体转换分词器（ST Convert）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/stconvert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/tokenizers/stconvert/</guid><description>ST Convert 分词器 # stconvert 分词器用于中文简繁体转换，可将简体中文转换为繁体中文，或将繁体中文转换为简体中文。
需要安装 analysis-stconvert 插件。
参数 # 参数 说明 默认值 convert_type 转换类型：s2t（简→繁）或 t2s（繁→简） s2t keep_both 是否同时保留转换前后的词元 false delimiter 用于分隔转换结果的分隔符 , 示例 # PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_stconvert&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stconvert&amp;#34;, &amp;#34;convert_type&amp;#34;: &amp;#34;s2t&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;my_stconvert&amp;#34; } } } } } 相关指南 # ST Convert 分析器 文本分析基础</description></item></channel></rss>