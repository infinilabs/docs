<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>分析器（Analyzers） on INFINI Easysearch</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/</link><description>Recent content in 分析器（Analyzers） on INFINI Easysearch</description><generator>Hugo -- gohugo.io</generator><atom:link href="/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/index.xml" rel="self" type="application/rss+xml"/><item><title>标准分析器（Standard）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/standard-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/standard-analyzer/</guid><description>Standard 分析器 # standard 分析器是在未指定其他分析器时默认使用的分析器。它旨在为通用文本处理提供一种基础且高效的方法。
该分析器由以下分词器和分词过滤器组成：
standard 分词器：去除大部分标点符号，并依据空格和其他常见分隔符对文本进行分割。 lowercase 分词过滤器：将所有词元转换为小写，以确保匹配时不区分大小写。 stop 分词过滤器：从分词后的输出中移除常见的停用词，例如 &amp;ldquo;the&amp;rdquo;、&amp;ldquo;is&amp;rdquo; 和 &amp;ldquo;and&amp;rdquo;。 相关指南（先读这些） # 文本分析基础 文本分析：识别词元 参考样例 # 以下命令创建一个名为 my_standard_index 并使用标准分词器的索引：
PUT /my_standard_index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;standard&amp;#34; } } } } 参数说明 # 你可以使用以下参数来配置标准分词器。
参数 必填/可选 数据类型 描述 max_token_length 可选 整数 设置生成的词元的最大长度。如果超过此长度，词元将按照 max_token_length 中配置的长度拆分为多个词元。默认值为 255。 stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 配置自定义分词器 # 以下命令配置一个索引，该索引带有一个等同于标准分词器的自定义分词器：</description></item><item><title>简单分析器（Simple）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/simple-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/simple-analyzer/</guid><description>Simple 分析器 # simple 分析器是一种非常基础的分析器，它会将文本中的非字母字符串拆分成词项，并将这些词项转换为小写形式。与 standard 分析器不同的是，simple 分析器将除字母字符之外的所有内容都视为分隔符，这意味着它不会把数字、标点符号或特殊字符识别为词元的一部分。
相关指南（先读这些） # 文本分析基础 文本分析：识别词元 参考样例 # 以下命令创建一个名为 my_simple_index 并使用简单分词器的索引：
PUT /my_simple_index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;simple&amp;#34; } } } } 配置自定义分词器 # 以下命令配置了一个索引，该索引带有一个自定义分词器，这个自定义分词器等同于添加了 html_strip 字符过滤器的简单分词器：
PUT /my_custom_simple_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;char_filter&amp;#34;: { &amp;#34;html_strip&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;html_strip&amp;#34; } }, &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_lowercase_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;lowercase&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_custom_simple_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;char_filter&amp;#34;: [&amp;#34;html_strip&amp;#34;], &amp;#34;tokenizer&amp;#34;: &amp;#34;my_lowercase_tokenizer&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;] } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_custom_simple_analyzer&amp;#34; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>空白分析器（Whitespace）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/whitespace-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/whitespace-analyzer/</guid><description>Whitespace 分析器 # whitespace 分析器仅基于空白字符（例如，空格和制表符）将文本拆分为词元。比如转换为小写形式或移除停用词这样的转换操作，它都不会应用，因此文本的原始大小写形式会被保留，并且标点符号也会作为词元的一部分包含在内。
相关指南（先读这些） # 文本分析基础 文本分析：识别词元 参考样例 # 以下命令创建一个名为 my_whitespace_index 并使用空格分词器的索引：
PUT /my_whitespace_index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;whitespace&amp;#34; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于空格分词器：
PUT /my_custom_whitespace_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_custom_whitespace_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;whitespace&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;] } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_custom_whitespace_analyzer&amp;#34; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>停用词分析器（Stop）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/stop-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/stop-analyzer/</guid><description>Stop 分析器 # stop 分析器会在文本中移除预定义的停用词。该分析器由一个小写分词器和一个停用词分词过滤器组成。
相关指南（先读这些） # 文本分析基础 文本分析：停用词 文本分析：识别词元 参数说明 # 你可以使用以下参数来配置一个停用词分词器。
参数 必填/可选 数据类型 描述 stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_stop_index 并使用停词器分词器的索引：
PUT /my_stop_index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;stop&amp;#34; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于停用词分词器：</description></item><item><title>关键字分析器（Keyword）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/keyword-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/keyword-analyzer/</guid><description>Keyword 分析器 # keyword 分析器根本不会对文本进行分词。相反，它将整个输入视为单个词元，不会将其拆分成单个的词项。keyword 分析器常用于包含电子邮件地址、网址或产品 ID 的字段，以及其他不需要进行分词的情况。
相关指南（先读这些） # 文本分析基础 文本分析：识别词元 参考样例 # 以下命令创建一个名为 my_keyword_index 使用关键词分词器的索引：
PUT /my_keyword_index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;keyword&amp;#34; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于关键词分词器：
PUT /my_custom_keyword_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_keyword_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;keyword&amp;#34; } } } } } 产生的词元 # 以下请求来检查使用该分词器生成的词元：
POST /my_custom_keyword_index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;my_keyword_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Just one token&amp;#34; } 返回内容包含产生的词元</description></item><item><title>正则表达式分析器（Pattern）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/pattern-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/pattern-analyzer/</guid><description>Pattern 分析器 # pattern 分析器允许你定义一个自定义分析器，该分析器使用正则表达式（regex）将输入文本分割成词元。它还提供了应用正则表达式、将词元转换为小写以及过滤掉停用词的功能。
相关指南（先读这些） # 文本分析基础 文本分析：识别词元 参数说明 # 匹配模式分词器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 pattern 可选 字符串 用于对输入内容进行分词匹配的 Java 正则表达式。默认值是 \W+。 flags 可选 字符串 以竖线分隔的 Java 正则表达式的字符串标志，这些标志会修改正则表达式的行为。 lowercase 可选 布尔值 是否将词元转换为小写。默认值为 true。 stopwords 可选 字符串或字符串列表 个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_pattern_index 并使用匹配模式分词器的索引：</description></item><item><title>指纹分析器（Fingerprint）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/fingerprint-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/fingerprint-analyzer/</guid><description>Fingerprint 分析器 # fingerprint 分析器会创建一个文本指纹。该分析器对从输入生成的词项（词元）进行排序和去重，然后使用一个分隔符将它们连接起来。它通常用于数据去重，对于包含相同单词的相似输入，无论单词顺序如何，它都可以产生相同的输出结果。
fingerprint 分析器由以下组件组成：
standard 分词器 lowercase 分词过滤器 asciifolding 分词过滤器 stop 分词过滤器 fingerprint 分词过滤器 相关指南（先读这些） # 文本分析基础 文本分析：规范化 参数说明 # 指纹分词器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 separator 可选 字符串 在对词项进行词元生成、排序和去重后，用于连接这些词项的字符。默认值为一个空格（ ）。 max_output_size 可选 整数 定义输出词元的最大大小。如果连接后的指纹超过此大小，将被截断。默认值为 255。 stopwords 可选 字符串或字符串列表 自定义的停用词列表。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_custom_fingerprint_index 带有指纹分词器的索引：</description></item><item><title>中文分析器（Chinese）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/chinese-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/chinese-analyzer/</guid><description>Chinese 分析器 # chinese 分析器为中文文本设计，提供基础的中文处理能力。对于更复杂的中文分词需求，建议使用专门的中文分词插件如 IK、HanLP 或 Jieba。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
cjk 分词器：将 CJK（中日韩）字符分解为二元组（bigrams） lowercase 分词过滤器：转换为小写 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;chinese&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;快速的棕色狐狸跳过懒惰的狗&amp;#34; } 分析结果 # [ &amp;#34;快&amp;#34;, &amp;#34;速&amp;#34;, &amp;#34;的&amp;#34;, &amp;#34;棕&amp;#34;, &amp;#34;色&amp;#34;, &amp;#34;狐&amp;#34;, &amp;#34;狸&amp;#34;, &amp;#34;跳&amp;#34;, &amp;#34;过&amp;#34;, &amp;#34;懒&amp;#34;, &amp;#34;惰&amp;#34;, &amp;#34;的&amp;#34;, &amp;#34;狗&amp;#34; ] 推荐用法 # 对于生产环境的中文处理，建议使用专门的中文分词插件：
IK 分词器 - 中文分词的常用选择 HanLP 分词器 - 功能完整的中文 NLP 分词 Jieba 分词器 - 基于 Python Jieba 的分词 详见相关插件文档。</description></item><item><title>语言分析器（Language）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/language-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/language-analyzer/</guid><description>Language 分析器 # Easysearch 内置了 34 种语言专用分析器，每种都针对该语言的停用词、词干提取和字符归一化进行了优化。使用时只需将 analyzer 设置为对应的语言名称即可。
支持的语言列表 # 语言 分析器名称 特点 阿拉伯语 arabic 阿拉伯语归一化 + 词干提取 亚美尼亚语 armenian Snowball 词干提取 巴斯克语 basque Snowball 词干提取 孟加拉语 bengali 印度语系归一化 + 词干提取 巴西葡萄牙语 brazilian 巴西葡萄牙语词干提取 保加利亚语 bulgarian 保加利亚语词干提取 加泰罗尼亚语 catalan 省音处理 + Snowball 词干提取 捷克语 czech 捷克语词干提取 丹麦语 danish Snowball 词干提取 荷兰语 dutch 词干覆盖字典 + Snowball 英语 english 所有格处理 + Porter 词干提取 爱沙尼亚语 estonian Snowball 词干提取 芬兰语 finnish Snowball 词干提取 法语 french 省音处理 + 轻量词干提取 加利西亚语 galician 加利西亚语词干提取 德语 german 字符归一化 + 轻量词干提取 希腊语 greek 希腊语专用小写 + 词干提取 印地语 hindi 印度语系归一化 + 词干提取 匈牙利语 hungarian Snowball 词干提取 印度尼西亚语 indonesian 印度尼西亚语词干提取 爱尔兰语 irish 专用小写 + 双重停用词过滤 意大利语 italian 省音处理 + 轻量词干提取 拉脱维亚语 latvian 拉脱维亚语词干提取 立陶宛语 lithuanian Snowball 词干提取 挪威语 norwegian Snowball 词干提取 波斯语 persian 字符过滤 + 阿拉伯语/波斯语归一化 波兰语 polish 波兰语词干提取 葡萄牙语 portuguese 轻量词干提取 罗马尼亚语 romanian 字符归一化 + Snowball 词干提取 俄语 russian Snowball 词干提取 索拉尼语 sorani 索拉尼语归一化 + 词干提取 西班牙语 spanish 轻量词干提取 瑞典语 swedish Snowball 词干提取 泰语 thai Java BreakIterator 泰语分词 土耳其语 turkish 专用小写（İ/I）+ Snowball 使用方式 # 在映射中指定对应语言名称即可：</description></item><item><title>丹麦语分析器（Danish）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/danish-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/danish-analyzer/</guid><description>Danish 分析器 # danish 分析器是为丹麦语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤丹麦语停用词 snowball(Danish) 分词过滤器：丹麦语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;danish&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Hundene løber i parken&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _danish_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>亚美尼亚语分析器（Armenian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/armenian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/armenian-analyzer/</guid><description>Armenian 分析器 # armenian 分析器是为亚美尼亚语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤亚美尼亚语停用词 snowball(Armenian) 分词过滤器：亚美尼亚语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;armenian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Վիdelays հայերենի տեքdelays&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _armenian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>俄语分析器（Russian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/russian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/russian-analyzer/</guid><description>Russian 分析器 # russian 分析器是为俄语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤俄语停用词 snowball(Russian) 分词过滤器：俄语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;russian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Собаки бегают в парке&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _russian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>保加利亚语分析器（Bulgarian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/bulgarian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/bulgarian-analyzer/</guid><description>Bulgarian 分析器 # bulgarian 分析器是为保加利亚语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤保加利亚语停用词 bulgarian_stem 分词过滤器：保加利亚语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;bulgarian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Кучетата тичат в парка&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _bulgarian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>加利西亚语分析器（Galician）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/galician-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/galician-analyzer/</guid><description>Galician 分析器 # galician 分析器是为加利西亚语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤加利西亚语停用词 galician_stem 分词过滤器：加利西亚语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;galician&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Os cans corren no parque&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _galician_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>加泰罗尼亚语分析器（Catalan）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/catalan-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/catalan-analyzer/</guid><description>Catalan 分析器 # catalan 分析器是为加泰罗尼亚语文本特别设计的语言分析器，包含省音处理。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 elision 分词过滤器：移除加泰罗尼亚语省音符号 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤加泰罗尼亚语停用词 snowball(Catalan) 分词过滤器：加泰罗尼亚语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;catalan&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Els gossos corren al parc&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _catalan_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>匈牙利语分析器（Hungarian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hungarian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hungarian-analyzer/</guid><description>Hungarian 分析器 # hungarian 分析器是为匈牙利语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤匈牙利语停用词 snowball(Hungarian) 分词过滤器：匈牙利语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;hungarian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;A kutyák futnak a parkban&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _hungarian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>印地语分析器（Hindi）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hindi-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hindi-analyzer/</guid><description>Hindi 分析器 # hindi 分析器是为印地语文本特别设计的语言分析器，包含印度语系归一化处理。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 indic_normalization 分词过滤器：印度语系字符归一化 hindi_normalization 分词过滤器：印地语字符归一化 stop 分词过滤器：过滤印地语停用词 hindi_stem 分词过滤器：印地语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;hindi&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;कुत्ते पार्क में दौड़ रहे हैं&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _hindi_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>印度尼西亚语分析器（Indonesian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/indonesian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/indonesian-analyzer/</guid><description>Indonesian 分析器 # indonesian 分析器是为印度尼西亚语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤印度尼西亚语停用词 indonesian_stem 分词过滤器：印度尼西亚语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;indonesian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Anjing-anjing berlari di taman&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _indonesian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>土耳其语分析器（Turkish）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/turkish-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/turkish-analyzer/</guid><description>Turkish 分析器 # turkish 分析器是为土耳其语文本特别设计的语言分析器，使用土耳其语专用的小写转换。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 apostrophe 分词过滤器：移除撇号及其后字符 turkish_lowercase 分词过滤器：土耳其语专用小写转换（正确处理 İ/I） stop 分词过滤器：过滤土耳其语停用词 snowball(Turkish) 分词过滤器：土耳其语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;turkish&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Köpekler parkta koşuyor&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _turkish_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>孟加拉语分析器（Bengali）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/bengali-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/bengali-analyzer/</guid><description>Bengali 分析器 # bengali 分析器是为孟加拉语文本特别设计的语言分析器，包含印度语系归一化处理。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 indic_normalization 分词过滤器：印度语系字符归一化 bengali_normalization 分词过滤器：孟加拉语字符归一化 stop 分词过滤器：过滤孟加拉语停用词 bengali_stem 分词过滤器：孟加拉语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;bengali&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;বাংলা ভাষার বিশ্লেষণ&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _bengali_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>巴斯克语分析器（Basque）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/basque-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/basque-analyzer/</guid><description>Basque 分析器 # basque 分析器是为巴斯克语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤巴斯克语停用词 snowball(Basque) 分词过滤器：巴斯克语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;basque&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Euskara hizkuntza da&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _basque_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>巴西葡萄牙语分析器（Brazilian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/brazilian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/brazilian-analyzer/</guid><description>Brazilian 分析器 # brazilian 分析器是为巴西葡萄牙语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤巴西葡萄牙语停用词 brazilian_stem 分词过滤器：巴西葡萄牙语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;brazilian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Os cachorros correm no parque&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _brazilian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>希腊语分析器（Greek）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/greek-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/greek-analyzer/</guid><description>Greek 分析器 # greek 分析器是为希腊语文本特别设计的语言分析器，使用专用的希腊语小写转换。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 greek_lowercase 分词过滤器：希腊语专用小写转换 stop 分词过滤器：过滤希腊语停用词 greek_stem 分词过滤器：希腊语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;greek&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Τα σκυλιά τρέχουν στο πάρκο&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _greek_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>德语分析器（German）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/german-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/german-analyzer/</guid><description>German 分析器 # german 分析器是为德语文本特别设计的语言分析器，包含德语归一化和轻量词干提取。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤德语停用词 german_normalization 分词过滤器：德语字符归一化（ä→a, ö→o, ü→u, ß→ss） german_light_stem 分词过滤器：德语轻量词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;german&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Die Hunde laufen im Park&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _german_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>意大利语分析器（Italian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/italian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/italian-analyzer/</guid><description>Italian 分析器 # italian 分析器是为意大利语文本特别设计的语言分析器，包含省音处理。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 elision 分词过滤器：移除意大利语省音符号 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤意大利语停用词 italian_light_stem 分词过滤器：意大利语轻量词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;italian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;I cani corrono nel parco&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _italian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>拉脱维亚语分析器（Latvian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/latvian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/latvian-analyzer/</guid><description>Latvian 分析器 # latvian 分析器是为拉脱维亚语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤拉脱维亚语停用词 latvian_stem 分词过滤器：拉脱维亚语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;latvian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Suņi skrien parkā&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _latvian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>挪威语分析器（Norwegian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/norwegian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/norwegian-analyzer/</guid><description>Norwegian 分析器 # norwegian 分析器是为挪威语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤挪威语停用词 snowball(Norwegian) 分词过滤器：挪威语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;norwegian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Hundene løper i parken&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _norwegian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>捷克语分析器（Czech）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/czech-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/czech-analyzer/</guid><description>Czech 分析器 # czech 分析器是为捷克语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤捷克语停用词 czech_stem 分词过滤器：捷克语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;czech&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Psi běží v parku&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _czech_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>日语分析器（Japanese）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/japanese-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/japanese-analyzer/</guid><description>Japanese 分析器 # japanese 分析器是为日语文本设计的基础语言分析器，使用 CJK 二元组分词方式。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
cjk 分词器：将 CJK（中日韩）字符分解为二元组（bigrams） lowercase 分词过滤器：转换为小写 cjk_width 分词过滤器：全角/半角字符归一化 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;japanese&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;東京都の天気は晴れです&amp;#34; } 相关指南 # 语言分析器 文本分析基础</description></item><item><title>法语分析器（French）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/french-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/french-analyzer/</guid><description>French 分析器 # french 分析器是为法语文本特别设计的语言分析器，包含省音处理。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 elision 分词过滤器：移除法语省音符号（l', d', qu' 等） lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤法语停用词 french_light_stem 分词过滤器：法语轻量词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;french&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Les chiens courent dans le parc&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _french_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>波兰语分析器（Polish）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/polish-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/polish-analyzer/</guid><description>Polish 分析器 # polish 分析器是为波兰语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤波兰语停用词 polish_stem 分词过滤器：波兰语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;polish&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Psy biegają w parku&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _polish_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>波斯语分析器（Persian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/persian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/persian-analyzer/</guid><description>Persian 分析器 # persian 分析器是为波斯语文本特别设计的语言分析器，包含阿拉伯语归一化和波斯语字符过滤。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
persian_char_filter 字符过滤器：将零宽非连接符替换为空格 standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 arabic_normalization 分词过滤器：阿拉伯语字符归一化 persian_normalization 分词过滤器：波斯语字符归一化 stop 分词过滤器：过滤波斯语停用词 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;persian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;سگ‌ها در پارک می‌دوند&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _persian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>泰语分析器（Thai）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/thai-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/thai-analyzer/</guid><description>Thai 分析器 # thai 分析器是为泰语文本特别设计的语言分析器，使用 Java 内置的泰语分词算法。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
thai 分词器：使用 Java BreakIterator 进行泰语分词 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 stop 分词过滤器：过滤泰语停用词 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;thai&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;สุนัขวิ่งในสวน&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _thai_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>爱尔兰语分析器（Irish）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/irish-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/irish-analyzer/</guid><description>Irish 分析器 # irish 分析器是为爱尔兰语文本特别设计的语言分析器，使用爱尔兰语专用的小写转换和双重停用词过滤。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 stop(hyphenations) 分词过滤器：移除连字符 elision 分词过滤器：移除爱尔兰语省音符号 irish_lowercase 分词过滤器：爱尔兰语专用小写转换 stop 分词过滤器：过滤爱尔兰语停用词 snowball(Irish) 分词过滤器：爱尔兰语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;irish&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Rithfidh na madraí sa pháirc&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _irish_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>爱沙尼亚语分析器（Estonian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/estonian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/estonian-analyzer/</guid><description>Estonian 分析器 # estonian 分析器是为爱沙尼亚语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤爱沙尼亚语停用词 snowball(Estonian) 分词过滤器：爱沙尼亚语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;estonian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Koerad jooksevad pargis&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _estonian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>瑞典语分析器（Swedish）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/swedish-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/swedish-analyzer/</guid><description>Swedish 分析器 # swedish 分析器是为瑞典语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤瑞典语停用词 snowball(Swedish) 分词过滤器：瑞典语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;swedish&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Hundarna springer i parken&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _swedish_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>立陶宛语分析器（Lithuanian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/lithuanian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/lithuanian-analyzer/</guid><description>Lithuanian 分析器 # lithuanian 分析器是为立陶宛语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤立陶宛语停用词 snowball(Lithuanian) 分词过滤器：立陶宛语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;lithuanian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Šunys bėga parke&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _lithuanian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>索拉尼语分析器（Sorani）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/sorani-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/sorani-analyzer/</guid><description>Sorani 分析器 # sorani 分析器是为索拉尼库尔德语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 sorani_normalization 分词过滤器：索拉尼语字符归一化 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 stop 分词过滤器：过滤索拉尼语停用词 sorani_stem 分词过滤器：索拉尼语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;sorani&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;سەگەکان لە پارکەکەدا ڕادەکەن&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _sorani_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>罗马尼亚语分析器（Romanian）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/romanian-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/romanian-analyzer/</guid><description>Romanian 分析器 # romanian 分析器是为罗马尼亚语文本特别设计的语言分析器，包含罗马尼亚语字符归一化。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤罗马尼亚语停用词 romanian_normalization 分词过滤器：罗马尼亚语字符归一化 snowball(Romanian) 分词过滤器：罗马尼亚语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;romanian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Câinii aleargă în parc&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _romanian_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>芬兰语分析器（Finnish）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/finnish-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/finnish-analyzer/</guid><description>Finnish 分析器 # finnish 分析器是为芬兰语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤芬兰语停用词 snowball(Finnish) 分词过滤器：芬兰语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;finnish&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Koirat juoksevat puistossa&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _finnish_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>英语分析器（English）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/english-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/english-analyzer/</guid><description>English 分析器 # english 分析器是为英语文本特别设计的语言分析器，包含了英语特定的处理规则和停用词。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤英语停用词（the, a, an, and 等） porter_stem 分词过滤器：英语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;english&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;The quick brown foxes jumping over the lazy dogs&amp;#34; } 分析结果 # 停用词（the, over 等）被移除，词根被提取：
[ &amp;#34;quick&amp;#34;, &amp;#34;brown&amp;#34;, &amp;#34;fox&amp;#34;, &amp;#34;jump&amp;#34;, &amp;#34;lazi&amp;#34;, &amp;#34;dog&amp;#34; ] 相关指南 # 文本分析：词干提取 文本分析基础</description></item><item><title>荷兰语分析器（Dutch）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/dutch-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/dutch-analyzer/</guid><description>Dutch 分析器 # dutch 分析器是为荷兰语文本特别设计的语言分析器，包含词干覆盖字典。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤荷兰语停用词 stemmer_override 分词过滤器：应用荷兰语词干覆盖字典 snowball(Dutch) 分词过滤器：荷兰语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;dutch&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;De honden rennen in het park&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _dutch_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>葡萄牙语分析器（Portuguese）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/portuguese-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/portuguese-analyzer/</guid><description>Portuguese 分析器 # portuguese 分析器是为葡萄牙语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤葡萄牙语停用词 portuguese_light_stem 分词过滤器：葡萄牙语轻量词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;portuguese&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Os cães correm no parque&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _portuguese_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>西班牙语分析器（Spanish）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/spanish-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/spanish-analyzer/</guid><description>Spanish 分析器 # spanish 分析器是为西班牙语文本特别设计的语言分析器。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 stop 分词过滤器：过滤西班牙语停用词 spanish_light_stem 分词过滤器：西班牙语轻量词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;spanish&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Los perros corren en el parque&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _spanish_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>阿拉伯语分析器（Arabic）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/arabic-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/arabic-analyzer/</guid><description>Arabic 分析器 # arabic 分析器是为阿拉伯语文本特别设计的语言分析器，包含了阿拉伯语特有的归一化和词干提取规则。
分析器组成 # 该分析器由以下分词器和分词过滤器组成：
standard 分词器：标准的文本分割 lowercase 分词过滤器：转换为小写 decimal_digit 分词过滤器：将各种 Unicode 数字转换为 ASCII 数字 stop 分词过滤器：过滤阿拉伯语停用词 arabic_normalization 分词过滤器：阿拉伯语字符归一化 arabic_stem 分词过滤器：阿拉伯语词干提取 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;arabic&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;الكلاب تركض في الحديقة&amp;#34; } 自定义配置 # 可通过以下参数自定义该分析器：
参数 说明 stopwords 自定义停用词列表，默认 _arabic_ stem_exclusion 不进行词干提取的词语列表 相关指南 # 语言分析器 文本分析基础</description></item><item><title>IK 中文分析器（IK）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/ik-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/ik-analyzer/</guid><description>IK 分析器 # IK 分析器是一款专为处理中文文本设计的分析器，高效且智能。支持 ik_smart 和 ik_max_word 两种分词模式。
相关指南（先读这些） # 文本分析基础 文本分析：识别词元 IK 分词器安装 # IK 分词插件安装命令如下：
bin/easysearch-plugin install analysis-ik 同时也需要安装 ingest-common 插件：
bin/easysearch-plugin install ingest-common 如果觉得比较麻烦，也可以直接使用 Easysearch 的 bundle 包安装部署。
使用样例 # 下面的命令样例展示了 IK 的使用方式。
# 1.创建索引 PUT index_ik # 2.创建映射关系 POST index_ik/_mapping { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;ik_max_word&amp;#34;, &amp;#34;search_analyzer&amp;#34;: &amp;#34;ik_smart&amp;#34; } } } # 3.写入文档 POST index_ik/_create/1 {&amp;#34;content&amp;#34;:&amp;#34;美国留给伊拉克的是个烂摊子吗&amp;#34;} POST index_ik/_create/2 {&amp;#34;content&amp;#34;:&amp;#34;公安部：各地校车将享最高路权&amp;#34;} POST index_ik/_create/3 {&amp;#34;content&amp;#34;:&amp;#34;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&amp;#34;} POST index_ik/_create/4 {&amp;#34;content&amp;#34;:&amp;#34;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&amp;#34;} # 4.</description></item><item><title>IK 智能分析器（IK Smart）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/ik-smart-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/ik-smart-analyzer/</guid><description>IK Smart 分析器 # ik_smart 分析器是为中文智能分词的分析器，使用 IK 分词器的智能模式，会尽量将文本切分为最少的词语。
需要安装 analysis-ik 插件。 分析器组成 # 该分析器由以下分词器和分词过滤器组成：
ik_smart 分词器：使用 IK 智能分词模式，倾向于切分出较长的词语 lowercase 分词过滤器：转换为小写 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;ik_smart&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;中华人民共和国国歌&amp;#34; } 相关指南 # 文本分析基础</description></item><item><title>拼音分析器（Pinyin）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/pinyin-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/pinyin-analyzer/</guid><description>Pinyin 分析器 # pinyin 分析器能够在索引阶段将中文字符实时转写为拼音，并在检索阶段对拼音、汉字及混输查询进行统一匹配。借助该插件，你可以轻松实现：
相关指南（先读这些） # 文本分析基础
文本分析：识别词元
支持 全拼、首字母、全拼拼接 等多种检索方式；
保留非中文字符，实现「中英混输」搜索；
借助 token filter 在分词链中灵活组合不同策略；
在联想输入、排序、聚合等场景下提升中文用户体验。
适用于人名、地名、品牌、歌曲、商品等多种中文文本检索场景。
参数说明 # 下表整理了 pinyin 分词器 / 过滤器支持的全部可选参数及默认值。
参数 说明 默认值 keep_first_letter 仅保留每个汉字的拼音首字母。例如 刘德华 → ldh true keep_separate_first_letter 将首字母分开存储，提高命中率。例如 刘德华 → l, d, h，注意：这可能会由于词频增加而增加查询模糊性。 false limit_first_letter_length 首字母结果最长长度 16 keep_full_pinyin 保留每个汉字的全拼。如 刘德华 → liu, de, hua true keep_joined_full_pinyin 将全拼连接在一起。如 刘德华 → liudehua false keep_none_chinese 保留非中文字符（数字/字母等） true keep_none_chinese_together 将连续非中文字符作为整体保留。例如， DJ 音乐家 变成 DJ 、 yin 、 yue 、 jia 。当设置为 false 时， DJ 音乐家 变成 D 、 J 、 yin 、 yue 、 jia 。注意： keep_none_chinese 应该先启用。 true keep_none_chinese_in_first_letter 在首字母结果中保留非中文字。例如， 刘德华 AT2016 变成 ldhat2016 。符 true keep_none_chinese_in_joined_full_pinyin 在拼接全部拼音时保留非中文字。例如， 刘德华 2016 变成 liudehua2016 。符 false none_chinese_pinyin_tokenize 将非中文字符中可能的拼音继续拆分例如， liudehuaalibaba13zhuanghan 变成 liu 、 de 、 hua 、 a 、 li 、 ba 、 ba 、 13 、 zhuang 、 han 。注意： keep_none_chinese 和 keep_none_chinese_together 应该先启用。 true keep_original 同时保留原始文本 false lowercase 对非中文字符强制小写 true trim_whitespace 去除首尾空格 true remove_duplicated_term 开启时，移除重复术语以节省索引空间。例如， de 的 变为 de 。注意：位置相关的查询可能会受影响。积 false ignore_pinyin_offset 忽略 offset 以允许重叠 token。在 6.</description></item><item><title>简繁转换分析器（STConvert）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/stconvert-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/stconvert-analyzer/</guid><description>STConvert 分析器 # stconvert 分析器可在索引与查询阶段将简体中文与繁体中文之间进行双向转换，解决两种文字体系混合检索的问题。
相关指南（先读这些） # 文本分析基础 文本分析：规范化 参数说明 # 参数 说明 默认值 convert_type 转换方向，可选：s2t（简 → 繁），t2s（繁 → 简） s2t keep_both 是否同时保留转换前后两种 token false delimiter 当 keep_both=true 时，两种 token 之间的分隔符 , 使用介绍 # 映射创建
PUT /stconvert/ { &amp;#34;settings&amp;#34; : { &amp;#34;analysis&amp;#34; : { &amp;#34;analyzer&amp;#34; : { &amp;#34;tsconvert&amp;#34; : { &amp;#34;tokenizer&amp;#34; : &amp;#34;tsconvert&amp;#34; } }, &amp;#34;tokenizer&amp;#34; : { &amp;#34;tsconvert&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;stconvert&amp;#34;, &amp;#34;delimiter&amp;#34; : &amp;#34;#&amp;#34;, &amp;#34;keep_both&amp;#34; : false, &amp;#34;convert_type&amp;#34; : &amp;#34;t2s&amp;#34; } }, &amp;#34;filter&amp;#34;: { &amp;#34;tsconvert&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;stconvert&amp;#34;, &amp;#34;delimiter&amp;#34; : &amp;#34;#&amp;#34;, &amp;#34;keep_both&amp;#34; : false, &amp;#34;convert_type&amp;#34; : &amp;#34;t2s&amp;#34; } }, &amp;#34;char_filter&amp;#34; : { &amp;#34;tsconvert&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;stconvert&amp;#34;, &amp;#34;convert_type&amp;#34; : &amp;#34;t2s&amp;#34; } } } } } 分词测试</description></item><item><title>HanLP CRF 分析器（HanLP CRF）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-crf-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-crf-analyzer/</guid><description>HanLP CRF 分析器 # hanlp_crf 分析器是为中文 CRF 分词的分析器，使用 HanLP CRF（条件随机场）模型进行分词，适合学术研究场景。
需要安装 analysis-hanlp 插件。 分析器组成 # 该分析器由以下分词器和分词过滤器组成：
hanlp_crf 分词器：使用 HanLP CRF 条件随机场模型进行分词 lowercase 分词过滤器：转换为小写 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;hanlp_crf&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;中国科学院计算技术研究所&amp;#34; } 相关指南 # 文本分析基础</description></item><item><title>HanLP NLP 分析器（HanLP NLP）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-nlp-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-nlp-analyzer/</guid><description>HanLP NLP 分析器 # hanlp_nlp 分析器是为中文 NLP 分词的分析器，使用 HanLP NLP 模式，支持命名实体识别，适合需要高精度语义分析的场景。
需要安装 analysis-hanlp 插件。 分析器组成 # 该分析器由以下分词器和分词过滤器组成：
hanlp_nlp 分词器：使用 HanLP NLP 模式，支持命名实体识别 lowercase 分词过滤器：转换为小写 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;hanlp_nlp&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;刘德华和张学友是好朋友&amp;#34; } 相关指南 # 文本分析基础</description></item><item><title>HanLP 标准分析器（HanLP Standard）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-standard-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-standard-analyzer/</guid><description>HanLP Standard 分析器 # hanlp_standard 分析器是为中文标准分词的分析器，使用 HanLP 标准分词模式，适合大多数通用中文搜索场景。
需要安装 analysis-hanlp 插件。 分析器组成 # 该分析器由以下分词器和分词过滤器组成：
hanlp_standard 分词器：使用 HanLP 标准分词模式 lowercase 分词过滤器：转换为小写 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;hanlp_standard&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;中华人民共和国国歌&amp;#34; } 相关指南 # 文本分析基础</description></item><item><title>HanLP 索引分析器（HanLP Index）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-index-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/hanlp-index-analyzer/</guid><description>HanLP Index 分析器 # hanlp_index 分析器是为中文索引分词的分析器，使用 HanLP 索引分词模式，会对文本进行更细粒度的切分。
需要安装 analysis-hanlp 插件。 分析器组成 # 该分析器由以下分词器和分词过滤器组成：
hanlp_index 分词器：使用 HanLP 索引分词模式，对文本进行细粒度切分 lowercase 分词过滤器：转换为小写 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;hanlp_index&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;中华人民共和国国歌&amp;#34; } 相关指南 # 文本分析基础</description></item><item><title>ICU 分析器（ICU）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/icu-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/icu-analyzer/</guid><description>ICU 分析器 # icu 分析器是为多语言文本分析的分析器，基于 ICU（International Components for Unicode）实现，对亚洲语言混合文本提供比标准分析器更好的分词效果。
需要安装 analysis-icu 插件。 分析器组成 # 该分析器由以下分词器和分词过滤器组成：
icu_tokenizer 分词器：使用 ICU Unicode 文本分割算法 icu_normalizer 分词过滤器：Unicode 归一化（NFC 模式） 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;icu&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Elasticsearch の全文検索エンジン&amp;#34; } 相关指南 # 文本分析基础</description></item><item><title>Jieba 搜索分析器（Jieba Search）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/jieba-search-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/jieba-search-analyzer/</guid><description>Jieba Search 分析器 # jieba_search 分析器是为中文搜索分词的分析器，使用 Jieba 分词器的搜索模式，在精确模式的基础上对长词再次切分。
需要安装 analysis-jieba 插件。 分析器组成 # 该分析器由以下分词器和分词过滤器组成：
jieba_search 分词器：使用 Jieba 搜索模式，对长词进行二次切分以提高召回率 lowercase 分词过滤器：转换为小写 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;jieba_search&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;小明硕士毕业于中国科学院计算所&amp;#34; } 相关指南 # 文本分析基础</description></item><item><title>Jieba 索引分析器（Jieba Index）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/jieba-index-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/jieba-index-analyzer/</guid><description>Jieba Index 分析器 # jieba_index 分析器是为中文索引分词的分析器，使用 Jieba 分词器的索引模式。
需要安装 analysis-jieba 插件。 分析器组成 # 该分析器由以下分词器和分词过滤器组成：
jieba_index 分词器：使用 Jieba 索引模式，适合索引时的细粒度分词 lowercase 分词过滤器：转换为小写 示例 # POST _analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;jieba_index&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;小明硕士毕业于中国科学院计算所&amp;#34; } 相关指南 # 文本分析基础</description></item><item><title>俄语形态分析器（Russian Morphology）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/russian-morphology-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/russian-morphology-analyzer/</guid><description>Russian Morphology 分析器 # russian_morphology 分析器专为处理俄语文本而设计。与 standard 分析器不同，它能够识别俄语词汇的形态变化，将单词还原为其词干或原型（Lemmatization）。这使得用户在搜索某个单词的特定形式（如单复数、格的变化）时，能够匹配到该单词的其他形态。
该分析器由以下分词器和分词过滤器组成：
standard 分词器：去除大部分标点符号，并依据空格和其他常见分隔符对文本进行分割。 lowercase 分词过滤器：将所有词元转换为小写，以确保匹配时不区分大小写。 russian_morphology 分词过滤器：执行俄语词汇的形态分析，将不同格、性、数的单词映射到统一原型。 相关指南（先读这些） # 文本分析：词干提取 文本分析基础 安装 # 俄语形态分词器包含在Morphological Analysis插件中。此插件已包含在Easysearch的bundle包中。
analysis-morphology插件安装命令如下：
bin/easysearch-plugin install analysis-morphology 参考样例 # 以下命令创建一个名为 my_morphology_index 的索引，并为 my_field 字段配置俄语形态分词器的索引：
PUT /my_morphology_index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;russian_morphology&amp;#34; } } } } 配置自定义分词器 # 在生产环境中，为了兼顾性能和准确度，建议定义一个包含小写化、形态还原和停用词过滤的自定义分析器：
PUT /my_custom_morphology_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;russian_morphology&amp;#34;, &amp;#34;english_morphology&amp;#34;, &amp;#34;my_stopwords&amp;#34; ] } }, &amp;#34;filter&amp;#34;: { &amp;#34;my_stopwords&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stop&amp;#34;, &amp;#34;stopwords&amp;#34;: &amp;#34;_russian_&amp;#34; } } } } } 产生的词元 # 通过形态分析，不同的词形会被索引为相同的词元。</description></item><item><title>英语形态分析器（English Morphology）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/english-morphology-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/english-morphology-analyzer/</guid><description>English Morphology 分析器 # english_morphology 分析器专为处理复杂的英语文本而设计。与仅执行简单算法剪裁的常规分析器不同，它基于词形还原（Lemmatization）技术，能够精准识别英语词汇的形态变化，并将其还原为词典中的标准原型。
这确保了用户在搜索单词的不同形态（如动词时态 ran/running、名词单复数 foxes/fox、或不规则变化 feet/foot）时，能够实现精准的跨形态匹配。
该分析器由以下分词器和分词过滤器组成：
standard 分词器：去除大部分标点符号，并依据空格和其他常见分隔符对文本进行分割。 lowercase 分词过滤器：将所有词元转换为小写，以确保匹配时不区分大小写。 english_morphology 分词过滤器：执行英语词汇的形态分析，将动词的时态、形容词的比较级以及名词的复数形式映射到其唯一的语义原型。 相关指南（先读这些） # 文本分析：词干提取 文本分析基础 安装 # 英语形态分词器包含在Morphological Analysis插件中。此插件已包含在Easysearch的bundle包中。
analysis-morphology插件安装命令如下：
bin/easysearch-plugin install analysis-morphology 参考样例 # 以下命令创建一个名为 my_morphology_index 的索引，并为 my_field 字段配置俄语形态分词器的索引：
PUT /my_morphology_index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;english_morphology&amp;#34; } } } } 配置自定义分词器 # 在生产环境中，为了兼顾性能和准确度，建议定义一个包含小写化、形态还原和停用词过滤的自定义分析器：
PUT /my_custom_morphology_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;english_morphology&amp;#34;, &amp;#34;stop&amp;#34; ] } } } } } 产生的词元 # 通过形态分析，不同的词形会被索引为相同的词元。</description></item><item><title>创建自定义分析器（Custom）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/creating-a-custom-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/creating-a-custom-analyzer/</guid><description>创建一个自定义分词器 # 要创建一个自定义分词器，需要指定以下组成内容：
字符过滤器（零个或多个） 词元生成器（一个） 词元过滤器（零个或多个） 相关配置 # 以下参数可用于配置自定义分词器。
参数 必填/可选 描述 type 可选 分词器类型。默认值为custom。你也可以给这个参数指定一个预构建的分词器。 tokenizer 必填 每个分词器必须要有一个词元生成器。 char_filter 可选 要包含在分词器中的字符过滤器列表。 filter 可选 要包含在分词器中的分词过滤器列表。 position_increment_gap 可选 在对包含多个词项内容的文本字段建立索引时，应用于各词项之间的额外间隔。有关更多信息，请参阅位置间隔增量。默认值为 100。 参考样例 # 以下示例展示了各种自定义分词器的配置。
自定义分词器用于去除 HTML 格式标签 # 以下示例中的分词器在分词之前会从文本中移除 HTML 格式标签：
PUT simple_html_strip_analyzer_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;html_strip_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;char_filter&amp;#34;: [&amp;#34;html_strip&amp;#34;], &amp;#34;tokenizer&amp;#34;: &amp;#34;whitespace&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;] } } } } } 使用以下请求来查看使用该分词器生成的词元：</description></item><item><title>索引分析器（Index Analyzer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/index-analyzers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/index-analyzers/</guid><description>索引分词器 # 索引分词器是在索引创建时指定的，在文档写入时对文本字段进行分词。
写入索引分词器的生效流程 # 为了确定在对文档建立索引时对某个字段使用哪种分词器，Easysearch 会按以下顺序检查以下参数：
字段的分词器映射参数，即analyzer analysis.analyzer.default 索引设置 standard标准分词器（默认分词器） 在指定索引分词器时，请记住，在大多数情况下，为索引中的每个文本字段指定一个分词器效果最佳。使用相同的分词器在文本字段（在建立索引时）和查询字符串（在查询时），可确保搜索所使用的词项与存储在索引中的词项相同。
为字段指定索引分词器 # 在创建索引映射时，可以为每个文本字段提供 analyzer（分词器）参数。例如，以下请求为 text_entry 字段指定了 simple 简单分词器：
PUT testindex { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;text_entry&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;simple&amp;#34; } } } } 为索引指定默认索引分词器 # 如果想对索引中的所有文本字段使用相同的分词器，则可以在索引设置中按如下方式进行指定analysis.analyzer.default：
PUT testindex { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;default&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;simple&amp;#34; } } } } } 如果您未指定默认分词器，那么将使用standard标准分词器。</description></item><item><title>搜索分析器（Search Analyzer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/search-analyzers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/analyzers/search-analyzers/</guid><description>搜索分词器 # 搜索分词器是在查询时指定的，当你对text字段进行全文匹配查询时，它会被用于分析被查询字符串。
搜索分词器的生效流程 # 在查询时确定对查询字符串使用哪种分词器，Easysearch 会按以下顺序检查以下参数：
查询的 analyzer 参数 字段的 search_analyzer 映射参数 索引设置 analysis.analyzer.default_search 字段的 analyzer 映射参数 standard 分词器（默认分词器） 在大多数情况下，没有必要指定与索引分词器不同的搜索分词器，而且这样做可能会对搜索结果的相关性产生负面影响，或者导致出现意想不到的搜索结果。
为查询内容指定搜索分词器 # 在查询时，你可以在 analyzer 字段中指定想要使用的分词器：
GET shakespeare/_search { &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;text_entry&amp;#34;: { &amp;#34;query&amp;#34;: &amp;#34;speak the truth&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;english&amp;#34; } } } } 为字段指定搜索分词器 # 在创建索引映射时，你可以为每个文本字段提供 search_analyzer 参数。在提供 search_analyzer 时，你还必须提供 analyzer 参数，该参数指定在索引创建时的写入索引分词器内应用。
例如，以下请求将 simple 分词器指定为 text_entry 字段的索引分词器，将 whitespace 分词器指定为该字段的搜索分词器：
PUT testindex { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;text_entry&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;simple&amp;#34;, &amp;#34;search_analyzer&amp;#34;: &amp;#34;whitespace&amp;#34; } } } } 为索引指定默认的搜索分词器 # 如果你希望在搜索时使用同一个分词器来解析所有的查询，你可以在 analysis.</description></item></channel></rss>