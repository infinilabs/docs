<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>词元过滤器（Token Filters） on INFINI Easysearch (main)</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/</link><description>Recent content in 词元过滤器（Token Filters） on INFINI Easysearch (main)</description><generator>Hugo -- gohugo.io</generator><atom:link href="/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/index.xml" rel="self" type="application/rss+xml"/><item><title>小写分词过滤器（Lowercase）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/lowercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/lowercase/</guid><description>Lowercase 分词过滤器 # lowercase 分词过滤器用于将分词流中的所有字符转换为小写，从而使搜索不区分大小写。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参数 # 小写分词过滤器可以使用以下参数进行配置。
参数 必填/可选 描述 language 可选 指定一个特定语言的分词过滤器。有效值为：
- 希腊语 greek
- 爱尔兰语irish
- 土耳其语turkish。
默认值是 Lucene 的小写过滤器。 参考样例 # 以下示例请求创建了一个名为 custom_lowercase_example 的新索引。它配置了一个带有小写过滤器的分析器，并指定语言为希腊语。
PUT /custom_lowercase_example { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;greek_lowercase_example&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;greek_lowercase&amp;#34;] } }, &amp;#34;filter&amp;#34;: { &amp;#34;greek_lowercase&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;lowercase&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;greek&amp;#34; } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>大写分词过滤器（Uppercase）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/uppercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/uppercase/</guid><description>Uppercase 分词过滤器 # uppercase 分词过滤器用于在分析过程中将所有词元（单词）转换为大写形式。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参考样例 # 以下示例请求创建了一个名为 uppercase_example 的新索引，并配置了一个带有大写过滤器的分词器。
PUT /uppercase_example { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;uppercase_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;uppercase&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;uppercase_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;uppercase_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
GET /uppercase_example/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;uppercase_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Easysearch is powerful&amp;#34; } 返回内容包含产生的词元
{ &amp;#34;tokens&amp;#34;: [ { &amp;#34;token&amp;#34;: &amp;#34;EASYSEARCH&amp;#34;, &amp;#34;start_offset&amp;#34;: 0, &amp;#34;end_offset&amp;#34;: 10, &amp;#34;type&amp;#34;: &amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;, &amp;#34;position&amp;#34;: 0 }, { &amp;#34;token&amp;#34;: &amp;#34;IS&amp;#34;, &amp;#34;start_offset&amp;#34;: 11, &amp;#34;end_offset&amp;#34;: 13, &amp;#34;type&amp;#34;: &amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;, &amp;#34;position&amp;#34;: 1 }, { &amp;#34;token&amp;#34;: &amp;#34;POWERFUL&amp;#34;, &amp;#34;start_offset&amp;#34;: 14, &amp;#34;end_offset&amp;#34;: 22, &amp;#34;type&amp;#34;: &amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;, &amp;#34;position&amp;#34;: 2 } ] }</description></item><item><title>ASCII 折叠分词过滤器（ASCII Folding）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/ascii-folding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/ascii-folding/</guid><description>ASCII Folding 分词过滤器 # asciifolding 分词过滤器将非 ASCII 字符转换为与其最接近的 ASCII 等效字符。例如，&amp;ldquo;é&amp;rdquo; 变为 &amp;ldquo;e&amp;rdquo;，&amp;ldquo;ü&amp;rdquo; 变为 &amp;ldquo;u&amp;rdquo;，&amp;ldquo;ñ&amp;rdquo; 变为 &amp;ldquo;n&amp;rdquo;。这个过程被称为&amp;quot;音译&amp;quot;。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 ASCII 折叠分词过滤器有许多优点：
增强搜索灵活性：用户在输入查询内容时常常会省略重音符号或特殊字符。ASCII 折叠分词过滤器可确保即使是这样的查询也仍然能返回相关结果。 规范化：通过确保重音字符始终被转换为其 ASCII 等效字符，使索引编制过程标准化。 国际化：对于包含多种语言和字符集的应用程序特别有用。 尽管 ASCII 折叠分词过滤器可以简化搜索，但它也可能导致特定信息的丢失，尤其是当数据集中重音字符和非重音字符之间的区别很重要的时候。
参数说明 # 你可以使用 preserve_original 参数来配置 ASCII 折叠分词过滤器。将此参数设置为 true 时，会在词元流中同时保留原始词元及其 ASCII 折叠后的版本。当你希望在搜索查询中同时匹配一个词项的原始版本（带重音符号）和规范化版本（不带重音符号）时，这一点会特别有用。该参数的默认值为 false。
参考样例 # 以下示例请求创建了一个名为 example_index 的新索引，并定义了一个分词器，该分词器使用了 ASCII 折叠过滤器，且将 preserve_original 参数设置为 true：
PUT /example_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;custom_ascii_folding&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;asciifolding&amp;#34;, &amp;#34;preserve_original&amp;#34;: true } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;custom_ascii_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;custom_ascii_folding&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>十进制数字分词过滤器（Decimal Digit）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/decimal-digit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/decimal-digit/</guid><description>Decimal Digit 分词过滤器 # decimal_digit 分词过滤器用于将各种字符集中的十进制数字字符（0 到 9）规范化为它们对应的 ASCII 字符。当你希望在文本分析中确保所有数字都能被统一处理，而不管这些数字是以何种字符集书写时，这个过滤器就非常有用。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有十进制数字过滤器的分词器：
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;my_decimal_digit_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;decimal_digit&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;my_decimal_digit_filter&amp;#34;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;my_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;123 ١٢٣ १२३&amp;#34; } text分词：</description></item><item><title>Porter 词干分词过滤器（Porter Stem）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/porter-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/porter-stem/</guid><description>Porter Stem 分词过滤器 # porter_stem 分词过滤器会将单词还原为其基本（或词干）形式，并去除单词中常见的后缀，这有助于通过单词的词根来匹配相似的单词。例如，单词&amp;quot;running&amp;quot;会被词干提取为&amp;quot;run&amp;quot;。此分词过滤器主要用于英语，并基于 波特词干提取算法进行词干提取操作。
相关指南（先读这些） # 文本分析：词干提取 文本分析：规范化 参考样例 # 以下示例请求创建了一个名为 my_stem_index 的新索引，并配置了一个带有波特词干提取过滤器的分词器。
PUT /my_stem_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;my_porter_stem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;porter_stem&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;porter_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;my_porter_stem&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_stem_index/_analyze { &amp;#34;text&amp;#34;: &amp;#34;running runners ran&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;porter_analyzer&amp;#34; } 返回内容包含产生的词元
{ &amp;#34;tokens&amp;#34;: [ { &amp;#34;token&amp;#34;: &amp;#34;run&amp;#34;, &amp;#34;start_offset&amp;#34;: 0, &amp;#34;end_offset&amp;#34;: 7, &amp;#34;type&amp;#34;: &amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;, &amp;#34;position&amp;#34;: 0 }, { &amp;#34;token&amp;#34;: &amp;#34;runner&amp;#34;, &amp;#34;start_offset&amp;#34;: 8, &amp;#34;end_offset&amp;#34;: 15, &amp;#34;type&amp;#34;: &amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;, &amp;#34;position&amp;#34;: 1 }, { &amp;#34;token&amp;#34;: &amp;#34;ran&amp;#34;, &amp;#34;start_offset&amp;#34;: 16, &amp;#34;end_offset&amp;#34;: 19, &amp;#34;type&amp;#34;: &amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;, &amp;#34;position&amp;#34;: 2 } ] }</description></item><item><title>KStem 词干分词过滤器（KStem）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/kstem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/kstem/</guid><description>KStem 分词过滤器 # kstem 分词过滤器是一种词干提取过滤器，用于将单词还原为其词根形式。该过滤器是一种专为英语设计的轻量级算法词干提取器，它执行以下词干提取操作：
将复数形式的单词还原为单数形式。 将不同的动词时态转换为其基本形式。 去除常见的派生词尾，例如 &amp;ldquo;-ing&amp;rdquo; 或 &amp;ldquo;-ed&amp;rdquo;。 kstem 分词过滤器等同于配置了 light_english 语言的词干提取器过滤器。与其他词干提取过滤器（如 porter_stem）相比，它提供了更为保守的词干提取方式。
相关指南（先读这些） # 文本分析：词干提取 文本分析：规范化 KStem 分词过滤器基于 Lucene 的 KStemFilter。如需了解更多信息，请参阅 Lucene 文档。
参考样例 # 以下示例请求创建了一个名为 my_kstem_index 的新索引，并配置了一个带有 KStem 过滤器的分词器：
PUT /my_kstem_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;kstem_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;kstem&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_kstem_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;kstem_filter&amp;#34; ] } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;my_kstem_analyzer&amp;#34; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词干分词过滤器（Stemmer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stemmer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stemmer/</guid><description>Stemmer 分词过滤器 # stemmer 分词过滤器会将单词缩减为其词根或基本形式（也称为词干 stem）。
相关指南（先读这些） # 词干提取 文本分析基础 参数说明 # 词干提取分词过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：
阿拉伯语：arabic 亚美尼亚语：armenian 巴斯克语：basque 孟加拉语：bengali 巴西葡萄牙语：brazilian 保加利亚语：bulgarian 加泰罗尼亚语：catalan 捷克语：czech 丹麦语：danish 荷兰语：dutch、dutch_kp 英语：english（默认）、light_english、lovins、minimal_english、porter2、possessive_english 爱沙尼亚语：estonian 芬兰语：finnish、light_finnish 法语：light_french、french、minimal_french 加利西亚语：galician、minimal_galician（仅复数处理步骤） 德语：light_german、german、german2、minimal_german 希腊语：greek 印地语：hindi 匈牙利语：hungarian、light_hungarian 印尼语：indonesian 爱尔兰语：irish 意大利语：light_italian、italian 库尔德语（索拉尼方言）：sorani 拉脱维亚语：latvian 立陶宛语：lithuanian 挪威语（书面挪威语）：norwegian、light_norwegian、minimal_norwegian 挪威语（新挪威语）：light_nynorsk、minimal_nynorsk 葡萄牙语：light_portuguese、minimal_portuguese、portuguese、portuguese_rslp 罗马尼亚语：romanian 俄语：russian、light_russian 西班牙语：light_spanish、spanish 瑞典语：swedish、light_swedish 土耳其语：turkish 你也可以使用 name 参数作为 language 参数的别名。如果两个参数都被设置，name 参数将被忽略。
参考样例 # 以下示例请求创建了一个名为 my-stemmer-index 的新索引，并配置了一个带有词干提取过滤器的分词器。</description></item><item><title>Snowball 词干分词过滤器（Snowball）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/snowball/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/snowball/</guid><description>Snowball 分词过滤器 # snowball 分词过滤器是一种基于 雪球算法的词干提取过滤器。它支持多种语言，并且比波特词干提取算法更加高效和准确。
相关指南（先读这些） # 文本分析：词干提取 文本分析：规范化 参数说明 # 雪球分词过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：
阿拉伯语（Arabic） 亚美尼亚语（Armenian） 巴斯克语（Basque） 加泰罗尼亚语（Catalan） 丹麦语（Danish） 荷兰语（Dutch） 英语（English，默认值） 爱沙尼亚语（Estonian） 芬兰语（Finnish） 法语（French） 德语（German） 德语 2（German2） 匈牙利语（Hungarian） 意大利语（Italian） 爱尔兰语（Irish） Kp 立陶宛语（Lithuanian） 洛文斯（Lovins） 挪威语（Norwegian） 波特（Porter） 葡萄牙语（Portuguese） 罗马尼亚语（Romanian） 俄语（Russian） 西班牙语（Spanish） 瑞典语（Swedish） 土耳其语（Turkish） 参考样例 # 以下示例请求创建了一个名为 my-snowball-index 的新索引，并配置了一个带有雪球过滤器的分词器。
PUT /my-snowball-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;my_snowball_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;snowball&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;English&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_snowball_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;my_snowball_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词干覆盖分词过滤器（Stemmer Override）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stemmer-override/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stemmer-override/</guid><description>Stemmer Override 分词过滤器 # stemmer_override 分词过滤器允许你定义自定义的词干提取规则，这些规则可以覆盖像波特（Porter）或雪球（Snowball）等默认词干提取器的行为。当你希望对某些特定的单词应用特殊的词干提取方式，而这些单词可能无法被标准的词干提取算法正确处理时，这个过滤器就会非常有用。
相关指南（先读这些） # 文本分析：词干提取 文本分析：规范化 参数说明 # 词干覆盖分词过滤器必须使用以下参数中的一个进行配置。
参数 数据类型 描述 rules 字符串 直接在设置中定义覆盖规则。 rules_path 字符串 指定包含自定义规则（映射）的文件的路径。该路径可以是绝对路径，也可以是相对于配置目录的相对路径。 参考样例 # 以下示例请求创建了一个名为 my-index 的新索引，并配置了一个带有词干覆盖过滤器的分词器。
PUT /my-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;my_stemmer_override_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer_override&amp;#34;, &amp;#34;rules&amp;#34;: [ &amp;#34;running, runner =&amp;gt; run&amp;#34;, &amp;#34;bought =&amp;gt; buy&amp;#34;, &amp;#34;best =&amp;gt; good&amp;#34; ] } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_custom_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;my_stemmer_override_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>停用词分词过滤器（Stop）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stop/</guid><description>Stop 分词过滤器 # stop 分词过滤器用于在分词过程中从词元流中去除常见词汇（也称为停用词）。停用词通常是冠词和介词，比如 &amp;ldquo;a&amp;rdquo; 或 &amp;ldquo;for&amp;rdquo;。这些词在搜索查询中意义不大，并且常常被排除在外，以提高搜索效率和相关性。
默认的英语停用词列表包含以下单词：a、an、and、are、as、at、be、but、by、for、if、in、into、is、it、no、not、of、on、or、such、that、the、their、then、there、these、they、this、to、was、will 以及 with。
相关指南（先读这些） # 停用词 文本分析基础 参数说明 # 停用词分词过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 stopwords 可选 字符串 既可以指定自定义的停用词数组，也可以指定一种语言以获取预定义的 Lucene 停用词列表。可指定的语言如下：
- _arabic_
- _armenian_
- _basque_
- _bengali_
- _brazilian_（巴西葡萄牙语）
- _bulgarian_
- _catalan_
- _cjk_（中文、日语和韩语）
- _czech_
- _danish_
- _dutch_
- _english_（默认值）
- _estonian_
- _finnish_</description></item><item><title>同义词分词过滤器（Synonym）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/synonym/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/synonym/</guid><description>Synonym 分词过滤器 # synonym 分词过滤器允许将多个术语映射到单个术语，或在单词之间创建等价组，从而提高搜索的灵活性。
相关指南（先读这些） # 同义词 文本分析基础 参数说明 # 同义词分词过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。 synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。 lenient 可选 布尔值 加载规则配置时是否忽略异常。默认值为 false。 format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：
- solr
- wordnet
默认值为 solr。 expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。</description></item><item><title>同义词图分词过滤器（Synonym Graph）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/synonym-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/synonym-graph/</guid><description>Synonym Graph 分词过滤器 # synonym_graph 分词过滤器是同义词分词过滤器的更高级版本。它支持多词同义词，并能跨多个词元处理同义词，这使得它非常适合处理短语或者那些词元之间关系很重要的场景。
相关指南（先读这些） # 文本分析：同义词 文本分析：规范化 参数说明 # 同义词图分词过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。 synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。 lenient 可选 布尔值 在加载规则配置时是否忽略异常。默认值为 false。 format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：
- solr
- wordnet
默认值为 solr。 expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。</description></item><item><title>保留类型分词过滤器（Keep Types）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keep-types/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keep-types/</guid><description>Keep Types 分词过滤器 # keep_types 分词过滤器是一种用于文本分析的分词过滤器，它可用于控制保留或丢弃哪些词元类型。不同的分词器会产生不同的词元类型，例如 &amp;lt;HOST&amp;gt;、&amp;lt;NUM&amp;gt; 或 &amp;lt;ALPHANUM&amp;gt;。
注意：keyword 分词器、simple_pattern 分词器和 simple_pattern_split 分词器不支持 keep_types 分词过滤器，因为这些分词器不支持词元类型属性。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 参数说明 # 保留类型分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 types 必需 字符串列表 要保留或丢弃的词元类型列表（由 mode 参数决定）。 mode 可选 字符串 是要包含(include)还是排除(exclude) types 中指定的词元类型。默认值为 include（包含）。 参考样例 # 以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有保留类型过滤器的分析器：
PUT /test_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;custom_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;keep_types_filter&amp;#34;] } }, &amp;#34;filter&amp;#34;: { &amp;#34;keep_types_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;keep_types&amp;#34;, &amp;#34;types&amp;#34;: [&amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>保留词分词过滤器（Keep Words）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keep-words/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keep-words/</guid><description>Keep Words 分词过滤器 # keep_words 分词过滤器旨在在分析过程中仅保留特定的词。如果你有大量文本，但只对某些关键字或术语感兴趣，那么这个过滤器就会很有用。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 参数说明 # 词保留分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 keep_words 若未配置 keep_words_path 则为必需 字符串列表 要保留的词的列表。 keep_words_path 若未配置 keep_words 则为必需 字符串 包含要保留的词列表的文件的路径。 keep_words_case 可选 布尔值 在比较时是否将所有词转换为小写。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有词保留过滤器的分词器：
PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;custom_keep_word&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;keep_words_filter&amp;#34; ] } }, &amp;#34;filter&amp;#34;: { &amp;#34;keep_words_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;keep&amp;#34;, &amp;#34;keep_words&amp;#34;: [&amp;#34;example&amp;#34;, &amp;#34;world&amp;#34;, &amp;#34;easysearch&amp;#34;], &amp;#34;keep_words_case&amp;#34;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>关键字标记分词过滤器（Keyword Marker）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keyword-marker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keyword-marker/</guid><description>Keyword Marker 分词过滤器 # keyword_marker 分词过滤器用于防止某些词元被词干提取器或其他过滤器修改。该过滤器通过将指定的词元标记为关键词来实现这一点，这样就能避免词干提取或其他方式的处理。这确保了特定的单词能保持其原始形式。
相关指南（先读这些） # 文本分析：词干提取 文本分析：规范化 参数说明 # 关键词标记分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 ignore_case 可选 布尔值 在匹配关键词时是否忽略字母大小写。默认值为false。 keywords 若未设置 keywords_path 或 keywords_pattern 则为必需 字符串列表 要标记为关键词的词元列表。 keywords_path 若未设置 keywords 或 keywords_pattern 则为必需 字符串 关键词列表的路径（相对于配置目录或绝对路径）。 keywords_pattern 若未设置 keywords 或 keywords_path 则为必需 字符串 用于匹配要标记为关键词的词元的正则表达式。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词标记过滤器的分词器。该过滤器将单词 example 标记为关键词：</description></item><item><title>关键字重复分词过滤器（Keyword Repeat）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keyword-repeat/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/keyword-repeat/</guid><description>Keyword Repeat 分词过滤器 # keyword_repeat 分词过滤器会将词元的关键词版本发送到词元流中。当你希望在经过进一步的词元转换（例如词干提取或同义词扩展）之后，既保留原始词元，又保留其修改后的版本时，通常会使用这个过滤器。这些重复的词元使得原始的、未改变的词元版本能够与修改后的版本一起保留在最终的分析结果中。
注意：keyword_repeat 分词过滤器应该放置在词干提取过滤器之前。词干提取并非应用于每个词元，因此在词干提取之后，你可能会在同一位置得到重复的词元。为了去除重复词元，可以在词干提取器之后使用 remove_duplicates 分词过滤器。
相关指南（先读这些） # 文本分析：词干提取 文本分析：规范化 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词重复过滤器的分词器：
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;my_kstem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;kstem&amp;#34; }, &amp;#34;my_lowercase&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;lowercase&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_custom_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;my_lowercase&amp;#34;, &amp;#34;keyword_repeat&amp;#34;, &amp;#34;my_kstem&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>最小哈希分词过滤器（Min Hash）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/min-hash/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/min-hash/</guid><description>Min Hash 分词过滤器 # min_hash 分词过滤器用于基于最小哈希近似算法为词元生成哈希值，这对于检测文档间的相似度很有用。min_hash 分词过滤器会为一组词元（通常来自一个经过分词的字段）生成哈希值。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 参数说明 # 最小哈希分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 hash_count 可选 整数 为每个词元生成的哈希值数量。增加此值通常会提高相似度估计的准确性，但会增加计算成本。默认值为 1。 bucket_count 可选 整数 要使用的哈希桶数量。这会影响哈希的粒度。更多的桶能提供更细的粒度并减少哈希冲突，但需要更多内存。默认值为 512。 hash_set_size 可选 整数 每个桶中要保留的哈希数量。这会影响哈希质量。更大的集合大小可能会带来更好的相似度检测效果，但会消耗更多内存。默认值为 1。 with_rotation 可选 布尔值 当设置为 true 时，如果 hash_set_size 为 1，过滤器会用从其右侧按环形顺序找到的第一个非空桶的值填充空桶。如果 bucket_count 参数大于 1，此设置会自动默认为 true；否则，默认为 false。 参考样例 # 以下示例请求创建了一个名为 minhash_index 的新索引，并配置了一个带有最小哈希过滤器的分词器：</description></item><item><title>规范化分词过滤器（Normalization）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/normalization/</guid><description>Normalization 分词过滤器 # 归一化分词过滤器旨在以减少文本差异（尤其是特殊字符差异）的方式对文本进行调整和简化。它主要用于通过对特定语言中的字符进行标准化处理，来应对书写上的差异。
相关指南（先读这些） # 文本分析：规范化 文本分析基础 以下是可用的归一化分词过滤器：
阿拉伯语归一化: arabic_normalization 德语归一化: german_normalization 印地语归一化: hindi_normalization 印度语系归一化: indic_normalization 索拉尼语归一化: sorani_normalization 波斯语归一化: persian_normalization 斯堪的纳维亚语归一化: scandinavian_normalization 斯堪的纳维亚语折叠处理归一化: scandinavian_folding 塞尔维亚语归一化: serbian_normalization 参考样例 # 以下示例请求创建了一个名为 german_normalizer_example 的新索引，并配置了一个带有 german_normalization的分词器：
PUT /german_normalizer_example { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;german_normalizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;german_normalization&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;german_normalizer_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;german_normalizer&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>限制分词过滤器（Limit）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/limit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/limit/</guid><description>Limit 分词过滤器 # limit 分词过滤器用于限制分词链通过的词元数量。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参数说明 # 限制分词过滤器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 max_token_count 可选 整数 要生成的词元的最大数量。默认值为 1。 consume_all_tokens 可选 布尔值 （专家级设置）即使结果超过 max_token_count，也会使用来自词元生成器的所有词元。当设置此参数时，输出仍然只包含 max_token_count 指定数量的词元。不过，词元生成器生成的所有词元都会被处理。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有限制过滤器的分析器：
PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;three_token_limit&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;custom_token_limit&amp;#34; ] } }, &amp;#34;filter&amp;#34;: { &amp;#34;custom_token_limit&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;limit&amp;#34;, &amp;#34;max_token_count&amp;#34;: 3 } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>N-gram 分词过滤器（N-gram）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/n-gram/</guid><description>N-gram 分词过滤器 # ngram 分词过滤器是一种强大的工具，用于将文本拆分为更小的组件，即 n-gram，这有助于提升部分匹配和模糊搜索能力。它通过将一个词元拆分成指定长度的子字符串来工作。这些过滤器在搜索应用程序中很常用，可用于支持自动补全、部分匹配以及容错拼写搜索。
相关指南（先读这些） # 部分匹配 文本分析：模糊匹配 文本分析：识别词元 参数说明 # n-gram 分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_gram 可选 整数 n-gram 的最小长度。默认值为 1。 max_gram 可选 整数 n-gram 的最大长度。默认值为 2。 preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 false。 参考样例 # 以下示例请求创建了一个名为“ngram_example_index”的新索引，并配置了一个带有 n-gram 过滤器的分词器：
PUT /ngram_example_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;ngram_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;ngram&amp;#34;, &amp;#34;min_gram&amp;#34;: 2, &amp;#34;max_gram&amp;#34;: 3 } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;ngram_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;ngram_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>边缘 N-gram 分词过滤器（Edge N-gram）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/edge-n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/edge-n-gram/</guid><description>Edge N-gram 分词过滤器 # edge_ngram 分词过滤器与 ngram 分词过滤器非常相似，二者都会将特定字符串拆分成不同长度的子字符串。不过，edge_ngram 分词过滤器仅从词元的开头（边缘）生成 n 元子字符串。在自动补全或前缀匹配等场景中，它特别有用，因为在这些场景里，你希望在用户输入单词或短语时就能匹配词项的开头部分。
相关指南（先读这些） # 部分匹配 文本分析：识别词元 文本分析：规范化 参数说明 # 边缘 n 元分词过滤器可使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_gram 可选 整数 要生成的 n 元词项的最小长度。默认值为1。 max_gram 可选 整数 要生成的 n 元词项的最大长度。对于边缘 n 元分词过滤器，默认值为1；对于自定义分词过滤器，默认值为2。避免将此参数设置为较低的值。如果该值设置得过低，将只会生成非常短的 n 元词项，并且可能找不到搜索词。例如，将max_gram设置为 3，并且对单词“banana”建立索引，那么生成的最长词元将是“ban”。如果用户搜索“banana”，将不会返回任何匹配结果。你可以使用截断truncate分词过滤器作为搜索分析器来降低这种风险。 preserve_original 可选 布尔值 将原始词元包含在输出中。默认值为false。 参考样例 # 以下示例请求创建了一个名为 edge_ngram_example 的新索引，并配置了一个带有边缘 n 元过滤器的分词器：</description></item><item><title>词片分词过滤器（Shingle）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/shingle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/shingle/</guid><description>Shingle 分词过滤器 # shingle 分词过滤器用于从输入文本中生成词 n 元组（即词片）。例如，对于字符串 &amp;ldquo;slow green turtle&amp;rdquo;，词片过滤器会创建以下一元词片和二元词片：&amp;ldquo;slow&amp;rdquo;、&amp;ldquo;slow green&amp;rdquo;、&amp;ldquo;green&amp;rdquo;、&amp;ldquo;green turtle&amp;rdquo; 以及 &amp;ldquo;turtle&amp;rdquo;。
这个分词过滤器常常与其他过滤器结合使用，通过对短语而不是单个词元进行索引来提高搜索的准确性。
相关指南（先读这些） # 邻近匹配 文本分析：识别词元 文本分析：规范化 参数说明 # 词片分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_shingle_size 可选 整数 要连接的词元的最小数量。默认值为 2。 max_shingle_size 可选 整数 要连接的词元的最大数量。默认值为 2。 output_unigrams 可选 布尔值 是否将一元词（单个词元）包含在输出中。默认值为 true。 output_unigrams_if_no_shingles 可选 布尔值 如果未生成词片，是否输出一元词。默认值为 false。 token_separator 可选 字符串 用于将词元连接成词片的分隔符。默认值为一个空格（ ）。 filler_token 可选 字符串 插入到词元之间的空位置或间隙中的词元。默认值为下划线（_）。 如果 output_unigrams 和 output_unigrams_if_no_shingles 都设置为 true，则 output_unigrams_if_no_shingles 将被忽略。</description></item><item><title>CJK 二元组分词过滤器（CJK Bigram）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/cjk-bigram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/cjk-bigram/</guid><description>CJK Bigram 分词过滤器 # cjk_bigram 分词过滤器是专门为处理东亚语言（如中文、日文和韩文，简称 CJK）而设计的，这些语言通常不使用空格来分隔单词。二元组是指在词元字符串中两个相邻元素的序列，这些元素可以是字符或单词。对于 CJK 语言来说，二元组有助于近似确定单词边界，并捕捉那些能够传达意义的重要字符对。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 参数说明 # CJK 二元分词过滤器可以通过两个参数进行配置：ignore_scripts（忽略字符集）和 output_unigrams（输出一元组）。
ignore_scripts（忽略字符集） # CJK 二元分词过滤器会忽略所有非 CJK 字符集（如拉丁字母或西里尔字母等书写系统），并且仅将 CJK 文本分词为二元组。可使用此选项指定要忽略的 CJK 字符集。该选项有以下有效值：
han（汉字）：汉字字符集处理汉字。汉字是用于中国、日本和韩国书面语言的意符文字。该过滤器可帮助处理文本相关任务，例如对用中文、日文汉字或韩文汉字书写的文本进行分词、规范化或词干提取。 hangul（韩文）：韩文（谚文）字符集处理韩文字符，这些字符是韩语所特有的，不存在于其他东亚字符集中。 hiragana（平假名）：平假名字符集处理平假名，平假名是日语书写系统中使用的两种音节文字之一。平假名通常用于日语的本土词汇、语法元素以及某些形式的标点符号。 katakana（片假名）：片假名字符集处理片假名，片假名是日语的另一种音节文字。片假名主要用于外来借词、拟声词、科学名称以及某些日语词汇。 output_unigrams（输出一元组） # 当此选项设置为 true 时，会同时输出一元组（单个字符）和二元组。默认值为 false。
参考样例 # 以下示例请求创建了一个名为 devanagari_example_index 的新索引，并定义了一个分词器，该分词器使用了 cjk_bigram_filter 过滤器，且将 ignored_scripts 参数设置为 katakana（片假名）：
PUT /cjk_bigram_example { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;cjk_bigrams_no_katakana&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;cjk_bigrams_no_katakana_filter&amp;#34; ] } }, &amp;#34;filter&amp;#34;: { &amp;#34;cjk_bigrams_no_katakana_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;cjk_bigram&amp;#34;, &amp;#34;ignored_scripts&amp;#34;: [ &amp;#34;katakana&amp;#34; ], &amp;#34;output_unigrams&amp;#34;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>CJK 宽度分词过滤器（CJK Width）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/cjk-width/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/cjk-width/</guid><description>CJK Width 分词过滤器 # cjk_width 分词过滤器通过将全角 ASCII 字符转换为其标准（半角）ASCII 等效字符，并将半角片假名字符转换为全角等效字符，来对中文、日文和韩文（CJK）词元进行规范化处理。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 转换全角 ASCII 字符 # 在 CJK 文本中，ASCII 字符（如字母和数字）可能会以全角形式出现，占据两个半角字符的空间。全角 ASCII 字符在东亚排版中通常用于与 CJK 字符的宽度对齐。然而，出于索引和搜索的目的，这些全角字符需要规范化为其标准（半角）ASCII 等效字符。
以下示例说明了 ASCII 字符的规范化过程：
全角: ＡＢＣＤＥ １２３４５ 规范化 (半角): ABCDE 12345 转换半角片假名字符 # CJK 宽度分词过滤器会将半角片假名字符转换为对应的全角片假名字符，全角片假名是日语文本中使用的标准形式。如下例所示，这种规范化处理对于文本处理和搜索的一致性至关重要：
半角 katakana: ｶﾀｶﾅ 规范化 (全角) katakana: カタカナ 参考样例 # 以下示例请求创建了一个名为 cjk_width_example_index 的新索引，并定义了一个包含 cjk_width 过滤器的分词器：
PUT /cjk_width_example_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;cjk_width_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;cjk_width&amp;#34;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>单词分隔分词过滤器（Word Delimiter）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/word-delimiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/word-delimiter/</guid><description>Word Delimiter 分词过滤器 # word_delimiter 分词过滤器用于根据预定义的字符拆分词元，还能根据可定制规则对词元进行可选的规范化处理。
注意：我们建议尽可能使用 word_delimiter_graph 过滤器而非 word_delimiter 过滤器，因为 word_delimiter 过滤器有时会生成无效的词元图。
提示：word_delimiter 过滤器可用于从零件编号或产品 ID 等复杂标识符中去除标点符号。在这种情况下，它最好与 keyword 分词器配合使用。对于带连字符的单词，建议使用 synonym_graph 分词过滤器而非 word_delimiter 过滤器，因为用户搜索这些词项时，常常既会搜索带连字符的形式，也会搜索不带连字符的形式。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 默认情况下，该过滤器应用以下规则：
描述 输入 输出 将非字母数字字符视为分隔符 ultra-fast ultra, fast 去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder 当字母大小写发生转换时拆分词元 Easysearch Easy, search 当字母和数字之间发生转换时拆分词元 T1000 T, 1000 去除词元末尾的所有格形式（&amp;lsquo;s） John's John 重要提示：不要将该过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并影响诸如 catenate_all 或 preserve_original 等选项的效果。我们建议将此过滤器与关键(keyword)字分词器或空白(whitespace)分词器配合使用。</description></item><item><title>单词分隔图分词过滤器（Word Delimiter Graph）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/word-delimiter-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/word-delimiter-graph/</guid><description>Word Delimiter Graph 分词过滤器 # word_delimiter_graph 分词过滤器用于依据预定义的字符对词元进行拆分，还能基于可定制的规则对词元进行可选的规范化处理。
提示：word_delimiter_graph 分词过滤器可用于去除零件编号或产品 ID 这类复杂标识符中的标点符号。在这种情况下，它最好与 keyword 分词器搭配使用。对于带有连字符的单词，建议使用 synonym_graph 分词过滤器而非 word_delimiter_graph 分词过滤器，因为用户在搜索这些词时，往往既会搜索带连字符的形式，也会搜索不带连字符的形式。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 默认情况下，该过滤器会应用以下规则：
描述 输入 输出 将非字母数字字符视为分隔符 ultra-fast ultra, fast 去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder 当字母大小写发生转换时拆分词元 Easysearch Easy, search 当字母和数字之间发生转换时拆分词元 T1000 T, 1000 去除词元末尾的所有格形式（&amp;lsquo;s） John's John 重要的是，不要将此过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并且会影响诸如 catenate_all 或 preserve_original 之类的选项。我们建议将此过滤器与关键字(keyword)分词器或空白(whitespace)分词器搭配使用。</description></item><item><title>展平图分词过滤器（Flatten Graph）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/flatten-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/flatten-graph/</guid><description>Flatten Graph 分词过滤器 # flatten_graph 分词过滤器用于处理在图结构中同一位置生成多个词元时出现的复杂词元关系。一些分词过滤器，比如 synonym_graph 和 word_delimiter_graph，会生成多位置词元——即相互重叠或跨越多个位置的词元。这些词元图对于搜索查询很有用，但在索引过程中却不被直接支持。flatten_graph 分词过滤器会将多位置词元解析为一个线性的词元序列。对图进行扁平化处理可确保与索引过程兼容。
注意：词元图的扁平化是一个有损耗的过程。只要有可能，就应避免使用 flatten_graph 过滤器。相反，应仅在搜索分词器中应用图分词过滤器，这样就无需使用 flatten_graph 分词过滤器了。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 参考样例 # 以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有平图化分词过滤器的分词器：
PUT /test_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;my_index_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;my_custom_filter&amp;#34;, &amp;#34;flatten_graph&amp;#34; ] } }, &amp;#34;filter&amp;#34;: { &amp;#34;my_custom_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;word_delimiter_graph&amp;#34;, &amp;#34;catenate_all&amp;#34;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>唯一分词过滤器（Unique）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/unique/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/unique/</guid><description>Unique 分词过滤器 # unique 分词过滤器可确保在分词过程中仅保留唯一的词元，它会去除在单个字段或文本块中出现的重复词元。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参数说明 # 唯一分词过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 only_on_same_position 可选 布尔值 如果设置为 true，该分词过滤器将充当去重分词过滤器，仅移除位于相同位置的词元。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 unique_example 的新索引，并配置了一个带有唯一过滤器的分词器。
PUT /unique_example { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;unique_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;unique&amp;#34;, &amp;#34;only_on_same_position&amp;#34;: false } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;unique_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;unique_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>去重分词过滤器（Remove Duplicates）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/remove-duplicates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/remove-duplicates/</guid><description>Remove Duplicates 分词过滤器 # remove_duplicates 分词过滤器用于去除在分词过程中在相同位置生成的重复词元。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参考样例 # 以下示例请求创建了一个带有 keyword_repeat（关键词重复）分词过滤器的索引。该过滤器会在每个词元自身所在的相同位置添加该词元的关键词版本，然后使用 kstem（K 词干提取）过滤器来创建该词元的词干形式。
PUT /example-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;custom_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;keyword_repeat&amp;#34;, &amp;#34;kstem&amp;#34; ] } } } } } 使用以下请求来分析字符串 “Slower turtle”。
GET /example-index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;custom_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Slower turtle&amp;#34; } 返回内容中在同一位置包含了两次词元 “turtle”。
{ &amp;#34;tokens&amp;#34;: [ { &amp;#34;token&amp;#34;: &amp;#34;slower&amp;#34;, &amp;#34;start_offset&amp;#34;: 0, &amp;#34;end_offset&amp;#34;: 6, &amp;#34;type&amp;#34;: &amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;, &amp;#34;position&amp;#34;: 0 }, { &amp;#34;token&amp;#34;: &amp;#34;slow&amp;#34;, &amp;#34;start_offset&amp;#34;: 0, &amp;#34;end_offset&amp;#34;: 6, &amp;#34;type&amp;#34;: &amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;, &amp;#34;position&amp;#34;: 0 }, { &amp;#34;token&amp;#34;: &amp;#34;turtle&amp;#34;, &amp;#34;start_offset&amp;#34;: 7, &amp;#34;end_offset&amp;#34;: 13, &amp;#34;type&amp;#34;: &amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;, &amp;#34;position&amp;#34;: 1 }, { &amp;#34;token&amp;#34;: &amp;#34;turtle&amp;#34;, &amp;#34;start_offset&amp;#34;: 7, &amp;#34;end_offset&amp;#34;: 13, &amp;#34;type&amp;#34;: &amp;#34;&amp;lt;ALPHANUM&amp;gt;&amp;#34;, &amp;#34;position&amp;#34;: 1 } ] } 可以通过在索引设置中添加一个去重分词过滤器来移除重复的词元。</description></item><item><title>修剪分词过滤器（Trim）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/trim/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/trim/</guid><description>Trim 分词过滤器 # trim 分词过滤器会从词元中去除前导和尾随的空白字符。
注意：许多常用的分词器，例如 standard 分词器、keyword 分词器和 whitespace 分词器，在分词过程中会自动去除前导和尾随的空白字符。当使用这些分词器时，无需额外配置 trim 分词过滤器。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参考样例 # 以下示例请求创建了一个名为 my_pattern_trim_index 的新索引，并配置了一个带有修剪过滤器和匹配词元生成器的分词器。其中，匹配词元生成器不会去除词元的前导和尾随空白字符。
PUT /my_pattern_trim_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;my_trim_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;trim&amp;#34; } }, &amp;#34;tokenizer&amp;#34;: { &amp;#34;my_pattern_tokenizer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;pattern&amp;#34;, &amp;#34;pattern&amp;#34;: &amp;#34;,&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_pattern_trim_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;my_pattern_tokenizer&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;my_trim_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>反转分词过滤器（Reverse）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/reverse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/reverse/</guid><description>Reverse 分词过滤器 # reverse 分词过滤器会反转每个词元中字符的顺序，这样在分析过程中，后缀信息就会位于反转后词元的开头。
相关指南（先读这些） # 文本分析：识别词元 部分匹配 文本分析基础 这对于基于后缀的搜索很有用：
反转分词过滤器在你需要进行基于后缀的搜索时很有帮助，例如以下场景：
后缀匹配：根据单词的后缀来搜索单词，比如识别以特定后缀结尾的单词（例如 -tion 或 -ing）。 文件扩展名搜索：通过文件的扩展名来搜索文件，例如 .txt 或 .jpg。 自定义排序或排名：通过反转词元，你可以基于后缀实现独特的排序或排名逻辑。 后缀自动补全：实现基于后缀而非前缀的自动补全建议。 参考说明 # 以下示例请求创建了一个名为 my-reverse-index 的新索引，并配置了一个带有反转过滤器的分词器。
PUT /my-reverse-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;reverse_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;reverse&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_reverse_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;reverse_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>截断分词过滤器（Truncate）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/truncate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/truncate/</guid><description>Truncate 分词过滤器 # truncate 分词过滤器用于缩短超过指定长度的词元。它会将词元修剪至最大字符数，确保超过该限制的词元被截断。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参数说明 # 截断分词过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 length 可选 整数 指定生成的词元的最大长度。默认值为 10。 参考样例 # 以下示例请求创建了一个名为 truncate_example 的新索引，并配置了一个带有截断过滤器的分词器。
PUT /truncate_example { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;truncate_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;truncate&amp;#34;, &amp;#34;length&amp;#34;: 5 } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;truncate_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;truncate_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>撇号分词过滤器（Apostrophe）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/apostrophe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/apostrophe/</guid><description>Apostrophe 分词过滤器 # apostrophe 分词过滤器的主要功能是移除所有格形式的撇号以及跟在撇号后面的所有内容。这在分析那些大量使用撇号的语言的文本时非常有用，比如土耳其语。在土耳其语中，撇号用于将词根与后缀分隔开来，这些后缀包括所有格后缀、格标记以及其他语法词尾。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参考样例 # 以下示例请求创建了一个名为 custom_text_index 的新索引，在索引设置中配置了一个自定义分词器，并在映射中使用该分词器：
PUT /custom_text_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;custom_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;apostrophe&amp;#34; ] } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;content&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;custom_analyzer&amp;#34; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /custom_text_index/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;custom_analyzer&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;John&amp;#39;s car is faster than Peter&amp;#39;s bike&amp;#34; } 返回内容包含产生的词元</description></item><item><title>省略词分词过滤器（Elision）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/elision/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/elision/</guid><description>Elision 分词过滤器 # elision 分词过滤器用于从某些语言的单词中去除省略的字符。省略现象通常出现在像法语这样的语言中，在这些语言里，单词常常会发生缩合，并与后面的单词结合，常见的方式是省略一个元音字母，并用一个撇号来替代。
注意：elision 分词过滤器已经在以下语言分词器中预先配置好了：加泰罗尼亚语（catalan）、法语（french）、爱尔兰语（irish）和意大利语（italian）。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参数说明 # 自定义省略词分词过滤器可使用以下参数进行配置。
参数 必需/可选 数据类型 描述 articles 若未配置 articles_path 则为必需 字符串数组 定义当某些冠词或短词作为省略形式的一部分出现时，哪些应该被移除。 articles_path 若未配置 articles 则为必需 字符串 指定在分析过程中应被移除的自定义冠词列表的路径。 articles_case 可选 布尔值 指定在匹配省略形式时，该过滤器是否区分大小写。默认值为 false。 参考样例 # 法语中默认的省略形式集合包括 l'、m'、t'、qu'、n'、s'、j'、d'、c'、jusqu'、quoiqu'、lorsqu' 和 puisqu'。你可以通过配置 french_elision 分词过滤器来更新这个集合。以下示例请求创建了一个名为 french_texts 的新索引，并配置了一个带有 french_elision 过滤器的分词器：</description></item><item><title>长度分词过滤器（Length）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/length/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/length/</guid><description>Length 分词过滤器 # length 分词过滤器用于从词元流中移除那些不符合指定长度标准（最小和最大长度值）的词元。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参数说明 # 长度分词过滤器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 min 可选 整数 词元的最小长度。默认值为 0。 max 可选 整数 词元的最大长度。默认值为整数类型的最大值（2147483647）。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有长度过滤器的分词器：
PUT my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;only_keep_4_to_10_characters&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;whitespace&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;length_4_to_10&amp;#34; ] } }, &amp;#34;filter&amp;#34;: { &amp;#34;length_4_to_10&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;length&amp;#34;, &amp;#34;min&amp;#34;: 4, &amp;#34;max&amp;#34;: 10 } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>正则捕获分词过滤器（Pattern Capture）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/pattern-capture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/pattern-capture/</guid><description>Pattern Capture 分词过滤器 # pattern_capture 分词过滤器是一种功能强大的过滤器，它使用正则表达式根据特定模式来捕获和提取文本的部分内容。当你想要提取词元的特定部分，例如电子邮件域名、话题标签或数字，并将其重新用于进一步的分析或索引编制时，这个过滤器会非常有用。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 参数说明 # 捕获匹配分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 patterns 必需 字符串数组 用于捕获文本部分的正则表达式数组。 preserve_original 必需 布尔值 是否在输出中保留原始词元。默认值为 true。 参考样例 # 以下示例请求创建了一个名为 email_index 的新索引，并配置了一个带有捕获匹配过滤器的分词器，以便从电子邮件地址中提取本地部分和域名。
PUT /email_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;email_pattern_capture&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;pattern_capture&amp;#34;, &amp;#34;preserve_original&amp;#34;: true, &amp;#34;patterns&amp;#34;: [ &amp;#34;^([^@]+)&amp;#34;, &amp;#34;@(.+)$&amp;#34; ] } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;email_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;uax_url_email&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;email_pattern_capture&amp;#34;, &amp;#34;lowercase&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>正则替换分词过滤器（Pattern Replace）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/pattern-replace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/pattern-replace/</guid><description>Pattern Replace 分词过滤器 # pattern_replace 分词过滤器允许您使用正则表达式来修改词元。此过滤器会将词元中的模式替换为指定的值，这使您在对词元进行索引之前，能够灵活地转换或规范化词元。当您在分析过程中需要清理或规范文本时，该过滤器尤其有用。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参数说明 # 匹配替换分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 pattern 必需 字符串 一个正则表达式模式，用于匹配需要被替换的文本。 all 可选 布尔值 是否替换所有匹配的模式。如果为 false，则仅替换第一个匹配项。默认值为 true。 replacement 可选 字符串 用于替换匹配模式的字符串。默认值为空字符串。 参考样例 # 以下示例请求创建了一个名为 text_index 的新索引，并配置了一个带有匹配替换过滤器的分词器，用于将包含数字的词元替换为字符串 [NUM]：
PUT /text_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;number_replace_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;pattern_replace&amp;#34;, &amp;#34;pattern&amp;#34;: &amp;#34;\\d+&amp;#34;, &amp;#34;replacement&amp;#34;: &amp;#34;[NUM]&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;number_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;number_replace_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>字典复合词分词过滤器（Dictionary Decompounder）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/dictionary-decompounder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/dictionary-decompounder/</guid><description>Dictionary Decompounder 分词过滤器 # dictionary_decompounder 分词过滤器用于根据预定义的词典将复合词拆分为其组成部分。该过滤器对于德语、荷兰语或芬兰语等复合词较为常见的语言特别有用，因为将复合词拆分可以提高搜索的相关性。dictionary_decompounder 分词过滤器会根据已知词列表判断每个词元（单词）是否可以拆分为更小的词元。如果该词元可以拆分为已知单词，过滤器就会为该词元生成子词元。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 参数说明 # 词典复合词分词过滤器具有以下参数：
参数 必需/可选 数据类型 描述 word_list 除非配置了 word_list_path，否则为必需 字符串数组 过滤器用于拆分复合词的单词词典。 word_list_path 除非配置了 word_list，否则为必需 字符串 包含词典单词的文本文件的文件路径。可以接受绝对路径或相对于配置(config)目录的相对路径。词典文件必须采用 UTF-8 编码，且每个单词必须单独占一行。 min_word_size 可选 整数 会被考虑进行拆分的整个复合词的最小长度。如果复合词短于该值，则不会进行拆分。默认值为 5。 min_subword_size 可选 整数 任何子词的最小长度。如果子词短于该值，则不会包含在输出中。默认值为 2。 max_subword_size 可选 整数 任何子词的最大长度。如果子词长于该值，则不会包含在输出中。默认值为 15。 only_longest_match 可选 布尔值 如果设置为 true，则仅返回最长匹配的子词。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 decompound_example 的新索引，并配置了一个带有词典复合词过滤器的分词器：</description></item><item><title>多路复用分词过滤器（Multiplexer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/multiplexer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/multiplexer/</guid><description>Multiplexer 分词过滤器 # multiplexer 分词过滤器允许你通过应用不同的过滤器来创建同一词元的多个版本。当你想要以多种方式分析同一个词元时，这非常有用。例如，你可能希望使用不同的词干提取、同义词或 n-gram 过滤器来分析一个词元，并将所有生成的词元一起使用。这个分词过滤器的工作方式是复制分词流，并对每个副本应用不同的过滤器。
注意：multiplexer 分词过滤器会从分词流中移除重复的词元。
限制：multiplexer 分词过滤器不支持 synonym 过滤器、synonym_graph 分词过滤器或 shingle 分词过滤器，因为这些过滤器不仅需要分析当前词元，还需要分析后续的词元，以便正确确定如何转换输入内容。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 参数说明 # 多路复用分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 filters 可选 字符串列表 要应用于分词流每个副本的分词过滤器的逗号分隔列表。默认值为空列表。 preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 true。 参考样例 # 以下示例请求创建了一个名为 multiplexer_index 的新索引，并配置了一个带有多路复用过滤器的分词器：
PUT /multiplexer_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;english_stemmer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer&amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;english&amp;#34; }, &amp;#34;synonym_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;synonym&amp;#34;, &amp;#34;synonyms&amp;#34;: [ &amp;#34;quick,fast&amp;#34; ] }, &amp;#34;multiplexer_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;multiplexer&amp;#34;, &amp;#34;filters&amp;#34;: [&amp;#34;english_stemmer&amp;#34;, &amp;#34;synonym_filter&amp;#34;], &amp;#34;preserve_original&amp;#34;: true } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;multiplexer_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;multiplexer_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>常见词组分词过滤器（Common Grams）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/common-grams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/common-grams/</guid><description>Common Grams 分词过滤器 # common_grams 分词过滤器通过保留文本中常见的短语（常用词组）来提高搜索的相关性。当处理某些语言或数据集时，特定的单词组合经常作为一个整体出现，如果将它们当作单独的词元处理，可能会影响搜索的相关性，此时该过滤器就非常有用。如果输入字符串中常用词，这个分词过滤器会同时生成它们的一元词组（单个词）和二元词组（两个词的组合）。
使用这个分词过滤器可以保持常用短语的完整性，从而提高搜索的相关性。这有助于更准确地匹配查询内容，尤其是对于频繁出现的单词组合。它还能通过减少不相关的匹配结果来提高搜索的精度。
相关指南（先读这些） # 文本分析：停用词 邻近匹配 文本分析：规范化 使用此过滤器时，你必须谨慎选择并维护常用词(common_words)列表。
参数说明 # 常用词组分词过滤器可通过以下参数进行配置：
参数 必需/可选 数据类型 描述 common_words 必需 字符串列表 一个应被视为常用词词组的单词列表。如果 common_words 参数是一个空列表，common_grams 分词过滤器将成为一个无操作过滤器，即它根本不会修改输入的词元。 ignore_case 可选 布尔值 指示过滤器在匹配常用词时是否应忽略大小写差异。默认值为 false。 query_mode 可选 布尔值 当设置为 true 时，应用以下规则：
- 从 common_words 生成的一元词组（单个词）不包含在输出中。
- 非常用词后跟常用词形成的二元词组会保留在输出中。
- 如果非常用词的一元词组后面紧接着一个常用词，则该一元词组会被排除。
- 如果非常用词出现在文本末尾且前面是一个常用词，其一元词组不包含在输出中。 参考样例 # 以下示例请求创建了一个名为 my_common_grams_index 的新索引，并配置了一个使用常用词组（common_grams）过滤器的分析器。</description></item><item><title>指纹分词过滤器（Fingerprint）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/fingerprint/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/fingerprint/</guid><description>Fingerprint 分词过滤器 # fingerprint 分词过滤器用于对文本进行标准化和去重处理。当文本处理的唯一性或者一致性至关重要时，这个过滤器特别有用。fingerprint 分词过滤器通过以下步骤处理文本以实现这一目的：
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 小写转换：将所有文本转换为小写。 分词：把文本拆分成词元。 排序：按字母顺序排列这些词元。 去重：消除重复的词元。 合并词元：将这些词元合并成一个字符串，通常使用空格或其他指定的分隔符进行连接。 参数说明 # 指纹分词过滤器可以使用以下两个参数进行配置。
参数 必需/可选 数据类型 描述 max_output_size 可选 整数 限制生成的指纹字符串的长度。如果拼接后的字符串超过了 max_output_size，过滤器将不会产生任何输出，从而得到一个空词元。默认值是 255 separator 可选 字符串 定义在词元排序和去重后，用于将词元连接成单个字符串。默认是空格（&amp;quot; &amp;quot;）。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有指纹分词过滤器的分词器：
PUT /my_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;my_fingerprint&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;fingerprint&amp;#34;, &amp;#34;max_output_size&amp;#34;: 200, &amp;#34;separator&amp;#34;: &amp;#34;-&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;my_fingerprint&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>条件分词过滤器（Condition）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/condition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/condition/</guid><description>Condition 分词过滤器 # condition 分词过滤器是一种特殊类型的过滤器，它允许你根据特定标准有条件地应用其他分词过滤器。这使得在文本分析过程中，你能更精准地控制何时应用特定的分词过滤器。你可以配置多个过滤器，并且只有当满足你定义的条件时，这些过滤器才会被应用。该分词过滤器在进行特定语言处理和特殊字符处理时非常有用。
相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参数说明 # 要使用条件分词过滤器，必须配置两个参数，具体如下：
参数 必需/可选 数据类型 描述 filter 必需 数组 当满足由 script 参数定义的指定条件时，指定应对词元应用哪个分词过滤器。 script 必需 对象 配置一个内联脚本，该脚本定义了要应用 filter 参数中指定的过滤器所需满足的条件（仅接受内联脚本）。 参考样例 # 以下示例请求创建了一个名为 my_conditional_index 的新索引，并配置了一个带有条件过滤器的分词器。该过滤器会对任何包含字符序列 “um” 的词元应用小写（lowercase）字母过滤器。
PUT /my_conditional_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;my_conditional_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;condition&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;], &amp;#34;script&amp;#34;: { &amp;#34;source&amp;#34;: &amp;#34;token.</description></item><item><title>经典分词过滤器（Classic）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/classic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/classic/</guid><description>Classic 分词过滤器 # classic 分词过滤器的主要功能是与 classic 分词器配合使用。它通过应用以下常见的转换操作来处理词元，这些操作有助于文本分析和搜索：
移除所有格词尾，例如 &amp;ldquo;&amp;rsquo;s&amp;rdquo;。比如，&amp;ldquo;John&amp;rsquo;s&amp;rdquo; 会变为 &amp;ldquo;John&amp;rdquo;。 从首字母缩略词中移除句点。例如，&amp;ldquo;D.A.R.P.A.&amp;rdquo; 会变为 &amp;ldquo;DARPA&amp;rdquo;。 相关指南（先读这些） # 文本分析：规范化 文本分析：识别词元 参考样例 # 以下示例请求创建了一个名为 custom_classic_filter 的新索引，并配置了一个使用经典分词过滤器的分词器。
PUT /custom_classic_filter { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;analyzer&amp;#34;: { &amp;#34;custom_classic&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;classic&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;classic&amp;#34;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /custom_classic_filter/_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;custom_classic&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;John&amp;#39;s co-operate was excellent.&amp;#34; } 返回内容包含产生的词元</description></item><item><title>谓词分词过滤器（Predicate Token Filter）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/predicate-token-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/predicate-token-filter/</guid><description>Predicate Token Filter 分词过滤器 # predicate_token_filter 分词过滤器会根据自定义脚本中定义的条件来评估词元是应该保留还是丢弃。词元的评估是在分析谓词上下文中进行的。此过滤器仅支持内联 Painless 脚本。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 参数说明 # 谓词分词过滤器有一个必需参数：script。该参数提供一个条件，用于评估词元是否应被保留。
参考样例 # 以下示例请求创建了一个名为 predicate_index 的新索引，并配置了一个带有谓词分词过滤器的分词器。该过滤器指定仅输出长度超过 7 个字符的词元。
PUT /predicate_index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;my_predicate_filter&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;predicate_token_filter&amp;#34;, &amp;#34;script&amp;#34;: { &amp;#34;source&amp;#34;: &amp;#34;token.term.length() &amp;gt; 7&amp;#34; } } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;predicate_analyzer&amp;#34;: { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [ &amp;#34;lowercase&amp;#34;, &amp;#34;my_predicate_filter&amp;#34; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>分隔符负载分词过滤器（Delimited Payload）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/delimited-payload/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/delimited-payload/</guid><description>Delimited Payload 分词过滤器 # delimited_payload 分词过滤器用于在分词过程中解析包含负载的词元。例如，字符串 red|1.5 fast|2.0 car|1.0 会被解析为词元 red（负载为 1.5）、fast（负载为 2.0）和 car（负载为 1.0）。当你的词元包含额外的关联数据（如权重、分数或其他数值），且你打算将这些数据用于评分或自定义查询逻辑时，这个过滤器就特别有用。该过滤器可以处理不同类型的负载，包括整数、浮点数和字符串，并将负载（额外的元数据）附加到词元上。
在文本分词时，delimited_payload 分词过滤器会解析每个词元，提取负载并将其附加到词元上。后续在查询中可以使用这个负载来影响评分、提升权重或实现其他自定义行为。
负载以 Base64 编码的字符串形式存储。默认情况下，负载不会随词元一起在查询响应中返回。若要返回负载，你必须配置额外的参数。
相关指南（先读这些） # 文本分析：识别词元 文本分析：规范化 参数说明 # 分隔式负载分词过滤器有两个参数：
参数 必需/可选 数据类型 描述 encoding 可选 字符串 指定附加到词元的负载的数据类型。这决定了在分析和查询期间如何解释负载数据。
有效值为：
- float：使用 IEEE 754 格式将负载解释为 32 位浮点数（例如，car\|2.5 中的 2.5）。
- identity：将负载解释为字符序列（例如，在 user\|admin 中，admin 被解释为字符串）。
- int：将负载解释为 32 位整数（例如，priority \| 1中的1）。</description></item><item><title>印地语归一化过滤器（Hindi Normalization）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/hindi-normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/hindi-normalization/</guid><description>印地语归一化过滤器 # hindi_normalization 词元过滤器对印地语（हिन्दी）文本进行正字法归一化，统一 Devanagari 脚本中的字符变体。
归一化规则 # 处理 说明 Nukta 移除 移除 nukta 标记（用于外来词音译的点） 视觉归一 统一视觉上相同但编码不同的字符变体 末尾 Chandra 移除 移除词尾的 chandra 符号 搭配 indic_normalization 建议先应用 indic_normalization 过滤器 使用示例 # PUT my-hindi-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;hindi_norm&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;hindi_normalization&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_hindi&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;indic_normalization&amp;#34;, &amp;#34;hindi_norm&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;indic_normalization&amp;#34;, &amp;#34;hindi_normalization&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;हिन्दी भाषा&amp;#34; } 参数 # 此过滤器不接受任何参数。</description></item><item><title>印度语系归一化过滤器（Indic Normalization）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/indic-normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/indic-normalization/</guid><description>印度语系归一化过滤器 # indic_normalization 词元过滤器对印度语系（Indic）文本进行 Unicode 归一化，统一各印度语系脚本中字符的多种表示形式。它是孟加拉语、印地语等语言归一化的基础层。
归一化规则 # 处理 说明 Unicode 分解与合成 将组合字符序列转为标准的预组合形式（NFC 归一化） 零宽字符清理 移除零宽连接符（ZWJ）和零宽非连接符（ZWNJ） Nukta 统一 将 base + nukta 两码点序列合并为等价的单码点字符 变体编码统一 统一不同 Unicode 编码表示的相同字符 使用示例 # PUT my-indic-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;indic_norm&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;indic_normalization&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_indic&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;indic_norm&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;indic_normalization&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;हिन्दी&amp;#34; } 参数 # 此过滤器不接受任何参数。</description></item><item><title>土耳其语词干过滤器（Turkish Stemmer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/turkish-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/turkish-stem/</guid><description>土耳其语词干过滤器 # turkish_stemmer 词元过滤器使用 Snowball 算法对土耳其语文本进行词干提取。
功能说明 # 土耳其语是黏着语，一个词可以有多层后缀。此词干提取器移除常见的名词格后缀、所有格后缀和动词变位后缀。
注意：土耳其语有特殊的大小写规则（İ↔i、I↔ı），需要搭配 turkish_lowercase 过滤器。
使用示例 # PUT my-turkish-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;tr_stem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;turkish&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_turkish&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;apostrophe&amp;#34;, &amp;#34;turkish_lowercase&amp;#34;, &amp;#34;tr_stem&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;turkish&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;programcıların programlama&amp;#34; } 参数 # 参数 值 说明 type stemmer 过滤器类型 language turkish 指定土耳其语 Snowball 词干算法 在语言分析器中的位置 # 土耳其语分析器 内置了此过滤器，搭配 apostrophe 和 turkish_lowercase。</description></item><item><title>塞尔维亚语归一化过滤器（Serbian Normalization）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/serbian-normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/serbian-normalization/</guid><description>塞尔维亚语归一化过滤器 # serbian_normalization 词元过滤器将塞尔维亚语西里尔字母转写为对应的拉丁字母形式，使两种书写系统的文本在搜索时可以互相匹配。
归一化规则 # 塞尔维亚语同时使用西里尔字母和拉丁字母，此过滤器将西里尔形式统一为拉丁形式：
西里尔字母 拉丁字母 西里尔字母 拉丁字母 а a п p б b р r в v с s г g т t д d ћ ć ђ đ у u е e ф f ж ž х h з z ц c и i ч č ј j џ dž к k ш š л l љ lj м m њ nj н n о o 使用示例 # PUT my-serbian-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;serbian_norm&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;serbian_normalization&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_serbian&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;serbian_norm&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;serbian_normalization&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;Београд&amp;#34; } 响应中 Београд（西里尔）→ Beograd（拉丁）。</description></item><item><title>孟加拉语归一化过滤器（Bengali Normalization）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/bengali-normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/bengali-normalization/</guid><description>孟加拉语归一化过滤器 # bengali_normalization 词元过滤器对孟加拉语（বাংলা）文本进行 Unicode 归一化，统一字符的多种表示形式。
归一化规则 # 处理 说明 Nukta 合成 将 base + nukta 组合转为对应的预组合字符 变体统一 统一视觉上相同但编码不同的字符 印度语系通用归一化 在 indic_normalization 基础上进一步处理 使用示例 # PUT my-bengali-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;bengali_norm&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;bengali_normalization&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_bengali&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;indic_normalization&amp;#34;, &amp;#34;bengali_norm&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;bengali_normalization&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;বাংলাদেশ&amp;#34; } 参数 # 此过滤器不接受任何参数。</description></item><item><title>巴西葡萄牙语词干过滤器（Brazilian Stemmer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/brazilian-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/brazilian-stem/</guid><description>巴西葡萄牙语词干过滤器 # brazilian_stemmer 词元过滤器对巴西葡萄牙语文本进行词干提取，使用 Lucene 的 BrazilianStemmer 算法。
功能说明 # 与通用葡萄牙语词干提取不同，此过滤器专门针对巴西葡萄牙语的特点进行优化：
处理巴西特有的动词变位形式 移除名词/形容词的阴阳性和单复数后缀 处理副词后缀 -mente 使用示例 # PUT my-brazilian-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;br_stem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;brazilian&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_brazilian&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;br_stem&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;brazilian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;brasileiros programação&amp;#34; } 参数 # 参数 值 说明 type stemmer 过滤器类型 language brazilian 指定巴西葡萄牙语词干算法 在语言分析器中的位置 # 巴西葡萄牙语分析器 内置了此过滤器。</description></item><item><title>德语归一化过滤器（German Normalization）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/german-normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/german-normalization/</guid><description>德语归一化过滤器 # german_normalization 词元过滤器将德语特有的元音变音和 ß 归一化为 ASCII 等价形式。
归一化规则 # 原始 归一化 说明 ä a 元音变音归一化 ö o 元音变音归一化 ü u 元音变音归一化 Ä A 大写变音归一化 Ö O 大写变音归一化 Ü U 大写变音归一化 ß ss 双 s 替换 注意：这与 ae → a 不同。此过滤器只处理变音符号字符本身，不处理 ae/oe/ue 的拼写变体。
使用示例 # PUT my-german-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;german_norm&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;german_normalization&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_german&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;german_norm&amp;#34;, &amp;#34;german_stemmer&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;german_normalization&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;Straße Übung Ärger&amp;#34; } 响应中 Straße → strasse、Übung → ubung、Ärger → arger。</description></item><item><title>德语词干过滤器（German Stemmer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/german-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/german-stem/</guid><description>德语词干过滤器 # german_light_stem 词元过滤器使用轻量级算法对德语文本进行词干提取。
功能说明 # Easysearch 提供多种德语词干算法：
算法 language 值 说明 轻量级 light_german 默认，最保守 最小化 minimal_german 只处理复数 Snowball german 标准 Snowball 算法 Snowball2 german2 改进的 Snowball 使用示例 # PUT my-german-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;de_stem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;light_german&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_german&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;german_normalization&amp;#34;, &amp;#34;de_stem&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;german&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;Programmierung Programmierer Programme&amp;#34; } 参数 # 参数 值 说明 type stemmer 过滤器类型 language light_german / minimal_german / german / german2 选择词干算法 在语言分析器中的位置 # 德语分析器 内置了 light_german 词干提取器。</description></item><item><title>捷克语词干过滤器（Czech Stemmer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/czech-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/czech-stem/</guid><description>捷克语词干过滤器 # czech_stemmer 词元过滤器使用 Lucene 的轻量级捷克语词干算法，移除捷克语常见的形态后缀。
功能说明 # 此过滤器使用轻量级算法，适度移除名词格变化和动词变位后缀，在词干精度和召回率之间取得平衡。
使用示例 # PUT my-czech-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;czech_stem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;czech&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_czech&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;czech_stem&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;czech&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;programátorů programování&amp;#34; } 参数 # 参数 值 说明 type stemmer 过滤器类型 language czech 指定捷克语词干算法 在语言分析器中的位置 # 捷克语分析器 内置了此过滤器。</description></item><item><title>斯堪的纳维亚字符折叠过滤器（Scandinavian Folding）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/scandinavian-folding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/scandinavian-folding/</guid><description>斯堪的纳维亚字符折叠过滤器 # scandinavian_folding 词元过滤器将斯堪的纳维亚语言中互换使用的字符统一归一化。
折叠规则 # 原始字符 折叠为 涉及语言 å a 瑞典语、丹麦语、挪威语 ä, æ a 瑞典语(ä) / 丹麦语、挪威语(æ) ö, ø o 瑞典语(ö) / 丹麦语、挪威语(ø) 与 scandinavian_normalization 的区别：scandinavian_normalization 只折叠互换字符对（如 ä↔æ），而 scandinavian_folding 还会进一步折叠到 ASCII 基础字符（如 å→a）。
使用示例 # PUT my-scandi-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;scandi_fold&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;scandinavian_folding&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_scandi&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;scandi_fold&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;scandinavian_folding&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;räksmörgås&amp;#34; } 响应中 räksmörgås → raksmørgas 或 raksmorgas（取决于折叠层级）。</description></item><item><title>斯堪的纳维亚归一化过滤器（Scandinavian Normalization）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/scandinavian-normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/scandinavian-normalization/</guid><description>斯堪的纳维亚归一化过滤器 # scandinavian_normalization 词元过滤器将斯堪的纳维亚语言中互换使用的字符对统一为一种形式，使跨语言搜索更一致。
归一化规则 # 原始字符 归一化为 说明 ä æ 瑞典语 ä → 丹麦语/挪威语 æ ö ø 瑞典语 ö → 丹麦语/挪威语 ø Ä Æ 大写同理 Ö Ø 大写同理 与 scandinavian_folding 的区别：scandinavian_normalization 只统一互换字符对（ä↔æ, ö↔ø），不会折叠到 ASCII 基础字符。而 scandinavian_folding 会进一步折叠为 a、o 等 ASCII 字符。
使用示例 # PUT my-scandi-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;scandi_norm&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;scandinavian_normalization&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_scandi&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;scandi_norm&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;scandinavian_normalization&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;räksmörgås&amp;#34; } 响应中 räksmörgås → pair ræksmørgås（ä→æ, ö→ø，但 å 保持不变）。</description></item><item><title>法语词干过滤器（French Stemmer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/french-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/french-stem/</guid><description>法语词干过滤器 # french_light_stem 词元过滤器使用 Lucene 的轻量级法语词干算法，移除法语常见的形态后缀。
功能说明 # 法语分析器默认使用轻量级词干提取（light_french），而非 Snowball 算法。轻量级算法更保守：
算法 说明 适用场景 light_french 轻量级，只移除明显后缀 默认推荐 french Snowball 完整词干 更激进的归约 minimal_french 最小化词干 最保守 使用示例 # PUT my-french-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;fr_stem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;light_french&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_french&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;elision&amp;#34;, &amp;#34;fr_stem&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;french&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;programmation programmeurs programmes&amp;#34; } 参数 # 参数 值 说明 type stemmer 过滤器类型 language light_french / french / minimal_french 选择词干算法 在语言分析器中的位置 # 法语分析器 内置了 light_french 词干提取器。</description></item><item><title>波斯语归一化过滤器（Persian Normalization）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/persian-normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/persian-normalization/</guid><description>波斯语归一化过滤器 # persian_normalization 词元过滤器对波斯语（فارسی）文本进行字符归一化，统一阿拉伯语和波斯语书写变体。
归一化规则 # 处理 说明 阿拉伯语 Yaa → 波斯语 Yaa 统一 ي → ی 阿拉伯语 Kaf → 波斯语 Kaf 统一 ك → ک 变音符号移除 移除阿拉伯语 harakat 标记 搭配 arabic_normalization 建议与 arabic_normalization 一起使用 使用示例 # PUT my-persian-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;persian_norm&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;persian_normalization&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_persian&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;arabic_normalization&amp;#34;, &amp;#34;persian_norm&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;arabic_normalization&amp;#34;, &amp;#34;persian_normalization&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;فارسی&amp;#34; } 参数 # 此过滤器不接受任何参数。</description></item><item><title>瑞典语词干过滤器（Swedish Stemmer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/swedish-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/swedish-stem/</guid><description>瑞典语词干过滤器 # swedish_stemmer 词元过滤器使用 Snowball 算法对瑞典语文本进行词干提取。
功能说明 # Easysearch 提供两种瑞典语词干算法：
算法 language 值 说明 Snowball swedish 标准 Snowball 算法 轻量级 light_swedish 更保守的词干提取 使用示例 # PUT my-swedish-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;sv_stem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;swedish&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_swedish&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;sv_stem&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;swedish&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;programmering programmerare&amp;#34; } 参数 # 参数 值 说明 type stemmer 过滤器类型 language swedish / light_swedish 选择词干算法 在语言分析器中的位置 # 瑞典语分析器 内置了 Snowball 词干提取器。</description></item><item><title>索拉尼语归一化过滤器（Sorani Normalization）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/sorani-normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/sorani-normalization/</guid><description>索拉尼语归一化过滤器 # sorani_normalization 词元过滤器对索拉尼库尔德语（سۆرانی）文本进行字符归一化。索拉尼语使用修改后的阿拉伯字母书写。
归一化规则 # 处理 说明 Yaa 归一化 统一 ي/ی 变体 Kaf 归一化 统一 ك/ک 变体 Haa 归一化 统一 haa 变体 变音符号移除 移除可选的变音标记 使用示例 # PUT my-sorani-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;sorani_norm&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;sorani_normalization&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_sorani&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;sorani_norm&amp;#34;, &amp;#34;sorani_stemmer&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;sorani_normalization&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;کوردی&amp;#34; } 参数 # 此过滤器不接受任何参数。</description></item><item><title>罗马尼亚语词干过滤器（Romanian Stemmer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/romanian-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/romanian-stem/</guid><description>罗马尼亚语词干过滤器 # romanian_stemmer 词元过滤器使用 Snowball 算法对罗马尼亚语文本进行词干提取。
功能说明 # 此过滤器移除罗马尼亚语名词的格变化和定冠词后缀，以及动词变位后缀。
使用示例 # PUT my-romanian-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;ro_stem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;romanian&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_romanian&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;ro_stem&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;romanian&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;programarea programatori&amp;#34; } 参数 # 参数 值 说明 type stemmer 过滤器类型 language romanian 指定罗马尼亚语 Snowball 词干算法 在语言分析器中的位置 # 罗马尼亚语分析器 内置了此过滤器。</description></item><item><title>荷兰语词干过滤器（Dutch Stemmer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/dutch-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/dutch-stem/</guid><description>荷兰语词干过滤器 # dutch_stemmer 词元过滤器使用 Snowball 算法对荷兰语文本进行词干提取。
功能说明 # 荷兰语词干提取使用 Snowball 算法，结合词干覆盖字典处理不规则变形：
移除常见名词/动词后缀 荷兰语分析器额外使用 stemmer_override 字典处理不规则形式 适合荷兰语和佛兰德语文本 使用示例 # PUT my-dutch-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;dutch_stem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;dutch&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_dutch&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;dutch_stem&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;analyzer&amp;#34;: &amp;#34;dutch&amp;#34;, &amp;#34;text&amp;#34;: &amp;#34;programma&amp;#39;s programmering&amp;#34; } 参数 # 参数 值 说明 type stemmer 过滤器类型 language dutch 指定荷兰语 Snowball 词干算法 可选的 dutch_kp 变体使用 Krovetz-Porter 混合算法。</description></item><item><title>阿拉伯语归一化过滤器（Arabic Normalization）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/arabic-normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/arabic-normalization/</guid><description>阿拉伯语归一化过滤器 # arabic_normalization 词元过滤器对阿拉伯语文本进行正字法归一化，将各种书写变体统一为标准形式，提高搜索的召回率。
归一化规则 # 原始形式 归一化结果 说明 \u0623 \u0625 \u0622 (带 hamza 的 alef) \u0627 (bare alef) 统一 alef 变体 \u0629 (taa marbuta) \u0647 (haa) 统一词尾形式 \u064e \u064f \u0650 \u064b \u064c \u064d (harakat) 删除 移除变音符号 \u0640 (tatweel/kashida) 删除 移除装饰性延长符 使用示例 # PUT my-arabic-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;arabic_norm&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;arabic_normalization&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_arabic&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;arabic_norm&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;arabic_normalization&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;\u0625\u0628\u0631\u0627\u0647\u064a\u0645&amp;#34; } 参数 # 此过滤器不接受任何参数。</description></item><item><title>阿拉伯语词干过滤器（Arabic Stemmer）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/arabic-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/arabic-stem/</guid><description>阿拉伯语词干过滤器 # arabic_stemmer 词元过滤器使用 Lucene 的 ArabicStemmer 对阿拉伯语词元进行词干提取，去除常见的前缀和后缀。
词干规则 # 此词干提取器基于 Shereen Khoja 的轻量级方法，处理以下词缀：
类型 示例 定冠词前缀 ال (al-) 介词前缀 و (wa-), ب (bi-), ك (ka-) 代词后缀 ها, هم, هن 等 阴性/双数/复数后缀 ة, ات, ين 等 使用示例 # PUT my-arabic-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;arabic_stem&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stemmer&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;arabic&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_arabic&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;lowercase&amp;#34;, &amp;#34;arabic_normalization&amp;#34;, &amp;#34;arabic_stem&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;arabic_normalization&amp;#34;, &amp;#34;stemmer&amp;#34;], &amp;#34;text&amp;#34;: &amp;#34;الكتابات&amp;#34; } 参数 # 通过 stemmer 过滤器使用时：</description></item><item><title>ICU 排序过滤器（ICU Collation）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/icu-collation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/icu-collation/</guid><description>ICU 排序过滤器 # icu_collation_keyword 词元过滤器使用 ICU 排序规则将词元转换为排序键（collation key），实现语言感知的排序和范围查询。
前提条件 # 需要安装 analysis-icu 插件：
bin/easysearch-plugin install analysis-icu 功能说明 # 此过滤器将文本词元转换为 ICU CollationKey 的字节表示。转换后的词元可用于：
语言感知排序：按特定语言的排序规则排列结果 范围查询：在 keyword 字段上执行符合语言习惯的范围过滤 重音/大小写不敏感匹配：通过调整排序强度控制匹配精度 使用示例 # 基本用法 — 德语排序 # PUT my-german-sort { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;german_collation&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;icu_collation_keyword&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;de&amp;#34;, &amp;#34;country&amp;#34;: &amp;#34;DE&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;german_sort&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;keyword&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;german_collation&amp;#34;] } } } }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;name&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;fields&amp;#34;: { &amp;#34;sort&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;keyword&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;german_sort&amp;#34; } } } } } } 忽略重音的匹配 # PUT my-accent-insensitive { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;collation_primary&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;icu_collation_keyword&amp;#34;, &amp;#34;language&amp;#34;: &amp;#34;en&amp;#34;, &amp;#34;strength&amp;#34;: &amp;#34;primary&amp;#34; } } } } } 参数 # 参数 类型 说明 language string ICU 语言代码（如 de, fr, zh） country string ICU 国家代码（如 DE, FR） strength string 排序强度：primary（忽略重音+大小写）、secondary（区分重音）、tertiary（区分大小写）、quaternary、identical decomposition string Unicode 分解模式：no、canonical alternate string 空白/标点处理：shifted（忽略）、non-ignorable caseLevel boolean 是否在 primary 强度下区分大小写 caseFirst string 大小写优先：lower、upper numeric boolean 是否按数字值排序（true 使 &amp;ldquo;2&amp;rdquo; &amp;lt; &amp;ldquo;10&amp;rdquo;） rules string 自定义 ICU 排序规则字符串 相关链接 # ICU 分析器 ICU Transform 过滤器 文本分析</description></item><item><title>ICU 脚本转换过滤器（ICU Transform）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/icu-transform/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/icu-transform/</guid><description>ICU 脚本转换过滤器 # icu_transform 词元过滤器使用 ICU 的 Transliterator 引擎将文本从一种脚本转写为另一种脚本。
前提条件 # 需要安装 analysis-icu 插件：
bin/easysearch-plugin install analysis-icu 功能说明 # 此过滤器可以实现：
脚本转写：中文→拉丁、西里尔→拉丁、阿拉伯→拉丁等 大小写转换：Upper、Lower、Title Unicode 归一化：NFC、NFD、NFKC、NFKD 自定义转换规则 使用示例 # 西里尔字母转拉丁字母 # PUT my-transliterate-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;cyrillic_to_latin&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;icu_transform&amp;#34;, &amp;#34;id&amp;#34;: &amp;#34;Cyrillic-Latin&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;my_translit&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;cyrillic_to_latin&amp;#34;] } } } } } 中文转拼音 # PUT my-pinyin-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;han_to_latin&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;icu_transform&amp;#34;, &amp;#34;id&amp;#34;: &amp;#34;Han-Latin&amp;#34; } } } } } 链式转换 # PUT my-chain-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;to_ascii&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;icu_transform&amp;#34;, &amp;#34;id&amp;#34;: &amp;#34;Cyrillic-Latin; NFD; [:Nonspacing Mark:] Remove; NFC&amp;#34; } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;filter&amp;#34;: [{&amp;#34;type&amp;#34;: &amp;#34;icu_transform&amp;#34;, &amp;#34;id&amp;#34;: &amp;#34;Cyrillic-Latin&amp;#34;}], &amp;#34;text&amp;#34;: &amp;#34;Москва&amp;#34; } 响应：Moskva</description></item><item><title>拼音过滤器（Pinyin Filter）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/pinyin-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/pinyin-filter/</guid><description>拼音过滤器 # pinyin 词元过滤器将中文词元转换为拼音表示，来自 analysis-pinyin 插件。
前提条件 # 需要安装 analysis-pinyin 插件：
bin/easysearch-plugin install analysis-pinyin 功能说明 # 此过滤器可以：
将汉字转换为拼音全拼或首字母 保留或移除原始中文词元 支持多种输出格式组合 使用示例 # PUT my-pinyin-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;my_pinyin&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;pinyin&amp;#34;, &amp;#34;keep_full_pinyin&amp;#34;: true, &amp;#34;keep_first_letter&amp;#34;: true, &amp;#34;keep_original&amp;#34;: true, &amp;#34;limit_first_letter_length&amp;#34;: 16 } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;pinyin_analyzer&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;ik_smart&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;my_pinyin&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;ik_smart&amp;#34;, &amp;#34;filter&amp;#34;: [{&amp;#34;type&amp;#34;: &amp;#34;pinyin&amp;#34;, &amp;#34;keep_full_pinyin&amp;#34;: true}], &amp;#34;text&amp;#34;: &amp;#34;中华人民共和国&amp;#34; } 参数 # 参数 默认值 说明 keep_first_letter true 保留拼音首字母（如 &amp;ldquo;中国&amp;rdquo; → zg） keep_full_pinyin true 保留全拼（如 &amp;ldquo;中&amp;rdquo; → zhong） keep_joined_full_pinyin false 连接全拼（如 &amp;ldquo;中国&amp;rdquo; → zhongguo） keep_original false 保留原始中文词元 keep_none_chinese true 保留非中文字符 keep_none_chinese_in_first_letter true 首字母模式中保留非中文 keep_none_chinese_together true 非中文字符连续保留 none_chinese_pinyin_tokenize true 拼音字母也参与分词 limit_first_letter_length 16 首字母最大长度 lowercase true 拼音小写 trim_whitespace true 移除空白 remove_duplicated_term false 移除重复词项 相关链接 # 拼音分析器 拼音分词器</description></item><item><title>简繁转换过滤器（STConvert Filter）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stconvert-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/stconvert-filter/</guid><description>简繁转换过滤器 # stconvert 词元过滤器在简体中文和繁体中文之间进行转换，来自 analysis-stconvert 插件。
前提条件 # 需要安装 analysis-stconvert 插件：
bin/easysearch-plugin install analysis-stconvert 功能说明 # 此过滤器支持：
简体 → 繁体（s2t） 繁体 → 简体（t2s） 可用于实现跨简繁的统一搜索。
使用示例 # PUT my-stconvert-index { &amp;#34;settings&amp;#34;: { &amp;#34;analysis&amp;#34;: { &amp;#34;filter&amp;#34;: { &amp;#34;to_simplified&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;stconvert&amp;#34;, &amp;#34;convert_type&amp;#34;: &amp;#34;t2s&amp;#34; } }, &amp;#34;analyzer&amp;#34;: { &amp;#34;unified_chinese&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;custom&amp;#34;, &amp;#34;tokenizer&amp;#34;: &amp;#34;ik_smart&amp;#34;, &amp;#34;filter&amp;#34;: [&amp;#34;to_simplified&amp;#34;] } } } } } 测试效果 # GET /_analyze { &amp;#34;tokenizer&amp;#34;: &amp;#34;ik_smart&amp;#34;, &amp;#34;filter&amp;#34;: [{&amp;#34;type&amp;#34;: &amp;#34;stconvert&amp;#34;, &amp;#34;convert_type&amp;#34;: &amp;#34;t2s&amp;#34;}], &amp;#34;text&amp;#34;: &amp;#34;計算機程式設計&amp;#34; } 响应：计算机 程式 设计</description></item><item><title>俄语形态分词过滤器（Russian Morphology）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/russian-morphology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/russian-morphology/</guid><description>Russian Morphology 分词过滤器 # russian_morphology 分词过滤器基于 Lucene Morphology 库，专门用于解决俄语搜索中&amp;quot;搜不全&amp;quot;和&amp;quot;语义偏差&amp;quot;的痛点。俄语由于其极其复杂的**变格（Declension）和变位（Conjugation）**系统，是形态分析最具挑战性的语言之一。
相关指南（先读这些） # 文本分析：词干提取 文本分析：规范化 核心处理逻辑 # 该过滤器在处理俄语文本时具备以下核心能力：
变格还原：处理名词、形容词、代词的 6 个格位及单复数变化。 示例：автомобили (复数) 和 автомобилем (单数工具格) 都会还原为 автомоль (汽车)。 动词变位还原：处理动词的人称、时态和语气。 示例：бежал (跑/过去时) 还原为 бежать (跑/原型)。 多路径歧义处理 (Homonymy)：当一个词形对应多个原型时，同时索引它们以防漏搜。 示例：Мире 会同时产生 мир (世界/和平) 和 миро (圣油)。 俄语形态分析的必要性 # 与英语不同，俄语的一个单词根据格、性、数、时态的变化，可能会产生数十种不同的拼写形式。
普通分词器（如 Snowball）：只能简单地去掉词尾（如 -ом, -ами），经常导致词根被错误切分。 形态过滤器 (russian_morphology)：通过查阅俄语语言学词典，将所有变形统一回归到其 第一格（Nominative）或动词不定式（Infinitive） 原型。 安装与使用 # 详见 俄语形态分词器 部分。</description></item><item><title>英语形态分词过滤器（English Morphology）</title><link>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/english-morphology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/main/docs/features/mapping-and-analysis/text-analysis/token-filters/english-morphology/</guid><description>English Morphology 分词过滤器 # english_morphology 分词过滤器是基于 Lucene Morphology 库的语言处理组件。与传统的算法分词器不同，它不依赖于简单的字符裁剪规则，而是通过语言学词典对单词进行深度的词形还原（Lemmatization）。
相关指南（先读这些） # 文本分析：词干提取 文本分析：规范化 核心处理逻辑 # 该过滤器在处理文本时遵循“字典比对 + 语义还原”的原则，其核心逻辑包括：
屈折变化还原 (Inflectional)：处理动词时态（went → go）、名词单复数（children → child）等。 派生词识别 (Derivational)：识别单词间的词根关联，如将“执行者名词”关联至“动作动词”（runner → run）。 多路径索引 (Token Expansion)：当一个单词具有多重身份时，同时保留原词和还原后的原型（如 running → running, run）。 安装与使用 # 详见 英语形态分词器 部分。</description></item></channel></rss>