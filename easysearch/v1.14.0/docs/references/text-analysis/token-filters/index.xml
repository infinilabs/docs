<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>分词过滤器 on INFINI Easysearch</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/</link><description>Recent content in 分词过滤器 on INFINI Easysearch</description><generator>Hugo -- gohugo.io</generator><atom:link href="/easysearch/v1.14.0/docs/references/text-analysis/token-filters/index.xml" rel="self" type="application/rss+xml"/><item><title>ASCII 折叠分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/ascii-folding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/ascii-folding/</guid><description>ASCII 折叠分词过滤器 # ASCII 折叠(asciifolding)分词过滤器将非 ASCII 字符转换为与其最接近的 ASCII 等效字符。例如，“é” 变为 “e”，“ü” 变为 “u”，“ñ” 变为 “n”。这个过程被称为&amp;quot;音译&amp;quot;。
ASCII 折叠分词过滤器有许多优点：
增强搜索灵活性：用户在输入查询内容时常常会省略重音符号或特殊字符。ASCII 折叠分词过滤器可确保即使是这样的查询也仍然能返回相关结果。 规范化：通过确保重音字符始终被转换为其 ASCII 等效字符，使索引编制过程标准化。 国际化：对于包含多种语言和字符集的应用程序特别有用。 尽管 ASCII 折叠分词过滤器可以简化搜索，但它也可能导致特定信息的丢失，尤其是当数据集中重音字符和非重音字符之间的区别很重要的时候。
参数说明 # 你可以使用 preserve_original 参数来配置 ASCII 折叠分词过滤器。将此参数设置为 true 时，会在词元流中同时保留原始词元及其 ASCII 折叠后的版本。当你希望在搜索查询中同时匹配一个词项的原始版本（带重音符号）和规范化版本（不带重音符号）时，这一点会特别有用。该参数的默认值为 false。
参考样例 # 以下示例请求创建了一个名为 example_index 的新索引，并定义了一个分词器，该分词器使用了 ASCII 折叠过滤器，且将 preserve_original 参数设置为 true：
PUT /example_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;custom_ascii_folding&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;asciifolding&amp;quot;, &amp;quot;preserve_original&amp;quot;: true } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_ascii_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;custom_ascii_folding&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>CJK 二元分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/cjk-bigram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/cjk-bigram/</guid><description>CJK 二元分词过滤器 # CJK 二元(cjk_bigram)分词过滤器是专门为处理东亚语言（如中文、日文和韩文，简称 CJK）而设计的，这些语言通常不使用空格来分隔单词。二元组是指在词元字符串中两个相邻元素的序列，这些元素可以是字符或单词。对于 CJK 语言来说，二元组有助于近似确定单词边界，并捕捉那些能够传达意义的重要字符对。
参数说明 # CJK 二元分词过滤器可以通过两个参数进行配置：ignore_scripts（忽略字符集）和 output_unigrams（输出一元组）。
ignore_scripts（忽略字符集） # CJK 二元分词过滤器会忽略所有非 CJK 字符集（如拉丁字母或西里尔字母等书写系统），并且仅将 CJK 文本分词为二元组。可使用此选项指定要忽略的 CJK 字符集。该选项有以下有效值：
han（汉字）：汉字字符集处理汉字。汉字是用于中国、日本和韩国书面语言的意符文字。该过滤器可帮助处理文本相关任务，例如对用中文、日文汉字或韩文汉字书写的文本进行分词、规范化或词干提取。 hangul（韩文）：韩文（谚文）字符集处理韩文字符，这些字符是韩语所特有的，不存在于其他东亚字符集中。 hiragana（平假名）：平假名字符集处理平假名，平假名是日语书写系统中使用的两种音节文字之一。平假名通常用于日语的本土词汇、语法元素以及某些形式的标点符号。 katakana（片假名）：片假名字符集处理片假名，片假名是日语的另一种音节文字。片假名主要用于外来借词、拟声词、科学名称以及某些日语词汇。 output_unigrams（输出一元组） # 当此选项设置为 true 时，会同时输出一元组（单个字符）和二元组。默认值为 false。
参考样例 # 以下示例请求创建了一个名为 devanagari_example_index 的新索引，并定义了一个分词器，该分词器使用了 cjk_bigram_filter 过滤器，且将 ignored_scripts 参数设置为 katakana（片假名）：
PUT /cjk_bigram_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;cjk_bigrams_no_katakana&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;cjk_bigrams_no_katakana_filter&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;cjk_bigrams_no_katakana_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;cjk_bigram&amp;quot;, &amp;quot;ignored_scripts&amp;quot;: [ &amp;quot;katakana&amp;quot; ], &amp;quot;output_unigrams&amp;quot;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>CJK 宽度分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/cjk-width/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/cjk-width/</guid><description>CJK 宽度分词过滤器 # CJK 宽度(cjk_width)分词过滤器通过将全角 ASCII 字符转换为其标准（半角）ASCII 等效字符，并将半角片假名字符转换为全角等效字符，来对中文、日文和韩文（CJK）词元进行规范化处理。
转换全角 ASCII 字符 # 在 CJK 文本中，ASCII 字符（如字母和数字）可能会以全角形式出现，占据两个半角字符的空间。全角 ASCII 字符在东亚排版中通常用于与 CJK 字符的宽度对齐。然而，出于索引和搜索的目的，这些全角字符需要规范化为其标准（半角）ASCII 等效字符。
以下示例说明了 ASCII 字符的规范化过程：
全角: ＡＢＣＤＥ １２３４５ 规范化 (半角): ABCDE 12345 转换半角片假名字符 # CJK 宽度分词过滤器会将半角片假名字符转换为对应的全角片假名字符，全角片假名是日语文本中使用的标准形式。如下例所示，这种规范化处理对于文本处理和搜索的一致性至关重要：
半角 katakana: ｶﾀｶﾅ 规范化 (全角) katakana: カタカナ 参考样例 # 以下示例请求创建了一个名为 cjk_width_example_index 的新索引，并定义了一个包含 cjk_width 过滤器的分词器：
PUT /cjk_width_example_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;cjk_width_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;cjk_width&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>KStem 分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/kstem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/kstem/</guid><description>KStem 分词过滤器 # KStem 分词过滤器是一种词干提取过滤器，用于将单词还原为其词根形式。该过滤器是一种专为英语设计的轻量级算法词干提取器，它执行以下词干提取操作：
将复数形式的单词还原为单数形式。 将不同的动词时态转换为其基本形式。 去除常见的派生词尾，例如 “-ing” 或 “-ed”。 KStem 分词过滤器等同于配置了 light_english 语言的词干提取器过滤器。与其他词干提取过滤器（如词干提取器 porter_stem）相比，它提供了更为保守的词干提取方式。
KStem 分词过滤器基于 Lucene 的 KStemFilter。如需了解更多信息，请参阅 Lucene 文档。
参考样例 # 以下示例请求创建了一个名为 my_kstem_index 的新索引，并配置了一个带有 KStem 过滤器的分词器：
PUT /my_kstem_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;kstem_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;kstem&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_kstem_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;kstem_filter&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_kstem_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>Kuromoji 补全分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/kuromoji-completion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/kuromoji-completion/</guid><description>Kuromoji 补全分词过滤器 # Kuromoji 补全（kuromoji_completion）分词过滤器用于对日语中的片假名单词进行词干提取，片假名单词常常用于表示外来词或借词。这个过滤器在自动补全或建议查询中特别有用，在这些查询中，对片假名单词的部分匹配可以扩展为包含它们的完整形式。
要使用此分词过滤器，你必须首先在所有节点上安装 analysis-kuromoji 插件，方法是运行 bin/easysearch-plugin install analysis-kuromoji，然后重新启动集群。
参考样例 # 以下示例请求创建了一个名为 kuromoji_sample 的新索引，并配置了一个带有 Kuromoji 补全过滤器的分词器：
PUT kuromoji_sample { &amp;quot;settings&amp;quot;: { &amp;quot;index&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;kuromoji_tokenizer&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_katakana_stemmer&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;my_katakana_stemmer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;kuromoji_completion&amp;quot; } } } } } } 产生的词元 # 使用以下请求，通过输入翻译为“使用电脑”的文本，来查看使用该分词器生成的词元。
POST /kuromoji_sample/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;コンピューターを使う&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;コンピューター&amp;quot;, // 原片假名单词“computer”（在日语中用片假名表示的话，比如 コンピューター ）。 &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;konpyuーtaー&amp;quot;, // “コンピューター” 的罗马字拼写（罗马音）版本是 “konpyūtā”。 &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;konnpyuーtaー&amp;quot;, // “コンピューター” 的另一种可能的罗马字拼写版本可能是 “kompyuuta”。这种差异可能是由于不同的罗马字转写系统 &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;を&amp;quot;, // 一个日语助词，“wo” 或 “o” &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 8, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;wo&amp;quot;, // 助词「を」（通常发音为“o”）的罗马字拼写形式。 &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 8, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;o&amp;quot;, // 另一种罗马字转写版本。 &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 8, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;使う&amp;quot;, // 日语中表示 “使用” 的动词 &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;tukau&amp;quot;, // “使う”（つかう）的罗马字版本是 “tsukau”。 &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;tsukau&amp;quot;, // “使う”的另一种罗马字转写形式，其中“つ”的写法在语音上更为准确。 &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>n-gram 分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/n-gram/</guid><description>n-gram 分词过滤器 # n-gram 分词过滤器是一种强大的工具，用于将文本拆分为更小的组件，即 n-gram，这有助于提升部分匹配和模糊搜索能力。它通过将一个词元拆分成指定长度的子字符串来工作。这些过滤器在搜索应用程序中很常用，可用于支持自动补全、部分匹配以及容错拼写搜索。
参数说明 # n-gram 分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_gram 可选 整数 n-gram 的最小长度。默认值为 1。 max_gram 可选 整数 n-gram 的最大长度。默认值为 2。 preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 false。 参考样例 # 以下示例请求创建了一个名为“ngram_example_index”的新索引，并配置了一个带有 n-gram 过滤器的分词器：
PUT /ngram_example_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;ngram_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 2, &amp;quot;max_gram&amp;quot;: 3 } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;ngram_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;ngram_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>保留类型分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keep-types/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keep-types/</guid><description>保留类型分词过滤器 # 保留类型（keep_types）分词过滤器是一种用于文本分析的分词过滤器，它可用于控制保留或丢弃哪些词元类型。不同的分词器会产生不同的词元类型，例如 &amp;lt;HOST&amp;gt;、&amp;lt;NUM&amp;gt; 或 &amp;lt;ALPHANUM&amp;gt;。
分词器keyword）、简单匹配（simple_pattern）和简单匹配拆分（simple_pattern_split）分词器不支持保留类型分词过滤器，因为这些分词器不支持词元类型属性。
参数说明 # 保留类型分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 types 必需 字符串列表 要保留或丢弃的词元类型列表（由 mode 参数决定）。 mode 可选 字符串 是要包含(include)还是排除(exclude) types 中指定的词元类型。默认值为 include（包含）。 参考样例 # 以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有保留类型过滤器的分析器：
PUT /test_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;keep_types_filter&amp;quot;] } }, &amp;quot;filter&amp;quot;: { &amp;quot;keep_types_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keep_types&amp;quot;, &amp;quot;types&amp;quot;: [&amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>修剪词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/trim/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/trim/</guid><description>修剪词元过滤器 # 修剪(trim)词元过滤器会从词元中去除前导和尾随的空白字符。
许多常用的分词器，例如标准(standard)分词器、关键字(keyword)分词器和空白(whitespace)分词器，在分词过程中会自动去除前导和尾随的空白字符。当使用这些分词器时，无需额外配置修剪词元过滤器。
参考样例 # 以下示例请求创建了一个名为 my_pattern_trim_index 的新索引，并配置了一个带有修剪过滤器和匹配词元生成器的分词器。其中，匹配词元生成器不会去除词元的前导和尾随空白字符。
PUT /my_pattern_trim_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_trim_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;trim&amp;quot; } }, &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_pattern_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;,&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_trim_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_pattern_tokenizer&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_trim_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
GET /my_pattern_trim_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_trim_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot; Easysearch , is , powerful &amp;quot; } 返回内容包含产生的词元</description></item><item><title>停用词词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stop/</guid><description>停用词词元过滤器 # 停用词（stop）词元过滤器用于在分词过程中从词元流中去除常见词汇（也称为停用词）。停用词通常是冠词和介词，比如“a” 或 “for” 。这些词在搜索查询中意义不大，并且常常被排除在外，以提高搜索效率和相关性。
默认的英语停用词列表包含以下单词：a、an、and、are、as、at、be、but、by、for、if、in、into、is、it、no、not、of、on、or、such、that、the、their、then、there、these、they、this、to、was、will 以及 with。
参数说明 # 停用词词元过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 stopwords 可选 字符串 既可以指定自定义的停用词数组，也可以指定一种语言以获取预定义的 Lucene 停用词列表。可指定的语言如下：
- _arabic_
- _armenian_
- _basque_
- _bengali_
- _brazilian_（巴西葡萄牙语）
- _bulgarian_
- _catalan_
- _cjk_（中文、日语和韩语）
- _czech_
- _danish_
- _dutch_
- _english_（默认值）
- _estonian_
- _finnish_
- _french_
- _galician_
- _german_
- _greek_
- _hindi_
- _hungarian_</description></item><item><title>关键词标记分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keyword-marker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keyword-marker/</guid><description>关键词标记分词过滤器 # 关键词标记（keyword_marker）分词过滤器用于防止某些词元被词干提取器或其他过滤器修改。该过滤器通过将指定的词元标记为关键词来实现这一点，这样就能避免词干提取或其他方式的处理。这确保了特定的单词能保持其原始形式。
参数说明 # 关键词标记分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 ignore_case 可选 布尔值 在匹配关键词时是否忽略字母大小写。默认值为false。 keywords 若未设置 keywords_path 或 keywords_pattern 则为必需 字符串列表 要标记为关键词的词元列表。 keywords_path 若未设置 keywords 或 keywords_pattern 则为必需 字符串 关键词列表的路径（相对于配置目录或绝对路径）。 keywords_pattern 若未设置 keywords 或 keywords_path 则为必需 字符串 用于匹配要标记为关键词的词元的正则表达式。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词标记过滤器的分词器。该过滤器将单词 example 标记为关键词：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;keyword_marker_filter&amp;quot;, &amp;quot;stemmer&amp;quot;] } }, &amp;quot;filter&amp;quot;: { &amp;quot;keyword_marker_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword_marker&amp;quot;, &amp;quot;keywords&amp;quot;: [&amp;quot;example&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>关键词重复分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keyword-repeat/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keyword-repeat/</guid><description>关键词重复分词过滤器 # 关键词重复（keyword_repeat）分词过滤器会将词元的关键词版本发松到词元流中。当你希望在经过进一步的词元转换（例如词干提取或同义词扩展）之后，既保留原始词元，又保留其修改后的版本时，通常会使用这个过滤器。这些重复的词元使得原始的、未改变的词元版本能够与修改后的版本一起保留在最终的分析结果中。
关键词重复分词过滤器应该放置在词干提取过滤器之前。词干提取并非应用于每个词元，因此在词干提取之后，你可能会在同一位置得到重复的词元。为了去除重复词元，可以在词干提取器之后使用 remove_duplicates 分词过滤器。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词重复过滤器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_kstem&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;kstem&amp;quot; }, &amp;quot;my_lowercase&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_lowercase&amp;quot;, &amp;quot;keyword_repeat&amp;quot;, &amp;quot;my_kstem&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Stopped quickly&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;stopped&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;stop&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;quickly&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;quick&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 } ] } 你可以通过在 _analyze 查询中添加以下参数，进一步研究关键词重复分词过滤器的影响：</description></item><item><title>分隔式负载分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/delimited-payload/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/delimited-payload/</guid><description>分隔式负载分词过滤器 # 分隔式负载（delimited_payload）分词过滤器用于在分词过程中解析包含负载的词元。例如，字符串 red|1.5 fast|2.0 car|1.0 会被解析为词元 red（负载为 1.5）、fast（负载为 2.0）和 car（负载为 1.0）。当你的词元包含额外的关联数据（如权重、分数或其他数值），且你打算将这些数据用于评分或自定义查询逻辑时，这个过滤器就特别有用。该过滤器可以处理不同类型的负载，包括整数、浮点数和字符串，并将负载（额外的元数据）附加到词元上。
在文本分词时，分隔式负载分词过滤器会解析每个词元，提取负载并将其附加到词元上。后续在查询中可以使用这个负载来影响评分、提升权重或实现其他自定义行为。
负载以 Base64 编码的字符串形式存储。默认情况下，负载不会随词元一起在查询响应中返回。若要返回负载，你必须配置额外的参数。更多信息，请参阅“带有负载存储的分词示例”。
参数说明 # 分隔式负载分词过滤器有两个参数：
参数 必需/可选 数据类型 描述 encoding 可选 字符串 指定附加到词元的负载的数据类型。这决定了在分析和查询期间如何解释负载数据。
有效值为：
- float：使用 IEEE 754 格式将负载解释为 32 位浮点数（例如，car\|2.5 中的 2.5）。
- identity：将负载解释为字符序列（例如，在 user\|admin 中，admin 被解释为字符串）。
- int：将负载解释为 32 位整数（例如，priority \| 1中的1）。
默认值为 float。 delimiter 可选 字符串 指定在输入文本中分隔词元及其负载的字符。默认值为竖线字符（\|）。 不带负载存储的分词示例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有分隔式负载过滤器的分词器：</description></item><item><title>匹配替换词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/pattern-replace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/pattern-replace/</guid><description>匹配替换词元过滤器 # 匹配替换词元过滤器允许您使用正则表达式来修改词元。此过滤器会将词元中的模式替换为指定的值，这使您在对词元进行索引之前，能够灵活地转换或规范化词元。当您在分析过程中需要清理或规范文本时，该过滤器尤其有用。
参数说明 # 匹配替换词元过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 pattern 必需 字符串 一个正则表达式模式，用于匹配需要被替换的文本。 all 可选 布尔值 是否替换所有匹配的模式。如果为 false，则仅替换第一个匹配项。默认值为 true。 replacement 可选 字符串 用于替换匹配模式的字符串。默认值为空字符串。 参考样例 # 以下示例请求创建了一个名为 text_index 的新索引，并配置了一个带有匹配替换过滤器的分词器，用于将包含数字的词元替换为字符串 [NUM]：
PUT /text_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;number_replace_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;\\d+&amp;quot;, &amp;quot;replacement&amp;quot;: &amp;quot;[NUM]&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;number_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;number_replace_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>十进制数字分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/decimal-digit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/decimal-digit/</guid><description>十进制数字分词过滤器 # 十进制数字（decimal_digit）分词过滤器用于将各种字符集中的十进制数字字符（0 到 9）规范化为它们对应的 ASCII 字符。当你希望在文本分析中确保所有数字都能被统一处理，而不管这些数字是以何种字符集书写时，这个过滤器就非常有用。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有十进制数字过滤器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_decimal_digit_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;decimal_digit&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;my_decimal_digit_filter&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;123 ١٢٣ १२३&amp;quot; } text分词：
“123”（ASCII 数字） “١٢٣”（阿拉伯 - 印度数字） “१२३”（梵文数字） 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;123&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 3, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;123&amp;quot;, &amp;quot;start_offset&amp;quot;: 4, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;123&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 11, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>单词分隔符图词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/word-delimiter-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/word-delimiter-graph/</guid><description>单词分隔符图词元过滤器 # 单词分隔符图(word_delimiter_graph)词元过滤器用于依据预定义的字符对词元进行拆分，还能基于可定制的规则对词元进行可选的规范化处理。
单词分隔符图词元过滤器可用于去除零件编号或产品 ID 这类复杂标识符中的标点符号。在这种情况下，它最好与关键字分词器搭配使用。对于带有连字符的单词，建议使用同义词图词元过滤器而非单词分隔符图词元过滤器，因为用户在搜索这些词时，往往既会搜索带连字符的形式，也会搜索不带连字符的形式。
默认情况下，该过滤器会应用以下规则：
描述 输入 输出 将非字母数字字符视为分隔符 ultra-fast ultra, fast 去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder 当字母大小写发生转换时拆分词元 Easysearch Easy, search 当字母和数字之间发生转换时拆分词元 T1000 T, 1000 去除词元末尾的所有格形式（&amp;lsquo;s） John's John 重要的是，不要将此过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并且会影响诸如 catenate_all 或 preserve_original 之类的选项。我们建议将此过滤器与关键字(keyword)分词器或空白(whitespace)分词器搭配使用。
参数说明 # 你可以使用以下参数来配置单词分隔符图词元过滤器。
参数 必需/可选 数据类型 描述 adjust_offsets 可选 布尔值 决定是否要为拆分或连接后的词元重新计算词元偏移量。若为 true，过滤器会调整词元偏移量，以准确呈现词元在词元流中的位置。这种调整能确保词元在文本中的位置与处理后的修改形式相匹配，这对高亮显示或短语查询等应用特别有用。若为 false，偏移量保持不变，这可能会导致处理后的词元映射回原始文本位置时出现错位。如果你的分词器使用了像 trim 这类会改变词元长度但不改变偏移量的过滤器，建议将此参数设为 false。默认值为 true。 catenate_all 可选 布尔值 从一系列字母数字部分生成连接后的词元。例如，quick-fast-200 会变成 [ quickfast200, quick, fast, 200 ]。默认值为 false。 catenate_numbers 可选 布尔值 连接数字序列。例如，10-20-30 会变成 [ 102030, 10, 20, 30 ]。默认值为 false。 catenate_words 可选 布尔值 连接字母单词。例如，high-speed-level 会变成 [ highspeedlevel, high, speed, level ]。默认值为 false。 generate_number_parts 可选 布尔值 若为 true，输出中会包含纯数字词元（仅由数字组成的词元）。默认值为 true。 generate_word_parts 可选 布尔值 若为 true，输出中会包含纯字母词元（仅由字母字符组成的词元）。默认值为 true。 ignore_keywords 可选 布尔值 是否处理标记为关键字的词元。默认值为 false。 preserve_original 可选 布尔值 在输出中，除了生成的词元外，还保留原始词元（可能包含非字母数字分隔符）。例如，auto-drive-300 会变成 [ auto - drive - 300, auto, drive, 300 ]。如果为 true，该过滤器会生成索引不支持的多位置词元，因此请勿在索引分词器中使用此过滤器，或者在该过滤器之后使用 flatten_graph 过滤器。默认值为 false。 protected_words 可选 字符串数组 指定不应被拆分的词元。 protected_words_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含不应被分隔的词元，词元需分行列出。 split_on_case_change 可选 布尔值 在连续字母大小写不同（一个为小写，另一个为大写）的位置拆分词元。例如，“EasySearch” 会变成 [ Easy, Search ]。默认值为 true。 split_on_numerics 可选 布尔值 在连续字母和数字的位置拆分词元。例如，v8engine 会变成 [ v, 8, engine ]。默认值为 true。 stem_english_possessive 可选 布尔值 去除英语所有格结尾，如 &amp;lsquo;s。默认值为 true。 type_table 可选 字符串数组 一个自定义映射，用于指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。例如，要将连字符（-）视为字母数字字符，可指定 [&amp;quot;- =&amp;gt; ALPHA&amp;quot;]，这样单词就不会在连字符处拆分。有效类型有：</description></item><item><title>单词分隔符词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/word-delimiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/word-delimiter/</guid><description>单词分隔符词元过滤器 # 单词分隔符(word_delimiter)词元过滤器用于根据预定义的字符拆分词元，还能根据可定制规则对词元进行可选的规范化处理。
我们建议尽可能使用 word_delimiter_graph 过滤器而非 word_delimiter 过滤器，因为 word_delimiter 过滤器有时会生成无效的词元图。
word_delimiter 过滤器可用于从零件编号或产品 ID 等复杂标识符中去除标点符号。在这种情况下，它最好与关键字分词器配合使用。对于带连字符的单词，建议使用同义词图(synonym_graph)词元过滤器而非 word_delimiter 过滤器，因为用户搜索这些词项时，常常既会搜索带连字符的形式，也会搜索不带连字符的形式。
默认情况下，该过滤器应用以下规则：
描述 输入 输出 将非字母数字字符视为分隔符 ultra-fast ultra, fast 去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder 当字母大小写发生转换时拆分词元 Easysearch Easy, search 当字母和数字之间发生转换时拆分词元 T1000 T, 1000 去除词元末尾的所有格形式（&amp;lsquo;s） John's John 重要提示：不要将该过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并影响诸如 catenate_all 或 preserve_original 等选项的效果。我们建议将此过滤器与关键(keyword)字分词器或空白(whitespace)分词器配合使用。
参数说明 # 你可以使用以下参数配置单词分隔符词元过滤器。</description></item><item><title>去重词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/remove-duplicates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/remove-duplicates/</guid><description>去重词元过滤器 # 去重（remove_duplicates）词元过滤器用于去除在分词过程中在相同位置生成的重复词元。
参考样例 # 以下示例请求创建了一个带有 keyword_repeat（关键词重复）词元过滤器的索引。该过滤器会在每个词元自身所在的相同位置添加该词元的关键词版本，然后使用 kstem（K 词干提取）过滤器来创建该词元的词干形式。
PUT /example-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;keyword_repeat&amp;quot;, &amp;quot;kstem&amp;quot; ] } } } } } 使用以下请求来分析字符串 “Slower turtle”。
GET /example-index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;custom_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Slower turtle&amp;quot; } 返回内容中在同一位置包含了两次词元 “turtle”。
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;slower&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;slow&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;turtle&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;turtle&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 } ] } 可以通过在索引设置中添加一个去重词元过滤器来移除重复的词元。</description></item><item><title>反转词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/reverse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/reverse/</guid><description>反转词元过滤器 # 反转（reverse）词元过滤器会反转每个词元中字符的顺序，这样在分析过程中，后缀信息就会位于反转后词元的开头。
这对于基于后缀的搜索很有用：
反转词元过滤器在你需要进行基于后缀的搜索时很有帮助，例如以下场景：
后缀匹配：根据单词的后缀来搜索单词，比如识别以特定后缀结尾的单词（例如 -tion 或 -ing）。 文件扩展名搜索：通过文件的扩展名来搜索文件，例如 .txt 或 .jpg。 自定义排序或排名：通过反转词元，你可以基于后缀实现独特的排序或排名逻辑。 后缀自动补全：实现基于后缀而非前缀的自动补全建议。 参考说明 # 以下示例请求创建了一个名为 my-reverse-index 的新索引，并配置了一个带有反转过滤器的分词器。
PUT /my-reverse-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;reverse_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;reverse&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_reverse_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;reverse_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
GET /my-reverse-index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_reverse_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;hello world&amp;quot; } 返回内容包含产生的词元</description></item><item><title>同义词图词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/synonym-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/synonym-graph/</guid><description>同义词图词元过滤器 # 同义词图(synonym_graph)词元过滤器是同义词词元过滤器的更高级版本。它支持多词同义词，并能跨多个词元处理同义词，这使得它非常适合处理短语或者那些词元之间关系很重要的场景。
参数说明 # 同义词图词元过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。 synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。 lenient 可选 布尔值 在加载规则配置时是否忽略异常。默认值为 false。 format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：
- solr
- wordnet
默认值为 solr。 expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。
例如：
若同义词定义为 “quick, fast” 且 expand 设置为 true，则同义词规则配置如下：</description></item><item><title>同义词词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/synonym/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/synonym/</guid><description>同义词词元过滤器 # 同义词(synonym)词元过滤器允许将多个术语映射到单个术语，或在单词之间创建等价组，从而提高搜索的灵活性。
参数说明 # 同义词词元过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。 synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。 lenient 可选 布尔值 加载规则配置时是否忽略异常。默认值为 false。 format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：
- solr
- wordnet
默认值为 solr。 expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。
例如：
如果同义词定义为 &amp;quot;quick, fast&amp;quot; 且 expand 设置为 true，则同义词规则配置如下：</description></item><item><title>唯一词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/unique/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/unique/</guid><description>唯一词元过滤器 # 唯一(unique)词元过滤器可确保在分词过程中仅保留唯一的词元，它会去除在单个字段或文本块中出现的重复词元。
参数说明 # 唯一词元过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 only_on_same_position 可选 布尔值 如果设置为 true，该词元过滤器将充当去重词元过滤器，仅移除位于相同位置的词元。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 unique_example 的新索引，并配置了一个带有唯一过滤器的分词器。
PUT /unique_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;unique_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;unique&amp;quot;, &amp;quot;only_on_same_position&amp;quot;: false } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;unique_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;unique_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>多路复用分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/multiplexer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/multiplexer/</guid><description>多路复用分词过滤器 # 多路复用(multiplexer)分词过滤器允许你通过应用不同的过滤器来创建同一词元的多个版本。当你想要以多种方式分析同一个词元时，这非常有用。例如，你可能希望使用不同的词干提取、同义词或 n-gram 过滤器来分析一个词元，并将所有生成的词元一起使用。这个分词过滤器的工作方式是复制分词流，并对每个副本应用不同的过滤器。
多路复用分词过滤器会从分词流中移除重复的词元。
多路复用分词过滤器不支持多词同义词(synonym)过滤器、同义词图(synonym_graph)分词过滤器或组合词（shingle）分词过滤器，因为这些过滤器不仅需要分析当前词元，还需要分析后续的词元，以便正确确定如何转换输入内容。
参数说明 # 多路复用分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 filters 可选 字符串列表 要应用于分词流每个副本的分词过滤器的逗号分隔列表。默认值为空列表。 preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 true。 参考样例 # 以下示例请求创建了一个名为 multiplexer_index 的新索引，并配置了一个带有多路复用过滤器的分词器：
PUT /multiplexer_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;english_stemmer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;english&amp;quot; }, &amp;quot;synonym_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;synonym&amp;quot;, &amp;quot;synonyms&amp;quot;: [ &amp;quot;quick,fast&amp;quot; ] }, &amp;quot;multiplexer_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;multiplexer&amp;quot;, &amp;quot;filters&amp;quot;: [&amp;quot;english_stemmer&amp;quot;, &amp;quot;synonym_filter&amp;quot;], &amp;quot;preserve_original&amp;quot;: true } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;multiplexer_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;multiplexer_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>大写词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/uppercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/uppercase/</guid><description>大写词元过滤器 # 大写(uppercase)词元过滤器用于在分析过程中将所有词元（单词）转换为大写形式。
参考样例 # 以下示例请求创建了一个名为 uppercase_example 的新索引，并配置了一个带有大写过滤器的分词器。
PUT /uppercase_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;uppercase_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;uppercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;uppercase_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;uppercase_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
GET /uppercase_example/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;uppercase_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch is powerful&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;EASYSEARCH&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;IS&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;POWERFUL&amp;quot;, &amp;quot;start_offset&amp;quot;: 14, &amp;quot;end_offset&amp;quot;: 22, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>小写分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/lowercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/lowercase/</guid><description>小写分词过滤器 # 小写(lowercase)分词过滤器用于将分词流中的所有字符转换为小写，从而使搜索不区分大小写。
参数 # 小写分词过滤器可以使用以下参数进行配置。
参数 必填/可选 描述 language 可选 指定一个特定语言的分词过滤器。有效值为：
- 希腊语 greek
- 爱尔兰语irish
- 土耳其语turkish。
默认值是 Lucene 的小写过滤器。 参考样例 # 以下示例请求创建了一个名为 custom_lowercase_example 的新索引。它配置了一个带有小写过滤器的分析器，并指定语言为希腊语。
PUT /custom_lowercase_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;greek_lowercase_example&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;greek_lowercase&amp;quot;] } }, &amp;quot;filter&amp;quot;: { &amp;quot;greek_lowercase&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;greek&amp;quot; } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>常用词组分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/common-grams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/common-grams/</guid><description>常用词组分词过滤器 # 常用词组(common_grams)分词过滤器通过保留文本中常见的短语（常用词组）来提高搜索的相关性。当处理某些语言或数据集时，特定的单词组合经常作为一个整体出现，如果将它们当作单独的词元处理，可能会影响搜索的相关性，此时该过滤器就非常有用。如果输入字符串中常用词，这个分词过滤器会同时生成它们的一元词组（单个词）和二元词组（两个词的组合）。
使用这个分词过滤器可以保持常用短语的完整性，从而提高搜索的相关性。这有助于更准确地匹配查询内容，尤其是对于频繁出现的单词组合。它还能通过减少不相关的匹配结果来提高搜索的精度。
使用此过滤器时，你必须谨慎选择并维护常用词(common_words)列表。
参数说明 # 常用词组分词过滤器可通过以下参数进行配置：
参数 必需/可选 数据类型 描述 common_words 必需 字符串列表 一个应被视为常用词词组的单词列表。如果 common_words 参数是一个空列表，common_grams 分词过滤器将成为一个无操作过滤器，即它根本不会修改输入的词元。 ignore_case 可选 布尔值 指示过滤器在匹配常用词时是否应忽略大小写差异。默认值为 false。 query_mode 可选 布尔值 当设置为 true 时，应用以下规则：
- 从 common_words 生成的一元词组（单个词）不包含在输出中。
- 非常用词后跟常用词形成的二元词组会保留在输出中。
- 如果非常用词的一元词组后面紧接着一个常用词，则该一元词组会被排除。
- 如果非常用词出现在文本末尾且前面是一个常用词，其一元词组不包含在输出中。 参考样例 # 以下示例请求创建了一个名为 my_common_grams_index 的新索引，并配置了一个使用常用词组（common_grams）过滤器的分析器。
PUT /my_common_grams_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_common_grams_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;common_grams&amp;quot;, &amp;quot;common_words&amp;quot;: [&amp;quot;a&amp;quot;, &amp;quot;in&amp;quot;, &amp;quot;for&amp;quot;], &amp;quot;ignore_case&amp;quot;: true, &amp;quot;query_mode&amp;quot;: true } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_common_grams_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>平图化分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/flatten-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/flatten-graph/</guid><description>平图化分词过滤器 # 平图化（flatten_graph）分词过滤器用于处理在图结构中同一位置生成多个词元时出现的复杂词元关系。一些分词过滤器，比如同义词图（synonym_graph）和词分隔符图（word_delimiter_graph），会生成多位置词元——即相互重叠或跨越多个位置的词元。这些词元图对于搜索查询很有用，但在索引过程中却不被直接支持。平图化分词过滤器会将多位置词元解析为一个线性的词元序列。对图进行扁平化处理可确保与索引过程兼容。
词元图的扁平化是一个有损耗的过程。只要有可能，就应避免使用平图化过滤器。相反，应仅在搜索分词器中应用图分词过滤器，这样就无需使用平图化分词过滤器了。
参考样例 # 以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有平图化分词过滤器的分词器：
PUT /test_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_index_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_custom_filter&amp;quot;, &amp;quot;flatten_graph&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;my_custom_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;word_delimiter_graph&amp;quot;, &amp;quot;catenate_all&amp;quot;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /test_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_index_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch helped many employers&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;helped&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 17, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;many&amp;quot;, &amp;quot;start_offset&amp;quot;: 18, &amp;quot;end_offset&amp;quot;: 22, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;employers&amp;quot;, &amp;quot;start_offset&amp;quot;: 23, &amp;quot;end_offset&amp;quot;: 32, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 } ] }</description></item><item><title>归一化词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/normalization/</guid><description>归一化词元过滤器 # 归一化(normalization)词元过滤器旨在以减少文本差异（尤其是特殊字符差异）的方式对文本进行调整和简化。它主要用于通过对特定语言中的字符进行标准化处理，来应对书写上的差异。
以下是可用的归一化词元过滤器：
阿拉伯语归一化: arabic_normalization 德语归一化: german_normalization 印地语归一化: hindi_normalization 印度语系归一化: indic_normalization 索拉尼语归一化: sorani_normalization 波斯语归一化: persian_normalization 斯堪的纳维亚语归一化: scandinavian_normalization 斯堪的纳维亚语折叠处理归一化: scandinavian_folding 塞尔维亚语归一化: serbian_normalization 参考样例 # 以下示例请求创建了一个名为 german_normalizer_example 的新索引，并配置了一个带有 german_normalization的分词器：
PUT /german_normalizer_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;german_normalizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;german_normalization&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;german_normalizer_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;german_normalizer&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /german_normalizer_example/_analyze { &amp;quot;text&amp;quot;: &amp;quot;Straße München&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;german_normalizer_analyzer&amp;quot; } 返回内容包含产生的词元</description></item><item><title>截断词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/truncate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/truncate/</guid><description>截断词元过滤器 # 截断(truncate)词元过滤器用于缩短超过指定长度的词元。它会将词元修剪至最大字符数，确保超过该限制的词元被截断。
参数说明 # 截断词元过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 length 可选 整数 指定生成的词元的最大长度。默认值为 10。 参考样例 # 以下示例请求创建了一个名为 truncate_example 的新索引，并配置了一个带有截断过滤器的分词器。
PUT /truncate_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;truncate_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;truncate&amp;quot;, &amp;quot;length&amp;quot;: 5 } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;truncate_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;truncate_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>指纹分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/fingerprint/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/fingerprint/</guid><description>指纹分词过滤器 # 指纹（fingerprint）分词过滤器用于对文本进行标准化和去重处理。当文本处理的唯一性或者一致性至关重要时，这个过滤器特别有用。指纹分词过滤器通过以下步骤处理文本以实现这一目的：
小写转换：将所有文本转换为小写。 分词：把文本拆分成词元。 排序：按字母顺序排列这些词元。 去重：消除重复的词元。 合并词元：将这些词元合并成一个字符串，通常使用空格或其他指定的分隔符进行连接。 参数说明 # 指纹分词过滤器可以使用以下两个参数进行配置。
参数 必需/可选 数据类型 描述 max_output_size 可选 整数 限制生成的指纹字符串的长度。如果拼接后的字符串超过了 max_output_size，过滤器将不会产生任何输出，从而得到一个空词元。默认值是 255 separator 可选 字符串 定义在词元排序和去重后，用于将词元连接成单个字符串。默认是空格（&amp;quot; &amp;quot;）。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有指纹分词过滤器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_fingerprint&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;fingerprint&amp;quot;, &amp;quot;max_output_size&amp;quot;: 200, &amp;quot;separator&amp;quot;: &amp;quot;-&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_fingerprint&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>捕获匹配词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/pattern-capture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/pattern-capture/</guid><description>捕获匹配词元过滤器 # 捕获匹配(pattern_capture)词元过滤器是一种功能强大的过滤器，它使用正则表达式根据特定模式来捕获和提取文本的部分内容。当你想要提取词元的特定部分，例如电子邮件域名、话题标签或数字，并将其重新用于进一步的分析或索引编制时，这个过滤器会非常有用。
参数说明 # 捕获匹配词元过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 patterns 必需 字符串数组 用于捕获文本部分的正则表达式数组。 preserve_original 必需 布尔值 是否在输出中保留原始词元。默认值为 true。 参考样例 # 以下示例请求创建了一个名为 email_index 的新索引，并配置了一个带有捕获匹配过滤器的分词器，以便从电子邮件地址中提取本地部分和域名。
PUT /email_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;email_pattern_capture&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern_capture&amp;quot;, &amp;quot;preserve_original&amp;quot;: true, &amp;quot;patterns&amp;quot;: [ &amp;quot;^([^@]+)&amp;quot;, &amp;quot;@(.+)$&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;email_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;uax_url_email&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;email_pattern_capture&amp;quot;, &amp;quot;lowercase&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>最小哈希分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/min-hash/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/min-hash/</guid><description>最小哈希分词过滤器 # 最小哈希（min_hash）分词过滤器用于基于最小哈希近似算法为词元生成哈希值，这对于检测文档间的相似度很有用。最小哈希分词过滤器会为一组词元（通常来自一个经过分词的字段）生成哈希值。
参数说明 # 最小哈希分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 hash_count 可选 整数 为每个词元生成的哈希值数量。增加此值通常会提高相似度估计的准确性，但会增加计算成本。默认值为 1。 bucket_count 可选 整数 要使用的哈希桶数量。这会影响哈希的粒度。更多的桶能提供更细的粒度并减少哈希冲突，但需要更多内存。默认值为 512。 hash_set_size 可选 整数 每个桶中要保留的哈希数量。这会影响哈希质量。更大的集合大小可能会带来更好的相似度检测效果，但会消耗更多内存。默认值为 1。 with_rotation 可选 布尔值 当设置为 true 时，如果 hash_set_size 为 1，过滤器会用从其右侧按环形顺序找到的第一个非空桶的值填充空桶。如果 bucket_count 参数大于 1，此设置会自动默认为 true；否则，默认为 false。 参考样例 # 以下示例请求创建了一个名为 minhash_index 的新索引，并配置了一个带有最小哈希过滤器的分词器：
PUT /minhash_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;minhash_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;min_hash&amp;quot;, &amp;quot;hash_count&amp;quot;: 3, &amp;quot;bucket_count&amp;quot;: 512, &amp;quot;hash_set_size&amp;quot;: 1, &amp;quot;with_rotation&amp;quot;: false } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;minhash_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;minhash_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>条件分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/condition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/condition/</guid><description>条件分词过滤器 # 条件(condition)分词过滤器是一种特殊类型的过滤器，它允许你根据特定标准有条件地应用其他分词过滤器。这使得在文本分析过程中，你能更精准地控制何时应用特定的分词过滤器。你可以配置多个过滤器，并且只有当满足你定义的条件时，这些过滤器才会被应用。该分词过滤器在进行特定语言处理和特殊字符处理时非常有用。
参数说明 # 要使用条件分词过滤器，必须配置两个参数，具体如下：
参数 必需/可选 数据类型 描述 filter 必需 数组 当满足由 script 参数定义的指定条件时，指定应对词元应用哪个分词过滤器。 script 必需 对象 配置一个内联脚本，该脚本定义了要应用 filter 参数中指定的过滤器所需满足的条件（仅接受内联脚本）。 参考样例 # 以下示例请求创建了一个名为 my_conditional_index 的新索引，并配置了一个带有条件过滤器的分词器。该过滤器会对任何包含字符序列 “um” 的词元应用小写（lowercase）字母过滤器。
PUT /my_conditional_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_conditional_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;condition&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;], &amp;quot;script&amp;quot;: { &amp;quot;source&amp;quot;: &amp;quot;token.getTerm().toString().contains('um')&amp;quot; } } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_conditional_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>波特词干词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/porter-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/porter-stem/</guid><description>波特词干词元过滤器 # 波特词干(porter_stem)词元过滤器会将单词还原为其基本（或词干）形式，并去除单词中常见的后缀，这有助于通过单词的词根来匹配相似的单词。例如，单词“running”会被词干提取为“run”。此词元过滤器主要用于英语，并基于 波特词干提取算法进行词干提取操作。
参考样例 # 以下示例请求创建了一个名为 my_stem_index 的新索引，并配置了一个带有波特词干提取过滤器的分词器。
PUT /my_stem_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_porter_stem&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;porter_stem&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;porter_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_porter_stem&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_stem_index/_analyze { &amp;quot;text&amp;quot;: &amp;quot;running runners ran&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;porter_analyzer&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;run&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;runner&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;ran&amp;quot;, &amp;quot;start_offset&amp;quot;: 16, &amp;quot;end_offset&amp;quot;: 19, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>省略符号分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/apostrophe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/apostrophe/</guid><description>省略符号分词过滤器 # 省略符号（apostrophe）分词过滤器的主要功能是移除所有格形式的撇号以及跟在撇号后面的所有内容。这在分析那些大量使用撇号的语言的文本时非常有用，比如土耳其语。在土耳其语中，撇号用于将词根与后缀分隔开来，这些后缀包括所有格后缀、格标记以及其他语法词尾。
参考样例 # 以下示例请求创建了一个名为 custom_text_index 的新索引，在索引设置中配置了一个自定义分词器，并在映射中使用该分词器：
PUT /custom_text_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;apostrophe&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;custom_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /custom_text_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;custom_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;John's car is faster than Peter's bike&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;john&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;car&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;faster&amp;quot;, &amp;quot;start_offset&amp;quot;: 14, &amp;quot;end_offset&amp;quot;: 20, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;than&amp;quot;, &amp;quot;start_offset&amp;quot;: 21, &amp;quot;end_offset&amp;quot;: 25, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 4 }, { &amp;quot;token&amp;quot;: &amp;quot;peter&amp;quot;, &amp;quot;start_offset&amp;quot;: 26, &amp;quot;end_offset&amp;quot;: 33, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 5 }, { &amp;quot;token&amp;quot;: &amp;quot;bike&amp;quot;, &amp;quot;start_offset&amp;quot;: 34, &amp;quot;end_offset&amp;quot;: 38, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 6 } ] } 内置的省略符号分词过滤器并不适用于像法语这样的语言，在法语中撇号会出现在单词的开头。例如，C'est l'amour de l'école 这句话使用该过滤器分词后将会得到四个词元：“C”、“l”、“de” 和 “l”。</description></item><item><title>省略词分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/elision/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/elision/</guid><description>省略词分词过滤器 # 省略词（Elision）分词过滤器用于从某些语言的单词中去除省略的字符。省略现象通常出现在像法语这样的语言中，在这些语言里，单词常常会发生缩合，并与后面的单词结合，常见的方式是省略一个元音字母，并用一个撇号来替代。
省略词分词过滤器已经在以下语言分词器中预先配置好了：加泰罗尼亚语/catalan、法语/french、爱尔兰语/irish和意大利语/italian。
参数说明 # 自定义省略词分词过滤器可使用以下参数进行配置。
参数 必需/可选 数据类型 描述 articles 若未配置 articles_path 则为必需 字符串数组 定义当某些冠词或短词作为省略形式的一部分出现时，哪些应该被移除。 articles_path 若未配置 articles 则为必需 字符串 指定在分析过程中应被移除的自定义冠词列表的路径。 articles_case 可选 布尔值 指定在匹配省略形式时，该过滤器是否区分大小写。默认值为 false。 参考样例 # 法语中默认的省略形式集合包括 l'、m'、t'、qu'、n'、s'、j'、d'、c'、jusqu'、quoiqu'、lorsqu' 和 puisqu'。你可以通过配置 french_elision 分词过滤器来更新这个集合。以下示例请求创建了一个名为 french_texts 的新索引，并配置了一个带有 french_elision 过滤器的分词器：
PUT /french_texts { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;french_elision&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;elision&amp;quot;, &amp;quot;articles&amp;quot;: [ &amp;quot;l&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;m&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;s&amp;quot;, &amp;quot;j&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;french_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;french_elision&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;french_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>经典分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/classic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/classic/</guid><description>经典分词过滤器 # 经典（classic）分词过滤器的主要功能是与经典词元生成器配合使用。它通过应用以下常见的转换操作来处理词元，这些操作有助于文本分析和搜索：
移除所有格词尾，例如 “’s” 。比如，“John’s” 会变为 “John”。 从首字母缩略词中移除句点。例如，“D.A.R.P.A.” 会变为 “DARPA”。 参考样例 # 以下示例请求创建了一个名为 custom_classic_filter 的新索引，并配置了一个使用经典分词过滤器的分词器。
PUT /custom_classic_filter { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_classic&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;classic&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;classic&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /custom_classic_filter/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;custom_classic&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;John's co-operate was excellent.&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;John&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;APOSTROPHE&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;co&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 9, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;operate&amp;quot;, &amp;quot;start_offset&amp;quot;: 10, &amp;quot;end_offset&amp;quot;: 17, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;was&amp;quot;, &amp;quot;start_offset&amp;quot;: 18, &amp;quot;end_offset&amp;quot;: 21, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;excellent&amp;quot;, &amp;quot;start_offset&amp;quot;: 22, &amp;quot;end_offset&amp;quot;: 31, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 4 } ] }</description></item><item><title>词保留分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keep-words/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keep-words/</guid><description>词保留分词过滤器 # 词保留（keep_words）分词过滤器旨在在分析过程中仅保留特定的词。如果你有大量文本，但只对某些关键字或术语感兴趣，那么这个过滤器就会很有用。
参数说明 # 词保留分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 keep_words 若未配置 keep_words_path 则为必需 字符串列表 要保留的词的列表。 keep_words_path 若未配置 keep_words 则为必需 字符串 包含要保留的词列表的文件的路径。 keep_words_case 可选 布尔值 在比较时是否将所有词转换为小写。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有词保留过滤器的分词器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_keep_word&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;keep_words_filter&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;keep_words_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keep&amp;quot;, &amp;quot;keep_words&amp;quot;: [&amp;quot;example&amp;quot;, &amp;quot;world&amp;quot;, &amp;quot;easysearch&amp;quot;], &amp;quot;keep_words_case&amp;quot;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词典复合词分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/dictionary-decompounder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/dictionary-decompounder/</guid><description>词典复合词分词过滤器 # 词典复合词（dictionary_decompounder）分词过滤器用于根据预定义的词典将复合词拆分为其组成部分。该过滤器对于德语、荷兰语或芬兰语等复合词较为常见的语言特别有用，因为将复合词拆分可以提高搜索的相关性。词典复合词分词过滤器会根据已知词列表判断每个词元（单词）是否可以拆分为更小的词元。如果该词元可以拆分为已知单词，过滤器就会为该词元生成子词元。
参数说明 # 词典复合词分词过滤器具有以下参数：
参数 必需/可选 数据类型 描述 word_list 除非配置了 word_list_path，否则为必需 字符串数组 过滤器用于拆分复合词的单词词典。 word_list_path 除非配置了 word_list，否则为必需 字符串 包含词典单词的文本文件的文件路径。可以接受绝对路径或相对于配置(config)目录的相对路径。词典文件必须采用 UTF-8 编码，且每个单词必须单独占一行。 min_word_size 可选 整数 会被考虑进行拆分的整个复合词的最小长度。如果复合词短于该值，则不会进行拆分。默认值为 5。 min_subword_size 可选 整数 任何子词的最小长度。如果子词短于该值，则不会包含在输出中。默认值为 2。 max_subword_size 可选 整数 任何子词的最大长度。如果子词长于该值，则不会包含在输出中。默认值为 15。 only_longest_match 可选 布尔值 如果设置为 true，则仅返回最长匹配的子词。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 decompound_example 的新索引，并配置了一个带有词典复合词过滤器的分词器：</description></item><item><title>词干提取词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stemmer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stemmer/</guid><description>词干提取词元过滤器 # 词干提取(stemmer)词元过滤器会将单词缩减为其词根或基本形式（也称为词干stem）。
参数说明 # 词干提取词元过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：
阿拉伯语：arabic 亚美尼亚语：armenian 巴斯克语：basque 孟加拉语：bengali 巴西葡萄牙语：brazilian 保加利亚语：bulgarian 加泰罗尼亚语：catalan 捷克语：czech 丹麦语：danish 荷兰语：dutch、dutch_kp 英语：english（默认）、light_english、lovins、minimal_english、porter2、possessive_english 爱沙尼亚语：estonian 芬兰语：finnish、light_finnish 法语：light_french、french、minimal_french 加利西亚语：galician、minimal_galician（仅复数处理步骤） 德语：light_german、german、german2、minimal_german 希腊语：greek 印地语：hindi 匈牙利语：hungarian、light_hungarian 印尼语：indonesian 爱尔兰语：irish 意大利语：light_italian、italian 库尔德语（索拉尼方言）：sorani 拉脱维亚语：latvian 立陶宛语：lithuanian 挪威语（书面挪威语）：norwegian、light_norwegian、minimal_norwegian 挪威语（新挪威语）：light_nynorsk、minimal_nynorsk 葡萄牙语：light_portuguese、minimal_portuguese、portuguese、portuguese_rslp 罗马尼亚语：romanian 俄语：russian、light_russian 西班牙语：light_spanish、spanish 瑞典语：swedish、light_swedish 土耳其语：turkish 你也可以使用 name 参数作为 language 参数的别名。如果两个参数都被设置，name 参数将被忽略。
参考样例 # 以下示例请求创建了一个名为 my-stemmer-index 的新索引，并配置了一个带有词干提取过滤器的分词器。
PUT /my-stemmer-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_english_stemmer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;english&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_stemmer_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_english_stemmer&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词干覆盖词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stemmer-override/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stemmer-override/</guid><description>词干覆盖词元过滤器 # 词干覆盖（stemmer_override）词元过滤器允许你定义自定义的词干提取规则，这些规则可以覆盖像波特（Porter）或雪球（Snowball）等默认词干提取器的行为。当你希望对某些特定的单词应用特殊的词干提取方式，而这些单词可能无法被标准的词干提取算法正确处理时，这个过滤器就会非常有用。
参数说明 # 词干覆盖词元过滤器必须使用以下参数中的一个进行配置。
参数 数据类型 描述 rules 字符串 直接在设置中定义覆盖规则。 rules_path 字符串 指定包含自定义规则（映射）的文件的路径。该路径可以是绝对路径，也可以是相对于配置目录的相对路径。 参考样例 # 以下示例请求创建了一个名为 my-index 的新索引，并配置了一个带有词干覆盖过滤器的分词器。
PUT /my-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_stemmer_override_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer_override&amp;quot;, &amp;quot;rules&amp;quot;: [ &amp;quot;running, runner =&amp;gt; run&amp;quot;, &amp;quot;bought =&amp;gt; buy&amp;quot;, &amp;quot;best =&amp;gt; good&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_stemmer_override_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词片词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/shingle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/shingle/</guid><description>词片词元过滤器 # 词片(shingle)词元过滤器用于从输入文本中生成词 n 元组（即词片）。例如，对于字符串 “slow green turtle”，词片过滤器会创建以下一元词片和二元词片：“slow”、“slow green”、“green”、“green turtle” 以及 “turtle”。
这个词元过滤器常常与其他过滤器结合使用，通过对短语而不是单个词元进行索引来提高搜索的准确性。
参数说明 # 词片词元过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_shingle_size 可选 整数 要连接的词元的最小数量。默认值为 2。 max_shingle_size 可选 整数 要连接的词元的最大数量。默认值为 2。 output_unigrams 可选 布尔值 是否将一元词（单个词元）包含在输出中。默认值为 true。 output_unigrams_if_no_shingles 可选 布尔值 如果未生成词片，是否输出一元词。默认值为 false。 token_separator 可选 字符串 用于将词元连接成词片的分隔符。默认值为一个空格（ ）。 filler_token 可选 字符串 插入到词元之间的空位置或间隙中的词元。默认值为下划线（_）。 如果 output_unigrams 和 output_unigrams_if_no_shingles 都设置为 true，则 output_unigrams_if_no_shingles 将被忽略。</description></item><item><title>谓词词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/predicate-token-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/predicate-token-filter/</guid><description>谓词词元过滤器 # 谓词词元过滤器(predicate_token_filter)会根据自定义脚本中定义的条件来评估词元是应该保留还是丢弃。词元的评估是在分析谓词上下文中进行的。此过滤器仅支持内联 Painless 脚本。
参数说明 # 谓词词元过滤器有一个必需参数：script。该参数提供一个条件，用于评估词元是否应被保留。
参考样例 # 以下示例请求创建了一个名为 predicate_index 的新索引，并配置了一个带有谓词词元过滤器的分词器。该过滤器指定仅输出长度超过 7 个字符的词元。
PUT /predicate_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_predicate_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;, &amp;quot;script&amp;quot;: { &amp;quot;source&amp;quot;: &amp;quot;token.term.length() &amp;gt; 7&amp;quot; } } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;predicate_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_predicate_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /predicate_index/_analyze { &amp;quot;text&amp;quot;: &amp;quot;The Easysearch community is growing rapidly&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;predicate_analyzer&amp;quot; } 返回内容包含产生的词元</description></item><item><title>边缘 n 元分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/edge-n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/edge-n-gram/</guid><description>边缘 n 元分词过滤器 # 边缘 n 元（edge_ngram）分词过滤器与 n 元（ngram）分词过滤器非常相似，二者都会将特定字符串拆分成不同长度的子字符串。不过，边缘 n 元分词过滤器仅从词元的开头（边缘）生成 n 元子字符串。在自动补全或前缀匹配等场景中，它特别有用，因为在这些场景里，你希望在用户输入单词或短语时就能匹配词项的开头部分。
参数说明 # 边缘 n 元分词过滤器可使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_gram 可选 整数 要生成的 n 元词项的最小长度。默认值为1。 max_gram 可选 整数 要生成的 n 元词项的最大长度。对于边缘 n 元分词过滤器，默认值为1；对于自定义分词过滤器，默认值为2。避免将此参数设置为较低的值。如果该值设置得过低，将只会生成非常短的 n 元词项，并且可能找不到搜索词。例如，将max_gram设置为 3，并且对单词“banana”建立索引，那么生成的最长词元将是“ban”。如果用户搜索“banana”，将不会返回任何匹配结果。你可以使用截断truncate分词过滤器作为搜索分析器来降低这种风险。 preserve_original 可选 布尔值 将原始词元包含在输出中。默认值为false。 参考样例 # 以下示例请求创建了一个名为 edge_ngram_example 的新索引，并配置了一个带有边缘 n 元过滤器的分词器：
PUT /edge_ngram_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_edge_ngram&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 3, &amp;quot;max_gram&amp;quot;: 4 } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;my_edge_ngram&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>长度分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/length/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/length/</guid><description>长度分词过滤器 # 长度分词过滤器用于从词元流中移除那些不符合指定长度标准（最小和最大长度值）的词元。
参数说明 # 长度分词过滤器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 min 可选 整数 词元的最小长度。默认值为 0。 max 可选 整数 词元的最大长度。默认值为整数类型的最大值（2147483647）。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有长度过滤器的分词器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;only_keep_4_to_10_characters&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;length_4_to_10&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;length_4_to_10&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;length&amp;quot;, &amp;quot;min&amp;quot;: 4, &amp;quot;max&amp;quot;: 10 } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>限制分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/limit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/limit/</guid><description>限制分词过滤器 # 限制分词过滤器用于限制分词链通过的词元数量。
参数说明 # 限制分词过滤器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 max_token_count 可选 整数 要生成的词元的最大数量。默认值为 1。 consume_all_tokens 可选 布尔值 （专家级设置）即使结果超过 max_token_count，也会使用来自词元生成器的所有词元。当设置此参数时，输出仍然只包含 max_token_count 指定数量的词元。不过，词元生成器生成的所有词元都会被处理。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有限制过滤器的分析器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;three_token_limit&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;custom_token_limit&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;custom_token_limit&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;limit&amp;quot;, &amp;quot;max_token_count&amp;quot;: 3 } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>雪球算法词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/snowball/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/snowball/</guid><description>雪球算法词元过滤器 # 雪球算法（snowball）词元过滤器是一种基于 雪球算法的词干提取过滤器。它支持多种语言，并且比波特词干提取算法更加高效和准确。
参数说明 # 雪球词元过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：
阿拉伯语（Arabic） 亚美尼亚语（Armenian） 巴斯克语（Basque） 加泰罗尼亚语（Catalan） 丹麦语（Danish） 荷兰语（Dutch） 英语（English，默认值） 爱沙尼亚语（Estonian） 芬兰语（Finnish） 法语（French） 德语（German） 德语 2（German2） 匈牙利语（Hungarian） 意大利语（Italian） 爱尔兰语（Irish） Kp 立陶宛语（Lithuanian） 洛文斯（Lovins） 挪威语（Norwegian） 波特（Porter） 葡萄牙语（Portuguese） 罗马尼亚语（Romanian） 俄语（Russian） 西班牙语（Spanish） 瑞典语（Swedish） 土耳其语（Turkish） 参考样例 # 以下示例请求创建了一个名为 my-snowball-index 的新索引，并配置了一个带有雪球过滤器的分词器。
PUT /my-snowball-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_snowball_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;snowball&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;English&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_snowball_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_snowball_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item></channel></rss>