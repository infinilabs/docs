<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>INFINI Easysearch</title><link>/easysearch/v1.14.0/</link><description>Recent content on INFINI Easysearch</description><generator>Hugo -- gohugo.io</generator><atom:link href="/easysearch/v1.14.0/index.xml" rel="self" type="application/rss+xml"/><item><title>部署 Operator</title><link>/easysearch/v1.14.0/docs/getting-started/install/operator/deploy_operator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/operator/deploy_operator/</guid><description>部署 Easysearch Operator # Easysearch Operator 只能在 k8s 环境下部署安装，请准备好一套 k8s 环境
部署前准备 # k8s 环境
要求Kubernetes 1.9以上版本，自 1.9 版本以后，StatefulSet成为了在Kubernetes中管理有状态应用的标准方式。 StorageClass
StorageClass 允许集群管理员定义多种存储方案，如快速的 SSD、标准的硬盘，或者其他的存储系统。无需手动预先创建存储资源，用户只需要在 PersistentVolumeClaim (PVC) 中指定需要的 StorageClass，存储资源就可以根据需求动态地创建。 ServiceAccount
创建一个 ServiceAccount 用于 Easysearch Operator 获取和操作 k8s 资源 apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/name: serviceaccount app.kubernetes.io/instance: controller-manager-sa app.kubernetes.io/component: rbac app.kubernetes.io/created-by: k8s-operator app.kubernetes.io/part-of: k8s-operator app.kubernetes.io/managed-by: kustomize name: controller-manager # ServiceAccount 的名字是 controller-manager namespace: default ClusterRole
创建 ClusterRole，用于定义访问 k8s 集群的角色权限 展开查看完整代码 .</description></item><item><title>本地配置</title><link>/easysearch/v1.14.0/docs/references/security/configuration/yaml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/configuration/yaml/</guid><description>本地配置 # 通过安全模块的本地 YAML 配置文件可以方便的管理默认的内置用户或 隐藏的保留资源，例如 admin 管理员用户。不过通过 INFINI Console 或者 REST API 来创建其他用户、角色、映射、操作组和租户可能更容易。
user.yml # 此文件包含您要添加到内部用户数据库的默认初始用户。
配置文件里面的密码不能是明文，必须使用 Hash 之后的密码，通过命令 ./bin/hash_password.sh -p &amp;lt;new-password&amp;gt; 可以生成一个密码哈希。
_meta: type: &amp;#34;user&amp;#34; config_version: 2 admin: hash: &amp;#34;$2y$12$ZXx5R8NfuW2TYPOdGNY7a.43WKKBMCtN9aJywYWjAz9i11w7SrkqG&amp;#34; reserved: true external_roles: - &amp;#34;admin&amp;#34; description: &amp;#34;Default admin user&amp;#34; readonly: hash: &amp;#34;$2y$12$d9I16.5qpYhhsbiGN4zqdeA4k6BeMl/yEKRvTo3gzxFp8UC57EgJ.&amp;#34; reserved: false external_roles: - &amp;#34;readall&amp;#34; description: &amp;#34;Default readonly user&amp;#34; role.yml # 此文件包含要添加到内置数据库的默认初始角色。除了一些元数据之外，这个文件默认是是空的，因为系统已经了内置了若干角色，可以根据需要进行角色扩展。
complex-role: reserved: false hidden: false cluster: - &amp;#34;read&amp;#34; - &amp;#34;cluster:monitor/nodes/stats&amp;#34; - &amp;#34;cluster:monitor/task/get&amp;#34; indices: - names: - &amp;#34;kibana_sample_data_*&amp;#34; query: &amp;#39;{&amp;#34;match&amp;#34;: {&amp;#34;FlightDelay&amp;#34;: true}}&amp;#39; field_security: - &amp;#34;~FlightNum&amp;#34; field_mask: - &amp;#34;Carrier&amp;#34; privileges: - &amp;#34;read&amp;#34; static: false _meta: type: &amp;#34;roles&amp;#34; config_version: 2 role_mapping.</description></item><item><title>后端配置</title><link>/easysearch/v1.14.0/docs/references/security/configuration/backend/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/configuration/backend/</guid><description>后端配置 # 配置安全模块的第一步是确定如何验证用户身份。尽管 Easysearch 本身可以充当一个内部用户数据库，但许多人更喜欢集成企业现有的身份认证体系，例如 LDAP 服务器，或两者组合。设置身份验证和授权服务端的主要配置文件位于 config/security/config.yml。它定义了安全模块如何检索用户凭据、如何验证这些凭据以及如何从后端系统获取其他角色（可选）。
config.yml 主要包含三大部分：
security: dynamic: http: ... authc: ... authz: ... HTTP # 配置 http 具有以下格式：
anonymous_auth_enabled: &amp;lt;true|false&amp;gt; 可以选择是否开启匿名访问，如果禁用匿名身份验证，则至少在 authc里面提供一个认证后端，否则安全模块将不予初始化，默认为 false。
认证 # 认证配置 authc 具有以下格式：
&amp;lt;name&amp;gt;: http_enabled: &amp;lt;true|false&amp;gt; transport_enabled: &amp;lt;true|false&amp;gt; order: &amp;lt;integer&amp;gt; http_authenticator: ... authentication_backend: ... 配置 authc 里面的每一项被称为 身份验证域。它指定来在何处获取用户凭据以及应针对哪个后端对它们进行身份验证。
您可以使用多个身份验证域。每个身份验证域都有一个名称（例如，basic_auth_internal）、enabled 开关和排序参数 order。该顺序使将身份验证域链接在一起成为可能。安全模块按您提供的顺序依次使用它们。如果用户成功通过一个域进行了身份验证，安全模块将跳过剩余的验证域。
http_authenticator 指定要在 HTTP 层上使用的身份验证方法。
以下是在 HTTP 层上定义身份验证器的语法：
http_authenticator: type: &amp;lt;type&amp;gt; challenge: &amp;lt;true|false&amp;gt; config: ... 参数 type 支持以下几种类型:</description></item><item><title>证书配置</title><link>/easysearch/v1.14.0/docs/references/security/configuration/tls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/configuration/tls/</guid><description>配置 TLS 证书 # Easysearch 可以通过启用 TLS 传输加密来保护您数据的网络传输安全。 TLS 的相关设置要在配置文件 easysearch.yml 里面进行。主要包括两个部分的配置：传输层和 HTTP 层。传输层的 TLS 是必需的，HTTP 层的 TLS 的配置是可选的。
默认的配置如下：
security.ssl.transport.cert_file: instance.crt security.ssl.transport.key_file: instance.key security.ssl.transport.ca_file: ca.crt security.ssl.http.enabled: true security.ssl.http.cert_file: instance.crt security.ssl.http.key_file: instance.key security.ssl.http.ca_file: ca.crt 一键生成证书 # 启用 TLS 需要设置证书才能工作，通过执行命令 ./bin/initialize.sh 可以一键生成 TLS 证书，如下：
➜ ./bin/initialize.sh Generating RSA private key, 2048 bit long modulus .......................+++ ...............................+++ e is 65537 (0x10001) Generating RSA private key, 2048 bit long modulus .......................................................................................................................................................................................+++ .</description></item><item><title>系统索引</title><link>/easysearch/v1.14.0/docs/references/security/configuration/system-indices/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/configuration/system-indices/</guid><description>系统索引 # Easysearch 默认的身份信息存放在一个受保护的系统索引里面，名称为：.security， 将索引设置为系统索引可以对该索引的数据进行额外的保护，因为即使您的用户帐户对所有索引具有读取权限，也无法直接访问此系统索引中的数据。
您可以在 easysearch.yml 中添加其它您希望需要受到保护的索引。
security.system_indices.enabled: true security.system_indices.indices: [&amp;#34;.infini-*&amp;#34;] 如果要访问系统索引，必须使用管理员证书的方式来进行： 配置管理证书:
curl -k --cert ./admin.crt --key ./admin.key -XGET &amp;#39;https://localhost:9200/.security/_search&amp;#39; 另一种方法是从每个节点上的 security.system_indices.index 列表中删除该索引，然后重新启动 Easysearch 即可正常操作该索引。</description></item><item><title>指标聚合</title><link>/easysearch/v1.14.0/docs/references/aggregation/metric-agg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/aggregation/metric-agg/</guid><description>指标聚合 # 指标聚合让您可以执行简单的计算，例如查找字段的最小值、最大值和平均值。
指标聚合的类型 # 指标聚合有两种类型：单值指标聚合和多值指标聚合。
单值指标聚合 # 单值 指标聚合返回单个指标。 例如，sum 、 min 、 max 、 avg 、 cardinality 和 value_count 。
多值指标聚合 # 多值 指标聚合返回多个指标。 例如， stats 、 extended_stats 、 matrix_stats 、 percentile 、 percentile_ranks 、 geo_bound 、 top_hits 和 scripted_metric。
sum, min, max, avg # sum 、 min 、 max 和 avg metric 是单值 指标聚合，分别返回字段的总和、最小值、最大值和平均值。
以下示例计算 taxful_total_price 字段的总和：
GET kibana_sample_data_ecommerce/_search { &amp;#34;size&amp;#34;: 0, &amp;#34;aggs&amp;#34;: { &amp;#34;sum_taxful_total_price&amp;#34;: { &amp;#34;sum&amp;#34;: { &amp;#34;field&amp;#34;: &amp;#34;taxful_total_price&amp;#34; } } } } 返回示例 # .</description></item><item><title>部署 Easysearch</title><link>/easysearch/v1.14.0/docs/getting-started/install/operator/deploy_easysearch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/operator/deploy_easysearch/</guid><description>部署Easysearch Operator # 这里我们准备部署一个 3 节点的Easysearch 集群，准备 three-nodes-easysearch-cluster.yaml 文件，文件内容如下所示，并对关键字段都进行了注释。
apiVersion: infinilabs.infinilabs.com/v1 kind: SearchCluster # 自定义的资源类型 metadata: name: threenodes # Easysearch 集群的名称 namespace: default # Easysearch 集群所在的命名空间 spec: # 规格 security: # 安全相关 config: adminSecret: # admin证书配置 name: easysearch-admin-certs adminCredentialsSecret: # 账户密码配置 name: threenodes-admin-password tls: # tls 协议配置，包括节点间的transport，以及访问集群的http http: # 访问集群http配置 generate: false # 是否需要集群自动生成证书 secret: # 自定义证书配置 name: easysearch-certs transport: # 集群间访问配置 generate: false perNode: false # 是否给每一个节点配置证书 secret: # 自定义证书配置 name: easysearch-certs nodesDn: [&amp;#34;CN=Easysearch_Node&amp;#34;] adminDn: [&amp;#34;CN=Easysearch_Admin&amp;#34;] general: # 通用配置 snapshotRepositories: # s3 快照配置 - name: s3_repository # 配置的s3快照的名称 type: s3 # 快照类型 settings: # 快照配置 bucket: es-operator-bucket # s3中的桶，需提前建好 access_key: minioadmin # 访问s3密钥 secret_key: minioadmin endpoint: http://192.</description></item><item><title>用户角色</title><link>/easysearch/v1.14.0/docs/references/security/access-control/users-roles/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/access-control/users-roles/</guid><description>角色与用户 # 安全模块包括一个内部用户数据库。使用此数据库代替外部身份验证系统（如 LDAP 或 Active Directory）或作为外部身份验证系统的补充。
角色是控制对群集的访问的核心方式。角色包含集群范围权限、特定于索引的权限、文档和字段级安全性以及租户的任意组合。然后，将用户映射到这些角色，以便用户获得这些权限。
除非您需要创建新的 只读或隐藏用户，我们强烈建议使用 REST API 来创建新的用户、角色和角色映射。.yml 文件用于初始设置，而不是持续使用。
创建用户 # user.yml # 参照 本地文件配置。
REST API # 参照 创建用户。
创建角色 # role.yml # 参照 本地文件配置。
REST API # 参照 创建角色。
映射用户到角色 # role_mapping.yml # 参照 本地文件配置。
REST API # 参照 创建角色映射。</description></item><item><title>搭建集群</title><link>/easysearch/v1.14.0/docs/references/management/cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/cluster/</guid><description>搭建集群 # 在深入研究 Easysearch 以及搜索和聚合数据之前，你首先需要创建一个 Easysearch 集群。
Easysearch 可以作为一个单节点或多节点集群运行。一般来说，配置两者的步骤是非常相似的。本页演示了如何创建和配置一个多节点集群，但只需做一些小的调整，你可以按照同样的步骤创建一个单节点集群。
要根据你的要求创建和部署一个 Easysearch 集群，重要的是要了解节点发现和集群形成是如何工作的，以及哪些设置对它们有影响。
有许多方法可以设计一个集群。下面的插图显示了一个基本架构。
这是一个四节点的集群，有一个专用的主节点，一个专用的协调节点，还有两个数据节点，这两个节点是主节点，也是用来摄取数据的。
下表提供了节点类型的简要描述。
节点类型 描述 最佳实践 Master 管理集群的整体运作并跟踪集群的状态。这包括创建和删除索引，跟踪加入和离开集群的节点，检查集群中每个节点的健康状况（通过运行 ping 请求），并将分片分配给节点。 在三个不同区域的三个专用主节点是几乎所有生产用例的正确方法。这可以确保你的集群永远不会失去法定人数。两个节点在大部分时间都是空闲的，除非一个节点宕机或需要一些维护。 Data 存储和搜索数据。在本地分片上执行所有与数据有关的操作（索引、搜索、聚合）。这些是你的集群的工作节点，需要比其他任何节点类型更多的磁盘空间。 当你添加数据节点时，保持它们在各区之间的平衡。例如，如果你有三个区，以三的倍数添加数据节点，每个区一个。我们建议使用存储和内存重的节点。 默认情况下，每个节点是一个主节点和数据节点。决定节点的数量，分配节点类型，并为每个节点类型选择硬件，取决于你的使用情况。你必须考虑到一些因素，如你想保留数据的时间，你的文件的平均大小，你的典型工作负载（索引、搜索、聚合），你的预期性价比，你的风险容忍度，等等。
在你评估所有这些要求之后，我们建议你使用一个管理工具。要开始使用 INFINI Console，请参阅 INFINI Console 文档。
本页演示了如何处理不同的节点类型。它假设你有一个类似于前面插图的四节点集群。
前提条件 # 在你开始之前，你必须在你的所有节点上安装和配置 Easysearch。有关可用选项的信息，请参见 安装和配置。
完成后，使用 SSH 连接到每个节点，然后打开 config/easysearch.yml 文件。
你可以在这个文件中为你的集群设置所有的配置。
Step 1: 命名集群 # 为集群指定一个唯一的名字。如果你不指定集群名称，它将被默认设置为 easysearch。设置一个描述性的集群名称很重要，特别是如果你想在一个网络内运行多个集群。
要指定集群名称，请修改下面一行。
#cluster.name: my-application to</description></item><item><title>文档权限</title><link>/easysearch/v1.14.0/docs/references/security/access-control/document-level-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/access-control/document-level-security/</guid><description>文档级权限 # 文档级权限允许您将角色限制为索引中文档的一部分子集。
参考设置 # 文档级权限使用 Easysearch 查询 DSL 来定义角色授予对哪些文档的访问权限。
{ &amp;#34;bool&amp;#34;: { &amp;#34;must&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;public&amp;#34;: &amp;#34;true&amp;#34; } } } } 上面的查询指定了该角色访问的文档里面，其字段 public 必须匹配 true。
指定字段 query 并设置为将上面的查询，并对查询字符串进行转义，最后的角色设置如下：
PUT _security/role/public_data { &amp;#34;cluster&amp;#34;: [ &amp;#34;*&amp;#34; ], &amp;#34;indices&amp;#34;: [{ &amp;#34;names&amp;#34;: [ &amp;#34;pub*&amp;#34; ], &amp;#34;query&amp;#34;: &amp;#34;{\&amp;#34;term\&amp;#34;: { \&amp;#34;public\&amp;#34;: true}}&amp;#34;, &amp;#34;privileges&amp;#34;: [ &amp;#34;read&amp;#34; ] }] } 上面的查询也可以根据需要写的很复杂，但是我们建议保持简单，以最大程度地减少文档级安全功能对集群的性能影响。
参数替换 # 查询过程中可以利用上下文变量，可根据当前用户的属性来强制实施规则替换。例如 ${user.name} 将替换为当前用户的名称。
如下规则允许用户读取字段 readable_by 为其用户名值的任何文档：
PUT _security/role/user_data { &amp;#34;cluster&amp;#34;: [ &amp;#34;*&amp;#34; ], &amp;#34;indices&amp;#34;: [{ &amp;#34;names&amp;#34;: [ &amp;#34;pub*&amp;#34; ], &amp;#34;query&amp;#34;: &amp;#34;{\&amp;#34;term\&amp;#34;: { \&amp;#34;readable_by\&amp;#34;: \&amp;#34;${user.</description></item><item><title>CAT API</title><link>/easysearch/v1.14.0/docs/references/management/catapis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/catapis/</guid><description>cat API # 您可以使用紧凑且对齐的文本 （CAT） API 以易于理解的表格格式获取有关集群的基本统计信息。cat API 是一个人类可读的接口，它返回纯文本而不是传统的 JSON。
使用 cat API，您可以回答诸如哪个节点是选定的主节点、集群处于什么状态、每个索引中有多少文档等问题。
要查看 cat API 中的可用操作，请使用以下命令：
GET _cat 还可以在查询中使用以下字符串参数。
参数 描述 ?v 通过向列添加标题使输出更详细。它还添加了一些格式，以帮助将每列对齐在一起。此页面上的所有示例都包含 v 参数。 ?help 列出给定操作的默认标头和其他可用标头。 ?h 将输出限制为特定标头。 ?format 以 JSON、YAML 或 CBOR 格式输出结果。 ?sort 按指定列对输出进行排序。 要查看每列表示的内容，请使用 ?v 参数：
GET _cat/&amp;lt;operation_name&amp;gt;?v 要查看所有可用的标头，请使用 ?help 参数：
GET _cat/&amp;lt;operation_name&amp;gt;?help 要将输出限制为标头的子集，请使用 ?h 参数：</description></item><item><title>增删改查</title><link>/easysearch/v1.14.0/docs/references/document/index-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/document/index-data/</guid><description>增删改查 # 您可以使用 REST API 对数据进行索引。存在两个 API：索引 API 和 _bulk API。
对于新数据增量到达的情况（例如，来自小型企业的客户订单），您可以使用索引 API 在文档到达时单独添加文档。对于数据流不太频繁的情况（例如，每周更新一次营销网站），您可能更希望生成一个文件并将其发送到 _bulk API。对于大量文档，将请求汇总在一起并使用 _bulk API 可提供优异的性能。然而，如果您的文档非常庞大，您可能需要单独对它们进行索引。
索引介绍 # 在搜索数据之前，必须对其进行索引。索引是搜索引擎组织数据以便快速检索的方法。生成的结构称为索引。
在 Easysearch 中，数据的基本单位是 JSON 文档。在索引中，Easysearch 使用唯一的 ID 标识每个文档。
对索引 API 的请求如下所示：
PUT &amp;lt;index&amp;gt;/_doc/&amp;lt;id&amp;gt; { &amp;#34;A JSON&amp;#34;: &amp;#34;document&amp;#34; } 对 _bulk API 的请求看起来有点不同，因为您在批量数据中指定了索引和 ID：
POST _bulk { &amp;#34;index&amp;#34;: { &amp;#34;_index&amp;#34;: &amp;#34;&amp;lt;index&amp;gt;&amp;#34;, &amp;#34;_id&amp;#34;: &amp;#34;&amp;lt;id&amp;gt;&amp;#34; } } { &amp;#34;A JSON&amp;#34;: &amp;#34;document&amp;#34; } 批量数据采用 _bulk API 操作必须符合特定的格式，该格式要求在每行（包括最后一行）的末尾都有一个换行符（ \n ）。这是基本格式：
Action and metadata\n Optional document\n Action and metadata\n Optional document\n 文档是可选的，因为 删除 操作不需要文档。其他操作（ 索引 、 创建 和 更新 ）都需要文档。如果您特别希望在文档已存在的情况下操作失败，请使用 创建 操作而不是 索引 操作。</description></item><item><title>字段权限</title><link>/easysearch/v1.14.0/docs/references/security/access-control/field-level-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/access-control/field-level-security/</guid><description>字段级权限 # 字段级权限允许您控制用户可以查看哪些文档的字段。就像 文档级权限，您可以通过角色中的索引控制访问。
包括或排除字段 # 配置字段级权限时，有两个选项：包括或排除字段。如果包含字段，则用户在检索文档时 只能看到 这些字段。例如，如果您包含 actors、title 和 year 字段，则搜索结果可能如下所示：
POST movies/_doc/1 { &amp;#34;year&amp;#34;: 2013, &amp;#34;title&amp;#34;: &amp;#34;Rush&amp;#34;, &amp;#34;actors&amp;#34;: [ &amp;#34;Daniel Brühl&amp;#34;, &amp;#34;Chris Hemsworth&amp;#34;, &amp;#34;Olivia Wilde&amp;#34; ] } 如果是排除字段，则用户在检索文档时会看到除这些字段之外的所有内容。例如，如果排除这些相同的字段，则相同的搜索结果可能如下所示：
POST movies/_doc/2 { &amp;#34;directors&amp;#34;: [ &amp;#34;Ron Howard&amp;#34; ], &amp;#34;plot&amp;#34;: &amp;#34;A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.&amp;#34;, &amp;#34;genres&amp;#34;: [ &amp;#34;Action&amp;#34;, &amp;#34;Biography&amp;#34;, &amp;#34;Drama&amp;#34;, &amp;#34;Sport&amp;#34; ] } 您可以使用配置文件 role.yml 和 REST API 来指定字段级安全设置。</description></item><item><title>ASCII 折叠分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/ascii-folding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/ascii-folding/</guid><description>ASCII 折叠分词过滤器 # ASCII 折叠(asciifolding)分词过滤器将非 ASCII 字符转换为与其最接近的 ASCII 等效字符。例如，“é” 变为 “e”，“ü” 变为 “u”，“ñ” 变为 “n”。这个过程被称为&amp;quot;音译&amp;quot;。
ASCII 折叠分词过滤器有许多优点：
增强搜索灵活性：用户在输入查询内容时常常会省略重音符号或特殊字符。ASCII 折叠分词过滤器可确保即使是这样的查询也仍然能返回相关结果。 规范化：通过确保重音字符始终被转换为其 ASCII 等效字符，使索引编制过程标准化。 国际化：对于包含多种语言和字符集的应用程序特别有用。 尽管 ASCII 折叠分词过滤器可以简化搜索，但它也可能导致特定信息的丢失，尤其是当数据集中重音字符和非重音字符之间的区别很重要的时候。
参数说明 # 你可以使用 preserve_original 参数来配置 ASCII 折叠分词过滤器。将此参数设置为 true 时，会在词元流中同时保留原始词元及其 ASCII 折叠后的版本。当你希望在搜索查询中同时匹配一个词项的原始版本（带重音符号）和规范化版本（不带重音符号）时，这一点会特别有用。该参数的默认值为 false。
参考样例 # 以下示例请求创建了一个名为 example_index 的新索引，并定义了一个分词器，该分词器使用了 ASCII 折叠过滤器，且将 preserve_original 参数设置为 true：
PUT /example_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;custom_ascii_folding&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;asciifolding&amp;quot;, &amp;quot;preserve_original&amp;quot;: true } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_ascii_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;custom_ascii_folding&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>CJK 二元分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/cjk-bigram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/cjk-bigram/</guid><description>CJK 二元分词过滤器 # CJK 二元(cjk_bigram)分词过滤器是专门为处理东亚语言（如中文、日文和韩文，简称 CJK）而设计的，这些语言通常不使用空格来分隔单词。二元组是指在词元字符串中两个相邻元素的序列，这些元素可以是字符或单词。对于 CJK 语言来说，二元组有助于近似确定单词边界，并捕捉那些能够传达意义的重要字符对。
参数说明 # CJK 二元分词过滤器可以通过两个参数进行配置：ignore_scripts（忽略字符集）和 output_unigrams（输出一元组）。
ignore_scripts（忽略字符集） # CJK 二元分词过滤器会忽略所有非 CJK 字符集（如拉丁字母或西里尔字母等书写系统），并且仅将 CJK 文本分词为二元组。可使用此选项指定要忽略的 CJK 字符集。该选项有以下有效值：
han（汉字）：汉字字符集处理汉字。汉字是用于中国、日本和韩国书面语言的意符文字。该过滤器可帮助处理文本相关任务，例如对用中文、日文汉字或韩文汉字书写的文本进行分词、规范化或词干提取。 hangul（韩文）：韩文（谚文）字符集处理韩文字符，这些字符是韩语所特有的，不存在于其他东亚字符集中。 hiragana（平假名）：平假名字符集处理平假名，平假名是日语书写系统中使用的两种音节文字之一。平假名通常用于日语的本土词汇、语法元素以及某些形式的标点符号。 katakana（片假名）：片假名字符集处理片假名，片假名是日语的另一种音节文字。片假名主要用于外来借词、拟声词、科学名称以及某些日语词汇。 output_unigrams（输出一元组） # 当此选项设置为 true 时，会同时输出一元组（单个字符）和二元组。默认值为 false。
参考样例 # 以下示例请求创建了一个名为 devanagari_example_index 的新索引，并定义了一个分词器，该分词器使用了 cjk_bigram_filter 过滤器，且将 ignored_scripts 参数设置为 katakana（片假名）：
PUT /cjk_bigram_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;cjk_bigrams_no_katakana&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;cjk_bigrams_no_katakana_filter&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;cjk_bigrams_no_katakana_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;cjk_bigram&amp;quot;, &amp;quot;ignored_scripts&amp;quot;: [ &amp;quot;katakana&amp;quot; ], &amp;quot;output_unigrams&amp;quot;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>CJK 宽度分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/cjk-width/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/cjk-width/</guid><description>CJK 宽度分词过滤器 # CJK 宽度(cjk_width)分词过滤器通过将全角 ASCII 字符转换为其标准（半角）ASCII 等效字符，并将半角片假名字符转换为全角等效字符，来对中文、日文和韩文（CJK）词元进行规范化处理。
转换全角 ASCII 字符 # 在 CJK 文本中，ASCII 字符（如字母和数字）可能会以全角形式出现，占据两个半角字符的空间。全角 ASCII 字符在东亚排版中通常用于与 CJK 字符的宽度对齐。然而，出于索引和搜索的目的，这些全角字符需要规范化为其标准（半角）ASCII 等效字符。
以下示例说明了 ASCII 字符的规范化过程：
全角: ＡＢＣＤＥ １２３４５ 规范化 (半角): ABCDE 12345 转换半角片假名字符 # CJK 宽度分词过滤器会将半角片假名字符转换为对应的全角片假名字符，全角片假名是日语文本中使用的标准形式。如下例所示，这种规范化处理对于文本处理和搜索的一致性至关重要：
半角 katakana: ｶﾀｶﾅ 规范化 (全角) katakana: カタカナ 参考样例 # 以下示例请求创建了一个名为 cjk_width_example_index 的新索引，并定义了一个包含 cjk_width 过滤器的分词器：
PUT /cjk_width_example_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;cjk_width_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;cjk_width&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>HTML 剥离字符过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/character-filters/html-strip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/character-filters/html-strip/</guid><description>HTML 剥离字符过滤器 # HTML 剥离（html_strip）字符过滤器会从输入文本中移除 HTML 标签（例如 &amp;lt;div&amp;gt;、&amp;lt;p&amp;gt; 和 &amp;lt;a&amp;gt; 等）并输出纯文本。该过滤器可以配置保留某些标签，或者配置把特定的 HTML 标签实体（如 &amp;amp;nbsp;）解码为空格。
参考样例 # 以下请求展示将 html_strip 字符过滤器应用于文本：
GET /_analyze { &amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;, &amp;quot;char_filter&amp;quot;: [ &amp;quot;html_strip&amp;quot; ], &amp;quot;text&amp;quot;: &amp;quot;&amp;lt;p&amp;gt;Commonly used calculus symbols include &amp;amp;alpha;, &amp;amp;beta; and &amp;amp;theta; &amp;lt;/p&amp;gt;&amp;quot; } 返回内容中包含的词元里，可以看到 HTML 字符已被转换为它们的解码后的值：
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;\nCommonly used calculus symbols include α, β and θ \n&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 74, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] } 参数说明 # html_strip 字符过滤器可以使用以下参数进行配置。</description></item><item><title>IK 分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/ik-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/ik-analyzer/</guid><description>IK 分词器 # IK 分词器是一款专为处理中文文本设计的分词器，高效且智能。支持 ik_smart 和 ik_max_word 两种分词模式。
IK 分词器安装 # IK 分词插件安装命令如下：
bin/easysearch-plugin install analysis-ik 同时也需要安装 ingest-common 插件：
bin/easysearch-plugin install ingest-common 如果觉得比较麻烦，也可以直接使用 Easysearch 的 bundle 包安装部署。
使用样例 # 下面的命令样例展示了 IK 的使用方式。
# 1.创建索引 PUT index_ik # 2.创建映射关系 POST index_ik/_mapping { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;, &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_smart&amp;quot; } } } # 3.写入文档 POST index_ik/_create/1 {&amp;quot;content&amp;quot;:&amp;quot;美国留给伊拉克的是个烂摊子吗&amp;quot;} POST index_ik/_create/2 {&amp;quot;content&amp;quot;:&amp;quot;公安部：各地校车将享最高路权&amp;quot;} POST index_ik/_create/3 {&amp;quot;content&amp;quot;:&amp;quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&amp;quot;} POST index_ik/_create/4 {&amp;quot;content&amp;quot;:&amp;quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&amp;quot;} # 4.</description></item><item><title>Keyword 字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/string-field-type/keyword/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/string-field-type/keyword/</guid><description>Keyword 字段类型 # keyword 字段类型包含未经分析的字符串。它只允许精确的大小写敏感匹配。
默认情况下，keyword 字段既被索引（因为 index 已启用）也存储在磁盘上（因为 doc_values 已启用）。为了减少磁盘空间，您可以通过将 index 设置为 false 来指定不索引 keyword 字段。
如果您需要对字段进行全文搜索，请将其映射为 text 类型。
代码样例 # 以下查询创建了一个带有 keyword 字段的映射。将 index 设置为 false 指定将 genre 字段存储在磁盘上，并使用 doc_values 检索它：
PUT movies { &amp;#34;mappings&amp;#34; : { &amp;#34;properties&amp;#34; : { &amp;#34;genre&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;keyword&amp;#34;, &amp;#34;index&amp;#34; : false } } } } 参数说明 # 下表列出了 keyword 字段类型接受的参数。所有参数都是可选的。
参数 描述 boost 浮点值，指定此字段对相关性分数的权重。大于 1.</description></item><item><title>KStem 分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/kstem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/kstem/</guid><description>KStem 分词过滤器 # KStem 分词过滤器是一种词干提取过滤器，用于将单词还原为其词根形式。该过滤器是一种专为英语设计的轻量级算法词干提取器，它执行以下词干提取操作：
将复数形式的单词还原为单数形式。 将不同的动词时态转换为其基本形式。 去除常见的派生词尾，例如 “-ing” 或 “-ed”。 KStem 分词过滤器等同于配置了 light_english 语言的词干提取器过滤器。与其他词干提取过滤器（如词干提取器 porter_stem）相比，它提供了更为保守的词干提取方式。
KStem 分词过滤器基于 Lucene 的 KStemFilter。如需了解更多信息，请参阅 Lucene 文档。
参考样例 # 以下示例请求创建了一个名为 my_kstem_index 的新索引，并配置了一个带有 KStem 过滤器的分词器：
PUT /my_kstem_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;kstem_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;kstem&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_kstem_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;kstem_filter&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_kstem_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>Kuromoji 补全分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/kuromoji-completion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/kuromoji-completion/</guid><description>Kuromoji 补全分词过滤器 # Kuromoji 补全（kuromoji_completion）分词过滤器用于对日语中的片假名单词进行词干提取，片假名单词常常用于表示外来词或借词。这个过滤器在自动补全或建议查询中特别有用，在这些查询中，对片假名单词的部分匹配可以扩展为包含它们的完整形式。
要使用此分词过滤器，你必须首先在所有节点上安装 analysis-kuromoji 插件，方法是运行 bin/easysearch-plugin install analysis-kuromoji，然后重新启动集群。
参考样例 # 以下示例请求创建了一个名为 kuromoji_sample 的新索引，并配置了一个带有 Kuromoji 补全过滤器的分词器：
PUT kuromoji_sample { &amp;quot;settings&amp;quot;: { &amp;quot;index&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;kuromoji_tokenizer&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_katakana_stemmer&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;my_katakana_stemmer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;kuromoji_completion&amp;quot; } } } } } } 产生的词元 # 使用以下请求，通过输入翻译为“使用电脑”的文本，来查看使用该分词器生成的词元。
POST /kuromoji_sample/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;コンピューターを使う&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;コンピューター&amp;quot;, // 原片假名单词“computer”（在日语中用片假名表示的话，比如 コンピューター ）。 &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;konpyuーtaー&amp;quot;, // “コンピューター” 的罗马字拼写（罗马音）版本是 “konpyūtā”。 &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;konnpyuーtaー&amp;quot;, // “コンピューター” 的另一种可能的罗马字拼写版本可能是 “kompyuuta”。这种差异可能是由于不同的罗马字转写系统 &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;を&amp;quot;, // 一个日语助词，“wo” 或 “o” &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 8, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;wo&amp;quot;, // 助词「を」（通常发音为“o”）的罗马字拼写形式。 &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 8, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;o&amp;quot;, // 另一种罗马字转写版本。 &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 8, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;使う&amp;quot;, // 日语中表示 “使用” 的动词 &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;tukau&amp;quot;, // “使う”（つかう）的罗马字版本是 “tsukau”。 &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;tsukau&amp;quot;, // “使う”的另一种罗马字转写形式，其中“つ”的写法在语音上更为准确。 &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>n-gram 分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/n-gram/</guid><description>n-gram 分词过滤器 # n-gram 分词过滤器是一种强大的工具，用于将文本拆分为更小的组件，即 n-gram，这有助于提升部分匹配和模糊搜索能力。它通过将一个词元拆分成指定长度的子字符串来工作。这些过滤器在搜索应用程序中很常用，可用于支持自动补全、部分匹配以及容错拼写搜索。
参数说明 # n-gram 分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_gram 可选 整数 n-gram 的最小长度。默认值为 1。 max_gram 可选 整数 n-gram 的最大长度。默认值为 2。 preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 false。 参考样例 # 以下示例请求创建了一个名为“ngram_example_index”的新索引，并配置了一个带有 n-gram 过滤器的分词器：
PUT /ngram_example_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;ngram_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 2, &amp;quot;max_gram&amp;quot;: 3 } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;ngram_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;ngram_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>N-gram 词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/n-gram/</guid><description>N-gram 词元生成器 # N-gram 词元生成器会将文本拆分为指定长度的重叠 n-gram（固定长度为 n 的字符序列）。当你希望实现部分单词匹配或自动补全搜索功能时，这个分词器特别有用，因为它会生成原始输入文本的子字符串（ n-gram 字符串）。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 n-gram 词元生成器的分词器。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_ngram_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 3, &amp;quot;max_gram&amp;quot;: 4, &amp;quot;token_chars&amp;quot;: [&amp;quot;letter&amp;quot;, &amp;quot;digit&amp;quot;] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_ngram_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_ngram_tokenizer&amp;quot; } } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_ngram_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Search&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ {&amp;quot;token&amp;quot;: &amp;quot;Sea&amp;quot;,&amp;quot;start_offset&amp;quot;: 0,&amp;quot;end_offset&amp;quot;: 3,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 0}, {&amp;quot;token&amp;quot;: &amp;quot;Sear&amp;quot;,&amp;quot;start_offset&amp;quot;: 0,&amp;quot;end_offset&amp;quot;: 4,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 1}, {&amp;quot;token&amp;quot;: &amp;quot;ear&amp;quot;,&amp;quot;start_offset&amp;quot;: 1,&amp;quot;end_offset&amp;quot;: 4,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 2}, {&amp;quot;token&amp;quot;: &amp;quot;earc&amp;quot;,&amp;quot;start_offset&amp;quot;: 1,&amp;quot;end_offset&amp;quot;: 5,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 3}, {&amp;quot;token&amp;quot;: &amp;quot;arc&amp;quot;,&amp;quot;start_offset&amp;quot;: 2,&amp;quot;end_offset&amp;quot;: 5,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 4}, {&amp;quot;token&amp;quot;: &amp;quot;arch&amp;quot;,&amp;quot;start_offset&amp;quot;: 2,&amp;quot;end_offset&amp;quot;: 6,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 5}, {&amp;quot;token&amp;quot;: &amp;quot;rch&amp;quot;,&amp;quot;start_offset&amp;quot;: 3,&amp;quot;end_offset&amp;quot;: 6,&amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;,&amp;quot;position&amp;quot;: 6} ] } 参数说明 # N-gram 词元生成器可以使用以下参数进行配置。</description></item><item><title>Rank 字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/rank/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/rank/</guid><description>Rank 字段类型 # 下表列出了 Easysearch 支持的所有 rank 字段类型。
字段数据类型 描述 rank_feature 提升或降低文档的相关性得分。 rank_features 提升或降低文档的相关性得分。用于特征列表稀疏的情况。 Rank feature 和 rank features 字段只能使用 rank feature 查询进行查询。它们不支持聚合或排序。
Rank feature 字段类型 # Rank feature 字段类型使用正浮点值来提升或降低文档在 rank_feature 查询中的相关性得分。默认情况下，该值会提升相关性得分。要降低相关性得分，请将可选参数 positive_score_impact 设置为 false。
示例 # 创建一个包含 rank feature 字段的映射：
PUT chessplayers { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;name&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34; }, &amp;#34;rating&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;rank_feature&amp;#34; }, &amp;#34;age&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;rank_feature&amp;#34;, &amp;#34;positive_score_impact&amp;#34;: false } } } } 索引三个文档，其中包含一个提升得分的 rank_feature 字段（rating）和一个降低得分的 rank_feature 字段（age）：</description></item><item><title>SQL-JDBC</title><link>/easysearch/v1.14.0/docs/references/sql/sql-jdbc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/sql/sql-jdbc/</guid><description>SQL-JDBC 驱动 # Easysearch 的 SQL jdbc 驱动程序是一个独立、直连的纯 Java 驱动程序，可将 JDBC 调用转换为 Easysearch SQL。
安装 # JDBC 驱动 可以从官网下载：https://release.infinilabs.com/easysearch/archive/plugins/sql-jdbc-1.7.1.jar
在 gradle 项目中安装 # 需要把sql-jdbc-1.x.x.jar 集成到本地 gradle 项目的libs目录，假设项目名称叫 jdbc-test
将下载的sql-jdbc jar 包放到 jdbc-test/libs/ 下：
在 项目 build.gradle 添加依赖
implementation files(&amp;lsquo;libs/sql-jdbc-1.0.0.jar&amp;rsquo;)
完整的build.gradle 配置：
plugins { id 'java' } group 'org.example' version '1.0-SNAPSHOT' repositories { mavenCentral() flatDir { dirs 'libs' } } dependencies { implementation files('libs/sql-jdbc-1.0.0.jar') testImplementation 'org.junit.jupiter:junit-jupiter-api:5.7.0' testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.7.0' } test { useJUnitPlatform() } 初始化 # String url = &amp;quot;jdbc:easysearch://https://localhost:9210&amp;quot;; Properties properties = new Properties(); properties.</description></item><item><title>Text 字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/string-field-type/text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/string-field-type/text/</guid><description>Text 字段类型 # text 字段类型包含经过分词器分析的字符串。它用于全文搜索，因为它允许部分匹配。对多个词条的搜索可以匹配其中的一部分而不是全部。根据分词器的不同，搜索结果可以是大小写不敏感的、词干化的、去除停用词的、应用同义词的等等。
如果您需要进行精确值搜索，请将字段映射为 keyword 类型。
代码样例 # 创建一个带有 text 字段的映射：
PUT movies { &amp;#34;mappings&amp;#34; : { &amp;#34;properties&amp;#34; : { &amp;#34;title&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;text&amp;#34; } } } } 参数说明 # 下表列出了 text 字段类型接受的参数。所有参数都是可选的。
参数 描述 analyzer 用于此字段的分词器。默认情况下，它将在索引时和搜索时使用。要在搜索时覆盖它，请设置 search_analyzer 参数。默认是 standard 分词器，它使用基于语法的分词，并基于 Unicode 文本分段算法。 boost 浮点值，指定此字段对相关性分数的权重。大于 1.0 的值会增加字段的相关性。0.0 到 1.0 之间的值会降低字段的相关性。默认值为 1.0。 eager_global_ordinals 指定是否应在刷新时立即加载全局序号。如果该字段经常用于聚合，此参数应设置为 true。默认值为 false。 fielddata 布尔值，指定是否访问此字段的已分析标记以进行排序、聚合和脚本编写。默认值为 false。 fielddata_frequency_filter JSON 对象，指定仅将文档频率在 min 和 max 值之间的已分析标记加载到内存中（以绝对数字或百分比提供）。频率按段计算。参数：min、max、min_segment_size。默认加载所有已分析的标记。 fields 要以多种方式索引同一个字符串（例如，作为 keyword 和 text），请提供 fields 参数。您可以指定字段的一个版本用于搜索，另一个版本用于排序和聚合。 index 布尔值，指定字段是否应可搜索。默认值为 true。 index_options 指定要存储在索引中用于搜索和突出显示的信息。有效值：docs（仅文档编号）、freqs（文档编号和词频）、positions（文档编号、词频和词位置）、offsets（文档编号、词频、词位置以及开始和结束字符偏移量）。默认值为 positions。 index_phrases 布尔值，指定是否单独索引 2-gram。2-gram 是此字段字符串中两个连续单词的组合。导致精确短语查询更快但索引更大。当不删除停用词时效果最好。默认值为 false。 index_prefixes JSON 对象，指定单独索引词条前缀。前缀中的字符数在 min_chars 和 max_chars 之间（包含）。导致前缀搜索更快但索引更大。可选参数：min_chars、max_chars。默认 min_chars 为 2，max_chars 为 5。 meta 接受此字段的元数据。 norms 布尔值，指定在计算相关性分数时是否应使用字段长度。默认值为 false。 position_increment_gap 当文本字段被分析时，它们被分配位置。如果一个字段包含一个字符串数组，并且这些位置是连续的，这将导致可能跨不同数组元素匹配。为防止这种情况，在连续的数组元素之间插入一个人工间隙。您可以通过指定整数 position_increment_gap 来更改此间隙。注意：如果 slop 大于 position_element_gap，可能会发生跨不同数组元素的匹配。默认值为 100。 similarity 用于计算相关性分数的排名算法。默认值为 BM25。 term_vector 布尔值，指定是否应存储此字段的词向量。默认值为 no。 词向量参数 # 词向量在分析过程中产生。它包含：</description></item><item><title>UAX URL 邮件词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/uax-url-email/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/uax-url-email/</guid><description>UAX URL 邮件词元生成器 # 除了常规文本之外，UAX URL 邮件（uax_url_email）词元生成器还专门用于处理网址、电子邮件地址和域名。它基于 Unicode 文本分割算法（ UAX #29），这使得它能够正确地对复杂文本（包括网址和电子邮件地址）进行分词处理。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 UAX URL 邮件词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;uax_url_email_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;uax_url_email&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_uax_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;uax_url_email_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_uax_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_uax_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Contact us at support@example.</description></item><item><title>保留类型分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keep-types/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keep-types/</guid><description>保留类型分词过滤器 # 保留类型（keep_types）分词过滤器是一种用于文本分析的分词过滤器，它可用于控制保留或丢弃哪些词元类型。不同的分词器会产生不同的词元类型，例如 &amp;lt;HOST&amp;gt;、&amp;lt;NUM&amp;gt; 或 &amp;lt;ALPHANUM&amp;gt;。
分词器keyword）、简单匹配（simple_pattern）和简单匹配拆分（simple_pattern_split）分词器不支持保留类型分词过滤器，因为这些分词器不支持词元类型属性。
参数说明 # 保留类型分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 types 必需 字符串列表 要保留或丢弃的词元类型列表（由 mode 参数决定）。 mode 可选 字符串 是要包含(include)还是排除(exclude) types 中指定的词元类型。默认值为 include（包含）。 参考样例 # 以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有保留类型过滤器的分析器：
PUT /test_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;keep_types_filter&amp;quot;] } }, &amp;quot;filter&amp;quot;: { &amp;quot;keep_types_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keep_types&amp;quot;, &amp;quot;types&amp;quot;: [&amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>修剪词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/trim/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/trim/</guid><description>修剪词元过滤器 # 修剪(trim)词元过滤器会从词元中去除前导和尾随的空白字符。
许多常用的分词器，例如标准(standard)分词器、关键字(keyword)分词器和空白(whitespace)分词器，在分词过程中会自动去除前导和尾随的空白字符。当使用这些分词器时，无需额外配置修剪词元过滤器。
参考样例 # 以下示例请求创建了一个名为 my_pattern_trim_index 的新索引，并配置了一个带有修剪过滤器和匹配词元生成器的分词器。其中，匹配词元生成器不会去除词元的前导和尾随空白字符。
PUT /my_pattern_trim_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_trim_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;trim&amp;quot; } }, &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_pattern_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;,&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_trim_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_pattern_tokenizer&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_trim_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
GET /my_pattern_trim_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_trim_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot; Easysearch , is , powerful &amp;quot; } 返回内容包含产生的词元</description></item><item><title>停用词分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/stop-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/stop-analyzer/</guid><description>停用词分词器 # 停用词（stop）分词器会在文本中移除预定义的停用词。该分词器由一个小写词元生成器和一个停用词词元过滤器组成。
参数说明 # 你可以使用以下参数来配置一个停用词分词器。
参数 必填/可选 数据类型 描述 stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_stop_index 并使用停词器分词器的索引：
PUT /my_stop_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;stop&amp;quot; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于停用词分词器：
PUT /my_custom_stop_analyzer_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_stop_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;lowercase&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;stop&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_stop_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>停用词词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stop/</guid><description>停用词词元过滤器 # 停用词（stop）词元过滤器用于在分词过程中从词元流中去除常见词汇（也称为停用词）。停用词通常是冠词和介词，比如“a” 或 “for” 。这些词在搜索查询中意义不大，并且常常被排除在外，以提高搜索效率和相关性。
默认的英语停用词列表包含以下单词：a、an、and、are、as、at、be、but、by、for、if、in、into、is、it、no、not、of、on、or、such、that、the、their、then、there、these、they、this、to、was、will 以及 with。
参数说明 # 停用词词元过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 stopwords 可选 字符串 既可以指定自定义的停用词数组，也可以指定一种语言以获取预定义的 Lucene 停用词列表。可指定的语言如下：
- _arabic_
- _armenian_
- _basque_
- _bengali_
- _brazilian_（巴西葡萄牙语）
- _bulgarian_
- _catalan_
- _cjk_（中文、日语和韩语）
- _czech_
- _danish_
- _dutch_
- _english_（默认值）
- _estonian_
- _finnish_
- _french_
- _galician_
- _german_
- _greek_
- _hindi_
- _hungarian_</description></item><item><title>关键词分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/keyword-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/keyword-analyzer/</guid><description>关键词分词器 # 关键词（keyword）分词器根本不会对文本进行分词。相反，它将整个输入视为单个词元，不会将其拆分成单个的词项。关键词分词器常用于包含电子邮件地址、网址或产品 ID 的字段，以及其他不需要进行分词的情况。
参考样例 # 以下命令创建一个名为 my_keyword_index 使用关键词分词器的索引：
PUT /my_keyword_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;keyword&amp;quot; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于关键词分词器：
PUT /my_custom_keyword_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_keyword_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot; } } } } } 产生的词元 # 以下请求来检查使用该分词器生成的词元：
POST /my_custom_keyword_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_keyword_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Just one token&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Just one token&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 14, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] }</description></item><item><title>关键词标记分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keyword-marker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keyword-marker/</guid><description>关键词标记分词过滤器 # 关键词标记（keyword_marker）分词过滤器用于防止某些词元被词干提取器或其他过滤器修改。该过滤器通过将指定的词元标记为关键词来实现这一点，这样就能避免词干提取或其他方式的处理。这确保了特定的单词能保持其原始形式。
参数说明 # 关键词标记分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 ignore_case 可选 布尔值 在匹配关键词时是否忽略字母大小写。默认值为false。 keywords 若未设置 keywords_path 或 keywords_pattern 则为必需 字符串列表 要标记为关键词的词元列表。 keywords_path 若未设置 keywords 或 keywords_pattern 则为必需 字符串 关键词列表的路径（相对于配置目录或绝对路径）。 keywords_pattern 若未设置 keywords 或 keywords_path 则为必需 字符串 用于匹配要标记为关键词的词元的正则表达式。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词标记过滤器的分词器。该过滤器将单词 example 标记为关键词：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;keyword_marker_filter&amp;quot;, &amp;quot;stemmer&amp;quot;] } }, &amp;quot;filter&amp;quot;: { &amp;quot;keyword_marker_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword_marker&amp;quot;, &amp;quot;keywords&amp;quot;: [&amp;quot;example&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>关键词词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/keyword/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/keyword/</guid><description>关键词词元生成器 # 关键字（keyword）词元生成器接收文本并将其原封不动地作为单个词元输出。当你希望输入内容保持完整时，比如在管理像姓名、产品代码或电子邮件地址这类结构化数据时，这个词元生成器就特别有用。
关键字词元生成器可以与词元过滤器搭配使用来处理文本。例如，对文本进行规范化处理或去除多余的字符。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用关键字词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_keyword_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_keyword_analyzer&amp;quot; } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_keyword_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch Example&amp;quot; } 返回内容会是包含原始内容的单个词元：
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Easysearch Example&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 18, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] } 参数说明 # 关键字词元生成器可以使用以下参数进行配置。</description></item><item><title>关键词重复分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keyword-repeat/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keyword-repeat/</guid><description>关键词重复分词过滤器 # 关键词重复（keyword_repeat）分词过滤器会将词元的关键词版本发松到词元流中。当你希望在经过进一步的词元转换（例如词干提取或同义词扩展）之后，既保留原始词元，又保留其修改后的版本时，通常会使用这个过滤器。这些重复的词元使得原始的、未改变的词元版本能够与修改后的版本一起保留在最终的分析结果中。
关键词重复分词过滤器应该放置在词干提取过滤器之前。词干提取并非应用于每个词元，因此在词干提取之后，你可能会在同一位置得到重复的词元。为了去除重复词元，可以在词干提取器之后使用 remove_duplicates 分词过滤器。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词重复过滤器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_kstem&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;kstem&amp;quot; }, &amp;quot;my_lowercase&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_lowercase&amp;quot;, &amp;quot;keyword_repeat&amp;quot;, &amp;quot;my_kstem&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Stopped quickly&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;stopped&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;stop&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;quickly&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;quick&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 } ] } 你可以通过在 _analyze 查询中添加以下参数，进一步研究关键词重复分词过滤器的影响：</description></item><item><title>写入数据文本向量化</title><link>/easysearch/v1.14.0/docs/references/AI-Integration/ingest-text-embedding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/AI-Integration/ingest-text-embedding/</guid><description>写入数据文本向量化 # Easysearch 使用摄取管道 ingest pipeline 中的一系列处理器，可以对写入的数据进行处理，并且支持对文本进行向量化，本文档介绍如何在 Easysearch 中使用 text_embedding 处理器对写入数据进行向量化。
先决条件 # 支持与 OpenAI API 兼容的 embedding 接口，支持 Ollama embedding 接口。
需要安装 Easysearch 的 knn 和 ai 插件。
在生产环境中使用数据采集时，您的集群应至少包含一个节点，且该节点的节点角色权限设置为 ingest 。
创建带有向量字段的索引 # 首先，需要创建一个包含 knn mapping 的索引，text_vector 是存储向量的字段，向量维度是 768。
PUT /my-index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text_vector&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;knn_dense_float_vector&amp;quot;, &amp;quot;knn&amp;quot;: { &amp;quot;dims&amp;quot;: 768, &amp;quot;model&amp;quot;: &amp;quot;lsh&amp;quot;, &amp;quot;similarity&amp;quot;: &amp;quot;cosine&amp;quot;, &amp;quot;L&amp;quot;: 99, &amp;quot;k&amp;quot;: 1 } } } } } 创建或更新 text_embedding 处理器 # 请求路径：</description></item><item><title>分隔式负载分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/delimited-payload/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/delimited-payload/</guid><description>分隔式负载分词过滤器 # 分隔式负载（delimited_payload）分词过滤器用于在分词过程中解析包含负载的词元。例如，字符串 red|1.5 fast|2.0 car|1.0 会被解析为词元 red（负载为 1.5）、fast（负载为 2.0）和 car（负载为 1.0）。当你的词元包含额外的关联数据（如权重、分数或其他数值），且你打算将这些数据用于评分或自定义查询逻辑时，这个过滤器就特别有用。该过滤器可以处理不同类型的负载，包括整数、浮点数和字符串，并将负载（额外的元数据）附加到词元上。
在文本分词时，分隔式负载分词过滤器会解析每个词元，提取负载并将其附加到词元上。后续在查询中可以使用这个负载来影响评分、提升权重或实现其他自定义行为。
负载以 Base64 编码的字符串形式存储。默认情况下，负载不会随词元一起在查询响应中返回。若要返回负载，你必须配置额外的参数。更多信息，请参阅“带有负载存储的分词示例”。
参数说明 # 分隔式负载分词过滤器有两个参数：
参数 必需/可选 数据类型 描述 encoding 可选 字符串 指定附加到词元的负载的数据类型。这决定了在分析和查询期间如何解释负载数据。
有效值为：
- float：使用 IEEE 754 格式将负载解释为 32 位浮点数（例如，car\|2.5 中的 2.5）。
- identity：将负载解释为字符序列（例如，在 user\|admin 中，admin 被解释为字符串）。
- int：将负载解释为 32 位整数（例如，priority \| 1中的1）。
默认值为 float。 delimiter 可选 字符串 指定在输入文本中分隔词元及其负载的字符。默认值为竖线字符（\|）。 不带负载存储的分词示例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有分隔式负载过滤器的分词器：</description></item><item><title>创建一个自定义分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/creating-a-custom-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/creating-a-custom-analyzer/</guid><description>创建一个自定义分词器 # 要创建一个自定义分词器，需要指定以下组成内容：
字符过滤器（零个或多个） 词元生成器（一个） 词元过滤器（零个或多个） 相关配置 # 以下参数可用于配置自定义分词器。
参数 必填/可选 描述 type 可选 分词器类型。默认值为custom。你也可以给这个参数指定一个预构建的分词器。 tokenizer 必填 每个分词器必须要有一个词元生成器。 char_filter 可选 要包含在分词器中的字符过滤器列表。 filter 可选 要包含在分词器中的分词过滤器列表。 position_increment_gap 可选 在对包含多个词项内容的文本字段建立索引时，应用于各词项之间的额外间隔。有关更多信息，请参阅位置间隔增量。默认值为 100。 参考样例 # 以下示例展示了各种自定义分词器的配置。
自定义分词器用于去除 HTML 格式标签 # 以下示例中的分词器在分词之前会从文本中移除 HTML 格式标签：
PUT simple_html_strip_analyzer_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;html_strip_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;char_filter&amp;quot;: [&amp;quot;html_strip&amp;quot;], &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;] } } } } } 使用以下请求来查看使用该分词器生成的词元：</description></item><item><title>别名操作</title><link>/easysearch/v1.14.0/docs/references/document/index-alias/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/document/index-alias/</guid><description>索引别名 # 别名是可以指向一个或多个索引的虚拟索引名称。
如果数据分布在多个索引中，而不是跟踪要查询的索引，则可以创建别名并进行查询。
例如，如果要将日志存储到基于月份的索引中，并且经常查询前两个月的日志，则可以创建一个 last_2_months 别名，并每月更新其指向的索引。
您可以随时更改别名指向的索引，所以在应用程序中使用别名引用索引可以让您在不停机的情况下重新索引数据。
创建索引别名 # 要创建索引别名，请使用 POST 请求:
POST _aliases 使用 actions 方法指定要执行的操作列表。此命令创建名为 alias1 的别名，并将 index-1 添加到此别名：
POST _aliases { &amp;#34;actions&amp;#34;: [ { &amp;#34;add&amp;#34;: { &amp;#34;index&amp;#34;: &amp;#34;index-1&amp;#34;, &amp;#34;alias&amp;#34;: &amp;#34;alias1&amp;#34; } } ] } 您应该看到以下响应:
{ &amp;#34;acknowledged&amp;#34;: true } 如果此请求失败，请确保要添加到别名的索引已存在.
要检查 alias1 是否引用 index-1 ，请运行以下命令
GET alias1 索引别名添加与删除操作 # 您可以在同一 别名 操作中执行多个操作。
例如，以下命令删除 index-1 并将 index-2 添加到 alias1 ：
POST _aliases { &amp;#34;actions&amp;#34;: [ { &amp;#34;remove&amp;#34;: { &amp;#34;index&amp;#34;: &amp;#34;index-1&amp;#34;, &amp;#34;alias&amp;#34;: &amp;#34;alias1&amp;#34; } }, { &amp;#34;add&amp;#34;: { &amp;#34;index&amp;#34;: &amp;#34;index-2&amp;#34;, &amp;#34;alias&amp;#34;: &amp;#34;alias1&amp;#34; } } ] } add 和 remove 操作以原子方式发生，这意味着 alias1 不会同时指向 index-1 和 index-2 .</description></item><item><title>前缀 n 元词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/edge-n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/edge-n-gram/</guid><description>前缀 n 元词元生成器 # 前缀 n 元词元生成器会从每个单词的开头生成部分单词词元，也就是 n 元组。它会根据指定的字符分割文本，并在定义的最小和最大长度范围内生成词元。这种词元生成器在实现即输即搜（search-as-you-type）功能时特别有用。
前缀 n 元组非常适合用于自动补全搜索，在这种搜索中单词的顺序可能会有所不同，例如在搜索产品名称或地址时。有关更多信息，请参阅 “自动补全” 相关内容。不过，对于像电影或歌曲标题这种顺序固定的文本，完成建议器(completion suggester)可能会更准确。
默认情况下，前缀 n 元词元生成器生成的词元最小长度为 1，最大长度为 2。例如，当分析文本 Easysearch 时，默认配置会生成 “E” 和 “Ea” 这两个 n 元组。这些短的 n 元组常常会匹配到太多不相关的词条，所以调整 n 元组的长度，对词元生成器进行优化是很有必要的。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用前缀 n 元词元生成器的分词器。该词元生成器生成长度为 3 到 6 个字符的词元，且将字母和符号都视为有效的词元字符。
PUT /edge_n_gram_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;my_custom_tokenizer&amp;quot; } }, &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_custom_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 3, &amp;quot;max_gram&amp;quot;: 6, &amp;quot;token_chars&amp;quot;: [ &amp;quot;letter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>匹配替换字符过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/character-filters/pattern-replace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/character-filters/pattern-replace/</guid><description>匹配替换字符过滤器 # 匹配替换（pattern_replace）字符过滤器使你能够使用正则表达式来定义文本匹配替换的模式。对于文本转换的高阶需求场景，尤其是在处理复杂的字符串模式时，它是一种的灵活工具。
这个过滤器会用替换符合匹配模式的所有匹配项，从而可以轻松地对输入文本进行替换、删除或复杂的修改。你可以在分词之前使用它对输入内容进行规范化处理。
参考样例 # 为了规范电话号码，你可以使用正则表达式 [\\s()-]+去替换号码里的特殊格式：
[]：定义一个字符类，意味着它将匹配方括号内的任意一个字符。 \\s：匹配任何空白字符，如空格、制表符或换行符。 ()：匹配字面意义上的括号（( 或 )）。 -：匹配字面意义上的连字符（-）。 +：指定该模式应匹配前面字符的一次或多次出现。 模式 [\\s()-]+ 将匹配由一个或多个空白字符、括号或连字符组成的任意序列，并将其从输入文本中移除。这确保了电话号码得到规范处理，结果将仅包含数字。
以下请求通过移除空格、连字符和括号来规范电话号码：
GET /_analyze { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;char_filter&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;[\\s()-]+&amp;quot;, &amp;quot;replacement&amp;quot;: &amp;quot;&amp;quot; } ], &amp;quot;text&amp;quot;: &amp;quot;(555) 123-4567&amp;quot; } 返回内容中包含生成的词元：
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;5551234567&amp;quot;, &amp;quot;start_offset&amp;quot;: 1, &amp;quot;end_offset&amp;quot;: 14, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] } 参数说明 # pattern_replace 字符过滤器必须使用以下参数进行配置。
参数 必需/可选 数据类型 描述 pattern 必需 字符串 用于匹配输入文本部分内容的正则表达式。过滤器会识别并匹配此模式以执行替换操作。 replacement 可选 字符串 用于替换匹配内容的字符串。使用空字符串（&amp;quot;&amp;quot;）可移除匹配到的文本。默认值为空字符串（&amp;quot;&amp;quot;）。 创建自定义分词器 # 以下请求创建一个索引，该索引带有一个配置了 pattern_replace 字符过滤器的自定义分词器。此过滤器会从数字中移除货币符号以及千位分隔符（包括欧洲的 “.</description></item><item><title>匹配替换词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/pattern-replace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/pattern-replace/</guid><description>匹配替换词元过滤器 # 匹配替换词元过滤器允许您使用正则表达式来修改词元。此过滤器会将词元中的模式替换为指定的值，这使您在对词元进行索引之前，能够灵活地转换或规范化词元。当您在分析过程中需要清理或规范文本时，该过滤器尤其有用。
参数说明 # 匹配替换词元过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 pattern 必需 字符串 一个正则表达式模式，用于匹配需要被替换的文本。 all 可选 布尔值 是否替换所有匹配的模式。如果为 false，则仅替换第一个匹配项。默认值为 true。 replacement 可选 字符串 用于替换匹配模式的字符串。默认值为空字符串。 参考样例 # 以下示例请求创建了一个名为 text_index 的新索引，并配置了一个带有匹配替换过滤器的分词器，用于将包含数字的词元替换为字符串 [NUM]：
PUT /text_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;number_replace_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;\\d+&amp;quot;, &amp;quot;replacement&amp;quot;: &amp;quot;[NUM]&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;number_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;number_replace_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>匹配模式分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/pattern-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/pattern-analyzer/</guid><description>匹配模式分词器 # 匹配模式（pattern）分词器允许你定义一个自定义分词器，该分词器使用正则表达式（regex）将输入文本分割成词元。它还提供了应用正则表达式、将词元转换为小写以及过滤掉停用词的功能。
参数说明 # 匹配模式分词器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 pattern 可选 字符串 用于对输入内容进行分词匹配的 Java 正则表达式。默认值是 \W+。 flags 可选 字符串 以竖线分隔的 Java 正则表达式的字符串标志，这些标志会修改正则表达式的行为。 lowercase 可选 布尔值 是否将词元转换为小写。默认值为 true。 stopwords 可选 字符串或字符串列表 个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_pattern_index 并使用匹配模式分词器的索引：
PUT /my_pattern_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;\\W+&amp;quot;, &amp;quot;lowercase&amp;quot;: true, &amp;quot;stopwords&amp;quot;: [&amp;quot;and&amp;quot;, &amp;quot;is&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>匹配词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/pattern/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/pattern/</guid><description>匹配词元生成器 # 匹配（pattern）词元生成器是一种高度灵活的词元生成器，它允许你根据自定义的 Java 正则表达式将文本拆分为词元。与使用 Lucene 正则表达式的简单匹配（simple_pattern）词元生成器和简单分割匹配（simple_pattern_split）词元生成器不同，匹配词元生成器能够处理更复杂、更细致的正则表达式模式，从而让你对文本的分词方式拥有更强的掌控力。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用匹配词元生成器的分词器。该分词器会在 -、_ 或 . 字符处对文本进行分割。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_pattern_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;[-_.]&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_pattern_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch-2024_v1.</description></item><item><title>十进制数字分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/decimal-digit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/decimal-digit/</guid><description>十进制数字分词过滤器 # 十进制数字（decimal_digit）分词过滤器用于将各种字符集中的十进制数字字符（0 到 9）规范化为它们对应的 ASCII 字符。当你希望在文本分析中确保所有数字都能被统一处理，而不管这些数字是以何种字符集书写时，这个过滤器就非常有用。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有十进制数字过滤器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_decimal_digit_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;decimal_digit&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;my_decimal_digit_filter&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;123 ١٢٣ १२३&amp;quot; } text分词：
“123”（ASCII 数字） “١٢٣”（阿拉伯 - 印度数字） “१२३”（梵文数字） 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;123&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 3, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;123&amp;quot;, &amp;quot;start_offset&amp;quot;: 4, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;123&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 11, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;NUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>单词分隔符图词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/word-delimiter-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/word-delimiter-graph/</guid><description>单词分隔符图词元过滤器 # 单词分隔符图(word_delimiter_graph)词元过滤器用于依据预定义的字符对词元进行拆分，还能基于可定制的规则对词元进行可选的规范化处理。
单词分隔符图词元过滤器可用于去除零件编号或产品 ID 这类复杂标识符中的标点符号。在这种情况下，它最好与关键字分词器搭配使用。对于带有连字符的单词，建议使用同义词图词元过滤器而非单词分隔符图词元过滤器，因为用户在搜索这些词时，往往既会搜索带连字符的形式，也会搜索不带连字符的形式。
默认情况下，该过滤器会应用以下规则：
描述 输入 输出 将非字母数字字符视为分隔符 ultra-fast ultra, fast 去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder 当字母大小写发生转换时拆分词元 Easysearch Easy, search 当字母和数字之间发生转换时拆分词元 T1000 T, 1000 去除词元末尾的所有格形式（&amp;lsquo;s） John's John 重要的是，不要将此过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并且会影响诸如 catenate_all 或 preserve_original 之类的选项。我们建议将此过滤器与关键字(keyword)分词器或空白(whitespace)分词器搭配使用。
参数说明 # 你可以使用以下参数来配置单词分隔符图词元过滤器。
参数 必需/可选 数据类型 描述 adjust_offsets 可选 布尔值 决定是否要为拆分或连接后的词元重新计算词元偏移量。若为 true，过滤器会调整词元偏移量，以准确呈现词元在词元流中的位置。这种调整能确保词元在文本中的位置与处理后的修改形式相匹配，这对高亮显示或短语查询等应用特别有用。若为 false，偏移量保持不变，这可能会导致处理后的词元映射回原始文本位置时出现错位。如果你的分词器使用了像 trim 这类会改变词元长度但不改变偏移量的过滤器，建议将此参数设为 false。默认值为 true。 catenate_all 可选 布尔值 从一系列字母数字部分生成连接后的词元。例如，quick-fast-200 会变成 [ quickfast200, quick, fast, 200 ]。默认值为 false。 catenate_numbers 可选 布尔值 连接数字序列。例如，10-20-30 会变成 [ 102030, 10, 20, 30 ]。默认值为 false。 catenate_words 可选 布尔值 连接字母单词。例如，high-speed-level 会变成 [ highspeedlevel, high, speed, level ]。默认值为 false。 generate_number_parts 可选 布尔值 若为 true，输出中会包含纯数字词元（仅由数字组成的词元）。默认值为 true。 generate_word_parts 可选 布尔值 若为 true，输出中会包含纯字母词元（仅由字母字符组成的词元）。默认值为 true。 ignore_keywords 可选 布尔值 是否处理标记为关键字的词元。默认值为 false。 preserve_original 可选 布尔值 在输出中，除了生成的词元外，还保留原始词元（可能包含非字母数字分隔符）。例如，auto-drive-300 会变成 [ auto - drive - 300, auto, drive, 300 ]。如果为 true，该过滤器会生成索引不支持的多位置词元，因此请勿在索引分词器中使用此过滤器，或者在该过滤器之后使用 flatten_graph 过滤器。默认值为 false。 protected_words 可选 字符串数组 指定不应被拆分的词元。 protected_words_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含不应被分隔的词元，词元需分行列出。 split_on_case_change 可选 布尔值 在连续字母大小写不同（一个为小写，另一个为大写）的位置拆分词元。例如，“EasySearch” 会变成 [ Easy, Search ]。默认值为 true。 split_on_numerics 可选 布尔值 在连续字母和数字的位置拆分词元。例如，v8engine 会变成 [ v, 8, engine ]。默认值为 true。 stem_english_possessive 可选 布尔值 去除英语所有格结尾，如 &amp;lsquo;s。默认值为 true。 type_table 可选 字符串数组 一个自定义映射，用于指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。例如，要将连字符（-）视为字母数字字符，可指定 [&amp;quot;- =&amp;gt; ALPHA&amp;quot;]，这样单词就不会在连字符处拆分。有效类型有：</description></item><item><title>单词分隔符词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/word-delimiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/word-delimiter/</guid><description>单词分隔符词元过滤器 # 单词分隔符(word_delimiter)词元过滤器用于根据预定义的字符拆分词元，还能根据可定制规则对词元进行可选的规范化处理。
我们建议尽可能使用 word_delimiter_graph 过滤器而非 word_delimiter 过滤器，因为 word_delimiter 过滤器有时会生成无效的词元图。
word_delimiter 过滤器可用于从零件编号或产品 ID 等复杂标识符中去除标点符号。在这种情况下，它最好与关键字分词器配合使用。对于带连字符的单词，建议使用同义词图(synonym_graph)词元过滤器而非 word_delimiter 过滤器，因为用户搜索这些词项时，常常既会搜索带连字符的形式，也会搜索不带连字符的形式。
默认情况下，该过滤器应用以下规则：
描述 输入 输出 将非字母数字字符视为分隔符 ultra-fast ultra, fast 去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder 当字母大小写发生转换时拆分词元 Easysearch Easy, search 当字母和数字之间发生转换时拆分词元 T1000 T, 1000 去除词元末尾的所有格形式（&amp;lsquo;s） John's John 重要提示：不要将该过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并影响诸如 catenate_all 或 preserve_original 等选项的效果。我们建议将此过滤器与关键(keyword)字分词器或空白(whitespace)分词器配合使用。
参数说明 # 你可以使用以下参数配置单词分隔符词元过滤器。</description></item><item><title>去重词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/remove-duplicates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/remove-duplicates/</guid><description>去重词元过滤器 # 去重（remove_duplicates）词元过滤器用于去除在分词过程中在相同位置生成的重复词元。
参考样例 # 以下示例请求创建了一个带有 keyword_repeat（关键词重复）词元过滤器的索引。该过滤器会在每个词元自身所在的相同位置添加该词元的关键词版本，然后使用 kstem（K 词干提取）过滤器来创建该词元的词干形式。
PUT /example-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;keyword_repeat&amp;quot;, &amp;quot;kstem&amp;quot; ] } } } } } 使用以下请求来分析字符串 “Slower turtle”。
GET /example-index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;custom_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Slower turtle&amp;quot; } 返回内容中在同一位置包含了两次词元 “turtle”。
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;slower&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;slow&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;turtle&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;turtle&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 } ] } 可以通过在索引设置中添加一个去重词元过滤器来移除重复的词元。</description></item><item><title>反转词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/reverse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/reverse/</guid><description>反转词元过滤器 # 反转（reverse）词元过滤器会反转每个词元中字符的顺序，这样在分析过程中，后缀信息就会位于反转后词元的开头。
这对于基于后缀的搜索很有用：
反转词元过滤器在你需要进行基于后缀的搜索时很有帮助，例如以下场景：
后缀匹配：根据单词的后缀来搜索单词，比如识别以特定后缀结尾的单词（例如 -tion 或 -ing）。 文件扩展名搜索：通过文件的扩展名来搜索文件，例如 .txt 或 .jpg。 自定义排序或排名：通过反转词元，你可以基于后缀实现独特的排序或排名逻辑。 后缀自动补全：实现基于后缀而非前缀的自动补全建议。 参考说明 # 以下示例请求创建了一个名为 my-reverse-index 的新索引，并配置了一个带有反转过滤器的分词器。
PUT /my-reverse-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;reverse_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;reverse&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_reverse_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;reverse_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
GET /my-reverse-index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_reverse_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;hello world&amp;quot; } 返回内容包含产生的词元</description></item><item><title>各类语种分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/language-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/language-analyzer/</guid><description>各类语种分词器 # Easysearch 在分词器选项中支持以下语种：阿拉伯语（arabic）、亚美尼亚语（armenian）、巴斯克语（basque）、孟加拉语（bengali）、巴西葡萄牙语（brazilian）、保加利亚语（bulgarian）、加泰罗尼亚语（catalan）、捷克语（czech）、丹麦语（danish）、荷兰语（dutch）、英语（english）、爱沙尼亚语（estonian）、芬兰语（finnish）、法语（french）、加利西亚语（galician）、德语（german）、希腊语（greek）、印地语（hindi）、匈牙利语（hungarian）、印度尼西亚语（indonesian）、爱尔兰语（irish）、意大利语（italian）、拉脱维亚语（latvian）、立陶宛语（lithuanian）、挪威语（norwegian）、波斯语（persian）、葡萄牙语（portuguese）、罗马尼亚语（romanian）、俄语（russian）、索拉尼语（sorani）、西班牙语（spanish）、瑞典语（swedish）、土耳其语（turkish）以及泰语（thai）。
当你映射索引时要使用该分词器，需在查询中指定相应的值。例如，要使用法语语种分词器来映射您的索引，为分词器字段指定值 french：
&amp;quot;analyzer&amp;quot;: &amp;quot;french&amp;quot; 参考样例 # 以下请求指定了一个名为 my-index 的索引，其中 content 字段被配置为多字段，并且一个名为 french 的子字段配置了french语种分词器
PUT my-index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;fields&amp;quot;: { &amp;quot;french&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;french&amp;quot; } } } } } } 也可以使用以下查询为整个索引配置默认的french分词器：
PUT my-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;default&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;french&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; } } } }</description></item><item><title>同义词图词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/synonym-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/synonym-graph/</guid><description>同义词图词元过滤器 # 同义词图(synonym_graph)词元过滤器是同义词词元过滤器的更高级版本。它支持多词同义词，并能跨多个词元处理同义词，这使得它非常适合处理短语或者那些词元之间关系很重要的场景。
参数说明 # 同义词图词元过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。 synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。 lenient 可选 布尔值 在加载规则配置时是否忽略异常。默认值为 false。 format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：
- solr
- wordnet
默认值为 solr。 expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。
例如：
若同义词定义为 “quick, fast” 且 expand 设置为 true，则同义词规则配置如下：</description></item><item><title>同义词词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/synonym/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/synonym/</guid><description>同义词词元过滤器 # 同义词(synonym)词元过滤器允许将多个术语映射到单个术语，或在单词之间创建等价组，从而提高搜索的灵活性。
参数说明 # 同义词词元过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。 synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。 lenient 可选 布尔值 加载规则配置时是否忽略异常。默认值为 false。 format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：
- solr
- wordnet
默认值为 solr。 expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。
例如：
如果同义词定义为 &amp;quot;quick, fast&amp;quot; 且 expand 设置为 true，则同义词规则配置如下：</description></item><item><title>唯一词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/unique/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/unique/</guid><description>唯一词元过滤器 # 唯一(unique)词元过滤器可确保在分词过程中仅保留唯一的词元，它会去除在单个字段或文本块中出现的重复词元。
参数说明 # 唯一词元过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 only_on_same_position 可选 布尔值 如果设置为 true，该词元过滤器将充当去重词元过滤器，仅移除位于相同位置的词元。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 unique_example 的新索引，并配置了一个带有唯一过滤器的分词器。
PUT /unique_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;unique_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;unique&amp;quot;, &amp;quot;only_on_same_position&amp;quot;: false } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;unique_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;unique_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>多路复用分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/multiplexer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/multiplexer/</guid><description>多路复用分词过滤器 # 多路复用(multiplexer)分词过滤器允许你通过应用不同的过滤器来创建同一词元的多个版本。当你想要以多种方式分析同一个词元时，这非常有用。例如，你可能希望使用不同的词干提取、同义词或 n-gram 过滤器来分析一个词元，并将所有生成的词元一起使用。这个分词过滤器的工作方式是复制分词流，并对每个副本应用不同的过滤器。
多路复用分词过滤器会从分词流中移除重复的词元。
多路复用分词过滤器不支持多词同义词(synonym)过滤器、同义词图(synonym_graph)分词过滤器或组合词（shingle）分词过滤器，因为这些过滤器不仅需要分析当前词元，还需要分析后续的词元，以便正确确定如何转换输入内容。
参数说明 # 多路复用分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 filters 可选 字符串列表 要应用于分词流每个副本的分词过滤器的逗号分隔列表。默认值为空列表。 preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 true。 参考样例 # 以下示例请求创建了一个名为 multiplexer_index 的新索引，并配置了一个带有多路复用过滤器的分词器：
PUT /multiplexer_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;english_stemmer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;english&amp;quot; }, &amp;quot;synonym_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;synonym&amp;quot;, &amp;quot;synonyms&amp;quot;: [ &amp;quot;quick,fast&amp;quot; ] }, &amp;quot;multiplexer_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;multiplexer&amp;quot;, &amp;quot;filters&amp;quot;: [&amp;quot;english_stemmer&amp;quot;, &amp;quot;synonym_filter&amp;quot;], &amp;quot;preserve_original&amp;quot;: true } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;multiplexer_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;multiplexer_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>大写词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/uppercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/uppercase/</guid><description>大写词元过滤器 # 大写(uppercase)词元过滤器用于在分析过程中将所有词元（单词）转换为大写形式。
参考样例 # 以下示例请求创建了一个名为 uppercase_example 的新索引，并配置了一个带有大写过滤器的分词器。
PUT /uppercase_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;uppercase_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;uppercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;uppercase_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;uppercase_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
GET /uppercase_example/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;uppercase_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch is powerful&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;EASYSEARCH&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;IS&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;POWERFUL&amp;quot;, &amp;quot;start_offset&amp;quot;: 14, &amp;quot;end_offset&amp;quot;: 22, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>字母词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/letter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/letter/</guid><description>字母词元生成器 # 字母(letter)词元生成器会根据任何非字母字符将文本拆分成单词。它在处理许多欧洲语种时效果良好，但对于一些亚洲语种却不太有效，因为在亚洲语种中，单词之间不是由空格来分隔的。
参考样例 # 下面的示例请求创建了一个名为 my_index 的新索引，并配置了一个使用字母词元生成器的分词器。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_letter_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;letter&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_letter_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST _analyze { &amp;quot;tokenizer&amp;quot;: &amp;quot;letter&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Cats 4EVER love chasing butterflies!&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Cats&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 4, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;EVER&amp;quot;, &amp;quot;start_offset&amp;quot;: 6, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;love&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;chasing&amp;quot;, &amp;quot;start_offset&amp;quot;: 16, &amp;quot;end_offset&amp;quot;: 23, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;butterflies&amp;quot;, &amp;quot;start_offset&amp;quot;: 24, &amp;quot;end_offset&amp;quot;: 35, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 4 } ] }</description></item><item><title>字符组词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/character-group/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/character-group/</guid><description>字符组词元生成器 # 字符组（char_group）词元生成器使用特定字符作为分隔符将文本拆分为词元。它适用于需要简单直接进行分词的场景，为基于词元生成器的匹配模式提供了一种更简单的替代方案，避免了额外的复杂性。
参考样例 # 以下示例请求创建了一个名为my_index的新索引，并配置了一个带有 char_group 字符组词元生成器的分词器。该词元生成器会依据空格、连字符 - 和冒号 : 来分割文本。
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_char_group_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;char_group&amp;quot;, &amp;quot;tokenize_on_chars&amp;quot;: [ &amp;quot;whitespace&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;:&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_char_group_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_char_group_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_char_group_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_char_group_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Fast-driving cars: they drive fast!</description></item><item><title>小写分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/lowercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/lowercase/</guid><description>小写分词过滤器 # 小写(lowercase)分词过滤器用于将分词流中的所有字符转换为小写，从而使搜索不区分大小写。
参数 # 小写分词过滤器可以使用以下参数进行配置。
参数 必填/可选 描述 language 可选 指定一个特定语言的分词过滤器。有效值为：
- 希腊语 greek
- 爱尔兰语irish
- 土耳其语turkish。
默认值是 Lucene 的小写过滤器。 参考样例 # 以下示例请求创建了一个名为 custom_lowercase_example 的新索引。它配置了一个带有小写过滤器的分析器，并指定语言为希腊语。
PUT /custom_lowercase_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;greek_lowercase_example&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;greek_lowercase&amp;quot;] } }, &amp;quot;filter&amp;quot;: { &amp;quot;greek_lowercase&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;greek&amp;quot; } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>小写词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/lowercase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/lowercase/</guid><description>小写词元生成器 # 小写（lowercase）词元生成器会在空白处将文本拆分成词条，然后把所有词条转换为小写形式。从功能上来说，这与配置一个字母词元生成器并搭配一个小写词元过滤器的效果是一样的。不过，使用小写词元生成器效率更高，因为分词操作是在一步之内完成的。
参考样例 # 以下示例请求创建了一个名为 my-lowercase-index 的新索引，并配置了一个使用小写词元生成器的分词器：
PUT /my-lowercase-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_lowercase_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_lowercase_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_lowercase_tokenizer&amp;quot; } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my-lowercase-index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_lowercase_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;This is a Test. Easysearch 123!&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;this&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 4, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 5, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 9, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;test&amp;quot;, &amp;quot;start_offset&amp;quot;: 10, &amp;quot;end_offset&amp;quot;: 14, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 16, &amp;quot;end_offset&amp;quot;: 26, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 4 } ] }</description></item><item><title>常用词组分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/common-grams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/common-grams/</guid><description>常用词组分词过滤器 # 常用词组(common_grams)分词过滤器通过保留文本中常见的短语（常用词组）来提高搜索的相关性。当处理某些语言或数据集时，特定的单词组合经常作为一个整体出现，如果将它们当作单独的词元处理，可能会影响搜索的相关性，此时该过滤器就非常有用。如果输入字符串中常用词，这个分词过滤器会同时生成它们的一元词组（单个词）和二元词组（两个词的组合）。
使用这个分词过滤器可以保持常用短语的完整性，从而提高搜索的相关性。这有助于更准确地匹配查询内容，尤其是对于频繁出现的单词组合。它还能通过减少不相关的匹配结果来提高搜索的精度。
使用此过滤器时，你必须谨慎选择并维护常用词(common_words)列表。
参数说明 # 常用词组分词过滤器可通过以下参数进行配置：
参数 必需/可选 数据类型 描述 common_words 必需 字符串列表 一个应被视为常用词词组的单词列表。如果 common_words 参数是一个空列表，common_grams 分词过滤器将成为一个无操作过滤器，即它根本不会修改输入的词元。 ignore_case 可选 布尔值 指示过滤器在匹配常用词时是否应忽略大小写差异。默认值为 false。 query_mode 可选 布尔值 当设置为 true 时，应用以下规则：
- 从 common_words 生成的一元词组（单个词）不包含在输出中。
- 非常用词后跟常用词形成的二元词组会保留在输出中。
- 如果非常用词的一元词组后面紧接着一个常用词，则该一元词组会被排除。
- 如果非常用词出现在文本末尾且前面是一个常用词，其一元词组不包含在输出中。 参考样例 # 以下示例请求创建了一个名为 my_common_grams_index 的新索引，并配置了一个使用常用词组（common_grams）过滤器的分析器。
PUT /my_common_grams_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_common_grams_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;common_grams&amp;quot;, &amp;quot;common_words&amp;quot;: [&amp;quot;a&amp;quot;, &amp;quot;in&amp;quot;, &amp;quot;for&amp;quot;], &amp;quot;ignore_case&amp;quot;: true, &amp;quot;query_mode&amp;quot;: true } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_common_grams_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>平图化分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/flatten-graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/flatten-graph/</guid><description>平图化分词过滤器 # 平图化（flatten_graph）分词过滤器用于处理在图结构中同一位置生成多个词元时出现的复杂词元关系。一些分词过滤器，比如同义词图（synonym_graph）和词分隔符图（word_delimiter_graph），会生成多位置词元——即相互重叠或跨越多个位置的词元。这些词元图对于搜索查询很有用，但在索引过程中却不被直接支持。平图化分词过滤器会将多位置词元解析为一个线性的词元序列。对图进行扁平化处理可确保与索引过程兼容。
词元图的扁平化是一个有损耗的过程。只要有可能，就应避免使用平图化过滤器。相反，应仅在搜索分词器中应用图分词过滤器，这样就无需使用平图化分词过滤器了。
参考样例 # 以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有平图化分词过滤器的分词器：
PUT /test_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_index_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_custom_filter&amp;quot;, &amp;quot;flatten_graph&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;my_custom_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;word_delimiter_graph&amp;quot;, &amp;quot;catenate_all&amp;quot;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /test_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_index_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch helped many employers&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;helped&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 17, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;many&amp;quot;, &amp;quot;start_offset&amp;quot;: 18, &amp;quot;end_offset&amp;quot;: 22, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;employers&amp;quot;, &amp;quot;start_offset&amp;quot;: 23, &amp;quot;end_offset&amp;quot;: 32, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 } ] }</description></item><item><title>归一化词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/normalization/</guid><description>归一化词元过滤器 # 归一化(normalization)词元过滤器旨在以减少文本差异（尤其是特殊字符差异）的方式对文本进行调整和简化。它主要用于通过对特定语言中的字符进行标准化处理，来应对书写上的差异。
以下是可用的归一化词元过滤器：
阿拉伯语归一化: arabic_normalization 德语归一化: german_normalization 印地语归一化: hindi_normalization 印度语系归一化: indic_normalization 索拉尼语归一化: sorani_normalization 波斯语归一化: persian_normalization 斯堪的纳维亚语归一化: scandinavian_normalization 斯堪的纳维亚语折叠处理归一化: scandinavian_folding 塞尔维亚语归一化: serbian_normalization 参考样例 # 以下示例请求创建了一个名为 german_normalizer_example 的新索引，并配置了一个带有 german_normalization的分词器：
PUT /german_normalizer_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;german_normalizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;german_normalization&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;german_normalizer_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;german_normalizer&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /german_normalizer_example/_analyze { &amp;quot;text&amp;quot;: &amp;quot;Straße München&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;german_normalizer_analyzer&amp;quot; } 返回内容包含产生的词元</description></item><item><title>截断词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/truncate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/truncate/</guid><description>截断词元过滤器 # 截断(truncate)词元过滤器用于缩短超过指定长度的词元。它会将词元修剪至最大字符数，确保超过该限制的词元被截断。
参数说明 # 截断词元过滤器可以使用以下参数进行配置：
参数 必需/可选 数据类型 描述 length 可选 整数 指定生成的词元的最大长度。默认值为 10。 参考样例 # 以下示例请求创建了一个名为 truncate_example 的新索引，并配置了一个带有截断过滤器的分词器。
PUT /truncate_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;truncate_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;truncate&amp;quot;, &amp;quot;length&amp;quot;: 5 } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;truncate_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;truncate_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>指纹分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/fingerprint-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/fingerprint-analyzer/</guid><description>指纹分词器 # 指纹（fingerprint）分词器会创建一个文本指纹。该分词器对从输入生成的词项（词元）进行排序和去重，然后使用一个分隔符将它们连接起来。它通常用于数据去重，对于包含相同单词的相似输入，无论单词顺序如何，它都可以产生相同的输出结果。
指纹分词器由以下组件组成：
标准分词生成器 词元小写化过滤器 ASCII 词元过滤器 停用词词元过滤器 指纹词元过滤器 参数说明 # 指纹分词器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 separator 可选 字符串 在对词项进行词元生成、排序和去重后，用于连接这些词项的字符。默认值为一个空格（ ）。 max_output_size 可选 整数 定义输出词元的最大大小。如果连接后的指纹超过此大小，将被截断。默认值为 255。 stopwords 可选 字符串或字符串列表 自定义的停用词列表。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_custom_fingerprint_index 带有指纹分词器的索引：
PUT /my_custom_fingerprint_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_fingerprint_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;fingerprint&amp;quot;, &amp;quot;separator&amp;quot;: &amp;quot;-&amp;quot;, &amp;quot;max_output_size&amp;quot;: 50, &amp;quot;stopwords&amp;quot;: [&amp;quot;to&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;over&amp;quot;, &amp;quot;and&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_fingerprint_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查使用该分词器生成的词元：</description></item><item><title>指纹分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/fingerprint/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/fingerprint/</guid><description>指纹分词过滤器 # 指纹（fingerprint）分词过滤器用于对文本进行标准化和去重处理。当文本处理的唯一性或者一致性至关重要时，这个过滤器特别有用。指纹分词过滤器通过以下步骤处理文本以实现这一目的：
小写转换：将所有文本转换为小写。 分词：把文本拆分成词元。 排序：按字母顺序排列这些词元。 去重：消除重复的词元。 合并词元：将这些词元合并成一个字符串，通常使用空格或其他指定的分隔符进行连接。 参数说明 # 指纹分词过滤器可以使用以下两个参数进行配置。
参数 必需/可选 数据类型 描述 max_output_size 可选 整数 限制生成的指纹字符串的长度。如果拼接后的字符串超过了 max_output_size，过滤器将不会产生任何输出，从而得到一个空词元。默认值是 255 separator 可选 字符串 定义在词元排序和去重后，用于将词元连接成单个字符串。默认是空格（&amp;quot; &amp;quot;）。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有指纹分词过滤器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_fingerprint&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;fingerprint&amp;quot;, &amp;quot;max_output_size&amp;quot;: 200, &amp;quot;separator&amp;quot;: &amp;quot;-&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_fingerprint&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>捕获匹配词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/pattern-capture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/pattern-capture/</guid><description>捕获匹配词元过滤器 # 捕获匹配(pattern_capture)词元过滤器是一种功能强大的过滤器，它使用正则表达式根据特定模式来捕获和提取文本的部分内容。当你想要提取词元的特定部分，例如电子邮件域名、话题标签或数字，并将其重新用于进一步的分析或索引编制时，这个过滤器会非常有用。
参数说明 # 捕获匹配词元过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 patterns 必需 字符串数组 用于捕获文本部分的正则表达式数组。 preserve_original 必需 布尔值 是否在输出中保留原始词元。默认值为 true。 参考样例 # 以下示例请求创建了一个名为 email_index 的新索引，并配置了一个带有捕获匹配过滤器的分词器，以便从电子邮件地址中提取本地部分和域名。
PUT /email_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;email_pattern_capture&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern_capture&amp;quot;, &amp;quot;preserve_original&amp;quot;: true, &amp;quot;patterns&amp;quot;: [ &amp;quot;^([^@]+)&amp;quot;, &amp;quot;@(.+)$&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;email_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;uax_url_email&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;email_pattern_capture&amp;quot;, &amp;quot;lowercase&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>搜索分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/search-analyzers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/search-analyzers/</guid><description>搜索分词器 # 搜索分词器是在查询时指定的，当你对text字段进行全文匹配查询时，它会被用于分析被查询字符串。
搜索分词器的生效流程 # 在查询时确定对查询字符串使用哪种分词器，Easysearch 会按以下顺序检查以下参数：
查询的 analyzer 参数 字段的 search_analyzer 映射参数 索引设置 analysis.analyzer.default_search 字段的 analyzer 映射参数 standard 分词器（默认分词器） 在大多数情况下，没有必要指定与索引分词器不同的搜索分词器，而且这样做可能会对搜索结果的相关性产生负面影响，或者导致出现意想不到的搜索结果。
为查询内容指定搜索分词器 # 在查询时，你可以在 analyzer 字段中指定想要使用的分词器：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;text_entry&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;speak the truth&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;english&amp;quot; } } } } 为字段指定搜索分词器 # 在创建索引映射时，你可以为每个文本字段提供 search_analyzer 参数。在提供 search_analyzer 时，你还必须提供 analyzer 参数，该参数指定在索引创建时的写入索引分词器内应用。
例如，以下请求将 simple 分词器指定为 text_entry 字段的索引分词器，将 whitespace 分词器指定为该字段的搜索分词器：
PUT testindex { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text_entry&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;simple&amp;quot;, &amp;quot;search_analyzer&amp;quot;: &amp;quot;whitespace&amp;quot; } } } } 为索引指定默认的搜索分词器 # 如果你希望在搜索时使用同一个分词器来解析所有的查询，你可以在 analysis.</description></item><item><title>数据脱敏</title><link>/easysearch/v1.14.0/docs/references/security/access-control/field-masking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/access-control/field-masking/</guid><description>数据脱敏 # 如果您的数据里面包含一些敏感信息，除了通过 字段级权限 来进行访问控制，您还可以通过混淆字段里面的内容来进行脱敏。目前，字段数据脱敏仅适用于基于字符串的字段，支持加密哈希和正则替换字段的内容。
字段脱敏与字段级权限一起可以在相同的角色级别和索引级别上工作。您可以允许某些角色查看明文格式的敏感字段，并为其他角色脱敏这些字段。带有脱敏字段的搜索结果可能如下所示：
{ &amp;#34;_index&amp;#34;: &amp;#34;movies&amp;#34;, &amp;#34;_type&amp;#34;: &amp;#34;_doc&amp;#34;, &amp;#34;_source&amp;#34;: { &amp;#34;year&amp;#34;: 2013, &amp;#34;directors&amp;#34;: [&amp;#34;Ron Howard&amp;#34;], &amp;#34;title&amp;#34;: &amp;#34;ca998e768dd2e6cdd84c77015feb29975f9f498a472743f159bec6f1f1db109e&amp;#34; } } 设置盐值 # 可以在 easysearch.yml 设置一个随机字符串:
security.compliance.salt: abcdefghijklmnopqrstuvqxyz1234567890 属性 说明 security.compliance.salt 生成哈希值时要使用的盐值。必须至少为 32 个字符。仅允许使用 ASCII 字符。选填。 配置脱敏字段 # role.yml # masked_movie: cluster: [] indices: - names: - movies privileges: - &amp;#34;read&amp;#34; field_mask: - &amp;#34;genres&amp;#34; - &amp;#34;title&amp;#34; REST API # 参照 创建角色.</description></item><item><title>映射字符过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/character-filters/mapping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/character-filters/mapping/</guid><description>映射字符过滤器 # 映射（mapping）字符过滤器接受一个用于字符替换的键值对映射。每当该过滤器遇到与某个键匹配的字符串时，它就会用相应的值来替换这些字符。替换值可以是空字符串。
该过滤器采用贪婪匹配方式，这意味着会匹配最长的匹配结果。
在分词过程之前，需要进行特定文本替换的场景下，映射字符过滤器会很有帮助。
参考样例 # 以下请求配置了一个映射字符过滤器，该过滤器可将罗马数字（如 I、II 或 III）转换为对应的阿拉伯数字（1、2 和 3）：
GET /_analyze { &amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;, &amp;quot;char_filter&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;mapping&amp;quot;, &amp;quot;mappings&amp;quot;: [ &amp;quot;I =&amp;gt; 1&amp;quot;, &amp;quot;II =&amp;gt; 2&amp;quot;, &amp;quot;III =&amp;gt; 3&amp;quot;, &amp;quot;IV =&amp;gt; 4&amp;quot;, &amp;quot;V =&amp;gt; 5&amp;quot; ] } ], &amp;quot;text&amp;quot;: &amp;quot;I have III apples and IV oranges&amp;quot; } 返回内容中包含一个词元，其中罗马数字已被替换为阿拉伯数字：
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;1 have 3 apples and 4 oranges&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 32, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] } 参数说明 # 你可以使用以下任意一个参数来配置键值映射。</description></item><item><title>最小哈希分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/min-hash/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/min-hash/</guid><description>最小哈希分词过滤器 # 最小哈希（min_hash）分词过滤器用于基于最小哈希近似算法为词元生成哈希值，这对于检测文档间的相似度很有用。最小哈希分词过滤器会为一组词元（通常来自一个经过分词的字段）生成哈希值。
参数说明 # 最小哈希分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 hash_count 可选 整数 为每个词元生成的哈希值数量。增加此值通常会提高相似度估计的准确性，但会增加计算成本。默认值为 1。 bucket_count 可选 整数 要使用的哈希桶数量。这会影响哈希的粒度。更多的桶能提供更细的粒度并减少哈希冲突，但需要更多内存。默认值为 512。 hash_set_size 可选 整数 每个桶中要保留的哈希数量。这会影响哈希质量。更大的集合大小可能会带来更好的相似度检测效果，但会消耗更多内存。默认值为 1。 with_rotation 可选 布尔值 当设置为 true 时，如果 hash_set_size 为 1，过滤器会用从其右侧按环形顺序找到的第一个非空桶的值填充空桶。如果 bucket_count 参数大于 1，此设置会自动默认为 true；否则，默认为 false。 参考样例 # 以下示例请求创建了一个名为 minhash_index 的新索引，并配置了一个带有最小哈希过滤器的分词器：
PUT /minhash_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;minhash_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;min_hash&amp;quot;, &amp;quot;hash_count&amp;quot;: 3, &amp;quot;bucket_count&amp;quot;: 512, &amp;quot;hash_set_size&amp;quot;: 1, &amp;quot;with_rotation&amp;quot;: false } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;minhash_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;minhash_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>条件分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/condition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/condition/</guid><description>条件分词过滤器 # 条件(condition)分词过滤器是一种特殊类型的过滤器，它允许你根据特定标准有条件地应用其他分词过滤器。这使得在文本分析过程中，你能更精准地控制何时应用特定的分词过滤器。你可以配置多个过滤器，并且只有当满足你定义的条件时，这些过滤器才会被应用。该分词过滤器在进行特定语言处理和特殊字符处理时非常有用。
参数说明 # 要使用条件分词过滤器，必须配置两个参数，具体如下：
参数 必需/可选 数据类型 描述 filter 必需 数组 当满足由 script 参数定义的指定条件时，指定应对词元应用哪个分词过滤器。 script 必需 对象 配置一个内联脚本，该脚本定义了要应用 filter 参数中指定的过滤器所需满足的条件（仅接受内联脚本）。 参考样例 # 以下示例请求创建了一个名为 my_conditional_index 的新索引，并配置了一个带有条件过滤器的分词器。该过滤器会对任何包含字符序列 “um” 的词元应用小写（lowercase）字母过滤器。
PUT /my_conditional_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_conditional_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;condition&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;], &amp;quot;script&amp;quot;: { &amp;quot;source&amp;quot;: &amp;quot;token.getTerm().toString().contains('um')&amp;quot; } } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;my_conditional_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>标准分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/standard-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/standard-analyzer/</guid><description>标准分词器 # 标准（standard）分词器是在未指定其他分词器时默认使用的分词器。它旨在为通用文本处理提供一种基础且高效的方法。
该分词器由以下词元生成器和词元过滤器组成：
标准（standard）词元生成器：去除大部分标点符号，并依据空格和其他常见分隔符对文本进行分割。 小写（lowercase）词元过滤器：将所有词元转换为小写，以确保匹配时不区分大小写。 停用词（stop）词元过滤器：从分词后的输出中移除常见的停用词，例如 “the”、“is” 和 “and”。 参考样例 # 以下命令创建一个名为 my_standard_index 并使用标准分词器的索引：
PUT /my_standard_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot; } } } } 参数说明 # 你可以使用以下参数来配置标准分词器。
参数 必填/可选 数据类型 描述 max_token_length 可选 整数 设置生成的词元的最大长度。如果超过此长度，词元将按照 max_token_length 中配置的长度拆分为多个词元。默认值为 255。 stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 配置自定义分词器 # 以下命令配置一个索引，该索引带有一个等同于标准分词器的自定义分词器：</description></item><item><title>标准词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/standard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/standard/</guid><description>标准词元生成器 # 标准（standard）词元生成器是 Easysearch 中的默认分词器。它基于单词边界，采用一种基于语法的方法对文本进行分词，这种方法能够识别字母、数字以及标点等其他字符。它具有高度的通用性，适用于多种语言，它使用了 Unicode 文本分割规则（ UAX#29）来将文本分割成词元。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用标准词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_standard_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;standard&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_standard_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_standard_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch is powerful, fast, and scalable.&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;powerful&amp;quot;, &amp;quot;start_offset&amp;quot;: 14, &amp;quot;end_offset&amp;quot;: 22, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;fast&amp;quot;, &amp;quot;start_offset&amp;quot;: 24, &amp;quot;end_offset&amp;quot;: 28, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;and&amp;quot;, &amp;quot;start_offset&amp;quot;: 30, &amp;quot;end_offset&amp;quot;: 33, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 4 }, { &amp;quot;token&amp;quot;: &amp;quot;scalable&amp;quot;, &amp;quot;start_offset&amp;quot;: 34, &amp;quot;end_offset&amp;quot;: 42, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 5 } ] } 参数说明 # 标准词元生成器可以使用以下参数进行配置。</description></item><item><title>波特词干词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/porter-stem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/porter-stem/</guid><description>波特词干词元过滤器 # 波特词干(porter_stem)词元过滤器会将单词还原为其基本（或词干）形式，并去除单词中常见的后缀，这有助于通过单词的词根来匹配相似的单词。例如，单词“running”会被词干提取为“run”。此词元过滤器主要用于英语，并基于 波特词干提取算法进行词干提取操作。
参考样例 # 以下示例请求创建了一个名为 my_stem_index 的新索引，并配置了一个带有波特词干提取过滤器的分词器。
PUT /my_stem_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_porter_stem&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;porter_stem&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;porter_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_porter_stem&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_stem_index/_analyze { &amp;quot;text&amp;quot;: &amp;quot;running runners ran&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;porter_analyzer&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;run&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 7, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;runner&amp;quot;, &amp;quot;start_offset&amp;quot;: 8, &amp;quot;end_offset&amp;quot;: 15, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;ran&amp;quot;, &amp;quot;start_offset&amp;quot;: 16, &amp;quot;end_offset&amp;quot;: 19, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 } ] }</description></item><item><title>省略符号分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/apostrophe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/apostrophe/</guid><description>省略符号分词过滤器 # 省略符号（apostrophe）分词过滤器的主要功能是移除所有格形式的撇号以及跟在撇号后面的所有内容。这在分析那些大量使用撇号的语言的文本时非常有用，比如土耳其语。在土耳其语中，撇号用于将词根与后缀分隔开来，这些后缀包括所有格后缀、格标记以及其他语法词尾。
参考样例 # 以下示例请求创建了一个名为 custom_text_index 的新索引，在索引设置中配置了一个自定义分词器，并在映射中使用该分词器：
PUT /custom_text_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;apostrophe&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;custom_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /custom_text_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;custom_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;John's car is faster than Peter's bike&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;john&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;car&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;faster&amp;quot;, &amp;quot;start_offset&amp;quot;: 14, &amp;quot;end_offset&amp;quot;: 20, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;than&amp;quot;, &amp;quot;start_offset&amp;quot;: 21, &amp;quot;end_offset&amp;quot;: 25, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 4 }, { &amp;quot;token&amp;quot;: &amp;quot;peter&amp;quot;, &amp;quot;start_offset&amp;quot;: 26, &amp;quot;end_offset&amp;quot;: 33, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 5 }, { &amp;quot;token&amp;quot;: &amp;quot;bike&amp;quot;, &amp;quot;start_offset&amp;quot;: 34, &amp;quot;end_offset&amp;quot;: 38, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 6 } ] } 内置的省略符号分词过滤器并不适用于像法语这样的语言，在法语中撇号会出现在单词的开头。例如，C'est l'amour de l'école 这句话使用该过滤器分词后将会得到四个词元：“C”、“l”、“de” 和 “l”。</description></item><item><title>省略词分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/elision/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/elision/</guid><description>省略词分词过滤器 # 省略词（Elision）分词过滤器用于从某些语言的单词中去除省略的字符。省略现象通常出现在像法语这样的语言中，在这些语言里，单词常常会发生缩合，并与后面的单词结合，常见的方式是省略一个元音字母，并用一个撇号来替代。
省略词分词过滤器已经在以下语言分词器中预先配置好了：加泰罗尼亚语/catalan、法语/french、爱尔兰语/irish和意大利语/italian。
参数说明 # 自定义省略词分词过滤器可使用以下参数进行配置。
参数 必需/可选 数据类型 描述 articles 若未配置 articles_path 则为必需 字符串数组 定义当某些冠词或短词作为省略形式的一部分出现时，哪些应该被移除。 articles_path 若未配置 articles 则为必需 字符串 指定在分析过程中应被移除的自定义冠词列表的路径。 articles_case 可选 布尔值 指定在匹配省略形式时，该过滤器是否区分大小写。默认值为 false。 参考样例 # 法语中默认的省略形式集合包括 l'、m'、t'、qu'、n'、s'、j'、d'、c'、jusqu'、quoiqu'、lorsqu' 和 puisqu'。你可以通过配置 french_elision 分词过滤器来更新这个集合。以下示例请求创建了一个名为 french_texts 的新索引，并配置了一个带有 french_elision 过滤器的分词器：
PUT /french_texts { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;french_elision&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;elision&amp;quot;, &amp;quot;articles&amp;quot;: [ &amp;quot;l&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;m&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;s&amp;quot;, &amp;quot;j&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;french_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;french_elision&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;french_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>空格分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/whitespace-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/whitespace-analyzer/</guid><description>空格分词器 # 空格（whitespace）分词器仅基于空白字符（例如，空格和制表符）将文本拆分为词元。比如转换为小写形式或移除停用词这样的转换操作，它都不会应用，因此文本的原始大小写形式会被保留，并且标点符号也会作为词元的一部分包含在内。
参考样例 # 以下命令创建一个名为 my_whitespace_index 并使用空格分词器的索引：
PUT /my_whitespace_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;whitespace&amp;quot; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于空格分词器：
PUT /my_custom_whitespace_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_whitespace_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_whitespace_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>空格词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/whitespace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/whitespace/</guid><description>空格词元生成器 # 空格（whitespace）词元生成器会依据空白字符（如空格、制表符和换行符）对文本进行拆分。它将由空白字符分隔开的每个单词都视为一个词元，并且不会执行诸如转换为小写形式或去除标点符号等额外的规范化操作。
参考样例 # 以下示例请求创建一个名为 my_index 的新索引，并配置一个使用空格词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;whitespace_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;whitespace&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_whitespace_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_whitespace_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_whitespace_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Easysearch is fast! Really fast.&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Easysearch&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 10, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;is&amp;quot;, &amp;quot;start_offset&amp;quot;: 11, &amp;quot;end_offset&amp;quot;: 13, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;fast!</description></item><item><title>简单分割匹配词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/simple-pattern-split/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/simple-pattern-split/</guid><description>简单分割匹配词元生成器 # 简单分割匹配（simple_pattern_split）词元生成器使用正则表达式将文本分割成词元。该正则表达式定义了用于确定文本分割位置的模式。文本中任何匹配该模式的部分都会被用作分隔符，分隔符之间的文本则成为一个词元。当你想定义分隔符，并根据某个模式对文本的其余部分进行分词时，可以使用此词元生成器。
该词元生成器仅将输入文本中匹配到的部分（基于正则表达式）用作分隔符或边界，以将文本分割成词条。匹配到的部分不会包含在生成的词元中。例如，如果将词元生成器配置为在点号（.）处分割文本，输入文本为 one.two.three，那么生成的词元就是 one、two 和 three。点号本身不会包含在生成的词条中。
参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用简单分割匹配词元生成器的分词器。该分词器被配置为在连字符（-）处分割文本：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_pattern_split_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;simple_pattern_split&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;-&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_split_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_pattern_split_tokenizer&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_split_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_split_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;2024-10-09&amp;quot; } 返回内容包含产生的词元</description></item><item><title>简单分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/simple-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/simple-analyzer/</guid><description>简单分词器 # 简单（simple）分词器是一种非常基础的分词器，它会将文本中的非字母字符串拆分成词项，并将这些词项转换为小写形式。与标准分词器不同的是，简单分词器将除字母字符之外的所有内容都视为分隔符，这意味着它不会把数字、标点符号或特殊字符识别为词元的一部分。
参考样例 # 以下命令创建一个名为 my_simple_index 并使用简单分词器的索引：
PUT /my_simple_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;simple&amp;quot; } } } } 配置自定义分词器 # 以下命令配置了一个索引，该索引带有一个自定义分词器，这个自定义分词器等同于添加了 html_strip 字符过滤器的简单分词器：
PUT /my_custom_simple_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;char_filter&amp;quot;: { &amp;quot;html_strip&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;html_strip&amp;quot; } }, &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_lowercase_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_simple_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;char_filter&amp;quot;: [&amp;quot;html_strip&amp;quot;], &amp;quot;tokenizer&amp;quot;: &amp;quot;my_lowercase_tokenizer&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_simple_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>系统日志</title><link>/easysearch/v1.14.0/docs/references/management/logs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/logs/</guid><description>系统日志 # Easysearch 日志包含监控群集操作和故障排除问题的重要信息。日志的位置因安装类型而异：
在 Docker 上，Easysearch 将大多数日志写入控制台，并将其余日志存储在 Easysearch/logs/ 中。 tarball 安装也使用 easysearch/logs/ 。 在 RPM 和 Debian 安装上， Easysearch 将日志写入 /var/log/Easysearch/ 。 日志可作为 .log （纯文本）和 .json 文件使用。
应用程序日志 # 对于其应用程序日志，Easysearch 使用 Apache Log4j 2 其内置日志级别（从最低到最高）为 TRACE 、 DEBUG 、 INFO 、 WARN 、 ERROR 和 FATAL 。默认 Easysearch 日志级别为 INFO 。
您可以更改各个 Easysearch 模块的日志级别，而不是更改默认日志级别（ logger.level ）：
PUT /_cluster/settings { &amp;#34;persistent&amp;#34; : { &amp;#34;logger.org.easysearch.index.reindex&amp;#34; : &amp;#34;DEBUG&amp;#34; } } 此示例更改后，Easysearch 在重新索引操作期间会发出更详细的日志：</description></item><item><title>索引分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/index-analyzers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/analyzers/index-analyzers/</guid><description>索引分词器 # 索引分词器是在索引创建时指定的，在文档写入时对文本字段进行分词。
写入索引分词器的生效流程 # 为了确定在对文档建立索引时对某个字段使用哪种分词器，Easysearch 会按以下顺序检查以下参数：
字段的分词器映射参数，即analyzer analysis.analyzer.default 索引设置 standard标准分词器（默认分词器） 在指定索引分词器时，请记住，在大多数情况下，为索引中的每个文本字段指定一个分词器效果最佳。使用相同的分词器在文本字段（在建立索引时）和查询字符串（在查询时），可确保搜索所使用的词项与存储在索引中的词项相同。
为字段指定索引分词器 # 在创建索引映射时，可以为每个文本字段提供 analyzer（分词器）参数。例如，以下请求为 text_entry 字段指定了 simple 简单分词器：
PUT testindex { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text_entry&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;simple&amp;quot; } } } } 为索引指定默认索引分词器 # 如果想对索引中的所有文本字段使用相同的分词器，则可以在索引设置中按如下方式进行指定analysis.analyzer.default：
PUT testindex { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;default&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;simple&amp;quot; } } } } } 如果您未指定默认分词器，那么将使用standard标准分词器。</description></item><item><title>索引压缩</title><link>/easysearch/v1.14.0/docs/references/document/index-compression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/document/index-compression/</guid><description>索引压缩 # 索引编码 # 索引编码决定索引的存储字段如何被压缩和存储在磁盘上。索引编码由静态的 index.codec 设置来控制，该设置指定压缩算法。这个设置会影响索引分片的大小和索引操作的性能。
Easysearch 提供了多种基于索引编码的压缩方案，以降低索引的存储成本。
default – 该编码使用LZ4算法和预设字典，优先考虑性能而非压缩比。与best_compression相比，它提供更快的索引和搜索操作，但可能导致更大的索引/分片大小。如果在索引设置中未提供编码，则默认使用LZ4作为压缩算法。
best_compression – 该编码底层使用zlib算法进行压缩。它能实现高压缩比，从而减小索引大小。然而，这可能会增加索引操作期间的额外CPU使用，并可能随后导致较高的索引和搜索延迟。
从 Easysearch 1.1 开始，增加了基于 Zstandard 压缩算法的新编码方式。这种算法在压缩比和速度之间提供了良好的平衡。
ZSTD 与默认编解码器相比，该编解码器提供了与best_compression编解码器相当的压缩比，CPU使用合理，索引和搜索性能也有所提高。
source 复用 # source_reuse： 启用 source_reuse 配置项能够去除 _source 字段中与 doc_values 或倒排索引重复的部分，从而有效减小索引总体大小，这个功能对日志类索引效果尤其明显。
source_reuse 支持对以下数据类型进行压缩：keyword，integer，long，short，boolean，float，half_float，double，geo_point，ip， 如果是 text 类型，需要默认启用 keyword 类型的 multi-field 映射。 以上类型必须启用 doc_values 映射（默认启用）才能压缩。
使用限制 # 当索引里包含 nested 类型映射，或插件额外提供的数据类型时，不能启用 source_reuse，例如 knn 索引。
压缩效果对比 # Easysearch 压缩效果对比如下
使用 Nginx 日志作为数据样本 就 Elasticsearch 6.4.3 和 Easysearch 1.</description></item><item><title>索引模板</title><link>/easysearch/v1.14.0/docs/references/management/index-templates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/index-templates/</guid><description>索引模板 # 索引模板允许您使用预定义的映射和设置初始化新索引。例如，如果连续索引日志数据，可以定义一个索引模板，以便所有这些索引都具有相同数量的碎片和副本。
创建模板 # 要创建索引模板，请使用 POST 请求：
POST _index_template 此命令创建一个名为 daily_logs 的模板，并将其应用于名称与正则表达式 logs-2023-01-* 匹配的任何新索引，还将其添加到 my_log 别名中：
PUT _index_template/daily_logs { &amp;#34;index_patterns&amp;#34;: [ &amp;#34;logs-2023-01-*&amp;#34; ], &amp;#34;template&amp;#34;: { &amp;#34;aliases&amp;#34;: { &amp;#34;my_logs&amp;#34;: {} }, &amp;#34;settings&amp;#34;: { &amp;#34;number_of_shards&amp;#34;: 2, &amp;#34;number_of_replicas&amp;#34;: 1 }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;timestamp&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&amp;#34; }, &amp;#34;value&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;double&amp;#34; } } } } } 您应该看到以下响应：
{ &amp;#34;acknowledged&amp;#34;: true } 如果创建名为 logs-2023-01-01 的索引，可以看到它具有模板中的映射和设置：
PUT logs-2023-01-01 GET logs-2023-01-01 { &amp;#34;logs-2023-01-01&amp;#34;: { &amp;#34;aliases&amp;#34;: { &amp;#34;my_logs&amp;#34;: {} }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;timestamp&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&amp;#34; }, &amp;#34;value&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;double&amp;#34; } } }, &amp;#34;settings&amp;#34;: { &amp;#34;index&amp;#34;: { &amp;#34;creation_date&amp;#34;: &amp;#34;1673588860779&amp;#34;, &amp;#34;number_of_shards&amp;#34;: &amp;#34;2&amp;#34;, &amp;#34;number_of_replicas&amp;#34;: &amp;#34;1&amp;#34;, &amp;#34;uuid&amp;#34;: &amp;#34;S1vMSMDHSAuS2IzPcOHpOA&amp;#34;, &amp;#34;version&amp;#34;: { &amp;#34;created&amp;#34;: &amp;#34;7110199&amp;#34; }, &amp;#34;provided_name&amp;#34;: &amp;#34;logs-2023-01-01&amp;#34; } } } } 与此模式匹配的任何其他索引&amp;mdash; logs-2023-01-02 、 logs-2033-01-03等&amp;mdash; 都将继承相同的映射和设置。</description></item><item><title>经典分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/classic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/classic/</guid><description>经典分词过滤器 # 经典（classic）分词过滤器的主要功能是与经典词元生成器配合使用。它通过应用以下常见的转换操作来处理词元，这些操作有助于文本分析和搜索：
移除所有格词尾，例如 “’s” 。比如，“John’s” 会变为 “John”。 从首字母缩略词中移除句点。例如，“D.A.R.P.A.” 会变为 “DARPA”。 参考样例 # 以下示例请求创建了一个名为 custom_classic_filter 的新索引，并配置了一个使用经典分词过滤器的分词器。
PUT /custom_classic_filter { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_classic&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;classic&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;classic&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /custom_classic_filter/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;custom_classic&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;John's co-operate was excellent.&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;John&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;APOSTROPHE&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;co&amp;quot;, &amp;quot;start_offset&amp;quot;: 7, &amp;quot;end_offset&amp;quot;: 9, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 1 }, { &amp;quot;token&amp;quot;: &amp;quot;operate&amp;quot;, &amp;quot;start_offset&amp;quot;: 10, &amp;quot;end_offset&amp;quot;: 17, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 2 }, { &amp;quot;token&amp;quot;: &amp;quot;was&amp;quot;, &amp;quot;start_offset&amp;quot;: 18, &amp;quot;end_offset&amp;quot;: 21, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 3 }, { &amp;quot;token&amp;quot;: &amp;quot;excellent&amp;quot;, &amp;quot;start_offset&amp;quot;: 22, &amp;quot;end_offset&amp;quot;: 31, &amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;, &amp;quot;position&amp;quot;: 4 } ] }</description></item><item><title>经典词元生成器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/classic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/classic/</guid><description>经典词元生成器 # 经典（classic）词元生成器会解析文本，并应用英语语法规则将文本拆分为词元。它包含以下特定的逻辑来处理匹配规则：
首字母缩写词 电子邮件地址 域名 某些类型的标点符号 这种词元生成器最适合处理英语文本。对于其他语言，尤其是那些具有不同语法结构的语言，它可能无法产生最佳效果。
经典词元生成器按如下方式解析文本：
标点符号：在大多数标点符号处拆分文本，并移除标点字符。后面不跟空格的点号会被视为词元的一部分。 连字符：在连字符处拆分单词，但当词元中存在数字时除外。当词元中存在数字时，该词元不会被拆分，而是被当作产品编号处理。 电子邮件：识别电子邮件地址和主机名，并将它们作为单个词元保留。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用经典词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_classic_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;classic&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_classic_analyzer&amp;quot; } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_classic_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;For product AB3423, visit X&amp;amp;Y at example.</description></item><item><title>规范化</title><link>/easysearch/v1.14.0/docs/references/text-analysis/normalizers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/normalizers/</guid><description>规范化 # 规范化的功能与分词器类似，但它仅输出单个词元。它不包含分词器，并且只能包含特定类型的字符过滤器和词元过滤器。这些过滤器只能执行字符级别的操作，例如字符或模式替换，而不能对整个词元进行操作。这意味着不支持用同义词替换词元或进行词干提取。
规范化在关键字搜索（即基于词项的查询）中很有用，因为它允许你对任何给定的输入运行词元过滤器和字符过滤器。例如，它使得能够将传入的查询 “Naïve” 与索引词项 “naive” 进行匹配。
考虑以下示例：
创建一个带有自定义规范化的新索引：
PUT /sample-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;normalizer&amp;quot;: { &amp;quot;normalized_keyword&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;char_filter&amp;quot;: [], &amp;quot;filter&amp;quot;: [ &amp;quot;asciifolding&amp;quot;, &amp;quot;lowercase&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;approach&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;, &amp;quot;normalizer&amp;quot;: &amp;quot;normalized_keyword&amp;quot; } } } } 索引一个文档
POST /sample-index/_doc/ { &amp;quot;approach&amp;quot;: &amp;quot;naive&amp;quot; } 以下查询与该文档匹配。这是预期的结果：
GET /sample-index/_search { &amp;quot;query&amp;quot;: { &amp;quot;term&amp;quot;: { &amp;quot;approach&amp;quot;: &amp;quot;naive&amp;quot; } } } 但这个查询同样也与该文档匹配：</description></item><item><title>词保留分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keep-words/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/keep-words/</guid><description>词保留分词过滤器 # 词保留（keep_words）分词过滤器旨在在分析过程中仅保留特定的词。如果你有大量文本，但只对某些关键字或术语感兴趣，那么这个过滤器就会很有用。
参数说明 # 词保留分词过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 keep_words 若未配置 keep_words_path 则为必需 字符串列表 要保留的词的列表。 keep_words_path 若未配置 keep_words 则为必需 字符串 包含要保留的词列表的文件的路径。 keep_words_case 可选 布尔值 在比较时是否将所有词转换为小写。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有词保留过滤器的分词器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;custom_keep_word&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;keep_words_filter&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;keep_words_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keep&amp;quot;, &amp;quot;keep_words&amp;quot;: [&amp;quot;example&amp;quot;, &amp;quot;world&amp;quot;, &amp;quot;easysearch&amp;quot;], &amp;quot;keep_words_case&amp;quot;: true } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词典复合词分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/dictionary-decompounder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/dictionary-decompounder/</guid><description>词典复合词分词过滤器 # 词典复合词（dictionary_decompounder）分词过滤器用于根据预定义的词典将复合词拆分为其组成部分。该过滤器对于德语、荷兰语或芬兰语等复合词较为常见的语言特别有用，因为将复合词拆分可以提高搜索的相关性。词典复合词分词过滤器会根据已知词列表判断每个词元（单词）是否可以拆分为更小的词元。如果该词元可以拆分为已知单词，过滤器就会为该词元生成子词元。
参数说明 # 词典复合词分词过滤器具有以下参数：
参数 必需/可选 数据类型 描述 word_list 除非配置了 word_list_path，否则为必需 字符串数组 过滤器用于拆分复合词的单词词典。 word_list_path 除非配置了 word_list，否则为必需 字符串 包含词典单词的文本文件的文件路径。可以接受绝对路径或相对于配置(config)目录的相对路径。词典文件必须采用 UTF-8 编码，且每个单词必须单独占一行。 min_word_size 可选 整数 会被考虑进行拆分的整个复合词的最小长度。如果复合词短于该值，则不会进行拆分。默认值为 5。 min_subword_size 可选 整数 任何子词的最小长度。如果子词短于该值，则不会包含在输出中。默认值为 2。 max_subword_size 可选 整数 任何子词的最大长度。如果子词长于该值，则不会包含在输出中。默认值为 15。 only_longest_match 可选 布尔值 如果设置为 true，则仅返回最长匹配的子词。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 decompound_example 的新索引，并配置了一个带有词典复合词过滤器的分词器：</description></item><item><title>词干提取</title><link>/easysearch/v1.14.0/docs/references/text-analysis/stemming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/stemming/</guid><description>词干提取 # 词干提取是将单词还原为其词根或基本形式（即词干）的过程。这项技术可确保在搜索操作中，单词的不同变体都能匹配到相应结果。例如，单词 “running”（跑步，现在分词形式）、“runner”（跑步者，名词形式）和 “ran”（跑步，过去式）都可以还原为词干 “run”（跑步，原形），这样一来，搜索这些词中的任何一个都能返回相关结果。
在自然语言中，由于动词变位、名词复数变化或词的派生等原因，单词常常以各种形式出现。词干提取在以下方面提升了搜索操作的效果：
提高搜索召回率：通过将不同的单词形式匹配到同一个词干，词干提取增加了检索到的相关文档的数量。 减小索引大小：仅存储单词的词干形式可以减少搜索索引的总体大小。 词干提取是通过在分词器中使用词元过滤器来配置的。一个分词器包含以下组件：
字符过滤器：在分词之前修改字符流。 词元生成器：将文本拆分为词元（通常是单词）。 词元过滤器：在分词之后修改词元，例如，应用词干提取操作。 使用内置词元过滤器进行词干提取的示例 # 要实现词干提取，你可以配置一个内置的词元过滤器，比如 porter_stem 或 kstem 过滤器。
波特词干提取算法（ Porter stemming algorithm）是一种常用于英语的词干提取算法。
创建带有自定义分词器的索引 # 以下示例请求创建了一个名为 my_stemming_index 的新索引，并配置了一个使用 porter_stem 词元过滤器的分词器：
PUT /my_stemming_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_stemmer_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;porter_stem&amp;quot; ] } } } } } 此配置包含以下内容：
标准分词器：根据单词边界将文本拆分为词项。 小写字母过滤器：将所有词元转换为小写形式。 波特词干过滤器（porter_stem 过滤器）：将单词还原为它们的词根形式。 测试分词器 # 为了检验词干提取的效果，使用之前配置好的自定义分词器来分析一段示例文本：</description></item><item><title>词干提取词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stemmer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stemmer/</guid><description>词干提取词元过滤器 # 词干提取(stemmer)词元过滤器会将单词缩减为其词根或基本形式（也称为词干stem）。
参数说明 # 词干提取词元过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：
阿拉伯语：arabic 亚美尼亚语：armenian 巴斯克语：basque 孟加拉语：bengali 巴西葡萄牙语：brazilian 保加利亚语：bulgarian 加泰罗尼亚语：catalan 捷克语：czech 丹麦语：danish 荷兰语：dutch、dutch_kp 英语：english（默认）、light_english、lovins、minimal_english、porter2、possessive_english 爱沙尼亚语：estonian 芬兰语：finnish、light_finnish 法语：light_french、french、minimal_french 加利西亚语：galician、minimal_galician（仅复数处理步骤） 德语：light_german、german、german2、minimal_german 希腊语：greek 印地语：hindi 匈牙利语：hungarian、light_hungarian 印尼语：indonesian 爱尔兰语：irish 意大利语：light_italian、italian 库尔德语（索拉尼方言）：sorani 拉脱维亚语：latvian 立陶宛语：lithuanian 挪威语（书面挪威语）：norwegian、light_norwegian、minimal_norwegian 挪威语（新挪威语）：light_nynorsk、minimal_nynorsk 葡萄牙语：light_portuguese、minimal_portuguese、portuguese、portuguese_rslp 罗马尼亚语：romanian 俄语：russian、light_russian 西班牙语：light_spanish、spanish 瑞典语：swedish、light_swedish 土耳其语：turkish 你也可以使用 name 参数作为 language 参数的别名。如果两个参数都被设置，name 参数将被忽略。
参考样例 # 以下示例请求创建了一个名为 my-stemmer-index 的新索引，并配置了一个带有词干提取过滤器的分词器。
PUT /my-stemmer-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_english_stemmer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;english&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_stemmer_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_english_stemmer&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词干覆盖词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stemmer-override/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/stemmer-override/</guid><description>词干覆盖词元过滤器 # 词干覆盖（stemmer_override）词元过滤器允许你定义自定义的词干提取规则，这些规则可以覆盖像波特（Porter）或雪球（Snowball）等默认词干提取器的行为。当你希望对某些特定的单词应用特殊的词干提取方式，而这些单词可能无法被标准的词干提取算法正确处理时，这个过滤器就会非常有用。
参数说明 # 词干覆盖词元过滤器必须使用以下参数中的一个进行配置。
参数 数据类型 描述 rules 字符串 直接在设置中定义覆盖规则。 rules_path 字符串 指定包含自定义规则（映射）的文件的路径。该路径可以是绝对路径，也可以是相对于配置目录的相对路径。 参考样例 # 以下示例请求创建了一个名为 my-index 的新索引，并配置了一个带有词干覆盖过滤器的分词器。
PUT /my-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_stemmer_override_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer_override&amp;quot;, &amp;quot;rules&amp;quot;: [ &amp;quot;running, runner =&amp;gt; run&amp;quot;, &amp;quot;bought =&amp;gt; buy&amp;quot;, &amp;quot;best =&amp;gt; good&amp;quot; ] } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_stemmer_override_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>词片词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/shingle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/shingle/</guid><description>词片词元过滤器 # 词片(shingle)词元过滤器用于从输入文本中生成词 n 元组（即词片）。例如，对于字符串 “slow green turtle”，词片过滤器会创建以下一元词片和二元词片：“slow”、“slow green”、“green”、“green turtle” 以及 “turtle”。
这个词元过滤器常常与其他过滤器结合使用，通过对短语而不是单个词元进行索引来提高搜索的准确性。
参数说明 # 词片词元过滤器可以使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_shingle_size 可选 整数 要连接的词元的最小数量。默认值为 2。 max_shingle_size 可选 整数 要连接的词元的最大数量。默认值为 2。 output_unigrams 可选 布尔值 是否将一元词（单个词元）包含在输出中。默认值为 true。 output_unigrams_if_no_shingles 可选 布尔值 如果未生成词片，是否输出一元词。默认值为 false。 token_separator 可选 字符串 用于将词元连接成词片的分隔符。默认值为一个空格（ ）。 filler_token 可选 字符串 插入到词元之间的空位置或间隙中的词元。默认值为下划线（_）。 如果 output_unigrams 和 output_unigrams_if_no_shingles 都设置为 true，则 output_unigrams_if_no_shingles 将被忽略。</description></item><item><title>谓词词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/predicate-token-filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/predicate-token-filter/</guid><description>谓词词元过滤器 # 谓词词元过滤器(predicate_token_filter)会根据自定义脚本中定义的条件来评估词元是应该保留还是丢弃。词元的评估是在分析谓词上下文中进行的。此过滤器仅支持内联 Painless 脚本。
参数说明 # 谓词词元过滤器有一个必需参数：script。该参数提供一个条件，用于评估词元是否应被保留。
参考样例 # 以下示例请求创建了一个名为 predicate_index 的新索引，并配置了一个带有谓词词元过滤器的分词器。该过滤器指定仅输出长度超过 7 个字符的词元。
PUT /predicate_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_predicate_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;, &amp;quot;script&amp;quot;: { &amp;quot;source&amp;quot;: &amp;quot;token.term.length() &amp;gt; 7&amp;quot; } } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;predicate_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_predicate_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /predicate_index/_analyze { &amp;quot;text&amp;quot;: &amp;quot;The Easysearch community is growing rapidly&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;predicate_analyzer&amp;quot; } 返回内容包含产生的词元</description></item><item><title>路径词元分词器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/path-hierarchy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/tokenizers/path-hierarchy/</guid><description>路径词元分词器 # 路径（path_hierarchy）词元分词器可以把文件路径（或类似的层次结构）的文本按照各个层级分解为词元。当处理诸如文件路径、网址或其他有分隔符的层次结构数据时，这个词元生成器特别有用。
参考样例 # 以下示例请求创建一个名为 my_index 的新索引，并配置一个使用路径词元生成器的分词器：
PUT /my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_path_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;path_hierarchy&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_path_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;my_path_tokenizer&amp;quot; } } } } } 生成的词元 # 使用以下请求来检查使用该分词器生成的词元：
POST /my_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_path_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;/users/john/documents/report.txt&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;/users&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 6, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;/users/john&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 11, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;/users/john/documents&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 21, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 }, { &amp;quot;token&amp;quot;: &amp;quot;/users/john/documents/report.</description></item><item><title>边缘 n 元分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/edge-n-gram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/edge-n-gram/</guid><description>边缘 n 元分词过滤器 # 边缘 n 元（edge_ngram）分词过滤器与 n 元（ngram）分词过滤器非常相似，二者都会将特定字符串拆分成不同长度的子字符串。不过，边缘 n 元分词过滤器仅从词元的开头（边缘）生成 n 元子字符串。在自动补全或前缀匹配等场景中，它特别有用，因为在这些场景里，你希望在用户输入单词或短语时就能匹配词项的开头部分。
参数说明 # 边缘 n 元分词过滤器可使用以下参数进行配置。
参数 必需/可选 数据类型 描述 min_gram 可选 整数 要生成的 n 元词项的最小长度。默认值为1。 max_gram 可选 整数 要生成的 n 元词项的最大长度。对于边缘 n 元分词过滤器，默认值为1；对于自定义分词过滤器，默认值为2。避免将此参数设置为较低的值。如果该值设置得过低，将只会生成非常短的 n 元词项，并且可能找不到搜索词。例如，将max_gram设置为 3，并且对单词“banana”建立索引，那么生成的最长词元将是“ban”。如果用户搜索“banana”，将不会返回任何匹配结果。你可以使用截断truncate分词过滤器作为搜索分析器来降低这种风险。 preserve_original 可选 布尔值 将原始词元包含在输出中。默认值为false。 参考样例 # 以下示例请求创建了一个名为 edge_ngram_example 的新索引，并配置了一个带有边缘 n 元过滤器的分词器：
PUT /edge_ngram_example { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_edge_ngram&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;, &amp;quot;min_gram&amp;quot;: 3, &amp;quot;max_gram&amp;quot;: 4 } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;, &amp;quot;my_edge_ngram&amp;quot;] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>长度分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/length/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/length/</guid><description>长度分词过滤器 # 长度分词过滤器用于从词元流中移除那些不符合指定长度标准（最小和最大长度值）的词元。
参数说明 # 长度分词过滤器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 min 可选 整数 词元的最小长度。默认值为 0。 max 可选 整数 词元的最大长度。默认值为整数类型的最大值（2147483647）。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有长度过滤器的分词器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;only_keep_4_to_10_characters&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;length_4_to_10&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;length_4_to_10&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;length&amp;quot;, &amp;quot;min&amp;quot;: 4, &amp;quot;max&amp;quot;: 10 } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>限制分词过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/limit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/limit/</guid><description>限制分词过滤器 # 限制分词过滤器用于限制分词链通过的词元数量。
参数说明 # 限制分词过滤器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 max_token_count 可选 整数 要生成的词元的最大数量。默认值为 1。 consume_all_tokens 可选 布尔值 （专家级设置）即使结果超过 max_token_count，也会使用来自词元生成器的所有词元。当设置此参数时，输出仍然只包含 max_token_count 指定数量的词元。不过，词元生成器生成的所有词元都会被处理。默认值为 false。 参考样例 # 以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有限制过滤器的分析器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;three_token_limit&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;custom_token_limit&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;custom_token_limit&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;limit&amp;quot;, &amp;quot;max_token_count&amp;quot;: 3 } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>雪球算法词元过滤器</title><link>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/snowball/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/text-analysis/token-filters/snowball/</guid><description>雪球算法词元过滤器 # 雪球算法（snowball）词元过滤器是一种基于 雪球算法的词干提取过滤器。它支持多种语言，并且比波特词干提取算法更加高效和准确。
参数说明 # 雪球词元过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：
阿拉伯语（Arabic） 亚美尼亚语（Armenian） 巴斯克语（Basque） 加泰罗尼亚语（Catalan） 丹麦语（Danish） 荷兰语（Dutch） 英语（English，默认值） 爱沙尼亚语（Estonian） 芬兰语（Finnish） 法语（French） 德语（German） 德语 2（German2） 匈牙利语（Hungarian） 意大利语（Italian） 爱尔兰语（Irish） Kp 立陶宛语（Lithuanian） 洛文斯（Lovins） 挪威语（Norwegian） 波特（Porter） 葡萄牙语（Portuguese） 罗马尼亚语（Romanian） 俄语（Russian） 西班牙语（Spanish） 瑞典语（Swedish） 土耳其语（Turkish） 参考样例 # 以下示例请求创建了一个名为 my-snowball-index 的新索引，并配置了一个带有雪球过滤器的分词器。
PUT /my-snowball-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;filter&amp;quot;: { &amp;quot;my_snowball_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;snowball&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;English&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_snowball_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_snowball_filter&amp;quot; ] } } } } } 产生的词元 # 使用以下请求来检查使用该分词器生成的词元：</description></item><item><title>搜索请求文本向量化</title><link>/easysearch/v1.14.0/docs/references/AI-Integration/search-text-embedding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/AI-Integration/search-text-embedding/</guid><description>搜索请求文本向量化 # Easysearch 使用搜索管道的 semantic_query_enricher 处理器，协助 semantic query，将文本转为向量。
搜索管道 # 您可以使用搜索管道构建新的或重用现有的结果重排器、查询重写器以及其他对查询或结果进行操作的组件。搜索管道使您能够更轻松地在 Easysearch 中处理搜索查询和搜索结果。 将部分应用程序功能迁移到 Easysearch 搜索管道中可以降低应用程序的整体复杂性。作为搜索管道的一部分，您可以指定执行模块化任务的处理器列表。 然后，您可以轻松添加或重新排序这些处理器，以自定义应用程序的搜索结果。
以下是与搜索管道相关的术语列表：
搜索请求处理器（Search request processor）：拦截搜索请求（即查询及随请求传入的元数据），对其执行某种操作，然后返回处理后的搜索请求。 搜索响应处理器（Search response processor）：拦截搜索响应及其对应的搜索请求（即查询、结果及随请求传入的元数据），对其执行某种操作，然后返回处理后的搜索响应。 处理器（Processor）：泛指搜索请求处理器或搜索响应处理器。 搜索管道（Search pipeline）：在 Easysearch 中集成的一个有序处理器列表。该管道会拦截查询，先对查询进行处理，再将其发往 Easysearch；随后拦截返回的结果，对结果进行处理，最后将结果返回给调用方，如下图所示。 上图中的 semantic_query_enricher 是一个专门为向量查询补充 Embedding 请求所需配置的处理器，使用户无需在每个查询中重复指定模型 ID、API 密钥等参数。
先决条件 # 服务兼容性 需满足以下任一条件：
支持与 OpenAI API 兼容的 embedding 接口 支持 Ollama embedding 接口 插件要求 必须安装 Easysearch 的以下插件：
knn ai 数据准备 需预先完成：</description></item><item><title>聚合查询</title><link>/easysearch/v1.14.0/docs/references/sql/aggregations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/sql/aggregations/</guid><description>聚合查询 # 介绍 # 聚合函数作用于一组值。它们通常与GROUP BY子句一起使用，将值分组为子集。
GROUP BY 子句 # GROUP BY 表达式可以是：
标识符：Identifier 序数：Ordinal 表达式：Expression 标识符 # group by 表达式可以是标识符：
os&amp;gt; SELECT gender, sum(age) FROM accounts GROUP BY gender; fetched rows / total rows = 2/2 +----------+------------+ | gender | sum(age) | |----------+------------| | F | 28 | | M | 101 | +----------+------------+ 序数 # group by 表达式可以是序数：
os&amp;gt; SELECT gender, sum(age) FROM accounts GROUP BY 1; fetched rows / total rows = 2/2 +----------+------------+ | gender | sum(age) | |----------+------------| | F | 28 | | M | 101 | +----------+------------+ group by 表达式可以是一个表达式。</description></item><item><title>身份模拟</title><link>/easysearch/v1.14.0/docs/references/security/access-control/run-as/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/access-control/run-as/</guid><description>身份模拟 # 用户模拟允许具备特定权限的用户以另外的身份来进行集群的访问。
用户模拟可用于测试和故障排除，或允许系统服务安全地充当用户。
在 REST 接口或 TCP 传输层上都可以进行用户模拟。
REST 接口 # 要允许一个用户模拟另一个用户，请将以下内容添加到 easysearch.yml :
security.authcz.rest_impersonation_user: &amp;lt;AUTHENTICATED_USER&amp;gt;: - &amp;lt;IMPERSONATED_USER_1&amp;gt; - &amp;lt;IMPERSONATED_USER_2&amp;gt; 模拟用户字段支持通配符。将其设置为 * 允许 AUTHENTICATED_USER 来模拟任意用户。
传输层配置 # 类似的配置方法如下：
security.authcz.impersonation_dn: &amp;#34;CN=spock,OU=client,O=client,L=Test,C=DE&amp;#34;: - worf 模拟其他用户 # 要模拟其他用户，请向系统提交请求，并将 HTTP 标头 security_run_as 设置为要模拟的用户的名称。例如：
curl -XGET -u &amp;#39;admin:admin&amp;#39; -k -H &amp;#34;security_run_as: user_1&amp;#34; https://localhost:9200/_security/authinfo?pretty</description></item><item><title>重建数据</title><link>/easysearch/v1.14.0/docs/references/management/reindex-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/reindex-data/</guid><description>重新索引数据 # 创建索引后，如果您需要进行广泛的更改，例如为每个文档添加一个新字段或合并多个索引以形成一个新的索引，而不是删除索引，使更改脱机，然后重新索引数据，则可以使用 reindex 操作。
使用 reindex 操作，可以将通过查询选择的所有文档或文档子集复制到另一个索引。重新索引是一个 POST 操作。在最基本的形式中，指定源索引和目标索引。
重新编制索引可能是一项昂贵的操作，具体取决于源索引的大小。我们建议您通过将 number_of_replicas 设置为 0 来禁用目标索引中的副本，并在重新索引过程完成后重新启用它们。
重新索引所有文档 # 您可以将所有文档从一个索引复制到另一个索引。
首先需要使用所需的字段映射和设置创建目标索引，或者可以从源索引中复制这些映射和设置：
PUT destination { &amp;#34;mappings&amp;#34;:{ &amp;#34;Add in your desired mappings&amp;#34; }, &amp;#34;settings&amp;#34;:{ &amp;#34;Add in your desired settings&amp;#34; } } reindex 命令将所有文档从源索引复制到目标索引：
POST _reindex { &amp;#34;source&amp;#34;:{ &amp;#34;index&amp;#34;:&amp;#34;source&amp;#34; }, &amp;#34;dest&amp;#34;:{ &amp;#34;index&amp;#34;:&amp;#34;destination&amp;#34; } } 如果尚未创建目标索引，则 reindex 操作将使用默认配置创建新的目标索引。
从远程群集 reindex # 您可以从远程集群中的索引复制文档。使用 remote 选项指定远程主机名和所需的登录凭据。
此命令会到达远程集群，使用用户名和密码登录，并将所有文档从该远程集群中的源索引复制到本地集群中的目标索引：
POST _reindex { &amp;#34;source&amp;#34;:{ &amp;#34;remote&amp;#34;:{ &amp;#34;host&amp;#34;:&amp;#34;https://&amp;lt;REST_endpoint_of_remote_cluster&amp;gt;:9200&amp;#34;, &amp;#34;username&amp;#34;:&amp;#34;YOUR_USERNAME&amp;#34;, &amp;#34;password&amp;#34;:&amp;#34;YOUR_PASSWORD&amp;#34; } }, &amp;#34;dest&amp;#34;:{ &amp;#34;index&amp;#34;:&amp;#34;destination&amp;#34; } } 您可以指定以下选项：</description></item><item><title>全文搜索</title><link>/easysearch/v1.14.0/docs/references/sql/fulltext/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/sql/fulltext/</guid><description>全文搜索 # 全文搜索是对存储的单个文档进行搜索，这与基于数据库中的原始文本的常规搜索有所区别。全文搜索尝试通过检查每个文档中的所有单词来匹配搜索条件。 在 Easysearch 中，提供的全文查询使你能够搜索在索引过程中分析的文本字段。
Match Query # 在 Easysearch 中，Match 查询是执行全文搜索的标准查询。MATCHQUERY 和 MATCH_QUERY 都是用于执行匹配查询的函数。
示例 1 # 这两个函数都可以接受字段名称作为第一个参数，文本作为第二个参数。
SQL query:
POST /_sql { &amp;quot;query&amp;quot; : &amp;quot;&amp;quot;&amp;quot; SELECT account_number, address FROM accounts WHERE MATCH_QUERY(address, 'Holmes') &amp;quot;&amp;quot;&amp;quot; } 解释：
{ &amp;quot;from&amp;quot; : 0, &amp;quot;size&amp;quot; : 200, &amp;quot;query&amp;quot; : { &amp;quot;bool&amp;quot; : { &amp;quot;filter&amp;quot; : [ { &amp;quot;bool&amp;quot; : { &amp;quot;must&amp;quot; : [ { &amp;quot;match&amp;quot; : { &amp;quot;address&amp;quot; : { &amp;quot;query&amp;quot; : &amp;quot;Holmes&amp;quot;, &amp;quot;operator&amp;quot; : &amp;quot;OR&amp;quot;, &amp;quot;prefix_length&amp;quot; : 0, &amp;quot;max_expansions&amp;quot; : 50, &amp;quot;fuzzy_transpositions&amp;quot; : true, &amp;quot;lenient&amp;quot; : false, &amp;quot;zero_terms_query&amp;quot; : &amp;quot;NONE&amp;quot;, &amp;quot;auto_generate_synonyms_phrase_query&amp;quot; : true, &amp;quot;boost&amp;quot; : 1.</description></item><item><title>文本向量化</title><link>/easysearch/v1.14.0/docs/references/AI-Integration/text-embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/AI-Integration/text-embeddings/</guid><description>[已废弃] 文本向量化 # 本文档描述的功能已不再支持，将在下个版本删除，请使用新的 写入数据文本向量化替代。
本文档介绍如何在 Easysearch 中集成和使用预先部署的 Ollama 服务来生成文本嵌入向量。
先决条件 # 需要预先部署好 Ollama 服务，现阶段集成的服务版本是 0.5.4。
可以用以下命令测试服务是否正常：
curl http://localhost:11434/api/embed -d '{ &amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text:latest&amp;quot;, &amp;quot;input&amp;quot;: &amp;quot;Why is the sky blue?&amp;quot; }' 配置 Ollama 服务 # 可以通过 ollama_url 配置项指定 Ollama 服务的地址。您可以通过以下 API 查看当前配置：
GET _cluster/settings?flat_settings=true&amp;amp;include_defaults=true&amp;amp;filter_path=*.ollama_url 如果没有修改，会输出默认值：
{ &amp;quot;defaults&amp;quot;: { &amp;quot;ollama_url&amp;quot;: &amp;quot;http://localhost:11434&amp;quot; } } REST API # POST /_ai/embed { &amp;quot;model&amp;quot;: &amp;quot;模型名称&amp;quot;, &amp;quot;input&amp;quot;: &amp;quot;文本内容&amp;quot; } 请求示例 # POST /_ai/embed { &amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text:latest&amp;quot;, &amp;quot;input&amp;quot;: &amp;quot;Llamas are members of the camelid family&amp;quot; } 批量生成 Embeddings # 可以一次为多个文本生成嵌入向量。</description></item><item><title>跨集群搜索</title><link>/easysearch/v1.14.0/docs/references/security/access-control/cross-cluster-search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/access-control/cross-cluster-search/</guid><description>跨集群搜索 # 跨集群搜索正是它听起来的样子：它允许集群中的任何节点对其他集群执行搜索请求。Easysearch 支持开箱即用的跨集群搜索。
身份验证流程 # 当跨集群搜索通过 协调集群 访问 远程集群 时：
安全模块对协调集群上的用户进行身份验证。 安全模块在协调集群上获取用户的后端角色。 请求调用（包括经过身份验证的用户）将转发到远程集群。 在远程群集上评估用户的权限。 远程群集和协调集群可以分别配置不同的身份验证和授权配置，但我们建议在两者上使用相同的设置。
权限信息 # 要查询远程集群上的索引，除了 READ 或 SEARCH 权限外，用户还需要具有以下索引权限：
indices:admin/shards/search_shards role.yml 样例配置 # humanresources: cluster: - CLUSTER_COMPOSITE_OPS_RO indices: &amp;#34;humanresources&amp;#34;: &amp;#34;*&amp;#34;: - READ - indices:admin/shards/search_shards # needed for CCS 配置流程 # 分别启动两个集群，如下：
➜ curl -k &amp;#39;https://localhost:9200/_cluster/health?pretty&amp;#39; -u admin:admin { &amp;#34;cluster_name&amp;#34; : &amp;#34;easysearch&amp;#34;, &amp;#34;status&amp;#34; : &amp;#34;green&amp;#34;, &amp;#34;timed_out&amp;#34; : false, &amp;#34;number_of_nodes&amp;#34; : 1, &amp;#34;number_of_data_nodes&amp;#34; : 1, &amp;#34;active_primary_shards&amp;#34; : 1, &amp;#34;active_shards&amp;#34; : 1, &amp;#34;relocating_shards&amp;#34; : 0, &amp;#34;initializing_shards&amp;#34; : 0, &amp;#34;unassigned_shards&amp;#34; : 0, &amp;#34;delayed_unassigned_shards&amp;#34; : 0, &amp;#34;number_of_pending_tasks&amp;#34; : 0, &amp;#34;number_of_in_flight_fetch&amp;#34; : 0, &amp;#34;task_max_waiting_in_queue_millis&amp;#34; : 0, &amp;#34;active_shards_percent_as_number&amp;#34; : 100.</description></item><item><title>Easysearch-client</title><link>/easysearch/v1.14.0/docs/release-notes/client/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/release-notes/client/</guid><description>版本发布日志 # 这里是 INFINI Easysearch-client 历史版本发布的相关说明。
2.0.2(2024-08-13) # Improvements # 升级相关依赖项至安全版本 2.0.0(2024-04-17) # Breaking changes # Features # 发布全新的 Easysearch java 客户端 2.0 版本。 客户端经过完全重构，更加轻量级，避免冗余的第三方依赖。 为常用 Easysearch API 提供强类型的请求和响应。 API 均支持阻塞和异步方式。 使用流式构建器和函数式模式，以在创建复杂嵌套结构时编写简洁易读的代码。 通过使用 Jackson 无缝集成应用程序类。 自带 Java 低级别 REST 客户端，处理所有传输级别的问题：HTTP 连接池、重试、节点发现等。 Bug fix # Improvements # 1.0.1(2023-11-14) # Breaking changes # Features # 正式发布 Easysearch Java 客户端。这一里程碑式的更新为开发人员带来了前所未有的便利性，使得与 Easysearch 集群的交互变得更加简洁和直观。现在，通过 Easysearch-client 客户端，开发者可以直接使用 Java 方法和数据结构来进行交互，而不再需要依赖于传统的 HTTP 方法和 JSON。这一变化大大简化了操作流程，使得数据管理和索引更加高效。高级客户端的功能范围包括处理数据操作，管理集群，包括查看和维护集群的健康状态，并对 Security 模块全面兼容。它提供了一系列 API，用于管理角色、用户、权限、角色映射和账户。这意味着安全性和访问控制现在可以更加细粒度地管理，确保了数据的安全性和合规性。 Bug fix # Improvements #</description></item><item><title>任务管理</title><link>/easysearch/v1.14.0/docs/references/management/tasksapis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/tasksapis/</guid><description>任务管理 # 任务是在集群中运行的任何操作。例如，搜索图书数据集以查找标题或作者姓名是一项任务。将自动创建任务以监视集群的运行状况和性能。有关集群中当前执行的所有任务的详细信息，可以使用 tasks API 操作。
以下请求返回有关所有任务的信息：
GET _tasks 通过包含任务 ID，您可以获得特定任务的信息。请注意，任务 ID 由节点的标识字符串和任务的数字 ID 组成。例如，如果节点的标识串是 nodestring ，任务的数字标识是 1234 ，则任务 ID 是 nodestring:1234 。您可以通过运行 tasks 操作来查找此信息。
GET _tasks/&amp;lt;task_id&amp;gt; 请注意，如果任务完成运行，它将不会作为请求的一部分返回。对于一个需要稍长时间才能完成的任务的示例，可以在较大的文档上运行 _reindex API 操作，然后运行 tasks 。
Sample Response
{ &amp;#34;nodes&amp;#34;: { &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;easy-node1&amp;#34;, &amp;#34;transport_address&amp;#34;: &amp;#34;30.18.0.3:9300&amp;#34;, &amp;#34;host&amp;#34;: &amp;#34;30.18.0.3&amp;#34;, &amp;#34;ip&amp;#34;: &amp;#34;30.18.0.3:9300&amp;#34;, &amp;#34;roles&amp;#34;: [&amp;#34;data&amp;#34;, &amp;#34;ingest&amp;#34;, &amp;#34;master&amp;#34;, &amp;#34;remote_cluster_client&amp;#34;], &amp;#34;tasks&amp;#34;: { &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ:17416&amp;#34;: { &amp;#34;node&amp;#34;: &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ&amp;#34;, &amp;#34;id&amp;#34;: 17416, &amp;#34;type&amp;#34;: &amp;#34;transport&amp;#34;, &amp;#34;action&amp;#34;: &amp;#34;cluster:monitor/tasks/lists&amp;#34;, &amp;#34;start_time_in_millis&amp;#34;: 1613599752458, &amp;#34;running_time_in_nanos&amp;#34;: 994000, &amp;#34;cancellable&amp;#34;: false, &amp;#34;headers&amp;#34;: {} }, &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ:17413&amp;#34;: { &amp;#34;node&amp;#34;: &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ&amp;#34;, &amp;#34;id&amp;#34;: 17413, &amp;#34;type&amp;#34;: &amp;#34;transport&amp;#34;, &amp;#34;action&amp;#34;: &amp;#34;indices:data/write/bulk&amp;#34;, &amp;#34;start_time_in_millis&amp;#34;: 1613599752286, &amp;#34;running_time_in_nanos&amp;#34;: 30846500, &amp;#34;cancellable&amp;#34;: false, &amp;#34;parent_task_id&amp;#34;: &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ:17366&amp;#34;, &amp;#34;headers&amp;#34;: {} }, &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ:17366&amp;#34;: { &amp;#34;node&amp;#34;: &amp;#34;Mgqdm0f9SEGClWxp_RdnaQ&amp;#34;, &amp;#34;id&amp;#34;: 17366, &amp;#34;type&amp;#34;: &amp;#34;transport&amp;#34;, &amp;#34;action&amp;#34;: &amp;#34;indices:data/write/reindex&amp;#34;, &amp;#34;start_time_in_millis&amp;#34;: 1613599750929, &amp;#34;running_time_in_nanos&amp;#34;: 1529733100, &amp;#34;cancellable&amp;#34;: true, &amp;#34;headers&amp;#34;: {} } } } } } 您还可以在查询中使用以下参数。</description></item><item><title>复杂查询</title><link>/easysearch/v1.14.0/docs/references/sql/complex/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/sql/complex/</guid><description>复杂查询 # 子查询 (Subquery) # 子查询 (subquery) 是一个完整的 SELECT 语句，它被用在另一个语句中，并用括号括起来。从 explain 输出中，您可以注意到一些子查询实际上被转换为等效的联接查询来执行。
示例 1：表子查询 # SQL query:
POST /_sql { &amp;quot;query&amp;quot; : &amp;quot;&amp;quot;&amp;quot; SELECT a1.firstname, a1.lastname, a1.balance FROM accounts a1 WHERE a1.account_number IN ( SELECT a2.account_number FROM accounts a2 WHERE a2.balance &amp;gt; 10000 ) &amp;quot;&amp;quot;&amp;quot; } 解释：
{ &amp;quot;Physical Plan&amp;quot; : { &amp;quot;Project [ columns=[a1.balance, a1.firstname, a1.lastname] ]&amp;quot; : { &amp;quot;Top [ count=200 ]&amp;quot; : { &amp;quot;BlockHashJoin[ conditions=( a1.</description></item><item><title>Easysearch</title><link>/easysearch/v1.14.0/docs/release-notes/easysearch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/release-notes/easysearch/</guid><description>版本发布日志 # 这里是 INFINI Easysearch 历史版本发布的相关说明。
Latest (In development) # Breaking changes # Features # Bug fix # Improvements # 1.14.0 (2025-07-25) # Breaking changes # AI 模块 从 modules 迁移至 plugins 目录下，方便调用 knn 插件 旧的文本向量化接口 _ai/embed 已不再支持，将在后续版本删除 Features # 插件模块新增完整的文本嵌入模型集成功能，涵盖从数据导入到向量检索的全流程 新增语义检索 API，简化向量搜索使用流程 新增语义检索处理器配置大模型信息 新增搜索管道（Search pipelines），轻松地在 Easysearch 内部处理查询请求和查询结果 多模型集成支持 OpenAI 向量模型：直接调用 OpenAI 的嵌入接口（如 text-embedding-3-small）。 Ollama 本地模型：支持离线环境或私有化部署的向量生成。 Bug fix # Improvements # 增强数据摄取管道（ingest pipeline） 在数据索引阶段支持文本向量化，文档可自动生成向量表示 导入数据时通过 ingest 管道进行向量化时支持单条和批量模式，适配大模型的请求限制场景 更新 Easysearch Docker 初始化文档 1.</description></item><item><title>基础查询</title><link>/easysearch/v1.14.0/docs/references/sql/basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/sql/basics/</guid><description>基础查询 # 介绍 # SQL 中的 SELECT 语句是从 Easysearch 索引中检索数据的最常见查询。在本文档中，只涵盖涉及单个索引和查询的简单 SELECT 语句。 SELECT 语句包括 SELECT、FROM、WHERE、GROUP BY、HAVING、ORDER BY 和 LIMIT 子句。其中，SELECT 和 FROM 是指定要获取哪些字段以及它们应该从哪个索引获取的基础。 其它所有子句都是可选的，根据您的需求使用。请继续阅读以了解它们的详细描述、语法和用例。
语法 # SELECT 语句的语法如下：
SELECT [ALL | DISTINCT] (* | expression) [[AS] alias] [, ...] FROM index_name [WHERE predicates] [GROUP BY expression [, ...] [HAVING predicates]] [ORDER BY expression [ASC | DESC] [NULLS {FIRST | LAST}] [, ...]] [LIMIT [offset, ] size] 尽管不支持批量执行多个查询语句，但仍然允许以分号 ; 结束。例如，你可以运行 SELECT * FROM accounts; 而不会遇到问题。这对于支持其他工具生成的查询，如 Microsoft Excel 或 BI 工具，非常有用。</description></item><item><title>快速开始</title><link>/easysearch/v1.14.0/docs/references/client/java-client/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/client/java-client/</guid><description>Easysearch Java API Client 使用文档 # 简介 # Easysearch Java API Client 是 Easysearch 的官方 Java 客户端，提供了简洁、强大且类型安全的 API 接口。
全新重构的 2.0.x 版本，更轻量级的设计，移除冗余依赖。 兼容 Easysearch 各个版本。 为常用 Easysearch API 提供强类型的请求和响应。 API 均支持阻塞和异步方式。 使用流式构建器和函数式模式，以在创建复杂嵌套结构时编写简洁易读的代码。 通过使用 Jackson 无缝集成应用程序类。 快速开始 # 本页指导您完成Java客户端的安装过程，展示了如何实例化客户端，以及如何使用它执行基本的 Easysearch 操作。
安装 # easysearch-client 已经发布到 Maven https://mvnrepository.com/artifact/com.infinilabs/easysearch-client/2.0.2
安装需要 jdk8 或以上版本
easysearch-client 使用 Jackson 将业务代码和客户端 api 进行集成。
在 Maven 项目中安装 # 相比 1.x 版本的客户端，新版客户端的安装更加简单，只需在您项目的 pom 文件的 dependencies 区域添加以下依赖以引入客户端</description></item><item><title>词项查询</title><link>/easysearch/v1.14.0/docs/references/search/term/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/search/term/</guid><description>词项查询 # Easysearch 在搜索数据时支持两种类型的查询：词项 (term) 查询和全文查询。
下表显示了它们之间的差异：
词项查询 全文检索 描述 term 查询回应与查询匹配的文档。 全文查询回应文档与查询的匹配程度。 分词 搜索 term 是不分词的。这意味着 term 查询按原样搜索您的 term。 搜索 term 由索引时用于文档指定字段的分词器进行分词。这意味着您的搜索词将经历与文档字段相同的分词过程。 相关性 Term 级查询只返回匹配的文档，而不根据相关性得分对其进行排序。他们仍然计算相关性得分，但该得分对于返回的所有文档都是相同的。 全文查询计算每个匹配的相关性得分，并按相关性的降序对结果进行排序。 应用场景 当您希望匹配数字、日期、 tag 等精确值，并且不需要按相关性对匹配项进行排序时，请使用术语级查询。 在考虑大小写和词干变体等因素后，使用全文查询来匹配文本字段并按相关性排序。 Easysearch 使用名为 Okapi BM25 的概率排名框架来计算相关性得分。要了解更多关于 Okapi BM25 的信息，请参阅 维基百科.
假设您在 Easysearch 集群中索引了莎士比亚全集。我们使用 term 查询在 text_entry 字段中搜索短语 “To be，or not be”：</description></item><item><title>API 使用</title><link>/easysearch/v1.14.0/docs/references/client/client-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/client/client-api/</guid><description>Easysearch Java API Client 使用文档 # 管理索引 # 使用客户端对索引进行管理
String index = &amp;#34;test1&amp;#34;; if (client.indices().exists(r -&amp;gt; r.index(index)).value()) { LOGGER.info(&amp;#34;Deleting index &amp;#34; + index); DeleteIndexResponse deleteIndexResponse = client.indices().delete(new DeleteIndexRequest.Builder().index(index).build()); LOGGER.info(deleteIndexResponse.toString()); } LOGGER.info(&amp;#34;Creating index &amp;#34; + index); CreateIndexResponse createIndexResponse = client.indices().create(req -&amp;gt; req.index(index)); CloseIndexResponse closeIndexResponse = client.indices().close(req -&amp;gt; req.index(index)); OpenResponse openResponse = client.indices().open(req -&amp;gt; req.index(index)); RefreshResponse refreshResponse = client.indices().refresh(req -&amp;gt; req.index(index)); FlushResponse flushResponse = client.indices().flush(req -&amp;gt; req.index(index)); ForcemergeResponse forcemergeResponse = client.</description></item><item><title>API 接口</title><link>/easysearch/v1.14.0/docs/references/security/access-control/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/access-control/api/</guid><description>API # 通过 REST API 可以管理用户、角色、角色映射、权限集合和租户。
API 的访问控制 # 您可以控制哪些角色可以访问安全相关的 API，在配置文件 easysearch.yml:
security.restapi.roles_enabled: [&amp;#34;&amp;lt;role&amp;gt;&amp;#34;, ...] 如果希望阻止访问特定的 API：
security.restapi.endpoints_disabled.&amp;lt;role&amp;gt;.&amp;lt;endpoint&amp;gt;: [&amp;#34;&amp;lt;method&amp;gt;&amp;#34;, ...] 参数 endpoint 可以是:
PRIVILEGE ROLE ROLE_MAPPING USER CONFIG CACHE 参数 method 可以是:
GET PUT POST DELETE PATCH 例如，以下配置授予三个角色对 REST API 的访问权限，但随后会阻止 test-role 发送 PUT, POST, DELETE, 或 PATCH 到 _security/role 或 _security/user :
security.restapi.roles_enabled: [&amp;#34;superuser&amp;#34;, &amp;#34;security_rest_api_access&amp;#34;, &amp;#34;test-role&amp;#34;] security.restapi.endpoints_disabled.test-role.ROLE: [&amp;#34;PUT&amp;#34;, &amp;#34;POST&amp;#34;, &amp;#34;DELETE&amp;#34;, &amp;#34;PATCH&amp;#34;] security.restapi.endpoints_disabled.test-role.USER: [&amp;#34;PUT&amp;#34;, &amp;#34;POST&amp;#34;, &amp;#34;DELETE&amp;#34;, &amp;#34;PATCH&amp;#34;] 要为 API 配置 使用 PUT 和 PATCH 方法，请将以下行添加到 easysearch.</description></item><item><title>全文查询</title><link>/easysearch/v1.14.0/docs/references/search/full-text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/search/full-text/</guid><description>全文查询 # 尽管可以使用 HTTP 请求参数执行简单搜索，但 Easysearch 查询域特定语言（DSL）允许您指定全部搜索选项。查询 DSL 使用 HTTP 请求主体。以这种方式指定的查询还有一个额外的优点，即其意图更加明确，并且更易于随时间调整。
此页面列出了所有全文查询类型和常用选项。考虑到选项的数量和微妙的行为，确保有用搜索结果的最佳方法是根据代表性索引测试不同的查询并验证输出。
匹配 # 创建一个 布尔查询 ，如果字段中存在搜索项，则返回结果。
查询的最基本形式仅提供字段（ title ）和对应的值（ wind ）:
GET _search { &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;title&amp;#34;: &amp;#34;wind&amp;#34; } } } 采用 curl的方式:
curl --insecure -XGET -u &amp;#39;admin:admin&amp;#39; https://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;index&amp;gt;/_search \ -H &amp;#34;content-type: application/json&amp;#34; \ -d &amp;#39;{ &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;title&amp;#34;: &amp;#34;wind&amp;#34; } } }&amp;#39; 查询接受以下选项。有关每个参数的描述，请参见 查询选项
GET _search { &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;title&amp;#34;: { &amp;#34;query&amp;#34;: &amp;#34;wind&amp;#34;, &amp;#34;fuzziness&amp;#34;: &amp;#34;AUTO&amp;#34;, &amp;#34;fuzzy_transpositions&amp;#34;: true, &amp;#34;operator&amp;#34;: &amp;#34;or&amp;#34;, &amp;#34;minimum_should_match&amp;#34;: 1, &amp;#34;analyzer&amp;#34;: &amp;#34;standard&amp;#34;, &amp;#34;zero_terms_query&amp;#34;: &amp;#34;none&amp;#34;, &amp;#34;lenient&amp;#34;: false, &amp;#34;cutoff_frequency&amp;#34;: 0.</description></item><item><title>Date nanoseconds 字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/date-field-type/date-nanos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/date-field-type/date-nanos/</guid><description>Date nanoseconds 字段类型 # 日期纳秒字段类型与 日期 字段类型类似，它存储一个日期。然而，date 以毫秒分辨率存储日期，而 date_nanos 以纳秒分辨率存储日期。日期以 long 值的形式存储，表示自纪元以来的纳秒数。因此，支持的日期范围大约是 1970-2262 年。
对 date_nanos 字段的查询被转换为对字段值的 long 表示形式的范围查询。然后使用字段上设置的格式将存储的字段和聚合结果转换为字符串。
date_nano 字段支持 date 支持的所有格式和参数。你可以使用 || 分隔的多种格式。
对于 date_nanos 字段，你可以使用 strict_date_optional_time_nanos 格式来保留纳秒值。如果你在将字段映射为 date_nanos 时没有指定格式，默认格式是 strict_date_optional_time||epoch_millis，它允许你以 strict_date_optional_time 或 epoch_millis 格式传递值。strict_date_optional_time 格式支持纳秒的日期，但 epoch_millis 格式仅支持毫秒的日期。
示例 # 创建一个具有 strict_date_optional_time_nanos 格式的 date_nanos 类型的 date 字段的映射：
PUT testindex/_mapping { &amp;#34;properties&amp;#34;: { &amp;#34;date&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;date_nanos&amp;#34;, &amp;#34;format&amp;#34; : &amp;#34;strict_date_optional_time_nanos&amp;#34; } } } 将两个文档写入到索引中：
PUT testindex/_doc/1 { &amp;#34;date&amp;#34;: &amp;#34;2022-06-15T10:12:52.</description></item><item><title>Date 字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/date-field-type/date/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/date-field-type/date/</guid><description>Date 字段类型 # 在 Easysearch 中，日期可以表示为以下几种形式：
一个长整型值，对应自纪元以来的毫秒数（必须为非负数）。日期在内部以此形式存储。 一个格式化的字符串。 一个整数值，对应自纪元以来的秒数（必须为非负数）。 要表示日期范围，可以使用 date range 字段类型。
代码样例 # 创建一个有两种日期格式的 date 字段
PUT testindex { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;release_date&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;date&amp;quot;, &amp;quot;format&amp;quot; : &amp;quot;strict_date_optional_time||epoch_millis&amp;quot; } } } } 参数说明 # 下表列出了日期字段类型支持的参数，所有参数均为可选项。 参数 描述 默认值 boost 浮点值，指定字段对相关性评分的权重。值大于 1.0 增加相关性，0.0 到 1.0 降低相关性。 1.0 doc_values 布尔值，指定是否将字段存储到磁盘，以便用于聚合、排序或 script 脚本操作。 false format 用于解析日期的格式。 strict_date_time_no_millis || strict_date_optional_time || epoch_millis ignore_malformed 布尔值，指定是否忽略格式错误的值而不抛出异常。 false index 布尔值，指定字段是否可搜索。 true locale 指定基于区域和语言的日期表示格式。 ROOT（区域和语言中立的本地设置） meta 接受字段的元数据。 null_value 指定替代 null 的值，必须与字段类型一致。如果未指定，字段值为 null 时会被视为缺失值。 null store 布尔值，指定字段值是否单独存储并可从 _source 字段外检索。 false 格式 # Easysearch 提供内置的日期格式，但您也可以自定义日期格式。</description></item><item><title>Flat 对象字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/object-field-type/flattened/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/object-field-type/flattened/</guid><description>Flat 对象字段类型 # 在 Easysearch 中，您不需要在索引文档之前指定映射。如果您不指定映射，Easysearch 会使用动态映射自动映射文档中的每个字段及其子字段。当您摄取诸如日志之类的文档时，您可能事先不知道每个字段的子字段名称和类型。在这种情况下，动态映射所有新的子字段可能会快速导致&amp;quot;映射爆炸&amp;quot;，其中不断增长的字段数量可能会降低集群的性能。
Flat 对象字段类型通过将整个 JSON 对象视为字符串来解决这个问题。可以使用标准的点路径表示法访问 JSON 对象中的子字段，但它们不会被索引成单独的字段以供快速查找。
点表示法（a.b）中的字段名最大长度为 2^24 − 1。
Flat 对象字段类型提供以下优势：
高效读取：获取性能类似于关键字字段。 内存效率：将整个复杂的 JSON 对象存储在一个字段中而不索引其所有子字段，可以减少索引中的字段数量。 空间效率：Easysearch 不会为 flat 对象中的子字段创建倒排索引，从而节省空间。 迁移兼容性：您可以将数据从支持类似 flat 字段的数据库系统迁移到 Easysearch。 当字段及其子字段主要用于读取而不是用作搜索条件时，应将字段映射为 flat 对象，因为子字段不会被索引。当对象具有大量字段或您事先不知道内容时，flat 对象非常有用。
Flat 对象支持带有和不带有点路径表示法的精确匹配查询。有关支持的查询类型的完整列表，请参见支持的查询。
在文档中搜索特定嵌套字段的值可能效率低下，因为它可能需要对索引进行完整扫描，这可能是一个昂贵的操作。
Flat 对象不支持：
特定类型的解析。 数值运算，如数值比较或数值排序。 文本分析。 高亮显示。 使用点表示法(a.b)的子字段聚合。 按子字段过滤。 支持的查询 # Flat 对象字段类型支持以下查询：
Term Terms Terms set Prefix Range Match Multi-match Query string Simple query string Exists Wildcard 限制 # 以下限制适用于 Easysearch 中的 flat 对象：</description></item><item><title>Flat 对象字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/object-field-type/flattened_text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/object-field-type/flattened_text/</guid><description>flattened_text 字段类型 # Introduced 1.10.0
flattened_text 类型是一种特殊的数据结构，适用于存储和查询嵌套层次的数据，同时保留类似于 text 类型的灵活搜索特性，例如分词和全文匹配。它在处理结构化或者半结构化数据时非常有用，例如 JSON 对象或动态键值对映射。
定义映射 # { &amp;#34;properties&amp;#34;: { &amp;#34;my_field&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;flattened_text&amp;#34; } } } 特性 # 扁平化存储
将嵌套的 JSON 对象转换为扁平结构 保留完整的路径信息 支持点号访问内部字段 文本分析
支持标准分词器 支持短语查询 支持全文搜索功能 内部索引结构 每个 flattened_text 字段在 lucene 层面会创建多个子字段:
{field} - 存储所有键 {field}._value - 存储所有值 {field}._valueAndPath - 存储 &amp;ldquo;path=value&amp;rdquo; 格式 索引示例 # PUT my_index/_doc/1 { &amp;quot;my_field&amp;quot;: { &amp;quot;key1&amp;quot;: { &amp;quot;subkey1&amp;quot;: { &amp;quot;subkey2&amp;quot;: &amp;quot;quick brown fox&amp;quot; } } } } 查询示例 # 精确路径匹配 # // Match Query - 指定完整路径 GET my_index/_search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;my_field.</description></item><item><title>Geopoint 地理点字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/geo-filed-type/geo-point/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/geo-filed-type/geo-point/</guid><description>Geopoint 地理点字段类型 # Geopoint 地理点字段类型包含由纬度 latitude 和经度 longitude 指定的地理点。
代码示例 # 创建一个带有 Geopoint 地理点字段类型的映射：
PUT testindex1 { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;point&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;geo_point&amp;#34; } } } } 地理点格式 # Geopoint 地理点可以用以下格式索引：
包含纬度和经度的对象 PUT testindex1/_doc/1 { &amp;#34;point&amp;#34;: { &amp;#34;lat&amp;#34;: 40.71, &amp;#34;lon&amp;#34;: 74.00 } } 写入包含纬度,经度 的文档 PUT testindex1/_doc/2 { &amp;#34;point&amp;#34;: &amp;#34;40.71,74.00&amp;#34; } geohash 格式的文档 PUT testindex1/_doc/3 { &amp;#34;point&amp;#34;: &amp;#34;txhxegj0uyp3&amp;#34; } [经度, 纬度] 格式的数组 PUT testindex1/_doc/4 { &amp;#34;point&amp;#34;: [74.</description></item><item><title>Geoshape 地理形状字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/geo-filed-type/geo-shape/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/geo-filed-type/geo-shape/</guid><description>Geoshape 地理形状字段类型 # Geoshape 地理形状字段类型包含地理形状，例如多边形或地理点的集合。为了索引地理形状，Easysearch 会将形状分割成三角形网格，并将每个三角形存储在 BKD 树中。这提供了 10^-7 度的精度，代表了接近完美的空间分辨率。这个过程的性能主要受到您正在索引的多边形顶点数量多少的影响。
代码样例 # 创建一个带有地理形状字段类型的映射：
PUT testindex { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;location&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;geo_shape&amp;#34; } } } } 格式说明 # 地理形状可以用以下格式索引：
GeoJSON Well-Known Text (WKT) 在 GeoJSON 和 WKT 中，坐标必须在坐标数组中按照 经度, 纬度 的顺序指定。注意在这种格式中经度是在前面的。
地理形状类型 # 下表描述了可能的地理形状类型以及它们与 GeoJSON 和 WKT 类型的关系。
Easysearch 类型 GeoJSON 类型 WKT 类型 描述 point Point POINT 由纬度和经度指定的地理点。Easysearch 使用世界大地测量系统 (WGS84) 坐标。 linestring LineString LINESTRING 由两个或更多点指定的线。可以是直线或连接的线段路径。 polygon Polygon POLYGON 由坐标形式的顶点列表指定的多边形。多边形必须是闭合的，这意味着最后一个点必须与第一个点相同。因此，要创建一个 n 边形，需要 n+1 个顶点。最少需要四个顶点，这会创建一个三角形。 multipoint MultiPoint MULTIPOINT 不连接的离散相关点的数组。 multilinestring MultiLineString MULTILINESTRING 线串的数组。 multipolygon MultiPolygon MULTIPOLYGON 多边形的数组。 geometrycollection GeometryCollection GEOMETRYCOLLECTION 可能是不同类型的地理形状的集合。 envelope N/A BBOX 由左上和右下顶点指定的边界矩形。 Point 点位 # 一个点代表着由经度和纬度指定的单个坐标对。</description></item><item><title>ID 属性</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/id/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/id/</guid><description>ID 属性 # Easysearch 中的每个文档都有一个唯一的 _id 字段。此字段已被索引，允许您使用 GET API 或 ids 查询 检索文档。
如果您未提供 _id 值，则 Easysearch 会自动为文档生成一个。
以下示例请求创建一个名为 test-index1 的索引，并添加两个具有不同 _id 值的文档：
PUT test-index1/_doc/1 { &amp;quot;text&amp;quot;: &amp;quot;Document with ID 1&amp;quot; } PUT test-index1/_doc/2?refresh=true { &amp;quot;text&amp;quot;: &amp;quot;Document with ID 2&amp;quot; } 您可以使用 _id 字段查询文档，如以下示例请求所示：
GET test-index1/_search { &amp;quot;query&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;_id&amp;quot;: [&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;] } } } 返回 _id 值为 1 和 2 的两个文档：
{ &amp;quot;took&amp;quot;: 10, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 2, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 1, &amp;quot;hits&amp;quot;: [ { &amp;quot;_index&amp;quot;: &amp;quot;test-index1&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;_score&amp;quot;: 1, &amp;quot;_source&amp;quot;: { &amp;quot;text&amp;quot;: &amp;quot;Document with ID 1&amp;quot; } }, { &amp;quot;_index&amp;quot;: &amp;quot;test-index1&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;2&amp;quot;, &amp;quot;_score&amp;quot;: 1, &amp;quot;_source&amp;quot;: { &amp;quot;text&amp;quot;: &amp;quot;Document with ID 2&amp;quot; } } ] } _id 字段的限制 # 虽然 _id 字段可以在各种查询中使用，但它在聚合、排序和脚本中的使用受到限制。如果您需要对 _id 字段进行排序或聚合，建议将 _id 内容复制到另一个启用了 doc_values 的字段中。</description></item><item><title>IP 地址字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/ip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/ip/</guid><description>IP 地址字段类型 # IP 字段类型用于存储 IPv4 或 IPv6 格式的 IP 地址。
要表示 IP 地址范围，可以使用 IP 范围字段类型
参考代码 # 创建一个有 IP 地址的 mapping
PUT testindex { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;ip_address&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;ip&amp;quot; } } } } 索引一个有 IP 地址的文档
PUT testindex/_doc/1 { &amp;quot;ip_address&amp;quot; : &amp;quot;10.24.34.0&amp;quot; } 查询一个特定 IP 地址的索引
GET testindex/_doc/1 { &amp;quot;query&amp;quot;: { &amp;quot;term&amp;quot;: { &amp;quot;ip_address&amp;quot;: &amp;quot;10.24.34.0&amp;quot; } } } 搜索 IP 地址及其关联的网络掩码 # 您可以使用无类别域间路由 (CIDR) 表示法查询索引中的 IP 地址。在 CIDR 表示法中，通过斜杠 / 分隔 IP 地址和前缀长度（0–32）。例如，前缀长度为 24 表示匹配所有具有相同前 24 位的 IP 地址。</description></item><item><title>Join 字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/object-field-type/join/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/object-field-type/join/</guid><description>Join 字段类型 # Join 字段类型用于在同一索引中的文档之间建立父/子关系。
代码样例 # 模拟创建一个映射来建立一个产品和其品牌之间的父/子关系：
PUT testindex1 { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;product_to_brand&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;join&amp;#34;, &amp;#34;relations&amp;#34;: { &amp;#34;brand&amp;#34;: &amp;#34;product&amp;#34; } } } } } 索引一个父文档：
PUT testindex1/_doc/1 { &amp;#34;name&amp;#34;: &amp;#34;Brand 1&amp;#34;, &amp;#34;product_to_brand&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;brand&amp;#34; } } 您也可以使用更简单的格式：
PUT testindex1/_doc/1 { &amp;#34;name&amp;#34;: &amp;#34;Brand 1&amp;#34;, &amp;#34;product_to_brand&amp;#34;: &amp;#34;brand&amp;#34; } 在索引子文档时，您需要指定 routing 查询参数，因为同一父/子层级中的父文档和子文档必须索引在同一分片上。每个子文档在 parent 字段中引用其父文档的 ID。
为每个父文档索引两个子文档：
PUT testindex1/_doc/3?routing=1 { &amp;#34;name&amp;#34;: &amp;#34;Product 1&amp;#34;, &amp;#34;product_to_brand&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;product&amp;#34;, &amp;#34;parent&amp;#34;: &amp;#34;1&amp;#34; } } PUT testindex1/_doc/4?</description></item><item><title>match_only_text 字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/match_only_text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/match_only_text/</guid><description>match_only_text 字段类型 # Introduced 1.10.0
简介 # match_only_text 是一个为全文搜索优化的字段类型，是 text 类型的变体。它通过省略词条位置、词频和规范化信息来减少存储需求,适合对存储成本敏感但仍需要基本全文搜索功能的场景。
主要特点 # 存储优化:
不存储位置信息 不存储词频信息 不存储规范化信息 显著减少索引大小 评分机制:
禁用评分计算 所有匹配文档得分统一为 1.0 查询支持:
支持大多数查询类型 不支持 interval 查询 不支持 span 查询 支持但不优化短语查询 使用场景 # 适合用于:
需要快速查找包含特定词条的文档 对存储成本敏感的大数据集 不需要复杂相关性排序的场景 不适合用于:
需要基于相关性排序的查询 依赖词条位置或顺序的查询 需要精确短语匹配的场景 映射示例 # PUT my_index { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;title&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;match_only_text&amp;#34; } } } } 参数配置 # 参数 说明 默认值 analyzer 分析器设置 standard boost 评分提升因子 1.</description></item><item><title>Nested 字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/object-field-type/nested/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/object-field-type/nested/</guid><description>Nested 字段类型 # Nested 字段类型是一种特殊的 对象字段类型。
任何对象字段都可以包含一个对象数组。数组中的每个对象都会被动态映射为对象字段类型并以扁平化形式存储。这意味着数组中的对象会被分解成单独的字段，每个字段在所有对象中的值会被存储在一起。有时需要使用 Nested 嵌套类型来将嵌套对象作为一个整体保存，以便您可以对其关联性执行搜索。
扁平化形式 # 默认情况下，每个嵌套对象都被动态映射为对象字段类型。任何对象字段都可以包含一个对象数组。
PUT testindex1/_doc/100 { &amp;#34;patients&amp;#34;: [ {&amp;#34;name&amp;#34;: &amp;#34;John Doe&amp;#34;, &amp;#34;age&amp;#34;: 56, &amp;#34;smoker&amp;#34;: true}, {&amp;#34;name&amp;#34;: &amp;#34;Mary Major&amp;#34;, &amp;#34;age&amp;#34;: 85, &amp;#34;smoker&amp;#34;: false} ] } 当这些对象被存储时，它们会被扁平化，因此它们的内部表示形式具有每个字段的所有值的数组：
{ &amp;#34;patients.name&amp;#34;: [&amp;#34;John Doe&amp;#34;, &amp;#34;Mary Major&amp;#34;], &amp;#34;patients.age&amp;#34;: [56, 85], &amp;#34;patients.smoker&amp;#34;: [true, false] } 一些查询会在这种表示形式中正确工作。如果您搜索年龄大于 75 或者 吸烟的病人 &amp;quot;patients.smoker&amp;quot;: true，文档 id 100 应该匹配。
GET testindex1/_search { &amp;#34;query&amp;#34;: { &amp;#34;bool&amp;#34;: { &amp;#34;should&amp;#34;: [ { &amp;#34;term&amp;#34;: { &amp;#34;patients.</description></item><item><title>Object 对象字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/object-field-type/object/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/object-field-type/object/</guid><description>Object 对象字段类型 # 对象字段类型包含一个 JSON 对象（一组名称/值对）。JSON 对象中的值可以是另一个 JSON 对象。在映射对象字段时不需要指定 object 作为类型，因为 object 是默认类型。
代码示例 # 创建一个带有对象字段的映射：
PUT testindex1/_mappings { &amp;#34;properties&amp;#34;: { &amp;#34;patient&amp;#34;: { &amp;#34;properties&amp;#34; : { &amp;#34;name&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;text&amp;#34; }, &amp;#34;id&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;keyword&amp;#34; } } } } } 索引一个包含对象字段的文档：
PUT testindex1/_doc/1 { &amp;#34;patient&amp;#34;: { &amp;#34;name&amp;#34; : &amp;#34;John Doe&amp;#34;, &amp;#34;id&amp;#34; : &amp;#34;123456&amp;#34; } } 嵌套对象在内部存储为扁平的键/值对。要引用嵌套对象中的字段，使用 parent field.child field（例如，patient.id）。
搜索 ID 为 123456 的患者信息：</description></item><item><title>Percolator 过滤器字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/percolator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/percolator/</guid><description>Percolator 过滤器字段类型 # Percolator 字段类型将该字段视为查询处理。任何 JSON 对象字段都可以标记为 Percolator 字段。通常，文档被索引并用于搜索，而 Percolator 字段存储搜索条件，稍后通过 Percolate 查询将匹配文档到该条件。
参考代码 # 客户正在搜索价格在 400 美元或以下的桌子，并希望为此搜索创建警报。 创建一个映射，为查询字段分配一个 percolator 字段类型：
PUT testindex1 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;search&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;query&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot; } } }, &amp;quot;price&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;float&amp;quot; }, &amp;quot;item&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; } } } } 索引一个查询
PUT testindex1/_doc/1 { &amp;quot;search&amp;quot;: { &amp;quot;query&amp;quot;: { &amp;quot;bool&amp;quot;: { &amp;quot;filter&amp;quot;: [ { &amp;quot;match&amp;quot;: { &amp;quot;item&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;table&amp;quot; } } }, { &amp;quot;range&amp;quot;: { &amp;quot;price&amp;quot;: { &amp;quot;lte&amp;quot;: 400.</description></item><item><title>Search-as-you-type 字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/autocomplete-field-type/search-as-you-type/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/autocomplete-field-type/search-as-you-type/</guid><description>Search-as-you-type 字段类型 # search-as-you-type 字段类型通过前缀和中缀补全提供边输入边搜索的功能。
代码样例 # 将字段映射为 search-as-you-type 类型时，会为该字段创建 n-gram 子字段，其中 n 的范围为 [2, max_shingle_size]。此外，还会创建一个索引前缀子字段。
创建一个 search-as-you-type 的映射字段
PUT books { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;suggestions&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;search_as_you_type&amp;quot; } } } } 除了创建 suggestions 字段外，还会生成 suggestions._2gram、suggestions._3gram 和 suggestions._index_prefix 字段。
以下是使用 search-as-you-type 字段索引文档的示例：
PUT books/_doc/1 { &amp;quot;suggestions&amp;quot;: &amp;quot;one two three four&amp;quot; } 要匹配任意顺序的词项，可以使用 bool_prefix 或 multi-match 查询。
这些查询会将搜索词项按顺序匹配的文档排名提高，而将词项顺序不一致的文档排名降低。
GET books/_search { &amp;quot;query&amp;quot;: { &amp;quot;multi_match&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;tw one&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;bool_prefix&amp;quot;, &amp;quot;fields&amp;quot;: [ &amp;quot;suggestions&amp;quot;, &amp;quot;suggestions.</description></item><item><title>二进制字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/binary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/binary/</guid><description>二进制字段类型 # 二进制字段类型包含以 Base64 编码存储的二进制值，这些值不可被搜索。
参考代码 # 创建包含二进制字段的映射
PUT testindex { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;binary_value&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;binary&amp;quot; } } } } 索引一个二进制值的文档
PUT testindex/_doc/1 { &amp;quot;binary_value&amp;quot; : &amp;quot;bGlkaHQtd29rfx4=&amp;quot; } 使用 = 作为填充字符。不允许嵌入换行符。
参数说明 # 以下参数均为可选参数
doc_values：布尔值，指定字段是否应存储在磁盘上，以便用于聚合、排序或 script 操作。可选，默认为 false。 store：布尔值，指定字段值是否应存储，并可从 _source 字段中单独检索。可选，默认为 false。</description></item><item><title>元数据属性</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/meta/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/meta/</guid><description>Meta 元数据属性 # _meta 字段是一个映射属性，允许您为索引映射附加自定义元数据。您的应用程序可以使用这些元数据来存储与您的用例相关的信息，如版本控制、所有权、分类或审计。
用法 # 您可以在创建新索引或更新现有索引的映射时定义 _meta 字段，如以下示例所示：
PUT my-index { &amp;quot;mappings&amp;quot;: { &amp;quot;_meta&amp;quot;: { &amp;quot;application&amp;quot;: &amp;quot;MyApp&amp;quot;, &amp;quot;version&amp;quot;: &amp;quot;1.2.3&amp;quot;, &amp;quot;author&amp;quot;: &amp;quot;John Doe&amp;quot; }, &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; } } } } 在此示例中，添加了三个自定义元数据字段：application、version 和 author。您的应用程序可以使用这些字段来存储有关索引的任何相关信息，例如它所属的应用程序、应用程序版本或索引的作者。
您可以使用 Put Mapping API 操作更新 _meta 字段，如以下示例所示：
PUT my-index/_mapping { &amp;quot;_meta&amp;quot;: { &amp;quot;application&amp;quot;: &amp;quot;MyApp&amp;quot;, &amp;quot;version&amp;quot;: &amp;quot;1.3.0&amp;quot;, &amp;quot;author&amp;quot;: &amp;quot;Jane Smith&amp;quot; } } 检索元数据信息 # 您可以使用 Get Mapping API 操作检索索引的 _meta 信息，如以下示例所示：</description></item><item><title>分桶聚合</title><link>/easysearch/v1.14.0/docs/references/aggregation/bucket-agg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/aggregation/bucket-agg/</guid><description>分桶聚合 # Bucket aggregations 将文件集归类为桶。桶聚合的类型决定了一个给定的文档是否落入一个桶中
你可以使用桶聚合来实现分面导航（通常作为侧边栏放在搜索结果页面），以帮助你的用户缩小搜索结果范围。
terms # terms 聚合为字段的每个唯一 term 动态地创建一个桶。
下面的例子使用 terms 聚合来查找网络日志数据中每个响应码的文档数量:
GET kibana_sample_data_logs/_search { &amp;#34;size&amp;#34;: 0, &amp;#34;aggs&amp;#34;: { &amp;#34;response_codes&amp;#34;: { &amp;#34;terms&amp;#34;: { &amp;#34;field&amp;#34;: &amp;#34;response.keyword&amp;#34;, &amp;#34;size&amp;#34;: 10 } } } } Sample Response # ... &amp;#34;aggregations&amp;#34; : { &amp;#34;response_codes&amp;#34; : { &amp;#34;doc_count_error_upper_bound&amp;#34; : 0, &amp;#34;sum_other_doc_count&amp;#34; : 0, &amp;#34;buckets&amp;#34; : [ { &amp;#34;key&amp;#34; : &amp;#34;200&amp;#34;, &amp;#34;doc_count&amp;#34; : 12832 }, { &amp;#34;key&amp;#34; : &amp;#34;404&amp;#34;, &amp;#34;doc_count&amp;#34; : 801 }, { &amp;#34;key&amp;#34; : &amp;#34;503&amp;#34;, &amp;#34;doc_count&amp;#34; : 441 } ] } } .</description></item><item><title>分词器参数</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/analyzer/</guid><description>Analyzer 分词器参数 # 分析器 analyzer 映射参数用于定义在索引和搜索期间应用于文本字段的文本分析过程，即分词器的作用过程。
代码样例 # 以下示例配置定义了一个名为 my_custom_analyzer 的自定义分词器：
PUT my_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;, &amp;quot;my_stop_filter&amp;quot;, &amp;quot;my_stemmer&amp;quot; ] } }, &amp;quot;filter&amp;quot;: { &amp;quot;my_stop_filter&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stop&amp;quot;, &amp;quot;stopwords&amp;quot;: [&amp;quot;the&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;and&amp;quot;, &amp;quot;or&amp;quot;] }, &amp;quot;my_stemmer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;stemmer&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;english&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_text_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_analyzer&amp;quot;, &amp;quot;search_analyzer&amp;quot;: &amp;quot;standard&amp;quot;, &amp;quot;search_quote_analyzer&amp;quot;: &amp;quot;my_custom_analyzer&amp;quot; } } } } 在此示例中，my_custom_analyzer 使用标准分词器，将所有标记转换为小写，应用自定义停用词过滤器，并应用英语词干提取器。</description></item><item><title>别名字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/alias/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/alias/</guid><description>Alias 别名字段类型 # 别名字段类型为现有字段创建另一个名称。您可以在搜索和字段功能的 API 操作中使用别名字段，但存在一些例外情况。要设置别名，必须在 path 参数中指定原始字段名称。
参考代码 # PUT movies { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;year&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;date&amp;quot; }, &amp;quot;release_date&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;alias&amp;quot;, &amp;quot;path&amp;quot; : &amp;quot;year&amp;quot; } } } } 参数说明 # path：指向原始字段的完整路径，包括所有父对象。例如，parent.child.field_name。此参数为必填项。 别名（Alias）字段 # 别名（Alias）字段必须遵循以下规则：
一个别名字段只能引用一个原始字段。 在嵌套对象中，别名必须与原始字段位于相同的嵌套层级。 要更改别名引用的字段，需要更新映射配置。但请注意，之前存储的 Percolator 查询中的别名仍会继续引用原始字段，不会自动更新为新的字段引用。
原始字段 # 别名的原始字段必须遵守以下规则：
原始字段必须在别名字段创建之前定义。 原始字段不能是对象类型，也不能是另一个别名字段。 可以使用别名字段的搜索 API # 您可以在以下搜索 API 的只读操作中使用别名：</description></item><item><title>动态映射参数</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/dynamic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/dynamic/</guid><description>Dynamic 动态映射参数 # dynamic 参数指定是否可以动态地将新检测到的字段添加到映射中。它接受下表中列出的参数。
参数 描述 true 指定可以动态地将新字段添加到映射中。默认值为 true。 false 指定不能动态地将新字段添加到映射中。如果检测到新字段，则不会对其进行索引或搜索，但可以从 _source 字段中检索。 strict 当检测到文档中有新字段时，索引操作失败，抛出异常。 strict_allow_templates 如果新字段匹配映射中预定义的动态模板，则添加新字段。 示例：创建 dynamic 设置为 true 的索引 # 通过以下命令创建一个 dynamic 设置为 true 的索引：
PUT testindex1 { &amp;#34;mappings&amp;#34;: { &amp;#34;dynamic&amp;#34;: true } } 通过以下命令，索引一个包含两个字符串字段的对象字段 patient 的文档：
PUT testindex1/_doc/1 { &amp;#34;patient&amp;#34;: { &amp;#34;name&amp;#34; : &amp;#34;John Doe&amp;#34;, &amp;#34;id&amp;#34; : &amp;#34;123456&amp;#34; } } 通过以下命令确认映射按预期工作：</description></item><item><title>向量字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/knn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/knn/</guid><description>k-NN 向量字段类型 # 关于向量 # 在索引文档和运行查询时都需要指定向量类型。在这两种情况下，您都使用相同的 JSON 结构来定义向量类型。每个向量类型还有一个简写形式，这在使用不支持嵌套文档的工具时会很方便。以下示例展示了如何在索引向量时指定它们。
knn_dense_float_vector 密集向量类型 # 假设您已经定义了一个映射，其中 my_vec 的类型为 knn_dense_float_vector。
POST /my-index/_doc { &amp;#34;my_vec&amp;#34;: { &amp;#34;values&amp;#34;: [0.1, 0.2, 0.3, ...] # 1 } } POST /my-index/_doc { &amp;#34;my_vec&amp;#34;: [0.1, 0.2, 0.3, ...] # 2 } 说明 # 1 向量中所有浮点值的 JSON 列表。长度应与映射中的dims匹配。 2 #1 的简写形式。
knn_sparse_bool_vector 稀疏向量类型 # 假设您已经定义了一个映射，其中 my_vec 的类型为 knn_sparse_bool_vector。
POST /my-index/_doc { &amp;#34;my_vec&amp;#34;: { &amp;#34;true_indices&amp;#34;: [1, 3, 5, .</description></item><item><title>向量查询</title><link>/easysearch/v1.14.0/docs/references/search/knn_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/search/knn_api/</guid><description>向量查询 # 使用 kNN 检索 API 来进行向量查询。
先决条件 # 要运行 kNN 搜索，必须安装 knn 插件，参考 插件安装 。
创建 Mapping 和 Setting # 在索引向量之前，首先定义一个 Mapping，指定向量数据类型、索引模型和模型的参数。这决定了索引向量支持哪些查询。 并指定 index.knn 为 true ，这是为了启用近似相似度模型。
从1.11.1 版本开始，index.knn 已弃用，创建 knn 索引时，不再配置 index.knn 参数。
请求示例 # PUT /knn-test { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_vec&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;knn_dense_float_vector&amp;quot;, &amp;quot;knn&amp;quot;: { &amp;quot;dims&amp;quot;: 50, &amp;quot;model&amp;quot;: &amp;quot;lsh&amp;quot;, &amp;quot;similarity&amp;quot;: &amp;quot;cosine&amp;quot;, &amp;quot;L&amp;quot;: 99, &amp;quot;k&amp;quot;: 1 } } } } } 参数说明 # my_vec 存储向量的字段名称 knn_dense_float_vector 表示数据类型为密集型浮点向量.</description></item><item><title>字段名称</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/field-names/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/field-names/</guid><description>Field names 字段名称 # _field_names 字段索引包含非空值的字段名称。可以使用 exists 查询来识别指定字段是否具有非空值的文档。
但是，只有当 doc_values 和 norms 都被禁用时，_field_names 才会索引字段名称。如果启用了 doc_values 或 norms 中的任何一个，则 exists 查询仍然可以工作，但不会依赖 _field_names 字段。
映射示例 # PUT testindex { &amp;quot;mappings&amp;quot;: { &amp;quot;_field_names&amp;quot;: { &amp;quot;enabled&amp;quot;: &amp;quot;true&amp;quot; }, &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;doc_values&amp;quot;: false, &amp;quot;norms&amp;quot;: false }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;doc_values&amp;quot;: true, &amp;quot;norms&amp;quot;: false }, &amp;quot;price&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;float&amp;quot;, &amp;quot;doc_values&amp;quot;: false } } } }</description></item><item><title>字段复制参数</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/copy_to/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/copy_to/</guid><description>Copy_to 字段复制参数 # copy_to 参数允许您将多个字段的值复制到单个字段中。如果您经常跨多个字段搜索，此参数会很有用，因为这样可以达到搜索一组字段的效果。
只有字段值被复制，而不是分词器产生的词项。原始的 _source 字段保持不变，并且可以使用 copy_to 参数将相同的值复制到多个字段。但是，字段间不支持递归复制；相反，应该直接使用 copy_to 从源字段复制到多个目标字段。
代码样例 # 以下示例使用 copy_to 参数通过产品的名称和描述进行搜索，并将这些值复制到单个字段中：
PUT my-products-index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;name&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;copy_to&amp;quot;: &amp;quot;product_info&amp;quot; }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;copy_to&amp;quot;: &amp;quot;product_info&amp;quot; }, &amp;quot;product_info&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;price&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;float&amp;quot; } } } } PUT my-products-index/_doc/1 { &amp;quot;name&amp;quot;: &amp;quot;Wireless Headphones&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;High-quality wireless headphones with noise cancellation&amp;quot;, &amp;quot;price&amp;quot;: 99.99 } PUT my-products-index/_doc/2 { &amp;quot;name&amp;quot;: &amp;quot;Bluetooth Speaker&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Portable Bluetooth speaker with long battery life&amp;quot;, &amp;quot;price&amp;quot;: 49.</description></item><item><title>布尔字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/boolean/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/boolean/</guid><description>布尔字段类型 # 布尔字段类型接受 true 或 false 值，也支持字符串形式的 &amp;ldquo;true&amp;rdquo; 或 &amp;ldquo;false&amp;rdquo;。此外，还可以使用空字符串 &amp;quot;&amp;quot; 表示 false 值。
参考代码 # 创建一个由 a,b,c 三个布尔字段组成的 mapping
PUT testindex { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;a&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;boolean&amp;quot; }, &amp;quot;b&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;boolean&amp;quot; }, &amp;quot;c&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;boolean&amp;quot; } } } } 索引由布尔值组成的文档
PUT testindex/_doc/1 { &amp;quot;a&amp;quot; : true, &amp;quot;b&amp;quot; : &amp;quot;true&amp;quot;, &amp;quot;c&amp;quot; : &amp;quot;&amp;quot; } 因此，字段 a 和 b 将被设置为 true，而字段 c 将被设置为 false。</description></item><item><title>布尔查询</title><link>/easysearch/v1.14.0/docs/references/search/bool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/search/bool/</guid><description>布尔查询 # bool 查询允许您将多个搜索查询与布尔逻辑结合起来。您可以在查询之间使用布尔逻辑来缩小或扩大搜索结果。
bool 查询是一个查询组合器，因为它允许您通过组合几个简单的查询来构造高级查询。
在 bool 查询中使用以下子句（子查询）：
条件 说明 must 结果必须与此子句中的查询匹配。如果有多个查询，则每个查询都必须匹配。充当 and 运算符 must_not 结果中排除所有匹配项。充当 not 运算符 should 结果应该但不必与查询匹配。每个匹配的 should 子句都会增加相关性得分。作为选项，您可以要求一个或多个查询与 minimum_should_match 参数的值匹配（默认值为 1） filter 过滤器在应用查询之前减少数据集。筛选器子句中的查询是 yes-no 选项，其中如果文档与查询匹配，则将包含在结果中。筛选查询不会影响结果排序所依据的相关性分数。筛选查询的结果通常会被缓存，因此运行速度更快。使用筛选器查询根据精确匹配项、范围、日期、数字等筛选结果 bool 查询的结构如下:
GET _search { &amp;#34;query&amp;#34;: { &amp;#34;bool&amp;#34;: { &amp;#34;must&amp;#34;: [ {} ], &amp;#34;must_not&amp;#34;: [ {} ], &amp;#34;should&amp;#34;: [ {} ], &amp;#34;filter&amp;#34;: {} } } } 例如，假设您有一个 Easysearch 集群中的莎士比亚全集索引。您希望构造满足以下要求的单个查询：</description></item><item><title>强制类型转换参数</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/coerce/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/coerce/</guid><description>Coerce 强制类型转换参数 # coerce 映射参数控制数据在索引期间如何将其值转换为预期的字段数据类型。此参数让您可以验证数据是否按照预期的字段类型正确格式化和索引。这提高了搜索结果的准确性。
代码样例 # 以下示例演示如何使用 coerce 映射参数。
启用 coerce 去索引文档 # PUT products { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;price&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;integer&amp;quot;, &amp;quot;coerce&amp;quot;: true } } } } PUT products/_doc/1 { &amp;quot;name&amp;quot;: &amp;quot;Product A&amp;quot;, &amp;quot;price&amp;quot;: &amp;quot;19.99&amp;quot; } 在此示例中，price 字段被定义为 integer 类型，且 coerce 设置为 true。在索引文档时，字符串值 19.99 被强制转换为整数 19。
禁用 coerce 的文档索引 # PUT orders { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;quantity&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;integer&amp;quot;, &amp;quot;coerce&amp;quot;: false } } } } PUT orders/_doc/1 { &amp;quot;item&amp;quot;: &amp;quot;Widget&amp;quot;, &amp;quot;quantity&amp;quot;: &amp;quot;10&amp;quot; } 在此示例中，quantity 字段被定义为 integer 类型，且 coerce 设置为 false。在索引文档时，字符串值 10 不会被强制转换，由于类型不匹配，文档写入会被拒绝。</description></item><item><title>快照生命周期管理</title><link>/easysearch/v1.14.0/docs/references/management/slm_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/slm_api/</guid><description>快照生命周期管理 # 使用快照管理（SLM）API 自动创建快照。
创建快照策略 # 请求示例 # 每天上午 8 点自动创建一份快照,快照名称格式为 yyyy-MM-dd-HH:mm ，存储在 my_backup 快照仓库 每天凌晨 1 点自动删除最早 7 天前创建的快照、超过 21 个的快照以及保留至少 7 个快照 快照创建和删除的时间限制均为 1 小时 curl -XPOST -uadmin:admin -H 'Content-Type: application/json' 'https://localhost:9200/_slm/policies/daily-policy' -d ' { &amp;quot;description&amp;quot;: &amp;quot;每日快照策略&amp;quot;, &amp;quot;creation&amp;quot;: { &amp;quot;schedule&amp;quot;: { &amp;quot;cron&amp;quot;: { &amp;quot;expression&amp;quot;: &amp;quot;0 8 * * *&amp;quot;, &amp;quot;timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot; } }, &amp;quot;time_limit&amp;quot;: &amp;quot;1h&amp;quot; }, &amp;quot;deletion&amp;quot;: { &amp;quot;schedule&amp;quot;: { &amp;quot;cron&amp;quot;: { &amp;quot;expression&amp;quot;: &amp;quot;0 1 * * *&amp;quot;, &amp;quot;timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot; } }, &amp;quot;condition&amp;quot;: { &amp;quot;max_age&amp;quot;: &amp;quot;7d&amp;quot;, &amp;quot;max_count&amp;quot;: 21, &amp;quot;min_count&amp;quot;: 7 }, &amp;quot;time_limit&amp;quot;: &amp;quot;1h&amp;quot; }, &amp;quot;snapshot_config&amp;quot;: { &amp;quot;date_format&amp;quot;: &amp;quot;yyyy-MM-dd-HH:mm&amp;quot;, &amp;quot;date_format_timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot;, &amp;quot;indices&amp;quot;: &amp;quot;*&amp;quot;, &amp;quot;repository&amp;quot;: &amp;quot;my_backup&amp;quot;, &amp;quot;ignore_unavailable&amp;quot;: &amp;quot;true&amp;quot;, &amp;quot;include_global_state&amp;quot;: &amp;quot;false&amp;quot;, &amp;quot;partial&amp;quot;: &amp;quot;true&amp;quot;, &amp;quot;metadata&amp;quot;: { &amp;quot;any_key&amp;quot;: &amp;quot;any_value&amp;quot; } } }' 示例响应 # { &amp;quot;_id&amp;quot;: &amp;quot;daily-policy-sm-policy&amp;quot;, &amp;quot;_version&amp;quot;: 1, &amp;quot;_seq_no&amp;quot;: 0, &amp;quot;_primary_term&amp;quot;: 1, &amp;quot;sm_policy&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;daily-policy&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;每日快照策略&amp;quot;, &amp;quot;schema_version&amp;quot;: 17, &amp;quot;creation&amp;quot;: { &amp;quot;schedule&amp;quot;: { &amp;quot;cron&amp;quot;: { &amp;quot;expression&amp;quot;: &amp;quot;0 8 * * *&amp;quot;, &amp;quot;timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot; } }, &amp;quot;time_limit&amp;quot;: &amp;quot;1h&amp;quot; }, &amp;quot;deletion&amp;quot;: { &amp;quot;schedule&amp;quot;: { &amp;quot;cron&amp;quot;: { &amp;quot;expression&amp;quot;: &amp;quot;0 1 * * *&amp;quot;, &amp;quot;timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot; } }, &amp;quot;condition&amp;quot;: { &amp;quot;max_age&amp;quot;: &amp;quot;7d&amp;quot;, &amp;quot;min_count&amp;quot;: 7, &amp;quot;max_count&amp;quot;: 21 }, &amp;quot;time_limit&amp;quot;: &amp;quot;1h&amp;quot; }, &amp;quot;snapshot_config&amp;quot;: { &amp;quot;indices&amp;quot;: &amp;quot;*&amp;quot;, &amp;quot;metadata&amp;quot;: { &amp;quot;any_key&amp;quot;: &amp;quot;any_value&amp;quot; }, &amp;quot;ignore_unavailable&amp;quot;: &amp;quot;true&amp;quot;, &amp;quot;date_format_timezone&amp;quot;: &amp;quot;Asia/Shanghai&amp;quot;, &amp;quot;include_global_state&amp;quot;: &amp;quot;false&amp;quot;, &amp;quot;date_format&amp;quot;: &amp;quot;yyyy-MM-dd-HH:mm&amp;quot;, &amp;quot;repository&amp;quot;: &amp;quot;my_backup&amp;quot;, &amp;quot;partial&amp;quot;: &amp;quot;true&amp;quot; }, &amp;quot;schedule&amp;quot;: { &amp;quot;interval&amp;quot;: { &amp;quot;start_time&amp;quot;: 1685348095913, &amp;quot;period&amp;quot;: 1, &amp;quot;unit&amp;quot;: &amp;quot;Minutes&amp;quot; } }, &amp;quot;enabled&amp;quot;: true, &amp;quot;last_updated_time&amp;quot;: 1685348095938, &amp;quot;enabled_time&amp;quot;: 1685348095909 } } 获取策略 # 获取所有 SLM 策略</description></item><item><title>忽略属性</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/ignored/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/ignored/</guid><description>Ignored 忽略属性 # _ignored 字段帮助您管理文档中与格式错误数据相关的问题。由于在 索引映射中启用了 ignore_malformed 设置，此字段用于存储在数据索引过程中被忽略的字段名称。
_ignored 字段允许您搜索和识别包含被忽略字段的文档，以及被忽略的具体字段名称。这对于故障排除很有用。
您可以使用 term、terms 和 exists 查询来查询 _ignored 字段。
只有当索引映射中启用了 ignore_malformed 设置时，才会填充 _ignored 字段。如果 ignore_malformed 设置为 false（默认值），则格式错误的字段将导致整个文档被拒绝，并且不会填写 _ignored 字段。
以下示例展示了如何使用 _ignored 字段：
GET _search { &amp;quot;query&amp;quot;: { &amp;quot;exists&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;_ignored&amp;quot; } } } 使用 _ignored 字段的索引请求示例 # 以下示例向 test-ignored 索引添加一个新文档，其中 ignore_malformed 设置为 true，这样在数据索引时不会抛出错误：
PUT test-ignored { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;length&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;long&amp;quot;, &amp;quot;ignore_malformed&amp;quot;: true } } } } POST test-ignored/_doc { &amp;quot;title&amp;quot;: &amp;quot;correct text&amp;quot;, &amp;quot;length&amp;quot;: &amp;quot;not a number&amp;quot; } GET test-ignored/_search { &amp;quot;query&amp;quot;: { &amp;quot;exists&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;_ignored&amp;quot; } } } 示例返回内容 # { &amp;quot;took&amp;quot;: 42, &amp;quot;timed_out&amp;quot;: false, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 1, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;total&amp;quot;: { &amp;quot;value&amp;quot;: 1, &amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot; }, &amp;quot;max_score&amp;quot;: 1, &amp;quot;hits&amp;quot;: [ { &amp;quot;_index&amp;quot;: &amp;quot;test-ignored&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;qcf0wZABpEYH7Rw9OT7F&amp;quot;, &amp;quot;_score&amp;quot;: 1, &amp;quot;_ignored&amp;quot;: [ &amp;quot;length&amp;quot; ], &amp;quot;_source&amp;quot;: { &amp;quot;title&amp;quot;: &amp;quot;correct text&amp;quot;, &amp;quot;length&amp;quot;: &amp;quot;not a number&amp;quot; } } ] } } 忽略指定字段 # 您可以使用 term 查询来查找特定字段被忽略的文档，如以下示例请求所示：</description></item><item><title>数值字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/numeric-field/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/numeric-field/</guid><description>Numeric 字段类型 # 下表列出了 Easysearch 支持的所有数字字段类型。
字段数据类型 描述 byte 有符号的 8 位整数。最小值为 -128，最大值为 127。 double 双精度 64 位 IEEE 754 浮点数。最小值为 2^−1074，最大值为 (2 − 2^−52) · 2^1023。有效位数为 53，有效数字位为 15.95。 float 单精度 32 位 IEEE 754 浮点数。最小值为 2^−149，最大值为 (2 − 2^−23) · 2^127。有效位数为 24，有效数字位为 7.22。 half_float 半精度 16 位 IEEE 754 浮点数。最小值为 2^−24，最大值为 65504。有效位数为 11，有效数字位为 3.31。 integer 有符号的 32 位整数。最小值为 -2^31，最大值为 2^31 - 1。 long 有符号的 64 位整数。最小值为 -2^63，最大值为 2^63 - 1。 unsigned_long 无符号的 64 位整数。最小值为 0，最大值为 2^64 - 1。 short 有符号的 16 位整数。最小值为 -2^15，最大值为 2^15 - 1。 scaled_float 一个浮点值，它会被乘以双精度缩放因子并存储为长整型值。 Integer、long、float 和 double 字段类型都有对应的 范围字段类型。</description></item><item><title>文档列值参数</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/doc_values/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/doc_values/</guid><description>doc_values 文档值参数 # 默认情况下，Easysearch 会为搜索目的索引大多数字段的字段值。doc_values 参数启用文档到词项的正排查找，用于排序、聚合和脚本等操作。
doc_values 参数接受以下选项。
选项 Option 描述 true true 启用字段的 doc_values。默认值为 true。 false false 禁用字段的 doc_values。 示例：创建启用和禁用 doc_values 的索引 # 以下示例请求创建一个索引，其中一个字段启用 doc_values，另一个字段禁用：
PUT my-index-001 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;status_code&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;session_id&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;, &amp;quot;doc_values&amp;quot;: false } } } }</description></item><item><title>是否启用参数</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/enabled/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/enabled/</guid><description>Enabled 启用参数 # enabled 参数允许您控制 Easysearch 是否解析字段的内容。此参数可以应用于顶级映射定义和对象字段。
enabled 参数接受以下值：
参数 描述 true 字段被解析和索引。默认值为 true。 false 字段不被解析或索引，但仍可从 _source 字段中检索。当 enabled 设置为 false 时，Easysearch 将字段的值存储在 _source 字段中，但不索引或解析其内容。这对于您想要存储但不需要搜索、排序或聚合的字段很有用。 示例：使用 enabled 参数 # 在以下示例请求中，session_data 字段被禁用。Easysearch 将其内容存储在 _source 字段中，但不对其进行索引或解析：
PUT my-index-002 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;user_id&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; }, &amp;quot;last_updated&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;date&amp;quot; }, &amp;quot;session_data&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;enabled&amp;quot;: false } } } }</description></item><item><title>权重参数设置</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/boost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/mapping-parameters/boost/</guid><description>Boost 权重参数 # boost 映射参数用于在搜索查询期间增加或减少字段的相关性分数。它允许您在计算文档的整体相关性分数时，对特定字段应用更多或更少的权重。
boost 参数作为字段分数的乘数应用。例如，如果一个字段的 boost 值为 2，则该字段的分数的权重将翻倍。相反，boost 值为 0.5 将使该字段的分数的权重减半。
代码样例 # 以下是在 Easysearch 映射中使用 boost 参数的示例：
PUT my-index1 { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;boost&amp;quot;: 2 }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;boost&amp;quot;: 1 }, &amp;quot;tags&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;, &amp;quot;boost&amp;quot;: 1.5 } } } } 在此示例中，title 字段的提升值为 2，这意味着它对整体相关性分数的权重是描述字段（提升值为 1）的两倍。tags 字段的提升值为 1.5，因此它的权重是描述字段的一倍半。
当您想要对某些字段赋予更多权重时，boost 参数特别有用。例如，您可能想要将 title 字段的权重提升得比 description 字段更高，因为标题更能文档的相关性。
boost 参数是一个乘法因子，而不是加法因子。这意味着与具有较低权重值的字段相比，具有较高权重值的字段对整体相关性分数的影响将不成比例地大。在使用 boost 参数时，建议您从小值（1.5 或 2）开始，并测试其对搜索结果的影响。过高的权重值可能会扭曲相关性分数，并导致意外或不理想的搜索结果。</description></item><item><title>源文档属性</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/source/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/source/</guid><description>Source 源文档属性 # _source 字段包含已索引的原始 JSON 文档主体。虽然此字段不可搜索，但它会被存储，以便在执行获取请求（如 get 和 search）时可以返回完整文档。
禁用_source # 您可以通过将 enabled 参数设置为 false 来禁用 _source 字段，如以下示例所示：
PUT sample-index1 { &amp;quot;mappings&amp;quot;: { &amp;quot;_source&amp;quot;: { &amp;quot;enabled&amp;quot;: false } } } 禁用 _source 字段可能会影响某些功能的可用性，例如 update、update_by_query 和 reindex API，以及使用原始索引文档查询或聚合的能力。
包含或排除某些字段 # 您可以使用 includes 和 excludes 参数选择 _source 字段的内容。如以下示例：
PUT logs { &amp;quot;mappings&amp;quot;: { &amp;quot;_source&amp;quot;: { &amp;quot;includes&amp;quot;: [ &amp;quot;*.count&amp;quot;, &amp;quot;meta.*&amp;quot; ], &amp;quot;excludes&amp;quot;: [ &amp;quot;meta.description&amp;quot;, &amp;quot;meta.other.*&amp;quot; ] } } } 这些字段不会存储在 _source 中，但您仍然可以搜索它们，因为数据仍然被索引。</description></item><item><title>索引属性</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/</guid><description>Index 索引属性 # 当跨多个索引进行查询时，您可能需要根据文档所在的索引来过滤结果。index 字段根据文档的索引来匹配文档。
以下示例创建两个索引，products 和 customers，并向每个索引添加一个文档：
PUT products/_doc/1 { &amp;quot;name&amp;quot;: &amp;quot;Widget X&amp;quot; } PUT customers/_doc/2 { &amp;quot;name&amp;quot;: &amp;quot;John Doe&amp;quot; } 然后，您可以查询这两个索引，并使用 _index 属性过滤结果，如以下示例请求所示：
GET products,customers/_search { &amp;quot;query&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;_index&amp;quot;: [&amp;quot;products&amp;quot;, &amp;quot;customers&amp;quot;] } }, &amp;quot;aggs&amp;quot;: { &amp;quot;index_groups&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;field&amp;quot;: &amp;quot;_index&amp;quot;, &amp;quot;size&amp;quot;: 10 } } }, &amp;quot;sort&amp;quot;: [ { &amp;quot;_index&amp;quot;: { &amp;quot;order&amp;quot;: &amp;quot;desc&amp;quot; } } ], &amp;quot;script_fields&amp;quot;: { &amp;quot;index_name&amp;quot;: { &amp;quot;script&amp;quot;: { &amp;quot;lang&amp;quot;: &amp;quot;painless&amp;quot;, &amp;quot;source&amp;quot;: &amp;quot;doc['_index'].</description></item><item><title>索引生命周期管理</title><link>/easysearch/v1.14.0/docs/references/management/ilm_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/ilm_api/</guid><description>索引生命周期管理 # 使用索引状态管理操作，以编程方式处理策略和托管索引。
创建策略 # 引入版本 1.0
创建一个策略。
请求示例 # PUT _ilm/policy/ilm_test { &amp;quot;policy&amp;quot;: { &amp;quot;phases&amp;quot;: { &amp;quot;hot&amp;quot;: { &amp;quot;min_age&amp;quot;: &amp;quot;0ms&amp;quot;, &amp;quot;actions&amp;quot;: { &amp;quot;rollover&amp;quot;: { &amp;quot;max_age&amp;quot;: &amp;quot;10m&amp;quot;, &amp;quot;max_size&amp;quot;: &amp;quot;1mb&amp;quot;, &amp;quot;max_docs&amp;quot;: 100 }, &amp;quot;set_priority&amp;quot;: { &amp;quot;priority&amp;quot;: 100 } } }, &amp;quot;delete&amp;quot;: { &amp;quot;min_age&amp;quot;: &amp;quot;15m&amp;quot;, &amp;quot;actions&amp;quot;: { &amp;quot;delete&amp;quot;: { } } } } } } 示例响应 # { &amp;quot;acknowledged&amp;quot;: true } 应用生命周期策略到索引模板 要让策略生效，需要在索引模板中指定策略名称和滚动索引的别名。
请求示例 # PUT /_index_template/my_template?</description></item><item><title>自动补全字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/autocomplete-field-type/completion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/autocomplete-field-type/completion/</guid><description>Completion 自动补全字段类型 # 自动补全字段类型通过补全建议器提供自动补全功能。补全建议器是一个前缀建议器，所以它只匹配文本的开头部分。补全建议器会创建一个内存中的数据结构，这提供了更快的查找速度，但会导致内存使用增加。在使用此功能之前，你需要将所有可能的补全项上传到索引中。
代码样例 # 创建一个包含补全字段的映射：
PUT chess_store { &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;suggestions&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;completion&amp;#34; }, &amp;#34;product&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;keyword&amp;#34; } } } } 将建议内容索引到 Easysearch 中：
PUT chess_store/_doc/1 { &amp;#34;suggestions&amp;#34;: { &amp;#34;input&amp;#34;: [&amp;#34;Books on openings&amp;#34;, &amp;#34;Books on endgames&amp;#34;], &amp;#34;weight&amp;#34; : 10 } } 参数 # 下表列出了补全字段接受的参数。
参数 描述 input 可能的补全项列表，可以是字符串或字符串数组。不能包含 \u0000 (null),\u001f (信息分隔符一) 或 \u001e (信息分隔符二)。必需。 weight 用于对建议进行排序的正整数或正整数字符串。可选。 可以按以下方式索引多个建议：</description></item><item><title>范围字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/range-field-type/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/range-field-type/</guid><description>范围（Range）字段类型 # 以下表格列出了 Easysearch 支持的所有范围字段类型。
字段数据类型 描述 integer_range 整数值范围。 long_range 长整型值范围。 double_range 双精度浮点值范围。 float_range 浮点值范围。 ip_range IPv4 或 IPv6 地址范围，起始和结束地址可使用不同格式。 date_range 日期值范围，起始和结束日期可采用不同格式。内部以 64 位无符号整数存储，自纪元以来的毫秒数表示。 参考代码 # 创建一个有双精度浮点数范围字段和日期范围字段的映射
PUT testindex { &amp;quot;mappings&amp;quot; : { &amp;quot;properties&amp;quot; : { &amp;quot;gpa&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;double_range&amp;quot; }, &amp;quot;graduation_date&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;date_range&amp;quot;, &amp;quot;format&amp;quot; : &amp;quot;strict_year_month||strict_year_month_day&amp;quot; } } } } 索引一个包含这两个字段的文档</description></item><item><title>资源扩容</title><link>/easysearch/v1.14.0/docs/getting-started/install/operator/resource_manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/operator/resource_manager/</guid><description>资源扩容 # 查看当前 cpu、mem 和磁盘资源情况
kubectl get sts/threenodes-masters -o yaml resources: requests: cpu: &amp;#34;1&amp;#34; memory: 3Gi limits: cpu: &amp;#34;1&amp;#34; memory: 5Gi resources: requests: storage: 30Gi volumeMode: Filesystem 磁盘（磁盘扩容依赖于实际的 StorageClass，需要 StorageClass 本身支持扩容）
修改 Operator yaml 文件，执行 apply 操作
resources: requests: cpu: &amp;#34;1&amp;#34; memory: 4Gi limits: cpu: &amp;#34;2&amp;#34; memory: 6Gi resources: requests: storage: 50Gi 滚动更新中： 从 threenodes-masters-0 开始更新
threenodes-masters-0 更新完毕后，依次更新 threenodes-masters-1、threenodes-masters-2
最终全部更新完毕
查看更新后的资源情况：
可以发现，结果与预期的一致</description></item><item><title>跨集群复制</title><link>/easysearch/v1.14.0/docs/references/management/ccr_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/ccr_api/</guid><description>跨集群复制 # 使用跨集群复制 API 管理跨集群复制。
在跨集群复制中，可以将数据索引到一个领导者索引，然后 Easysearch 将这些数据复制到一个或多个只读的跟随者索引。所有在领导者上进行的后续操作都会在跟随者上复制，例如创建、更新或删除文档。
先决条件 # 1.11.1 版本之前，leader 和 follower 集群都必须安装 cross-cluster-replication 插件和 index-management 插件，1.11.1 版本开始，已经内置了 CCR 模块。 如果 follower 集群的 easysearch.yml 文件中覆盖了 node.roles，确保它也包括 remote_cluster_client 角色，默认启用。 node.roles: [&amp;lt;other_roles&amp;gt;, remote_cluster_client] 权限 # 确保安全功能在两个集群上都启用或都禁用。如果启用了安全功能，确保非管理员用户被映射到适当的权限，以便他们可以执行复制操作。 部署示例集群 # 在本地起 2 个单节点的 easysearch 测试集群，分别是 follower-application (9201 端口) 和 leader-application (9200 端口) 在 easysearch.yml 添加 discovery.type: single-node 如果启用 security 功能，确保 2 个集群的证书互信，测试环境可以直接合并 2 个节点的 ca 证书： 例如 cat ca.</description></item><item><title>路由属性</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/routing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/metadata-field/routing/</guid><description>Routing 路由属性 # Easysearch 使用哈希算法将文档路由到索引中的特定分片。默认情况下，文档的 _id 字段用作路由值，但您也可以为每个文档指定自定义路由值。
默认路由 # 以下是 Easysearch 的默认路由公式。_routing 值是文档的 _id。
shard_num = hash(_routing) % num_primary_shards 自定义路由 # 您可以在索引文档时指定自定义路由值，如以下示例所示：
PUT sample-index1/_doc/1?routing=JohnDoe1 { &amp;quot;title&amp;quot;: &amp;quot;This is a document&amp;quot; } 在此示例中，文档使用的路由值是 JohnDoe1 而不是默认的 _id 。
在检索、删除或更新文档时，您必须提供相同的路由值，如以下示例所示：
GET sample-index1/_doc/1?routing=JohnDoe1 通过路由查询 # 您可以使用 _routing 字段根据文档的路由值进行查询，如以下示例所示。此查询仅搜索与 JohnDoe1 路由值关联的分片：
GET sample-index1/_search { &amp;quot;query&amp;quot;: { &amp;quot;terms&amp;quot;: { &amp;quot;_routing&amp;quot;: [ &amp;quot;JohnDoe1&amp;quot; ] } } } 设置路由为必需项 # 您可以使索引上的所有 CRUD 操作都必需提供路由值，如以下示例。如果您尝试在不提供路由值的情况下索引文档，Easysearch 将抛出异常。</description></item><item><title>Wildcard 字段类型</title><link>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/wildcard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/mappings-and-field-types/field-types/wildcard/</guid><description>Wildcard 字段类型 # wildcard（通配符）字段是keyword（关键字）字段的一种变体，专为任意子字符串和正则表达式匹配而设计。
当您的内容由&amp;quot;字符串&amp;quot;而非&amp;quot;文本&amp;quot;组成时，应使用wildcard字段。示例包括非结构化日志行和计算机代码。
wildcard字段类型的索引方式与keyword字段类型不同。keyword字段将原始字段值写入索引，而wildcard字段类型则将字段值拆分为长度小于或等于3的子字符串，并将这些子字符串写入索引。例如，字符串test被拆分为t、te、tes、e、es和est这些子字符串。
在搜索时，将查询模式中所需的子字符串与索引进行匹配以生成候选文档，然后根据查询中的模式对这些文档进行过滤。例如，对于搜索词test，OpenSearch执行索引搜索tes AND est。如果搜索词包含少于三个字符，OpenSearch会使用长度为一或二的字符子字符串。对于每个匹配的文档，如果源值为test，则该文档将出现在结果中。这样可以排除误报值，如nikola tesla felt alternating current was best。
通常，精确匹配查询（如 term或 terms查询）在wildcard字段上的表现不如在keyword字段上有效， 而 wildcard、 prefix和 regexp查询在wildcard字段上表现更好。
示例 # 创建带有 wildcard 字段的映射：
PUT logs { &amp;#34;mappings&amp;#34; : { &amp;#34;properties&amp;#34; : { &amp;#34;log_line&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;wildcard&amp;#34; } } } } 参数 # 以下表格列出了 wildcard 字段可用的所有参数。 `
参数 描述 doc_values 布尔值，指定该字段是否应存储在磁盘上，以便用于聚合、排序或脚本操作。默认值为 false。 ignore_above 长度超过此整数值的任何字符串都不会被索引。默认值为 2147483647。 normalizer 用于预处理索引和搜索值的标准化器。默认情况下，不进行标准化，使用原始值。您可以使用 lowercase 标准化器在该字段上执行不区分大小写的匹配。 null_value 用于替代 null 的值。必须与字段类型相同。如果未指定此参数，则当字段值为 null 时，该字段将被视为缺失。默认值为 null。</description></item><item><title>查询模版</title><link>/easysearch/v1.14.0/docs/references/search/search-template/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/search/search-template/</guid><description>查询模版 # 您可以将全文查询转换为查询模版，以接受用户输入并将其动态插入到查询中。
例如，如果您使用 Easysearch 作为应用程序或网站的后端搜索引擎，则可以从搜索栏或表单字段接收用户查询，并将其作为参数传递到查询模版中。这样，创建 Easysearch 查询的语法就从最终用户那里抽象出来了。
当您编写代码将用户输入转换为 Easysearch 查询时，可以使用查询模版简化代码。如果需要将字段添加到搜索查询中，只需修改模板即可，而无需更改代码。
查询模版使用 Mustache 语言。有关所有语法选项的列表，请参阅 Mustache 手册。
创建查询模版 # 查询模版有两个组件：查询和参数。参数是放置在变量中的用户输入值。在 Mustache 符号中，变量用双括号表示。当在查询中遇到类似 {% raw %}{{var}}{% endraw %} 的变量时，Easysearch 会转到 params 部分，查找名为 var 的参数，并用指定的值替换它。
您可以编写应用程序代码，询问用户要搜索什么，然后在运行时将该值插入 params 对象中。
此命令定义了一个查询模版，用于按名称查找播放。查询中的 {% raw %}{{play_name}}{% endraw %} 被值 Henry IV 替换：
GET _search/template { &amp;#34;source&amp;#34;: { &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;play_name&amp;#34;: &amp;#34;{% raw %}{{play_name}}{% endraw %}&amp;#34; } } }, &amp;#34;params&amp;#34;: { &amp;#34;play_name&amp;#34;: &amp;#34;Henry IV&amp;#34; } } 此模板在整个集群上运行搜索。</description></item><item><title>异步搜索</title><link>/easysearch/v1.14.0/docs/references/search/async_search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/search/async_search/</guid><description>异步搜索 # 搜索大量数据可能会花费很长时间，尤其是当你在热节点或者多个远程集群中进行搜索时。
Easysearch 中的异步搜索允许你发送在后台运行的搜索请求。你可以监控这些搜索的进度，并且在部分结果可用时获取这些部分结果。在搜索完成之后，你可以保存结果以便日后查看。
先决条件 # Easysearch 从 1.11.1 版本开始，内置支持异步搜索。
REST API # 引入版本 1.11.0
要执行异步搜索，请向 /{index}/_async_search 发送请求，并在请求正文中包含您的查询：
POST test-index/_asynch_search 可以指定以下选项。
选项 描述 默认值 是否必填 wait_for_completion_timeout 计划等待结果的时间。在此时间内，您可以像在普通搜索中一样查看所获得的结果。您可以根据ID轮询剩余的结果。最大值为300秒。 1秒 否 keep_on_completion 搜索完成后，您是否希望将结果保存在集群中。您可以在稍后查看存储的结果。 false 否 keep_alive 结果在集群中保存的时间。例如，2d 表示结果在集群中存储48小时。保存的搜索结果在此时间段结束后或如果搜索被取消时将被删除。请注意，这包括查询执行时间。如果查询超过此时间，进程将自动取消该查询。 12小时 否 index 要搜索的索引名称。可以是单个名称、用逗号分隔的索引列表，或索引名称的通配符表达式。 集群中的所有索引 否 请求示例 # POST test-index/_async_search?wait_for_completion_timeout=1ms&amp;amp;keep_on_completion=true { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;张三&amp;quot; } } } 示例响应 # { &amp;quot;id&amp;quot;: &amp;quot;FmFqN0llTXlKVHF5cnV1NGdVNUlPancEMzMzMBRaOUNxU3BVQlRIdzczZmJfNnZtRQIyMA==&amp;quot;, &amp;quot;state&amp;quot;: &amp;quot;RUNNING&amp;quot;, &amp;quot;start_time_in_millis&amp;quot;: 1740714470020, &amp;quot;expiration_time_in_millis&amp;quot;: 1740800870020, &amp;quot;response&amp;quot;: { &amp;quot;took&amp;quot;: 0, &amp;quot;timed_out&amp;quot;: false, &amp;quot;num_reduce_phases&amp;quot;: 0, &amp;quot;_shards&amp;quot;: { &amp;quot;total&amp;quot;: 1, &amp;quot;successful&amp;quot;: 0, &amp;quot;skipped&amp;quot;: 0, &amp;quot;failed&amp;quot;: 0 }, &amp;quot;hits&amp;quot;: { &amp;quot;max_score&amp;quot;: null, &amp;quot;hits&amp;quot;: [] } } } 响应参数 # 选项 描述 id 异步搜索的ID。使用此ID来监控搜索进度、获取其部分结果和/或删除结果。如果异步搜索在超时期限内完成，响应中不包含ID，因为结果未存储在集群中。 state 指定搜索是仍在运行还是已经完成，以及结果是否在集群中持久保存。可能的状态有 RUNNING（运行中）、SUCCEEDED（成功）、FAILED（失败）、PERSISTING（正在持久化）、PERSIST_SUCCEEDED（持久化成功）、PERSIST_FAILED（持久化失败）、CLOSED（已关闭）和 STORE_RESIDENT（存储驻留）。 start_time_in_millis 开始时间，单位为毫秒。 expiration_time_in_millis 过期时间，单位为毫秒。 took 搜索运行的总时长。 response 实际的搜索响应。 num_reduce_phases 协调节点从分片响应批次中聚合结果的次数（默认值为5）。如果与上次检索到的结果相比，此数字增加，您可以预期搜索响应中将包含额外的结果。 total 执行搜索的分片总数。 successful 协调节点成功接收到的分片响应数量。 aggregations 分片到目前为止已完成的聚合部分结果。 获取部分结果 # 提交异步搜索请求后，您可以使用在异步搜索响应中看到的ID请求部分响应。</description></item><item><title>定点查询</title><link>/easysearch/v1.14.0/docs/references/search/pit_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/search/pit_api/</guid><description>定点查询 # 定点查询，也称为 Point in Time 搜索，具有与常规搜索相同的功能，不同之处在于 PIT 搜索作用于较旧的数据集，而常规搜索作用于实时数据集。PIT 搜索不绑定于特定查询，因此您可以在同一个冻结在时间点上的数据集上运行不同的查询。
您可以使用创建 PIT API 来创建 PIT。当您为一组索引创建 PIT 时，Easysearch 会锁定这些索引的一组段，使它们在时间上冻结。在底层，此 PIT 所需的资源不会被修改或删除。 如果作为 PIT 一部分的段被合并，Easysearch 会在 PIT 创建时通过 keep_alive 参数指定的时间段内保留这些段的副本。
创建 PIT 操作会返回一个 PIT ID，您可以使用该 ID 在冻结的数据集上运行多个查询。即使索引继续摄取数据并修改或删除文档，PIT 引用的数据自 PIT 创建以来不会发生变化。当您的查询包含 PIT ID 时， 您不需要将索引传递给搜索，因为它将使用该 PIT。使用 PIT ID 的搜索在多次运行时将产生完全相同的结果。
创建 PIT # 创建一个 PIT。查询参数 keep_alive 是必需的；它指定了保持 PIT 的时间长度。
端点 # POST /&amp;lt;target_indexes&amp;gt;/_pit?keep_alive=1h&amp;amp;routing=&amp;amp;expand_wildcards=&amp;amp;preference= 路径参数 # 参数 数据类型 描述 target_indexes 字符串 PIT 的目标索引名称。可以包含以逗号分隔的列表或通配符索引模式。 查询参数 # 参数 数据类型 描述 keep_alive 时间 保持 PIT 的时间长度。每次使用搜索 API 访问 PIT 时，PIT 的生命周期都会延长一段等于 keep_alive 参数的时间。必需。 preference 字符串 用于执行搜索的节点或分片。可选。默认为随机。 routing 字符串 指定将搜索请求路由到特定分片。可选。默认为文档的 _id。 expand_wildcards 字符串 可匹配通配符模式的索引类型。支持逗号分隔的值。有效值如下：</description></item><item><title>备份还原</title><link>/easysearch/v1.14.0/docs/references/management/snapshot-restore/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/snapshot-restore/</guid><description>备份还原 # 快照是集群索引和状态的备份。状态包括集群设置、节点信息、索引设置和分片的信息。
快照有两个主要用途：
从故障中恢复
例如，如果集群运行状况变为红色，则可以从快照恢复红色索引。
从一个群集迁移到另一个群集
例如，如果您要从概念验证迁移到生产集群，您可以拍摄前者的快照并在后者上进行恢复。
关于快照(snapshots) # 快照不是即时的。它们需要时间来完成，并不代表集群的完美时间点视图。当快照正在进行时，您仍然可以为文档编制索引并向集群发出其他请求，但快照中通常不包括新文档和对现有文档的更新。快照包括 Easysearch 启动快照时存在的主碎片。根据快照线程池的大小，快照中可能会在稍微不同的时间包含不同的碎片。
Easysearch 快照是增量的，这意味着它们只存储自上次成功快照以来已更改的数据。频繁快照和不频繁快照之间的磁盘使用率差异通常很小。
换句话说，一周内每小时拍摄一次快照（总共拍摄 168 个快照）可能不会比周末拍摄一个快照占用更多的磁盘空间。此外，拍摄快照的频率越高，完成快照所需的时间越短。一些 Easysearch 用户每半小时拍摄一次快照。
如果需要删除快照，请确保使用 Easysearch API，而不是导航到存储位置并清除文件。集群中的增量快照通常共享大量相同的数据；使用 API 时， Easysearch 仅删除其他快照未使用的数据。 {: .tip }
注册快照存储库 # 在拍摄快照之前，必须“注册”快照存储库。快照存储库只是一个存储位置：共享文件系统、 Amazon S3 、 Hadoop 分布式文件系统（HDFS）、 Azure 存储等。
Shared file system # 要将共享文件系统用作快照存储库，请将其添加到 easysearch.yml： path.repo: [&amp;#34;/mnt/snapshots&amp;#34;] 在 RPM 和 Debian 安装中，您可以安装文件系统。如果您使用 Docker 安装，请在启动集群之前，将文件系统添加到 docker-compose.yml 中的每个节点：
volumes: - /Users/jdoe/snapshots:/mnt/snapshots T1.</description></item><item><title>写入限流</title><link>/easysearch/v1.14.0/docs/references/management/throttling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/throttling/</guid><description>写入限流 # Easysearch 支持节点级别和分片级别的写入限流功能，可以将 bulk 操作对集群的压力，限制在可接受的范围。
最低版本 # 1.8.0
限流参数设置 # 以下是 Easysearch 集群级别的限流设置，并且是动态的，您可以更改此功能的默认行为，而无需重新启动集群。
限流参数说明
名称 类型 说明 默认值 cluster.throttle.node.write boolean 是否启用节点级别限流 false cluster.throttle.node.write.max_requests int 限定时间范围内单个节点允许的最大写入请求次数 0 cluster.throttle.node.write.max_bytes 字符串 限定时间范围内单个节点允许的最大写入请求字节数（kb, mb, gb 等） 0mb cluster.throttle.node.write.action 字符串 触发限速之后的处理动作，分为 retry 和 drop 两种，默认为 drop drop cluster.throttle.node.write.interval int 节点级别评估限速的单位时间间隔，默认为 1s 1 cluster.throttle.shard.write boolean 是否启用分片级别限流 false cluster.</description></item><item><title>可搜索快照</title><link>/easysearch/v1.14.0/docs/references/management/searchable_snapshot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/searchable_snapshot/</guid><description>可搜索快照 # 可搜索快照索引在实时搜索时从快照存储库中按需读取数据，而不是在恢复时将所有索引数据下载到集群存储中。由于索引数据仍然保持在快照格式中存储在存储库中，因此可搜索快照索引本质上是只读的。 任何尝试写入可搜索快照索引的操作都会导致错误。
可搜索快照功能采用了诸如在集群节点中缓存频繁使用的数据段以及删除集群节点中最不常使用的数据段等技术，以便为频繁使用的数据段腾出空间。从快照下载的数据段存储在块存储中，与集群节点的通用索引并存。 因此，集群节点的计算能力在索引、本地搜索和存储在低成本对象存储，例如 Amazon Simple Storage Service（Amazon S3）上的快照数据段之间共享。 尽管集群节点的资源利用效率要高得多，但大量的任务将导致快照搜索变得较慢且持续时间较长。节点的本地存储也用于缓存快照数据。
将节点配置为使用可搜索快照 # 只有角色为 search 的节点才能进行快照搜索，要启用可搜索快照功能，请在您的 easysearch.yml 文件中创建一个节点，并将节点角色定义为 &amp;ldquo;search&amp;rdquo;：
node.name: snapshots-node node.roles: [search] 如果您正在运行 Docker，可以通过在您的 docker-compose.yml 文件中添加以下行来创建一个具有搜索节点角色的节点：
- node.roles: [search] 创建可搜索的快照索引 # 可搜索的快照索引是通过使用 _restore API 并指定 remote_snapshot 存储类型来创建的。
storage_type:
local 表示所有快照的元数据和索引数据都将下载到本地存储。 remote_snapshot 表示快照的元数据将下载到集群，但远程存储库将保持索引数据的权威存储。数据将根据需要下载和缓存以提供查询服务。为了使用 remote_snapshot 类型还原快照，集群中至少必须配置一个节点具有 search 节点角色。 使用示例 # 下面我们以 MinIO 作为快照存储仓库，Minio 是专为云应用程序开发人员和 DevOps 构建的对象存储服务器，与 Amazon S3 对象存储兼容。
使用 _snapshot API 注册 MinIO 存储库</description></item><item><title>使用时间范围合并策略优化时序索引</title><link>/easysearch/v1.14.0/docs/references/management/time-series-Index-optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/time-series-Index-optimization/</guid><description>使用时间范围合并策略优化时序索引 # 在处理时序数据（如日志、监控指标、事件流）时，数据通常具有明显的时间先后顺序。Easysearch 底层的 Lucene Segment 合并是保证搜索性能和资源效率的关键操作。 然而，默认的合并策略（如 TieredMergePolicy）主要基于 Segment 的大小和删除文档比例来决定合并哪些 Segment，它并不感知数据的时间属性。
对于时序场景，这种默认策略可能导致：
冷热数据混合合并：较旧的（冷）数据 Segment 可能与较新的（热）数据 Segment 合并，导致不必要的 I/O 和 CPU 开销，因为冷数据通常访问频率低，合并它们带来的收益有限。
查询性能影响：跨时间范围的大 Segment 可能降低某些按时间范围过滤的查询效率。
为了解决这些问题，Easysearch 从 1.12.1 版本开始引入了基于时间范围的合并策略 (TimeRangeMergePolicy)，专门为时序索引优化 Segment 合并行为。
最低版本 # 1.12.1
核心概念：TimeRangeMergePolicy # TimeRangeMergePolicy 是一种特殊的合并策略，它在选择要合并的 Segment 时，除了考虑大小、删除比例等因素外，优先考虑 Segment 所覆盖的时间范围。
其核心思想是：
时间优先：倾向于合并时间上相邻的 Segment。
保留时间分区：尽量避免将时间跨度很大的 Segment 合并在一起，保持数据的“时间局部性”。
优先合并新数据：通常，新写入的数据变化更频繁（包括更新、删除），优先合并较新的 Segment 有助于更快地回收空间和优化最新数据的查询性能。
如何启用 # 要为你的时序索引启用时间范围合并策略，你需要更新索引的设置，指定用于时间排序的字段名。
步骤： # 确认时间字段：确保你的索引 Mapping 中有一个合适的日期或时间戳类型的字段（如 @timestamp、event_time 等），并且该字段准确反映了数据的时间属性。 更新索引设置：使用 Index Settings API 来设置 index.</description></item><item><title>数据汇总</title><link>/easysearch/v1.14.0/docs/references/management/rollup_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/rollup_api/</guid><description>数据汇总 # 数据汇总或上卷（Rollup），对于时序场景类的数据，往往会有大量的非常详细的聚合指标，随着时间的图推移，存储将持续增长。汇总功能可以将旧的、细粒度的数据汇总为粗粒度格式以进行长期存储。通过将数据汇总到一个单一的文档中，可以大大降低历史数据的存储成本。 Easysearch 的 rollup 具备一些独特的优势，可以自动对 rollup 索引进行滚动而不用依赖其他 API 去单独设置，并且在进行聚合查询时支持直接搜索原始索引，做到了对业务端的搜索代码完全兼容，从而对用户无感知。
支持的聚合类型 # 对数值类型字段支持的聚合
avg sum max min value_count percentiles 对 keyword 类型字段提供 terms 聚合。
对 date 类型字段 除了 date_histogram 聚合，还支持 date_range 聚合。(v1.10.0)
查询 rollup 数据时，增加支持 Filter aggregation，某些场景可以用来替代 query 过滤数据。(v1.10.1)
增加针对个别字段自定义 special_metrics 指标的配置项。 (v1.10.1)
增加支持 Bucket sort aggregation。 (v1.10.1)
混合查询原始索引和 rollup 索引时，返回的 response 里增加了 origin 参数，表示包含 rollup 数据。(v1.10.1)
Rollup 查询 API 提供了 debug 参数，显示 Easysearch 内部执行的查询语句。(v1.10.1)</description></item><item><title>权限列表</title><link>/easysearch/v1.14.0/docs/references/security/access-control/permissions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/security/access-control/permissions/</guid><description>权限列表 # 此页面是可用权限的完整列表。每个权限控制对数据类型或 API 的访问。
集群权限 # cluster:admin/ingest/pipeline/delete cluster:admin/ingest/pipeline/get cluster:admin/ingest/pipeline/put cluster:admin/ingest/pipeline/simulate cluster:admin/ingest/processor/grok/get cluster:admin/reindex/rethrottle cluster:admin/repository/delete cluster:admin/repository/get cluster:admin/repository/put cluster:admin/repository/verify cluster:admin/reroute cluster:admin/script/delete cluster:admin/script/get cluster:admin/script/put cluster:admin/settings/update cluster:admin/snapshot/create cluster:admin/snapshot/delete cluster:admin/snapshot/get cluster:admin/snapshot/restore cluster:admin/snapshot/status cluster:admin/snapshot/status* cluster:admin/tasks/cancel cluster:admin/tasks/test cluster:admin/tasks/testunblock cluster:monitor/allocation/explain cluster:monitor/health cluster:monitor/main cluster:monitor/nodes/hot_threads cluster:monitor/nodes/info cluster:monitor/nodes/liveness cluster:monitor/nodes/stats cluster:monitor/nodes/usage cluster:monitor/remote/info cluster:monitor/state cluster:monitor/stats cluster:monitor/task cluster:monitor/task/get cluster:monitor/tasks/list 索引权限 # indices:admin/aliases indices:admin/aliases/exists indices:admin/aliases/get indices:admin/analyze indices:admin/cache/clear indices:admin/close indices:admin/create indices:admin/delete indices:admin/exists indices:admin/flush indices:admin/flush* indices:admin/forcemerge indices:admin/get indices:admin/mapping/put indices:admin/mappings/fields/get indices:admin/mappings/fields/get* indices:admin/mappings/get indices:admin/open indices:admin/refresh indices:admin/refresh* indices:admin/resolve/index indices:admin/rollover indices:admin/seq_no/global_checkpoint_sync indices:admin/settings/update indices:admin/shards/search_shards indices:admin/shrink indices:admin/synced_flush indices:admin/template/delete indices:admin/template/get indices:admin/template/put indices:admin/types/exists indices:admin/upgrade indices:admin/validate/query indices:data/read/explain indices:data/read/field_caps indices:data/read/field_caps* indices:data/read/get indices:data/read/mget indices:data/read/mget* indices:data/read/msearch indices:data/read/msearch/template indices:data/read/mtv indices:data/read/mtv* indices:data/read/scroll indices:data/read/scroll/clear indices:data/read/search indices:data/read/search* indices:data/read/search/template indices:data/read/tv indices:data/write/bulk indices:data/write/bulk* indices:data/write/delete indices:data/write/delete/byquery indices:data/write/index indices:data/write/reindex indices:data/write/update indices:data/write/update/byquery indices:monitor/recovery indices:monitor/segments indices:monitor/settings/get indices:monitor/shard_stores indices:monitor/stats indices:monitor/upgrade 权限集合 # 为了提高权限设置的效率，我们对系统权限进行自定义管理，从而快速批量选择一组相关的权限，而不是分别选择单个权限，为了方便，系统内置了若干权限集合。</description></item><item><title>其它常用 API</title><link>/easysearch/v1.14.0/docs/references/management/popular-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/management/popular-api/</guid><description>其它常用 API # 此页面包含 Easysearch 常用 API 的示例请求。
使用非默认设置创建索引 # PUT my-logs { &amp;#34;settings&amp;#34;: { &amp;#34;number_of_shards&amp;#34;: 4, &amp;#34;number_of_replicas&amp;#34;: 2 }, &amp;#34;mappings&amp;#34;: { &amp;#34;properties&amp;#34;: { &amp;#34;title&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34; }, &amp;#34;year&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;integer&amp;#34; } } } } 索引单个文档并自动生成随机 ID # POST my-logs/_doc { &amp;#34;title&amp;#34;: &amp;#34;Your Name&amp;#34;, &amp;#34;year&amp;#34;: &amp;#34;2016&amp;#34; } 索引单个文档并指定 ID # PUT my-logs/_doc/1 { &amp;#34;title&amp;#34;: &amp;#34;Weathering with You&amp;#34;, &amp;#34;year&amp;#34;: &amp;#34;2019&amp;#34; } 一次索引多个文档 # 请求正文末尾的空白行是必填的。如果省略 _id 字段， Easysearch 将生成一个随机 id 。</description></item><item><title>管道聚合</title><link>/easysearch/v1.14.0/docs/references/aggregation/pipeline-agg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/references/aggregation/pipeline-agg/</guid><description>管道聚合 # 使用管道聚合，可以通过将一个聚合的结果作为输入管道传输到另一个聚合来链接聚合，以获得更细微的输出。
可以使用管道聚合来计算复杂的统计和数学度量值，如导数、移动平均值、累积总和等。
管道聚合语法 # 管道聚合使用 buckets_path 属性访问其他聚合的结果。 buckets_path 属性具有特定的语法：
buckets_path = &amp;lt;AGG_NAME&amp;gt;[&amp;lt;AGG_SEPARATOR&amp;gt;,&amp;lt;AGG_NAME&amp;gt;]*[&amp;lt;METRIC_SEPARATOR&amp;gt;, &amp;lt;METRIC&amp;gt;]; 分别是:
AGG_NAME 是聚合的名称。 AGG_SEPARATOR 分隔聚合，表示为 &amp;gt; 。 METRIC_SEPARATOR 将聚合与 metrics 分开，表示为 . 。 METRIC 指标名称，如果是多值指标聚合。 例如，my_sum.sum 选择名为 my_sum 的聚合的 sum 指标。popular_tags&amp;gt;my_sum.sum 将 my_sum.sum 嵌套到 popular_tags 聚合中。
您还可以指定以下附加参数：
gap_policy: 真实世界数据可以包含间隙或空值。您可以使用 gap_policy 属性指定处理此类丢失数据的策略。您可以将 gap_policy 属性设置为 skip ，以跳过丢失的数据并从下一个可用值继续，或将 insert_zeros 设置为零，以将丢失的值替换为零并继续运行。 format: 输出值的格式类型。例如，yyyy-MM-dd 表示日期值。 样例 # 要对 sum_total_memory 聚合返回的所有存储桶求和，请执行以下操作：
GET kibana_sample_data_logs/_search { &amp;#34;size&amp;#34;: 0, &amp;#34;aggs&amp;#34;: { &amp;#34;number_of_bytes&amp;#34;: { &amp;#34;histogram&amp;#34;: { &amp;#34;field&amp;#34;: &amp;#34;bytes&amp;#34;, &amp;#34;interval&amp;#34;: 10000 }, &amp;#34;aggs&amp;#34;: { &amp;#34;sum_total_memory&amp;#34;: { &amp;#34;sum&amp;#34;: { &amp;#34;field&amp;#34;: &amp;#34;phpmemory&amp;#34; } } } }, &amp;#34;sum_copies&amp;#34;: { &amp;#34;sum_bucket&amp;#34;: { &amp;#34;buckets_path&amp;#34;: &amp;#34;number_of_bytes&amp;gt;sum_total_memory&amp;#34; } } } } 返回示例 # .</description></item><item><title>系统调优</title><link>/easysearch/v1.14.0/docs/getting-started/settings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/settings/</guid><description>系统调优 # 芯片及操作系统兼容性 # 目前已在国产主流芯片及操作系统上进行了验证，分别为 openEuler、统信 UOS、麒麟、龙芯、申威、兆芯。同样也兼容 Windows、 MacOS、 CentOS、 Ubuntu、 RedHat 等常用操作系统。
Java 兼容性 # 默认情况下 Easysearch 并不包含 JDK, 推荐使用 Java 15.0.1+9 或 Java 17.0.6+10, 最低版本要求为 Java 11, 要使用不同的 Java 安装，请将 JAVA_HOME 环境变量设置为 Java 安装位置或将 JDK 软链接到 Easysearch 安装目录下取名为 jdk。
例如：
#设置 JAVA_HOME 环境变量，可放入 ~/.bashrc 或 /etc/profile export JAVA_HOME=/usr/local/jdk #软链接 sudo ln -s /usr/local/jdk /opt/easysearch/jdk 网络要求 # Easysearch 需要打开以下端口:
端口 模块说明 9200 REST API 9300 节点间通信 系统参数 # 要保证 Easysearch 运行在最佳状态，其所在服务器的操作系统也需要进行相应的调优，以 Linux 为例。</description></item><item><title>节点扩容</title><link>/easysearch/v1.14.0/docs/getting-started/install/operator/node_scale/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/operator/node_scale/</guid><description>节点扩容 # 与上述 cpu mem disk 扩容一样，只需要修改 Operator yaml 文件中的 replicas 字段值即可。 这里修改为 5 个节点，并 apply，将会并发创建新的节点：threenodes-masters-3, threenodes-masters-4
最终完成节点扩容。</description></item><item><title>密码修改</title><link>/easysearch/v1.14.0/docs/getting-started/install/operator/update_password/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/operator/update_password/</guid><description>密码修改 # Operator 将 Easysearch 的密码保存在 k8s 的 Secret 中，查看已有的 Secret
现在准备修改密码，编辑 admin-credentials-secret.yaml 文件，并 apply
apiVersion: v1 kind: Secret metadata: name: threenodes-admin-password type: Opaque data: # admin username: YWRtaW4= # admin123 password: YWRtaW4xMjM= operator 感知到 threenodes-admin-password 有变化后，会检查账号密码是否有更新（通过检查账号密码生成的 hash 值是否与 job 的 annotations: &amp;ldquo;securityconfig/checksum&amp;rdquo; 值相同来判断），如果有更新则重新执行 job（集群名称-securityconfig-update）,这里的名称是 threenodes-securityconfig-update
继续查看这个 job 的 spec
可以知道，这个 job 本质上也是 Easysearch，但是它执行 shell 命令来修改集群的密码：
until curl -k -XPUT --cert admin-credentials/tls.crt --key admin-credentials/tls.key \ -H &amp;#39;Content-Type: application/json&amp;#39; &amp;#39;https://threenodes.default.svc.cluster.local:9200/_security/user/admin \ -d &amp;#39; { &amp;#34;password&amp;#34;: &amp;#34;admin&amp;#34;, &amp;#34;external_roles&amp;#34;: [&amp;#34;admin&amp;#34;] }&amp;#39;;echo &amp;#39;Waiting to connect to the cluster&amp;#39;; sleep 60; 修改密码后，再次查看 secret： 可知密码已经修改为目标密码。</description></item><item><title>常用配置</title><link>/easysearch/v1.14.0/docs/getting-started/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/configuration/</guid><description>常用配置 # 大多数 Easysearch 配置都可以通过集群设置 API 进行更改，某些配置则需要修改 easysearch.yml 并重新启动集群。
easysearch.yml 对每个节点都是本地的，因此应尽可能使用集群设置 REST API, 将设置应用于集群中的所有节点，一般我们采用开发者工具来进行操作。
集群设置 API # 第一步是查看当前设置：
GET _cluster/settings?include_defaults=true 查看用户自进行的自定义设置
GET _cluster/settings 集群设置 API 中存在三类设置：持久（Persistent）、临时（Transient）和默认。持久设置在集群重新启动后仍然存在。重新启动后，Easysearch 会清临时设置。
如果在多个位置指定相同的设置，Easysearch 将使用以下优先级来读取配置：
Transient 设置 Persistent 设置 配置文件 easysearch.yml 默认设置 要更改设置，只需将新设置指定为持久或临时。采用单层 JSON 形式：
PUT /_cluster/settings { &amp;#34;persistent&amp;#34; : { &amp;#34;action.auto_create_index&amp;#34; : false } } 同样也可以使用多层 JSON 形式：
PUT /_cluster/settings { &amp;#34;persistent&amp;#34;: { &amp;#34;action&amp;#34;: { &amp;#34;auto_create_index&amp;#34;: false } } } 配置目录包括许多安全相关的设置。要了解更多信息，请参阅 安全配置。</description></item><item><title>版本升级</title><link>/easysearch/v1.14.0/docs/getting-started/install/operator/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/operator/upgrade/</guid><description>版本升级 # 查看已有的版本：Easysearch:1.7.0-223
现在准备升级到 Easysearch:1.7.1-225，修改 Operator yaml 中的 version 字段，并 apply
# version: &amp;#34;1.7.0-223&amp;#34; version: &amp;#34;1.7.1-225&amp;#34; httpPort: 9200 vendor: Easysearch serviceAccount: controller-manager serviceName: threenodes 升级会比较久，因为为了保证升级过程中的服务可用性，节点升级是滚动升级的形式进行。 threenodes-masters-0 开始滚动更新，然后是 threenodes-masters-1，依次滚动更新， 直至所有节点更新完毕，大概总耗时 10 分钟
查看 Easysearch 新版本可知为 1.7.1-225
至此，版本升级完毕。</description></item><item><title>配置说明</title><link>/easysearch/v1.14.0/docs/getting-started/configuration_file/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/configuration_file/</guid><description>配置文件 # 可以在每个 Easysearch 节点上找到 easysearch.yml , 通常为 Easysearch 安装目录下 config/easysearch.yml 。
警告 # 切勿将未受保护的节点暴露在公共互联网上！
常用网络设置： # Easysearch 默认只绑定到 localhost。
对于生产环境的集群，需要配置基本的网络设置。
network.host: （静态）节点绑定的主机名或 IP 地址。默认为 local，同时设置了 network.bind_host 和 network.publish_host。 discovery.seed_hosts: （静态）初始集群节点列表。默认为 [&amp;ldquo;127.0.0.1&amp;rdquo;, &amp;ldquo;[::1]&amp;quot;]。 http.port: HTTP （静态）请求的绑定端口。默认为 9200-9300。 transport.port: （静态）节点间通信的绑定端口。默认为 9300-9400。 network.bind_host: （静态）节点监听传入请求的地址。可以配置为外网地址（例如 0.0.0.0 监听所有接口）或其他特定的地址。 network.publish_host: （静态）用于节点之间的通信，在多网卡或多网络环境中，应显式设置 network.publish_host。 discovery.seed_hosts: （静态）提供集群中有资格成为主节点的节点地址列表。也可以是一个包含多个以逗号分隔的地址的单个字符串。每个地址的格式为 host:port 或 host。 discovery.type: （静态）指定 Easysearch 是否应形成一个多节点集群。如果将 discovery.type 设置为 single-node，Easysearch 将形成一个单节点集群。 cluster.initial_master_nodes: （静态）设置全新集群中的初始主节点候选节点列表。默认情况下，此列表为空，意味着该节点期望加入已经引导好的集群， 在生产环境中首次启动一个全新的 Easysearch 集群时，必须配置 cluster.initial_master_nodes，以明确哪些节点有资格参与主节点的选举。 当集群完成了首次主节点选举，集群已经正常运行时，就不再需要 cluster.</description></item><item><title>证书管理</title><link>/easysearch/v1.14.0/docs/getting-started/install/operator/cert_manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/operator/cert_manager/</guid><description>证书管理 # 使用了 cert-manager 进行自动化管理证书，对于过期证书会自动重新颁发。
在这里我们根据 cert-manager 官方的配置方式配置了3套 Certificate 证书：ca-certificate、easysearch-certs 和 easysearch-admin-certs，分别用于节点间证书、http 访问证书和admin 管理员证书，具体参考下属 yaml 文件，重点需要主要证书的有效期(duration 字段)、更新时间(renewBefore 字段)和 commonName(infinilabs) 字段。
展开查看完整代码 ... apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned-issuer namespace: default spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ca-certificate namespace: default spec: secretName: ca-cert duration: 9000h # ~1year renewBefore: 360h # 15d commonName: infinilabs isCA: true privateKey: size: 2048 usages: - digital signature - key encipherment issuerRef: name: selfsigned-issuer --- apiVersion: cert-manager.</description></item><item><title>s3 备份</title><link>/easysearch/v1.14.0/docs/getting-started/install/operator/s3_snapshot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/operator/s3_snapshot/</guid><description>s3 定期备份 # 与更新集群密码类似，也是根据 Operator yaml 的配置来启动一个 job, 然后请求集群 API 来配置相应的 s3 备份策略 具体参考文档： s3 定期备份</description></item><item><title>历史版本</title><link>/easysearch/v1.14.0/docs/getting-started/install/operator/history_version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/operator/history_version/</guid><description/></item><item><title>Docker</title><link>/easysearch/v1.14.0/docs/getting-started/install/docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/docker/</guid><description>Docker 环境下使用 Easysearch # 在使用 Docker 运行 Easysearch 之前，请确保已进行 系统调优并安装好 Docker 服务，且 Docker 服务正常运行。
最快方式：启动临时的 docker 容器，可以从前台查看到 admin 随机生成的初始密码
注： Docker 环境一般用于临时验证，如需要长期使用请务必进行数据持久化 # 直接运行镜像使用随机密码（数据及配置未持久化） docker run --name easysearch --ulimit memlock=-1:-1 -p 9200:9200 infinilabs/easysearch:1.13.1 # 使用自定义密码，可以使用环境变量配置 （需要 1.8.2 及以后的版本才支持） echo &amp;#34;EASYSEARCH_INITIAL_ADMIN_PASSWORD=you_complex_pass&amp;#34; | tee .env # 通过从环境变量文件设置初始密码（数据及配置未持久化） docker run --name easysearch --env-file ./.env --ulimit memlock=-1:-1 -p 9200:9200 infinilabs/easysearch:1.13.1 # 使用自定义密码及命名卷 (数据持久化到命名卷) docker run -d --name easysearch \ --ulimit memlock=-1:-1 \ --env-file .</description></item><item><title>FAQ</title><link>/easysearch/v1.14.0/docs/getting-started/install/operator/FAQ/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/operator/FAQ/</guid><description>Easysearch 版本降级会报错 # cannot downgrade a node from version [1.7.0] to version [1.6.1]
[2024-01-28T09:42:34,314][ERROR][o.e.b.EasysearchUncaughtExceptionHandler] [onenode-masters-0] uncaught exception in thread [main] org.easysearch.bootstrap.StartupException: java.lang.IllegalStateException: cannot downgrade a node from version [1.7.0] to version [1.6.1] at org.easysearch.bootstrap.easysearch.init(Easysearch.java:173) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.execute(Easysearch.java:160) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:71) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.cli.Command.mainWithoutErrorHandling(Command.java:112) ~[easysearch-cli-1.6.1.jar:1.6.1] at org.easysearch.cli.Command.main(Command.java:75) ~[easysearch-cli-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.main(Easysearch.java:125) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.main(Easysearch.java:67) ~[easysearch-1.6.1.jar:1.6.1] Caused by: java.lang.IllegalStateException: cannot downgrade a node from version [1.7.0] to version [1.</description></item><item><title>Docker Compose</title><link>/easysearch/v1.14.0/docs/getting-started/install/docker-compose/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/docker-compose/</guid><description>Docker Compose 环境下使用 Easysearch # 在使用 docker-compose 运行 Easysearch 集群之前，请确保已进行 系统调优并安装好 Docker 服务，且 Docker 服务正常运行。
# 安装docker-compose curl -L &amp;#34;https://ghproxy.com/github.com/docker/compose/releases/download/v2.6.1/docker-compose-$(uname -s)-$(uname -m)&amp;#34; -o /usr/local/bin/docker-compose # 增加执行权限 chmod +x /usr/local/bin/docker-compose # 检查版本信息 docker-compose -v 运行 2 节点 docker compose 项目 # 从官网下载文件并解压，然后运行初始化脚本，最后运行启动脚本。
在宿主机上创建工作目录 sudo mkdir -p /opt/docker/compose 下载文件并解压 curl -sSL https://release.infinilabs.com/easysearch/archive/compose/2node.tar.gz | sudo tar -xzC /opt/docker/compose --strip-components=1 # 如需测试 3 节点，只需把上面的下载文件名改为 3node.tar.gz 注意：解压之后，请把镜像的 latest 版本手工更新成具体的版本，可参考下面的命令</description></item><item><title>Helm Chart</title><link>/easysearch/v1.14.0/docs/getting-started/install/helm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/helm/</guid><description>Helm Chart 部署 # INFINI Easysearch 从 1.5.0 版本开始支持 Helm Chart 方式部署。
仓库信息 # INFINI Easysearch Helm Chart 仓库地址: https://helm.infinilabs.com。
可以使用以下命令添加仓库
helm repo add infinilabs https://helm.infinilabs.com 依赖项 # StorageClass INFINI Easysearch Helm Chart 包中默认使用 local-path 进行数据持久化存储，可参考 local-path官方文档进行安装。
如果使用其他 StorageClass，请修改 Chart 包中的 storageClassName: local-path配置项。
Secret INFINI Easysearch Helm Chart 默认使用 cert-manager 进行自签 CA 证书创建及分发, 可参考 cert-manager 官方文档进行安装。
安装示例 # cat &amp;lt;&amp;lt; EOF | kubectl apply -n &amp;lt;namespace&amp;gt; -f - apiVersion: cert-manager.</description></item><item><title>Linux</title><link>/easysearch/v1.14.0/docs/getting-started/install/linux/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/linux/</guid><description>Linux 环境下使用 Easysearch # 为了安全起见，Easysearch 不支持通过 root 身份来运行，需要新建普通用户，如 easysearch 用户来快速运行 Easysearch。
一键安装 # 通过我们提供的自动安装脚本可自动下载最新版本的 easysearch 进行解压安装，默认解压到 /opt/easysearch
curl -sSL http://get.infini.cloud | bash -s -- -p easysearch 脚本的可选参数如下：
-v [版本号]（默认采用最新版本号）
-d [安装目录]（默认安装到/opt/easysearch）
bundle 包运行 # bundle 是内置 JDK 的安装包，不需要额外下载 JDK，可直接解压运行。
# 创建 easysearch 用户 groupadd -g 602 easysearch useradd -u 602 -g easysearch -m -d /home/easysearch -c &amp;#39;easysearch&amp;#39; -s /bin/bash easysearch # 创建 easysearch 安装目录 mkdir -p /opt/easysearch # 下载 bundle 包并解压到安装目录 wget -O - https://release.</description></item><item><title>Windows</title><link>/easysearch/v1.14.0/docs/getting-started/install/windows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.14.0/docs/getting-started/install/windows/</guid><description>Windows 环境下使用 Easysearch # 目前，有多种方案可以在 Windows 下体验 Easysearch。
方案一 # 如果您的 Windows 环境上有 Docker，请查看 Docker 环境下使用 Easysearch
方案二 # 使用非 https 方式的 Easysearch
手工下载 Easysearch，并解压安装。 手工下载 JDK 将 JDK 解压到 Easysearch 安装目录下，并将目录的名称修改为 jdk。 由于 Windows 环境下默认没有 openssl，生成证书不太方便，您可以通过其他方式来生成证书，如在 Linux 环境提前生成证书。
#用记事本打开 config/easysearch.yml，并修改配置。 security.enabled: false 方案三 # 通过安装 git-for-windows 来执行 bash 操作。
注意：以下操作在 git-bash 中执行
通过在线脚本进行 Easysearch 安装 curl -sSL http://get.</description></item></channel></rss>