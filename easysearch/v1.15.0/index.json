[{"category":null,"content":"自动间隔日期直方图 #  与日期直方图聚合类似，其中你必须指定一个间隔， auto_date_histogram 是一个多分组聚合，根据你提供的分组数量和数据的时范围自动创建日期直方图分组。返回的实际分组数量总是小于或等于你指定的分组数量。当你在处理时间序列数据并希望在不同时间间隔上可视化或分析数据，而不需要手动指定间隔大小时，这种聚合特别有用。\n间隔参数 #  分组间隔是根据收集的数据选择的，以确保返回的分组数量小于或等于请求的数量。\n下表列出了每个时间单位可能的返回间隔。\n   单位 间隔参数     Seconds 1、5、10 和 30 的倍数   Minutes 1、5、10 和 30 的倍数   Hours 1、3 和 12 的倍数   Days 1 和 7 的倍数   Months 1 和 3 的倍数   Years 1、5、10、20、50 和 100 的倍数    如果一个聚合返回的分组太多（例如，每天一个分组），Easysearch 会自动减少分组的数量以确保结果可管理。它不会返回请求的确切数量的每日分组，而是会减少大约 1/7。例如，如果你请求 70 个分组，但数据中包含太多的每日间隔，Easysearch 可能只会返回 10 个分组，将数据分组到更大的间隔（如周）中，以避免结果数量过多。这有助于优化聚合，并在数据过多时防止过多细节。\n参考样例 #  在以下示例中，你将搜索一个包含博客文章的索引。\n首先，为这个索引创建一个映射，并将 date_posted 字段指定为 date 类型：\nPUT blogs { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;date_posted\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;date\u0026quot;, \u0026quot;format\u0026quot; : \u0026quot;yyyy-MM-dd\u0026quot; } } } } 接下来，将以下文档索引到 blogs 索引中：\nPUT blogs/_doc/1 { \u0026quot;name\u0026quot;: \u0026quot;Semantic search in Easysearch\u0026quot;, \u0026quot;date_posted\u0026quot;: \u0026quot;2022-04-17\u0026quot; } PUT blogs/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Sparse search in Easysearch\u0026quot;, \u0026quot;date_posted\u0026quot;: \u0026quot;2022-05-02\u0026quot; } PUT blogs/_doc/3 { \u0026quot;name\u0026quot;: \u0026quot;Distributed tracing with Data Prepper\u0026quot;, \u0026quot;date_posted\u0026quot;: \u0026quot;2022-04-25\u0026quot; } PUT blogs/_doc/4 { \u0026quot;name\u0026quot;: \u0026quot;Observability in Easysearch\u0026quot;, \u0026quot;date_posted\u0026quot;: \u0026quot;2023-03-23\u0026quot; } 要使用 auto_date_histogram 聚合，请指定包含日期或时间戳值的字段。例如，要将博客文章按 date_posted 聚合到两个分组中，请发送以下请求：\nGET /blogs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;auto_date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;date_posted\u0026quot;, \u0026quot;buckets\u0026quot;: 2 } } } } 返回内容显示博客文章被聚合到两个分组中。间隔被自动设置为 1 年，所有三个 2022 年的博客文章被收集到一个分组中，而 2023 年的博客文章在另一个分组中：\n{ \u0026quot;took\u0026quot;: 20, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2022-01-01\u0026quot;, \u0026quot;key\u0026quot;: 1640995200000, \u0026quot;doc_count\u0026quot;: 3 }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2023-01-01\u0026quot;, \u0026quot;key\u0026quot;: 1672531200000, \u0026quot;doc_count\u0026quot;: 1 } ], \u0026quot;interval\u0026quot;: \u0026quot;1y\u0026quot; } } } 分组信息 #\n 每个分组包含以下信息：\n{ \u0026quot;key_as_string\u0026quot;: \u0026quot;2023-01-01\u0026quot;, \u0026quot;key\u0026quot;: 1672531200000, \u0026quot;doc_count\u0026quot;: 1 } 在 Easysearch 中，日期内部存储为 64 位整数，表示自纪元以来的毫秒时间戳。在聚合响应中，每个分组 key 以时间戳的形式返回。 key_as_string 值显示相同的时间戳，但根据 format 参数格式化为日期字符串。 doc_count 字段包含分组中的文档数量。\n参数说明 #  自动间隔日期直方图聚合接受以下参数。\n   参数 数据类型 描述     field String 要对其聚合的字段。该字段必须包含日期或时间戳值。必须提供 field 或 script 。   buckets Integer 期望的分组数量。返回的分组数量小于或等于期望的分组数量。可选。默认为 10 。   minimum_interval String 要使用的最小间隔。指定最小间隔可以使聚合过程更高效。有效值为 year 、 month 、 day 、 hour 、 minute 和 second 。可选。   time_zone String 指定使用除默认（UTC）之外的时间区进行分组划分和舍入。您可以指定 time_zone 参数为 UTC 偏移量，例如 -04:00 ，或 IANA 时间区 ID，例如 America/New_York 。可选。默认为 UTC 。有关更多信息，请参阅 Time zone。   format String 返回表示分组键的日期的格式。可选。默认为字段映射中指定的格式。有关更多信息，请参阅 Date format。   script String 一个用于将值聚合到分组中的文档级或值级脚本。必须使用 field 或 script 。   missing String 指定如何处理字段值缺失的文档。默认情况下，此类文档会被忽略。如果你在 missing 参数中指定一个日期值，所有字段值缺失的文档都会被收集到指定日期的分组中。    关于日期格式 #  如果你没有指定 format 参数，将使用字段映射中定义的格式（如前一个响应所示）。要修改格式，请指定 format 参数：\nGET /blogs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;auto_date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;date_posted\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;yyyy-MM-dd HH:mm:ss\u0026quot; } } } } key_as_string 字段现在以指定的格式返回：\n{ \u0026quot;key_as_string\u0026quot;: \u0026quot;2023-01-01 00:00:00\u0026quot;, \u0026quot;key\u0026quot;: 1672531200000, \u0026quot;doc_count\u0026quot;: 1 } 或者，您可以指定一种内置的日期格式：\nGET /blogs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;auto_date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;date_posted\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;basic_date_time_no_millis\u0026quot; } } } } key_as_string 字段现在以指定的格式返回：\n{ \u0026quot;key_as_string\u0026quot;: \u0026quot;20230101T000000Z\u0026quot;, \u0026quot;key\u0026quot;: 1672531200000, \u0026quot;doc_count\u0026quot;: 1 } 时区使用 #\n 默认情况下，日期以 UTC 格式存储和处理。 time_zone 参数允许您为分分组指定不同的时区。您可以指定 time_zone 参数为 UTC 偏移量，例如 -04:00 ，或 IANA 时区 ID，例如 America/New_York 。\n例如，将以下文档索引到索引中：\nPUT blogs1/_doc/1 { \u0026quot;name\u0026quot;: \u0026quot;Semantic search in Easysearch\u0026quot;, \u0026quot;date_posted\u0026quot;: \u0026quot;2022-04-17T01:00:00.000Z\u0026quot; } PUT blogs1/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Sparse search in Easysearch\u0026quot;, \u0026quot;date_posted\u0026quot;: \u0026quot;2022-04-17T04:00:00.000Z\u0026quot; }\n首先，不指定时区运行聚合操作：\nGET /blogs1/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;auto_date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;date_posted\u0026quot;, \u0026quot;buckets\u0026quot;: 2, \u0026quot;format\u0026quot;: \u0026quot;yyyy-MM-dd HH:mm:ss\u0026quot; } } } } 返回内容包含两个 3 小时的分组，从 2022 年 4 月 17 日凌晨 UTC 开始：\n{ \u0026quot;took\u0026quot;: 6, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 2, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2022-04-17 01:00:00\u0026quot;, \u0026quot;key\u0026quot;: 1650157200000, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2022-04-17 04:00:00\u0026quot;, \u0026quot;key\u0026quot;: 1650168000000, \u0026quot;doc_count\u0026quot;: 1 } ], \u0026quot;interval\u0026quot;: \u0026quot;3h\u0026quot; } } } 现在，指定一个 time_zone 的 -02:00 ：\nGET /blogs1/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;auto_date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;date_posted\u0026quot;, \u0026quot;buckets\u0026quot;: 2, \u0026quot;format\u0026quot;: \u0026quot;yyyy-MM-dd HH:mm:ss\u0026quot;, \u0026quot;time_zone\u0026quot;: \u0026quot;-02:00\u0026quot; } } } } 返回内容包含两个分组，其中开始时间偏移 2 小时，从 2022 年 4 月 16 日 23:00 开始：\n{ \u0026quot;took\u0026quot;: 17, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 2, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2022-04-16 23:00:00\u0026quot;, \u0026quot;key\u0026quot;: 1650157200000, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2022-04-17 02:00:00\u0026quot;, \u0026quot;key\u0026quot;: 1650168000000, \u0026quot;doc_count\u0026quot;: 1 } ], \u0026quot;interval\u0026quot;: \u0026quot;3h\u0026quot; } } }  在使用有夏令时（DST）变化的时区时，接近转换点的分组的大小可能与相邻分组的大小略有不同。\n ","subcategory":null,"summary":"","tags":null,"title":"","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/auto-interval-date-histogram/"},{"category":null,"content":"Windows 环境下使用 Easysearch #  目前，有多种方案可以在 Windows 下体验 Easysearch。\n方案一 #  如果您的 Windows 环境上有 Docker，请查看 Docker 环境下使用 Easysearch\n方案二 #  使用非 https 方式的 Easysearch\n 手工下载 Easysearch，并解压安装。 手工下载 JDK 将 JDK 解压到 Easysearch 安装目录下，并将目录的名称修改为 jdk。   由于 Windows 环境下默认没有 openssl，生成证书不太方便，您可以通过其他方式来生成证书，如在 Linux 环境提前生成证书。\n #用记事本打开 config/easysearch.yml，并修改配置。 security.enabled: false 方案三 #  通过安装 git-for-windows 来执行 bash 操作。\n 注意：以下操作在 git-bash 中执行\n  通过在线脚本进行 Easysearch 安装  curl -sSL http://get.infini.cloud |bash -s -- -p easysearch -d /d/data/easysearch 下载 JDK  #下载JDK并存储到/d/opt目录 curl -# https://cdn.azul.com/zulu/bin/zulu17.54.21-ca-jre17.0.13-win_x64.zip -o /d/opt/jdk.zip 解压 JDK 文件  cd /d/data/easysearch \u0026amp;\u0026amp; unzip -q /d/opt/jdk.zip 将目录重命名为 jdk  mv zulu* jdk 设置 JAVA_HOME 环境变量  export JAVA_HOME=/d/data/easysearch/jdk 初始化证书，密码及插件  bin/initialize.sh 运行 Easysearch  bin/easysearch.bat 后续验证工作，请继续查看 安装指南\n","subcategory":null,"summary":"","tags":null,"title":"Windows","url":"/easysearch/v1.15.0/docs/getting-started/install/windows/"},{"category":null,"content":"Linux 环境下使用 Easysearch #   为了安全起见，Easysearch 不支持通过 root 身份来运行，需要新建普通用户，如 easysearch 用户来快速运行 Easysearch。\n 一键安装 #   通过我们提供的自动安装脚本可自动下载最新版本的 easysearch 进行解压安装，默认解压到 /data/easysearch\n curl -sSL http://get.infini.cloud | bash -s -- -p easysearch  脚本的可选参数如下：\n-v [版本号]（默认采用最新版本号）\n-d [安装目录]（默认安装到/data/easysearch）\n bundle 包运行 #   bundle 是内置 JDK 的安装包，不需要额外下载 JDK，可直接解压运行。\n # 创建 easysearch 用户 groupadd -g 602 easysearch useradd -u 602 -g easysearch -m -d /home/easysearch -c \u0026#39;easysearch\u0026#39; -s /bin/bash easysearch # 创建 easysearch 安装目录 mkdir -p /data/easysearch # 下载 bundle 包并解压到安装目录 wget -O - https://release.infinilabs.com/easysearch/stable/bundle/easysearch-1.14.0-2228-linux-amd64-bundle.tar.gz | tar -zx -C /data/easysearch # 初始化 cd /data/easysearch \u0026amp;\u0026amp; bin/initialize.sh # 调整目录权限 chown -R easysearch:easysearch /data/easysearch # 运行 Easysearch su easysearch -c \u0026#34;/data/easysearch/bin/easysearch -d -p pid\u0026#34; # 停止 Easysearch kill -9 $(cat pid) 手动安装 #  以 root 用户进行下面的操作\n 下载 JDK  #下载JDK并存储到/usr/src目录 wget -N https://cdn.azul.com/zulu/bin/zulu17.54.21-ca-jre17.0.13-linux_x64.tar.gz -P /usr/src 创建 JDK 解压后存储路径  mkdir -p /usr/local/jdk 解压文件到创建好的目录  tar -zxf /usr/src/zulu*.tar.gz -C /usr/local/jdk --strip-components 1 配置环境变量  #下载文件到/etc/profile.d wget -N https://release.infinilabs.com/easysearch/archive/java.sh -P /etc/profile.d 让配置生效  source /etc/profile 检查 java 版本信息  java -version 通过在线脚本进行 Easysearch 安装  curl -sSL http://get.infini.cloud |bash -s -- -p easysearch 创建 easysearch 用户组  groupadd -g 602 easysearch 创建 easysearch 用户，并添加到 easysearch 用户组  useradd -u 602 -g easysearch -m -d /home/easysearch -c \u0026#34;Easysearch user\u0026#34; -s /bin/bash easysearch 将 JDK 放置或通过软链接到 /data/easysearch/jdk  ln -s /usr/local/jdk /data/easysearch/jdk 初始化证书，密码及插件  cd /data/easysearch \u0026amp;\u0026amp; bin/initialize.sh 调整目录属主为 easysearch  chown -R easysearch:easysearch /data/easysearch 切换到 easysearch 用户  su - easysearch 运行 Easysearch  cd /data/easysearch \u0026amp;\u0026amp; bin/easysearch 将 Easysearch 配置为服务 #   如果您想通过服务的方式来运行 Easysearch，可手工配置 Easysearch 服务文件\n  下载服务文件  wget -N https://release.infinilabs.com/easysearch/archive/easysearch.service -P /usr/lib/systemd/system 或者直接编辑和执行下面的脚本:\nsudo tee /usr/lib/systemd/system/easysearch.service \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; [Unit] Description=easysearch Documentation=https://www.infinilabs.com After=network.target [Service] Type=forking User=easysearch Environment=\u0026#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/jdk/bin\u0026#34; ExecStart=/data/easysearch/bin/easysearch -d PrivateTmp=true LimitNOFILE=65536 LimitNPROC=65536 LimitAS=infinity LimitFSIZE=infinity LimitMEMLOCK=infinity TimeoutStopSec=0 KillSignal=SIGTERM KillMode=process SendSIGKILL=no SuccessExitStatus=143 [Install] WantedBy=multi-user.target EOF  如果您的 Easysearch 运行用户及安装目录不同，请修改服务文件中的 User 及 ExecStart。\n 重新加载服务配置文件  systemctl daemon-reload 启动 Easysearch 服务  systemctl start easysearch 检查 Easysearch 服务状态  systemctl status easysearch 后续验证工作，请继续查看 安装指南\n","subcategory":null,"summary":"","tags":null,"title":"Linux","url":"/easysearch/v1.15.0/docs/getting-started/install/linux/"},{"category":null,"content":"Helm Chart 部署 #  INFINI Easysearch 从 1.5.0 版本开始支持 Helm Chart 方式部署。\n仓库信息 #  INFINI Easysearch Helm Chart 仓库地址: https://helm.infinilabs.com。\n可以使用以下命令添加仓库\nhelm repo add infinilabs https://helm.infinilabs.com 依赖项 #   StorageClass  INFINI Easysearch Helm Chart 包中默认使用 local-path 进行数据持久化存储，可参考 local-path官方文档进行安装。\n如果使用其他 StorageClass，请修改 Chart 包中的 storageClassName: local-path配置项。\n Secret  INFINI Easysearch Helm Chart 默认使用 cert-manager 进行自签 CA 证书创建及分发, 可参考 cert-manager 官方文档进行安装。\n安装示例 #  cat \u0026lt;\u0026lt; EOF | kubectl apply -n \u0026lt;namespace\u0026gt; -f - apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: easysearch-ca-issuer spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: easysearch-ca-certificate spec: commonName: easysearch-ca-certificate duration: 87600h0m0s isCA: true issuerRef: kind: Issuer name: easysearch-ca-issuer privateKey: algorithm: ECDSA size: 256 renewBefore: 2160h0m0s secretName: easysearch-ca-secret EOF #安装 INFINI Easysearch, 默认单节点 helm install easysearch infinilabs/easysearch -n \u0026lt;namespace\u0026gt; \u0026ndash;create-namespace 卸载 #\n helm uninstall easysearch -n \u0026lt;namespace\u0026gt; kubectl delete pvc easysearch-data-easysearch-0 easysearch-config-easysearch-0 -n \u0026lt;namespace\u0026gt;  更多信息请参考技术博客， 通过 Helm Chart 部署 Easysearch\n ","subcategory":null,"summary":"","tags":null,"title":"Helm Chart","url":"/easysearch/v1.15.0/docs/getting-started/install/helm/"},{"category":null,"content":"离线安装 Easysearch #  本文档介绍如何在没有网络连接的环境中安装 Easysearch。\nLinux 环境离线安装 #  Bundle 包内置了 JDK，是最简单的离线安装方式。\n 提前准备（在有网络的环境中）  # 下载 bundle 包 wget https://release.infinilabs.com/easysearch/stable/bundle/easysearch-1.14.0-2228-linux-amd64-bundle.tar.gz 离线安装步骤  # 创建 easysearch 用户 groupadd -g 602 easysearch useradd -u 602 -g easysearch -m -d /home/easysearch -c \u0026#39;easysearch\u0026#39; -s /bin/bash easysearch # 创建 easysearch 安装目录 mkdir -p /data/easysearch\n# 解压 bundle 包到安装目录 tar -zxf easysearch-1.14.0-2228-linux-amd64-bundle.tar.gz -C /data/easysearch\n# 初始化 cd /data/easysearch \u0026amp;\u0026amp; bin/initialize.sh\n# 调整目录权限 chown -R easysearch:easysearch /data/easysearch\n# 运行 Easysearch su easysearch -c \u0026quot;/data/easysearch/bin/easysearch -d -p pid\u0026quot;\n# 停止 Easysearch kill -9 $(cat pid) Windows 环境离线安装 #\n 简化安装（无 HTTPS）\n 提前准备  - 下载 Easysearch: https://release.infinilabs.com/easysearch/stable/easysearch-1.15.0-windows-amd64.zip - 下载 JDK: https://cdn.azul.com/zulu/bin/zulu17.54.21-ca-jre17.0.13-win_x64.zip 离线安装步骤  # 解压 Easysearch 到目标目录 # 解压 JDK 到 Easysearch 目录下，重命名为 jdk # 修改配置文件 config/easysearch.yml security.enabled: false\n# 运行 bin/easysearch.bat \n","subcategory":null,"summary":"","tags":null,"title":"离线安装","url":"/easysearch/v1.15.0/docs/getting-started/install/offline-install/"},{"category":null,"content":"Docker Compose 环境下使用 Easysearch #  在使用 docker-compose 运行 Easysearch 集群之前，请确保已进行 系统调优并安装好 Docker 服务，且 Docker 服务正常运行。\n# 安装docker-compose curl -L \u0026#34;https://github.com/docker/compose/releases/download/v2.6.1/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose # 增加执行权限 chmod +x /usr/local/bin/docker-compose # 检查版本信息 docker-compose -v 运行 2 节点 docker compose 项目 #  从官网下载文件并解压，然后运行初始化脚本，最后运行启动脚本。\n 在宿主机上创建工作目录  # 创建操作目录 sudo mkdir -p /data/docker/compose 下载文件并解压   如需测试 3 节点，只需把下面的下载文件名改为 3node.tar.gz 即可。\n curl -sSL https://release.infinilabs.com/easysearch/archive/compose/2node.tar.gz | sudo tar -xzC /data/docker/compose --strip-components=1 # 调整目录权限 sudo chown -R ${USER} /data/docker/compose  注意：解压之后，请把镜像的 latest 版本手工更新成具体的版本，可参考下面的命令\ncd /data/docker/compose # 替换成具体的版本, 注：如果是 MacOS 请调整为 sudo sed -i \u0026#39;\u0026#39; \u0026#34;s/easysearch:latest/easysearch:1.15.0/;s/console:latest/console:1.29.8/\u0026#34; docker-compose.yml sudo sed -i \u0026#34;s/easysearch:latest/easysearch:1.15.0/;s/console:latest/console:1.29.8/\u0026#34; docker-compose.yml #检查版本是否替换成功 grep image docker-compose.yml  运行 docker-compose 项目  cd /data/docker/compose # 调整目录权限 sudo ./init.sh # 下载镜像 docker-compose pull # 启动 docker-compose 项目 ./start.sh # 如果启动失败，提示没权限，可进行权限调整，如 MacOS Docker 需要使用当前用户权限 sudo chown -R ${USER}:staff /data/docker/compose 停止 docker-compose 项目  # 停止项目 ./stop.sh 清理 docker-compose 项目  # 清理项目，将清理 Easysearch 的 data 和 logs。 sudo ./reset.sh 检查集群节点  # 集群默认的密码为admin curl -ku admin:xxxxxxxxxxxx https://localhost:9201/_cat/nodes?v # 输出信息如下： ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name 172.24.0.3 68 31 31 1.67 0.57 0.21 dimr - easysearch-node1 172.24.0.2 55 31 31 1.67 0.57 0.21 dimr * easysearch-node2 未获取到最新镜像处理方法 #  # 通过访问 https://hub.docker.com/r/infinilabs/easysearch/tags ，然后点击 latest 找出镜像的 sha256 值。 HASH=803b8aab2ec012728901112e916f1aa0fadc85c9b6b21b887a051aa8c5e53e8a docker pull infinilabs/easysearch:latest@sha256:$HASH IMGID=$(docker image ls --format \u0026#34;table {{.ID}}\u0026#34; --digests |grep \u0026#34;$HASH\u0026#34; |awk \u0026#39;{print $1}\u0026#39;) docker tag $IMGID infinilabs/easysearch:latest docker images |grep -v none |grep $IMGID # 或者直接通过版本号进行镜像拉取 docker pull infinilabs/easysearch:1.15.0 \n","subcategory":null,"summary":"","tags":null,"title":"Docker Compose","url":"/easysearch/v1.15.0/docs/getting-started/install/docker-compose/"},{"category":null,"content":"  Easysearch 版本降级会报错 #    cannot downgrade a node from version [1.7.0] to version [1.6.1]\n[2024-01-28T09:42:34,314][ERROR][o.e.b.EasysearchUncaughtExceptionHandler] [onenode-masters-0] uncaught exception in thread [main] org.easysearch.bootstrap.StartupException: java.lang.IllegalStateException: cannot downgrade a node from version [1.7.0] to version [1.6.1] at org.easysearch.bootstrap.easysearch.init(Easysearch.java:173) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.execute(Easysearch.java:160) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:71) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.cli.Command.mainWithoutErrorHandling(Command.java:112) ~[easysearch-cli-1.6.1.jar:1.6.1] at org.easysearch.cli.Command.main(Command.java:75) ~[easysearch-cli-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.main(Easysearch.java:125) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.main(Easysearch.java:67) ~[easysearch-1.6.1.jar:1.6.1] Caused by: java.lang.IllegalStateException: cannot downgrade a node from version [1.7.0] to version [1.6.1] at org.easysearch.env.NodeMetadata.upgradeToCurrentVersion(NodeMetadata.java:79) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.env.NodeEnvironment.loadNodeMetadata(NodeEnvironment.java:418) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.env.NodeEnvironment.\u0026lt;init\u0026gt;(NodeEnvironment.java:315) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.node.Node.\u0026lt;init\u0026gt;(Node.java:343) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.node.Node.\u0026lt;init\u0026gt;(Node.java:273) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.Bootstrap$5.\u0026lt;init\u0026gt;(Bootstrap.java:212) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.Bootstrap.setup(Bootstrap.java:212) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.Bootstrap.init(Bootstrap.java:378) ~[easysearch-1.6.1.jar:1.6.1] at org.easysearch.bootstrap.easysearch.init(Easysearch.java:169) ~[easysearch-1.6.1.jar:1.6.1]   基于已有的 PVC 构建新集群报错 #    2024-03-24T20:21:36.975095805+08:00 Caused by: org.easysearch.cluster.coordination.CoordinationStateRejectedException: join validation on cluster state with a different cluster uuid rCyF7ZGyRL275eWC0qoTDQ than local cluster uuid uSFTnF1kRU661ENZRAbq-A, rejecting 2024-03-24T20:21:36.975099446+08:00 at org.easysearch.cluster.coordination.JoinHelper.lambda$new$5(JoinHelper.java:149) ~[easysearch-1.7.1.jar:1.7.1] 2024-03-24T20:21:36.975103455+08:00 at com.infinilabs.security.ssl.transport.SecuritySSLRequestHandler.messageReceivedDecorate(SecuritySSLRequestHandler.java:201) ~[?:?] 2024-03-24T20:21:36.975107101+08:00 at com.infinilabs.security.transport.SecurityRequestHandler.messageReceivedDecorate(SecurityRequestHandler.java:330) ~[?:?] ","subcategory":null,"summary":"","tags":null,"title":"FAQ","url":"/easysearch/v1.15.0/docs/getting-started/install/operator/FAQ/"},{"category":null,"content":"Docker 环境下使用 Easysearch #  在使用 Docker 运行 Easysearch 之前，请确保已进行 系统调优并安装好 Docker 服务，且 Docker 服务正常运行。\n 最快方式：启动临时的 docker 容器，可以从前台查看到 admin 随机生成的初始密码\n 注： Docker 环境一般用于临时验证，如需要长期使用请务必进行数据持久化   # 直接运行镜像使用随机密码（数据及配置未持久化） docker run --name easysearch --ulimit memlock=-1:-1 -p 9200:9200 infinilabs/easysearch:1.15.0 # 使用自定义密码，可以使用环境变量配置 （需要 1.8.2 及以后的版本才支持） echo \u0026quot;EASYSEARCH_INITIAL_ADMIN_PASSWORD=$(openssl rand -hex 10)\u0026quot; | tee .env\n# 通过从环境变量文件设置初始密码（数据及配置未持久化） docker run \u0026ndash;name easysearch \u0026ndash;env-file ./.env \u0026ndash;ulimit memlock=-1:-1 -p 9200:9200 infinilabs/easysearch:1.15.0\n# 使用自定义密码及命名卷 (数据持久化到命名卷) docker run -d \u0026ndash;name easysearch  \u0026ndash;ulimit memlock=-1:-1  \u0026ndash;env-file ./.env -p 9200:9200  -v ezs-data:/app/easysearch/data  -v ezs-config:/app/easysearch/config  -v ezs-logs:/app/easysearch/logs  infinilabs/easysearch:1.15.0 数据持久化到本地（数据可长期使用） #\n 设置自定义密码，并从宿主机挂载数据目录、配置目录及日志目录，配置 jvm 内存为 512m。\n 在宿主机上创建目录  # 创建数据及日志存储目录 sudo mkdir -p /data/easysearch/{data,logs} # 根据自己的需求，设置成安全的密码 echo \u0026#34;EASYSEARCH_INITIAL_ADMIN_PASSWORD=$(openssl rand -hex 10)\u0026#34; | sudo tee /data/easysearch/.env 修改目录权限   非必须步骤，视具体操作环境而定\n # 注意：需要根据 Docker 运行环境判断是否需要调整权限，如在 MacOS 上使用 OrbStack 则不需要调整权限。 # 容器内 ezs 用户的 uid 为 602，通过调整宿主机的目录权限，确保在容器内部 ezs 用户有权限读写挂载的数据卷 sudo chown -R 602:602 /data/easysearch 从镜像初始化 config 目录  # 将镜像中的 config 目录复制到本地目录 docker run --rm --env-file /data/easysearch/.env -v /data/easysearch:/work infinilabs/easysearch:1.15.0 cp -rf /app/easysearch/config /work 后台运行容器  #后台启动容器，并指定内存大小及挂载数据、日志目录，设定好容器名称及容器主机名称 #如需调整配置文件，可以修改以下配置文件 # 1. /data/easysearch/config/easysearcy.yml # 2. /data/easysearch/config/jvm.options # 3. /data/easysearch/config/log4j2.properties docker run -d --restart always -p 9200:9200 \\  --ulimit memlock=-1:-1 \\  -e ES_JAVA_OPTS=\u0026#34;-Xms512m -Xmx512m\u0026#34; \\  -v /data/easysearch/data:/app/easysearch/data \\  -v /data/easysearch/config:/app/easysearch/config \\  -v /data/easysearch/logs:/app/easysearch/logs \\  --name easysearch --hostname easysearch \\  infinilabs/easysearch:1.15.0 升级 Easysearch 版本  # 先停止并删除正在运行的容器 docker stop easysearch \u0026amp;\u0026amp; docker rm easyserach # 再修改第 3 步中镜像的版本，重新运行命令即可 后续验证工作，请继续查看 安装指南\n提示 #   如果遇到 vm.max_map_count 错误， 请参考 内核调优 调整宿主机配置。\n ","subcategory":null,"summary":"","tags":null,"title":"Docker","url":"/easysearch/v1.15.0/docs/getting-started/install/docker/"},{"category":null,"content":"","subcategory":null,"summary":"","tags":null,"title":"历史版本","url":"/easysearch/v1.15.0/docs/getting-started/install/operator/history_version/"},{"category":null,"content":"s3 定期备份 #  与更新集群密码类似，也是根据 Operator yaml 的配置来启动一个 job, 然后请求集群 API 来配置相应的 s3 备份策略 具体参考文档： s3 定期备份","subcategory":null,"summary":"","tags":null,"title":"s3 备份","url":"/easysearch/v1.15.0/docs/getting-started/install/operator/s3_snapshot/"},{"category":null,"content":"证书管理 #  使用了 cert-manager 进行自动化管理证书，对于过期证书会自动重新颁发。\n在这里我们根据 cert-manager 官方的配置方式配置了3套 Certificate 证书：ca-certificate、easysearch-certs 和 easysearch-admin-certs，分别用于节点间证书、http 访问证书和admin 管理员证书，具体参考下属 yaml 文件，重点需要主要证书的有效期(duration 字段)、更新时间(renewBefore 字段)和 commonName(infinilabs) 字段。\n 展开查看完整代码 ...  apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned-issuer namespace: default spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ca-certificate namespace: default spec: secretName: ca-cert duration: 9000h # ~1year renewBefore: 360h # 15d commonName: infinilabs isCA: true privateKey: size: 2048 usages: - digital signature - key encipherment issuerRef: name: selfsigned-issuer --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: ca-issuer namespace: default spec: ca: secretName: ca-cert --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: easysearch-certs namespace: default spec: secretName: easysearch-certs duration: 9000h # ~1year renewBefore: 360h # 15d isCA: false privateKey: size: 2048 algorithm: RSA encoding: PKCS8 dnsNames: - threenodes - threenodes-masters-0 - threenodes-masters-1 - threenodes-masters-2 - threenodes-masters-3 - threenodes-masters-4 - threenodes-bootstrap-0 usages: - signing - key encipherment - server auth - client auth commonName: infinilabs issuerRef: name: ca-issuer --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: easysearch-admin-certs namespace: default spec: secretName: easysearch-admin-certs duration: 9000h # ~1year renewBefore: 360h # 15d isCA: false privateKey: size: 2048 algorithm: RSA encoding: PKCS8 commonName: infinilabs usages: - signing - key encipherment - server auth - client auth issuerRef: name: ca-issuer     可以在证书所在目录查看证书的有效期\nopenssl x509 -in tls.crt -dates -noout notBefore=Feb 23 16:02:03 2024 GMT notAfter=Mar 4 16:02:03 2025 GMT \n","subcategory":null,"summary":"","tags":null,"title":"证书管理","url":"/easysearch/v1.15.0/docs/getting-started/install/operator/cert_manager/"},{"category":null,"content":"配置文件 #  可以在每个 Easysearch 节点上找到 easysearch.yml , 通常为 Easysearch 安装目录下 config/easysearch.yml 。\n警告 #  切勿将未受保护的节点暴露在公共互联网上！\n常用网络设置： #  Easysearch 默认只绑定到 localhost。\n对于生产环境的集群，需要配置基本的网络设置。\n network.host: （静态）节点绑定的主机名或 IP 地址。默认为 local，同时设置了 network.bind_host 和 network.publish_host。 discovery.seed_hosts: （静态）初始集群节点列表。默认为 [\u0026ldquo;127.0.0.1\u0026rdquo;, \u0026ldquo;[::1]\u0026quot;]。 http.port: HTTP （静态）请求的绑定端口。默认为 9200-9300。 transport.port: （静态）节点间通信的绑定端口。默认为 9300-9400。 network.bind_host: （静态）节点监听传入请求的地址。可以配置为外网地址（例如 0.0.0.0 监听所有接口）或其他特定的地址。 network.publish_host: （静态）用于节点之间的通信，在多网卡或多网络环境中，应显式设置 network.publish_host。 discovery.seed_hosts: （静态）提供集群中有资格成为主节点的节点地址列表。也可以是一个包含多个以逗号分隔的地址的单个字符串。每个地址的格式为 host:port 或 host。 discovery.type: （静态）指定 Easysearch 是否应形成一个多节点集群。如果将 discovery.type 设置为 single-node，Easysearch 将形成一个单节点集群。 cluster.initial_master_nodes: （静态）设置全新集群中的初始主节点候选节点列表。默认情况下，此列表为空，意味着该节点期望加入已经引导好的集群， 在生产环境中首次启动一个全新的 Easysearch 集群时，必须配置 cluster.initial_master_nodes，以明确哪些节点有资格参与主节点的选举。 当集群完成了首次主节点选举，集群已经正常运行时，就不再需要 cluster.initial_master_nodes 设置了。这时应该从每个节点的配置中移除这项设置。 node.roles: （静态）定义节点的角色。节点默认具有以下角色：master, data, ingest, remote_cluster_client 如果设置了 node.roles，则节点只会被分配指定的角色。  分布式集群模式 #  以下配置适用于 Easysearch 的分布式集群模式，确保各节点可以发现彼此并组成一个集群，分布式模式建议 3 个独立的 Master 节点，配置如下：\ncluster.name: easysearch node.roles: [ \u0026quot;master\u0026quot; ] network.host: 0.0.0.0 http.port: 19201 transport.port: 19301 discovery.seed_hosts: [\u0026quot;192.168.101.5:19301\u0026quot;, \u0026quot;192.168.101.6:19302\u0026quot;, \u0026quot;192.168.101.7:19303\u0026quot;] cluster.initial_master_nodes: [\u0026quot;192.168.101.5:19301\u0026quot;, \u0026quot;192.168.101.6:19302\u0026quot;, \u0026quot;192.168.101.7:19303\u0026quot;] 数据节点配置如下：\ncluster.name: easysearch node.roles: [ \u0026quot;data\u0026quot; ] network.host: 0.0.0.0 http.port: 19200 transport.port: 19300 discovery.seed_hosts: [\u0026quot;192.168.101.5:19301\u0026quot;, \u0026quot;192.168.101.6:19302\u0026quot;, \u0026quot;192.168.101.7:19303\u0026quot;]  Master 使用 3 个就够了，数据节点可以根据动态增加。\n 单节点服务模式 #  如果希望单个 Easysearch 节点模拟集群，能监听所有网卡并提供对外服务供其他主机访问，也就是单节点模式，可以使用以下配置：\ncluster.name: easysearch network.host: 0.0.0.0 http.port: 13200 transport.port: 13300 cluster.initial_master_nodes: [\u0026quot;localhost:13300\u0026quot;] 了解更多，请查看 搭建集群\n","subcategory":null,"summary":"","tags":null,"title":"配置说明","url":"/easysearch/v1.15.0/docs/getting-started/configuration_file/"},{"category":null,"content":"版本升级 #  查看已有的版本：Easysearch:1.7.0-223\n现在准备升级到 Easysearch:1.7.1-225，修改 Operator yaml 中的 version 字段，并 apply\n# version: \u0026#34;1.7.0-223\u0026#34; version: \u0026#34;1.7.1-225\u0026#34; httpPort: 9200 vendor: Easysearch serviceAccount: controller-manager serviceName: threenodes 升级会比较久，因为为了保证升级过程中的服务可用性，节点升级是滚动升级的形式进行。 threenodes-masters-0 开始滚动更新，然后是 threenodes-masters-1，依次滚动更新， 直至所有节点更新完毕，大概总耗时 10 分钟\n查看 Easysearch 新版本可知为 1.7.1-225\n至此，版本升级完毕。\n  autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  ","subcategory":null,"summary":"","tags":null,"title":"版本升级","url":"/easysearch/v1.15.0/docs/getting-started/install/operator/upgrade/"},{"category":null,"content":"常用配置 #  大多数 Easysearch 配置都可以通过集群设置 API 进行更改，某些配置则需要修改 easysearch.yml 并重新启动集群。\n easysearch.yml 对每个节点都是本地的，因此应尽可能使用集群设置 REST API, 将设置应用于集群中的所有节点，一般我们采用开发者工具来进行操作。\n 集群设置 API #  第一步是查看当前设置：\nGET _cluster/settings?include_defaults=true 查看用户自进行的自定义设置\nGET _cluster/settings 集群设置 API 中存在三类设置：持久（Persistent）、临时（Transient）和默认。持久设置在集群重新启动后仍然存在。重新启动后，Easysearch 会清临时设置。\n如果在多个位置指定相同的设置，Easysearch 将使用以下优先级来读取配置：\n Transient 设置 Persistent 设置 配置文件 easysearch.yml 默认设置  要更改设置，只需将新设置指定为持久或临时。采用单层 JSON 形式：\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34; : { \u0026#34;action.auto_create_index\u0026#34; : false } } 同样也可以使用多层 JSON 形式：\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;action\u0026#34;: { \u0026#34;auto_create_index\u0026#34;: false } } }  配置目录包括许多安全相关的设置。要了解更多信息，请参阅 安全配置。\n","subcategory":null,"summary":"","tags":null,"title":"常用配置","url":"/easysearch/v1.15.0/docs/getting-started/configuration/"},{"category":null,"content":"密码修改 #  Operator 将 Easysearch 的密码保存在 k8s 的 Secret 中，查看已有的 Secret\n现在准备修改密码，编辑 admin-credentials-secret.yaml 文件，并 apply\napiVersion: v1 kind: Secret metadata: name: threenodes-admin-password type: Opaque data: # admin username: YWRtaW4= # admin123 password: YWRtaW4xMjM= operator 感知到 threenodes-admin-password 有变化后，会检查账号密码是否有更新（通过检查账号密码生成的 hash 值是否与 job 的 annotations: \u0026ldquo;securityconfig/checksum\u0026rdquo; 值相同来判断），如果有更新则重新执行 job（集群名称-securityconfig-update）,这里的名称是 threenodes-securityconfig-update\n继续查看这个 job 的 spec\n可以知道，这个 job 本质上也是 Easysearch，但是它执行 shell 命令来修改集群的密码：\nuntil curl -k -XPUT --cert admin-credentials/tls.crt --key admin-credentials/tls.key \\ -H \u0026#39;Content-Type: application/json\u0026#39; \u0026#39;https://threenodes.default.svc.cluster.local:9200/_security/user/admin \\ -d \u0026#39; { \u0026#34;password\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;external_roles\u0026#34;: [\u0026#34;admin\u0026#34;] }\u0026#39;;echo \u0026#39;Waiting to connect to the cluster\u0026#39;; sleep 60; 修改密码后，再次查看 secret： 可知密码已经修改为目标密码。\n继续测试一下新密码是否可用，依然进入 console-0，查看集群状态。发现原来的密码 admin 已经不可用，用新密码测试，新密码 admin123 为正确的密码，修改密码生效。\n  autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  ","subcategory":null,"summary":"","tags":null,"title":"密码修改","url":"/easysearch/v1.15.0/docs/getting-started/install/operator/update_password/"},{"category":null,"content":"节点扩容 #  与上述 cpu mem disk 扩容一样，只需要修改 Operator yaml 文件中的 replicas 字段值即可。 这里修改为 5 个节点，并 apply，将会并发创建新的节点：threenodes-masters-3, threenodes-masters-4\n最终完成节点扩容。\n  autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  ","subcategory":null,"summary":"","tags":null,"title":"节点扩容","url":"/easysearch/v1.15.0/docs/getting-started/install/operator/node_scale/"},{"category":null,"content":"系统调优 #  芯片及操作系统兼容性 #  目前已在国产主流芯片及操作系统上进行了验证，分别为 openEuler、统信 UOS、麒麟、龙芯、申威、兆芯。同样也兼容 Windows、 MacOS、 CentOS、 Ubuntu、 RedHat 等常用操作系统。\nJava 兼容性 #  默认情况下 Easysearch 并不包含 JDK, 推荐使用 Java 15.0.1+9 或 Java 17.0.6+10, 最低版本要求为 Java 11, 要使用不同的 Java 安装，请将 JAVA_HOME 环境变量设置为 Java 安装位置或将 JDK 软链接到 Easysearch 安装目录下取名为 jdk。\n例如：\n#设置 JAVA_HOME 环境变量，可放入 ~/.bashrc 或 /etc/profile export JAVA_HOME=/usr/local/jdk #软链接 sudo ln -s /usr/local/jdk /data/easysearch/jdk 网络要求 #  Easysearch 需要打开以下端口:\n   端口 模块说明     9200 REST API   9300 节点间通信    系统参数 #  要保证 Easysearch 运行在最佳状态，其所在服务器的操作系统也需要进行相应的调优，以 Linux 为例。\n 展开查看完整代码 ...  sudo tee /etc/security/limits.d/21-infini.conf \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; * soft nofile 1048576 * hard nofile 1048576 * soft memlock unlimited * hard memlock unlimited root soft nofile 1048576 root hard nofile 1048576 root soft memlock unlimited root hard memlock unlimited EOF     内核调优 #   展开查看完整代码 ...  cat \u0026lt;\u0026lt; SETTINGS | sudo tee /etc/sysctl.d/70-infini.conf fs.file-max = 10485760 fs.nr_open = 10485760 vm.max_map_count = 262145 net.core.somaxconn = 65535 net.core.netdev_max_backlog = 65535 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.core.rmem_max = 4194304 net.core.wmem_max = 4194304 net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1 net.ipv4.ip_local_port_range = 1024 65535 net.ipv4.conf.default.accept_redirects = 0 net.ipv4.conf.default.rp_filter = 1 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.send_redirects = 0 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_max_tw_buckets = 300000 net.ipv4.tcp_timestamps = 1 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn_backlog = 65535 net.ipv4.tcp_synack_retries = 0 net.ipv4.tcp_keepalive_intvl = 30 net.ipv4.tcp_keepalive_time = 900 net.ipv4.tcp_keepalive_probes = 3 net.ipv4.tcp_fin_timeout = 10 net.ipv4.tcp_max_orphans = 131072 net.ipv4.tcp_rmem = 4096 4096 16777216 net.ipv4.tcp_wmem = 4096 4096 16777216 net.ipv4.tcp_mem = 786432 3145728 4194304 SETTINGS     执行下面的命令验证配置参数是否合法。\nsysctl -p /etc/sysctl.d/70-infini.conf  您也可以重启操作系统，然后检查配置是否生效。\n ","subcategory":null,"summary":"","tags":null,"title":"系统调优","url":"/easysearch/v1.15.0/docs/getting-started/settings/"},{"category":null,"content":"数据流（Data streams） #  如果你正在将连续生成的时间序列数据（如日志、事件和指标）摄入 Easysearch，那么你很可能处于这样一种场景：文档数量快速增长，且你无需更新旧文档。\n管理时间序列数据的典型工作流程包含多个步骤，例如创建滚动索引别名、定义写入索引，以及为底层索引定义通用的映射和设置。\n数据流简化了这一过程，并强制采用最适合时间序列数据的配置方式，例如主要为仅追加（append-only）数据设计，并确保每个文档都包含一个时间戳字段。\n数据流在内部由多个底层索引组成。搜索请求会被路由到所有底层索引，而写入请求则被路由到最新的写入索引。通过 索引生命周期管理（ILM） 策略，你可以自动处理索引滚动（rollover）或删除操作。\n数据流使用说明 #  步骤 1：创建索引模板 #  要创建数据流，首先需要创建一个索引模板，用于将一组索引配置为数据流。data_stream 对象表明这是一个数据流，而非普通索引模板。索引模式需与数据流的名称匹配：\nPUT _index_template/logs-template-nginx { \u0026quot;index_patterns\u0026quot;: \u0026quot;logs-nginx\u0026quot;, \u0026quot;data_stream\u0026quot;: { }, \u0026quot;priority\u0026quot;: 200, \u0026quot;template\u0026quot;: { \u0026quot;settings\u0026quot;: { \u0026quot;number_of_shards\u0026quot;: 1, \u0026quot;number_of_replicas\u0026quot;: 0 } } } 在此情况下，每个摄入的文档都必须包含一个 @timestamp 字段。\n你也可以在 data_stream 对象中自定义时间戳字段名称。此外，你还可以在此处定义索引映射和其他设置，就像为普通索引模板所做的那样。\nPUT _index_template/logs-template-nginx { \u0026quot;index_patterns\u0026quot;: \u0026quot;logs-nginx\u0026quot;, \u0026quot;data_stream\u0026quot;: { \u0026quot;timestamp_field\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;request_time\u0026quot; } }, \u0026quot;priority\u0026quot;: 200, \u0026quot;template\u0026quot;: { \u0026quot;settings\u0026quot;: { \u0026quot;number_of_shards\u0026quot;: 1, \u0026quot;number_of_replicas\u0026quot;: 0 } } } 在此示例中，logs-nginx 索引会匹配 logs-template-nginx 模板。当存在多个匹配时，Easysearch 会选择优先级更高的模板。\n步骤 2：创建数据流 #  创建索引模板后，即可创建数据流。\n你可以使用数据流 API 显式创建数据流。该 API 将初始化第一个底层索引：\nPUT _data_stream/logs-nginx 你也可以直接开始摄入数据，而无需预先创建数据流。\n由于我们已有包含 data_stream 对象的匹配索引模板，Easysearch 将自动创建数据流：\nPOST logs-nginx/_doc { \u0026quot;message\u0026quot;: \u0026quot;login attempt failed\u0026quot;, \u0026quot;@timestamp\u0026quot;: \u0026quot;2013-03-01T00:00:00\u0026quot; } 要查看特定数据流的信息：\nGET _data_stream/logs-nginx 示例响应 #  { \u0026quot;data_streams\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;logs-nginx\u0026quot;, \u0026quot;timestamp_field\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;@timestamp\u0026quot; }, \u0026quot;indices\u0026quot;: [ { \u0026quot;index_name\u0026quot;: \u0026quot;.ds-logs-nginx-000001\u0026quot;, \u0026quot;index_uuid\u0026quot;: \u0026quot;Z8RATqnzTbGxbm-khb5LKw\u0026quot; }, { \u0026quot;index_name\u0026quot;: \u0026quot;.ds-logs-nginx-000002\u0026quot;, \u0026quot;index_uuid\u0026quot;: \u0026quot;o8xka3CxSjqzNrw1BoRP6A\u0026quot; } ], \u0026quot;generation\u0026quot;: 2, \u0026quot;status\u0026quot;: \u0026quot;GREEN\u0026quot;, \u0026quot;template\u0026quot;: \u0026quot;logs-template-nginx\u0026quot; } ] } 你可以看到时间戳字段的名称、底层索引列表、用于创建数据流的模板，以及数据流的健康状态（反映其所有底层索引中最低的状态）。\n要获取更多关于数据流的详细信息，请使用 _stats 端点：\nGET _data_stream/logs-nginx/_stats 示例响应 #  { \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 2, \u0026quot;successful\u0026quot;: 2, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;data_stream_count\u0026quot;: 1, \u0026quot;backing_indices\u0026quot;: 2, \u0026quot;total_store_size_bytes\u0026quot;: 18125, \u0026quot;data_streams\u0026quot;: [ { \u0026quot;data_stream\u0026quot;: \u0026quot;logs-nginx\u0026quot;, \u0026quot;backing_indices\u0026quot;: 2, \u0026quot;store_size_bytes\u0026quot;: 18125, \u0026quot;maximum_timestamp\u0026quot;: 1362096000000 } ] } 要查看所有数据流的信息，请使用以下请求：\nGET _data_stream 步骤 3：向数据流中摄入数据 #  你可以使用常规的索引 API 将数据摄入数据流。请确保每个索引的文档都包含时间戳字段。如果尝试摄入没有时间戳字段的文档，将会报错。\nPOST logs-nginx/_doc { \u0026quot;message\u0026quot;: \u0026quot;login attempt\u0026quot;, \u0026quot;@timestamp\u0026quot;: \u0026quot;2013-03-01T00:00:00\u0026quot; } 步骤 4：搜索数据流 #  你可以像搜索普通索引或索引别名一样搜索数据流。搜索操作将应用于所有底层索引（即数据流中的全部数据）。\nGET logs-nginx/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;login\u0026quot; } } } 示例响应 #  { \u0026quot;took\u0026quot; : 514, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 5, \u0026quot;successful\u0026quot; : 5, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.2876821, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;.ds-logs-redis-000001\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;-rhVmXoBL6BAVWH3mMpC\u0026quot;, \u0026quot;_score\u0026quot; : 0.2876821, \u0026quot;_source\u0026quot; : { \u0026quot;message\u0026quot; : \u0026quot;login attempt\u0026quot;, \u0026quot;@timestamp\u0026quot; : \u0026quot;2013-03-01T00:00:00\u0026quot; } } ] } } 步骤 5：滚动数据流 #  滚动操作会创建一个新的底层索引，并将其设置为数据流的新写入索引。\n要对数据流执行手动滚动操作：\nPOST logs-nginx/_rollover 示例响应 #  { \u0026quot;acknowledged\u0026quot;: true, \u0026quot;shards_acknowledged\u0026quot;: true, \u0026quot;old_index\u0026quot;: \u0026quot;.ds-logs-nginx-000001\u0026quot;, \u0026quot;new_index\u0026quot;: \u0026quot;.ds-logs-nginx-000002\u0026quot;, \u0026quot;rolled_over\u0026quot;: true, \u0026quot;dry_run\u0026quot;: false, \u0026quot;conditions\u0026quot;: {} } 现在，如果你对 logs-nginx 数据流执行 GET 操作，你会看到其生成（generation） ID 从 1 增加到了 2。\n你还可以设置一个 索引生命周期管理（ILM）策略 来自动执行数据流的滚动操作。ILM 策略在底层索引创建时即被应用。当你将策略关联到数据流时，它仅影响该数据流未来的底层索引。\n此外，你无需提供 rollover_alias 设置，因为 ILM 策略会从底层索引中自动推断此信息。\n步骤 7：删除数据流 #  删除操作会自动删除数据流的所有底层索引，然后删除数据流本身。\n要删除一个数据流及其所有隐藏的底层索引：\nDELETE _data_stream/\u0026lt;data_stream_name\u0026gt; 你可以使用通配符来删除多个数据流。\n我们建议使用 ILM 策略来删除数据流中的数据。\n","subcategory":null,"summary":"","tags":null,"title":"数据流","url":"/easysearch/v1.15.0/docs/references/management/data-streams/"},{"category":null,"content":"其它常用 API #  此页面包含 Easysearch 常用 API 的示例请求。\n使用非默认设置创建索引 #  PUT my-logs { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 4, \u0026#34;number_of_replicas\u0026#34;: 2 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;year\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } 索引单个文档并自动生成随机 ID #  POST my-logs/_doc { \u0026#34;title\u0026#34;: \u0026#34;Your Name\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;2016\u0026#34; } 索引单个文档并指定 ID #  PUT my-logs/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;Weathering with You\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;2019\u0026#34; } 一次索引多个文档 #  请求正文末尾的空白行是必填的。如果省略 _id 字段， Easysearch 将生成一个随机 id 。\nPOST _bulk { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;my-logs\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34; } } { \u0026#34;title\u0026#34;: \u0026#34;The Garden of Words\u0026#34;, \u0026#34;year\u0026#34;: 2013 } { \u0026#34;index\u0026#34; : { \u0026#34;_index\u0026#34;: \u0026#34;my-logs\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;3\u0026#34; } } { \u0026#34;title\u0026#34;: \u0026#34;5 Centimeters Per Second\u0026#34;, \u0026#34;year\u0026#34;: 2007 } 列出所有索引 #\n GET _cat/indices?v 打开或关闭与模式匹配的所有索引 #  POST my-logs*/_open POST my-logs*/_close 删除与模式匹配的所有索引 #  DELETE my-logs* 创建索引别名 #  此请求为索引 my-logs-2019-11-13 创建别名 my-logs-today 。\nPUT my-logs-2019-11-13/_alias/my-logs-today 列出所有别名 #  GET _cat/aliases?v 搜索单个索引或与模式匹配的所有索引 #  GET my-logs/_search?q=test GET my-logs*/_search?q=test 获取群集设置，包括默认值 #  GET _cluster/settings?include_defaults=true 更改磁盘使用限制 #  PUT _cluster/settings { \u0026#34;transient\u0026#34;: { \u0026#34;cluster.routing.allocation.disk.watermark.low\u0026#34;: \u0026#34;80%\u0026#34;, \u0026#34;cluster.routing.allocation.disk.watermark.high\u0026#34;: \u0026#34;85%\u0026#34; } } 获取群集运行状况 #  GET _cluster/health 列出群集中的节点 #  GET _cat/nodes?v 获取节点统计信息 #  GET _nodes/stats 在存储库中获取快照 #  GET _snapshot/my-repository/_all 生成快照 #  PUT _snapshot/my-repository/my-snapshot 从快照还原 #  POST _snapshot/my-repository/my-snapshot/_restore { \u0026#34;indices\u0026#34;: \u0026#34;-.security\u0026#34;, \u0026#34;include_global_state\u0026#34;: false } 统计索引每个字段的访问次数 #  GET metrics/_field_usage_stats 分析指定索引每个字段的磁盘占用大小 #  POST metrics/_disk_usage?run_expensive_tasks=true ","subcategory":null,"summary":"","tags":null,"title":"其它常用 API","url":"/easysearch/v1.15.0/docs/references/management/popular-api/"},{"category":null,"content":"权限列表 #  此页面是可用权限的完整列表。每个权限控制对数据类型或 API 的访问。\n集群权限 #   cluster:admin/ingest/pipeline/delete cluster:admin/ingest/pipeline/get cluster:admin/ingest/pipeline/put cluster:admin/ingest/pipeline/simulate cluster:admin/ingest/processor/grok/get cluster:admin/reindex/rethrottle cluster:admin/repository/delete cluster:admin/repository/get cluster:admin/repository/put cluster:admin/repository/verify cluster:admin/reroute cluster:admin/script/delete cluster:admin/script/get cluster:admin/script/put cluster:admin/settings/update cluster:admin/snapshot/create cluster:admin/snapshot/delete cluster:admin/snapshot/get cluster:admin/snapshot/restore cluster:admin/snapshot/status cluster:admin/snapshot/status* cluster:admin/tasks/cancel cluster:admin/tasks/test cluster:admin/tasks/testunblock cluster:monitor/allocation/explain cluster:monitor/health cluster:monitor/main cluster:monitor/nodes/hot_threads cluster:monitor/nodes/info cluster:monitor/nodes/liveness cluster:monitor/nodes/stats cluster:monitor/nodes/usage cluster:monitor/remote/info cluster:monitor/state cluster:monitor/stats cluster:monitor/task cluster:monitor/task/get cluster:monitor/tasks/list  索引权限 #   indices:admin/aliases indices:admin/aliases/exists indices:admin/aliases/get indices:admin/analyze indices:admin/cache/clear indices:admin/close indices:admin/create indices:admin/delete indices:admin/exists indices:admin/flush indices:admin/flush* indices:admin/forcemerge indices:admin/get indices:admin/mapping/put indices:admin/mappings/fields/get indices:admin/mappings/fields/get* indices:admin/mappings/get indices:admin/open indices:admin/refresh indices:admin/refresh* indices:admin/resolve/index indices:admin/rollover indices:admin/seq_no/global_checkpoint_sync indices:admin/settings/update indices:admin/shards/search_shards indices:admin/shrink indices:admin/synced_flush indices:admin/template/delete indices:admin/template/get indices:admin/template/put indices:admin/types/exists indices:admin/upgrade indices:admin/validate/query indices:data/read/explain indices:data/read/field_caps indices:data/read/field_caps* indices:data/read/get indices:data/read/mget indices:data/read/mget* indices:data/read/msearch indices:data/read/msearch/template indices:data/read/mtv indices:data/read/mtv* indices:data/read/scroll indices:data/read/scroll/clear indices:data/read/search indices:data/read/search* indices:data/read/search/template indices:data/read/tv indices:data/write/bulk indices:data/write/bulk* indices:data/write/delete indices:data/write/delete/byquery indices:data/write/index indices:data/write/reindex indices:data/write/update indices:data/write/update/byquery indices:monitor/recovery indices:monitor/segments indices:monitor/settings/get indices:monitor/shard_stores indices:monitor/stats indices:monitor/upgrade  权限集合 #  为了提高权限设置的效率，我们对系统权限进行自定义管理，从而快速批量选择一组相关的权限，而不是分别选择单个权限，为了方便，系统内置了若干权限集合。\n超级权限 #     名称 描述     unlimited 超级权限，集群和索引级别，等同于 \u0026quot;*\u0026quot; 。    集群级别 #     名称 描述     cluster_all 授予全部集群级别的权限，等同于 cluster:*。   cluster_monitor 授予全部集群的监控相关的权限，等同于 cluster:monitor/*。   cluster_composite_ops_ro 授予请求的只读权限，如 mget、msearch 或 mtv, 再加上别名的访问。   cluster_composite_ops 与 cluster_composite_ops_ro 相同，但是额外包括 bulk 和所有的别名权限。   manage_snapshots 授予所有和快照、仓库管理相关的权限。    索引级别 #     名称 描述     indices_all 授予全部索引级别的权限，等同于 indices:* .   get 授予 get 和 mget 操作的权限。   read 授予只读相关的权限，如搜索、获取字段 Mapping、get 和 mget 操作。   write 授予在已有索引里面创建和更新文档的权限。如果要创建新索引，参照 create_index 权限。   delete 授予删除文档的权限。   crud 增删改查的组合。   search 授予文档搜索的权限，包括 suggest 权限。   suggest 授予使用搜索提示 API 的权限，包括读取（read）权限。   create_index 授予创建索引和 Mapping 的权限。   indices_monitor 授予访问索引监控相关接口的权限 (如：recovery、segments、index stats 和 status 等)。   manage_aliases 授予管理别名的权限。   manage 授予所有索引监控和管理相关的权限。    ","subcategory":null,"summary":"","tags":null,"title":"权限列表","url":"/easysearch/v1.15.0/docs/references/security/access-control/permissions/"},{"category":null,"content":"数据汇总 #  数据汇总或上卷（Rollup），对于时序场景类的数据，往往会有大量的非常详细的聚合指标，随着时间的图推移，存储将持续增长。汇总功能可以将旧的、细粒度的数据汇总为粗粒度格式以进行长期存储。通过将数据汇总到一个单一的文档中，可以大大降低历史数据的存储成本。 Easysearch 的 rollup 具备一些独特的优势，可以自动对 rollup 索引进行滚动而不用依赖其他 API 去单独设置，并且在进行聚合查询时支持直接搜索原始索引，做到了对业务端的搜索代码完全兼容，从而对用户无感知。\n支持的聚合类型 #  对数值类型字段支持的聚合\n avg sum max min value_count percentiles  对 keyword 类型字段提供 terms 聚合。\n对 date 类型字段 除了 date_histogram 聚合，还支持 date_range 聚合。(v1.10.0)\n查询 rollup 数据时，增加支持 Filter aggregation，某些场景可以用来替代 query 过滤数据。(v1.10.1)\n增加针对个别字段自定义 special_metrics 指标的配置项。 (v1.10.1)\n增加支持 Bucket sort aggregation。 (v1.10.1)\n混合查询原始索引和 rollup 索引时，返回的 response 里增加了 origin 参数，表示包含 rollup 数据。(v1.10.1)\nRollup 查询 API 提供了 debug 参数，显示 Easysearch 内部执行的查询语句。(v1.10.1)\n使用汇总（rollup）的先决条件 #  必须安装索引生命周期管理插件，rollup 属于该插件功能的一部分。\n要汇总的指标索引必须具备 date 类型的字段。\n全局启用或禁用 rollup 搜索的设置 #  rollup 功能在 index-management 插件里， 从 1.10.0 版本开始，索引生命周期管理插件不再默认启用 rollup 搜索功能，如果想启用搜索 rollup 搜索功能，需要设置\nPUT /_cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;rollup.search.enabled\u0026quot;: true } } 通配符方式批量启动停止 rollup job #\n 从 1.10.0 版本开始，创建 rollup job 之后不再自动启动，需要手动启动，但是可以通过通配符的方式批量启动、停止 job\nPOST _rollup/jobs/rollup*/_start POST _rollup/jobs/rollup*/_stop\n设置 rollup 索引自动滚动的条数 #\n 达到此条数就会自动滚动新的索引\nPUT /_cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;rollup.max_docs\u0026quot;: 10000000 } } 新增 ROLLUP_SEARCH_MAX_COUNT 配置 #\n 从 1.10.0 版本开始，新增了 ROLLUP_SEARCH_MAX_COUNT 配置项，用于控制 Rollup 在运行 Job 时收集历史数据的最大并发分片请求数。这个配置项可以帮助你优化 Rollup 任务的性能，并避免集群资源过载。\n设置 Rollup 查询的时间范围上限 #  从 1.10.1 版本开始，可以通过 rollup.hours_before 集群配置项，设置 rollup 数据的查询时间范围上限，默认值是 1，表示 针对 rollup 数据的查询不会超过当前时间的 1 小时之前， 此参数的使用场景：\n 在混合查询时将 rollup 数据和原始数据的查询分割，防止查询的时间范围和数据不匹配。 限制 rollup 数据的查询时间上限。  使用示例：\nPUT /_cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;rollup.hours_before\u0026quot;: 24 } } 设置后，对 rollup 数据的查询范围上限不会超过 24 小时之前。\n功能： #   控制并发请求数：限制 Rollup 任务在执行搜索请求时的最大并发分片请求数。 动态调整：支持在集群运行时动态调整，无需重启集群。 默认值：2，即默认情况下，Rollup 任务最多会同时发送 2 个并发分片请求。  示例： #  PUT /_cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;rollup.search.max_count\u0026quot;: 2 } } 在这个例子中，ROLLUP_SEARCH_MAX_COUNT 被设置为 2，表示 Rollup 任务在执行搜索请求时，最多会同时发送 2 个并发分片请求。\n配置建议： #   小规模集群：建议设置为较小的值（如 2），以避免资源竞争。 大规模集群：可以适当增加该值（如 4），以提高并发性能。 动态调整：根据集群负载情况动态调整该值，以优化性能和资源利用率。  rollup 自动滚动后的索引列表 #  health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open rollup1_.infini_metrics-000001 aNET5la8QBa8AtCAtplPAw 1 0 179783 0 1gb 1gb green open rollup1_.infini_metrics-000002 7TDSIvWvSvKuKbB0DH-J0w 1 0 179170 0 1gb 1gb green open rollup1_.infini_metrics-000003 MhtBEj_-RgCg29gAbkbg6g 1 0 178297 0 1gb 1gb green open rollup1_.infini_metrics-000004 -Gfp80nUR5Cz-XnFy3F0rw 1 0 178297 0 1gb 1gb green open rollup1_.infini_metrics-000005 CuhhEf9SQ--yzMkG0QgLQw 1 0 177665 0 1gb 1gb green open rollup1_.infini_metrics-000006 9LqFJ28XRKexgwlMgIy3Bg 1 0 178624 0 1gb 1gb green open rollup1_.infini_metrics-000007 XkrPz8DDSRSq8xhnWo3nbA 1 0 180581 0 1gb 1gb green open rollup1_.infini_metrics-000008 xrfETulmT7OZnfcfVrIJTw 1 0 180050 0 1gb 1gb 汇总（rollup） API： #\n  创建或替换索引汇总任务 获取索引汇总任务 删除索引汇总任务 启动或停止索引汇总任务 解释索引汇总任务  创建或更新索引汇总任务 #  创建或自动替换一个索引汇总任务\nPUT _rollup/jobs/\u0026lt;rollup_id\u0026gt;?replace\n\u0026lt;rollup_id\u0026gt;: 为您的 Rollup 任务指定一个唯一标识符。\n?replace: 如果携带此参数，将替换已存在的 Rollup 所有配置，请谨慎使用。\n请求示例 #  下面的例子会创建一个自动滚动的 rollup 索引\nPUT _rollup/jobs/rollup_node_stats { \u0026quot;rollup\u0026quot;: { \u0026quot;source_index\u0026quot;: \u0026quot;.infini_metrics\u0026quot;, \u0026quot;target_index\u0026quot;: \u0026quot;rollup_node_stats_{{ctx.source_index}}\u0026quot;, \u0026quot;timestamp\u0026quot;: \u0026quot;timestamp\u0026quot;, \u0026quot;continuous\u0026quot;: true, \u0026quot;page_size\u0026quot;: 200, \u0026quot;cron\u0026quot;: \u0026quot;*/5 1-23 * * *\u0026quot;, \u0026quot;timezone\u0026quot;: \u0026quot;UTC+8\u0026quot;, \u0026quot;stats\u0026quot;: [ { \u0026quot;max\u0026quot;: {} }, { \u0026quot;min\u0026quot;: {} }, { \u0026quot;value_count\u0026quot;: {} } ], \u0026quot;interval\u0026quot;: \u0026quot;1m\u0026quot;, \u0026quot;identity\u0026quot;: [ \u0026quot;metadata.labels.cluster_id\u0026quot;, \u0026quot;metadata.labels.cluster_uuid\u0026quot;, \u0026quot;metadata.category\u0026quot;, \u0026quot;metadata.labels.node_id\u0026quot;, \u0026quot;metadata.labels.transport_address\u0026quot; ], \u0026quot;attributes\u0026quot;: [ \u0026quot;agent.*\u0026quot;, \u0026quot;metadata.*\u0026quot; ], \u0026quot;filter\u0026quot;: { \u0026quot;metadata.name\u0026quot;: \u0026quot;node_stats\u0026quot; }, \u0026quot;metrics\u0026quot;: [ \u0026quot;payload.elasticsearch.node_stats.*\u0026quot; ], \u0026quot;special_metrics\u0026quot;: [ { \u0026quot;source_field\u0026quot;: \u0026quot;payload.elasticsearch.node_stats.jvm.mem.heap_used_in_bytes\u0026quot;, \u0026quot;metrics\u0026quot;: [ { \u0026quot;avg\u0026quot;: {} }, { \u0026quot;max\u0026quot;: {} }, { \u0026quot;min\u0026quot;: {} }, { \u0026quot;percentiles\u0026quot;: {} } ] } ], \u0026quot;write_optimization\u0026quot;: true, \u0026quot;field_abbr\u0026quot;: true } } 参数详细说明 #     参数 解释 示例值说明     source_index 源索引，数据将从此索引中收集 .infini_metrics: 表示从名为 .infini_metrics 的索引中收集数据   target_index 目标索引，汇总数据将存储在此索引中 rollup1_{{ctx.source_index}}: 动态生成目标索引名，使用源索引名作为一部分   timestamp 用于时间聚合的时间戳字段 timestamp: 指定用于时间聚合的字段名   continuous 设置为 true，表示这是一个连续的 rollup 作业，默认为false true: 任务将持续运行，而不是一次性执行   page_size 每次处理的文档数量 1000: 每批处理1000个文档，可根据系统性能调整   cron 作业运行的调度表达式 */10 1-23 * * *: 每10分钟执行一次，在1点到23点之间   timezone 用于 cron 表达式的时区 UTC+8: 使用UTC+8时区（如北京时间）   stats 要计算的统计信息 示例中计算最大值和计数，可根据需求添加其他统计如avg, min等   interval 时间聚合的间隔 1m: 每分钟聚合一次数据   identity 标识字段，用于分组聚合 列出的字段将用于创建唯一的聚合组   attributes 要包含在 rollup 中的属性字段 agent.*, metadata.*: 包含所有以agent和metadata开头的字段   metrics 要汇总的指标字段 payload.elasticsearch.index_stats.*: 汇总所有index_stats下的指标   special_metrics 对特别关注的指标配置更丰富的统计方式 专门对 JVM 堆内存使用量这个字段计算 avg max min percentiles 4 个指标，不再计算 stats 里的默认指标   exclude 要排除的指标字段 payload.elasticsearch.index_stats.routing.*: metrics 会排除符合模式的字段   filter 用于过滤源文档的条件 {\u0026quot;metadata.name\u0026quot;: \u0026quot;index_stats\u0026quot;}: 只处理metadata.name为index_stats的文档   write_optimization 启用后采用自动生成文档 ID 的策略，提升写入速度 true: 启用写入优化（1.12.0 版本增加）   field_abbr 启用字段名缩写，可降低内存消耗 true: 启用字段名缩写（1.12.0 版本增加）   is_continue 当创建的 Rollup 已经有历史数据并要断点续跑时配置 true: 启用断点续跑，默认不配置为 false（1.12.3 版本增加）    注意事项 #   cron 表达式需要谨慎设置，确保不会对系统造成过大负担。 page_size 应根据您的系统性能和数据量来调整。 identity 字段的选择会影响聚合的粒度，请根据您的分析需求仔细选择。 使用通配符（如 agent.*）时要注意，这可能会包含大量字段，影响性能。 filter 可以有效减少处理的数据量，提高效率。 write_optimization 和 field_abbr 是从 1.12.0 新增的参数。  获取索引汇总任务 #  请求\nGET _rollup/jobs/\u0026lt;rollup_id\u0026gt; 响应\n{ \u0026quot;_id\u0026quot;: \u0026quot;rollup4\u0026quot;, \u0026quot;_version\u0026quot;: 3, \u0026quot;_seq_no\u0026quot;: 5, \u0026quot;_primary_term\u0026quot;: 1, \u0026quot;rollup\u0026quot;: { \u0026quot;rollup_id\u0026quot;: \u0026quot;rollup4\u0026quot;, \u0026quot;enabled\u0026quot;: false, \u0026quot;schedule\u0026quot;: { \u0026quot;interval\u0026quot;: { \u0026quot;start_time\u0026quot;: 1718105163766, \u0026quot;period\u0026quot;: 1, \u0026quot;unit\u0026quot;: \u0026quot;Minutes\u0026quot;, \u0026quot;schedule_delay\u0026quot;: 0 } }, \u0026quot;last_updated_time\u0026quot;: 1718105163767, \u0026quot;enabled_time\u0026quot;: null, \u0026quot;description\u0026quot;: \u0026quot;Example rollup job\u0026quot;, \u0026quot;schema_version\u0026quot;: 17, \u0026quot;source_index\u0026quot;: \u0026quot;test-data\u0026quot;, \u0026quot;target_index\u0026quot;: \u0026quot;rollup4-test\u0026quot;, \u0026quot;metadata_id\u0026quot;: \u0026quot;nuMNB5ABnvQbLaVPM5qO\u0026quot;, \u0026quot;page_size\u0026quot;: 200, \u0026quot;delay\u0026quot;: 0, \u0026quot;continuous\u0026quot;: false, \u0026quot;dimensions\u0026quot;: [ { \u0026quot;date_histogram\u0026quot;: { \u0026quot;fixed_interval\u0026quot;: \u0026quot;1m\u0026quot;, \u0026quot;source_field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;target_field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;timezone\u0026quot;: \u0026quot;UTC\u0026quot; } } ], \u0026quot;metrics\u0026quot;: [ { \u0026quot;source_field\u0026quot;: \u0026quot;passenger_count\u0026quot;, \u0026quot;metrics\u0026quot;: [ { \u0026quot;avg\u0026quot;: {} }, { \u0026quot;sum\u0026quot;: {} }, { \u0026quot;max\u0026quot;: {} }, { \u0026quot;min\u0026quot;: {} }, { \u0026quot;value_count\u0026quot;: {} } ] } ] } } 删除索引汇总任务 #  请求\nDELETE _rollup/jobs/\u0026lt;rollup_id\u0026gt; 响应\n{ \u0026quot;_index\u0026quot;: \u0026quot;.easysearch-ilm-config\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;rollup4\u0026quot;, \u0026quot;_version\u0026quot;: 7, \u0026quot;result\u0026quot;: \u0026quot;deleted\u0026quot;, \u0026quot;forced_refresh\u0026quot;: true, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 2, \u0026quot;successful\u0026quot;: 1, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;_seq_no\u0026quot;: 23, \u0026quot;_primary_term\u0026quot;: 1 } 启动或停止索引汇总任务 #  请求\nPOST _rollup/jobs/\u0026lt;rollup_id\u0026gt;/_start POST _rollup/jobs/\u0026lt;rollup_id\u0026gt;/_stop 响应\n{ \u0026quot;acknowledged\u0026quot;: true } 解释索引汇总任务 #  请求\nGET _rollup/jobs/\u0026lt;rollup_id\u0026gt;/_explain 响应\n{ \u0026quot;rollup1\u0026quot;: { \u0026quot;metadata_id\u0026quot;: \u0026quot;VCAnj5IBQpC3NDmZwgL7\u0026quot;, \u0026quot;rollup_metadata\u0026quot;: { \u0026quot;rollup_id\u0026quot;: \u0026quot;rollup1\u0026quot;, \u0026quot;last_updated_time\u0026quot;: 1729151524871, \u0026quot;continuous\u0026quot;: { \u0026quot;next_window_start_time\u0026quot;: 1728196860000, \u0026quot;next_window_end_time\u0026quot;: 1728196920000 }, \u0026quot;status\u0026quot;: \u0026quot;started\u0026quot;, \u0026quot;failure_reason\u0026quot;: null, \u0026quot;stats\u0026quot;: { \u0026quot;pages_processed\u0026quot;: 77014, \u0026quot;documents_processed\u0026quot;: 135026197, \u0026quot;rollups_indexed\u0026quot;: 26812185, \u0026quot;index_time_in_millis\u0026quot;: 38094616, \u0026quot;search_time_in_millis\u0026quot;: 88930100 } } } }    字段名 类型 描述     metadata_id string rollup 元数据的唯一标识符   rollup_metadata.rollup_id string rollup 作业的 ID   rollup_metadata.last_updated_time long 上次更新时间的 Unix 时间戳（毫秒）   rollup_metadata.continuous.next_window_start_time long 下一个 rollup 窗口的开始时间（Unix 时间戳，毫秒）   rollup_metadata.continuous.next_window_end_time long 下一个 rollup 窗口的结束时间（Unix 时间戳，毫秒）   rollup_metadata.status string rollup 作业当前的状态   rollup_metadata.failure_reason string null   rollup_metadata.stats.pages_processed long 已处理的页面数   rollup_metadata.stats.documents_processed long 已处理的文档数   rollup_metadata.stats.rollups_indexed long 已索引的 rollup 文档数   rollup_metadata.stats.index_time_in_millis long 索引所用的总时间（毫秒）   rollup_metadata.stats.search_time_in_millis long 搜索所用的总时间（毫秒）    修改已经运行 Job 的 interval 和 page_size 参数 (1.13.0) #  在线上环境运行 Rollup 时会遇到这种情况：由于机器硬件资源或其他原因发现某个 Rollup 压力较大，需要调整最小时间区间或单次请求条数。\n从 1.13.0 版本开始，可以使用 POST _rollup/jobs/\u0026lt;rollup_id\u0026gt; API 进行更新，此操作内部会自动执行\n 停止 Job 更新 参数 重启 Job 运行 Job 时 索引数据会增加 unique 字段标识当前 Job 批次的数据，方便数据排重  请求\nPOST _rollup/jobs/rollup_node_stats { \u0026quot;page_size\u0026quot;: 100, \u0026quot;interval\u0026quot;: \u0026quot;5m\u0026quot; } 响应\n{ \u0026quot;acknowledged\u0026quot;: true } 如何使用 rollup 索引进行搜索 #  无需特意搜索 rollup 索引，只需使用标准的 search API 对原始目标索引进行搜索。查询时，请确保查询符合目标索引的约束条件。 例如，如果你没有在某个字段上设置 terms 聚合，那么你不会收到该字段的 terms 聚合结果。如果你没有设置 maximum 聚合，也不会收到 maximum 聚合的结果。\n无法访问目标索引中数据的内部结构，因为插件会在后台自动重写查询以适应目标索引。这是为了确保对源索引和目标索引使用相同的查询。\n请求示例\nGET target-test/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;a\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;fixed_interval\u0026quot;: \u0026quot;1h\u0026quot; } }, \u0026quot;total_passenger_count\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;passenger_count\u0026quot; } } } } 响应\n{ \u0026quot;took\u0026quot;: 3, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 2, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;a\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2023-08-01T10:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1690884000000, \u0026quot;doc_count\u0026quot;: 10 } ] }, \u0026quot;total_passenger_count\u0026quot;: { \u0026quot;value\u0026quot;: 38 } } } 注意 #  不能在同一个搜索请求中同时搜索汇总索引和非汇总索引。\n为自定义用户赋予rollup 权限 #  以下示例表示创建了一个自定义用户，这个用户只对 test 开头的索引具备所有权限，并赋予他 rollup 的权限，授权后，test_user将可以创建 rollup job，并对rollup 开头的索引有查询权限。\nPUT /_security/role/test_role { \u0026quot;indices\u0026quot;: [ { \u0026quot;names\u0026quot;: [ \u0026quot;test*\u0026quot;], \u0026quot;privileges\u0026quot;: [ \u0026quot;*\u0026quot; ] } ] } PUT /_security/user/test_user { \u0026quot;password\u0026quot; : \u0026quot;123456\u0026quot;, \u0026quot;roles\u0026quot; : [ \u0026quot;test_role\u0026quot;,\u0026quot;rollup_all\u0026quot; ] } 断点续跑 (1.13.0) #\n 创建Rollup 时如果检测到历史索引及元数据，会自动从中断的状态点继续处理，避免重复计算。\nRollup 配置增加了 window_start_time 字段，当重建 Rollup 时 会把历史 metadata 的最新时间戳自动写到 window_start_time 里， 通过观察 window_start_time 字段的值可以判断当前 Job 开始的 ’断点时间‘。\n","subcategory":null,"summary":"","tags":null,"title":"数据汇总","url":"/easysearch/v1.15.0/docs/references/management/rollup_api/"},{"category":null,"content":"使用时间范围合并策略优化时序索引 #  在处理时序数据（如日志、监控指标、事件流）时，数据通常具有明显的时间先后顺序。Easysearch 底层的 Lucene Segment 合并是保证搜索性能和资源效率的关键操作。 然而，默认的合并策略（如 TieredMergePolicy）主要基于 Segment 的大小和删除文档比例来决定合并哪些 Segment，它并不感知数据的时间属性。\n对于时序场景，这种默认策略可能导致：\n冷热数据混合合并：较旧的（冷）数据 Segment 可能与较新的（热）数据 Segment 合并，导致不必要的 I/O 和 CPU 开销，因为冷数据通常访问频率低，合并它们带来的收益有限。\n查询性能影响：跨时间范围的大 Segment 可能降低某些按时间范围过滤的查询效率。\n为了解决这些问题，Easysearch 从 1.12.1 版本开始引入了基于时间范围的合并策略 (TimeRangeMergePolicy)，专门为时序索引优化 Segment 合并行为。\n最低版本 #  1.12.1\n核心概念：TimeRangeMergePolicy #  TimeRangeMergePolicy 是一种特殊的合并策略，它在选择要合并的 Segment 时，除了考虑大小、删除比例等因素外，优先考虑 Segment 所覆盖的时间范围。\n其核心思想是：\n时间优先：倾向于合并时间上相邻的 Segment。\n保留时间分区：尽量避免将时间跨度很大的 Segment 合并在一起，保持数据的“时间局部性”。\n优先合并新数据：通常，新写入的数据变化更频繁（包括更新、删除），优先合并较新的 Segment 有助于更快地回收空间和优化最新数据的查询性能。\n如何启用 #  要为你的时序索引启用时间范围合并策略，你需要更新索引的设置，指定用于时间排序的字段名。\n步骤： #   确认时间字段：确保你的索引 Mapping 中有一个合适的日期或时间戳类型的字段（如 @timestamp、event_time 等），并且该字段准确反映了数据的时间属性。 更新索引设置：使用 Index Settings API 来设置 index.merge.policy.time_range_field。  示例： #  假设你的时间字段是 @timestamp，索引名称是 my-timeseries-index：\nPUT /my-timeseries-index/_settings { \u0026quot;index\u0026quot;: { \u0026quot;merge.policy.time_range_field\u0026quot;: \u0026quot;timestamp\u0026quot; } } 设置完成后，该索引后续的 Segment 合并操作将自动采用 TimeRangeMergePolicy。如果你想对新创建的索引默认启用，可以将此设置添加到索引模板（Index Template）中。\n优势 #  降低合并开销：显著减少冷热数据混合合并，降低不必要的 I/O 和 CPU 消耗。\n提高资源效率：通过更智能地合并，可能降低存储空间占用（更快回收已删除空间）和计算资源使用。\n优化查询性能：保持 Segment 的时间局部性，可能提升涉及时间范围过滤的查询速度。\n对时序数据友好：特别适合日志、指标等严格按时间增长的数据模式。\n注意事项 #  时间字段依赖：必须正确指定一个能代表数据时间的字段。如果时间字段不存在或数据时间分布混乱，该策略效果可能不佳。\n适用场景：最适用于具有明显时间序列特征的数据。对于非时序数据，默认的 TieredMergePolicy 可能更合适。\nSegment 时间戳精度：Segment 元数据中记录的时间戳取决于索引时使用的 Lucene 版本和字段类型，通常能精确到毫秒。\n","subcategory":null,"summary":"","tags":null,"title":"使用时间范围合并策略优化时序索引","url":"/easysearch/v1.15.0/docs/references/management/time-series-Index-optimization/"},{"category":null,"content":"可搜索快照 #  可搜索快照索引在实时搜索时从快照存储库中按需读取数据，而不是在恢复时将所有索引数据下载到集群存储中。由于索引数据仍然保持在快照格式中存储在存储库中，因此可搜索快照索引本质上是只读的。 任何尝试写入可搜索快照索引的操作都会导致错误。\n可搜索快照功能采用了诸如在集群节点中缓存频繁使用的数据段以及删除集群节点中最不常使用的数据段等技术，以便为频繁使用的数据段腾出空间。从快照下载的数据段存储在块存储中，与集群节点的通用索引并存。 因此，集群节点的计算能力在索引、本地搜索和存储在低成本对象存储，例如 Amazon Simple Storage Service（Amazon S3）上的快照数据段之间共享。 尽管集群节点的资源利用效率要高得多，但大量的任务将导致快照搜索变得较慢且持续时间较长。节点的本地存储也用于缓存快照数据。\n将节点配置为使用可搜索快照 #  只有角色为 search 的节点才能进行快照搜索，要启用可搜索快照功能，请在您的 easysearch.yml 文件中创建一个节点，并将节点角色定义为 \u0026ldquo;search\u0026rdquo;：\nnode.name: snapshots-node node.roles: [search] 如果您正在运行 Docker，可以通过在您的 docker-compose.yml 文件中添加以下行来创建一个具有搜索节点角色的节点：\n- node.roles: [search] 创建可搜索的快照索引 #  可搜索的快照索引是通过使用 _restore API 并指定 remote_snapshot 存储类型来创建的。\nstorage_type:\n local 表示所有快照的元数据和索引数据都将下载到本地存储。 remote_snapshot 表示快照的元数据将下载到集群，但远程存储库将保持索引数据的权威存储。数据将根据需要下载和缓存以提供查询服务。为了使用 remote_snapshot 类型还原快照，集群中至少必须配置一个节点具有 search 节点角色。  使用示例 #  下面我们以 MinIO 作为快照存储仓库，Minio 是专为云应用程序开发人员和 DevOps 构建的对象存储服务器，与 Amazon S3 对象存储兼容。\n使用 _snapshot API 注册 MinIO 存储库\ncurl -XPUT \u0026quot;http://localhost:9200/_snapshot/my-minio-repository\u0026quot; -H 'Content-Type: application/json' -d' { \u0026quot;type\u0026quot;: \u0026quot;s3\u0026quot;, \u0026quot;settings\u0026quot;: { \u0026quot;access_key\u0026quot;: \u0026quot;minioadmin\u0026quot;, \u0026quot;secret_key\u0026quot;: \u0026quot;minioadmin\u0026quot;, \u0026quot;bucket\u0026quot;: \u0026quot;es-bucket\u0026quot;, \u0026quot;endpoint\u0026quot;: \u0026quot;http://127.0.0.1:9000\u0026quot;, \u0026quot;compress\u0026quot;: true } }' 将事先准备好的 nginx_default_1g 索引备份到快照\ncurl -XPUT \u0026quot;http://localhost:920/_snapshot/my-minio-repository/4\u0026quot; -H 'Content-Type: application/json' -d' { \u0026quot;indices\u0026quot;: \u0026quot;nginx_default_1g\u0026quot;, \u0026quot;ignore_unavailable\u0026quot;: true, \u0026quot;include_global_state\u0026quot;: false, \u0026quot;partial\u0026quot;: false }' 恢复可搜索的快照索引，关键的配置是将 storage_type 设置为 remote_snapshot\ncurl -XPOST \u0026quot;http://localhost:9211/_snapshot/my-minio-repository/4/_restore\u0026quot; -H 'Content-Type: application/json' -d' { \u0026quot;indices\u0026quot;: \u0026quot;nginx_default_1g\u0026quot;, \u0026quot;ignore_unavailable\u0026quot;: true, \u0026quot;include_global_state\u0026quot;: false, \u0026quot;include_aliases\u0026quot;: false, \u0026quot;partial\u0026quot;: false, \u0026quot;storage_type\u0026quot;: \u0026quot;remote_snapshot\u0026quot;, \u0026quot;rename_pattern\u0026quot;: \u0026quot;nginx_default_1g\u0026quot;, \u0026quot;rename_replacement\u0026quot;: \u0026quot;remote_nginx_default_1g\u0026quot; }' 然后 我们就可以像本地索引一样进行搜索了。\n","subcategory":null,"summary":"","tags":null,"title":"可搜索快照","url":"/easysearch/v1.15.0/docs/references/management/searchable_snapshot/"},{"category":null,"content":"写入限流 #  Easysearch 支持节点级别和分片级别的写入限流功能，可以将 bulk 操作对集群的压力，限制在可接受的范围。\n最低版本 #  1.8.0\n限流参数设置 #  以下是 Easysearch 集群级别的限流设置，并且是动态的，您可以更改此功能的默认行为，而无需重新启动集群。\n限流参数说明\n   名称 类型 说明 默认值     cluster.throttle.node.write boolean 是否启用节点级别限流 false   cluster.throttle.node.write.max_requests int 限定时间范围内单个节点允许的最大写入请求次数 0   cluster.throttle.node.write.max_bytes 字符串 限定时间范围内单个节点允许的最大写入请求字节数（kb, mb, gb 等） 0mb   cluster.throttle.node.write.action 字符串 触发限速之后的处理动作，分为 retry 和 drop 两种，默认为 drop drop   cluster.throttle.node.write.interval int 节点级别评估限速的单位时间间隔，默认为 1s 1   cluster.throttle.shard.write boolean 是否启用分片级别限流 false   cluster.throttle.shard.write.max_requests int 限定时间范围内单个分片允许的最大写入请求次数 0   cluster.throttle.shard.write.max_bytes 字符串 限定时间范围内单个分片允许的最大写入请求字节数（kb, mb, gb 等） 0mb   cluster.throttle.shard.write.action 字符串 触发限速之后的处理动作，分为 retry 和 drop 两种，默认为 drop drop   cluster.throttle.shard.write.interval int 分片级别评估限速的单位时间间隔，默认为 1s 1   index.throttle.write.enable boolean 是否针对当前索引启用写入限流（索引 Settings 配置） false   index.throttle.write.max_requests int 限定时间范围内当前索引允许的最大写入请求次数（索引 Settings 配置） 0   index.throttle.write.max_bytes 字符串 限定时间范围内当前索引允许的最大写入请求字节数（kb, mb, gb 等）（索引 Settings 配置） 0mb   index.throttle.write.action 字符串 触发限速之后的处理动作，分为 retry 和 drop 两种，默认为 drop（索引 Settings 配置） drop   index.throttle.write.interval int 当前索引评估限速的单位时间间隔，默认为 1s（索引 Settings 配置） 1    使用示例 #  节点级别限流 #  PUT _cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;cluster.throttle.node.write\u0026quot;: true, \u0026quot;cluster.throttle.node.write.max_bytes\u0026quot;: \u0026quot;50MB\u0026quot;, \u0026quot;cluster.throttle.node.write.max_requests\u0026quot;: 1000000, \u0026quot;cluster.throttle.node.write.action\u0026quot;: \u0026quot;retry\u0026quot; } } 以上配置表示开启节点限流功能，限定时间范围内单个节点允许最大写入50MB的数据，并且写入条数限制在100万，超过设定的阈值后会持续重试1秒钟，实际流量计算会稍有偏差。\n分片级别限流 #  PUT _cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;cluster.throttle.shard.write\u0026quot;: true, \u0026quot;cluster.throttle.shard.write.max_bytes\u0026quot;: \u0026quot;50MB\u0026quot;, \u0026quot;cluster.throttle.shard.write.max_requests\u0026quot;: 1000000, \u0026quot;cluster.throttle.shard.write.action\u0026quot;: \u0026quot;drop\u0026quot; } } 以上配置表示开启分片限流功能，限定时间范围内单个分片允许最大写入50MB的数据，并且写入条数限制在100万，超过设定的阈值后会立即拒绝写入，返回 rejected execution， 实际流量计算会稍有偏差。\n索引级别限流 #  有时，我们需要只针对个别索引进行写入限流，又不想影响其他索引的写入速度，可以在创建索引时在 Settings 里指定相应的限流配置项：\nPUT test_0 { \u0026quot;settings\u0026quot;: { \u0026quot;number_of_replicas\u0026quot;: 1, \u0026quot;number_of_shards\u0026quot;: 3, \u0026quot;index.throttle.write.max_requests\u0026quot;: 6000, \u0026quot;index.throttle.write.action\u0026quot;: \u0026quot;retry\u0026quot;, \u0026quot;index.throttle.write.enable\u0026quot;: true } } 注意事项 #  节点级别限流是针对所有 DataNode。\n分片级别限流只计算从协调节点分发到数据节点主分片的 bulk 请求。\n节点级别和分片级别限流不冲突，可以同时启用。\n限流功能不会限制系统索引流量，只针对业务索引。\n","subcategory":null,"summary":"","tags":null,"title":"写入限流","url":"/easysearch/v1.15.0/docs/references/management/throttling/"},{"category":null,"content":"备份还原 #  快照是集群索引和状态的备份。状态包括集群设置、节点信息、索引设置和分片的信息。\n快照有两个主要用途：\n  从故障中恢复\n例如，如果集群运行状况变为红色，则可以从快照恢复红色索引。\n  从一个群集迁移到另一个群集\n例如，如果您要从概念验证迁移到生产集群，您可以拍摄前者的快照并在后者上进行恢复。\n  关于快照(snapshots) #  快照不是即时的。它们需要时间来完成，并不代表集群的完美时间点视图。当快照正在进行时，您仍然可以为文档编制索引并向集群发出其他请求，但快照中通常不包括新文档和对现有文档的更新。快照包括 Easysearch 启动快照时存在的主碎片。根据快照线程池的大小，快照中可能会在稍微不同的时间包含不同的碎片。\nEasysearch 快照是增量的，这意味着它们只存储自上次成功快照以来已更改的数据。频繁快照和不频繁快照之间的磁盘使用率差异通常很小。\n换句话说，一周内每小时拍摄一次快照（总共拍摄 168 个快照）可能不会比周末拍摄一个快照占用更多的磁盘空间。此外，拍摄快照的频率越高，完成快照所需的时间越短。一些 Easysearch 用户每半小时拍摄一次快照。\n如果需要删除快照，请确保使用 Easysearch API，而不是导航到存储位置并清除文件。集群中的增量快照通常共享大量相同的数据；使用 API 时， Easysearch 仅删除其他快照未使用的数据。 {: .tip }\n注册快照存储库 #  在拍摄快照之前，必须“注册”快照存储库。快照存储库只是一个存储位置：共享文件系统、 Amazon S3 、 Hadoop 分布式文件系统（HDFS）、 Azure 存储等。\nShared file system #   要将共享文件系统用作快照存储库，请将其添加到 easysearch.yml：  path.repo: [\u0026#34;/mnt/snapshots\u0026#34;] 在 RPM 和 Debian 安装中，您可以安装文件系统。如果您使用 Docker 安装，请在启动集群之前，将文件系统添加到 docker-compose.yml 中的每个节点：\nvolumes: - /Users/jdoe/snapshots:/mnt/snapshots  T1.然后使用 REST API 注册存储库：  PUT _snapshot/my-fs-repository { \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;/mnt/snapshots\u0026#34; } } 如果请求成功，Easysearch 的响应很小：\n{ \u0026#34;acknowledged\u0026#34;: true } 您可能只需要指定 location ，但下表总结了选项：\n   Setting Description     location 快照的共享文件系统。必选的。   chunk_size 在快照操作期间将大型文件分成块（例如 64mb 、 1gb ），这对于云存储提供商来说很重要，而对于共享文件系统来说则不那么重要。默认值为 null（unlimited）。可选的   compress 是否压缩元数据文件。此设置不会影响数据文件，数据文件可能已被压缩，具体取决于索引设置。默认值为 false 。可选的   max_restore_bytes_per_sec 快照恢复的最大速率。默认值为每秒 40 MB（ 40m ）。可选的   max_snapshot_bytes_per_sec 快照 s 生成的最大速率。默认值为每秒 40 MB（ 40m ）。可选的   readonly 存储库是否为只读。当从一个集群（ \u0026quot;readonly\u0026quot;: false ）迁移到另一个集群时（ \u0026quot;readonly\u0026quot;: true ）非常有用。可选的    Amazon S3 #   将 AWS 访问权限和密钥添加到 Easysearch 密钥库：  sudo ./bin/easysearch-keystore add s3.client.default.access_key sudo ./bin/easysearch-keystore add s3.client.default.secret_key (Optional) 如果您使用的是临时凭据，请添加会话令牌：  sudo ./bin/easysearch-keystore add s3.client.default.session_token (Optional) 如果您通过代理连接到 internet，请添加这些凭据：  sudo ./bin/easysearch-keystore add s3.client.default.proxy.username sudo ./bin/easysearch-keystore add s3.client.default.proxy.password (Optional) 将其他设置添加到 easysearch.yml:  s3.client.default.disable_chunked_encoding: false # Disables chunked encoding for compatibility with some storage services, but you probably don\u0026#39;t need to change this value. s3.client.default.endpoint: s3.amazonaws.com # S3 has alternate endpoints, but you probably don\u0026#39;t need to change this value. s3.client.default.max_retries: 3 # number of retries if a request fails s3.client.default.path_style_access: false # whether to use the deprecated path-style bucket URLs. # You probably don\u0026#39;t need to change this value, but for more information, see https://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html#path-style-access. s3.client.default.protocol: https # http or https s3.client.default.proxy.host: my-proxy-host # the hostname for your proxy server s3.client.default.proxy.port: 8080 # port for your proxy server s3.client.default.read_timeout: 50s # the S3 connection timeout s3.client.default.use_throttle_retries: true # whether the client should wait a progressively longer amount of time (exponential backoff) between each successive retry 如果更改了 easysearch.yml ，则必须重新启动集群中的每个节点。否则，您只需要重新加载安全集群设置：   POST _nodes/reload_secure_settings 如果您还没有 S3 bucket ，请创建一个。要生成快照，您需要访问 bucket 的权限。以下 IAM 策略是这些权限的示例：  { \u0026#34;Version\u0026#34;: \u0026#34;2022-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [\u0026#34;s3:*\u0026#34;], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;arn:aws:s3:::your-bucket\u0026#34;, \u0026#34;arn:aws:s3:::your-bucket/*\u0026#34;] } ] } 使用 REST API 注册存储库：  PUT _snapshot/my-s3-repository { \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;bucket\u0026#34;: \u0026#34;my-s3-bucket\u0026#34;, \u0026#34;base_path\u0026#34;: \u0026#34;my/snapshot/directory\u0026#34; } } 除了 bucket 和 base_path 之外，您可能不需要指定任何内容，但下表总结了选项：\n   Setting Description     base_path 桶中要存储快照的路径（e.g. my/snapshot/directory ）。可选择的如果未指定，快照将存储在存储桶根目录中。   bucket S3 桶的名称。必选。   buffer_size 超过阈值的块（ chunk_size ）应被分割成块（ buffer_size ）并使用不同的 API 发送到 S3。默认值是两个值中的较小值：100 MB 或 Java 堆的 5% 。有效值介于 5mb 和 5gb 之间。我们不建议更改此选项。   canned_acl S3 有 canned ACLs， repository-s3 插件可以在 s3 中创建对象时将其添加到对象中。默认值为 private 。可选的   chunk_size 在快照操作期间将文件分成块（e.g. 64mb , 1gb ），这对于云存储提供商来说很重要，而对于共享文件系统来说则不那么重要。默认值为 1gb 。可选的   client 在指定客户端设置（e.g. s3.client.default.access_key ）时，可以使用除 default 之外的字符串（e.g. s3.client.backup-role.access_key）。如果使用了备用名称，请更改此值以匹配。默认值和推荐值为 default 。可选的   compress 是否压缩元数据文件。此设置不会影响数据文件，数据文件可能已被压缩，具体取决于索引设置。默认值为 false 。可选的   max_restore_bytes_per_sec 快照恢复的最大速率。默认值为每秒 40 MB（ 40m ）。可选的   max_snapshot_bytes_per_sec 快照拍摄的最大速率。默认值为每秒 40 MB（ 40m ）。可选的   readonly 存储库是否为只读。当从一个集群（ \u0026quot;readonly\u0026quot;: false when registering）迁移到另一个集群时（ \u0026quot;readonly\u0026quot;: true when registering)）非常有用。可选的   server_side_encryption 是否加密 S3 存储桶中的快照文件。此设置使用 AES-256 和 S3 托管密钥。请参阅 使用服务器端加密保护数据. 默认值为 false 。可选的   storage_class 指定 S3 存储类用于快照文件。默认值为 standard 。不要使用 glacier 和 deep_archive 存储类。可选的    创建快照 #  创建快照时，可以指定两条信息：\n 快照库名称 快照名称  以下快照包括所有索引和群集状态：\nPUT _snapshot/my-repository/1 您还可以添加请求主体以包括或排除某些索引或指定其他设置：\nPUT _snapshot/my-repository/2 { \u0026#34;indices\u0026#34;: \u0026#34;kibana*,my-index*,-my-index-2016\u0026#34;, \u0026#34;ignore_unavailable\u0026#34;: true, \u0026#34;include_global_state\u0026#34;: false, \u0026#34;partial\u0026#34;: false }    Setting Description     indices 要包含在快照中的索引。可以使用 , 创建索引列表，使用 * 指定索引模式，使用 - 排除某些索引。不要在项目之间放置空格。默认为所有索引。   ignore_unavailable 如果 indices 列表中的索引不存在，则是否忽略该索引而不是使快照失败。默认值为 false 。   include_global_state 是否在快照中包含群集状态。默认值为 true 。   partial 是否允许部分快照。默认值为 false ，如果一个或多个碎片未能存储，则会导致整个快照失败。    如果在拍摄快照后立即请求快照，您可能会看到以下内容：\nGET _snapshot/my-repository/2 { \u0026#34;snapshots\u0026#34;: [{ \u0026#34;snapshot\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;6.5.4\u0026#34;, \u0026#34;indices\u0026#34;: [ \u0026#34;kibana_sample_data_ecommerce\u0026#34;, \u0026#34;my-index\u0026#34;, \u0026#34;kibana_sample_data_logs\u0026#34;, \u0026#34;kibana_sample_data_flights\u0026#34; ], \u0026#34;include_global_state\u0026#34;: true, \u0026#34;state\u0026#34;: \u0026#34;IN_PROGRESS\u0026#34;, ... }] } 请注意，快照仍在进行中。如果要在继续之前等待快照完成，请在请求中添加 wait_for_completion 参数。快照可能需要一段时间才能完成，因此请考虑此选项是否适合您的使用情况：\nPUT _snapshot/my-repository/3?wait_for_completion=true 快照具有以下状态：\n   State Description     SUCCESS 快照成功存储了所有碎片。   IN_PROGRESS 快照当前正在运行。   PARTIAL 至少有一个碎片未能成功存储。仅当您在拍摄快照时将 partial 设置为 true 时才会发生。   FAILED 快照遇到错误，未存储任何数据。   INCOMPATIBLE 快照与此群集上运行的 Easysearch 版本不兼容。请参阅 冲突和兼容性。    如果当前正在拍摄快照，则无法拍摄快照。要检查状态：\nGET _snapshot/_status 恢复快照 #  恢复快照的第一步是检索现有快照。要查看所有快照存储库，请执行以下操作：\nGET _snapshot/_all 要查看存储库中的所有快照，请执行以下操作：\nGET _snapshot/my-repository/_all 然后恢复快照：\nPOST _snapshot/my-repository/2/_restore 就像拍摄快照时一样，您可以添加请求主体以包括或排除某些索引或指定一些其他设置：\nPOST _snapshot/my-repository/2/_restore { \u0026#34;indices\u0026#34;: \u0026#34;console*,my-index*\u0026#34;, \u0026#34;ignore_unavailable\u0026#34;: true, \u0026#34;include_global_state\u0026#34;: false, \u0026#34;include_aliases\u0026#34;: false, \u0026#34;partial\u0026#34;: false, \u0026#34;rename_pattern\u0026#34;: \u0026#34;console(.+)\u0026#34;, \u0026#34;rename_replacement\u0026#34;: \u0026#34;restored-console$1\u0026#34;, \u0026#34;index_settings\u0026#34;: { \u0026#34;index.blocks.read_only\u0026#34;: false }, \u0026#34;ignore_index_settings\u0026#34;: [ \u0026#34;index.refresh_interval\u0026#34; ] }    设置 描述     indices 要恢复的索引。可以使用 , 创建索引列表，使用 * 指定索引模式，使用 - 排除某些索引。不要在项目之间放置空格。默认为所有索引。   ignore_unavailable 如果 indices 列表中的索引不存在，则是否忽略该索引而不是使还原操作失败。默认值为 false 。   include_global_state 是否还原群集状态。默认值为 false 。   include_aliases 是否恢复别名及其关联索引。默认值为 true 。   partial 是否允许恢复部分快照。默认值为 false 。   rename_pattern 如果要在还原索引时重命名索引，请使用此选项指定与要还原的所有索引匹配的正则表达式。使用捕获组（ () ）重用索引名称的部分。   rename_replacement 如果要在恢复索引时重命名索引，请使用此选项指定替换模式。使用 $0 包括整个匹配索引名称，使用 $1 包括第一个捕获组的内容，等等。   index_settings 如果要在还原时更改索引设置，请在此处指定它们。   ignore_index_settings 您可以忽略快照中的某些索引设置，并在还原时使用群集默认值，而不是使用 index_settings 显式指定新设置。    冲突与兼容性 #  恢复索引时避免命名冲突的一种方法是使用 rename_pattern 和 rename_replacement 选项。然后，如果需要，可以使用 _reindex API 将两者结合起来。更简单的方法是在从快照恢复之前删除现有索引。\n在从快照还原之前，可以使用 _close API 关闭现有索引，但快照中的索引必须具有与现有索引相同数量的碎片。\n我们建议在从快照恢复之前停止对群集的写入请求，这有助于避免以下情况：\n 您可以删除索引，同时也会删除其别名。 对现已删除的别名的写入请求将创建与别名同名的新索引。 由于与新索引的命名冲突，快照中的别名无法还原。  快照只能向前兼容一个主要版本。例如，您不能将在 2.x 集群上拍摄的快照还原到 1.x 集群或 6.x 集群，但您可以在 2.x 或 5.x 集群上还原快照。\n如果您有一个旧快照，有时可以将其恢复到中间集群中，重新索引所有索引，创建新快照，然后重复，直到达到所需的版本，但您可能会发现只需在新集群上手动索引数据就更容易了。\nSecurity 模块注意事项 #  如果您使用了是安全模块功能，快照还有一些额外的限制：\n 要执行快照和还原操作，用户必须具有内置 manage_snapshots 角色。 无法还原包含全局状态或 .security 索引的快照。  如果快照包含全局状态，则在执行还原时必须将其排除。如果快照还包含 .security 索引，请排除它或列出所有其他要包含的索引：\nPOST _snapshot/my-repository/3/_restore { \u0026#34;indices\u0026#34;: \u0026#34;-.security\u0026#34;, \u0026#34;include_global_state\u0026#34;: false } .security 索引包含敏感数据，因此建议在拍摄快照时将其排除。如果确实需要从快照还原索引，则必须在请求中包含管理员证书：\ncurl -k --cert ./admin.pem --key ./admin-key.pem -XPOST \u0026#39;https://localhost:9200/_snapshot/my-repository/3/_restore?pretty\u0026#39; ","subcategory":null,"summary":"","tags":null,"title":"备份还原","url":"/easysearch/v1.15.0/docs/references/management/snapshot-restore/"},{"category":null,"content":"定点查询 #  定点查询，也称为 Point in Time 搜索，具有与常规搜索相同的功能，不同之处在于 PIT 搜索作用于较旧的数据集，而常规搜索作用于实时数据集。PIT 搜索不绑定于特定查询，因此您可以在同一个冻结在时间点上的数据集上运行不同的查询。\n您可以使用创建 PIT API 来创建 PIT。当您为一组索引创建 PIT 时，Easysearch 会锁定这些索引的一组段，使它们在时间上冻结。在底层，此 PIT 所需的资源不会被修改或删除。 如果作为 PIT 一部分的段被合并，Easysearch 会在 PIT 创建时通过 keep_alive 参数指定的时间段内保留这些段的副本。\n创建 PIT 操作会返回一个 PIT ID，您可以使用该 ID 在冻结的数据集上运行多个查询。即使索引继续摄取数据并修改或删除文档，PIT 引用的数据自 PIT 创建以来不会发生变化。当您的查询包含 PIT ID 时， 您不需要将索引传递给搜索，因为它将使用该 PIT。使用 PIT ID 的搜索在多次运行时将产生完全相同的结果。\n创建 PIT #  创建一个 PIT。查询参数 keep_alive 是必需的；它指定了保持 PIT 的时间长度。\n端点 #  POST /\u0026lt;target_indexes\u0026gt;/_pit?keep_alive=1h\u0026amp;routing=\u0026amp;expand_wildcards=\u0026amp;preference= 路径参数 #     参数 数据类型 描述     target_indexes 字符串 PIT 的目标索引名称。可以包含以逗号分隔的列表或通配符索引模式。    查询参数 #     参数 数据类型 描述     keep_alive 时间 保持 PIT 的时间长度。每次使用搜索 API 访问 PIT 时，PIT 的生命周期都会延长一段等于 keep_alive 参数的时间。必需。   preference 字符串 用于执行搜索的节点或分片。可选。默认为随机。   routing 字符串 指定将搜索请求路由到特定分片。可选。默认为文档的 _id。   expand_wildcards 字符串 可匹配通配符模式的索引类型。支持逗号分隔的值。有效值如下：\n- all：匹配任何索引或数据流，包括隐藏的。\n- open：匹配开放的、非隐藏的索引或非隐藏的数据流。\n- closed：匹配关闭的、非隐藏的索引或非隐藏的数据流。\n- hidden：匹配隐藏的索引或数据流。必须与 open、closed 或同时与 open 和 closed 组合使用。\n- none：不接受通配符模式。\n可选。默认为 open。   allow_partial_pit_creation 布尔值 指定是否在部分失败的情况下创建 PIT。可选。默认为 true。    请求示例 #  POST /test-index/_pit?keep_alive=100m 示例响应 #  { \u0026#34;pit_id\u0026#34;: \u0026#34;o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAIWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;creation_time\u0026#34;: 1658146050064 } 响应体字段 #     字段 数据类型 描述     pit_id Base64 编码 PIT ID。   creation_time 长整型 PIT 创建的时间，以毫秒为单位（从纪元开始）。    延长 PIT 时间 #  您可以在执行搜索时，通过在 pit 对象中提供 keep_alive 参数来延长 PIT 时间：\nGET /_search { \u0026#34;size\u0026#34;: 10000, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;user.id\u0026#34; : \u0026#34;elkbee\u0026#34; } }, \u0026#34;pit\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\u0026#34;, \u0026#34;keep_alive\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;sort\u0026#34;: [ {\u0026#34;@timestamp\u0026#34;: {\u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34;}} ], \u0026#34;search_after\u0026#34;: [ \u0026#34;2021-05-20T05:30:04.832Z\u0026#34; ] } 搜索请求中的 keep_alive 参数是可选的。它指定了延长保持 PIT 时间的时长。\n列出所有 PIT #  返回 Easysearch 集群中的所有 PIT。\n请求示例 #  GET /_pit/_all 示例响应 #  { \u0026#34;pits\u0026#34;: [ { \u0026#34;pit_id\u0026#34;: \u0026#34;o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAEWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\u0026#34;, \u0026#34;creation_time\u0026#34;: 1658146048666, \u0026#34;keep_alive\u0026#34;: 6000000 }, { \u0026#34;pit_id\u0026#34;: \u0026#34;o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAIWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\u0026#34;, \u0026#34;creation_time\u0026#34;: 1658146050064, \u0026#34;keep_alive\u0026#34;: 6000000 } ] } 响应体字段 #     字段 数据类型 描述     pits JSON 对象数组 所有 PIT 的列表。    每个 PIT 对象包含以下字段。\n   字段 数据类型 描述     pit_id Base64 编码 PIT ID。   creation_time 长整型 PIT 创建的时间，以毫秒为单位（从纪元开始）。   keep_alive 长整型 保持 PIT 的时间长度，以毫秒为单位。    删除 PIT #  删除一个、多个或所有 PIT。当 keep_alive 时间段过后，PIT 会自动删除。但是，为了释放资源，您可以使用删除 PIT API 来删除 PIT。删除 PIT API 支持通过 ID 删除 PIT 列表或一次性删除所有 PIT。\n请求示例: 删除所有 PIT #  DELETE /_pit/_all 如果您想删除一个或多个PIT，请在请求体中指定 PIT ID。\n请求体字段 #     字段 数据类型 描述     pit_id Base64 编码 或二进制数组 要删除的 PIT 的 PIT ID。必需。    示例请求：通过 ID 删除 PIT #  DELETE /_pit { \u0026quot;pit_id\u0026quot;: [ \u0026quot;o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAEWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\u0026quot;, \u0026quot;o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAIWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\u0026quot; ] } 示例响应 #\n 对于每个 PIT，响应包含一个带有 PIT ID 和 successful 字段的 JSON 对象，该字段指定删除是否成功。部分失败被视为失败。\n{ \u0026#34;pits\u0026#34;: [ { \u0026#34;successful\u0026#34;: true, \u0026#34;pit_id\u0026#34;: \u0026#34;o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAEWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\u0026#34; }, { \u0026#34;successful\u0026#34;: false, \u0026#34;pit_id\u0026#34;: \u0026#34;o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAIWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\u0026#34; } ] } 响应体字段 #     字段 数据类型 描述     successful 布尔值 删除操作是否成功。   pit_id Base64 编码 要删除的 PIT 的 PIT ID。    PIT 设置 #  您可以通过设置 _cluaster/settings 的方式为 PIT 指定以下设置。\n   设置 描述 默认值     point_in_time.max_keep_alive 一个集群级别的设置，指定 keep_alive 参数的最大值。 24h   search.max_open_pit_context 一个节点级别的设置，指定节点的最大开放 PIT 上下文数量。 300    ","subcategory":null,"summary":"","tags":null,"title":"定点查询","url":"/easysearch/v1.15.0/docs/references/search/pit_api/"},{"category":null,"content":"异步搜索 #  搜索大量数据可能会花费很长时间，尤其是当你在热节点或者多个远程集群中进行搜索时。\nEasysearch 中的异步搜索允许你发送在后台运行的搜索请求。你可以监控这些搜索的进度，并且在部分结果可用时获取这些部分结果。在搜索完成之后，你可以保存结果以便日后查看。\n先决条件 #  Easysearch 从 1.11.1 版本开始，内置支持异步搜索。\nREST API #  引入版本 1.11.0\n要执行异步搜索，请向 /{index}/_async_search 发送请求，并在请求正文中包含您的查询：\nPOST test-index/_asynch_search 可以指定以下选项。\n   选项 描述 默认值 是否必填     wait_for_completion_timeout 计划等待结果的时间。在此时间内，您可以像在普通搜索中一样查看所获得的结果。您可以根据ID轮询剩余的结果。最大值为300秒。 1秒 否   keep_on_completion 搜索完成后，您是否希望将结果保存在集群中。您可以在稍后查看存储的结果。 false 否   keep_alive 结果在集群中保存的时间。例如，2d 表示结果在集群中存储48小时。保存的搜索结果在此时间段结束后或如果搜索被取消时将被删除。请注意，这包括查询执行时间。如果查询超过此时间，进程将自动取消该查询。 12小时 否   index 要搜索的索引名称。可以是单个名称、用逗号分隔的索引列表，或索引名称的通配符表达式。 集群中的所有索引 否    请求示例 #  POST test-index/_async_search?wait_for_completion_timeout=1ms\u0026amp;keep_on_completion=true { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;张三\u0026quot; } } } 示例响应 #  { \u0026quot;id\u0026quot;: \u0026quot;FmFqN0llTXlKVHF5cnV1NGdVNUlPancEMzMzMBRaOUNxU3BVQlRIdzczZmJfNnZtRQIyMA==\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;start_time_in_millis\u0026quot;: 1740714470020, \u0026quot;expiration_time_in_millis\u0026quot;: 1740800870020, \u0026quot;response\u0026quot;: { \u0026quot;took\u0026quot;: 0, \u0026quot;timed_out\u0026quot;: false, \u0026quot;num_reduce_phases\u0026quot;: 0, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 0, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] } } } 响应参数 #     选项 描述     id 异步搜索的ID。使用此ID来监控搜索进度、获取其部分结果和/或删除结果。如果异步搜索在超时期限内完成，响应中不包含ID，因为结果未存储在集群中。   state 指定搜索是仍在运行还是已经完成，以及结果是否在集群中持久保存。可能的状态有 RUNNING（运行中）、SUCCEEDED（成功）、FAILED（失败）、PERSISTING（正在持久化）、PERSIST_SUCCEEDED（持久化成功）、PERSIST_FAILED（持久化失败）、CLOSED（已关闭）和 STORE_RESIDENT（存储驻留）。   start_time_in_millis 开始时间，单位为毫秒。   expiration_time_in_millis 过期时间，单位为毫秒。   took 搜索运行的总时长。   response 实际的搜索响应。   num_reduce_phases 协调节点从分片响应批次中聚合结果的次数（默认值为5）。如果与上次检索到的结果相比，此数字增加，您可以预期搜索响应中将包含额外的结果。   total 执行搜索的分片总数。   successful 协调节点成功接收到的分片响应数量。   aggregations 分片到目前为止已完成的聚合部分结果。    获取部分结果 #  提交异步搜索请求后，您可以使用在异步搜索响应中看到的ID请求部分响应。\nGET _async_search/\u0026lt;ID\u0026gt; 示例响应 #  { \u0026quot;id\u0026quot;: \u0026quot;FmFqN0llTXlKVHF5cnV1NGdVNUlPancEMzMzMBRaOUNxU3BVQlRIdzczZmJfNnZtRQIyMA==\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;STORE_RESIDENT\u0026quot;, \u0026quot;start_time_in_millis\u0026quot;: 1740714470020, \u0026quot;expiration_time_in_millis\u0026quot;: 1740800870020, \u0026quot;response\u0026quot;: { \u0026quot;took\u0026quot;: 4, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 1, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 1.3862942, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;test-index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;7MFOQZUBDkJmGmNvda2z\u0026quot;, \u0026quot;_score\u0026quot;: 1.3862942, \u0026quot;_source\u0026quot;: { \u0026quot;date\u0026quot;: \u0026quot;2025-01-15\u0026quot;, \u0026quot;score\u0026quot;: 89.5, \u0026quot;name\u0026quot;: \u0026quot;张三\u0026quot;, \u0026quot;age\u0026quot;: 25 } } ] } } } 在响应成功持久化之后，你会在响应中得到 STORE_RESIDENT 状态。\n你可以使用 wait_for_completion_timeout 参数轮询该ID，以等待在你指定的时间内接收到的结果。\n对于 keep_on_completion 为 true 且 keep_alive 时间足够长的异步搜索，你可以持续轮询这些ID，直到搜索结束。如果你不想定期轮询每个ID，你可以使用 keep_alive 参数将结果保留在你的集群中，然后在稍后时间回来查看。\n删除搜索及其结果 #  要删除一个异步搜索：\nDELETE _async_search/\u0026lt;ID\u0026gt;  如果搜索仍在进行中，Easysearch 会取消该搜索。 如果搜索已完成，Easysearch 会删除已保存的结果。  示例响应 #  { \u0026quot;acknowledged\u0026quot;: \u0026quot;true\u0026quot; } 监控统计信息 #  您可以使用统计信息API操作来监控正在运行、已完成、或已持久化的异步搜索。\nGET _async_search/stats 示例响应 #  { \u0026quot;_nodes\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;cluster_name\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;nodes\u0026quot;: { \u0026quot;aj7IeMyJTqyruu4gU5IOjw\u0026quot;: { \u0026quot;asynchronous_search_stats\u0026quot;: { \u0026quot;submitted\u0026quot;: 21, \u0026quot;initialized\u0026quot;: 21, \u0026quot;running_current\u0026quot;: 0, \u0026quot;persisted\u0026quot;: 20, \u0026quot;search_failed\u0026quot;: 1, \u0026quot;search_completed\u0026quot;: 20, \u0026quot;rejected\u0026quot;: 0, \u0026quot;persist_failed\u0026quot;: 0, \u0026quot;cancelled\u0026quot;: 0 } } } } 响应参数 #     选项 描述     submitted 已提交的异步搜索请求数量。   initialized 已初始化的异步搜索请求数量。   rejected 已拒绝的异步搜索请求数量。   search_completed 成功响应完成的异步搜索请求数量。   search_failed 失败响应完成的异步搜索请求数量。   persisted 最终结果成功持久化到集群中的异步搜索请求数量。   persist_failed 最终结果未能持久化到集群中的异步搜索请求数量。   running_current 在指定协调节点上运行的异步搜索请求数量。   cancelled 在搜索运行期间被取消的异步搜索请求数量。    异步搜索设置 #  异步搜索插件为 Easysearch 集群设置添加了若干设置项。这些设置是动态的，因此您无需重启集群即可更改插件的默认行为。\n您可以将这些设置标记为persistent或transient。\n例如，要更新结果索引的保留期限：\nPUT _cluster/settings { \u0026quot;transient\u0026quot;: { \u0026quot;async_search.max_wait_for_completion_timeout\u0026quot;: \u0026quot;5m\u0026quot; } }    设置 默认值 描述     async_search.max_search_running_time 12 hours 搜索的最长运行时间，超过该时间后搜索将被终止。   async_search.node_concurrent_running_searches 20 每个协调节点同时运行的搜索数量。   async_search.max_keep_alive 5 days 搜索结果在集群中存储的最长时间。   async_search.max_wait_for_completion_timeout 1 minute wait_for_completion_timeout 参数的最大值。   async_search.persist_search_failures false 将以搜索失败结束的异步搜索结果持久化到系统索引中。    ","subcategory":null,"summary":"","tags":null,"title":"异步搜索","url":"/easysearch/v1.15.0/docs/references/search/async_search/"},{"category":null,"content":"查询模版 #  您可以将全文查询转换为查询模版，以接受用户输入并将其动态插入到查询中。\n例如，如果您使用 Easysearch 作为应用程序或网站的后端搜索引擎，则可以从搜索栏或表单字段接收用户查询，并将其作为参数传递到查询模版中。这样，创建 Easysearch 查询的语法就从最终用户那里抽象出来了。\n当您编写代码将用户输入转换为 Easysearch 查询时，可以使用查询模版简化代码。如果需要将字段添加到搜索查询中，只需修改模板即可，而无需更改代码。\n查询模版使用 Mustache 语言。有关所有语法选项的列表，请参阅 Mustache 手册。\n创建查询模版 #  查询模版有两个组件：查询和参数。参数是放置在变量中的用户输入值。在 Mustache 符号中，变量用双括号表示。当在查询中遇到类似 {% raw %}{{var}}{% endraw %} 的变量时，Easysearch 会转到 params 部分，查找名为 var 的参数，并用指定的值替换它。\n您可以编写应用程序代码，询问用户要搜索什么，然后在运行时将该值插入 params 对象中。\n此命令定义了一个查询模版，用于按名称查找播放。查询中的 {% raw %}{{play_name}}{% endraw %} 被值 Henry IV 替换：\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;{% raw %}{{play_name}}{% endraw %}\u0026#34; } } }, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } } 此模板在整个集群上运行搜索。\n要在特定索引上运行此搜索，请将索引名称添加到请求中：\nGET shakespeare/_search/template 指定 from 和 size 参数：\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;{% raw %}{{from}}{% endraw %}\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;{% raw %}{{size}}{% endraw %}\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;{% raw %}{{play_name}}{% endraw %}\u0026#34; } } }, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34;, \u0026#34;from\u0026#34;: 10, \u0026#34;size\u0026#34;: 10 } } 为了改善搜索体验，您可以定义默认值，这样用户就不必指定每个可能的参数。如果参数未在 params 部分中定义，Easysearch 将使用默认值。\n定义变量 var 默认值的语法如下：\n{% raw %}{{var}}{{^var}}default value{{/var}}{% endraw %} 此命令将 from 的默认值设置为 10，将 size 设置为 10：\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;{% raw %}{{from}}{{^from}}10{{/from}}{% endraw %}\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;{% raw %}{{size}}{{^size}}10{{/size}}{% endraw %}\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;{% raw %}{{play_name}}{% endraw %}\u0026#34; } } }, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } } 保存并执行查询模版 #  查询模版按您希望的方式工作后，可以将该模板的源保存为脚本，使其可用于不同的输入参数。\n将查询模版另存为脚本时，需要将 lang 参数指定为 muscle ：\nPOST _scripts/play_search_template { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;mustache\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;{% raw %}{{from}}{{^from}}0{{/from}}{% endraw %}\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;{% raw %}{{size}}{{^size}}10{{/size}}{% endraw %}\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;{{play_name}}\u0026#34; } } }, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } } } 现在，您可以通过引用模板的 id 参数来重用模板。\n您可以将此源模板用于不同的输入值。\nGET _search/template { \u0026#34;id\u0026#34;: \u0026#34;play_search_template\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34;, \u0026#34;from\u0026#34;: 0, \u0026#34;size\u0026#34;: 1 } } 输出示例 #  { \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 6, \u0026#34;successful\u0026#34;: 6, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3205, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 3.641852, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 3.641852, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 5, \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34;, \u0026#34;speech_number\u0026#34;: 1, \u0026#34;line_number\u0026#34;: \u0026#34;1.1.2\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;KING HENRY IV\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;Find we a time for frighted peace to pant,\u0026#34; } } ] } } 如果您有一个存储的模板并希望对其进行验证，请使用 render 操作：\nPOST _render/template { \u0026#34;id\u0026#34;: \u0026#34;play_search_template\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } } 输出示例 #  { \u0026#34;template_output\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34; } } } } 使用查询模版进行高级参数转换 #  Mustache 中有很多不同的语法选项，可以将输入参数转换为查询。\n您可以指定条件、运行循环、连接数组、将数组转换为 JSON 等。\n条件 #  使用 Mustache 中的条件表达方式：\n{% raw %}{{#var}}var{{/var}}{% endraw %} 当 var 是布尔值时，此语法充当 if 条件。只有当 var 的值为 true 时， {#var}} 和 {/var}}} 标记之间才会插入参数值。\n使用 section 标记会使 JSON 无效，因此必须改用字符串格式编写查询。\n只有当 limit 参数设置为 true 时，此命令才会在查询中包含 size 参数。\n在以下示例中， limit 参数为 true ，因此 size 参数被激活。因此，您只会得到两个文档。\nGET _search/template { \u0026#34;source\u0026#34;: \u0026#34;{% raw %}{ {{#limit}} \\\u0026#34;size\\\u0026#34;: \\\u0026#34;{{size}}\\\u0026#34;, {{/limit}} \\\u0026#34;query\\\u0026#34;:{\\\u0026#34;match\\\u0026#34;:{\\\u0026#34;play_name\\\u0026#34;: \\\u0026#34;{{play_name}}\\\u0026#34;}}}{% endraw %}\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34;, \u0026#34;limit\u0026#34;: true, \u0026#34;size\u0026#34;: 2 } } 您还可以设计 if-else 条件。\n如果 limit 为 true ，则此命令将 size 设置为 2 。否则，它将 大小 设置为 10 。\nGET _search/template { \u0026#34;source\u0026#34;: \u0026#34;{% raw %}{ {{#limit}} \\\u0026#34;size\\\u0026#34;: \\\u0026#34;2\\\u0026#34;, {{/limit}} {{^limit}} \\\u0026#34;size\\\u0026#34;: \\\u0026#34;10\\\u0026#34;, {{/limit}} \\\u0026#34;query\\\u0026#34;:{\\\u0026#34;match\\\u0026#34;:{\\\u0026#34;play_name\\\u0026#34;: \\\u0026#34;{{play_name}}\\\u0026#34;}}}{% endraw %}\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Henry IV\u0026#34;, \u0026#34;limit\u0026#34;: true } } 循环 #  您还可以使用 section 标记来实现 foreach 循环：\n{% raw %}{{#var}}{{.}}{{/var}}{% endraw %} 当 var 是一个数组时，查询模版遍历它并创建一个 terms 查询。\nGET _search/template { \u0026#34;source\u0026#34;: \u0026#34;{% raw %}{\\\u0026#34;query\\\u0026#34;:{\\\u0026#34;terms\\\u0026#34;:{\\\u0026#34;play_name\\\u0026#34;:[\\\u0026#34;{{#play_name}}\\\u0026#34;,\\\u0026#34;{{.}}\\\u0026#34;,\\\u0026#34;{{/play_name}}\\\u0026#34;]}}}{% endraw %}\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;play_name\u0026#34;: [ \u0026#34;Henry IV\u0026#34;, \u0026#34;Othello\u0026#34; ] } } 此模板呈现为：\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;play_name\u0026#34;: [ \u0026#34;Henry IV\u0026#34;, \u0026#34;Othello\u0026#34; ] } } } } 关联 #  可以使用 join 标记连接数组的值（用逗号分隔）：\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;{% raw %}{{#join}}{{text_entry}}{{/join}}{% endraw %}\u0026#34; } } }, \u0026#34;params\u0026#34;: { \u0026#34;text_entry\u0026#34;: [ \u0026#34;To be\u0026#34;, \u0026#34;or not to be\u0026#34; ] } } 此模板呈现为:\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;{0=To be, 1=or not to be}\u0026#34; } } } } 转换为 JSON #  您可以使用 toJson 标记将参数转换为 JSON 表示形式：\nGET _search/template { \u0026#34;source\u0026#34;: \u0026#34;{\\\u0026#34;query\\\u0026#34;:{\\\u0026#34;bool\\\u0026#34;:{\\\u0026#34;must\\\u0026#34;:[{\\\u0026#34;terms\\\u0026#34;: {\\\u0026#34;text_entries\\\u0026#34;: {% raw %}{{#toJson}}text_entries{{/toJson}}{% endraw %} }}] }}}\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;text_entries\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text_entry\u0026#34; : \u0026#34;love\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;text_entry\u0026#34; : \u0026#34;soldier\u0026#34; } } ] } } 渲染结果:\nGET _search/template { \u0026#34;source\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;terms\u0026#34;: { \u0026#34;text_entries\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;love\u0026#34; } }, { \u0026#34;term\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;soldier\u0026#34; } } ] } } ] } } } } 多个查询模版 #  您可以捆绑多个查询模版，并使用 msearch 操作在单个请求中将它们发送到 Easysearch 集群。\n这节省了网络往返时间，因此与独立请求相比，您可以更快地返回响应。\nGET _msearch/template {\u0026#34;index\u0026#34;:\u0026#34;shakespeare\u0026#34;} {\u0026#34;id\u0026#34;:\u0026#34;if_search_template\u0026#34;,\u0026#34;params\u0026#34;:{\u0026#34;play_name\u0026#34;:\u0026#34;Henry IV\u0026#34;,\u0026#34;limit\u0026#34;:false,\u0026#34;size\u0026#34;:2}} {\u0026#34;index\u0026#34;:\u0026#34;shakespeare\u0026#34;} {\u0026#34;id\u0026#34;:\u0026#34;play_search_template\u0026#34;,\u0026#34;params\u0026#34;:{\u0026#34;play_name\u0026#34;:\u0026#34;Henry IV\u0026#34;}} 管理查询模版 #  要列出所有脚本，请运行以下命令：\nGET _cluster/state/metadata?pretty\u0026amp;filter_path=**.stored_scripts 要检索特定查询模版，请运行以下命令：\nGET _scripts/\u0026lt;name_of_search_template\u0026gt; 要删除查询模版，请运行以下命令：\nDELETE _scripts/\u0026lt;name_of_search_template\u0026gt; ","subcategory":null,"summary":"","tags":null,"title":"查询模版","url":"/easysearch/v1.15.0/docs/references/search/search-template/"},{"category":null,"content":"Wildcard 字段类型 #  wildcard（通配符）字段是keyword（关键字）字段的一种变体，专为任意子字符串和正则表达式匹配而设计。\n当您的内容由\u0026quot;字符串\u0026quot;而非\u0026quot;文本\u0026quot;组成时，应使用wildcard字段。示例包括非结构化日志行和计算机代码。\nwildcard字段类型的索引方式与keyword字段类型不同。keyword字段将原始字段值写入索引，而wildcard字段类型则将字段值拆分为长度小于或等于3的子字符串，并将这些子字符串写入索引。例如，字符串test被拆分为t、te、tes、e、es和est这些子字符串。\n在搜索时，将查询模式中所需的子字符串与索引进行匹配以生成候选文档，然后根据查询中的模式对这些文档进行过滤。例如，对于搜索词test，OpenSearch执行索引搜索tes AND est。如果搜索词包含少于三个字符，OpenSearch会使用长度为一或二的字符子字符串。对于每个匹配的文档，如果源值为test，则该文档将出现在结果中。这样可以排除误报值，如nikola tesla felt alternating current was best。\n通常，精确匹配查询（如 term或 terms查询）在wildcard字段上的表现不如在keyword字段上有效， 而 wildcard、 prefix和 regexp查询在wildcard字段上表现更好。\n示例 #  创建带有 wildcard 字段的映射：\nPUT logs { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;log_line\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;wildcard\u0026#34; } } } } 参数 #  以下表格列出了 wildcard 字段可用的所有参数。 `\n   参数 描述     doc_values 布尔值，指定该字段是否应存储在磁盘上，以便用于聚合、排序或脚本操作。默认值为 false。   ignore_above 长度超过此整数值的任何字符串都不会被索引。默认值为 2147483647。   normalizer 用于预处理索引和搜索值的标准化器。默认情况下，不进行标准化，使用原始值。您可以使用 lowercase 标准化器在该字段上执行不区分大小写的匹配。   null_value 用于替代 null 的值。必须与字段类型相同。如果未指定此参数，则当字段值为 null 时，该字段将被视为缺失。默认值为 null。    ","subcategory":null,"summary":"","tags":null,"title":"Wildcard 字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/wildcard/"},{"category":null,"content":"Routing 路由属性 #  Easysearch 使用哈希算法将文档路由到索引中的特定分片。默认情况下，文档的 _id 字段用作路由值，但您也可以为每个文档指定自定义路由值。\n默认路由 #  以下是 Easysearch 的默认路由公式。_routing 值是文档的 _id。\nshard_num = hash(_routing) % num_primary_shards 自定义路由 #  您可以在索引文档时指定自定义路由值，如以下示例所示：\nPUT sample-index1/_doc/1?routing=JohnDoe1 { \u0026quot;title\u0026quot;: \u0026quot;This is a document\u0026quot; } 在此示例中，文档使用的路由值是 JohnDoe1 而不是默认的 _id 。\n在检索、删除或更新文档时，您必须提供相同的路由值，如以下示例所示：\nGET sample-index1/_doc/1?routing=JohnDoe1 通过路由查询 #  您可以使用 _routing 字段根据文档的路由值进行查询，如以下示例所示。此查询仅搜索与 JohnDoe1 路由值关联的分片：\nGET sample-index1/_search { \u0026quot;query\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;_routing\u0026quot;: [ \u0026quot;JohnDoe1\u0026quot; ] } } } 设置路由为必需项 #  您可以使索引上的所有 CRUD 操作都必需提供路由值，如以下示例。如果您尝试在不提供路由值的情况下索引文档，Easysearch 将抛出异常。\nPUT sample-index2 { \u0026quot;mappings\u0026quot;: { \u0026quot;_routing\u0026quot;: { \u0026quot;required\u0026quot;: true } } } 路由到特定分片组 #  您可以配置索引将自定义值路由到分片的子集，而不是单个分片。这是通过在创建索引时设置 index.routing_partition_size 来实现的。计算分片的公式是 shard_num = (hash(_routing) + hash(_id)) % routing_partition_size) % num_primary_shards。\n以下示例请求将文档路由到索引中的四个分片之一：\nPUT sample-index3 { \u0026quot;settings\u0026quot;: { \u0026quot;index.routing_partition_size\u0026quot;: 4 }, \u0026quot;mappings\u0026quot;: { \u0026quot;_routing\u0026quot;: { \u0026quot;required\u0026quot;: true } } } ","subcategory":null,"summary":"","tags":null,"title":"路由属性","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/metadata-field/routing/"},{"category":null,"content":"跨集群复制 #  使用跨集群复制 API 管理跨集群复制。\n 在跨集群复制中，可以将数据索引到一个领导者索引，然后 Easysearch 将这些数据复制到一个或多个只读的跟随者索引。所有在领导者上进行的后续操作都会在跟随者上复制，例如创建、更新或删除文档。\n先决条件 #   1.11.1 版本之前，leader 和 follower 集群都必须安装 cross-cluster-replication 插件和 index-management 插件，1.11.1 版本开始，已经内置了 CCR 模块。 如果 follower 集群的 easysearch.yml 文件中覆盖了 node.roles，确保它也包括 remote_cluster_client 角色，默认启用。 node.roles: [\u0026lt;other_roles\u0026gt;, remote_cluster_client]  权限 #   确保安全功能在两个集群上都启用或都禁用。如果启用了安全功能，确保非管理员用户被映射到适当的权限，以便他们可以执行复制操作。  部署示例集群 #   在本地起 2 个单节点的 easysearch 测试集群，分别是 follower-application (9201 端口) 和 leader-application (9200 端口) 在 easysearch.yml 添加 discovery.type: single-node 如果启用 security 功能，确保 2 个集群的证书互信，测试环境可以直接合并 2 个节点的 ca 证书： 例如 cat ca.crt ../../easysearch-1.0.0_2/config/ca.crt \u0026gt; trust-chain.pem 分别设置 security.ssl.transport.ca_file: trust-chain.pem   设置跨集群复制 #  设置跨群集连接 #  跨集群复制采用“pull”模型，在 follower 集群上，添加每个种子节点的 IP 地址（端口 9300）， 为连接提供一个描述性名称，您将在请求中使用该名称来启动复制。以下是对应的 curl 命令：\ncurl -XPUT -k -H 'Content-Type: application/json' -u 'admin:xxxxxxxxxxxx' 'https://localhost:9201/_cluster/settings?pretty' -d ' { \u0026quot;persistent\u0026quot;: { \u0026quot;cluster\u0026quot;: { \u0026quot;remote\u0026quot;: { \u0026quot;my-connection-alias\u0026quot;: { \u0026quot;seeds\u0026quot;: [\u0026quot;127.0.0.1:9300\u0026quot;] } } } } }'  开始复制 #  首先，在 leader 集群创建 leader-01 索引, 并向索引写入数据。\ncurl -XPUT -k -H 'Content-Type: application/json' -u 'admin:xxxxxxxxxxxx' 'https://localhost:9200/leader-01?pretty' 然后设置跟随者集群开始复制。在请求体中，提供你想要复制的连接名和领导者索引，以及你想要使用的安全角色：\ncurl -XPUT -k -H 'Content-Type: application/json' -u 'admin:xxxxxxxxxxxx' 'https://localhost:9201/_replication/follower-01/_start?pretty' -d ' { \u0026quot;leader_alias\u0026quot;: \u0026quot;my-connection-alias\u0026quot;, \u0026quot;leader_index\u0026quot;: \u0026quot;leader-01\u0026quot;, \u0026quot;use_roles\u0026quot;:{ \u0026quot;leader_cluster_role\u0026quot;: \u0026quot;all_access\u0026quot;, \u0026quot;follower_cluster_role\u0026quot;: \u0026quot;all_access\u0026quot; } }' 确认复制状态 #  curl -XGET -k -u 'admin:xxxxxxxxxxxx' 'https://localhost:9201/_replication/follower-01/_status?pretty' 响应\n{ \u0026#34;status\u0026#34; : \u0026#34;SYNCING\u0026#34;, \u0026#34;reason\u0026#34; : \u0026#34;User initiated\u0026#34;, \u0026#34;leader_alias\u0026#34; : \u0026#34;my-connection-alias\u0026#34;, \u0026#34;leader_index\u0026#34; : \u0026#34;leader-01\u0026#34;, \u0026#34;follower_index\u0026#34; : \u0026#34;follower-01\u0026#34;, \u0026#34;syncing_details\u0026#34; : { \u0026#34;leader_checkpoint\u0026#34; : 23, \u0026#34;follower_checkpoint\u0026#34; : 23, \u0026#34;seq_no\u0026#34; : 24 } } 列出 follow 集群所有正在复制的索引 #  包括同步中、暂停、失败等状态。\nGET /_replication/all_status 响应\n下面状态为 FAILED 或 PAUSED 的 索引 是因为 leader 集群启用了 ILM，将过期的索引删除了，CCR 会将被删除的索引在 status 中标记出来。\n[ { \u0026#34;status\u0026#34;: \u0026#34;FAILED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Pause failed with \\\u0026#34;Index log-test-000006 is already paused\\\u0026#34;. Original failure for initiating pause - [[log-test-000006][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000006]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000006\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000006\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;SYNCING\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;User initiated\u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;test2\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;test2\u0026#34;, \u0026#34;syncing_details\u0026#34;: { \u0026#34;leader_checkpoint\u0026#34;: 12, \u0026#34;follower_checkpoint\u0026#34;: 12, \u0026#34;seq_no\u0026#34;: 13 } }, { \u0026#34;status\u0026#34;: \u0026#34;PAUSED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;AutoPaused: [[log-test-000001][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000001]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000001\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000001\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;PAUSED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;AutoPaused: [[log-test-000003][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000003]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000003\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000003\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;SYNCING\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;User initiated\u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;syncing_details\u0026#34;: { \u0026#34;leader_checkpoint\u0026#34;: 20, \u0026#34;follower_checkpoint\u0026#34;: 20, \u0026#34;seq_no\u0026#34;: 21 } }, { \u0026#34;status\u0026#34;: \u0026#34;PAUSED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;AutoPaused: [[log-test-000002][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000002]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000002\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000002\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;FAILED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Pause failed with \\\u0026#34;Index log-test-000005 is already paused\\\u0026#34;. Original failure for initiating pause - [[log-test-000005][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000005]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000005\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000005\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;PAUSED\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;AutoPaused: [[log-test-000004][0] - org.easysearch.index.IndexNotFoundException - \\\u0026#34;no such index [log-test-000004]\\\u0026#34;], \u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000004\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000004\u0026#34; }, { \u0026#34;status\u0026#34;: \u0026#34;SYNCING\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;User initiated\u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000013\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000013\u0026#34;, \u0026#34;syncing_details\u0026#34;: { \u0026#34;leader_checkpoint\u0026#34;: -1, \u0026#34;follower_checkpoint\u0026#34;: -1, \u0026#34;seq_no\u0026#34;: 0 } }, { \u0026#34;status\u0026#34;: \u0026#34;SYNCING\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;User initiated\u0026#34;, \u0026#34;leader_alias\u0026#34;: \u0026#34;leader\u0026#34;, \u0026#34;leader_index\u0026#34;: \u0026#34;log-test-000014\u0026#34;, \u0026#34;follower_index\u0026#34;: \u0026#34;log-test-000014\u0026#34;, \u0026#34;syncing_details\u0026#34;: { \u0026#34;leader_checkpoint\u0026#34;: -1, \u0026#34;follower_checkpoint\u0026#34;: -1, \u0026#34;seq_no\u0026#34;: 0 } } ] 暂停和恢复复制 #  如果您需要修复问题或减轻主集群的负载，您可以暂时暂停索引的复制：\ncurl -XPOST -k -H 'Content-Type: application/json' -u 'admin:xxxxxxxxxxxx' 'https://localhost:9201/_replication/follower-01/_pause?pretty' -d '{}' 恢复\ncurl -XPOST -ku 'admin:xxxxxxxxxxxx' -H 'Content-Type: application/json' 'https://localhost:9201/_replication/follower-01/_resume?pretty' -d '{}' 获取 leader 集群状态 #  在 leader 集群执行\nGET /_replication/leader_stats 响应\n{ \u0026#34;num_replicated_indices\u0026#34;: 4, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;translog_size_bytes\u0026#34;: 0, \u0026#34;operations_read_lucene\u0026#34;: 0, \u0026#34;operations_read_translog\u0026#34;: 0, \u0026#34;total_read_time_lucene_millis\u0026#34;: 0, \u0026#34;total_read_time_translog_millis\u0026#34;: 0, \u0026#34;bytes_read\u0026#34;: 0, \u0026#34;index_stats\u0026#34;: { \u0026#34;test1\u0026#34;: { \u0026#34;operations_read\u0026#34;: 0, \u0026#34;translog_size_bytes\u0026#34;: 0, \u0026#34;operations_read_lucene\u0026#34;: 0, \u0026#34;operations_read_translog\u0026#34;: 0, \u0026#34;total_read_time_lucene_millis\u0026#34;: 0, \u0026#34;total_read_time_translog_millis\u0026#34;: 0, \u0026#34;bytes_read\u0026#34;: 0 }, \u0026#34;test2\u0026#34;: { \u0026#34;operations_read\u0026#34;: 0, \u0026#34;translog_size_bytes\u0026#34;: 0, \u0026#34;operations_read_lucene\u0026#34;: 0, \u0026#34;operations_read_translog\u0026#34;: 0, \u0026#34;total_read_time_lucene_millis\u0026#34;: 0, \u0026#34;total_read_time_translog_millis\u0026#34;: 0, \u0026#34;bytes_read\u0026#34;: 0 }, \u0026#34;log-test-000013\u0026#34;: { \u0026#34;operations_read\u0026#34;: 0, \u0026#34;translog_size_bytes\u0026#34;: 0, \u0026#34;operations_read_lucene\u0026#34;: 0, \u0026#34;operations_read_translog\u0026#34;: 0, \u0026#34;total_read_time_lucene_millis\u0026#34;: 0, \u0026#34;total_read_time_translog_millis\u0026#34;: 0, \u0026#34;bytes_read\u0026#34;: 0 }, \u0026#34;log-test-000014\u0026#34;: { \u0026#34;operations_read\u0026#34;: 0, \u0026#34;translog_size_bytes\u0026#34;: 0, \u0026#34;operations_read_lucene\u0026#34;: 0, \u0026#34;operations_read_translog\u0026#34;: 0, \u0026#34;total_read_time_lucene_millis\u0026#34;: 0, \u0026#34;total_read_time_translog_millis\u0026#34;: 0, \u0026#34;bytes_read\u0026#34;: 0 } } } 获取 follower 集群状态为同步中的索引的信息 #  curl -XGET -ku 'admin:xxxxxxxxxxxx' 'https://localhost:9200/_replication/follower_stats' 响应\n{ \u0026#34;num_syncing_indices\u0026#34;: 4, \u0026#34;num_bootstrapping_indices\u0026#34;: 0, \u0026#34;num_paused_indices\u0026#34;: 5, \u0026#34;num_failed_indices\u0026#34;: 2, \u0026#34;num_shard_tasks\u0026#34;: 4, \u0026#34;num_index_tasks\u0026#34;: 4, \u0026#34;operations_written\u0026#34;: 0, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;failed_read_requests\u0026#34;: 0, \u0026#34;throttled_read_requests\u0026#34;: 0, \u0026#34;failed_write_requests\u0026#34;: 0, \u0026#34;throttled_write_requests\u0026#34;: 0, \u0026#34;follower_checkpoint\u0026#34;: 30, \u0026#34;leader_checkpoint\u0026#34;: 0, \u0026#34;total_write_time_millis\u0026#34;: 0, \u0026#34;index_stats\u0026#34;: { \u0026#34;test1\u0026#34;: { \u0026#34;operations_written\u0026#34;: 0, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;failed_read_requests\u0026#34;: 0, \u0026#34;throttled_read_requests\u0026#34;: 0, \u0026#34;failed_write_requests\u0026#34;: 0, \u0026#34;throttled_write_requests\u0026#34;: 0, \u0026#34;follower_checkpoint\u0026#34;: 20, \u0026#34;leader_checkpoint\u0026#34;: 0, \u0026#34;total_write_time_millis\u0026#34;: 0 }, \u0026#34;test2\u0026#34;: { \u0026#34;operations_written\u0026#34;: 0, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;failed_read_requests\u0026#34;: 0, \u0026#34;throttled_read_requests\u0026#34;: 0, \u0026#34;failed_write_requests\u0026#34;: 0, \u0026#34;throttled_write_requests\u0026#34;: 0, \u0026#34;follower_checkpoint\u0026#34;: 12, \u0026#34;leader_checkpoint\u0026#34;: 0, \u0026#34;total_write_time_millis\u0026#34;: 0 }, \u0026#34;log-test-000014\u0026#34;: { \u0026#34;operations_written\u0026#34;: 0, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;failed_read_requests\u0026#34;: 0, \u0026#34;throttled_read_requests\u0026#34;: 0, \u0026#34;failed_write_requests\u0026#34;: 0, \u0026#34;throttled_write_requests\u0026#34;: 0, \u0026#34;follower_checkpoint\u0026#34;: -1, \u0026#34;leader_checkpoint\u0026#34;: 0, \u0026#34;total_write_time_millis\u0026#34;: 0 }, \u0026#34;log-test-000015\u0026#34;: { \u0026#34;operations_written\u0026#34;: 0, \u0026#34;operations_read\u0026#34;: 0, \u0026#34;failed_read_requests\u0026#34;: 0, \u0026#34;throttled_read_requests\u0026#34;: 0, \u0026#34;failed_write_requests\u0026#34;: 0, \u0026#34;throttled_write_requests\u0026#34;: 0, \u0026#34;follower_checkpoint\u0026#34;: -1, \u0026#34;leader_checkpoint\u0026#34;: 0, \u0026#34;total_write_time_millis\u0026#34;: 0 } } } 停止复制 #  在 follower 集群执行\ncurl -XPOST -k -H 'Content-Type: application/json' -u 'admin:xxxxxxxxxxxx' 'https://localhost:9201/_replication/follower-01/_stop?pretty' -d '{}' 跨集群复制的自动跟随 #  自动开始对匹配指定模式的索引进行复制。如果领导者集群上的新索引匹配该模式，Easysearch 会自动创建一个跟随者索引。 自动跟随让你可以根据匹配的模式自动复制在领导者集群上创建的索引。当你在领导者集群上创建一个名称与指定模式匹配的索引（例如，index-01*），在跟随者集群上会自动创建一个对应的跟随者索引。\n你可以为单个集群配置多个复制规则。目前的模式只支持通配符匹配。\n在启用自动跟随之前，确认在两个集群之间设置了跨集群连接。\n如果启用了安全插件，确保非管理员用户被映射到适当的权限，以便他们可以执行复制操作。关于索引和集群级别的权限要求，请参阅跨集群复制权限。\n复制规则是你针对单个跟随者集群创建的模式集合。当你创建一个复制规则时，它首先会自动复制任何匹配模式的现有索引。然后，它将继续复制你创建的任何新索引，只要这些新索引匹配该模式。\n示例 在 follower 集群执行：\ncurl -XPOST -k -H 'Content-Type: application/json' -u 'admin:xxxxxxxxxxxx' 'https://localhost:9201/_replication/_autofollow?pretty' -d ' { \u0026quot;leader_alias\u0026quot; : \u0026quot;my-connection-alias\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;my-replication-rule\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;nginx*\u0026quot;, \u0026quot;use_roles\u0026quot;:{ \u0026quot;leader_cluster_role\u0026quot;: \u0026quot;all_access\u0026quot;, \u0026quot;follower_cluster_role\u0026quot;: \u0026quot;all_access\u0026quot; } }' 然后 创建 nginx 开头的索引并写入数据\n查看 follower 集群的索引状态 #  curl -kuadmin:admin 'https://localhost:9201/_cat/indices?v' 如果发现文档数没变化可以执行 refresh 命令后再查看\n查看已设置的自动跟随规则 #  curl -u 'admin:xxxxxxxxxxxx' -k 'https://localhost:9200/_replication/autofollow_stats' 删除自动跟随规则 #  可以通过在从集群上调用 API 来删除自动跟随。调用 API 仅停止任何新的自动跟随活动，并不会停止已由自动跟随启动的复制，要停止现有的复制活动并打开索引进行写入，请使用停止复制API操作。\n示例\ncurl -XDELETE -k -H 'Content-Type: application/json' -u 'admin:xxxxxxxxxxxx' 'https://localhost:9201/_replication/_autofollow?pretty' -d ' { \u0026quot;leader_alias\u0026quot; : \u0026quot;my-connection-alias\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;my-replication-rule\u0026quot; }'  如何使用自定义用户 #  如果不使用 admin 用户，必须将你自定义的用户绑定到 cross_cluster_replication_leader_full_access，它在领导者集群上提供复制权限， cross_cluster_replication_follower_full_access，它在追随者集群上提供复制权限。\n示例 分别在 leader 和 follower 集群执行创建 replication_user 用户的命令：\ncurl -XPUT -k -H 'Content-Type: application/json' -u 'admin:xxxxxxxxxxxx' 'https://xxxx:9xxx/_security/user/replication_user' -d ' { \u0026quot;password\u0026quot;: \u0026quot;123456\u0026quot;, \u0026quot;roles\u0026quot;: [\u0026quot;cross_cluster_replication_leader_full_access\u0026quot;, \u0026quot;cross_cluster_replication_follower_full_access\u0026quot;] }' 用 replication_user 用户设置跟随者集群开始复制 #  curl -XPUT -k -H 'Content-Type: application/json' -u 'replication_user:123456' 'https://localhost:9288/_replication/follower-01/_start?pretty' -d ' { \u0026quot;leader_alias\u0026quot;: \u0026quot;my-connection-alias\u0026quot;, \u0026quot;leader_index\u0026quot;: \u0026quot;leader-01\u0026quot;, \u0026quot;use_roles\u0026quot;:{ \u0026quot;leader_cluster_role\u0026quot;: \u0026quot;cross_cluster_replication_leader_full_access\u0026quot;, \u0026quot;follower_cluster_role\u0026quot;: \u0026quot;cross_cluster_replication_follower_full_access\u0026quot; } }' 用 replication_user 用户设置跟随者集群自动复制 #  curl -XPOST -k -H 'Content-Type: application/json' -u 'replication_user:123456' 'https://localhost:9288/_replication/_autofollow?pretty' -d ' { \u0026quot;leader_alias\u0026quot; : \u0026quot;my-connection-alias\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;my-replication-rule\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;leader*\u0026quot;, \u0026quot;use_roles\u0026quot;:{ \u0026quot;leader_cluster_role\u0026quot;: \u0026quot;cross_cluster_replication_leader_full_access\u0026quot;, \u0026quot;follower_cluster_role\u0026quot;: \u0026quot;cross_cluster_replication_follower_full_access\u0026quot; } }' 配置项 #  跨集群复制有多个配置项，这些配置项是可以动态修改的，可以将设置标记为 persistent 或 transient。\n例如，要更新跟随集群轮询主集群以获取更新的频率：\nPUT _cluster/settings { \u0026quot;persistent\u0026quot;: { \u0026quot;replication.follower.metadata_sync_interval\u0026quot;: \u0026quot;30s\u0026quot; } } 这些配置项管理远程恢复所消耗的资源。我们不建议更改这些设置；默认设置应适用于大多数使用场景。\n   设置 默认值 描述     replication.follower.index.recovery.chunk_size 10MB follower 集群为文件传输请求的块大小。指定块大小的值和单位,例如10MB、5KB。请参阅支持的单位。   replication.follower.index.recovery.max_concurrent_file_chunks 4 每个恢复过程中可以并行发送的文件块请求数。   replication.follower.index.ops_batch_size 50000 在复制的同步阶段,一次可以获取的操作数量。   replication.follower.concurrent_readers_per_shard 2 在复制的同步阶段,每个分片上追随者集群的并发请求数。   replication.autofollow.fetch_poll_interval 30s 自动跟随任务轮询leader集群以获取新的匹配索引的频率。   replication.follower.metadata_sync_interval 60s follower 集群轮询leader集群以获取更新的索引元数据的频率。   replication.translog.retention_lease.pruning.enabled true 如果启用,基于保留租约来修剪translog。   replication.translog.retention_size 512MB 控制leader索引上translog的大小。     \n","subcategory":null,"summary":"","tags":null,"title":"跨集群复制","url":"/easysearch/v1.15.0/docs/references/management/ccr_api/"},{"category":null,"content":"资源扩容 #  查看当前 cpu、mem 和磁盘资源情况\nkubectl get sts/threenodes-masters -o yaml resources: requests: cpu: \u0026quot;1\u0026quot; memory: 3Gi limits: cpu: \u0026quot;1\u0026quot; memory: 5Gi\nresources: requests: storage: 30Gi volumeMode: Filesystem\n磁盘（磁盘扩容依赖于实际的 StorageClass，需要 StorageClass 本身支持扩容）\n修改 Operator yaml 文件，执行 apply 操作\nresources: requests: cpu: \u0026#34;1\u0026#34; memory: 4Gi limits: cpu: \u0026#34;2\u0026#34; memory: 6Gi \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;resources\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;requests\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;storage\u0026lt;/span\u0026gt;: \u0026lt;span style=\u0026quot;color:#ae81ff\u0026quot;\u0026gt;50Gi\u0026lt;/span\u0026gt;  滚动更新中： 从 threenodes-masters-0 开始更新\nthreenodes-masters-0 更新完毕后，依次更新 threenodes-masters-1、threenodes-masters-2\n最终全部更新完毕\n查看更新后的资源情况：\n可以发现，结果与预期的一致\n  autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  ","subcategory":null,"summary":"","tags":null,"title":"资源扩容","url":"/easysearch/v1.15.0/docs/getting-started/install/operator/resource_manager/"},{"category":null,"content":"范围（Range）字段类型 #  以下表格列出了 Easysearch 支持的所有范围字段类型。\n   字段数据类型 描述     integer_range 整数值范围。   long_range 长整型值范围。   double_range 双精度浮点值范围。   float_range 浮点值范围。   ip_range IPv4 或 IPv6 地址范围，起始和结束地址可使用不同格式。   date_range 日期值范围，起始和结束日期可采用不同格式。内部以 64 位无符号整数存储，自纪元以来的毫秒数表示。    参考代码 #  创建一个有双精度浮点数范围字段和日期范围字段的映射\nPUT testindex { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;gpa\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;double_range\u0026quot; }, \u0026quot;graduation_date\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;date_range\u0026quot;, \u0026quot;format\u0026quot; : \u0026quot;strict_year_month||strict_year_month_day\u0026quot; } } } } 索引一个包含这两个字段的文档\nPUT testindex/_doc/1 { \u0026quot;gpa\u0026quot; : { \u0026quot;gte\u0026quot; : 1.0, \u0026quot;lte\u0026quot; : 4.0 }, \u0026quot;graduation_date\u0026quot; : { \u0026quot;gte\u0026quot; : \u0026quot;2019-05-01\u0026quot;, \u0026quot;lte\u0026quot; : \u0026quot;2019-05-15\u0026quot; } } IP 地址范围字段 #  您可以使用两种格式指定 IP 地址范围字段：范围表示法和 CIDR 表示法。\n创建一个有 IP 地址范围字段两个格式的映射\nPUT testindex { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;ip_address_range\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;ip_range\u0026quot; }, \u0026quot;ip_address_cidr\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;ip_range\u0026quot; } } } } 索引一个包含这两个 IP 地址范围字段格式的文档\nPUT testindex/_doc/2 { \u0026quot;ip_address_range\u0026quot; : { \u0026quot;gte\u0026quot; : \u0026quot;10.24.34.0\u0026quot;, \u0026quot;lte\u0026quot; : \u0026quot;10.24.35.255\u0026quot; }, \u0026quot;ip_address_cidr\u0026quot; : \u0026quot;10.24.34.0/24\u0026quot; } 查询范围字段 #  您可以使用 Term 查询 或 Range 查询对范围字段进行搜索。\nTerm 查询 #  Term 查询可以使用一个具体值匹配到所有符合条件的范围字段。\n例如，以下查询将返回文档 1，因为 3.5 位于文档 1 的范围 [1.0, 4.0] 之内。\nGET testindex/_search { \u0026quot;query\u0026quot; : { \u0026quot;term\u0026quot; : { \u0026quot;gpa\u0026quot; : { \u0026quot;value\u0026quot; : 3.5 } } } } 范围查询 #  对范围字段的范围查询会返回位于指定范围内的文档。\n例如，可以查询 2019 年的所有毕业日期，并以“MM/dd/yyyy”格式提供日期范围。\nGET testindex/_search { \u0026quot;query\u0026quot;: { \u0026quot;range\u0026quot;: { \u0026quot;graduation_date\u0026quot;: { \u0026quot;gte\u0026quot;: \u0026quot;01/01/2019\u0026quot;, \u0026quot;lte\u0026quot;: \u0026quot;12/31/2019\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;MM/dd/yyyy\u0026quot;, \u0026quot;relation\u0026quot; : \u0026quot;within\u0026quot; } } } } 上面的查询在“within”和“intersects”关系中会返回文档 1，但在“contains”关系中不会返回它。\n参数详解 #  下面列出了范围字段的设置参数，所有参数都是可选的\n   参数 描述 默认值     boost 浮点值，指定字段对相关性评分的权重，大于 1.0 提高相关性，0.0 到 1.0 降低相关性。 1.0   coerce 布尔值，允许将小数截断为整数值，并将字符串转换为数值类型。 true   index 布尔值，指定字段是否可被搜索。 true   store 布尔值，指定字段值是否单独存储并可从 _source 外检索。 false    ","subcategory":null,"summary":"","tags":null,"title":"范围字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/range-field-type/"},{"category":null,"content":"Completion 自动补全字段类型 #  自动补全字段类型通过补全建议器提供自动补全功能。补全建议器是一个前缀建议器，所以它只匹配文本的开头部分。补全建议器会创建一个内存中的数据结构，这提供了更快的查找速度，但会导致内存使用增加。在使用此功能之前，你需要将所有可能的补全项上传到索引中。\n代码样例 #  创建一个包含补全字段的映射：\nPUT chess_store { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;suggestions\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;completion\u0026#34; }, \u0026#34;product\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } 将建议内容索引到 Easysearch 中：\nPUT chess_store/_doc/1 { \u0026#34;suggestions\u0026#34;: { \u0026#34;input\u0026#34;: [\u0026#34;Books on openings\u0026#34;, \u0026#34;Books on endgames\u0026#34;], \u0026#34;weight\u0026#34; : 10 } } 参数 #  下表列出了补全字段接受的参数。\n   参数 描述     input 可能的补全项列表，可以是字符串或字符串数组。不能包含 \\u0000 (null),\\u001f (信息分隔符一) 或 \\u001e (信息分隔符二)。必需。   weight 用于对建议进行排序的正整数或正整数字符串。可选。    可以按以下方式索引多个建议：\nPUT chess_store/_doc/2 { \u0026#34;suggestions\u0026#34;: [ { \u0026#34;input\u0026#34;: \u0026#34;Chess set\u0026#34;, \u0026#34;weight\u0026#34;: 20 }, { \u0026#34;input\u0026#34;: \u0026#34;Chess pieces\u0026#34;, \u0026#34;weight\u0026#34;: 10 }, { \u0026#34;input\u0026#34;: \u0026#34;Chess board\u0026#34;, \u0026#34;weight\u0026#34;: 5 } ] } 或者，你可以使用以下简写方式（注意在这种表示法中不能提供 weight 参数）：\nPUT chess_store/_doc/3 { \u0026#34;suggestions\u0026#34; : [ \u0026#34;Chess clock\u0026#34;, \u0026#34;Chess timer\u0026#34; ] } 查询补全字段类型 #  要查询补全字段类型，需要指定要搜索的前缀和要查找建议的字段名称。\n查询以单词 \u0026ldquo;chess\u0026rdquo; 开头的内容建议：\nGET chess_store/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;product-suggestions\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;chess\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggestions\u0026#34; } } } } 返回内容包含自动补全建议：\n{ \u0026#34;took\u0026#34; : 3, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 0, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : null, \u0026#34;hits\u0026#34; : [ ] }, \u0026#34;suggest\u0026#34; : { \u0026#34;product-suggestions\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;chess\u0026#34;, \u0026#34;offset\u0026#34; : 0, \u0026#34;length\u0026#34; : 5, \u0026#34;options\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;Chess set\u0026#34;, \u0026#34;_index\u0026#34; : \u0026#34;chess_store\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34; : 20.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : [ { \u0026#34;input\u0026#34; : \u0026#34;Chess set\u0026#34;, \u0026#34;weight\u0026#34; : 20 }, { \u0026#34;input\u0026#34; : \u0026#34;Chess pieces\u0026#34;, \u0026#34;weight\u0026#34; : 10 }, { \u0026#34;input\u0026#34; : \u0026#34;Chess board\u0026#34;, \u0026#34;weight\u0026#34; : 5 } ] } }, { \u0026#34;text\u0026#34; : \u0026#34;Chess clock\u0026#34;, \u0026#34;_index\u0026#34; : \u0026#34;chess_store\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : [ \u0026#34;Chess clock\u0026#34;, \u0026#34;Chess timer\u0026#34; ] } } ] } ] } } 在返回内容中，_score 字段包含了在索引时设置的 weight 参数值。text 字段填充了建议的 input 参数。\n默认情况下，响应包含整个文档，包括 _source 字段，这可能会影响性能。要只返回 suggestions 字段，你可以在 _source 参数中指定。你还可以通过指定 size 参数来限制返回的建议数量。\nGET chess_store/_search { \u0026#34;_source\u0026#34;: \u0026#34;suggestions\u0026#34;, \u0026#34;suggest\u0026#34;: { \u0026#34;product-suggestions\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;chess\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggestions\u0026#34;, \u0026#34;size\u0026#34; : 3 } } } } 返回内容会包含建议内容：\n{ \u0026#34;took\u0026#34; : 5, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 0, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : null, \u0026#34;hits\u0026#34; : [ ] }, \u0026#34;suggest\u0026#34; : { \u0026#34;product-suggestions\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;chess\u0026#34;, \u0026#34;offset\u0026#34; : 0, \u0026#34;length\u0026#34; : 5, \u0026#34;options\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;Chess set\u0026#34;, \u0026#34;_index\u0026#34; : \u0026#34;chess_store\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34; : 20.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : [ { \u0026#34;input\u0026#34; : \u0026#34;Chess set\u0026#34;, \u0026#34;weight\u0026#34; : 20 }, { \u0026#34;input\u0026#34; : \u0026#34;Chess pieces\u0026#34;, \u0026#34;weight\u0026#34; : 10 }, { \u0026#34;input\u0026#34; : \u0026#34;Chess board\u0026#34;, \u0026#34;weight\u0026#34; : 5 } ] } }, { \u0026#34;text\u0026#34; : \u0026#34;Chess clock\u0026#34;, \u0026#34;_index\u0026#34; : \u0026#34;chess_store\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : [ \u0026#34;Chess clock\u0026#34;, \u0026#34;Chess timer\u0026#34; ] } } ] } ] } }  要利用源内容过滤功能，请在 _search API 上使用建议功能。_suggest API 不支持源内容过滤。\n 自动补全查询参数 #  下表列出了自动补全查询接受的参数。\n   参数 描述     field 一个字符串，指定要运行查询的字段。必需。   size 一个整数，指定返回的建议的最大数量。可选。默认值为 5。   skip_duplicates 一个布尔值，指定是否跳过重复的建议。可选。默认值为 false。    自动补全的模糊查询 #  要允许模糊匹配，可以为补全查询指定 fuzziness 参数。在这种情况下，即使用户输入错误的搜索词，自动补全查询仍然会返回结果。此外，匹配查询的前缀越长，文档的得分越高。\nGET chess_store/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;product-suggestions\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;chesc\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggestions\u0026#34;, \u0026#34;size\u0026#34; : 3, \u0026#34;fuzzy\u0026#34; : { \u0026#34;fuzziness\u0026#34; : \u0026#34;AUTO\u0026#34; } } } } }  要使用所有默认的模糊选项，可以指定 \u0026quot;fuzzy\u0026quot;: {} 或 \u0026quot;fuzzy\u0026quot;: true。\n 下表列出了自动补全的模糊查询接受的参数。所有参数都是可选的。\n   参数 描述     fuzziness 模糊度可以设置为以下之一：1. 一个整数，指定允许的最大 Levenshtein 距离。2. AUTO：0-2 个字符的字符串必须完全匹配，3-5 个字符的字符串允许 1 次编辑，超过 5 个字符的字符串允许 2 次编辑。默认值为 AUTO。   min_length 一个整数，指定输入的最小长度，以开始返回建议。如果搜索词的长度小于 min_length，则不返回建议。默认值为 3。   prefix_length 一个整数，指定匹配的前缀的最小长度，以开始返回建议。如果 prefix_length 的前缀不匹配，但搜索词仍然在 Levenshtein 距离内，则不返回建议。默认值为 1。   transpositions 一个布尔值，指定将相邻字符的交换（transpositions）计为一次编辑，而不是两次编辑。示例：建议的 input 参数是 abcde，fuzziness 是 1。如果 transpositions 设置为 true，则 abdce 匹配，但如果 transpositions 设置为 false，则 abdce 不匹配。默认值为 true。   unicode_aware 一个布尔值，指定是否使用 Unicode 代码点来测量编辑距离、转置和长度。如果 unicode_aware 设置为 true，则测量速度较慢。默认值为 false，在这种情况下，距离以字节为单位测量。    正则表达式查询 #  可以使用正则表达式来定义自动补全查询的前缀。\n例如，要搜索以 \u0026ldquo;a\u0026rdquo; 开头并且后面有 \u0026ldquo;d\u0026rdquo; 的字符串，可以使用以下查询：\nGET chess_store/_search { \u0026#34;suggest\u0026#34;: { \u0026#34;product-suggestions\u0026#34;: { \u0026#34;regex\u0026#34;: \u0026#34;a.*d\u0026#34;, \u0026#34;completion\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;suggestions\u0026#34; } } } } 返回内容匹配字符串 \u0026ldquo;abcde\u0026rdquo;：\n{ \u0026#34;took\u0026#34; : 2, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 0, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : null, \u0026#34;hits\u0026#34; : [ ] }, \u0026#34;suggest\u0026#34; : { \u0026#34;product-suggestions\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;a.*d\u0026#34;, \u0026#34;offset\u0026#34; : 0, \u0026#34;length\u0026#34; : 4, \u0026#34;options\u0026#34; : [ { \u0026#34;text\u0026#34; : \u0026#34;abcde\u0026#34;, \u0026#34;_index\u0026#34; : \u0026#34;chess_store\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34; : 20.0, \u0026#34;_source\u0026#34; : { \u0026#34;suggestions\u0026#34; : [ { \u0026#34;input\u0026#34; : \u0026#34;abcde\u0026#34;, \u0026#34;weight\u0026#34; : 20 } ] } } ] } ] } } ","subcategory":null,"summary":"","tags":null,"title":"自动补全字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/autocomplete-field-type/completion/"},{"category":null,"content":"索引生命周期管理 #  使用索引状态管理操作，以编程方式处理策略和托管索引。\n 创建策略 #  引入版本 1.0\n创建一个策略。\n请求示例 #  PUT _ilm/policy/ilm_test { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;hot\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;0ms\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;rollover\u0026quot;: { \u0026quot;max_age\u0026quot;: \u0026quot;10m\u0026quot;, \u0026quot;max_size\u0026quot;: \u0026quot;1mb\u0026quot;, \u0026quot;max_docs\u0026quot;: 100 }, \u0026quot;set_priority\u0026quot;: { \u0026quot;priority\u0026quot;: 100 } } }, \u0026quot;delete\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;15m\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;delete\u0026quot;: { } } } } } } 示例响应 #  { \u0026quot;acknowledged\u0026quot;: true } 应用生命周期策略到索引模板 要让策略生效，需要在索引模板中指定策略名称和滚动索引的别名。\n请求示例 #  PUT /_index_template/my_template?pretty { \u0026quot;index_patterns\u0026quot;: [\u0026quot;log-test-*\u0026quot;], \u0026quot;template\u0026quot;: { \u0026quot;settings\u0026quot;: { \u0026quot;number_of_shards\u0026quot;: 1, \u0026quot;number_of_replicas\u0026quot;: 1, \u0026quot;index.lifecycle.name\u0026quot;: \u0026quot;ilm_test\u0026quot;, \u0026quot;index.lifecycle.rollover_alias\u0026quot;: \u0026quot;log-test\u0026quot; } } } 创建初始托管索引 当您为自己的滚动索引设置策略时，您需要手动创建第一个由策略管理的索引，并将其指定为写入索引。 索引的名称必须与索引模板中定义的模式匹配，并以数字结尾。该数字增加以生成通过rollover操作创建的索引名称。\n请求示例 #  PUT /log-test-000001 { \u0026quot;aliases\u0026quot;: { \u0026quot;log-test\u0026quot;:{ \u0026quot;is_write_index\u0026quot;: true } } }  获取策略 #  引入版本 1.0\n通过 policy_id 获取策略。\n请求示例 #  GET _ilm/policy/ilm_test 示例响应 #  { \u0026quot;_id\u0026quot;: \u0026quot;ilm_test\u0026quot;, \u0026quot;_version\u0026quot;: 3, \u0026quot;_seq_no\u0026quot;: 27, \u0026quot;_primary_term\u0026quot;: 2, \u0026quot;policy\u0026quot;: { \u0026quot;policy_id\u0026quot;: \u0026quot;ilm_test\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;last_updated_time\u0026quot;: 1682049301487, \u0026quot;schema_version\u0026quot;: 17, \u0026quot;default_state\u0026quot;: \u0026quot;hot\u0026quot;, \u0026quot;states\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;hot\u0026quot;, \u0026quot;actions\u0026quot;: [ { \u0026quot;retry\u0026quot;: { \u0026quot;count\u0026quot;: 3, \u0026quot;backoff\u0026quot;: \u0026quot;exponential\u0026quot;, \u0026quot;delay\u0026quot;: \u0026quot;1m\u0026quot; }, \u0026quot;rollover\u0026quot;: { \u0026quot;min_size\u0026quot;: \u0026quot;1mb\u0026quot;, \u0026quot;min_doc_count\u0026quot;: 100, \u0026quot;min_index_age\u0026quot;: \u0026quot;10m\u0026quot; } }, { \u0026quot;retry\u0026quot;: { \u0026quot;count\u0026quot;: 3, \u0026quot;backoff\u0026quot;: \u0026quot;exponential\u0026quot;, \u0026quot;delay\u0026quot;: \u0026quot;1m\u0026quot; }, \u0026quot;index_priority\u0026quot;: { \u0026quot;priority\u0026quot;: 100 } } ], \u0026quot;transitions\u0026quot;: [ { \u0026quot;state_name\u0026quot;: \u0026quot;delete\u0026quot;, \u0026quot;conditions\u0026quot;: { \u0026quot;min_index_age\u0026quot;: \u0026quot;15m\u0026quot; } } ] }, { \u0026quot;name\u0026quot;: \u0026quot;delete\u0026quot;, \u0026quot;actions\u0026quot;: [ { \u0026quot;retry\u0026quot;: { \u0026quot;count\u0026quot;: 3, \u0026quot;backoff\u0026quot;: \u0026quot;exponential\u0026quot;, \u0026quot;delay\u0026quot;: \u0026quot;1m\u0026quot; }, \u0026quot;delete\u0026quot;: {} } ], \u0026quot;transitions\u0026quot;: [] } ], \u0026quot;ilm_template\u0026quot;: [ { \u0026quot;index_patterns\u0026quot;: [], \u0026quot;priority\u0026quot;: 100, \u0026quot;last_updated_time\u0026quot;: 1682049301487 } ] } }  解释索引策略 #  引入版本 1.0\n获取索引的当前状态。您可以使用索引模式来获取多个索引的状态。\n请求示例 #  GET /_ilm/explain/log-test 示例响应 #  { \u0026quot;log-test-000003\u0026quot;: { \u0026quot;index.lifecycle.name\u0026quot;: \u0026quot;ilm_test\u0026quot;, \u0026quot;index\u0026quot;: \u0026quot;log-test-000003\u0026quot;, \u0026quot;index_uuid\u0026quot;: \u0026quot;58YtMBxaRcGBvOhHcJwQ5w\u0026quot;, \u0026quot;policy_id\u0026quot;: \u0026quot;ilm_test\u0026quot;, \u0026quot;policy_seq_no\u0026quot;: -2, \u0026quot;policy_primary_term\u0026quot;: 0, \u0026quot;rolled_over\u0026quot;: false, \u0026quot;index_creation_date\u0026quot;: 1682060631245, \u0026quot;state\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;hot\u0026quot;, \u0026quot;start_time\u0026quot;: 1682060706227 }, \u0026quot;action\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;rollover\u0026quot;, \u0026quot;start_time\u0026quot;: 1682060764614, \u0026quot;index\u0026quot;: 0, \u0026quot;failed\u0026quot;: false, \u0026quot;consumed_retries\u0026quot;: 0, \u0026quot;last_retry_time\u0026quot;: 0 }, \u0026quot;step\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;attempt_rollover\u0026quot;, \u0026quot;start_time\u0026quot;: 1682060764614, \u0026quot;step_status\u0026quot;: \u0026quot;condition_not_met\u0026quot; }, \u0026quot;retry_info\u0026quot;: { \u0026quot;failed\u0026quot;: false, \u0026quot;consumed_retries\u0026quot;: 0 }, \u0026quot;info\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;Pending rollover of index [index=log-test-000003]\u0026quot;, \u0026quot;conditions\u0026quot;: { \u0026quot;min_index_age\u0026quot;: { \u0026quot;condition\u0026quot;: \u0026quot;10m\u0026quot;, \u0026quot;current\u0026quot;: \u0026quot;2.2m\u0026quot;, \u0026quot;creationDate\u0026quot;: 1682060631245 }, \u0026quot;min_size\u0026quot;: { \u0026quot;condition\u0026quot;: \u0026quot;1mb\u0026quot;, \u0026quot;current\u0026quot;: \u0026quot;0b\u0026quot; }, \u0026quot;min_doc_count\u0026quot;: { \u0026quot;condition\u0026quot;: 100, \u0026quot;current\u0026quot;: 0 } } }, \u0026quot;enabled\u0026quot;: true }, \u0026quot;log-test-000002\u0026quot;: { \u0026quot;index.lifecycle.name\u0026quot;: \u0026quot;ilm_test\u0026quot;, \u0026quot;index\u0026quot;: \u0026quot;log-test-000002\u0026quot;, \u0026quot;index_uuid\u0026quot;: \u0026quot;uxZpQXqdRmGZT5xS1WW6ZA\u0026quot;, \u0026quot;policy_id\u0026quot;: \u0026quot;ilm_test\u0026quot;, \u0026quot;policy_seq_no\u0026quot;: -2, \u0026quot;policy_primary_term\u0026quot;: 0, \u0026quot;rolled_over\u0026quot;: true, \u0026quot;index_creation_date\u0026quot;: 1682060028261, \u0026quot;state\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;hot\u0026quot;, \u0026quot;start_time\u0026quot;: 1682060109834 }, \u0026quot;action\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;transition\u0026quot;, \u0026quot;start_time\u0026quot;: 1682060767610, \u0026quot;index\u0026quot;: -1, \u0026quot;failed\u0026quot;: false, \u0026quot;consumed_retries\u0026quot;: 0, \u0026quot;last_retry_time\u0026quot;: 0 }, \u0026quot;step\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;attempt_transition_step\u0026quot;, \u0026quot;start_time\u0026quot;: 1682060767610, \u0026quot;step_status\u0026quot;: \u0026quot;condition_not_met\u0026quot; }, \u0026quot;retry_info\u0026quot;: { \u0026quot;failed\u0026quot;: false, \u0026quot;consumed_retries\u0026quot;: 0 }, \u0026quot;info\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;Evaluating transition conditions [index=log-test-000002]\u0026quot; }, \u0026quot;enabled\u0026quot;: true }, \u0026quot;total_managed_indices\u0026quot;: 2 }  更新策略 #  需要在请求url里指定 seq_no 和 primary_term ，并注意名称要改成 if_seq_no=7\u0026amp;if_primary_term=1 这种\n请求示例 #  PUT _ilm/policy/my_policy?if_seq_no=2545\u0026amp;if_primary_term=1 { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;delete\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;wait_for_snapshot\u0026quot; : { \u0026quot;policy\u0026quot;: \u0026quot;daily-policy5\u0026quot; }, \u0026quot;delete\u0026quot;: {} } } } } }  删除策略 #  引入版本 1.0\n通过 policy_id 删除策略。\n请求示例 #  DELETE _ilm/policy/ilm_test 示例响应 #  { \u0026quot;_index\u0026quot;: \u0026quot;.easysearch-ilm-config\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;ilm_test\u0026quot;, \u0026quot;_version\u0026quot;: 2, \u0026quot;result\u0026quot;: \u0026quot;deleted\u0026quot;, \u0026quot;forced_refresh\u0026quot;: true, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 2, \u0026quot;successful\u0026quot;: 1, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;_seq_no\u0026quot;: 10, \u0026quot;_primary_term\u0026quot;: 1 }  操作 #  forcemerge #  通过合并单个分片的段（segment）来减少 Lucene 段的数量。此操作在开始合并过程之前尝试将索引设置为 read-only 状态。\n   参数 描述 类型 是否必需     max_num_segments 需要将分片合并到的段数目。 number Yes    PUT /_ilm/policy/my_policy?pretty { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;warm\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;forcemerge\u0026quot; : { \u0026quot;max_num_segments\u0026quot;: 1 } } } } } } readonly #  将托管索引设置为只读。\nPUT _ilm/policy/my_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;warm\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;readonly\u0026quot; : { } } } } } } 为托管索引设置 index.blocks.write 索引设置为 true 。注意: 该设置并不能阻止索引刷新。\nread_write #  将托管索引设置为可写。\nPUT _ilm/policy/my_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;warm\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;read_write\u0026quot; : { } } } } } } replica_count #  设置要分配给索引的副本数量。\n   Parameter Description Type Required     number_of_replicas 定义要分配给索引的副本数量。 number Yes    PUT _ilm/policy/my_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;warm\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;replica_count\u0026quot;: { \u0026quot;number_of_replicas\u0026quot;: 2 } } } } } } close #  关闭托管索引。\n关闭的索引仍然存在于磁盘上，但不会消耗 CPU 或内存。关闭的索引不能读取、写入或搜索。\n如果您需要保留数据的时间长于需要主动搜索它的时间，并且在数据节点上有足够的磁盘空间，则关闭索引是一个不错的选择。如果需要再次搜索数据，则重新打开关闭的索引比从快照中恢复索引更简单。\nopen #  打开一个托管索引。\ndelete #  删除一个托管索引，将其从集群中完全移除。\n   Parameter Description Type Required     timestamp_field 用于判断索引数据年龄的时间戳字段名称。如果指定了此参数，则必须同时指定min_data_age参数。 string No   min_data_age 索引中最新数据的最小年龄要求，只有当最新数据的年龄大于此值时才会执行删除操作。必须以天为单位（例如\u0026quot;7d\u0026quot;），并且最小值为1天。 string No    当timestamp_field和min_data_age同时指定时，系统会执行以下检查：\n 查询索引中按照指定时间戳字段排序的最新文档 计算最新文档的时间与当前时间的差值 只有当差值大于或等于min_data_age指定的时间时，才会执行删除操作  这种机制可以防止意外删除还在活跃使用的索引数据。\n基本用法示例：\nPUT _ilm/policy/my_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;delete\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;30d\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;delete\u0026quot;: {} } } } } } 带条件检查的删除示例：\nPUT _ilm/policy/conditional_delete_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;delete\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;30d\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;delete\u0026quot;: { \u0026quot;timestamp_field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;min_data_age\u0026quot;: \u0026quot;90d\u0026quot; } } } } } } 在上面的示例中，即使索引满足了30天的min_age条件进入删除阶段，也只有当索引中最新的文档（基于@timestamp字段）已经至少90天未更新时，才会执行实际的删除操作。\nrollover #  当现有索引满足指定的滚动条件时，该操作会将目标切换到新索引。\n滚动目标可以是数据流或索引别名。当目标为数据流时，新索引将变为数据流的写索引，并且其代数将递增。\n如果要滚动索引别名，别名及其写索引必须满足以下条件：\n 索引名称必须符合模式 ^.\\*-\\d+$，例如（my-index-000001）。 必须将 index.lifecycle.rollover_alias 配置为要滚动的别名。 索引必须是别名的写索引。     参数 描述 类型 示例 是否必需     max_size （可选，字节单位）当索引达到一定大小时触发滚动。这是索引中所有主分片的总大小。副本不计算在最大索引大小内。 string 20gb or 5mb No   max_docs （可选，整数）在达到指定的最大文档数后触发滚动。自上次刷新后添加的文档不包括在文档计数中。文档计数不包括副本分片中的文档。 number 2000000 No   max_age （可选，时间单位）在达到自索引创建开始的最大经过时间后触发滚动。经过的时间始终是从索引创建时间开始计算的，即使索引起始日期被配置为自定义日期. 支持的单位 d (days), h (hours), m (minutes), s (seconds), ms (milliseconds), and micros (microseconds)。 string 5d or 7h No    PUT _ilm/policy/my_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;hot\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;rollover\u0026quot; : { \u0026quot;max_size\u0026quot;: \u0026quot;100gb\u0026quot; } } } } } } PUT _ilm/policy/my_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;hot\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;rollover\u0026quot; : { \u0026quot;max_docs\u0026quot;: 100000000 } } } } } } PUT _ilm/policy/my_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;hot\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;rollover\u0026quot; : { \u0026quot;max_age\u0026quot;: \u0026quot;7d\u0026quot; } } } } } } snapshot #  备份你的集群索引和状态。\nsnapshot 操作具有以下参数:\n   参数 描述 类型 是否必须 默认     repository 通过本机快照 API 操作注册的仓库名称。 string Yes -   snapshot 快照的名称。接受字符串和 Mustache 变量 and。如果 Mustache 变量无效，则快照名称默认为索引的名称。 string or Mustache template Yes -    PUT _ilm/policy/my_policy2 { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;hot\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;rollover\u0026quot;: { \u0026quot;max_size\u0026quot;: \u0026quot;25GB\u0026quot; } } }, \u0026quot;cold\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;snapshot\u0026quot;: { \u0026quot;repository\u0026quot;: \u0026quot;my_repository\u0026quot;, \u0026quot;snapshot\u0026quot;: \u0026quot;\u0026quot; } } } } } } allocate #  将索引分配到具有特定属性集的节点上类似于此。例如，将 require 设置为 warm，只会将数据移动到“warm”节点上。\nallocate 操作有以下参数:\n   参数 描述 类型 是否必须     require 将索引分配给带有指定属性的节点。 string Yes   include 将索引分配给具有任何指定属性的节点。 string Yes   exclude 不将索引分配给具有任何指定属性的节点。 string Yes    以下策略中的分配操作将索引分配给 box_type 为 hot 或 warm 的节点。\nPUT _ilm/policy/my_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;warm\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;allocate\u0026quot; : { \u0026quot;include\u0026quot; : { \u0026quot;box_type\u0026quot;: \u0026quot;hot,warm\u0026quot; } } } } } } } 以下策略中的 allocate 操作会更新索引，使其每个 shard 有一个副本，并将其分配给 box_type 为 cold 的节点。\nPUT _ilm/policy/my_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;warm\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;allocate\u0026quot;: { \u0026quot;require\u0026quot;: { \u0026quot;box_type\u0026quot;: \u0026quot;cold\u0026quot; } }, \u0026quot;replica_count\u0026quot;: { \u0026quot;number_of_replicas\u0026quot;: 1 } } } } } } wait_for_snapshot #\n 在删除索引之前，等待执行指定的快照管理策略。这样可以确保已删除索引的快照可用。\nwait_for_snapshot 操作具有以下参数:\n   参数 描述 类型 是否必须 默认     policy 删除操作应等待的快照管理策略的名称。 string Yes -    PUT _ilm/policy/my_policy { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;delete\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;wait_for_snapshot\u0026quot; : { \u0026quot;policy\u0026quot;: \u0026quot;daily-policy5\u0026quot; }, \u0026quot;delete\u0026quot;: {} } } } } }  \n","subcategory":null,"summary":"","tags":null,"title":"索引生命周期管理","url":"/easysearch/v1.15.0/docs/references/management/ilm_api/"},{"category":null,"content":"Index 索引属性 #  当跨多个索引进行查询时，您可能需要根据文档所在的索引来过滤结果。index 字段根据文档的索引来匹配文档。\n以下示例创建两个索引，products 和 customers，并向每个索引添加一个文档：\nPUT products/_doc/1 { \u0026quot;name\u0026quot;: \u0026quot;Widget X\u0026quot; } PUT customers/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;John Doe\u0026quot; } 然后，您可以查询这两个索引，并使用 _index 属性过滤结果，如以下示例请求所示：\nGET products,customers/_search { \u0026quot;query\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;_index\u0026quot;: [\u0026quot;products\u0026quot;, \u0026quot;customers\u0026quot;] } }, \u0026quot;aggs\u0026quot;: { \u0026quot;index_groups\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;_index\u0026quot;, \u0026quot;size\u0026quot;: 10 } } }, \u0026quot;sort\u0026quot;: [ { \u0026quot;_index\u0026quot;: { \u0026quot;order\u0026quot;: \u0026quot;desc\u0026quot; } } ], \u0026quot;script_fields\u0026quot;: { \u0026quot;index_name\u0026quot;: { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;painless\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;doc['_index'].value\u0026quot; } } } } 在此示例中：\n query 部分使用 terms 查询来匹配来自 products 和 customers 索引的文档。 aggs 部分对 _index 字段执行 terms 聚合，按索引对结果进行分组。 sort 部分按 _index 字段以升序对结果进行排序。 script_fields 部分向搜索结果添加一个名为 index_name 的新字段，其中包含每个文档的 _index 字段值。  在 _index 字段上查询 #  _index 字段表示文档所在的索引。您可以在查询中使用此字段来过滤、聚合、排序或检索搜索结果的索引信息。\n由于 _index 字段会自动添加到每个文档中，您可以像使用其他字段一样在查询中使用它。例如，您可以使用 terms 查询来匹配来自多个索引的文档。以下示例查询返回来自 products 和 customers 索引的所有文档：\n{ \u0026quot;query\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;_index\u0026quot;: [\u0026quot;products\u0026quot;, \u0026quot;customers\u0026quot;] } } } ","subcategory":null,"summary":"","tags":null,"title":"索引属性","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/metadata-field/"},{"category":null,"content":"Source 源文档属性 #  _source 字段包含已索引的原始 JSON 文档主体。虽然此字段不可搜索，但它会被存储，以便在执行获取请求（如 get 和 search）时可以返回完整文档。\n禁用_source #  您可以通过将 enabled 参数设置为 false 来禁用 _source 字段，如以下示例所示：\nPUT sample-index1 { \u0026quot;mappings\u0026quot;: { \u0026quot;_source\u0026quot;: { \u0026quot;enabled\u0026quot;: false } } }  禁用 _source 字段可能会影响某些功能的可用性，例如 update、update_by_query 和 reindex API，以及使用原始索引文档查询或聚合的能力。\n 包含或排除某些字段 #  您可以使用 includes 和 excludes 参数选择 _source 字段的内容。如以下示例：\nPUT logs { \u0026quot;mappings\u0026quot;: { \u0026quot;_source\u0026quot;: { \u0026quot;includes\u0026quot;: [ \u0026quot;*.count\u0026quot;, \u0026quot;meta.*\u0026quot; ], \u0026quot;excludes\u0026quot;: [ \u0026quot;meta.description\u0026quot;, \u0026quot;meta.other.*\u0026quot; ] } } } 这些字段不会存储在 _source 中，但您仍然可以搜索它们，因为数据仍然被索引。\n","subcategory":null,"summary":"","tags":null,"title":"源文档属性","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/metadata-field/source/"},{"category":null,"content":"Boost 权重参数 #  boost 映射参数用于在搜索查询期间增加或减少字段的相关性分数。它允许您在计算文档的整体相关性分数时，对特定字段应用更多或更少的权重。\nboost 参数作为字段分数的乘数应用。例如，如果一个字段的 boost 值为 2，则该字段的分数的权重将翻倍。相反，boost 值为 0.5 将使该字段的分数的权重减半。\n代码样例 #  以下是在 Easysearch 映射中使用 boost 参数的示例：\nPUT my-index1 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;title\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;boost\u0026quot;: 2 }, \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;boost\u0026quot;: 1 }, \u0026quot;tags\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;boost\u0026quot;: 1.5 } } } } 在此示例中，title 字段的提升值为 2，这意味着它对整体相关性分数的权重是描述字段（提升值为 1）的两倍。tags 字段的提升值为 1.5，因此它的权重是描述字段的一倍半。\n当您想要对某些字段赋予更多权重时，boost 参数特别有用。例如，您可能想要将 title 字段的权重提升得比 description 字段更高，因为标题更能文档的相关性。\nboost 参数是一个乘法因子，而不是加法因子。这意味着与具有较低权重值的字段相比，具有较高权重值的字段对整体相关性分数的影响将不成比例地大。在使用 boost 参数时，建议您从小值（1.5 或 2）开始，并测试其对搜索结果的影响。过高的权重值可能会扭曲相关性分数，并导致意外或不理想的搜索结果。\n","subcategory":null,"summary":"","tags":null,"title":"权重参数设置","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/mapping-parameters/boost/"},{"category":null,"content":"Enabled 启用参数 #  enabled 参数允许您控制 Easysearch 是否解析字段的内容。此参数可以应用于顶级映射定义和对象字段。\nenabled 参数接受以下值：\n   参数 描述     true 字段被解析和索引。默认值为 true。   false 字段不被解析或索引，但仍可从 _source 字段中检索。当 enabled 设置为 false 时，Easysearch 将字段的值存储在 _source 字段中，但不索引或解析其内容。这对于您想要存储但不需要搜索、排序或聚合的字段很有用。    示例：使用 enabled 参数 #  在以下示例请求中，session_data 字段被禁用。Easysearch 将其内容存储在 _source 字段中，但不对其进行索引或解析：\nPUT my-index-002 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;user_id\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;last_updated\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot; }, \u0026quot;session_data\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;, \u0026quot;enabled\u0026quot;: false } } } } ","subcategory":null,"summary":"","tags":null,"title":"是否启用参数","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/mapping-parameters/enabled/"},{"category":null,"content":"doc_values 文档值参数 #  默认情况下，Easysearch 会为搜索目的索引大多数字段的字段值。doc_values 参数启用文档到词项的正排查找，用于排序、聚合和脚本等操作。\ndoc_values 参数接受以下选项。\n   选项 Option 描述     true true 启用字段的 doc_values。默认值为 true。   false false 禁用字段的 doc_values。    示例：创建启用和禁用 doc_values 的索引 #  以下示例请求创建一个索引，其中一个字段启用 doc_values，另一个字段禁用：\nPUT my-index-001 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;status_code\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;session_id\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;doc_values\u0026quot;: false } } } } ","subcategory":null,"summary":"","tags":null,"title":"文档列值参数","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/mapping-parameters/doc_values/"},{"category":null,"content":"Numeric 字段类型 #  下表列出了 Easysearch 支持的所有数字字段类型。\n   字段数据类型 描述     byte 有符号的 8 位整数。最小值为 -128，最大值为 127。   double 双精度 64 位 IEEE 754 浮点数。最小值为 2^−1074，最大值为 (2 − 2^−52) · 2^1023。有效位数为 53，有效数字位为 15.95。   float 单精度 32 位 IEEE 754 浮点数。最小值为 2^−149，最大值为 (2 − 2^−23) · 2^127。有效位数为 24，有效数字位为 7.22。   half_float 半精度 16 位 IEEE 754 浮点数。最小值为 2^−24，最大值为 65504。有效位数为 11，有效数字位为 3.31。   integer 有符号的 32 位整数。最小值为 -2^31，最大值为 2^31 - 1。   long 有符号的 64 位整数。最小值为 -2^63，最大值为 2^63 - 1。   unsigned_long 无符号的 64 位整数。最小值为 0，最大值为 2^64 - 1。   short 有符号的 16 位整数。最小值为 -2^15，最大值为 2^15 - 1。   scaled_float 一个浮点值，它会被乘以双精度缩放因子并存储为长整型值。     Integer、long、float 和 double 字段类型都有对应的 范围字段类型。\n  如果你的数字字段用来做标识符（如 ID），你可以将此字段映射为 keyword 以优化 term 查询的速度。如果你需要对此字段使用范围查询，你可以将此字段同时映射为数字字段类型和关键字字段类型。\n 示例 #  创建一个映射，其中 integer_value 是一个整数字段：\nPUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;integer_value\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } 写入一个integer_value的文档 PUT testindex/_doc/1 { \u0026quot;integer_value\u0026quot;: 123 } Scaled float 字段类型 #\n scaled float 字段类型是一个浮点值，它会被乘以缩放因子并存储为长整型值。它接受所有数字字段类型的可选参数，还需要一个额外的 scaling_factor 参数。在创建 scaled float 需要添加缩放因子 scale factor。\n scaled float 对于节省磁盘空间很有用。较大的 scaling_factor 值会带来更好的精度，但会占用更多空间。\n Scaled float 示例 #  创建一个映射，其中 scaled 是一个 scaled_float 字段：\nPUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;scaled\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;scaled_float\u0026#34;, \u0026#34;scaling_factor\u0026#34;: 10 } } } } 写入一个包含 scaled_float 字段的文档 PUT testindex/_doc/1 { \u0026quot;scaled\u0026quot;: 2.3 } scaled 值将被存储为 23。\n参数 #  下表列出了数字字段类型接受的参数。所有参数都是可选的。\n   参数 描述     boost 浮点值，指定此字段的权重。值大于 1.0 会增加此字段的相关性。值在 0.0 和 1.0 之间会降低此字段的相关性。默认值为 1.0。   coerce 布尔值，指定是否允许强制类型转换。如果为 true，则字符串会被强制转换为数字，并且小数会被截断为整数。默认为 true。   doc_values 布尔值，指定是否应该将字段添加到列存储中。默认为 true。   ignore_malformed 布尔值，指定是否应该忽略格式错误的数字。默认为 false。   index 布尔值，指定字段是否应该被索引以用于搜索。默认为 true。   meta 接受此字段的元数据。   null_value 用于替代 null 的值。必须与字段类型相同。如果未指定此参数，当字段值为 null 时，该字段将被视为缺失。默认为 null。   store 布尔值，指定字段值是否应该被存储并且可以与 _source 字段分开检索。默认为 false。    scaled float 有一个额外的必需参数：scaling_factor。\n   参数 描述     scaling_factor 一个双精度值，它会被乘以字段值并四舍五入为最接近的长整型。必需参数。    ","subcategory":null,"summary":"","tags":null,"title":"数值字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/numeric-field/"},{"category":null,"content":"Ignored 忽略属性 #  _ignored 字段帮助您管理文档中与格式错误数据相关的问题。由于在 索引映射中启用了 ignore_malformed 设置，此字段用于存储在数据索引过程中被忽略的字段名称。\n_ignored 字段允许您搜索和识别包含被忽略字段的文档，以及被忽略的具体字段名称。这对于故障排除很有用。\n您可以使用 term、terms 和 exists 查询来查询 _ignored 字段。\n 只有当索引映射中启用了 ignore_malformed 设置时，才会填充 _ignored 字段。如果 ignore_malformed 设置为 false（默认值），则格式错误的字段将导致整个文档被拒绝，并且不会填写 _ignored 字段。\n 以下示例展示了如何使用 _ignored 字段：\nGET _search { \u0026quot;query\u0026quot;: { \u0026quot;exists\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;_ignored\u0026quot; } } } 使用 _ignored 字段的索引请求示例 #  以下示例向 test-ignored 索引添加一个新文档，其中 ignore_malformed 设置为 true，这样在数据索引时不会抛出错误：\nPUT test-ignored { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;title\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;length\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;long\u0026quot;, \u0026quot;ignore_malformed\u0026quot;: true } } } } POST test-ignored/_doc { \u0026quot;title\u0026quot;: \u0026quot;correct text\u0026quot;, \u0026quot;length\u0026quot;: \u0026quot;not a number\u0026quot; }\nGET test-ignored/_search { \u0026quot;query\u0026quot;: { \u0026quot;exists\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;_ignored\u0026quot; } } } 示例返回内容 #\n { \u0026quot;took\u0026quot;: 42, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 1, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 1, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;test-ignored\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;qcf0wZABpEYH7Rw9OT7F\u0026quot;, \u0026quot;_score\u0026quot;: 1, \u0026quot;_ignored\u0026quot;: [ \u0026quot;length\u0026quot; ], \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;correct text\u0026quot;, \u0026quot;length\u0026quot;: \u0026quot;not a number\u0026quot; } } ] } } 忽略指定字段 #  您可以使用 term 查询来查找特定字段被忽略的文档，如以下示例请求所示：\nGET _search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;_ignored\u0026quot;: \u0026quot;created_at\u0026quot; } } } 返回内容 #  { \u0026quot;took\u0026quot;: 51, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 45, \u0026quot;successful\u0026quot;: 45, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 0, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] } } ","subcategory":null,"summary":"","tags":null,"title":"忽略属性","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/metadata-field/ignored/"},{"category":null,"content":"快照生命周期管理 #  使用快照管理（SLM）API 自动创建快照。\n 创建快照策略 #  请求示例 #   每天上午 8 点自动创建一份快照,快照名称格式为 yyyy-MM-dd-HH:mm ，存储在 my_backup 快照仓库 每天凌晨 1 点自动删除最早 7 天前创建的快照、超过 21 个的快照以及保留至少 7 个快照 快照创建和删除的时间限制均为 1 小时  curl -XPOST -uadmin:admin -H 'Content-Type: application/json' 'https://localhost:9200/_slm/policies/daily-policy' -d ' { \u0026quot;description\u0026quot;: \u0026quot;每日快照策略\u0026quot;, \u0026quot;creation\u0026quot;: { \u0026quot;schedule\u0026quot;: { \u0026quot;cron\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;0 8 * * *\u0026quot;, \u0026quot;timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot; } }, \u0026quot;time_limit\u0026quot;: \u0026quot;1h\u0026quot; }, \u0026quot;deletion\u0026quot;: { \u0026quot;schedule\u0026quot;: { \u0026quot;cron\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;0 1 * * *\u0026quot;, \u0026quot;timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot; } }, \u0026quot;condition\u0026quot;: { \u0026quot;max_age\u0026quot;: \u0026quot;7d\u0026quot;, \u0026quot;max_count\u0026quot;: 21, \u0026quot;min_count\u0026quot;: 7 }, \u0026quot;time_limit\u0026quot;: \u0026quot;1h\u0026quot; }, \u0026quot;snapshot_config\u0026quot;: { \u0026quot;date_format\u0026quot;: \u0026quot;yyyy-MM-dd-HH:mm\u0026quot;, \u0026quot;date_format_timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot;, \u0026quot;indices\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;repository\u0026quot;: \u0026quot;my_backup\u0026quot;, \u0026quot;ignore_unavailable\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;include_global_state\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;partial\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;any_key\u0026quot;: \u0026quot;any_value\u0026quot; } } }' 示例响应 #  { \u0026quot;_id\u0026quot;: \u0026quot;daily-policy-sm-policy\u0026quot;, \u0026quot;_version\u0026quot;: 1, \u0026quot;_seq_no\u0026quot;: 0, \u0026quot;_primary_term\u0026quot;: 1, \u0026quot;sm_policy\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;daily-policy\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;每日快照策略\u0026quot;, \u0026quot;schema_version\u0026quot;: 17, \u0026quot;creation\u0026quot;: { \u0026quot;schedule\u0026quot;: { \u0026quot;cron\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;0 8 * * *\u0026quot;, \u0026quot;timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot; } }, \u0026quot;time_limit\u0026quot;: \u0026quot;1h\u0026quot; }, \u0026quot;deletion\u0026quot;: { \u0026quot;schedule\u0026quot;: { \u0026quot;cron\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;0 1 * * *\u0026quot;, \u0026quot;timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot; } }, \u0026quot;condition\u0026quot;: { \u0026quot;max_age\u0026quot;: \u0026quot;7d\u0026quot;, \u0026quot;min_count\u0026quot;: 7, \u0026quot;max_count\u0026quot;: 21 }, \u0026quot;time_limit\u0026quot;: \u0026quot;1h\u0026quot; }, \u0026quot;snapshot_config\u0026quot;: { \u0026quot;indices\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;any_key\u0026quot;: \u0026quot;any_value\u0026quot; }, \u0026quot;ignore_unavailable\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;date_format_timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot;, \u0026quot;include_global_state\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;date_format\u0026quot;: \u0026quot;yyyy-MM-dd-HH:mm\u0026quot;, \u0026quot;repository\u0026quot;: \u0026quot;my_backup\u0026quot;, \u0026quot;partial\u0026quot;: \u0026quot;true\u0026quot; }, \u0026quot;schedule\u0026quot;: { \u0026quot;interval\u0026quot;: { \u0026quot;start_time\u0026quot;: 1685348095913, \u0026quot;period\u0026quot;: 1, \u0026quot;unit\u0026quot;: \u0026quot;Minutes\u0026quot; } }, \u0026quot;enabled\u0026quot;: true, \u0026quot;last_updated_time\u0026quot;: 1685348095938, \u0026quot;enabled_time\u0026quot;: 1685348095909 } }  获取策略 #  获取所有 SLM 策略\n请求示例 #  curl -XGET -uadmin:admin 'https://localhost:9200/_slm/policies' 获取特定的 SLM 策略。\n请求示例 #  curl -XGET -uadmin:admin 'https://localhost:9200/_slm/policies/daily-policy?pretty' 示例响应 #  { \u0026quot;_id\u0026quot; : \u0026quot;daily-policy-sm-policy\u0026quot;, \u0026quot;_version\u0026quot; : 1, \u0026quot;_seq_no\u0026quot; : 0, \u0026quot;_primary_term\u0026quot; : 1, \u0026quot;sm_policy\u0026quot; : { \u0026quot;name\u0026quot; : \u0026quot;daily-policy\u0026quot;, \u0026quot;description\u0026quot; : \u0026quot;每日快照策略\u0026quot;, \u0026quot;schema_version\u0026quot; : 17, \u0026quot;creation\u0026quot; : { \u0026quot;schedule\u0026quot; : { \u0026quot;cron\u0026quot; : { \u0026quot;expression\u0026quot; : \u0026quot;0 8 * * *\u0026quot;, \u0026quot;timezone\u0026quot; : \u0026quot;Asia/Shanghai\u0026quot; } }, \u0026quot;time_limit\u0026quot; : \u0026quot;1h\u0026quot; }, \u0026quot;deletion\u0026quot; : { \u0026quot;schedule\u0026quot; : { \u0026quot;cron\u0026quot; : { \u0026quot;expression\u0026quot; : \u0026quot;0 1 * * *\u0026quot;, \u0026quot;timezone\u0026quot; : \u0026quot;Asia/Shanghai\u0026quot; } }, \u0026quot;condition\u0026quot; : { \u0026quot;max_age\u0026quot; : \u0026quot;7d\u0026quot;, \u0026quot;min_count\u0026quot; : 7, \u0026quot;max_count\u0026quot; : 21 }, \u0026quot;time_limit\u0026quot; : \u0026quot;1h\u0026quot; }, \u0026quot;snapshot_config\u0026quot; : { \u0026quot;indices\u0026quot; : \u0026quot;*\u0026quot;, \u0026quot;metadata\u0026quot; : { \u0026quot;any_key\u0026quot; : \u0026quot;any_value\u0026quot; }, \u0026quot;ignore_unavailable\u0026quot; : \u0026quot;true\u0026quot;, \u0026quot;date_format_timezone\u0026quot; : \u0026quot;Asia/Shanghai\u0026quot;, \u0026quot;include_global_state\u0026quot; : \u0026quot;false\u0026quot;, \u0026quot;date_format\u0026quot; : \u0026quot;yyyy-MM-dd-HH:mm\u0026quot;, \u0026quot;repository\u0026quot; : \u0026quot;my_backup\u0026quot;, \u0026quot;partial\u0026quot; : \u0026quot;true\u0026quot; }, \u0026quot;schedule\u0026quot; : { \u0026quot;interval\u0026quot; : { \u0026quot;start_time\u0026quot; : 1685348095913, \u0026quot;period\u0026quot; : 1, \u0026quot;unit\u0026quot; : \u0026quot;Minutes\u0026quot; } }, \u0026quot;enabled\u0026quot; : true, \u0026quot;last_updated_time\u0026quot; : 1685348095938, \u0026quot;enabled_time\u0026quot; : 1685348095909 } }  更新策略 #  必须为更新请求提供 seq_no 和 primary_term 参数，其他同创建策略一样。\n请求示例 #  PUT /_slm/policies/daily-policy?if_seq_no=2\u0026amp;if_primary_term=1 { \u0026quot;description\u0026quot;: \u0026quot;每日快照策略\u0026quot;, \u0026quot;creation\u0026quot;: { \u0026quot;schedule\u0026quot;: { \u0026quot;cron\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;0 9 * * *\u0026quot;, \u0026quot;timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot; } }, \u0026quot;time_limit\u0026quot;: \u0026quot;1h\u0026quot; }, \u0026quot;deletion\u0026quot;: { \u0026quot;schedule\u0026quot;: { \u0026quot;cron\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;0 1 * * *\u0026quot;, \u0026quot;timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot; } }, \u0026quot;condition\u0026quot;: { \u0026quot;max_age\u0026quot;: \u0026quot;7d\u0026quot;, \u0026quot;max_count\u0026quot;: 21, \u0026quot;min_count\u0026quot;: 7 }, \u0026quot;time_limit\u0026quot;: \u0026quot;1h\u0026quot; }, \u0026quot;snapshot_config\u0026quot;: { \u0026quot;date_format\u0026quot;: \u0026quot;yyyy-MM-dd-HH:mm\u0026quot;, \u0026quot;date_format_timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot;, \u0026quot;indices\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;repository\u0026quot;: \u0026quot;my_backup\u0026quot;, \u0026quot;ignore_unavailable\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;include_global_state\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;partial\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;any_key\u0026quot;: \u0026quot;any_value\u0026quot; } } }  解释 Explain #  _explain API 可以检查快照策略的设置并返回对其的解释。这对理解您的快照策略及其操作非常有用。\n该 API 将返回以下内容的解释:\n快照策略的创建设置,包括下次计划创建快照的确切日期和时间。\n快照策略的删除设置,包括下次计划删除快照的确切日期和时间,以及会被删除的快照。\n用于计算快照创建和删除时间的 Cron 表达式。\n快照策略的所有其他设置,如快照设置、保留要求等。\n获取索引的当前状态。您可以使用索引模式来获取多个索引的状态。\n请求示例 #  curl -XGET -uadmin:admin 'https://localhost:9200/_slm/policies/daily*/_explain' 示例响应 #  { \u0026quot;policies\u0026quot; : [ { \u0026quot;name\u0026quot; : \u0026quot;daily-policy\u0026quot;, \u0026quot;creation\u0026quot; : { \u0026quot;current_state\u0026quot; : \u0026quot;CREATION_START\u0026quot;, \u0026quot;trigger\u0026quot; : { \u0026quot;time\u0026quot; : 1685404800000 } }, \u0026quot;deletion\u0026quot; : { \u0026quot;current_state\u0026quot; : \u0026quot;DELETION_START\u0026quot;, \u0026quot;trigger\u0026quot; : { \u0026quot;time\u0026quot; : 1685379600000 } }, \u0026quot;policy_seq_no\u0026quot; : 0, \u0026quot;policy_primary_term\u0026quot; : 1, \u0026quot;enabled\u0026quot; : true } ] }  删除策略 #  删除指定的 SLM 策略。\n请求示例 #  curl -XDELETE -uadmin:admin 'https://localhost:9200/_slm/policies/daily-policy?pretty' 示例响应 #  { \u0026quot;_index\u0026quot;: \u0026quot;.easysearch-ilm-config\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;daily-policy-sm-policy\u0026quot;, \u0026quot;_version\u0026quot;: 2, \u0026quot;result\u0026quot;: \u0026quot;deleted\u0026quot;, \u0026quot;forced_refresh\u0026quot;: true, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 2, \u0026quot;successful\u0026quot;: 1, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;_seq_no\u0026quot;: 8, \u0026quot;_primary_term\u0026quot;: 1 }  启动策略 #  通过将策略的 enabled 标志设置为 true 来启动策略。\n请求示例 #  我们先创建个每小时执行一次的快照策略\ncurl -XPOST -uadmin:admin -H 'Content-Type: application/json' 'https://localhost:9200/_slm/policies/hour-policy' -d ' { \u0026quot;description\u0026quot;: \u0026quot;每小时快照\u0026quot;, \u0026quot;creation\u0026quot;: { \u0026quot;schedule\u0026quot;: { \u0026quot;cron\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;0 * * * *\u0026quot;, \u0026quot;timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot; } }, \u0026quot;time_limit\u0026quot;: \u0026quot;1h\u0026quot; }, \u0026quot;deletion\u0026quot;: { \u0026quot;schedule\u0026quot;: { \u0026quot;cron\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;0 1 * * *\u0026quot;, \u0026quot;timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot; } }, \u0026quot;condition\u0026quot;: { \u0026quot;max_age\u0026quot;: \u0026quot;7d\u0026quot;, \u0026quot;max_count\u0026quot;: 21, \u0026quot;min_count\u0026quot;: 7 }, \u0026quot;time_limit\u0026quot;: \u0026quot;1h\u0026quot; }, \u0026quot;snapshot_config\u0026quot;: { \u0026quot;date_format\u0026quot;: \u0026quot;yyyy-MM-dd-HH:mm\u0026quot;, \u0026quot;date_format_timezone\u0026quot;: \u0026quot;Asia/Shanghai\u0026quot;, \u0026quot;indices\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;repository\u0026quot;: \u0026quot;my_backup\u0026quot;, \u0026quot;ignore_unavailable\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;include_global_state\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;partial\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;any_key\u0026quot;: \u0026quot;any_value\u0026quot; } } }' 启动一个策略 #\n 请求示例 #  curl -XPOST -uadmin:admin 'https://localhost:9200/_slm/policies/hour-policy/_start' 示例响应 #  {\u0026quot;acknowledged\u0026quot;:true} 停止策略 #  请求示例 #  curl -XPOST -uadmin:admin 'https://localhost:9200/_slm/policies/hour-policy/_stop' 示例响应 #  { \u0026quot;acknowledged\u0026quot; : true }  \n","subcategory":null,"summary":"","tags":null,"title":"快照生命周期管理","url":"/easysearch/v1.15.0/docs/references/management/slm_api/"},{"category":null,"content":"Coerce 强制类型转换参数 #  coerce 映射参数控制数据在索引期间如何将其值转换为预期的字段数据类型。此参数让您可以验证数据是否按照预期的字段类型正确格式化和索引。这提高了搜索结果的准确性。\n代码样例 #  以下示例演示如何使用 coerce 映射参数。\n启用 coerce 去索引文档 #  PUT products { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;price\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;integer\u0026quot;, \u0026quot;coerce\u0026quot;: true } } } } PUT products/_doc/1 { \u0026quot;name\u0026quot;: \u0026quot;Product A\u0026quot;, \u0026quot;price\u0026quot;: \u0026quot;19.99\u0026quot; } 在此示例中，price 字段被定义为 integer 类型，且 coerce 设置为 true。在索引文档时，字符串值 19.99 被强制转换为整数 19。\n禁用 coerce 的文档索引 #  PUT orders { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;quantity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;integer\u0026quot;, \u0026quot;coerce\u0026quot;: false } } } } PUT orders/_doc/1 { \u0026quot;item\u0026quot;: \u0026quot;Widget\u0026quot;, \u0026quot;quantity\u0026quot;: \u0026quot;10\u0026quot; } 在此示例中，quantity 字段被定义为 integer 类型，且 coerce 设置为 false。在索引文档时，字符串值 10 不会被强制转换，由于类型不匹配，文档写入会被拒绝。\n设置索引级别coerce强制转换设置 #  PUT inventory { \u0026quot;settings\u0026quot;: { \u0026quot;index.mapping.coerce\u0026quot;: false }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;stock_count\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;integer\u0026quot;, \u0026quot;coerce\u0026quot;: true }, \u0026quot;sku\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; } } } } PUT inventory/_doc/1 { \u0026quot;sku\u0026quot;: \u0026quot;ABC123\u0026quot;, \u0026quot;stock_count\u0026quot;: \u0026quot;50\u0026quot; } 在此示例中，索引级别的 index.mapping.coerce 设置为 false，这会禁用索引的强制转换。但是，stock_count 字段覆盖了此设置，这个字段可以自动转换。\n","subcategory":null,"summary":"","tags":null,"title":"强制类型转换参数","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/mapping-parameters/coerce/"},{"category":null,"content":"布尔查询 #  bool 查询允许您将多个搜索查询与布尔逻辑结合起来。您可以在查询之间使用布尔逻辑来缩小或扩大搜索结果。\nbool 查询是一个查询组合器，因为它允许您通过组合几个简单的查询来构造高级查询。\n在 bool 查询中使用以下子句（子查询）：\n   条件 说明     must 结果必须与此子句中的查询匹配。如果有多个查询，则每个查询都必须匹配。充当 and 运算符   must_not 结果中排除所有匹配项。充当 not 运算符   should 结果应该但不必与查询匹配。每个匹配的 should 子句都会增加相关性得分。作为选项，您可以要求一个或多个查询与 minimum_should_match 参数的值匹配（默认值为 1）   filter 过滤器在应用查询之前减少数据集。筛选器子句中的查询是 yes-no 选项，其中如果文档与查询匹配，则将包含在结果中。筛选查询不会影响结果排序所依据的相关性分数。筛选查询的结果通常会被缓存，因此运行速度更快。使用筛选器查询根据精确匹配项、范围、日期、数字等筛选结果    bool 查询的结构如下:\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ {} ], \u0026#34;must_not\u0026#34;: [ {} ], \u0026#34;should\u0026#34;: [ {} ], \u0026#34;filter\u0026#34;: {} } } } 例如，假设您有一个 Easysearch 集群中的莎士比亚全集索引。您希望构造满足以下要求的单个查询：\n text_entry 字段必须包含单词 love，并且应包含 life 或 grace speaker 字段不能包含 ROMEO 将这些结果过滤到戏剧 Romeo and Juliet 中，而不影响相关性评分。  使用以下查询:\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;love\u0026#34; } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;life\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;grace\u0026#34; } } ], \u0026#34;minimum_should_match\u0026#34;: 1, \u0026#34;must_not\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;speaker\u0026#34;: \u0026#34;ROMEO\u0026#34; } } ], \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Romeo and Juliet\u0026#34; } } } } } 响应示例 #  { \u0026#34;took\u0026#34;: 12, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 4, \u0026#34;successful\u0026#34;: 4, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 11.356054, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;88020\u0026#34;, \u0026#34;_score\u0026#34;: 11.356054, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 88021, \u0026#34;play_name\u0026#34;: \u0026#34;Romeo and Juliet\u0026#34;, \u0026#34;speech_number\u0026#34;: 19, \u0026#34;line_number\u0026#34;: \u0026#34;4.5.61\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;PARIS\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;O love! O life! not life, but love in death!\u0026#34; } } ] } } 如果要确定这些子句中的哪一个实际上导致了匹配结果，请使用 _name 参数命名每个查询。\n要添加 _name 参数，请将 match 查询中的字段名称更改为对象：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;love\u0026#34;, \u0026#34;_name\u0026#34;: \u0026#34;love-must\u0026#34; } } } ], \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;life\u0026#34;, \u0026#34;_name\u0026#34;: \u0026#34;life-should\u0026#34; } } }, { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;grace\u0026#34;, \u0026#34;_name\u0026#34;: \u0026#34;grace-should\u0026#34; } } } ], \u0026#34;minimum_should_match\u0026#34;: 1, \u0026#34;must_not\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;speaker\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;ROMEO\u0026#34;, \u0026#34;_name\u0026#34;: \u0026#34;ROMEO-must-not\u0026#34; } } } ], \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Romeo and Juliet\u0026#34; } } } } } Easysearch 返回一个 matched_queries 数组，其中列出了与这些结果匹配的查询：\n\u0026#34;matched_queries\u0026#34;: [ \u0026#34;love-must\u0026#34;, \u0026#34;life-should\u0026#34; ] 如果删除不在此列表中的查询，仍然会看到完全相同的结果。\n通过检查匹配的 should 子句，您可以更好地了解结果的相关性得分。\n您还可以通过嵌套 bool 查询来构造复杂的布尔表达式。\n例如，要找到与剧中的（ love 或 hate ）AND（ life 或 grace ）匹配的 text_entry 字段 Romeo and Juliet :\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;love\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;hate\u0026#34; } } ] } }, { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;life\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;grace\u0026#34; } } ] } } ], \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;Romeo and Juliet\u0026#34; } } } } } 响应示例 #  { \u0026#34;took\u0026#34;: 10, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 2, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;max_score\u0026#34;: 11.37006, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;88020\u0026#34;, \u0026#34;_score\u0026#34;: 11.37006, \u0026#34;_source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34;: 88021, \u0026#34;play_name\u0026#34;: \u0026#34;Romeo and Juliet\u0026#34;, \u0026#34;speech_number\u0026#34;: 19, \u0026#34;line_number\u0026#34;: \u0026#34;4.5.61\u0026#34;, \u0026#34;speaker\u0026#34;: \u0026#34;PARIS\u0026#34;, \u0026#34;text_entry\u0026#34;: \u0026#34;O love! O life! not life, but love in death!\u0026#34; } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"布尔查询","url":"/easysearch/v1.15.0/docs/references/search/bool/"},{"category":null,"content":"布尔字段类型 #  布尔字段类型接受 true 或 false 值，也支持字符串形式的 \u0026ldquo;true\u0026rdquo; 或 \u0026ldquo;false\u0026rdquo;。此外，还可以使用空字符串 \u0026quot;\u0026quot; 表示 false 值。\n参考代码 #  创建一个由 a,b,c 三个布尔字段组成的 mapping\nPUT testindex { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;a\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;boolean\u0026quot; }, \u0026quot;b\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;boolean\u0026quot; }, \u0026quot;c\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;boolean\u0026quot; } } } } 索引由布尔值组成的文档\nPUT testindex/_doc/1 { \u0026quot;a\u0026quot; : true, \u0026quot;b\u0026quot; : \u0026quot;true\u0026quot;, \u0026quot;c\u0026quot; : \u0026quot;\u0026quot; } 因此，字段 a 和 b 将被设置为 true，而字段 c 将被设置为 false。\n要搜索所有 c 为 false 的文档，可以使用以下查询示例：\nGET testindex/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot; : { \u0026quot;c\u0026quot; : false } } } 参数说明 #  下表列出了布尔字段类型支持的参数，所有参数均为可选项。\n   参数 描述 默认值     boost 浮点值，指定字段对相关性评分的权重，大于 1.0 提高相关性，0.0 到 1.0 降低相关性。 1.0   doc_values 布尔值，指定是否将字段存储到磁盘以供聚合、排序或脚本操作使用。 true   index 布尔值，指定字段是否可被搜索。 true   meta 接受字段的元数据。 无   null_value 替代 null 的值，类型必须与字段一致。如果未指定，当值为 null 时视为缺失。 null   store 布尔值，指定字段值是否单独存储并可从 _source 外检索。 false    在聚合和 script 中使用布尔值 #  在布尔字段的聚合操作中：key 返回数值，true 对应 1，false 对应 0；key_as_string 返回字符串表示，\u0026ldquo;true\u0026rdquo; 或 \u0026ldquo;false\u0026rdquo;。Script 操作直接返回布尔值 true 或 false。\n参考代码 #  运行一个 term 聚合在字段 a 上面\nGET testindex/_search { \u0026quot;aggs\u0026quot;: { \u0026quot;agg1\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;a\u0026quot; } } }, \u0026quot;script_fields\u0026quot;: { \u0026quot;a\u0026quot;: { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;painless\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;doc['a'].value\u0026quot; } } } } Script 将字段 a 的值返回为 true，key 将其值返回为数值 1，而 key_as_string 则返回字符串 \u0026ldquo;true\u0026rdquo;。\n{ \u0026quot;took\u0026quot; : 1133, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.0, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;testindex\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;fields\u0026quot; : { \u0026quot;a\u0026quot; : [ true ] } } ] }, \u0026quot;aggregations\u0026quot; : { \u0026quot;agg1\u0026quot; : { \u0026quot;doc_count_error_upper_bound\u0026quot; : 0, \u0026quot;sum_other_doc_count\u0026quot; : 0, \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : 1, \u0026quot;key_as_string\u0026quot; : \u0026quot;true\u0026quot;, \u0026quot;doc_count\u0026quot; : 1 } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"布尔字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/boolean/"},{"category":null,"content":"Copy_to 字段复制参数 #  copy_to 参数允许您将多个字段的值复制到单个字段中。如果您经常跨多个字段搜索，此参数会很有用，因为这样可以达到搜索一组字段的效果。\n只有字段值被复制，而不是分词器产生的词项。原始的 _source 字段保持不变，并且可以使用 copy_to 参数将相同的值复制到多个字段。但是，字段间不支持递归复制；相反，应该直接使用 copy_to 从源字段复制到多个目标字段。\n代码样例 #  以下示例使用 copy_to 参数通过产品的名称和描述进行搜索，并将这些值复制到单个字段中：\nPUT my-products-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;copy_to\u0026quot;: \u0026quot;product_info\u0026quot; }, \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;copy_to\u0026quot;: \u0026quot;product_info\u0026quot; }, \u0026quot;product_info\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;price\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;float\u0026quot; } } } } PUT my-products-index/_doc/1 { \u0026quot;name\u0026quot;: \u0026quot;Wireless Headphones\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;High-quality wireless headphones with noise cancellation\u0026quot;, \u0026quot;price\u0026quot;: 99.99 }\nPUT my-products-index/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Bluetooth Speaker\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Portable Bluetooth speaker with long battery life\u0026quot;, \u0026quot;price\u0026quot;: 49.99 } 在此示例中，name 和 description 字段的值被复制到 product_info 字段中。现在您可以通过查询 product_info 字段来搜索产品，如下所示：\nGET my-products-index/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;product_info\u0026quot;: \u0026quot;wireless headphones\u0026quot; } } } Response // 响应 #\n { \u0026quot;took\u0026quot;: 20, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 1, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 1.9061546, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;my-products-index\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 1.9061546, \u0026quot;_source\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;Wireless Headphones\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;High-quality wireless headphones with noise cancellation\u0026quot;, \u0026quot;price\u0026quot;: 99.99 } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"字段复制参数","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/mapping-parameters/copy_to/"},{"category":null,"content":"Field names 字段名称 #  _field_names 字段索引包含非空值的字段名称。可以使用 exists 查询来识别指定字段是否具有非空值的文档。\n但是，只有当 doc_values 和 norms 都被禁用时，_field_names 才会索引字段名称。如果启用了 doc_values 或 norms 中的任何一个，则 exists 查询仍然可以工作，但不会依赖 _field_names 字段。\n映射示例 #   PUT testindex { \u0026quot;mappings\u0026quot;: { \u0026quot;_field_names\u0026quot;: { \u0026quot;enabled\u0026quot;: \u0026quot;true\u0026quot; }, \u0026quot;properties\u0026quot;: { \u0026quot;title\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;doc_values\u0026quot;: false, \u0026quot;norms\u0026quot;: false }, \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;doc_values\u0026quot;: true, \u0026quot;norms\u0026quot;: false }, \u0026quot;price\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;float\u0026quot;, \u0026quot;doc_values\u0026quot;: false } } } } ","subcategory":null,"summary":"","tags":null,"title":"字段名称","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/metadata-field/field-names/"},{"category":null,"content":"向量查询 #  使用 kNN 检索 API 来进行向量查询。\n 先决条件 #  要运行 kNN 搜索，必须安装 knn 插件，参考 插件安装 。\n创建 Mapping 和 Setting #  在索引向量之前，首先定义一个 Mapping，指定向量数据类型、索引模型和模型的参数。这决定了索引向量支持哪些查询。 并指定 index.knn 为 true ，这是为了启用近似相似度模型。\n从1.11.1 版本开始，index.knn 已弃用，创建 knn 索引时，不再配置 index.knn 参数。\n请求示例 #  PUT /knn-test { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_vec\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;knn_dense_float_vector\u0026quot;, \u0026quot;knn\u0026quot;: { \u0026quot;dims\u0026quot;: 50, \u0026quot;model\u0026quot;: \u0026quot;lsh\u0026quot;, \u0026quot;similarity\u0026quot;: \u0026quot;cosine\u0026quot;, \u0026quot;L\u0026quot;: 99, \u0026quot;k\u0026quot;: 1 } } } } } 参数说明 #   my_vec 存储向量的字段名称 knn_dense_float_vector 表示数据类型为密集型浮点向量.  knn 字典对象，对象中的各个参数含义如下：\n dims：向量的维度。 model：模型类型。 similarity：相似度类型。 L：哈希表的数量。一般来说，增加此值会增加召回率。 k：用于形成单个哈希值的哈希函数的数量。一般来说，增加此值会增加精度。  导入测试向量数据 #  我这里采用斯坦福大学预训练好的词向量来演示 https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip\n下载后解压，导入 glove.6B.50d.txt 到 Easysearch，这个文件里每个词是 50 维度的向量。\n将单词存为 word 字段，向量存为 my_vec 字段。\n检索向量 #  以 bread 单词为例，先从索引中查出 bread 单词的向量。\n请求示例 #  GET knn-test/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;word\u0026quot;: \u0026quot;bread\u0026quot; } } } 然后用 bread 的向量到索引中查询和它相近的词。\n请求示例 #  GET knn-test/_search { \u0026quot;size\u0026quot;: 10, \u0026quot;_source\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;knn_nearest_neighbors\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;my_vec\u0026quot;, \u0026quot;vec\u0026quot;: { \u0026quot;values\u0026quot;: [ -0.37436, -0.11959, -0.87609, -1.1217, 1.2788, 0.48323, -0.53903, 0.053659, -0.23929, -0.12414, ...... ] }, \u0026quot;model\u0026quot;: \u0026quot;lsh\u0026quot;, \u0026quot;similarity\u0026quot;: \u0026quot;cosine\u0026quot;, \u0026quot;candidates\u0026quot;: 50 } } ] } } } 示例响应 #  返回的词都是和 bread 含义相近或者经常一起出现的。\n{ \u0026quot;took\u0026quot;: 214, ...... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 400, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 2, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;knn-test\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;WYPvCYkBkPNAx5w6LJ6H\u0026quot;, \u0026quot;_score\u0026quot;: 2, \u0026quot;_source\u0026quot;: { \u0026quot;word\u0026quot;: \u0026quot;bread\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;knn-test\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;PYPvCYkBkPNAx5w6Mbui\u0026quot;, \u0026quot;_score\u0026quot;: 1.8483887, \u0026quot;_source\u0026quot;: { \u0026quot;word\u0026quot;: \u0026quot;baked\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;knn-test\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;RoPvCYkBkPNAx5w6NsYl\u0026quot;, \u0026quot;_score\u0026quot;: 1.8451341, \u0026quot;_source\u0026quot;: { \u0026quot;word\u0026quot;: \u0026quot;toast\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;knn-test\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;o4PvCYkBkPNAx5w6LKCI\u0026quot;, \u0026quot;_score\u0026quot;: 1.84022, \u0026quot;_source\u0026quot;: { \u0026quot;word\u0026quot;: \u0026quot;butter\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;knn-test\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;koPvCYkBkPNAx5w6LKeK\u0026quot;, \u0026quot;_score\u0026quot;: 1.8374994, \u0026quot;_source\u0026quot;: { \u0026quot;word\u0026quot;: \u0026quot;soup\u0026quot; } } ] } } 精确映射（Exact Mapping） #  精确模型允许您运行精确搜索。这些搜索不使用任何索引结构，其运行时间为 O(n^2)，其中 n 是文档总数。 使用该模型不需要提供任何模型参数。\n请求示例 #  PUT /my-index/_mapping { \u0026quot;properties\u0026quot;: { \u0026quot;my_vec\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;knn_(dense_float | sparse_bool)_vector\u0026quot;, # 1. 向量数据类型 \u0026quot;knn\u0026quot;: { \u0026quot;dims\u0026quot;: 100 # 2. 向量的维度 } } } } 注意，使用精确模型可以进行精确搜索，但计算开销较大，适用于文档数量较少的情况。\n精确查询（Exact Query） #  计算查询向量与所有索引向量之间的精确相似度。相对于近似搜索，该算法效率较低，但实现已经经过广泛的优化和分析。\n请求示例 #  GET /my-index/_search { \u0026quot;query\u0026quot;: { \u0026quot;knn_nearest_neighbors\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;my_vec\u0026quot;, \u0026quot;vec\u0026quot;: { # 1 \u0026quot;values\u0026quot;: [0.1, 0.2, 0.3, ...], }, \u0026quot;model\u0026quot;: \u0026quot;exact\u0026quot;, # 2 \u0026quot;similarity\u0026quot;: \u0026quot;(cosine | l1 | l2)\u0026quot;, # 3 } } } 参数说明 #  #1 查询向量。必须匹配 my_vec 的数据类型 #2 模型名称。 #3 相似性函数。必须与向量类型兼容\n","subcategory":null,"summary":"","tags":null,"title":"向量查询","url":"/easysearch/v1.15.0/docs/references/search/knn_api/"},{"category":null,"content":"k-NN 向量字段类型 #  关于向量 #  在索引文档和运行查询时都需要指定向量类型。在这两种情况下，您都使用相同的 JSON 结构来定义向量类型。每个向量类型还有一个简写形式，这在使用不支持嵌套文档的工具时会很方便。以下示例展示了如何在索引向量时指定它们。\nknn_dense_float_vector 密集向量类型 #  假设您已经定义了一个映射，其中 my_vec 的类型为 knn_dense_float_vector。\nPOST /my-index/_doc { \u0026#34;my_vec\u0026#34;: { \u0026#34;values\u0026#34;: [0.1, 0.2, 0.3, ...] # 1 } } POST /my-index/_doc { \u0026#34;my_vec\u0026#34;: [0.1, 0.2, 0.3, ...] # 2 } 说明 #  1\t向量中所有浮点值的 JSON 列表。长度应与映射中的dims匹配。 2\t#1 的简写形式。\nknn_sparse_bool_vector 稀疏向量类型 #  假设您已经定义了一个映射，其中 my_vec 的类型为 knn_sparse_bool_vector。\nPOST /my-index/_doc { \u0026#34;my_vec\u0026#34;: { \u0026#34;true_indices\u0026#34;: [1, 3, 5, ...], # 1 \u0026#34;total_indices\u0026#34;: 100, # 2 } } POST /my-index/_doc { \u0026#34;my_vec\u0026#34;: [[1, 3, 5, ...], 100] # 3 } 说明 #  1\t向量中为 true 的索引的 JSON 列表。 2\t索引的向量总数。这应该与映射中的dims匹配。 3\t#1 和 #2 的简写形式。一个包含两个项目的列表，第一个项目是 true_indices，第二个是 total_indices。\n映射 #  在索引向量数据之前，您需要先定义一个映射，指定向量数据类型、索引模型和模型参数。这决定了索引向量支持哪些查询。\n基本结构 #  基本映射结构如下：\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { # 1 \u0026#34;my_vec\u0026#34;: { # 2 \u0026#34;type\u0026#34;: \u0026#34;knn_sparse_bool_vector\u0026#34;, # 3 \u0026#34;knn\u0026#34;: { # 4 \u0026#34;dims\u0026#34;: 100, # 5 \u0026#34;model\u0026#34;: \u0026#34;exact\u0026#34;, # 6 ... # 7 } } } } 说明 #  1\t文档字段的字典。与 PUT Mapping API 相同。 2\t向量的字段名称。 3\t要存储的向量类型。 4\tknn 的设置。 5\t向量的维度。存储在此字段 my_vec 中的所有向量必须具有相同的维度。 6\t模型类型。这和模型参数将决定您可以运行什么样的搜索。请参见下面的模型部分。 7\t额外的模型参数。请参见下面的模型部分。\nknn_sparse_bool_vector 数据类型 #  这种类型针对每个索引为 true 或 false 的向量进行了优化,大部分的情况为 false。例如，您可以表示文档的词袋编码，其中每个索引对应词汇表中的一个词，而任何单个文档只包含所有词的很小一部分。在内部，Knn 通过只存储 true 的数据来节省空间。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_sparse_bool_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 25000, # 2 ... # 3 } } } } 说明 #  1\t类型名称。 2\t向量的维度。 3\t额外的模型参数。请参见下面的模型部分。\nknn_dense_float_vector 数据类型 #  这种类型针对每个向量都是浮点数、所有索引都有值且维度通常不超过约 1000 的向量进行了优化。例如，您可以存储词嵌入或图像向量。在内部，Knn 使用 Java Float 类型来存储值。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 100, # 2 ... # 3 } } } } 说明 #  1\t类型名称。 2\t向量的维度。不应超过几千。如果超过，请考虑进行某种维度降低。 3\t额外的模型参数。请参见下面的模型部分。\n特定模型映射 #  特定模型映射允许您运行特定模型的搜索。这些搜索不利用任何索引结构，运行时间复杂度为 O(n^2)，其中 n 是文档总数。\n使用此配置时，您不需要提供任何 \u0026quot;model\u0026quot;: \u0026quot;...\u0026quot; 值或任何模型参数。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_(dense_float | sparse_bool)_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 100, # 2 } } } } 说明 #  1\t向量数据类型。支持 dense float 和 sparse bool 两种类型 2\t向量维度。\nJaccard LSH 映射 #  使用 Minhash 算法对稀疏布尔向量进行哈希和存储，以支持近似 Jaccard 相似度查询。\n该实现受到 《Mining Massive Datasets》第 3 章、Spark MinHash 实现、 tdebatty/java-LSH Github 项目和 Minhash for Dummies 博客文章的影响。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_sparse_bool_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 25000, # 2 \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, # 3 \u0026#34;similarity\u0026#34;: \u0026#34;jaccard\u0026#34;, # 4 \u0026#34;L\u0026#34;: 99, # 5 \u0026#34;k\u0026#34;: 1 # 6 } } } } 说明 #  1\t向量数据类型。必须是稀疏布尔向量。 2\t向量维度。 3\t模型类型。 4\t相似度度量。 5\t哈希表的数量。通常增加这个值会提高召回率。 6\t组合形成单个哈希值的哈希函数数量。通常，增加这个值会提高精确度。\n汉明 LSH 映射 #  使用 位采样算法对稀疏布尔向量进行哈希和存储，以支持近似汉明相似度查询。\n与标准位采样方法的唯一区别是它采样并组合 k 个位来形成单个哈希值。例如，如果设置 L = 100，k = 3，它会从向量中采样 100 * 3 = 300 个位，并将每 3 个位连接起来形成每个哈希值，总共生成 100 个哈希值。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_sparse_bool_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 25000, # 2 \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, # 3 \u0026#34;similarity\u0026#34;: \u0026#34;hamming\u0026#34;, # 4 \u0026#34;L\u0026#34;: 99, # 5 \u0026#34;k\u0026#34;: 2 # 6 } } } } 说明 #  1\t向量数据类型。必须是稀疏布尔向量。 2\t向量维度。 3\t模型类型。 4\t相似度度量。 5\t哈希表的数量。通常增加这个值会提高召回率。 6\t组合形成单个哈希值的哈希函数数量。通常增加这个值会提高精确度。\n余弦 LSH 映射 #  使用 随机投影算法对密集浮点向量进行哈希和存储，以支持近似余弦相似度查询。\n参考 Mining Massive Datasets第三章\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 100, # 2 \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, # 3 \u0026#34;similarity\u0026#34;: \u0026#34;cosine\u0026#34;, # 4 \u0026#34;L\u0026#34;: 99, # 5 \u0026#34;k\u0026#34;: 1 # 6 } } } } 说明 #  1\t向量数据类型。必须是密集浮点向量。 2\t向量维度。 3\t模型类型。 4\t相似度度量。 5\t哈希表的数量。通常增加这个值会提高召回率。 6\t组合形成单个哈希值的哈希函数数量。通常，增加这个值会提高精确度。\nL2 LSH 映射 #  使用 稳定分布方法对密集浮点向量进行哈希和存储，以支持近似 L2（欧几里得）相似度查询。\n参考 Mining Massive Datasets第三章\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 100, # 2 \u0026#34;model\u0026#34;: \u0026#34;lsh\u0026#34;, # 3 \u0026#34;similarity\u0026#34;: \u0026#34;l2\u0026#34;, # 4 \u0026#34;L\u0026#34;: 99, # 5 \u0026#34;k\u0026#34;: 1, # 6 \u0026#34;w\u0026#34;: 3 # 7 } } } } 说明 #  1\t向量数据类型。必须是密集浮点向量。 2\t向量维度。 3\t模型类型。 4\t相似度度量。 5\t哈希表的数量。通常增加这个值会提高召回率。 6\t组合形成单个哈希值的哈希函数数量。通常增加这个值会提高精确度。 7\t整数桶宽度。这决定了两个向量在投影到第三个公共向量上时必须多么接近，才能共享一个哈希值。典型值是低位整数。\n排列 LSH 映射 #  使用 Amato 等人在 《Large-Scale Image Retrieval with Elasticsearch》中描述的模型。\n该模型通过向量中绝对值最大的 k 个索引（向量中的位置）来描述向量。直觉是，每个索引对应于某个潜在概念，绝对值高的索引比绝对值低的索引携带更多关于其各自概念的信息。该方法的研究主要集中在余弦相似度上，尽管实现也支持 L1 和 L2。\n例子 #  向量 [10, -2, 0, 99, 0.1, -8, 42, -13, 6, 0.1] 中 k = 4 的索引为 [4, 7, -8, 1]。索引是 1 索引的，负值的索引被否定（因此 -8）。索引可以根据其排名可选地重复。在本例中，索引将被重复 [4, 4, 4, 4, 7, 7, 7, -8, -8, 1]。索引 4 具有最高的绝对值，因此它被重复 k - 0 = 4 次。索引 7 具有第二高的绝对值，因此它被重复 k - 1 = 3 次，依此类推。搜索算法将得分计算为存储向量表示和查询向量表示的交集的大小。因此，对于表示为 [2, 2, 2, 2, 7, 7, 7, 4, 4, 5] 的查询向量，交集为 [7, 7, 7, 4, 4]，得分为 5。在一些实验中，重复实际上降低了召回率，因此建议您尝试使用和不使用重复。\nPUT /my-index/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;my_vec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_dense_float_vector\u0026#34;, # 1 \u0026#34;knn\u0026#34;: { \u0026#34;dims\u0026#34;: 100, # 2 \u0026#34;model\u0026#34;: \u0026#34;permutation_lsh\u0026#34;, # 3 \u0026#34;k\u0026#34;: 10, # 4 \u0026#34;repeating\u0026#34;: true # 5 } } } } 说明 #  1\t向量数据类型。必须是密集浮点向量。 2\t向量维度。 3\t模型类型。 4\t选择的顶部索引数量。 5\t是否根据其排名重复索引。请参见上面的重复注释。\n","subcategory":null,"summary":"","tags":null,"title":"向量字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/knn/"},{"category":null,"content":"Dynamic 动态映射参数 #  dynamic 参数指定是否可以动态地将新检测到的字段添加到映射中。它接受下表中列出的参数。\n   参数 描述     true 指定可以动态地将新字段添加到映射中。默认值为 true。   false 指定不能动态地将新字段添加到映射中。如果检测到新字段，则不会对其进行索引或搜索，但可以从 _source 字段中检索。   strict 当检测到文档中有新字段时，索引操作失败，抛出异常。   strict_allow_templates 如果新字段匹配映射中预定义的动态模板，则添加新字段。    示例：创建 dynamic 设置为 true 的索引 #  通过以下命令创建一个 dynamic 设置为 true 的索引：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: true } } 通过以下命令，索引一个包含两个字符串字段的对象字段 patient 的文档：\nPUT testindex1/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;123456\u0026#34; } } 通过以下命令确认映射按预期工作：\nGET testindex1/_mapping 对象字段 patient 和两个子字段 name 和 id 被添加到映射中，如以下响应所示：\n{ \u0026#34;testindex1\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 256 } } }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 256 } } } } } } } } } 示例：创建 dynamic 设置为 false 的索引 #  通过以下命令，创建一个具有显式映射且 dynamic 设置为 false 的索引：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: false, \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } 通过以下命令，索引一个文档，其中包含两个字符串字段的对象字段 patient和额外未映射的 room 和 floor\nPUT testindex1/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;123456\u0026#34; }, \u0026#34;room\u0026#34;: \u0026#34;room1\u0026#34;, \u0026#34;floor\u0026#34;: \u0026#34;1\u0026#34; } 通过以下命令确认映射是否按预期工作：\nGET testindex1/_mapping 以下返回内容显示新字段 room 和 floor 未添加到映射中，映射保持不变：\n{ \u0026#34;testindex1\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } } 示例：创建 dynamic 设置为 strict 的索引 #  通过以下命令，创建一个具有显式映射且 dynamic 设置为 strict 的索引：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34;: \u0026#34;strict\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } } 通过以下命令，索引一个文档，其中包含两个字符串字段的对象字段 patient 和额外未映射的 room 和 floor\nPUT testindex1/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;123456\u0026#34; }, \u0026#34;room\u0026#34;: \u0026#34;room1\u0026#34;, \u0026#34;floor\u0026#34;: \u0026#34;1\u0026#34; } 注意会抛出异常，如下所示：\n{ \u0026#34;error\u0026#34;: { \u0026#34;root_cause\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;strict_dynamic_mapping_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;mapping set to strict, dynamic introduction of [room] within [_doc] is not allowed\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;strict_dynamic_mapping_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;mapping set to strict, dynamic introduction of [room] within [_doc] is not allowed\u0026#34; }, \u0026#34;status\u0026#34;: 400 } ","subcategory":null,"summary":"","tags":null,"title":"动态映射参数","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/mapping-parameters/dynamic/"},{"category":null,"content":"Alias 别名字段类型 #  别名字段类型为现有字段创建另一个名称。您可以在搜索和字段功能的 API 操作中使用别名字段，但存在一些例外情况。要设置别名，必须在 path 参数中指定原始字段名称。\n参考代码 #  PUT movies { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;year\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;date\u0026quot; }, \u0026quot;release_date\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;alias\u0026quot;, \u0026quot;path\u0026quot; : \u0026quot;year\u0026quot; } } } } 参数说明 #   path：指向原始字段的完整路径，包括所有父对象。例如，parent.child.field_name。此参数为必填项。  别名（Alias）字段 #  别名（Alias）字段必须遵循以下规则：\n 一个别名字段只能引用一个原始字段。 在嵌套对象中，别名必须与原始字段位于相同的嵌套层级。   要更改别名引用的字段，需要更新映射配置。但请注意，之前存储的 Percolator 查询中的别名仍会继续引用原始字段，不会自动更新为新的字段引用。\n 原始字段 #  别名的原始字段必须遵守以下规则：\n 原始字段必须在别名字段创建之前定义。 原始字段不能是对象类型，也不能是另一个别名字段。  可以使用别名字段的搜索 API #  您可以在以下搜索 API 的只读操作中使用别名：\n 查询 (Queries) 排序 (Sorts) 聚合 (Aggregations) 存储字段 (stored_fields) 文档值字段 (docvalue_fields) 建议 (Suggestions) 高亮显示 (Highlights) 访问字段值的脚本 (Scripts)  也可以在字段功能 API 操作中使用别名 #  要在字段功能 API 中使用别名，请将其指定在 fields 参数中。\nGET movies/_field_caps?fields=release_date 例外情况 #  别名不能用于以下场景：\n 写入请求（如更新请求）。 多字段（multi-fields）或 copy_to 的目标字段。 作为 _source 参数用于结果过滤。 接受字段名称的 API（如词向量 term vectors）。 terms、more_like_this 和 geo_shape 这类查询不支持别名字段，因为别名字段在检索文档时无法使用。  通配符（Wildcards） #  在搜索和字段功能的通配符(Wildcards)查询中，原始字段和别名都会与通配符模式匹配。\nGET movies/_field_caps?fields=release* ","subcategory":null,"summary":"","tags":null,"title":"别名字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/alias/"},{"category":null,"content":"Analyzer 分词器参数 #  分析器 analyzer 映射参数用于定义在索引和搜索期间应用于文本字段的文本分析过程，即分词器的作用过程。\n代码样例 #  以下示例配置定义了一个名为 my_custom_analyzer 的自定义分词器：\nPUT my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_stop_filter\u0026quot;, \u0026quot;my_stemmer\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;my_stop_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;stop\u0026quot;, \u0026quot;stopwords\u0026quot;: [\u0026quot;the\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;and\u0026quot;, \u0026quot;or\u0026quot;] }, \u0026quot;my_stemmer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;stemmer\u0026quot;, \u0026quot;language\u0026quot;: \u0026quot;english\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_text_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;search_quote_analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot; } } } } 在此示例中，my_custom_analyzer 使用标准分词器，将所有标记转换为小写，应用自定义停用词过滤器，并应用英语词干提取器。\n您可以映射一个文本字段，使其在索引和搜索操作中都使用此自定义分词器：\n\u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_text_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot; } } } ","subcategory":null,"summary":"","tags":null,"title":"分词器参数","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/mapping-parameters/analyzer/"},{"category":null,"content":"Meta 元数据属性 #  _meta 字段是一个映射属性，允许您为索引映射附加自定义元数据。您的应用程序可以使用这些元数据来存储与您的用例相关的信息，如版本控制、所有权、分类或审计。\n用法 #  您可以在创建新索引或更新现有索引的映射时定义 _meta 字段，如以下示例所示：\nPUT my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;_meta\u0026quot;: { \u0026quot;application\u0026quot;: \u0026quot;MyApp\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.2.3\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;John Doe\u0026quot; }, \u0026quot;properties\u0026quot;: { \u0026quot;title\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } } } 在此示例中，添加了三个自定义元数据字段：application、version 和 author。您的应用程序可以使用这些字段来存储有关索引的任何相关信息，例如它所属的应用程序、应用程序版本或索引的作者。\n您可以使用 Put Mapping API 操作更新 _meta 字段，如以下示例所示：\nPUT my-index/_mapping { \u0026quot;_meta\u0026quot;: { \u0026quot;application\u0026quot;: \u0026quot;MyApp\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.3.0\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;Jane Smith\u0026quot; } } 检索元数据信息 #  您可以使用 Get Mapping API 操作检索索引的 _meta 信息，如以下示例所示：\nGET my-index/_mapping 返回包含 _meta 字段的完整索引映射：\n{ \u0026quot;my-index\u0026quot;: { \u0026quot;mappings\u0026quot;: { \u0026quot;_meta\u0026quot;: { \u0026quot;application\u0026quot;: \u0026quot;MyApp\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.3.0\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;Jane Smith\u0026quot; }, \u0026quot;properties\u0026quot;: { \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;title\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"元数据属性","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/metadata-field/meta/"},{"category":null,"content":"二进制字段类型 #  二进制字段类型包含以 Base64 编码存储的二进制值，这些值不可被搜索。\n参考代码 #  创建包含二进制字段的映射\nPUT testindex { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;binary_value\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;binary\u0026quot; } } } } 索引一个二进制值的文档\nPUT testindex/_doc/1 { \u0026quot;binary_value\u0026quot; : \u0026quot;bGlkaHQtd29rfx4=\u0026quot; }  使用 = 作为填充字符。不允许嵌入换行符。\n 参数说明 #  以下参数均为可选参数\n doc_values：布尔值，指定字段是否应存储在磁盘上，以便用于聚合、排序或 script 操作。可选，默认为 false。 store：布尔值，指定字段值是否应存储，并可从 _source 字段中单独检索。可选，默认为 false。  ","subcategory":null,"summary":"","tags":null,"title":"二进制字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/binary/"},{"category":null,"content":"Search-as-you-type 字段类型 #  search-as-you-type 字段类型通过前缀和中缀补全提供边输入边搜索的功能。\n代码样例 #  将字段映射为 search-as-you-type 类型时，会为该字段创建 n-gram 子字段，其中 n 的范围为 [2, max_shingle_size]。此外，还会创建一个索引前缀子字段。\n创建一个 search-as-you-type 的映射字段\nPUT books { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;suggestions\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;search_as_you_type\u0026quot; } } } } 除了创建 suggestions 字段外，还会生成 suggestions._2gram、suggestions._3gram 和 suggestions._index_prefix 字段。\n以下是使用 search-as-you-type 字段索引文档的示例：\nPUT books/_doc/1 { \u0026quot;suggestions\u0026quot;: \u0026quot;one two three four\u0026quot; } 要匹配任意顺序的词项，可以使用 bool_prefix 或 multi-match 查询。\n这些查询会将搜索词项按顺序匹配的文档排名提高，而将词项顺序不一致的文档排名降低。\nGET books/_search { \u0026quot;query\u0026quot;: { \u0026quot;multi_match\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;tw one\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bool_prefix\u0026quot;, \u0026quot;fields\u0026quot;: [ \u0026quot;suggestions\u0026quot;, \u0026quot;suggestions._2gram\u0026quot;, \u0026quot;suggestions._3gram\u0026quot; ] } } } 返回内容包含匹配的文档：\n{ \u0026quot;took\u0026quot; : 13, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.0, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;books\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;suggestions\u0026quot; : \u0026quot;one two three four\u0026quot; } } ] } } 要按顺序匹配词项，可以使用 match_phrase_prefix 查询：\nGET books/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase_prefix\u0026quot;: { \u0026quot;suggestions\u0026quot;: \u0026quot;two th\u0026quot; } } } 返回内容包含匹配到的文档：\n{ \u0026quot;took\u0026quot; : 23, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.4793051, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;books\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.4793051, \u0026quot;_source\u0026quot; : { \u0026quot;suggestions\u0026quot; : \u0026quot;one two three four\u0026quot; } } ] } } 要精确匹配最后的词项，可以使用 match_phrase 查询：\nGET books/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;suggestions\u0026quot;: \u0026quot;four\u0026quot; } } } 返回内容：\n{ \u0026quot;took\u0026quot; : 2, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.2876821, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;books\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.2876821, \u0026quot;_source\u0026quot; : { \u0026quot;suggestions\u0026quot; : \u0026quot;one two three four\u0026quot; } } ] } } 参数说明 #  下表列出了 search-as-you-type 字段类型支持的参数，所有参数均为可选项。    参数 描述 默认值     analyzer 指定该字段使用的分析器，默认情况下用于索引和搜索阶段。如果需要在搜索时覆盖分析器，请设置 search_analyzer 参数。默认使用 standard analyzer，基于 Unicode 文本分割算法和语法规则进行分词。此设置适用于根字段和子字段。 standard analyzer   index 布尔值，指定字段是否可被搜索。此设置适用于根字段和子字段。 true   index_options 指定索引中存储的信息，以支持搜索和高亮显示。可选值：docs (仅文档编号)、freqs (文档编号和词频)、positions (文档编号、词频和位置)、offsets (文档编号、词频、位置及字符偏移)。此设置适用于根字段和子字段。 positions   max_shingle_size 整数值，指定最大 n-gram 大小，有效范围为 [2, 4]。创建的 n-gram 范围为 [2, max_shingle_size]。默认值为 3，会生成 2-gram 和 3-gram。较大的值更适合具体查询，但会导致索引大小增加。 3   norms 布尔值，指定在计算相关性评分时是否使用字段长度。适用于根字段和 n-gram 子字段（默认为 false）。不适用于前缀子字段（在前缀子字段中默认为 false）。 false   search_analyzer 指定搜索时使用的分析器。默认值为 analyzer 参数中指定的分析器。适用于根字段和子字段。 analyzer 参数值   search_quote_analyzer 指定搜索短语时使用的分析器。默认值为 analyzer 参数中指定的分析器。适用于根字段和子字段。 analyzer 参数值   similarity 用于计算相关性评分的排序算法。默认值为 BM25。适用于根字段和子字段。 BM25   store 布尔值，指定字段值是否单独存储并可从 _source 外检索。仅适用于根字段。 false   term_vector 布尔值，指定是否为该字段存储词向量。适用于根字段和 n-gram 子字段（默认为 no）。不适用于前缀子字段。 no    ","subcategory":null,"summary":"","tags":null,"title":"Search-as-you-type 字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/autocomplete-field-type/search-as-you-type/"},{"category":null,"content":"Percolator 过滤器字段类型 #  Percolator 字段类型将该字段视为查询处理。任何 JSON 对象字段都可以标记为 Percolator 字段。通常，文档被索引并用于搜索，而 Percolator 字段存储搜索条件，稍后通过 Percolate 查询将匹配文档到该条件。\n参考代码 #  客户正在搜索价格在 400 美元或以下的桌子，并希望为此搜索创建警报。 创建一个映射，为查询字段分配一个 percolator 字段类型：\nPUT testindex1 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;search\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } }, \u0026quot;price\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;float\u0026quot; }, \u0026quot;item\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } } } 索引一个查询\nPUT testindex1/_doc/1 { \u0026quot;search\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;filter\u0026quot;: [ { \u0026quot;match\u0026quot;: { \u0026quot;item\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;table\u0026quot; } } }, { \u0026quot;range\u0026quot;: { \u0026quot;price\u0026quot;: { \u0026quot;lte\u0026quot;: 400.00 } } } ] } } } }  查询中引用的字段必须已经存在于映射中。\n 运行一个 percolate 查询\nGET testindex1/_search { \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : { \u0026quot;percolate\u0026quot; : { \u0026quot;field\u0026quot; : \u0026quot;search.query\u0026quot;, \u0026quot;document\u0026quot; : { \u0026quot;item\u0026quot; : \u0026quot;Mahogany table\u0026quot;, \u0026quot;price\u0026quot;: 399.99 } } } } } } 返回结果包含原来索引的查询内容\n{ \u0026quot;took\u0026quot; : 30, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.0, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;testindex1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.0, \u0026quot;_source\u0026quot; : { \u0026quot;search\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : [ { \u0026quot;match\u0026quot; : { \u0026quot;item\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;table\u0026quot; } } }, { \u0026quot;range\u0026quot; : { \u0026quot;price\u0026quot; : { \u0026quot;lte\u0026quot; : 400.0 } } } ] } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"Percolator 过滤器字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/percolator/"},{"category":null,"content":"Object 对象字段类型 #  对象字段类型包含一个 JSON 对象（一组名称/值对）。JSON 对象中的值可以是另一个 JSON 对象。在映射对象字段时不需要指定 object 作为类型，因为 object 是默认类型。\n代码示例 #  创建一个带有对象字段的映射：\nPUT testindex1/_mappings { \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34; : { \u0026#34;name\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34; }, \u0026#34;id\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;keyword\u0026#34; } } } } } 索引一个包含对象字段的文档：\nPUT testindex1/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;123456\u0026#34; } } 嵌套对象在内部存储为扁平的键/值对。要引用嵌套对象中的字段，使用 parent field.child field（例如，patient.id）。\n搜索 ID 为 123456 的患者信息：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;patient.id\u0026#34;: \u0026#34;123456\u0026#34; } } } 参数 #  下表列出了对象字段类型接受的参数。所有参数都是可选的。\n   参数 描述     dynamic 指定是否可以动态地向对象添加新字段。有效值为 true、false、strict 和 strict_allow_templates。默认为 true。   enabled 一个布尔值，指定是否应解析对象的 JSON 内容。如果 enabled 设置为 false，则对象的内容不会被索引或搜索，但仍可以从 _source 字段中检索。默认为 true。   properties 此对象字段的属性（即子字段设置），可以是任何支持的类型。如果 dynamic 设置为 true，则可以动态地向此对象添加新属性。    dynamic 参数 #  dynamic 参数指定是否可以向已经索引的对象动态添加新字段。\n例如，您最初可以创建一个只有一个字段的 patient 对象的映射：\nPUT testindex1/_mappings { \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34; : { \u0026#34;name\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34; } } } } } 然后，您在 patient 中索引一个带有新 id 字段的文档：\nPUT testindex1/_doc/1 { \u0026#34;patient\u0026#34;: { \u0026#34;name\u0026#34; : \u0026#34;John Doe\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;123456\u0026#34; } } 结果，id 字段被添加到映射中：\n{ \u0026#34;testindex1\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;patient\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; } } } } } } } dynamic 参数有以下有效值：\n   值 描述     true 可以动态地向映射添加新字段。这是默认值。   false 不能动态地向映射添加新字段。如果检测到新字段，它不会被索引或搜索。但是，仍然可以从 _source 字段中检索它。   strict 当动态地向映射添加新字段时，会抛出异常。要向对象添加新字段，必须先将其添加到映射中。   strict_allow_templates 如果新检测到的字段匹配映射中的任何预定义动态模板，则它们会被添加到映射中；如果它们不匹配任何模板，则会抛出异常。     内部对象会继承其父对象的 dynamic 参数值，除非它们声明了自己的 dynamic 参数值。\n ","subcategory":null,"summary":"","tags":null,"title":"Object 对象字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/object-field-type/object/"},{"category":null,"content":"Nested 字段类型 #  Nested 字段类型是一种特殊的 对象字段类型。\n任何对象字段都可以包含一个对象数组。数组中的每个对象都会被动态映射为对象字段类型并以扁平化形式存储。这意味着数组中的对象会被分解成单独的字段，每个字段在所有对象中的值会被存储在一起。有时需要使用 Nested 嵌套类型来将嵌套对象作为一个整体保存，以便您可以对其关联性执行搜索。\n扁平化形式 #  默认情况下，每个嵌套对象都被动态映射为对象字段类型。任何对象字段都可以包含一个对象数组。\nPUT testindex1/_doc/100 { \u0026#34;patients\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 56, \u0026#34;smoker\u0026#34;: true}, {\u0026#34;name\u0026#34;: \u0026#34;Mary Major\u0026#34;, \u0026#34;age\u0026#34;: 85, \u0026#34;smoker\u0026#34;: false} ] } 当这些对象被存储时，它们会被扁平化，因此它们的内部表示形式具有每个字段的所有值的数组：\n{ \u0026#34;patients.name\u0026#34;: [\u0026#34;John Doe\u0026#34;, \u0026#34;Mary Major\u0026#34;], \u0026#34;patients.age\u0026#34;: [56, 85], \u0026#34;patients.smoker\u0026#34;: [true, false] } 一些查询会在这种表示形式中正确工作。如果您搜索年龄大于 75 或者 吸烟的病人 \u0026quot;patients.smoker\u0026quot;: true，文档 id 100 应该匹配。\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;patients.smoker\u0026#34;: true } }, { \u0026#34;range\u0026#34;: { \u0026#34;patients.age\u0026#34;: { \u0026#34;gte\u0026#34;: 75 } } } ] } } } 查询正确地返回文档 id 100：\n{ \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.3616575, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;_score\u0026#34;: 1.3616575, \u0026#34;_source\u0026#34;: { \u0026#34;patients\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;56\u0026#34;, \u0026#34;smoker\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Mary Major\u0026#34;, \u0026#34;age\u0026#34;: \u0026#34;85\u0026#34;, \u0026#34;smoker\u0026#34;: false } ] } } ] } } 或者，如果您搜索年龄大于 75 并且是吸烟的病人，文档 100 不应该匹配。\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;patients.smoker\u0026#34;: true } }, { \u0026#34;range\u0026#34;: { \u0026#34;patients.age\u0026#34;: { \u0026#34;gte\u0026#34;: 75 } } } ] } } } 然而，这个查询仍然错误地返回文档 100。这是因为当创建每个字段的值数组时，年龄和吸烟患者之间的关联关系丢失了。\nNested 嵌套字段类型 #  Nested 嵌套对象虽然作为独立的文档存储，但是父对象持有对其子对象的引用，这样就保留了子对象之间的关联关系。要将对象标记为嵌套类型，需要创建一个带有嵌套字段类型的映射。\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;patients\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34; } } } } 然后，索引一个带有嵌套字段类型的文档：\nPUT testindex1/_doc/100 { \u0026#34;patients\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 56, \u0026#34;smoker\u0026#34;: true}, {\u0026#34;name\u0026#34;: \u0026#34;Mary Major\u0026#34;, \u0026#34;age\u0026#34;: 85, \u0026#34;smoker\u0026#34;: false} ] } 您可以使用以下嵌套查询来搜索年龄大于 75 或者 吸烟的患者：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;patients\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;patients.smoker\u0026#34;: true } }, { \u0026#34;range\u0026#34;: { \u0026#34;patients.age\u0026#34;: { \u0026#34;gte\u0026#34;: 75 } } } ] } } } } } 查询正确地返回了两个患者：\n{ \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.8465736, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;_score\u0026#34;: 0.8465736, \u0026#34;_source\u0026#34;: { \u0026#34;patients\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 56, \u0026#34;smoker\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Mary Major\u0026#34;, \u0026#34;age\u0026#34;: 85, \u0026#34;smoker\u0026#34;: false } ] } } ] } } 您可以使用以下嵌套查询来搜索年龄大于 75 并且 吸烟的患者：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;nested\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;patients\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;patients.smoker\u0026#34;: true } }, { \u0026#34;range\u0026#34;: { \u0026#34;patients.age\u0026#34;: { \u0026#34;gte\u0026#34;: 75 } } } ] } } } } } 如预期一样，前面的查询没有返回任何结果：\n{ \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 0, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] } } 参数说明 #  下表列出了对象字段类型接受的参数。所有参数都是可选的。\n   参数 说明     dynamic 指定是否可以动态地向对象添加新字段。有效值为 true、false、strict 和 strict_allow_templates。默认为 true。   include_in_parent 一个布尔值，指定子嵌套对象中的所有字段是否也应以扁平化形式添加到父文档中。默认为 false。   include_in_root 一个布尔值，指定子嵌套对象中的所有字段是否也应以扁平化形式添加到根文档中。默认为 false。   properties 此对象的字段，可以是任何支持的类型。如果 dynamic 设置为 true，则可以动态地向此对象添加新属性。    ","subcategory":null,"summary":"","tags":null,"title":"Nested 字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/object-field-type/nested/"},{"category":null,"content":"match_only_text 字段类型 #  Introduced 1.10.0\n简介 #  match_only_text 是一个为全文搜索优化的字段类型，是 text 类型的变体。它通过省略词条位置、词频和规范化信息来减少存储需求,适合对存储成本敏感但仍需要基本全文搜索功能的场景。\n主要特点 #    存储优化:\n 不存储位置信息 不存储词频信息 不存储规范化信息 显著减少索引大小    评分机制:\n 禁用评分计算 所有匹配文档得分统一为 1.0    查询支持:\n 支持大多数查询类型 不支持 interval 查询 不支持 span 查询 支持但不优化短语查询    使用场景 #  适合用于:\n 需要快速查找包含特定词条的文档 对存储成本敏感的大数据集 不需要复杂相关性排序的场景  不适合用于:\n 需要基于相关性排序的查询 依赖词条位置或顺序的查询 需要精确短语匹配的场景  映射示例 #  PUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;match_only_text\u0026#34; } } } } 参数配置 #     参数 说明 默认值     analyzer 分析器设置 standard   boost 评分提升因子 1.0   eager_global_ordinals 是否预加载全局序号 false   fielddata 是否启用 fielddata false   fields 多字段定义 -   index 是否创建索引 true   meta 字段元数据 -    注意: 虽然支持多种参数设置,但建议保持默认配置以维持其优化效果。\n从 text 类型迁移 #  使用 Reindex API 可以将现有的 text 字段迁移到 match_only_text:\n// 1. 创建新索引 PUT destination { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;match_only_text\u0026#34; } } } } // 2. 重建索引 POST _reindex { \u0026quot;source\u0026quot;: { \u0026quot;index\u0026quot;: \u0026quot;source\u0026quot; }, \u0026quot;dest\u0026quot;: { \u0026quot;index\u0026quot;: \u0026quot;destination\u0026quot; } } 使用建议 #\n   评估需求:\n 确认是否需要复杂的相关性排序 确认是否需要位置相关的查询    存储优化:\n 尽量使用默认设置 避免启用额外的存储功能    查询限制:\n 避免使用依赖位置信息的查询 注意短语查询的性能影响    性能考虑:\n 适合大规模数据集 有助于减少存储成本    ","subcategory":null,"summary":"","tags":null,"title":"match_only_text 字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/match_only_text/"},{"category":null,"content":"Join 字段类型 #  Join 字段类型用于在同一索引中的文档之间建立父/子关系。\n代码样例 #  模拟创建一个映射来建立一个产品和其品牌之间的父/子关系：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;product_to_brand\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;relations\u0026#34;: { \u0026#34;brand\u0026#34;: \u0026#34;product\u0026#34; } } } } } 索引一个父文档：\nPUT testindex1/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Brand 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;brand\u0026#34; } } 您也可以使用更简单的格式：\nPUT testindex1/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;Brand 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: \u0026#34;brand\u0026#34; } 在索引子文档时，您需要指定 routing 查询参数，因为同一父/子层级中的父文档和子文档必须索引在同一分片上。每个子文档在 parent 字段中引用其父文档的 ID。\n为每个父文档索引两个子文档：\nPUT testindex1/_doc/3?routing=1 { \u0026#34;name\u0026#34;: \u0026#34;Product 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } PUT testindex1/_doc/4?routing=1 { \u0026#34;name\u0026#34;: \u0026#34;Product 2\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } 查询 join 字段 #  当您查询 join 字段时，返回内容中包含指定返回文档是父文档还是子文档的子字段。对于子对象，还会返回父文档 ID。\n搜索所有文档 #  GET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 返回内容：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Brand 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;brand\u0026#34; } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Product 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Product 2\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } } ] } } 搜索父文档的所有子文档 #  查找与 Brand 1 相关的所有产品：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;has_parent\u0026#34;: { \u0026#34;parent_type\u0026#34;: \u0026#34;brand\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Brand 1\u0026#34; } } } } } 返回内容：\n{ \u0026#34;took\u0026#34;: 7, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Product 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_routing\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Product 2\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;parent\u0026#34;: \u0026#34;1\u0026#34; } } } ] } } 搜索子文档的父文档 #  查找 Product 1 的父文档：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;has_child\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Product 1\u0026#34; } } } } } 返回内容：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.0, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1.0, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Brand 1\u0026#34;, \u0026#34;product_to_brand\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;brand\u0026#34; } } } ] } } 具有多个子文档的父文档 #  一个父文档可以有多个子文档。创建一个具有多个子文档的映射：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;parent_to_child\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;relations\u0026#34;: { \u0026#34;parent\u0026#34;: [\u0026#34;child 1\u0026#34;, \u0026#34;child 2\u0026#34;] } } } } } Join 字段类型注意事项 #    一个索引中只能有一个 join 字段映射。\n  在检索、更新或删除子文档时，您需要提供 routing 参数。这是因为同一关系中的父文档和子文档必须索引在同一分片上。\n  不支持一个子文档有多个父文档。\n  父子查询和聚合可能会很耗费资源。它们的性能取决于需要检索的子文档或父文档的数量。\n  您可以向现有的 join 字段添加新的关系。\n  ","subcategory":null,"summary":"","tags":null,"title":"Join 字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/object-field-type/join/"},{"category":null,"content":"IP 地址字段类型 #  IP 字段类型用于存储 IPv4 或 IPv6 格式的 IP 地址。\n 要表示 IP 地址范围，可以使用 IP 范围字段类型\n 参考代码 #  创建一个有 IP 地址的 mapping\nPUT testindex { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;ip_address\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;ip\u0026quot; } } } } 索引一个有 IP 地址的文档\nPUT testindex/_doc/1 { \u0026quot;ip_address\u0026quot; : \u0026quot;10.24.34.0\u0026quot; } 查询一个特定 IP 地址的索引\nGET testindex/_doc/1 { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;ip_address\u0026quot;: \u0026quot;10.24.34.0\u0026quot; } } } 搜索 IP 地址及其关联的网络掩码 #  您可以使用无类别域间路由 (CIDR) 表示法查询索引中的 IP 地址。在 CIDR 表示法中，通过斜杠 / 分隔 IP 地址和前缀长度（0–32）。例如，前缀长度为 24 表示匹配所有具有相同前 24 位的 IP 地址。\n查询 IPV4 格式的参考代码 #  GET testindex/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;ip_address\u0026quot;: \u0026quot;10.24.34.0/24\u0026quot; } } } 查询 IPV6 格式的参考代码 #  GET testindex/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;ip_address\u0026quot;: \u0026quot;2001:DB8::/24\u0026quot; } } } 如果在 query_string 查询中使用 IPv6 格式的 IP 地址，需要对 : 字符进行转义，因为它们会被解析为特殊字符。可以通过将 IP 地址用引号括起来，并使用 \\ 转义引号来实现。\nGET testindex/_search { \u0026quot;query\u0026quot; : { \u0026quot;query_string\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;ip_address:\\\u0026quot;2001:DB8::/24\\\u0026quot;\u0026quot; } } } 参数说明 #  下面是 IP 字段的参数说明，都是可选参数 参数说明：\n   参数 描述 默认值     boost 浮点值，指定字段对相关性评分的权重。大于 1.0 提高相关性，0.0 到 1.0 降低相关性。 1.0   doc_values 布尔值，指定是否将字段存储到磁盘，以便用于聚合、排序或 script 操作。 true   ignore_malformed 布尔值，指定是否忽略格式错误的值而不抛出异常。 false   index 布尔值，指定字段是否可被搜索。 true   null_value 指定替代 null 的值，必须与字段类型一致。如果未指定，值为 null 时视为缺失。 null   store 布尔值，指定字段值是否单独存储，并可在 _source 字段外被检索。 false    ","subcategory":null,"summary":"","tags":null,"title":"IP 地址字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/ip/"},{"category":null,"content":"ID 属性 #  Easysearch 中的每个文档都有一个唯一的 _id 字段。此字段已被索引，允许您使用 GET API 或 ids 查询 检索文档。\n 如果您未提供 _id 值，则 Easysearch 会自动为文档生成一个。\n 以下示例请求创建一个名为 test-index1 的索引，并添加两个具有不同 _id 值的文档：\nPUT test-index1/_doc/1 { \u0026quot;text\u0026quot;: \u0026quot;Document with ID 1\u0026quot; } PUT test-index1/_doc/2?refresh=true { \u0026quot;text\u0026quot;: \u0026quot;Document with ID 2\u0026quot; } 您可以使用 _id 字段查询文档，如以下示例请求所示：\nGET test-index1/_search { \u0026quot;query\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;_id\u0026quot;: [\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;] } } } 返回 _id 值为 1 和 2 的两个文档：\n{ \u0026quot;took\u0026quot;: 10, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 2, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 1, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;test-index1\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 1, \u0026quot;_source\u0026quot;: { \u0026quot;text\u0026quot;: \u0026quot;Document with ID 1\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;test-index1\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot;: 1, \u0026quot;_source\u0026quot;: { \u0026quot;text\u0026quot;: \u0026quot;Document with ID 2\u0026quot; } } ] } _id 字段的限制 #\n 虽然 _id 字段可以在各种查询中使用，但它在聚合、排序和脚本中的使用受到限制。如果您需要对 _id 字段进行排序或聚合，建议将 _id 内容复制到另一个启用了 doc_values 的字段中。\n","subcategory":null,"summary":"","tags":null,"title":"ID 属性","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/metadata-field/id/"},{"category":null,"content":"Geoshape 地理形状字段类型 #  Geoshape 地理形状字段类型包含地理形状，例如多边形或地理点的集合。为了索引地理形状，Easysearch 会将形状分割成三角形网格，并将每个三角形存储在 BKD 树中。这提供了 10^-7 度的精度，代表了接近完美的空间分辨率。这个过程的性能主要受到您正在索引的多边形顶点数量多少的影响。\n代码样例 #  创建一个带有地理形状字段类型的映射：\nPUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_shape\u0026#34; } } } } 格式说明 #  地理形状可以用以下格式索引：\n  GeoJSON  Well-Known Text (WKT)   在 GeoJSON 和 WKT 中，坐标必须在坐标数组中按照 经度, 纬度 的顺序指定。注意在这种格式中经度是在前面的。\n 地理形状类型 #  下表描述了可能的地理形状类型以及它们与 GeoJSON 和 WKT 类型的关系。\n   Easysearch 类型 GeoJSON 类型 WKT 类型 描述     point Point POINT 由纬度和经度指定的地理点。Easysearch 使用世界大地测量系统 (WGS84) 坐标。   linestring LineString LINESTRING 由两个或更多点指定的线。可以是直线或连接的线段路径。   polygon Polygon POLYGON 由坐标形式的顶点列表指定的多边形。多边形必须是闭合的，这意味着最后一个点必须与第一个点相同。因此，要创建一个 n 边形，需要 n+1 个顶点。最少需要四个顶点，这会创建一个三角形。   multipoint MultiPoint MULTIPOINT 不连接的离散相关点的数组。   multilinestring MultiLineString MULTILINESTRING 线串的数组。   multipolygon MultiPolygon MULTIPOLYGON 多边形的数组。   geometrycollection GeometryCollection GEOMETRYCOLLECTION 可能是不同类型的地理形状的集合。   envelope N/A BBOX 由左上和右下顶点指定的边界矩形。    Point 点位 #  一个点代表着由经度和纬度指定的单个坐标对。\n在 GeoJSON 格式中索引一个点：\nPUT testindex/_doc/1 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;point\u0026#34;, \u0026#34;coordinates\u0026#34; : [74.0060, 40.7128] } } 在 WKT 格式中索引一个点数据：\nPUT testindex/_doc/1 { \u0026#34;location\u0026#34; : \u0026#34;POINT (74.0060 40.7128)\u0026#34; } Linestring 线串 #  一个线串是由两个或更多点指定的线。如果点是共线的，则线串是直线。否则，线串表示由线段组成的路径。\n在 GeoJSON 格式中索引一个线串：\nPUT testindex/_doc/2 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;linestring\u0026#34;, \u0026#34;coordinates\u0026#34; : [[74.0060, 40.7128], [71.0589, 42.3601]] } } 在 WKT 格式中索引一个线串：\nPUT testindex/_doc/2 { \u0026#34;location\u0026#34; : \u0026#34;LINESTRING (74.0060 40.7128, 71.0589 42.3601)\u0026#34; } Polygon 多边形 #  一个多边形是由坐标形式的顶点列表指定的。多边形必须是闭合的，这意味着最后一个点必须与第一个点相同。在下面的例子中，创建了一个三角形，使用了四个点。\n GeoJSON 要求您以逆时针顺序列出多边形的顶点。WKT 不对顶点顺序施加任何限制。\n 在 GeoJSON 格式中索引一个多边形（三角形）：\nPUT testindex/_doc/3 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [ [74.0060, 40.7128], [73.7562, 42.6526], [71.0589, 42.3601], [74.0060, 40.7128] ] ] } } 在 WKT 格式中索引一个多边形（三角形）：\nPUT testindex/_doc/3 { \u0026#34;location\u0026#34; : \u0026#34;POLYGON ((74.0060 40.7128, 71.0589 42.3601, 73.7562 42.6526, 74.0060 40.7128))\u0026#34; } 多边形可以有内部的洞。在这种情况下，坐标字段将包含多个数组。第一个数组表示外部多边形，每个后续数组表示一个洞。洞表示为多边形，并指定为坐标数组。\n GeoJSON 要求您以逆时针顺序列出多边形的顶点，并以顺时针顺序列出洞的顶点。WKT 不对顶点顺序施加任何限制。\n 在 GeoJSON 格式中索引一个多边形（三角形）和一个三角形洞：\nPUT testindex/_doc/4 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;polygon\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [ [74.0060, 40.7128], [73.7562, 42.6526], [71.0589, 42.3601], [74.0060, 40.7128] ], [ [72.6734, 41.7658], [73.0515, 41.5582], [72.6506, 41.5623], [72.6734, 41.7658] ] ] } } 在 WKT 格式中索引一个多边形（三角形）和一个三角形洞：\nPUT testindex/_doc/4 { \u0026#34;location\u0026#34; : \u0026#34;POLYGON ((74.0060 40.7128, 71.0589 42.3601, 73.7562 42.6526, 74.0060 40.7128), (72.6734 41.7658, 72.6506 41.5623, 73.0515 41.5582, 72.6734 41.7658))\u0026#34; } 您可以通过以顺时针或逆时针顺序列出顶点来指定 Easysearch 中的多边形。这对于不跨越日期线（小于 180°）的多边形来说是有效的。然而，跨越日期线（大于 180°）的多边形可能是模糊的，因为 WKT 不对顶点顺序施加任何限制。因此，您必须通过以逆时针顺序列出顶点来指定跨越日期线的多边形。\n您可以定义一个 orientation 方向参数来指定顶点遍历顺序：\nPUT testindex { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_shape\u0026#34;, \u0026#34;orientation\u0026#34; : \u0026#34;left\u0026#34; } } } } 随后索引的文档可以覆盖 orientation 设置：\nPUT testindex/_doc/3 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;polygon\u0026#34;, \u0026#34;orientation\u0026#34; : \u0026#34;cw\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [[74.0060, 40.7128], [71.0589, 42.3601], [73.7562, 42.6526], [74.0060, 40.7128]] ] } } Multipoint 多点位 #  一个多点是由不连接的离散相关点组成的数组。\n在 GeoJSON 格式中索引一个多点：\nPUT testindex/_doc/6 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;multipoint\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [74.0060, 40.7128], [71.0589, 42.3601] ] } } 在 WKT 格式中索引一个多点：\nPUT testindex/_doc/6 { \u0026#34;location\u0026#34; : \u0026#34;MULTIPOINT (74.0060 40.7128, 71.0589 42.3601)\u0026#34; } Multilinestring 多线串 #  一个多线串是由线串组成的数组。\n在 GeoJSON 格式中索引一个多线串：\nPUT testindex/_doc/2 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;multilinestring\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [[74.0060, 40.7128], [71.0589, 42.3601]], [[73.7562, 42.6526], [72.6734, 41.7658]] ] } } 在 WKT 格式中索引一个多线串：\nPUT testindex/_doc/2 { \u0026#34;location\u0026#34; : \u0026#34;MULTILINESTRING ((74.0060 40.7128, 71.0589 42.3601), (73.7562 42.6526, 72.6734 41.7658))\u0026#34; } Multipolygon 多个多边形 #  多个多边形是由多边形组成的数组。在这个例子中，第一个多边形包含一个洞，第二个不包含。\n在 GeoJSON 格式中索引多个多边形：\nPUT testindex/_doc/4 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;multipolygon\u0026#34;, \u0026#34;coordinates\u0026#34; : [ [ [ [74.0060, 40.7128], [73.7562, 42.6526], [71.0589, 42.3601], [74.0060, 40.7128] ], [ [73.0515, 41.5582], [72.6506, 41.5623], [72.6734, 41.7658], [73.0515, 41.5582] ] ], [ [ [73.9146, 40.8252], [73.8871, 41.0389], [73.6853, 40.9747], [73.9146, 40.8252] ] ] ] } } 在 WKT 格式中索引多个多边形：\nPUT testindex/_doc/4 { \u0026#34;location\u0026#34; : \u0026#34;MULTIPOLYGON (((74.0060 40.7128, 71.0589 42.3601, 73.7562 42.6526, 74.0060 40.7128), (72.6734 41.7658, 72.6506 41.5623, 73.0515 41.5582, 72.6734 41.7658)), ((73.9146 40.8252, 73.6853 40.9747, 73.8871 41.0389, 73.9146 40.8252)))\u0026#34; } Geometry collection 几何集合 #  一个几何集合是可能是不同类型的地理形状的集合。\n在 GeoJSON 格式中索引一个几何集合：\nPUT testindex/_doc/7 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34;: \u0026#34;geometrycollection\u0026#34;, \u0026#34;geometries\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;point\u0026#34;, \u0026#34;coordinates\u0026#34;: [74.0060, 40.7128] }, { \u0026#34;type\u0026#34;: \u0026#34;linestring\u0026#34;, \u0026#34;coordinates\u0026#34;: [[73.7562, 42.6526], [72.6734, 41.7658]] } ] } } 在 WKT 格式中索引一个几何集合：\nPUT testindex/_doc/7 { \u0026#34;location\u0026#34; : \u0026#34;GEOMETRYCOLLECTION (POINT (74.0060 40.7128), LINESTRING(73.7562 42.6526, 72.6734 41.7658))\u0026#34; } Envelope 边界矩阵 #  一个边界矩形是由左上和右下顶点指定的。在 GeoJSON 格式中，边界矩形的坐标是 [[minLon, maxLat], [maxLon, minLat]]。\n在 GeoJSON 格式中索引一个边界矩形：\nPUT testindex/_doc/2 { \u0026#34;location\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;envelope\u0026#34;, \u0026#34;coordinates\u0026#34; : [[71.0589, 42.3601], [74.0060, 40.7128]] } } 在 WKT 格式中，使用 BBOX (minLon, maxLon, maxLat, minLat)。\n在 WKT 格式中索引一个边界矩形：\nPUT testindex/_doc/8 { \u0026#34;location\u0026#34; : \u0026#34;BBOX (71.0589, 74.0060, 42.3601, 40.7128)\u0026#34; } 参数说明 #  以下表格列出了地理形状字段类型接受的参数。所有参数都是可选的。\n   参数 描述     coerce 一个布尔值，指定是否自动关闭未闭合的线环。默认为 false。   ignore_malformed 一个布尔值，指定是否忽略格式错误的 GeoJSON 或 WKT 地理形状，而不是抛出异常。默认为 false（抛出异常）。   ignore_z_value 特定于具有三个坐标的点。如果 ignore_z_value 为 true，则第三个坐标不会被索引，但仍会存储在 _source 字段中。如果 ignore_z_value 为 false，则会抛出异常。默认为 true。   orientation 指定地理形状坐标列表中顶点的遍历顺序。orientation 可以取以下值：1. RIGHT：逆时针。使用以下字符串（大写或小写）指定 RIGHT 方向：right、counterclockwise、ccw。 2. LEFT：顺时针。使用以下字符串（大写或小写）指定 LEFT 方向：left、clockwise、cw。这个值可以被单个文档覆盖。 默认为 RIGHT。    ","subcategory":null,"summary":"","tags":null,"title":"Geoshape 地理形状字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/geo-filed-type/geo-shape/"},{"category":null,"content":"Geopoint 地理点字段类型 #  Geopoint 地理点字段类型包含由纬度 latitude 和经度 longitude 指定的地理点。\n代码示例 #  创建一个带有 Geopoint 地理点字段类型的映射：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;point\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; } } } } 地理点格式 #  Geopoint 地理点可以用以下格式索引：\n 包含纬度和经度的对象  PUT testindex1/_doc/1 { \u0026#34;point\u0026#34;: { \u0026#34;lat\u0026#34;: 40.71, \u0026#34;lon\u0026#34;: 74.00 } } 写入包含纬度,经度 的文档  PUT testindex1/_doc/2 { \u0026#34;point\u0026#34;: \u0026#34;40.71,74.00\u0026#34; } geohash 格式的文档  PUT testindex1/_doc/3 { \u0026#34;point\u0026#34;: \u0026#34;txhxegj0uyp3\u0026#34; } [经度, 纬度] 格式的数组  PUT testindex1/_doc/4 { \u0026#34;point\u0026#34;: [74.00, 40.71] }  Well-Known Text POINT 格式，格式为 \u0026ldquo;POINT(经度 纬度)\u0026rdquo;  PUT testindex1/_doc/5 { \u0026#34;point\u0026#34;: \u0026#34;POINT (74.00 40.71)\u0026#34; } 参数说明 #  下表列出了 Geopoint 地理点字段类型接受的参数。所有参数都是可选的。\n   参数 描述     ignore_malformed 布尔值，指定是否忽略格式错误的值而不抛出异常。纬度的有效值范围是 [-90, 90]。经度的有效值范围是 [-180, 180]。默认为 false。   ignore_z_value 特定于具有三个坐标的点。如果 ignore_z_value 为 true，则第三个坐标不会被索引，但仍会存储在 _source 字段中。如果 ignore_z_value 为 false，则会抛出异常。   null_value 用于替代 null 的值。必须与字段类型相同。如果未指定此参数，当字段值为 null 时，该字段将被视为缺失。默认为 null。    ","subcategory":null,"summary":"","tags":null,"title":"Geopoint 地理点字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/geo-filed-type/geo-point/"},{"category":null,"content":"flattened_text 字段类型 #  Introduced 1.10.0\nflattened_text 类型是一种特殊的数据结构，适用于存储和查询嵌套层次的数据，同时保留类似于 text 类型的灵活搜索特性，例如分词和全文匹配。它在处理结构化或者半结构化数据时非常有用，例如 JSON 对象或动态键值对映射。\n定义映射 #  { \u0026#34;properties\u0026#34;: { \u0026#34;my_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;flattened_text\u0026#34; } } } 特性 #    扁平化存储\n 将嵌套的 JSON 对象转换为扁平结构 保留完整的路径信息 支持点号访问内部字段    文本分析\n 支持标准分词器 支持短语查询 支持全文搜索功能    内部索引结构 每个 flattened_text 字段在 lucene 层面会创建多个子字段:\n {field} - 存储所有键 {field}._value - 存储所有值 {field}._valueAndPath - 存储 \u0026ldquo;path=value\u0026rdquo; 格式    索引示例 #  PUT my_index/_doc/1 { \u0026quot;my_field\u0026quot;: { \u0026quot;key1\u0026quot;: { \u0026quot;subkey1\u0026quot;: { \u0026quot;subkey2\u0026quot;: \u0026quot;quick brown fox\u0026quot; } } } } 查询示例 #  精确路径匹配 #  // Match Query - 指定完整路径 GET my_index/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;my_field.key1.subkey1.subkey2\u0026quot;: \u0026quot;quick brown fox\u0026quot; } } } // Match Phrase Query - 指定完整路径的短语匹配 GET my_index/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;my_field.key1.subkey1.subkey2\u0026quot;: \u0026quot;quick brown fox\u0026quot; } } } 通过根字段字段查询 #\n // Match Query - 匹配所有值 GET my_index/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;my_field\u0026quot;: \u0026quot;quick brown fox\u0026quot; } } } // Match Phrase Query - 短语匹配所有值 GET my_index/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;my_field\u0026quot;: \u0026quot;quick brown fox\u0026quot; } } } 使用场景 #\n  需要对 JSON 对象进行全文检索 需要保持对象结构但又想避免字段爆炸 对象结构不固定或未知  限制 #   不支持聚合操作 不支持排序 不支持通配符查询  对比 #  与普通 text 和 flattened 类型相比:\n vs text:支持嵌套对象访问 vs flattened:支持全文检索功能  ","subcategory":null,"summary":"","tags":null,"title":"Flat 对象字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/object-field-type/flattened_text/"},{"category":null,"content":"Flat 对象字段类型 #  在 Easysearch 中，您不需要在索引文档之前指定映射。如果您不指定映射，Easysearch 会使用动态映射自动映射文档中的每个字段及其子字段。当您摄取诸如日志之类的文档时，您可能事先不知道每个字段的子字段名称和类型。在这种情况下，动态映射所有新的子字段可能会快速导致\u0026quot;映射爆炸\u0026quot;，其中不断增长的字段数量可能会降低集群的性能。\nFlat 对象字段类型通过将整个 JSON 对象视为字符串来解决这个问题。可以使用标准的点路径表示法访问 JSON 对象中的子字段，但它们不会被索引成单独的字段以供快速查找。\n 点表示法（a.b）中的字段名最大长度为 2^24 − 1。\n Flat 对象字段类型提供以下优势：\n 高效读取：获取性能类似于关键字字段。 内存效率：将整个复杂的 JSON 对象存储在一个字段中而不索引其所有子字段，可以减少索引中的字段数量。 空间效率：Easysearch 不会为 flat 对象中的子字段创建倒排索引，从而节省空间。 迁移兼容性：您可以将数据从支持类似 flat 字段的数据库系统迁移到 Easysearch。  当字段及其子字段主要用于读取而不是用作搜索条件时，应将字段映射为 flat 对象，因为子字段不会被索引。当对象具有大量字段或您事先不知道内容时，flat 对象非常有用。\nFlat 对象支持带有和不带有点路径表示法的精确匹配查询。有关支持的查询类型的完整列表，请参见支持的查询。\n 在文档中搜索特定嵌套字段的值可能效率低下，因为它可能需要对索引进行完整扫描，这可能是一个昂贵的操作。\n Flat 对象不支持：\n 特定类型的解析。 数值运算，如数值比较或数值排序。 文本分析。 高亮显示。 使用点表示法(a.b)的子字段聚合。 按子字段过滤。  支持的查询 #  Flat 对象字段类型支持以下查询：\n Term Terms Terms set Prefix Range Match Multi-match Query string Simple query string Exists Wildcard  限制 #  以下限制适用于 Easysearch 中的 flat 对象：\n Flat 对象不支持开放参数。 不支持使用 Painless 脚本和通配符查询来检索子字段的值。  使用 flat 对象 #  以下示例说明怎么将字段映射为 flat 对象、怎么索引带有 flat 对象字段的文档以及在这些文档中怎么去搜索 flat 对象的值。\n首先，创建索引的映射，其中 issue 的类型为 flattened：\nPUT /test-index/ { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;issue\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;flattened\u0026#34; } } } } 然后，索引两个带有 flat 对象字段的文档：\nPUT /test-index/_doc/1 { \u0026#34;issue\u0026#34;: { \u0026#34;number\u0026#34;: \u0026#34;123456\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;2.1\u0026#34;, \u0026#34;backport\u0026#34;: [ \u0026#34;2.0\u0026#34;, \u0026#34;1.3\u0026#34; ], \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;API\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;enhancement\u0026#34; } } } } PUT /test-index/_doc/2 { \u0026quot;issue\u0026quot;: { \u0026quot;number\u0026quot;: \u0026quot;123457\u0026quot;, \u0026quot;labels\u0026quot;: { \u0026quot;version\u0026quot;: \u0026quot;2.2\u0026quot;, \u0026quot;category\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;API\u0026quot;, \u0026quot;level\u0026quot;: \u0026quot;bug\u0026quot; } } } } 要搜索 flat 对象的值，可以使用 GET 或 POST 请求。即使您不知道字段名称，也可以在整个 flat 对象中搜索叶值。例如，以下请求搜索所有标记为 bug 的问题：\nGET /test-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: {\u0026#34;issue\u0026#34;: \u0026#34;bug\u0026#34;} } } 或者，如果您知道要搜索的子字段名称，请使用点表示法提供字段的路径：\nGET /test-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: {\u0026#34;issue.labels.category.level\u0026#34;: \u0026#34;bug\u0026#34;} } } 在这两种情况下，响应都是相同的，并且包含文档 2：\n{ \u0026#34;took\u0026#34;: 1, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.0303539, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;test-index\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.0303539, \u0026#34;_source\u0026#34;: { \u0026#34;issue\u0026#34;: { \u0026#34;number\u0026#34;: \u0026#34;123457\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;2.2\u0026#34;, \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;API\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;bug\u0026#34; } } } } } ] } } 使用前缀查询，您可以搜索所有以 2. 开头的版本的问题：\nGET /test-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34;: {\u0026#34;issue.labels.version\u0026#34;: \u0026#34;2.\u0026#34;} } } 使用范围查询，您可以搜索版本 2.0-2.1 的所有问题：\nGET /test-index/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;issue\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2.1\u0026#34; } } } } 将子字段定义为 flat 对象 #  您可以将 JSON 对象的子字段定义为 flat 对象。例如，使用以下查询将 issue.labels 定义为 flattened：\nPUT /test-index/ { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;issue\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;number\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, \u0026#34;labels\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;flattened\u0026#34; } } } } } } 因为 issue.number 不是 flat 对象的一部分，所以您可以使用它来聚合和排序文档。\n","subcategory":null,"summary":"","tags":null,"title":"Flat 对象字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/object-field-type/flattened/"},{"category":null,"content":"Date 字段类型 #  在 Easysearch 中，日期可以表示为以下几种形式：\n 一个长整型值，对应自纪元以来的毫秒数（必须为非负数）。日期在内部以此形式存储。 一个格式化的字符串。 一个整数值，对应自纪元以来的秒数（必须为非负数）。   要表示日期范围，可以使用 date range 字段类型。\n 代码样例 #  创建一个有两种日期格式的 date 字段\nPUT testindex { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;release_date\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;date\u0026quot;, \u0026quot;format\u0026quot; : \u0026quot;strict_date_optional_time||epoch_millis\u0026quot; } } } } 参数说明 #  下表列出了日期字段类型支持的参数，所有参数均为可选项。    参数 描述 默认值     boost 浮点值，指定字段对相关性评分的权重。值大于 1.0 增加相关性，0.0 到 1.0 降低相关性。 1.0   doc_values 布尔值，指定是否将字段存储到磁盘，以便用于聚合、排序或 script 脚本操作。 false   format 用于解析日期的格式。 strict_date_time_no_millis || strict_date_optional_time || epoch_millis   ignore_malformed 布尔值，指定是否忽略格式错误的值而不抛出异常。 false   index 布尔值，指定字段是否可搜索。 true   locale 指定基于区域和语言的日期表示格式。 ROOT（区域和语言中立的本地设置）   meta 接受字段的元数据。    null_value 指定替代 null 的值，必须与字段类型一致。如果未指定，字段值为 null 时会被视为缺失值。 null   store 布尔值，指定字段值是否单独存储并可从 _source 字段外检索。 false    格式 #  Easysearch 提供内置的日期格式，但您也可以自定义日期格式。\n您可以指定多个日期格式，并使用 || 将它们分隔开。\n内置格式 #  大多数日期格式都有一个以 strict_ 开头的对应格式。当格式以 strict_ 开头时，日期必须严格符合格式中指定的位数要求。例如，如果格式设置为 strict_year_month_day（\u0026quot;yyyy-MM-dd\u0026quot;），月份和日期必须是两位数字。因此，\u0026quot;2020-06-09\u0026quot; 是有效的，而 \u0026quot;2020-6-9\u0026quot; 是无效的。\n Epoch 时间定义为 1970 年 1 月 1 日 00:00:00 UTC。\n  y：年份。\nY：基于周的年份。\nM：月份。\nw：年的序数周，从 01 到 53。\nd：日期（天）。\nD：年的序数日，从 001 到 365（闰年为 366）。\ne：周的序数日，从 1（星期一）到 7（星期日）。\nH：小时，从 0 到 23。\nm：分钟。\ns：秒。\nS：秒的小数部分。\nZ：时区偏移（例如，+0400；-0400；-04:00）。\n 数字化日期格式 #     格式名称 描述 示例     epoch_millis 自纪元以来的毫秒数。最小值为 -2⁶³，最大值为 2⁶³ − 1。 1553391286000   epoch_second 自纪元以来的秒数。最小值为 -2⁶³ ÷ 1000，最大值为 (2⁶³ − 1) ÷ 1000。 1553391286    基础日期格式 #  基本日期格式的各个组成部分之间没有分隔符。例如：“20190323”。\n   格式名称 描述 模式和示例     Dates     basic_date_time 基本日期和时间格式，以 T 分隔。 \u0026quot;yyyyMMddTHHmmss.SSSZ\u0026quot; \u0026quot;20190323T213446.123-04:00\u0026quot;   basic_date_time_no_millis 不含毫秒的基本日期和时间格式，以 T 分隔。 \u0026quot;yyyyMMddTHHmmssZ\u0026quot; \u0026quot;20190323T213446-04:00\u0026quot;   basic_date 包含四位数年份、两位数月份和两位数日期的基本格式。 \u0026quot;yyyyMMdd\u0026quot; \u0026quot;20190323\u0026quot;   Times     basic_time 包含小时、分钟、秒、毫秒和时区偏移的时间格式。 \u0026quot;HHmmss.SSSZ\u0026quot; \u0026quot;213446.123-04:00\u0026quot;   basic_time_no_millis 不含毫秒的基本时间格式。 \u0026quot;HHmmssZ\u0026quot; \u0026quot;213446-04:00\u0026quot;   T times     basic_t_time 以 T 开头的基本时间格式。 \u0026quot;THHmmss.SSSZ\u0026quot; \u0026quot;T213446.123-04:00\u0026quot;   basic_t_time_no_millis 以 T 开头的不含毫秒的基本时间格式。 \u0026quot;THHmmssZ\u0026quot; \u0026quot;T213446-04:00\u0026quot;   Ordinal dates     basic_ordinal_date_time 完整的序数日期和时间格式。 \u0026quot;yyyyDDDTHHmmss.SSSZ\u0026quot; \u0026quot;2019082T213446.123-04:00\u0026quot;   basic_ordinal_date_time_no_millis 不含毫秒的完整序数日期和时间格式。 \u0026quot;yyyyDDDTHHmmssZ\u0026quot; \u0026quot;2019082T213446-04:00\u0026quot;   basic_ordinal_date 包含四位数年份和三位数年内序数日期的格式。 \u0026quot;yyyyDDD\u0026quot; \u0026quot;2019082\u0026quot;   Week-based dates     basic_week_date_time\nstrict_basic_week_date_time 以 T 分隔的完整周日期和时间格式。 \u0026quot;YYYYWwweTHHmmss.SSSSSSSSSZ\u0026quot; \u0026quot;2019W126213446.123-04:00\u0026quot;   basic_week_date_time_no_millis\nstrict_basic_week_date_time_no_millis 不含毫秒的基本周日期和时间格式，以 T 分隔。 \u0026quot;YYYYWwweTHHmmssZ\u0026quot; \u0026quot;2019W126213446-04:00\u0026quot;   basic_week_date\nstrict_basic_week_date 包含四位数年份、两位数周编号和一位数周内天编号的完整周日期格式，以 W 分隔。 \u0026quot;YYYYWwwe\u0026quot; \u0026quot;2019W126\u0026quot;    完整日期格式 #  完整日期格式的各个组成部分，日期部分使用 - 作为分隔符，时间部分使用 : 作为分隔符。 例如：\u0026quot;2019-03-23T21:34\u0026quot;。\n日期 #     格式名称 描述 模式和示例     date_optional_time / strict_date_optional_time 通用日期和时间格式，年份必需，月份、日期和时间可选。时间用 T 分隔。 多种格式：\n2019--03--23T21:34:46.123456789--04:00\n2019-03-23T21:34:46\n2019-03-23T21:34\n2019   strict_date_optional_time_nanos 通用日期和时间格式，年份必需，月份、日期和时间可选。如果指定时间，必须包含小时、分钟和秒；秒的小数部分可选，最多 9 位，精度为纳秒，时间用 T 分隔。 多种格式：\n2019-03-23T21:34:46.123456789-04:00\n2019-03-23T21:34:46\n2019   date_time / strict_date_time 完整日期和时间，以 T 分隔，包含毫秒和时区偏移。 \u0026quot;yyyy-MM-ddTHH:mm:ss.SSSZ\u0026quot;\n2019-03-23T21:34:46.123-04:00   date_time_no_millis / strict_date_time_no_millis 完整日期和时间，不包含毫秒，以 T 分隔。 \u0026quot;yyyy-MM-dd'T'HH:mm:ssZ\u0026quot;\n2019-03-23T21:34:46-04:00   datehour_minute_second_fraction / strict* 完整日期、小时、分钟、秒以及 1 到 9 位的小数秒，以 T 分隔。 \u0026quot;yyyy-MM-ddTHH:mm:ss.SSSSSSSSS\u0026quot;\n2019-03-23T21:34:46.123456789\n2019-03-23T21:34:46.1   datehour_minute_second_millis / strict* 包含日期、小时、分钟、秒和 3 位毫秒，以 T 分隔。 \u0026quot;yyyy-MM-ddTHH:mm:ss.SSS\u0026quot;\n2019-03-23T21:34:46.123   datehour_minute_second / strict* 包含日期、小时、分钟和秒，以 T 分隔。 \u0026quot;yyyy-MM-ddTHH:mm:ss\u0026quot;\n2019-03-23T21:34:46   datehour_minute / strict* 包含日期、小时和分钟，以 T 分隔。 \u0026quot;yyyy-MM-ddTHH:mm\u0026quot;\n2019-03-23T21:34   datehour / strict* 包含日期和小时，以 T 分隔。 \u0026quot;yyyy-MM-ddTHH\u0026quot;\n2019-03-23T21   date / strict_date 包含年、月、日。 \u0026quot;yyyy-MM-dd\u0026quot;\n2019-03-23   year_month_day / strict_year_month_day 与日期格式相同，包含年、月、日。 \u0026quot;yyyy-MM-dd\u0026quot;\n2019-03-23   year_month / strict_year_month 包含年和月。 \u0026quot;yyyy-MM\u0026quot;\n2019-03   year / strict_year 仅包含年份。 \u0026quot;yyyy\u0026quot;\n2019   rfc3339_lenient 兼容 RFC3339 的日期格式，支持多种模式并解析速度更快。 \u0026quot;2019\u0026quot;\n2019-03\n2019-03-23\n2019-03-23T21:34Z\n2019-03-23T21:34:46.123456789-04:00    时间 #     格式名称 描述 模式和示例     time / strict_time 两位数小时、两位数分钟、两位数秒、1 到 9 位小数秒和时区偏移。 \u0026quot;HH:mm:ss.SSSSSSSSSZ\u0026quot;\n21:34:46.123456789-04:00\n21:34:46.1-04:00   time_no_millis / strict_time_no_millis 两位数小时、两位数分钟、两位数秒和时区偏移，不包含毫秒。 \u0026quot;HH:mm:ssZ\u0026quot;\n21:34:46-04:00   hour_minute_second_fraction / strict_hour_minute_second_fraction 两位数小时、两位数分钟、两位数秒以及 1 到 9 位小数秒。 \u0026quot;HH:mm:ss.SSSSSSSSS\u0026quot;\n21:34:46.1\n21:34:46.123456789   hour_minute_second_millis / strict_hour_minute_second_millis 两位数小时、两位数分钟、两位数秒以及 3 位毫秒。 \u0026quot;HH:mm:ss.SSS\u0026quot;\n21:34:46.123   hour_minute_second / strict_hour_minute_second 两位数小时、两位数分钟和两位数秒。 \u0026quot;HH:mm:ss\u0026quot;\n21:34:46   hour_minute / strict_hour_minute 两位数小时和两位数分钟。 \u0026quot;HH:mm\u0026quot;\n21:34   hour / strict_hour 两位数小时。 \u0026quot;HH\u0026quot;\n21    时区 #     格式名称 描述 模式和示例     t_time / strict_t_time 以 T 开头的格式，包括两位数小时、两位数分钟、两位数秒、1 到 9 位小数秒和时区偏移。 \u0026quot;THH:mm:ss.SSSSSSSSSZ\u0026quot;\nT21:34:46.123456789-04:00\nT21:34:46.1-04:00   t_time_no_millis / strict_t_time_no_millis 以 T 开头的格式，包括两位数小时、两位数分钟、两位数秒和时区偏移，不包含毫秒。 \u0026quot;THH:mm:ssZ\u0026quot;\nT21:34:46-04:00    完整时间 #     格式名称 描述 模式和示例     ordinal_date_time / strict_ordinal_date_time 完整的序数日期和时间格式，以 T 分隔，包含毫秒和时区偏移。 \u0026quot;yyyy-DDDTHH:mm:ss.SSSZ\u0026quot;\n2019-082T21:34:46.123-04:00   ordinal_date_time_no_millis / strict_ordinal_date_time_no_millis 完整的序数日期和时间格式，以 T 分隔，不包含毫秒，仅包含时区偏移。 \u0026quot;yyyy-DDDTHH:mm:ssZ\u0026quot;\n2019-082T21:34:46-04:00   ordinal_date / strict_ordinal_date 完整的序数日期格式，包含四位数年份和三位数年内序号日期。 \u0026quot;yyyy-DDD\u0026quot;\n2019-082    基于周的时间 #     格式名称 描述 模式和示例     week_date_time / strict_week_date_time 基于周的完整日期和时间，以 T 分隔。包含四位数年份、两位数周编号、一位数周内天编号、时间和时区偏移，时间可包含 1-9 位小数秒。 \u0026quot;YYYY-Www-eTHH:mm:ss.SSSSSSSSSZ\u0026quot;\n2019-W12-6T21:34:46.1-04:00\n2019-W12-6T21:34:46.123456789-04:00   week_date_time_no_millis / strict_week_date_time_no_millis 基于周的完整日期和时间，以 T 分隔，不包含毫秒。包含四位数年份、两位数周编号、一位数周内天编号、时间和时区偏移。 \u0026quot;YYYY-Www-eTHH:mm:ssZ\u0026quot;\n2019-W12-6T21:34:46-04:00   week_date / strict_week_date 基于周的完整日期，包含四位数年份、两位数周编号和一位数周内天编号。 \u0026quot;YYYY-Www-e\u0026quot;\n2019-W12-6   weekyear_week_day / strict_weekyear_week_day 包含四位数基于周的年份、两位数周编号和一位数天编号的日期格式。 \u0026quot;YYYY-'W'ww-e\u0026quot;\n2019-W12-6   weekyear_week / strict_weekyear_week 包含四位数基于周的年份和两位数周编号的日期格式。 \u0026quot;YYYY-Www\u0026quot;\n2019-W12   weekyear / strict_weekyear 包含四位数基于周的年份。 \u0026quot;YYYY\u0026quot;\n2019    定制格式 #  您可以为日期字段创建自定义格式。例如，以下请求将日期格式指定为常见的 \u0026quot;MM/dd/yyyy\u0026quot; 格式：\nPUT testindex { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;release_date\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;date\u0026quot;, \u0026quot;format\u0026quot; : \u0026quot;MM/dd/yyyy\u0026quot; } } } } 索引写入一个日期文档\nPUT testindex/_doc/21 { \u0026quot;release_date\u0026quot; : \u0026quot;03/21/2019\u0026quot; } 在搜索精确日期时，需要使用与指定格式相同的日期格式：\nGET testindex/_search { \u0026quot;query\u0026quot; : { \u0026quot;match\u0026quot;: { \u0026quot;release_date\u0026quot; : { \u0026quot;query\u0026quot;: \u0026quot;03/21/2019\u0026quot; } } } } 范围查询默认使用字段的映射格式。您也可以通过提供 format 参数，以不同的格式指定日期范围：\nGET testindex/_search { \u0026quot;query\u0026quot;: { \u0026quot;range\u0026quot;: { \u0026quot;release_date\u0026quot;: { \u0026quot;gte\u0026quot;: \u0026quot;2019-01-01\u0026quot;, \u0026quot;lte\u0026quot;: \u0026quot;2019-12-31\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;yyyy-MM-dd\u0026quot; } } } } 日期计算 #  日期字段类型支持使用日期数学运算来指定查询中的时间段。例如，在 范围查询中的 gt、gte、lt 和 lte 参数，以及 日期范围聚合中的 from 和 to 参数，都可以接受日期数学表达式。\n日期数学表达式包含一个固定日期，后面可以选择性地跟随一个或多个数学运算表达式。固定日期可以是 now（自纪元以来的当前日期和时间，以毫秒为单位），或者以 || 结尾的字符串，用于指定日期（例如，2022-05-18||）。日期必须采用 默认格式（默认为 strict_date_time_no_millis||strict_date_optional_time||epoch_millis）。\n 如果字段映射中指定了多个日期格式，Easysearch 将使用第一个格式将纪元时间值（毫秒）转换为字符串。 如果字段映射未指定格式，Easysearch 将使用 strict_date_optional_time 格式将纪元时间值转换为字符串。\n 日期数学运算支持以下数学运算符\n   运算符 描述 示例     + 加法 +1M：增加 1 个月。   - 减法 -1y：减少 1 年。   / 向下舍入 /h：舍入到当前小时的开始。    日期数学运算支持以下时间单位：\n y：年 M：月 w：周 d：天 h 或 H：小时 m：分钟 s：秒\n 示例表达式 #  以下表达式展示了日期数学运算的用法：\nnow+1M：自纪元以来当前日期和时间的毫秒数加 1 个月。\n2022-05-18||/M：将日期 2022-05-18 舍入到该月份的开始，解析为 2022-05-01。\n2022-05-18T15:23||/h：将时间 15:23 于 2022-05-18 舍入到小时的开始，解析为 2022-05-18T15。\n2022-05-18T15:23:17.789||+2M-1d/d：将时间 15:23:17.789 于 2022-05-18 加 2 个月减 1 天，然后舍入到当天的开始，解析为 2022-07-17。\n在范围查询中使用日期运算 #  以下示例展示了在 范围查询中使用日期数学运算。\n创建一个叫 release_date 的日期字段索引：\nPUT testindex { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;release_date\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;date\u0026quot; } } } } 写入两个文档：\nPUT testindex/_doc/1 { \u0026quot;release_date\u0026quot;: \u0026quot;2022-09-14\u0026quot; } PUT testindex/_doc/2 { \u0026quot;release_date\u0026quot;: \u0026quot;2022-11-15\u0026quot; } 以下查询搜索 release_date 从 2022/09/14 的前 2 个月到后 1 天范围内的文档。最后的时间范围会被四舍五入到 2022/09/14 当天的开始时间：\nGET testindex/_search { \u0026quot;query\u0026quot;: { \u0026quot;range\u0026quot;: { \u0026quot;release_date\u0026quot;: { \u0026quot;gte\u0026quot;: \u0026quot;2022-09-14T15:23||/d\u0026quot;, \u0026quot;lte\u0026quot;: \u0026quot;2022-09-14||+2M+1d\u0026quot; } } } } 返回内容包含两个文档：\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 2, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.0, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;testindex\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;release_date\u0026quot; : \u0026quot;2022-11-14\u0026quot; } }, { \u0026quot;_index\u0026quot; : \u0026quot;testindex\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;release_date\u0026quot; : \u0026quot;2022-09-14\u0026quot; } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"Date 字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/date-field-type/date/"},{"category":null,"content":"Date nanoseconds 字段类型 #  日期纳秒字段类型与 日期 字段类型类似，它存储一个日期。然而，date 以毫秒分辨率存储日期，而 date_nanos 以纳秒分辨率存储日期。日期以 long 值的形式存储，表示自纪元以来的纳秒数。因此，支持的日期范围大约是 1970-2262 年。\n对 date_nanos 字段的查询被转换为对字段值的 long 表示形式的范围查询。然后使用字段上设置的格式将存储的字段和聚合结果转换为字符串。\n date_nano 字段支持 date 支持的所有格式和参数。你可以使用 || 分隔的多种格式。\n 对于 date_nanos 字段，你可以使用 strict_date_optional_time_nanos 格式来保留纳秒值。如果你在将字段映射为 date_nanos 时没有指定格式，默认格式是 strict_date_optional_time||epoch_millis，它允许你以 strict_date_optional_time 或 epoch_millis 格式传递值。strict_date_optional_time 格式支持纳秒的日期，但 epoch_millis 格式仅支持毫秒的日期。\n示例 #  创建一个具有 strict_date_optional_time_nanos 格式的 date_nanos 类型的 date 字段的映射：\nPUT testindex/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date_nanos\u0026#34;, \u0026#34;format\u0026#34; : \u0026#34;strict_date_optional_time_nanos\u0026#34; } } } 将两个文档写入到索引中：\nPUT testindex/_doc/1 { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719622Z\u0026#34; } PUT testindex/_doc/2 { \u0026quot;date\u0026quot;: \u0026quot;2022-06-15T10:12:52.382719624Z\u0026quot; } 你可以使用范围查询来搜索日期范围：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;date\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719621Z\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719623Z\u0026#34; } } } } 响应包含日期在指定范围内的文档：\n{ \u0026#34;took\u0026#34;: 43, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719622Z\u0026#34; } } ] } } 在查询带有 date_nanos 字段的文档时，你可以使用 fields 或 docvalue_fields：\nGET testindex/_search { \u0026#34;fields\u0026#34;: [\u0026#34;date\u0026#34;] } GET testindex/_search { \u0026quot;docvalue_fields\u0026quot; : [ { \u0026quot;field\u0026quot; : \u0026quot;date\u0026quot; } ] } 上述任一查询的响应都包含两个已索引的文档：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719622Z\u0026#34; }, \u0026#34;fields\u0026#34;: { \u0026#34;date\u0026#34;: [\u0026#34;2022-06-15T10:12:52.382719622Z\u0026#34;] } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719624Z\u0026#34; }, \u0026#34;fields\u0026#34;: { \u0026#34;date\u0026#34;: [\u0026#34;2022-06-15T10:12:52.382719624Z\u0026#34;] } } ] } } 你可以按如下方式对 date_nanos 字段进行排序：\nGET testindex/_search { \u0026#34;sort\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;asc\u0026#34; } } 响应包含排序后的文档：\n{ \u0026#34;took\u0026#34;: 5, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: null, \u0026#34;_source\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719622Z\u0026#34; }, \u0026#34;sort\u0026#34;: [1655287972382719700] }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: null, \u0026#34;_source\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022-06-15T10:12:52.382719624Z\u0026#34; }, \u0026#34;sort\u0026#34;: [1655287972382719700] } ] } } 你也可以使用 Painless 脚本来访问字段的纳秒部分：\nGET testindex/_search { \u0026#34;script_fields\u0026#34; : { \u0026#34;my_field\u0026#34; : { \u0026#34;script\u0026#34; : { \u0026#34;lang\u0026#34; : \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34; : \u0026#34;doc[\u0026#39;date\u0026#39;].value.nano\u0026#34; } } } } 响应仅包含字段的纳秒部分：\n{ \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;fields\u0026#34;: { \u0026#34;my_field\u0026#34;: [382719622] } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;fields\u0026#34;: { \u0026#34;my_field\u0026#34;: [382719624] } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"Date nanoseconds 字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/date-field-type/date-nanos/"},{"category":null,"content":"全文查询 #  尽管可以使用 HTTP 请求参数执行简单搜索，但 Easysearch 查询域特定语言（DSL）允许您指定全部搜索选项。查询 DSL 使用 HTTP 请求主体。以这种方式指定的查询还有一个额外的优点，即其意图更加明确，并且更易于随时间调整。\n此页面列出了所有全文查询类型和常用选项。考虑到选项的数量和微妙的行为，确保有用搜索结果的最佳方法是根据代表性索引测试不同的查询并验证输出。\n匹配 #  创建一个 布尔查询 ，如果字段中存在搜索项，则返回结果。\n查询的最基本形式仅提供字段（ title ）和对应的值（ wind ）:\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;wind\u0026#34; } } } 采用 curl的方式:\ncurl --insecure -XGET -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; https://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;index\u0026gt;/_search \\  -H \u0026#34;content-type: application/json\u0026#34; \\  -d \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;wind\u0026#34; } } }\u0026#39; 查询接受以下选项。有关每个参数的描述，请参见 查询选项\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wind\u0026#34;, \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34;, \u0026#34;fuzzy_transpositions\u0026#34;: true, \u0026#34;operator\u0026#34;: \u0026#34;or\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: 1, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;zero_terms_query\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;lenient\u0026#34;: false, \u0026#34;cutoff_frequency\u0026#34;: 0.01, \u0026#34;prefix_length\u0026#34;: 0, \u0026#34;max_expansions\u0026#34;: 50, \u0026#34;boost\u0026#34;: 1 } } } } 多重匹配 #  类似于 匹配，但搜索多个字段.\n^ 允许您 增强 某些字段。助推是一种乘数，它对一个领域中的匹配比其他领域的匹配更为重要。在下面的示例中，标题字段中 wind 的匹配对 _score 的影响是绘图字段中匹配的四倍。结果是，像 The Wind Rises和Gone with the Wind 这样的电影接近搜索结果的顶部，而像 Twister 和 Sharknado 这样的影片，大概在剧情摘要中都有 wind 。\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wind\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title^4\u0026#34;, \u0026#34;plot\u0026#34;] } } } 查询接受以下选项。有关每个参数的描述，请参见 查询选项\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wind\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title^4\u0026#34;, \u0026#34;description\u0026#34;], \u0026#34;type\u0026#34;: \u0026#34;most_fields\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;and\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: 3, \u0026#34;tie_breaker\u0026#34;: 0.0, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;boost\u0026#34;: 1, \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34;, \u0026#34;fuzzy_transpositions\u0026#34;: true, \u0026#34;lenient\u0026#34;: false, \u0026#34;prefix_length\u0026#34;: 0, \u0026#34;max_expansions\u0026#34;: 50, \u0026#34;auto_generate_synonyms_phrase_query\u0026#34;: true, \u0026#34;cutoff_frequency\u0026#34;: 0.01, \u0026#34;zero_terms_query\u0026#34;: \u0026#34;none\u0026#34; } } } 匹配布尔前缀 #  类似于 匹配, 创建一个 前缀查询\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_bool_prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;rises wi\u0026#34; } } } 查询接受以下选项。有关每个参数的描述，请参见 查询选项\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_bool_prefix\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;rises wi\u0026#34;, \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34;, \u0026#34;fuzzy_transpositions\u0026#34;: true, \u0026#34;max_expansions\u0026#34;: 50, \u0026#34;prefix_length\u0026#34;: 0, \u0026#34;operator\u0026#34;: \u0026#34;or\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: 2, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; } } } } 匹配短语 #  创建一个 匹配短语\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;the wind rises\u0026#34; } } } 查询接受以下选项。有关每个参数的描述，请参见 查询选项\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;wind rises the\u0026#34;, \u0026#34;slop\u0026#34;: 3, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;zero_terms_query\u0026#34;: \u0026#34;none\u0026#34; } } } } 匹配短语前缀 #  类似于 匹配短语, 创建一个 匹配短语前缀\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;the wind ri\u0026#34; } } } 查询接受以下选项。有关每个参数的描述，请参见 查询选项\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase_prefix\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the wind ri\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;max_expansions\u0026#34;: 50, \u0026#34;slop\u0026#34;: 3 } } } } 通用条件 #  公共条件查询根据分片上出现的次数将查询字符串分成高频和低频条件。低频项在结果中的权重更大，高频项仅适用于已匹配一个或多个低频项的文档。从这个意义上说，您可以将这个查询看作是一个内置的、不断变化的停止词列表。\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;common\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the wind rises\u0026#34; } } } } 查询接受以下选项。有关每个参数的描述，请参见 查询选项\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;common\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the wind rises\u0026#34;, \u0026#34;cutoff_frequency\u0026#34;: 0.002, \u0026#34;low_freq_operator\u0026#34;: \u0026#34;or\u0026#34;, \u0026#34;boost\u0026#34;: 1, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;minimum_should_match\u0026#34;: { \u0026#34;low_freq\u0026#34; : 2, \u0026#34;high_freq\u0026#34; : 3 } } } } } 查询字符串 #  查询字符串查询根据运算符拆分文本，并逐个分析.\n 如果使用 HTTP 请求参数（即 _search?q=wind ）进行搜索，Easysearch 将创建一个查询字符串查询。\n GET _search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the wind AND (rises OR rising)\u0026#34; } } } 查询接受以下选项。有关每个参数的描述，请参见 查询选项\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;the wind AND (rises OR rising)\u0026#34;, \u0026#34;default_field\u0026#34;: \u0026#34;title\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;best_fields\u0026#34;, \u0026#34;fuzziness\u0026#34;: \u0026#34;AUTO\u0026#34;, \u0026#34;fuzzy_transpositions\u0026#34;: true, \u0026#34;fuzzy_max_expansions\u0026#34;: 50, \u0026#34;fuzzy_prefix_length\u0026#34;: 0, \u0026#34;minimum_should_match\u0026#34;: 1, \u0026#34;default_operator\u0026#34;: \u0026#34;or\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;lenient\u0026#34;: false, \u0026#34;boost\u0026#34;: 1, \u0026#34;allow_leading_wildcard\u0026#34;: true, \u0026#34;enable_position_increments\u0026#34;: true, \u0026#34;phrase_slop\u0026#34;: 3, \u0026#34;max_determinized_states\u0026#34;: 10000, \u0026#34;time_zone\u0026#34;: \u0026#34;-08:00\u0026#34;, \u0026#34;quote_field_suffix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;quote_analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;analyze_wildcard\u0026#34;: false, \u0026#34;auto_generate_synonyms_phrase_query\u0026#34;: true } } } 简单查询字符串 #  简单的查询字符串查询类似于查询字符串查询，但它允许高级用户直接在查询字符串中指定许多参数。查询将丢弃查询字符串的任何无效部分。\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;simple_query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;\\\u0026#34;rises wind the\\\u0026#34;~4 | *ising~2\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title\u0026#34;] } } }    特殊字符 说明     + 与 and 操作符一样   \\| 与 or 操作符一样   * 通配符   \u0026quot;\u0026quot; 将若干个字组成一个短语   () 将一组条件组合到一起   ~n 用于单词后面 (e.g. wnid~3 ), 设置模糊程度参数 fuzziness 。当用于词语后面，则设置 slop 参数。参见 查询选项.   - Negates the term.    查询接受以下选项。有关每个参数的描述，请参见 查询选项\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;simple_query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;\\\u0026#34;rises wind the\\\u0026#34;~4 | *ising~2\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title\u0026#34;], \u0026#34;flags\u0026#34;: \u0026#34;ALL\u0026#34;, \u0026#34;fuzzy_transpositions\u0026#34;: true, \u0026#34;fuzzy_max_expansions\u0026#34;: 50, \u0026#34;fuzzy_prefix_length\u0026#34;: 0, \u0026#34;minimum_should_match\u0026#34;: 1, \u0026#34;default_operator\u0026#34;: \u0026#34;or\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;lenient\u0026#34;: false, \u0026#34;quote_field_suffix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;analyze_wildcard\u0026#34;: false, \u0026#34;auto_generate_synonyms_phrase_query\u0026#34;: true } } } 全部匹配 #  匹配所有文档。可用于测试。\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } 不匹配 #  不匹配任何文档。很少有人用。\nGET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_none\u0026#34;: {} } } 查询选项 #     选项 类型 说明     allow_leading_wildcard 布尔值 * 和 ？ 是否允许作为搜索项的第一个字符。默认值为 true   analyze_wildcard Boolean Easysearch 是否应尝试分析通配符。一些分析器在这项任务上做得很差，因此默认值为 false   analyzer standard, simple, whitespace, stop, keyword, pattern, \u0026lt;language\u0026gt;, fingerprint 要用于查询的分析器。不同的分析器具有不同的字符过滤器、标记化器和标记过滤器。例如， stop 分析器从查询字符串中删除停止词（例如 an 、 but 、 this ）。   auto_generate_synonyms_phrase_query Boolean 值 true（默认值）自动生成 短语查询多术语同义词。例如，如果您有同义词 ba, batting average 并搜索 ba ，Easysearch 将搜索 ba OR batting average（如果此选项为 true ）或 ba OR（batting 与 average）（如果该选项为 false）。   boost Floating-point 按给定乘数增加子句。用于权衡复合查询中的子句。默认值为 1.0。   cutoff_frequency Between 0.0 and 1.0 or a positive integer 该值允许您根据索引中出现的次数定义高频和低频项。介于 0 和 1 之间的数字被视为百分比。例如，0.10 为 10%。这个值意味着，如果一个单词出现在碎片上超过 10%的文档中的搜索字段中，Easysearch 会考虑单词 高频 ，并在计算搜索分数时不再强调它因为此设置是每个碎片，所以测试它对搜索结果的影响可能很困难，除非集群有很多文档。   enable_position_increments Boolean 如果为 true，结果查询将知道位置增量。当删除停止词在术语之间留下不需要的 间隙 时，此设置非常有用。默认值为 true。   fields String array 要搜索的字段列表（例如 \u0026quot;fields\u0026quot;: [\u0026quot;title^4\u0026quot;, \u0026quot;description\u0026quot;]）。如果未指定，则默认为 index.query.default_field 设置，默认为 [ \u0026quot;*\u0026quot; ]。   flags String 要启用的以 \\| 分隔的[标志]字符串（ 简单查询字符串）（例如 AND \\| OR \\| NOT ），默认值为 ALL 。   fuzziness AUTO , 0 , or a positive integer 在确定术语是否与值匹配时，将一个单词更改为另一个单词所需的字符编辑（插入、删除、替换）次数。例如， wined 和 wind 之间的距离为 1。默认值 AUTO 基于每个术语的长度选择一个值，对于大多数用例来说是一个很好的选择。   fuzzy_transpositions Boolean 将 fuzzy_transposions 设置为 true（默认值）会将相邻字符的交换添加到 fuzzy 选项的插入、删除和替换操作中。例如，如果 fuzzy_transpositions 为真（交换 n 和 i ），则 wind 和 wnid 之间的距离为 1；如果为假（删除 n ，插入 n ），那么 wind 和 wnid 之间距离为 2。如果 fuzzy_transpositions 为假，则 rewind 和 wnid 与 wind 的距离相同（2），尽管更以人为中心的观点认为 wnid 是一个明显的拼写错误。对于大多数用例来说，默认值是一个不错的选择。   lenient Boolean 将 lenient 设置为 true 可以忽略查询和文档字段之间的数据类型不匹配。例如，查询字符串 8.2 可以匹配 float 类型的字段。默认值为 false。   low_freq_operator and, or 低频项的运算符。默认值为 或 。请参阅此表中的 通用条件查询和 operator 。   max_determinized_states Positive integer Lucene 可以为包含正则表达式的查询字符串（例如 \u0026quot;query\u0026quot; ：\u0026quot;/wind.+？/\u0026quot; ）创建的 “ states”（复杂性度量）的最大数量。较大的数字允许使用更多内存的查询。默认值为 10000。   max_expansions Positive integer 模糊查询 expand 在 fuzziness 中指定的距离内的许多匹配项。然后 Easysearch 试图将这些条款与其指数相匹配 max_expansions 指定模糊查询扩展到的最大项数。默认值为 50。   minimum_should_match 正或负整数、正或负百分比、组合 如果查询字符串包含多个搜索项，并且您使用了 或 运算符，则需要匹配的项数将被视为匹配项。例如，如果 minimum_should_match 为 2，则 wind often rising 与 The Wind Rises 不匹配。如果 minium_sshould_match 为 1，则匹配。此选项还具有 通用条件查询的 low_freq 和 high_freq。   operator or, and 如果查询字符串包含多个搜索项，则是否所有项都需要匹配（ and ），或者只有一个项需要匹配（ or ），才能将文档视为匹配项。   phrase_slop 0 (default) or a positive integer See slop .   prefix_length 0 (default) or a positive integer 在模糊性中可以不考虑的主要字符数量。   quote_field_suffix String 此选项允许您根据术语是否用引号括起来来搜索不同的字段。例如，如果 quote_field_suffix 是 .exact ，并且在 title 字段中搜索 light （用引号括起来），则 Easysearch 将搜索 title.exact 字段。第二个字段可能使用不同的类型（例如 关键字 而不是 文本 ）或不同的分析器。默认值为空。   rewrite constant_score, scoring_boolean, constant_score_boolean, top_terms_N, top_terms_boost_N, top_terms_blended_freqs_N 确定 Easysearch 如何重写和评分多术语查询。默认值为 constant_score 。   slop 0 (default) or a positive integer 控制查询中的单词可能被错误排序并且仍然被视为匹配的程度。来自 Lucene 文档： 查询短语中单词之间允许的其他单词的数量。例如，要切换两个单词的顺序，需要两次移动（第一次移动将单词放在另一个单词的顶部），因此为了允许短语的重新排序，slop 必须至少为两个。值为零需要完全匹配。   tie_breaker 0.0 (default) to 1.0 改变 Easysearch 评分搜索的方式。例如， 类型 的 best_fields 通常使用任何一个字段的最高分数。如果指定介于 0.0 和 1.0 之间的 tie_breaker 值，则所有其他匹配字段的得分将更改为最高得分 + tie_bbreaker * 得分。如果指定值为 1.0，Easysearch 会将所有匹配字段的分数相加（有效地挫败了 best_fields 的目的）。   time_zone UTC offset 如果查询字符串包含日期范围（例如 \u0026quot;query\u0026quot; ： \u0026quot;wind rises release_date[2012-01-01 to 2014-01-01]\u0026quot; ），则使用的时区（例如 -08:00 ）。默认值为 UTC 。   type best_fields, most_fields, cross-fields, phrase, phrase_prefix 确定 Easysearch 如何执行查询并对结果进行评分。默认值为 best_fields 。   zero_terms_query none, all 如果分析器从查询字符串中删除所有术语，则是不匹配任何文档（默认）还是匹配所有文档。例如， stop 分析器从字符串 an but this 中删除所有项    ","subcategory":null,"summary":"","tags":null,"title":"全文查询","url":"/easysearch/v1.15.0/docs/references/search/full-text/"},{"category":null,"content":"API #  通过 REST API 可以管理用户、角色、角色映射、权限集合和租户。\nAPI 的访问控制 #  您可以控制哪些角色可以访问安全相关的 API，在配置文件 easysearch.yml:\nsecurity.restapi.roles_enabled: [\u0026#34;\u0026lt;role\u0026gt;\u0026#34;, ...] 如果希望阻止访问特定的 API：\nsecurity.restapi.endpoints_disabled.\u0026lt;role\u0026gt;.\u0026lt;endpoint\u0026gt;: [\u0026#34;\u0026lt;method\u0026gt;\u0026#34;, ...] 参数 endpoint 可以是:\n PRIVILEGE ROLE ROLE_MAPPING USER CONFIG CACHE  参数 method 可以是:\n GET PUT POST DELETE PATCH  例如，以下配置授予三个角色对 REST API 的访问权限，但随后会阻止 test-role 发送 PUT, POST, DELETE, 或 PATCH 到 _security/role 或 _security/user :\nsecurity.restapi.roles_enabled: [\u0026#34;superuser\u0026#34;, \u0026#34;security_rest_api_access\u0026#34;, \u0026#34;test-role\u0026#34;] security.restapi.endpoints_disabled.test-role.ROLE: [\u0026#34;PUT\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;PATCH\u0026#34;] security.restapi.endpoints_disabled.test-role.USER: [\u0026#34;PUT\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;PATCH\u0026#34;] 要为 API 配置 使用 PUT 和 PATCH 方法，请将以下行添加到 easysearch.yml：\nsecurity.unsupported.restapi.allow_securityconfig_modification: true 隐藏的保留资源 #  通过修改以下参数，您可以将用户、角色、角色映射和权限集合标记为保留。随后将无法使用 REST API 修改这些资源。\nkibana_user: reserved: true 同样，可以将用户、角色、角色映射和权限集合标记为隐藏。REST API 不会返回此标志设置为 true 的资源：\nkibana_user: hidden: true 隐藏的资源默认就是保留的资源。\n账号信息 #  获取账号信息 #  返回当前用户的帐户详细信息。例如，如果您是以 admin 用户身份对请求进行登录的，则响应将包含该用户的详细信息。\n请求 #  GET _security/account 示例 #  curl -XGET -k -u booksuser:password \u0026#39;https://localhost:9200/_security/account?pretty\u0026#39; { \u0026#34;username\u0026#34; : \u0026#34;booksuser\u0026#34;, \u0026#34;reserved\u0026#34; : false, \u0026#34;hidden\u0026#34; : false, \u0026#34;builtin\u0026#34; : true, \u0026#34;external_roles\u0026#34; : [ ], \u0026#34;attributes\u0026#34; : [ ], \u0026#34;roles\u0026#34; : [ \u0026#34;booksrole\u0026#34; ] } 修改密码 #  修改当前用户的密码。\n请求 #  PUT _security/account { \u0026#34;current_password\u0026#34; : \u0026#34;old-password\u0026#34;, \u0026#34;password\u0026#34; : \u0026#34;new-password\u0026#34; } 示例 #  ✗ curl-json -XPUT -k -u booksuser:password \u0026#39;https://localhost:9200/_security/account\u0026#39; -d\u0026#39;{ \u0026#34;current_password\u0026#34; : \u0026#34;password\u0026#34;, \u0026#34;password\u0026#34; : \u0026#34;admin1\u0026#34; }\u0026#39; {\u0026quot;status\u0026quot;:\u0026quot;OK\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;'booksuser' updated.\u0026quot;}% \n权限集合 #  获取权限集合 #  获取一个权限集合信息。\n请求 #  GET _security/privilege/\u0026lt;name\u0026gt; 示例 #  { \u0026#34;custom_action_group\u0026#34;: { \u0026#34;reserved\u0026#34;: false, \u0026#34;hidden\u0026#34;: false, \u0026#34;privileges\u0026#34;: [ \u0026#34;kibana_all_read\u0026#34;, \u0026#34;indices:admin/aliases/get\u0026#34;, \u0026#34;indices:admin/aliases/exists\u0026#34; ], \u0026#34;description\u0026#34;: \u0026#34;My custom action group\u0026#34;, \u0026#34;static\u0026#34;: false } } 获取权限集合列表 #  获取所有的权限集合列表。\n请求 #  GET _security/privilege/ 示例 #  { \u0026#34;read\u0026#34;: { \u0026#34;reserved\u0026#34;: true, \u0026#34;hidden\u0026#34;: false, \u0026#34;privileges\u0026#34;: [ \u0026#34;indices:data/read*\u0026#34;, \u0026#34;indices:admin/mappings/fields/get*\u0026#34; ], \u0026#34;type\u0026#34;: \u0026#34;index\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Allow all read operations\u0026#34;, \u0026#34;static\u0026#34;: true }, ... } 删除一个权限集合 #  请求 #  DELETE _security/privilege/\u0026lt;name\u0026gt; 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;privilege SEARCH deleted.\u0026#34; } 创建一个权限集合 #  创建或替换指定的限集合。\n请求 #  PUT _security/privilege/\u0026lt;name\u0026gt; { \u0026#34;privileges\u0026#34;: [ \u0026#34;indices:data/write/index*\u0026#34;, \u0026#34;indices:data/write/update*\u0026#34;, \u0026#34;indices:admin/mapping/put\u0026#34;, \u0026#34;indices:data/write/bulk*\u0026#34;, \u0026#34;read\u0026#34;, \u0026#34;write\u0026#34; ] } 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;CREATED\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;my-action-group\u0026#39; created.\u0026#34; } 修改权限集合 #  修改权限集合的属性。\n请求 #  PATCH _security/privilege/\u0026lt;name\u0026gt; [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/privileges\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;indices:admin/create\u0026#34;, \u0026#34;indices:admin/mapping/put\u0026#34;] } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;privilege SEARCH deleted.\u0026#34; } 修改一批权限集合 #  一次修改多个权限集合。\n请求 #  PATCH _security/privilege [ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/CREATE_INDEX\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;privileges\u0026#34;: [\u0026#34;indices:admin/create\u0026#34;, \u0026#34;indices:admin/mapping/put\u0026#34;] } }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/CRUD\u0026#34; } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;privilege SEARCH deleted.\u0026#34; }  用户 #  这些 API 允许您创建、更新和删除内部用户。\n获取用户 #  请求 #  GET _security/user/\u0026lt;username\u0026gt; 实例 #  { \u0026#34;admin\u0026#34;: { \u0026#34;hash\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;captains\u0026#34;, \u0026#34;starfleet\u0026#34;], \u0026#34;attributes\u0026#34;: { \u0026#34;attribute1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;attribute2\u0026#34;: \u0026#34;value2\u0026#34; } } } 获取用户列表 #  请求 #  GET _security/user/ 示例 #  { \u0026#34;admin\u0026#34;: { \u0026#34;hash\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;captains\u0026#34;, \u0026#34;starfleet\u0026#34;], \u0026#34;attributes\u0026#34;: { \u0026#34;attribute1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;attribute2\u0026#34;: \u0026#34;value2\u0026#34; } } } 删除用户 #  请求 #  DELETE _security/user/\u0026lt;username\u0026gt; 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;user admin deleted.\u0026#34; } 创建用户 #  创建或替换指定的用户。您必须指定 password (明文)或 hash (哈希后的密码)。如果您指定 password,安全模块会在存储密码之前自动对密码进行哈希处理。\n请注意，您在 roles 数组中提供的任何角色都必须已经存在，安全模块才能将用户映射到该角色。要查看预定义的角色，请参阅 这里。有关如何创建角色的说明，请参阅 创建角色。\n请求 #  PUT _security/user/\u0026lt;username\u0026gt; { \u0026#34;password\u0026#34;: \u0026#34;adminpass\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;maintenance_staff\u0026#34;, \u0026#34;weapons\u0026#34;], \u0026#34;external_roles\u0026#34;: [\u0026#34;captains\u0026#34;, \u0026#34;starfleet\u0026#34;], \u0026#34;attributes\u0026#34;: { \u0026#34;attribute1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;attribute2\u0026#34;: \u0026#34;value2\u0026#34; } } 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;CREATED\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;User admin created\u0026#34; } 修改用户 #  修改用户信息。\nRequest #  PATCH _security/user/\u0026lt;username\u0026gt; [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/external_roles\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;klingons\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/roles\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;ship_manager\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/attributes\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;newattribute\u0026#34;: \u0026#34;newvalue\u0026#34; } } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;admin\u0026#39; updated.\u0026#34; } 修改一批用户 #  一次修改一批用户的操作。\n请求 #  PATCH _security/user [ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spock\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;testpassword1\u0026#34;, \u0026#34;external_roles\u0026#34;: [\u0026#34;testrole1\u0026#34;] } }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/worf\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;testpassword2\u0026#34;, \u0026#34;external_roles\u0026#34;: [\u0026#34;testrole2\u0026#34;] } }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/riker\u0026#34; } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Resource updated.\u0026#34; }  角色 #  获取角色 #  获取一个角色信息。\n请求 #  GET _security/role/\u0026lt;role\u0026gt; 示例 #  { \u0026#34;test-role\u0026#34;: { \u0026#34;reserved\u0026#34;: false, \u0026#34;hidden\u0026#34;: false, \u0026#34;cluster\u0026#34;: [\u0026#34;cluster_composite_ops\u0026#34;, \u0026#34;indices_monitor\u0026#34;], \u0026#34;indices\u0026#34;: [ { \u0026#34;names\u0026#34;: [\u0026#34;movies*\u0026#34;], \u0026#34;query\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;field_security\u0026#34;: [], \u0026#34;field_mask\u0026#34;: [], \u0026#34;privileges\u0026#34;: [\u0026#34;read\u0026#34;] } ], \u0026#34;static\u0026#34;: false } } 获取角色列表 #  请求 #  GET _security/role/ 示例 #  { \u0026#34;manage_snapshots\u0026#34;: { \u0026#34;reserved\u0026#34;: true, \u0026#34;hidden\u0026#34;: false, \u0026#34;description\u0026#34;: \u0026#34;Provide the minimum permissions for managing snapshots\u0026#34;, \u0026#34;cluster\u0026#34;: [ \u0026#34;manage_snapshots\u0026#34; ], \u0026#34;indices\u0026#34;: [{ \u0026#34;names\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;field_security\u0026#34;: [], \u0026#34;field_mask\u0026#34;: [], \u0026#34;privileges\u0026#34;: [ \u0026#34;indices:data/write/index\u0026#34;, \u0026#34;indices:admin/create\u0026#34; ] }], \u0026#34;static\u0026#34;: true }, ... } 删除一个角色 #  请求 #  DELETE _security/role/\u0026lt;role\u0026gt; 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;role test-role deleted.\u0026#34; } 创建一个角色 #  请求 #  PUT _security/role/\u0026lt;role\u0026gt; { \u0026#34;cluster\u0026#34;: [ \u0026#34;cluster_composite_ops\u0026#34;, \u0026#34;indices_monitor\u0026#34; ], \u0026#34;indices\u0026#34;: [{ \u0026#34;names\u0026#34;: [ \u0026#34;movies*\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;field_security\u0026#34;: [], \u0026#34;field_mask\u0026#34;: [], \u0026#34;privileges\u0026#34;: [ \u0026#34;read\u0026#34; ] }] } 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;test-role\u0026#39; updated.\u0026#34; } 修改角色 #  修改角色的属性。\n请求 #  PATCH _security/role/\u0026lt;role\u0026gt; [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/indices/0/field_security\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;myfield1\u0026#34;, \u0026#34;myfield2\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/indices/0/query\u0026#34; } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;\u0026lt;role\u0026gt;\u0026#39; updated.\u0026#34; } 批量修改角色 #  一次操作批量修改角色信息。\n请求 #  PATCH _security/role [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/role1/indices/0/field_security\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;test1\u0026#34;, \u0026#34;test2\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/role1/indices/0/query\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/role2/cluster\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;manage_snapshots\u0026#34;] } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Resource updated.\u0026#34; }  角色映射 #  获取一个角色映射 #  获取一个角色映射信息。\n请求 #  GET _security/role_mapping/\u0026lt;role\u0026gt; 示例 #  { \u0026#34;role_starfleet\u0026#34;: { \u0026#34;external_roles\u0026#34;: [ \u0026#34;starfleet\u0026#34;, \u0026#34;captains\u0026#34;, \u0026#34;defectors\u0026#34;, \u0026#34;cn=ldaprole,ou=groups,dc=example,dc=com\u0026#34; ], \u0026#34;hosts\u0026#34;: [\u0026#34;*.starfleetintranet.com\u0026#34;], \u0026#34;users\u0026#34;: [\u0026#34;worf\u0026#34;] } } 获取角色映射列表。 #  获取所有的角色映射列表。\n请求 #  GET _security/role_mapping 示例 #  { \u0026#34;role_starfleet\u0026#34;: { \u0026#34;external_roles\u0026#34;: [ \u0026#34;starfleet\u0026#34;, \u0026#34;captains\u0026#34;, \u0026#34;defectors\u0026#34;, \u0026#34;cn=ldaprole,ou=groups,dc=example,dc=com\u0026#34; ], \u0026#34;hosts\u0026#34;: [\u0026#34;*.starfleetintranet.com\u0026#34;], \u0026#34;users\u0026#34;: [\u0026#34;worf\u0026#34;] } } 删除角色映射 #  请求 #  DELETE _security/role_mapping/\u0026lt;role\u0026gt; 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;my-role\u0026#39; deleted.\u0026#34; } 创建角色映射 #  请求 #  PUT _security/role_mapping/\u0026lt;role\u0026gt; { \u0026#34;external_roles\u0026#34; : [ \u0026#34;starfleet\u0026#34;, \u0026#34;captains\u0026#34;, \u0026#34;defectors\u0026#34;, \u0026#34;cn=ldaprole,ou=groups,dc=example,dc=com\u0026#34; ], \u0026#34;hosts\u0026#34; : [ \u0026#34;*.starfleetintranet.com\u0026#34; ], \u0026#34;users\u0026#34; : [ \u0026#34;worf\u0026#34; ] } 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;CREATED\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;my-role\u0026#39; created.\u0026#34; } 修改角色映射 #  请求 #  PATCH _security/role_mapping/\u0026lt;role\u0026gt; [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/users\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;myuser\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/external_roles\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;mybackendrole\u0026#34;] } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#39;my-role\u0026#39; updated.\u0026#34; } 批量修改角色映射 #  请求 #  PATCH _security/role_mapping [ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/human_resources\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;users\u0026#34;: [\u0026#34;user1\u0026#34;], \u0026#34;external_roles\u0026#34;: [\u0026#34;backendrole2\u0026#34;] } }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/finance\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;users\u0026#34;: [\u0026#34;user2\u0026#34;], \u0026#34;external_roles\u0026#34;: [\u0026#34;backendrole2\u0026#34;] } } ] 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Resource updated.\u0026#34; } 缓存 #  刷新缓存 #  请求 #  DELETE _security/cache 示例 #  { \u0026#34;status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Cache flushed successfully.\u0026#34; } admin #  修改admin密码 #  进入Easysearch的config路径下，通过证书的方式进行密码修改，具体可查看 这里。\n示例 #  cd easysearch/config #执行 admin 密码修改 curl -k -XPUT --cert admin.crt --key admin.key -H \u0026#39;Content-Type: application/json\u0026#39; \u0026#39;https://localhost:9200/_security/user/admin\u0026#39; -d \u0026#39; { \u0026#34;password\u0026#34;: \u0026#34;C0mp1exP@ezs\u0026#34;, \u0026#34;external_roles\u0026#34;: [\u0026#34;admin\u0026#34;] }\u0026#39; #用新密码验证是否修改成功 curl -ku 'admin:C0mp1exP@ezs' https://localhost:9200 \n","subcategory":null,"summary":"","tags":null,"title":"API 接口","url":"/easysearch/v1.15.0/docs/references/security/access-control/api/"},{"category":null,"content":"Easysearch Java API Client 使用文档 #  管理索引 #  使用客户端对索引进行管理\nString index = \u0026#34;test1\u0026#34;; if (client.indices().exists(r -\u0026gt; r.index(index)).value()) { LOGGER.info(\u0026#34;Deleting index \u0026#34; + index); DeleteIndexResponse deleteIndexResponse = client.indices().delete(new DeleteIndexRequest.Builder().index(index).build()); LOGGER.info(deleteIndexResponse.toString()); } LOGGER.info(\u0026quot;Creating index \u0026quot; + index); CreateIndexResponse createIndexResponse = client.indices().create(req -\u0026gt; req.index(index)); CloseIndexResponse closeIndexResponse = client.indices().close(req -\u0026gt; req.index(index)); OpenResponse openResponse = client.indices().open(req -\u0026gt; req.index(index)); RefreshResponse refreshResponse = client.indices().refresh(req -\u0026gt; req.index(index)); FlushResponse flushResponse = client.indices().flush(req -\u0026gt; req.index(index)); ForcemergeResponse forcemergeResponse = client.indices().forcemerge(req -\u0026gt; req.index(index).maxNumSegments(1L)); 也可以用异步方式执行\nEasysearchAsyncClient asyncClient = SampleClient.createAsyncClient(); asyncClient.indices().exists(req -\u0026gt; req.index(index)).thenCompose(exists -\u0026gt; { if (exists.value()) { LOGGER.info(\u0026#34;Deleting index \u0026#34; + index); return asyncClient.indices().delete(r -\u0026gt; r.index(index)).thenAccept(deleteResponse -\u0026gt; { LOGGER.info(deleteResponse); }); } return CompletableFuture.completedFuture(null); }).thenCompose(v -\u0026gt; { LOGGER.info(\u0026#34;Creating index \u0026#34; + index); return asyncClient.indices().create(req -\u0026gt; req.index(index));  \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;whenComplete\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;((\u0026lt;/span\u0026gt;createResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; throwable\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;throwable \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;!=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;null\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; LOGGER\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;error\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Error during index operations\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; throwable\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;else\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; LOGGER\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Index created successfully\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;})\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;get\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;30\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; TimeUnit\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;SECONDS\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt;  滚动索引 Rollover #\n 滚动索引是一种管理时序数据的有效方式，当索引满足特定条件时（如大小、文档数量、年龄），会自动创建新的索引。\n示例\n// 创建初始索引并设置别名 String index = \u0026#34;test-00001\u0026#34;; client.indices().create(req -\u0026gt; req.index(index).aliases(\u0026#34;test_log\u0026#34;, a -\u0026gt; a.isWriteIndex(true))); // 配置并执行滚动  RolloverResponse res = client.indices().rollover(req -\u0026gt; req .alias(\u0026quot;test_log\u0026quot;) // 指定别名  .conditions(c -\u0026gt; c .maxDocs(100L) // 文档数量达到100时滚动  .maxAge(b -\u0026gt; b.time(\u0026quot;7d\u0026quot;)) // 索引时间达到7天时滚动  .maxSize(\u0026quot;5gb\u0026quot;))); // 索引大小达到5GB时滚动 Mapping 设置 #\n 基本设置\nString index = \u0026#34;test1\u0026#34;; PutMappingResponse response = client.indices().putMapping(req -\u0026gt; req.index(index) .properties(\u0026#34;field1\u0026#34;, p -\u0026gt; p.keyword(k -\u0026gt; k)) // keyword类型  .properties(\u0026#34;field2\u0026#34;, p -\u0026gt; p.text(t -\u0026gt; t)) // text类型 ); 更完整的示例，包含多种常用字段类型\nresponse = client.indices().putMapping(req -\u0026gt; req.index(index) // 常用字段类型示例  .properties(\u0026#34;keyword_field\u0026#34;, p -\u0026gt; p.keyword(k -\u0026gt; k)) // keyword 类型  .properties(\u0026#34;text_field\u0026#34;, p -\u0026gt; p.text(t -\u0026gt; t)) // text 类型  .properties(\u0026#34;long_field\u0026#34;, p -\u0026gt; p.long_(l -\u0026gt; l)) // long 类型  .properties(\u0026#34;integer_field\u0026#34;, p -\u0026gt; p.integer(i -\u0026gt; i)) // integer 类型  .properties(\u0026#34;short_field\u0026#34;, p -\u0026gt; p.short_(s -\u0026gt; s)) // short 类型  .properties(\u0026#34;byte_field\u0026#34;, p -\u0026gt; p.byte_(b -\u0026gt; b)) // byte 类型  .properties(\u0026#34;double_field\u0026#34;, p -\u0026gt; p.double_(d -\u0026gt; d)) // double 类型  .properties(\u0026#34;float_field\u0026#34;, p -\u0026gt; p.float_(f -\u0026gt; f)) // float 类型  .properties(\u0026#34;date_field\u0026#34;, p -\u0026gt; p.date(d -\u0026gt; d)) // date 类型  .properties(\u0026#34;boolean_field\u0026#34;, p -\u0026gt; p.boolean_(b -\u0026gt; b)) // boolean 类型  .properties(\u0026#34;binary_field\u0026#34;, p -\u0026gt; p.binary(b -\u0026gt; b)) // binary 类型  .properties(\u0026#34;ip_field\u0026#34;, p -\u0026gt; p.ip(i -\u0026gt; i)) // ip 类型  .properties(\u0026#34;geo_point_field\u0026#34;, p -\u0026gt; p.geoPoint(g -\u0026gt; g)) // geo_point 类型  .properties(\u0026#34;flat_field\u0026#34;, p -\u0026gt; p.flattened(f -\u0026gt; f)) // flattened 类型  \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// date 类型   .properties(\u0026quot;date_field\u0026quot;, p -\u0026gt; p.date(d -\u0026gt; d.format(\u0026quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026quot;)))\n\u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// object 类型   .properties(\u0026quot;object_field\u0026quot;, p -\u0026gt; p.object(o -\u0026gt; o .properties(\u0026quot;sub_field1\u0026quot;, sp -\u0026gt; sp.keyword(k -\u0026gt; k)) .properties(\u0026quot;sub_field2\u0026quot;, sp -\u0026gt; sp.long_(l -\u0026gt; l)) ) )\n\u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// nested 类型   .properties(\u0026quot;nested_field\u0026quot;, p -\u0026gt; p.nested(n -\u0026gt; n .properties(\u0026quot;sub_field1\u0026quot;, sp -\u0026gt; sp.keyword(k -\u0026gt; k)) .properties(\u0026quot;sub_field2\u0026quot;, sp -\u0026gt; sp.text(t -\u0026gt; t)) ) ) ); 常用字段类型配置\nresponse = client.indices().putMapping(req -\u0026gt; req.index(index) // text 类型配置  .properties(\u0026#34;text_field2\u0026#34;, p -\u0026gt; p .text(t -\u0026gt; t .analyzer(\u0026#34;standard\u0026#34;) // 分析器  .searchAnalyzer(\u0026#34;standard\u0026#34;) // 搜索分析器  .fields(\u0026#34;raw\u0026#34;, f -\u0026gt; f.keyword(k -\u0026gt; k)) // 添加keyword子字段  .copyTo(\u0026#34;other_field\u0026#34;) // 复制到其他字段  ) )  \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// keyword 类型配置   .properties(\u0026quot;keyword_field2\u0026quot;, p -\u0026gt; p .keyword(k -\u0026gt; k .ignoreAbove(256) // 忽略超过长度的值  .nullValue(\u0026quot;NULL\u0026quot;) // null值替换  .docValues(true) // 是否开启doc_values  ) )\n \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// date 类型配置   .properties(\u0026quot;date_field2\u0026quot;, p -\u0026gt; p .date(d -\u0026gt; d .format(\u0026quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026quot;) // 日期格式  ) ) ); 更新索引 Settings #\n 可以用JSON 的方式更新 特定索引的设置\nString settings = \u0026#34;{\u0026#34; + \u0026#34; \\\u0026#34;index\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;number_of_replicas\\\u0026#34;: 2,\u0026#34; + \u0026#34; \\\u0026#34;refresh_interval\\\u0026#34;: \\\u0026#34;5s\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;max_result_window\\\u0026#34;: 50000,\u0026#34; + \u0026#34; \\\u0026#34;analysis\\\u0026#34;: {\\\u0026#34; + \u0026#34; \\\u0026#34;analyzer\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;my_analyzer\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;custom\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;tokenizer\\\u0026#34;: \\\u0026#34;standard\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;filter\\\u0026#34;: [\\\u0026#34;lowercase\\\u0026#34;, \\\u0026#34;asciifolding\\\u0026#34;]\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34;}\u0026#34;; // 如果要更新静态设置（比如分词器），需要先关闭索引 String index = \u0026quot;test1\u0026quot;; client.indices().close(c -\u0026gt; c.index(index)); // 更新设置 client.indices().putSettings(req -\u0026gt; req .index(index) .withJson(new StringReader(settings)) ); // 重新打开索引 client.indices().open(c -\u0026gt; c.index(index)); 创建带有自定义映射和设置的索引 #\n 使用 Easysearch Java 客户端创建带有自定义映射和设置的索引。\n使用 JSON 配置\nString settingsJson = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;asciifolding\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34; }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;tags\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;create_time\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; }, \u0026#34;update_time\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; }, \u0026#34;view_count\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } \u0026#34;\u0026#34;\u0026#34;; // 创建索引 client.indices().create(req -\u0026gt; req .index(\u0026quot;my_index\u0026quot;) .withJson(new StringReader(settingsJson)) );\n创建索引模板 #\n 使用 Easysearch Java 客户端创建索引模板。\nString templateJson = \u0026#34;{\u0026#34; + \u0026#34; \\\u0026#34;index_patterns\\\u0026#34;: [\\\u0026#34;log-*\\\u0026#34;], \u0026#34; + \u0026#34; \\\u0026#34;template\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;settings\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;number_of_shards\\\u0026#34;: 1,\u0026#34; + \u0026#34; \\\u0026#34;number_of_replicas\\\u0026#34;: 1,\u0026#34; + \u0026#34; \\\u0026#34;refresh_interval\\\u0026#34;: \\\u0026#34;5s\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;analysis\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;analyzer\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;my_analyzer\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;custom\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;tokenizer\\\u0026#34;: \\\u0026#34;standard\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;filter\\\u0026#34;: [\\\u0026#34;lowercase\\\u0026#34;]\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;mappings\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;_source\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;enabled\\\u0026#34;: true\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;properties\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;@timestamp\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;date\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;message\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;text\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;analyzer\\\u0026#34;: \\\u0026#34;my_analyzer\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;level\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;keyword\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;service\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;keyword\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;trace_id\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;keyword\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;metrics\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;object\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;properties\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;value\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;double\\\u0026#34;\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;name\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;keyword\\\u0026#34;\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34; },\u0026#34; + \u0026#34; \\\u0026#34;priority\\\u0026#34;: 100\u0026#34; + \u0026#34;}\u0026#34;; // 创建或更新模板 PutIndexTemplateRequest request = PutIndexTemplateRequest.of(builder -\u0026gt; builder .name(\u0026quot;logs-template\u0026quot;) // 模板名称  .withJson(new StringReader(templateJson)) );\nclient.indices().putIndexTemplate(request); Bulk 批量写入 #\n EasysearchClient client = SampleClient.create(); String json2 = \u0026#34;{\u0026#34; + \u0026#34; \\\u0026#34;@timestamp\\\u0026#34;: \\\u0026#34;2023-01-08T22:50:13.059Z\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;agent\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;version\\\u0026#34;: \\\u0026#34;7.3.2\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;filebeat\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;ephemeral_id\\\u0026#34;: \\\u0026#34;3ff1f2c8-1f7f-48c2-b560-4272591b8578\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;hostname\\\u0026#34;: \\\u0026#34;ba-0226-msa-fbl-747db69c8d-ngff6\\\u0026#34;\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34;}\u0026#34;; BulkRequest.Builder br = new BulkRequest.Builder(); for (int i = 0; i \u0026lt; 10; i++) { br.operations(op -\u0026gt; op.index(idx -\u0026gt; idx.index(indexName).document(JsonData.fromJson(json2)))); } BulkResponse bulkResponse = client.bulk(br.build()); if (bulkResponse.errors()) { for (BulkResponseItem item : bulkResponse.items()) { System.out.println(item.toString()); } } 索引单个文档 #\n String json2 = \u0026#34;{\u0026#34; + \u0026#34; \\\u0026#34;@timestamp\\\u0026#34;: \\\u0026#34;2023-01-08T22:50:13.059Z\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;agent\\\u0026#34;: {\u0026#34; + \u0026#34; \\\u0026#34;version\\\u0026#34;: \\\u0026#34;7.3.2\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;type\\\u0026#34;: \\\u0026#34;filebeat\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;ephemeral_id\\\u0026#34;: \\\u0026#34;3ff1f2c8-1f7f-48c2-b560-4272591b8578\\\u0026#34;,\u0026#34; + \u0026#34; \\\u0026#34;hostname\\\u0026#34;: \\\u0026#34;ba-0226-msa-fbl-747db69c8d-ngff6\\\u0026#34;\u0026#34; + \u0026#34; }\u0026#34; + \u0026#34;}\u0026#34;; IndexRequest\u0026lt;JsonData\u0026gt; request = IndexRequest.of(i -\u0026gt; i .index(\u0026quot;logs\u0026quot;) .withJson(new StringReader(json2)) ); IndexResponse response = client.index(request); System.out.println(response);\n// 也可以这样 LogEntry logEntry = mapper.readValue(json2, LogEntry.class); IndexRequest\u0026lt;LogEntry\u0026gt; request2 = IndexRequest.of(i -\u0026gt; i .index(indexName) .id(logEntry.getAgent().getEphemeralId()) .document(logEntry) ); IndexResponse response2 = client.index(request2);\n// 或者这样 IndexRequest.Builder\u0026lt;LogEntry\u0026gt; indexReqBuilder = new IndexRequest.Builder\u0026lt;\u0026gt;(); indexReqBuilder.index(indexName); indexReqBuilder.id(logEntry.getAgent().getEphemeralId()); indexReqBuilder.document(logEntry); response2 = client.index(indexReqBuilder.build()); 删除文档 #\n DeleteRequest deleteRequest = new DeleteRequest.Builder() .index(indexName) .id(\u0026#34;3ff1f2c8-1f7f-48c2-b560-4272591b8578\u0026#34;) .build(); DeleteResponse response = client.delete(deleteRequest); deleteByQuery 删除 #  DeleteByQueryRequest deleteByQueryRequest = new DeleteByQueryRequest.Builder() .index(indexName) .query(q -\u0026gt; q.match(new MatchQuery.Builder() .field(\u0026#34;agent.type\u0026#34;).query(\u0026#34;filebeat\u0026#34;).build()) ).build(); DeleteByQueryResponse response = client.deleteByQuery(deleteByQueryRequest); 更新文档 #  UpdateRequest updateRequest = UpdateRequest.of(u -\u0026gt; u .index(indexName) .id(\u0026#34;3ff1f2c8-1f7f-48c2-b560-4272591b8578\u0026#34;) .doc(Map.of(\u0026#34;agent.type\u0026#34;, \u0026#34;logstash\u0026#34;)) ); UpdateResponse\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; response = client.update(updateRequest, Map.class); updateByQuery 更新 #\n Query query = Query.of(q -\u0026gt; q .term(t -\u0026gt; t .field(\u0026#34;agent.type\u0026#34;) .value(v -\u0026gt; v.stringValue(\u0026#34;filebeat\u0026#34;)) ) ); UpdateByQueryRequest updateByQueryRequest = UpdateByQueryRequest.of(u -\u0026gt; u .index(indexName).query(query).script(s -\u0026gt; s.inline(in -\u0026gt; in.source(\u0026#34;ctx._source.agent.type = params.param1\u0026#34;) .lang(\u0026#34;painless\u0026#34;) .params(Map.of(\u0026#34;param1\u0026#34;, JsonData.of(\u0026#34;logstash\u0026#34;))))).refresh(true) ); UpdateByQueryResponse response = client.updateByQuery(updateByQueryRequest); System.out.println(response.updated()); 搜索文档 #\n Query query = Query.of(q -\u0026gt; q .term(t -\u0026gt; t .field(\u0026#34;agent.type\u0026#34;) .value(v -\u0026gt; v.stringValue(\u0026#34;filebeat\u0026#34;)) ) ); SortOptions.Builder sb = new SortOptions.Builder(); SortOptions sortOptions = sb.field(fs -\u0026gt; fs.field(\u0026quot;@timestamp\u0026quot;).order(SortOrder.Desc)).build();\nfinal SearchRequest.Builder searchReq = new SearchRequest.Builder().allowPartialSearchResults(false) .index(indexName) .size(10) .sort(sortOptions) .source(sc -\u0026gt; sc.fetch(true)) .trackTotalHits(tr -\u0026gt; tr.enabled(true)) .query(query);\nSearchResponse\u0026lt;LogEntry\u0026gt; searchResponse = client.search(searchReq.build(), LogEntry.class); System.out.println(searchResponse.hits().total()); for (Hit\u0026lt;LogEntry\u0026gt; hit : searchResponse.hits().hits()) { System.out.println(JsonData.of(hit.source()).toJson(new JacksonJsonpMapper())); } 带子聚合的日期直方图聚合 #\n SearchRequest searchRequest = SearchRequest.of(s -\u0026gt; s .index(index) .size(0) // 不需要返回文档，只要聚合结果  .aggregations(\u0026#34;by_date\u0026#34;, a -\u0026gt; a .dateHistogram(dh -\u0026gt; dh .field(\u0026#34;create_time\u0026#34;) .calendarInterval(CalendarInterval.Month) // 按月聚合数据  .format(\u0026#34;yyyy-MM-dd\u0026#34;) .minDocCount(1) ).aggregations(\u0026#34;avg_price\u0026#34;, avg -\u0026gt; avg // 计算每月的平均价格  .avg(a1 -\u0026gt; a1 .field(\u0026#34;price\u0026#34;) ) ).aggregations(\u0026#34;avg_view_count\u0026#34;, avg -\u0026gt; avg // 计算每月的平均浏览次数  .avg(a1 -\u0026gt; a1 .field(\u0026#34;view_count\u0026#34;) ) ).aggregations(\u0026#34;by_status\u0026#34;, terms -\u0026gt; terms // 统计每月不同状态的文档数量  .terms(t -\u0026gt; t .field(\u0026#34;status\u0026#34;) .size(10) ) ).aggregations(\u0026#34;price_stats\u0026#34;, stats -\u0026gt; stats // 计算价格的统计信息（最小值、最大值、平均值、总和）  .stats(s1 -\u0026gt; s1 .field(\u0026#34;price\u0026#34;) ) ) )); SearchResponse\u0026lt;Void\u0026gt; response = client.search(searchRequest, Void.class); // 处理聚合结果 response.aggregations() .get(\u0026quot;by_date\u0026quot;) .dateHistogram() .buckets() .array() .forEach(bucket -\u0026gt; { // 基本信息  System.out.printf(\u0026quot;\\n日期: %s (文档数: %d)\\n\u0026quot;, bucket.keyAsString(), bucket.docCount());\n \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// 平均值   System.out.printf(\u0026quot;平均价格: %.2f\\n\u0026quot;, bucket.aggregations().get(\u0026quot;avg_price\u0026quot;).avg().value()); System.out.printf(\u0026quot;平均浏览: %.2f\\n\u0026quot;, bucket.aggregations().get(\u0026quot;avg_view_count\u0026quot;).avg().value());\n \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// 价格统计   StatsAggregate stats = bucket.aggregations().get(\u0026quot;price_stats\u0026quot;).stats(); System.out.printf(\u0026quot;价格统计: 最小值=%.2f, 最大值=%.2f, 平均值=%.2f\\n\u0026quot;, stats.min(), stats.max(), stats.avg());\n \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// 状态分布   System.out.println(\u0026quot;状态分布:\u0026quot;); bucket.aggregations() .get(\u0026quot;by_status\u0026quot;) .sterms() .buckets() .array() .forEach(status -\u0026gt; System.out.printf(\u0026quot; %s: %d\\n\u0026quot;, status.key().stringValue(), status.docCount()));\n System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;----------------------------------------\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;});\u0026lt;/span\u0026gt;  Reindex #\n ReindexResponse response = client.reindex(r -\u0026gt; r .source(s -\u0026gt; s.index(\u0026#34;test1\u0026#34;)) .dest(d -\u0026gt; d.index(\u0026#34;test1_new_index\u0026#34;)) .script(sc -\u0026gt; sc .inline(i -\u0026gt; i .source( \u0026#34;if (ctx._source.price != null) { \u0026#34; + \u0026#34; ctx._source.price *= 1.1; \u0026#34; + // 价格上调10%  \u0026#34; ctx._source.updated = true; \u0026#34; + \u0026#34;}\u0026#34; ) .lang(\u0026#34;painless\u0026#34;) ) ) ); 异步方式 Reindex #  ReindexResponse response = client.reindex(r -\u0026gt; r .source(s -\u0026gt; s.index(\u0026#34;test1\u0026#34;)) .dest(d -\u0026gt; d.index(\u0026#34;test1_new_index\u0026#34;)) .script(sc -\u0026gt; sc .inline(i -\u0026gt; i .source( \u0026#34;if (ctx._source.price != null) { \u0026#34; + \u0026#34; ctx._source.price *= 1.1; \u0026#34; + // 价格上调10%  \u0026#34; ctx._source.updated = true; \u0026#34; + \u0026#34;}\u0026#34; ) .lang(\u0026#34;painless\u0026#34;) ) ).waitForCompletion(false) ); String taskId = response.task(); System.out.println(\u0026#34;Started reindex task: \u0026#34; + taskId); // 监控任务进度  boolean completed = false; while (!completed) { try { Thread.sleep(1000); // 每1秒检查一次  GetTasksResponse taskResponse = client.tasks().get(g -\u0026gt; g.taskId(taskId).waitForCompletion(false)); Info taskInfo = taskResponse.task(); if (taskInfo == null) { // 任务可能已完成  System.out.println(\u0026#34;Task completed or not found\u0026#34;); break; }  \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// 获取任务状态   JsonData status = taskInfo.status(); if (status != null) { System.out.println(\u0026quot;Running time in millis: \u0026quot; + taskInfo.runningTimeInNanos() / 1_000_000L); System.out.println(\u0026quot;Current status: \u0026quot; + status.toJson()); }\n \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;taskResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;completed\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; completed \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Reindex completed successfully\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// 获取结果   JsonData result = taskInfo.status(); if (result != null) { // 解析状态信息中的统计数据  String resultStr = result.toJson().toString(); System.out.println(\u0026quot;Final result: \u0026quot; + resultStr); } }\n \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;catch\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Exception e\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; e\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;printStackTrace\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;break\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt;  附测试用例 #\n public class APITest { EasysearchClient client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; ObjectMapper mapper \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; ObjectMapper\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; String indexName \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;test1\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;try\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; client \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; SampleClient\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;create\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;catch\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Exception e\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;throw\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; RuntimeException\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;e\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// 创建索引并通过 JSON 设置 mappings 和 settings   @Test public void testCreateIndexByJSON() throws IOException { String index = \u0026quot;test1\u0026quot;; ESVersionInfo version = client.info().version(); LOGGER.info(\u0026quot;Server: \u0026quot; + version.number()); if (client.indices().exists(r -\u0026gt; r.index(index)).value()) { LOGGER.info(\u0026quot;Deleting index \u0026quot; + index); DeleteIndexResponse deleteIndexResponse = client.indices().delete(new DeleteIndexRequest.Builder().index(index).build()); LOGGER.info(deleteIndexResponse.toString());\n LOGGER\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Creating index \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; index\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// Settings and mappings in JSON format   String settingsJson = \u0026quot;{\\n\u0026quot; + \u0026quot; \u0026amp;#34;settings\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;number_of_shards\u0026amp;#34;: 1,\\n\u0026quot; + \u0026quot; \u0026amp;#34;number_of_replicas\u0026amp;#34;: 1,\\n\u0026quot; + \u0026quot; \u0026amp;#34;analysis\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;analyzer\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;my_analyzer\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;custom\u0026amp;#34;,\\n\u0026quot; + \u0026quot; \u0026amp;#34;tokenizer\u0026amp;#34;: \u0026amp;#34;standard\u0026amp;#34;,\\n\u0026quot; + \u0026quot; \u0026amp;#34;filter\u0026amp;#34;: [\u0026amp;#34;lowercase\u0026amp;#34;, \u0026amp;#34;asciifolding\u0026amp;#34;]\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;mappings\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;properties\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;id\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;keyword\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;title\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;text\u0026amp;#34;,\\n\u0026quot; + \u0026quot; \u0026amp;#34;analyzer\u0026amp;#34;: \u0026amp;#34;my_analyzer\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;content\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;text\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;status\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;keyword\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;tags\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;keyword\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;create_time\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;date\u0026amp;#34;,\\n\u0026quot; + \u0026quot; \u0026amp;#34;format\u0026amp;#34;: \u0026amp;#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;update_time\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;date\u0026amp;#34;,\\n\u0026quot; + \u0026quot; \u0026amp;#34;format\u0026amp;#34;: \u0026amp;#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;view_count\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;long\u0026amp;#34;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \u0026amp;#34;price\u0026amp;#34;: {\\n\u0026quot; + \u0026quot; \u0026amp;#34;type\u0026amp;#34;: \u0026amp;#34;double\u0026amp;#34;\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; }\\n\u0026quot; + \u0026quot; }\u0026quot;;\n \u0026lt;span style=\u0026quot;color:#75715e\u0026quot;\u0026gt;// Create index using JSON string   CreateIndexResponse createIndexResponse = client.indices().create(req -\u0026gt; req .index(index) .withJson(new StringReader(settingsJson)) );\n LOGGER\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;createIndexResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;@Test\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;testCatHealth\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; ESVersionInfo version \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Server: \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; version\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;number\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; HealthResponse res \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;cat\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;health\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;res\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;@Test\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;testCatIndices\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; ESVersionInfo version \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Server: \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; version\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;number\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; IndicesResponse res \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;cat\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;indices\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;res\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;@Test\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;testCatNodes\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; ESVersionInfo version \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;info\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Server: \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; version\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;number\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; NodesResponse res \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;cat\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;nodes\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;res\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;toString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;@Test\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;testScrollQuery\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; SortOptions\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; sb \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SortOptions\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; SortOptions sortOptions \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; sb\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;fs \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; fs\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;@timestamp\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;order\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;SortOrder\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Desc\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; Time time \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Time\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;t \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; t\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;time\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;1m\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; searchReq \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;allowPartialSearchResults\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;false\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Collections\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;singletonList\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;indexName\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;size\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;10\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;sort\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sortOptions\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;source\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sc \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; sc\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;fetch\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;trackTotalHits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;tr \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; tr\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;enabled\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;scroll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;time\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;query\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;q \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; q\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;matchAll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; MatchAllQuery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; SearchResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; searchResponse \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;search\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchReq\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(),\u0026lt;/span\u0026gt; LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;class\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; String scrollId \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;null\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; scrollId \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; searchResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;scrollId\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;total\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Hit\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; hit \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; searchResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;_id \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; hit\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;while\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; String finalScrollId \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; scrollId\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; ScrollResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; scrollResponse \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;scroll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sc \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; sc\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;scrollId\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;finalScrollId\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;scroll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;time\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;),\u0026lt;/span\u0026gt; LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;class\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;scrollResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;isEmpty\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;break\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Hit\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; hit \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; scrollResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;_id \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; hit\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; scrollId \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; scrollResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;scrollId\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;scrollId \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;!=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;null\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;clear \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; scrollId\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; String finalScrollId \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; scrollId\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;;\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;clearScroll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;cs \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; cs\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;scrollId\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;finalScrollId\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;@Test\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;testQuery\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; MatchQuery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; mb \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; MatchQuery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; mb\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;agent\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;query\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;xxx\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; Query query \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Query\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;qb \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; qb\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;match\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;mb\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; Instant now \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Instant\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;now\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; Instant threeDaysAgo \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; now\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;minus\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Duration\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;ofDays\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;3\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;long\u0026lt;/span\u0026gt; threeDaysAgoMillis \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; threeDaysAgo\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;toEpochMilli\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; Query rangequery \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; RangeQuery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;r \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; r\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;@timestamp\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;lte\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;JsonData\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;now\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;toEpochMilli\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;())).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;gte\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;JsonData\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;threeDaysAgoMillis\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;))).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;_toQuery\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; Query boolquery \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Query\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;qb \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; qb\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;bool\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;b \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; b\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;must\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;query\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;must\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;rangequery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)));\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;boolquery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; MatchPhraseQuery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; builder \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; MatchPhraseQuery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; builder\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;agent\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;query\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;xxx\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; Query phraseQuery \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Query\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;qb \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; qb\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;matchPhrase\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;builder\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;phraseQuery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;@Test\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;testSearch\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; SortOptions\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; sb \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SortOptions\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; SortOptions sortOptions \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; sb\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;fs \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; fs\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;@timestamp\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;order\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;SortOrder\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Desc\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; searchReq \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;allowPartialSearchResults\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;false\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Collections\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;singletonList\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;indexName\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;size\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;10\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;sort\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sortOptions\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;source\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sc \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; sc\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;fetch\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;trackTotalHits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;tr \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; tr\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;enabled\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;query\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;q \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; q\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;matchAll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; MatchAllQuery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; SearchResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; searchResponse \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;search\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchReq\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(),\u0026lt;/span\u0026gt; LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;class\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;total\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Hit\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; hit \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; searchResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;())\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;mapper\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;writeValueAsString\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;hit\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;source\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;@Test\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;testTermsAgg\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; MatchAllQuery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; mb \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; MatchAllQuery\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; Query query \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; Query\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;qb \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; qb\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;matchAll\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;mb\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()));\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt; searchReq \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SearchRequest\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;Builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;allowPartialSearchResults\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;false\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;Collections\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;singletonList\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;indexName\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;query\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;query\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;size\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;0\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;trackTotalHits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;tr \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; tr\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;enabled\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;aggregations\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;hostname_group\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; a \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; a\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;terms\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;t \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; t\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;field\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;agent.hostname.keyword\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)));\u0026lt;/span\u0026gt; SearchResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; searchResponse \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;search\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchReq\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(),\u0026lt;/span\u0026gt; LogEntry\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;class\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;searchResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;hits\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;total\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; List\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;StringTermsBucket\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; buckets \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; searchResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;aggregations\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;get\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;hostname_group\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;sterms\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;buckets\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;array\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;StringTermsBucket bucket \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; buckets\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;bucket\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;docCount\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34; terms under \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; bucket\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;key\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;().\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;stringValue\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;@Test\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;testSingleDocument\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; IOException \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; Product product \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; Product\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;bk-1\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;City bike\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; 123\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; IndexRequest\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;Product\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; request \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; IndexRequest\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;of\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;i \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; i\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;products\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;product\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;getSku\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;document\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;product\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; IndexResponse response \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; client\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;request\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Indexed with version \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; response\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;@Test\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;public\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;void\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;testSingleDocumentDSLAsync\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;throws\u0026lt;/span\u0026gt; Exception \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; Product product \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; Product\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;bk-1\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;City bike\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; 123\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;0\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; EasysearchAsyncClient asyncClient \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; SampleClient\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;createAsyncClient\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; CompletableFuture\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;lt;\u0026lt;/span\u0026gt;IndexResponse\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;\u0026amp;gt;\u0026lt;/span\u0026gt; future \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; asyncClient\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;i \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; i\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;index\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;products\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;id\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;product\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;getSku\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;document\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;product\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;whenComplete\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;((\u0026lt;/span\u0026gt;response\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; exception\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;exception \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;!=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;null\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;err\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Failed to index \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; exception\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;else\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;{\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Indexed with version \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; response\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;});\u0026lt;/span\u0026gt; IndexResponse response \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; future\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;get\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; System\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;out\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;println\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;Async Indexed with version \u0026amp;#34;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;+\u0026lt;/span\u0026gt; response\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;version\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt;  }\n\n","subcategory":null,"summary":"","tags":null,"title":"API 使用","url":"/easysearch/v1.15.0/docs/references/client/client-api/"},{"category":null,"content":"词项查询 #  Easysearch 在搜索数据时支持两种类型的查询：词项 (term) 查询和全文查询。\n下表显示了它们之间的差异：\n    词项查询 全文检索     描述 term 查询回应与查询匹配的文档。 全文查询回应文档与查询的匹配程度。   分词 搜索 term 是不分词的。这意味着 term 查询按原样搜索您的 term。 搜索 term 由索引时用于文档指定字段的分词器进行分词。这意味着您的搜索词将经历与文档字段相同的分词过程。   相关性 Term 级查询只返回匹配的文档，而不根据相关性得分对其进行排序。他们仍然计算相关性得分，但该得分对于返回的所有文档都是相同的。 全文查询计算每个匹配的相关性得分，并按相关性的降序对结果进行排序。   应用场景 当您希望匹配数字、日期、 tag 等精确值，并且不需要按相关性对匹配项进行排序时，请使用术语级查询。 在考虑大小写和词干变体等因素后，使用全文查询来匹配文本字段并按相关性排序。     Easysearch 使用名为 Okapi BM25 的概率排名框架来计算相关性得分。要了解更多关于 Okapi BM25 的信息，请参阅 维基百科.\n 假设您在 Easysearch 集群中索引了莎士比亚全集。我们使用 term 查询在 text_entry 字段中搜索短语 “To be，or not be”：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;To be, or not to be\u0026#34; } } } 响应示例 #  { \u0026#34;took\u0026#34;: 3, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 0, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] } } 我们没有找回任何匹配（ hits ）。这是因为 term “To be, or not to be”是在倒排索引中按字面搜索的，其中只存储文本字段的分词后的值。Term 查询不适合搜索分词的文本字段，因为它们通常会产生意外的结果。使用文本数据时，仅对仅映射为 keyword 的字段使用 term 查询。\n使用全文查询:\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;text_entry\u0026#34;: \u0026#34;To be, or not to be\u0026#34; } } } 搜索查询 “To be，or not be” 被分析并标记为一个标记数组，就像文档的 text_entry 字段一样。全文查询在搜索查询和所有文档的 text_entry 字段之间执行标记的交集，然后根据相关性得分对结果进行排序：\n响应示例 #  { \u0026#34;took\u0026#34; : 19, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 10000, \u0026#34;relation\u0026#34; : \u0026#34;gte\u0026#34; }, \u0026#34;max_score\u0026#34; : 17.419369, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;34229\u0026#34;, \u0026#34;_score\u0026#34; : 17.419369, \u0026#34;_source\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34; : 34230, \u0026#34;play_name\u0026#34; : \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34; : 19, \u0026#34;line_number\u0026#34; : \u0026#34;3.1.64\u0026#34;, \u0026#34;speaker\u0026#34; : \u0026#34;HAMLET\u0026#34;, \u0026#34;text_entry\u0026#34; : \u0026#34;To be, or not to be: that is the question:\u0026#34; } }, { \u0026#34;_index\u0026#34; : \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;109930\u0026#34;, \u0026#34;_score\u0026#34; : 14.883024, \u0026#34;_source\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34; : 109931, \u0026#34;play_name\u0026#34; : \u0026#34;A Winters Tale\u0026#34;, \u0026#34;speech_number\u0026#34; : 23, \u0026#34;line_number\u0026#34; : \u0026#34;4.4.153\u0026#34;, \u0026#34;speaker\u0026#34; : \u0026#34;PERDITA\u0026#34;, \u0026#34;text_entry\u0026#34; : \u0026#34;Not like a corse; or if, not to be buried,\u0026#34; } }, { \u0026#34;_index\u0026#34; : \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;103117\u0026#34;, \u0026#34;_score\u0026#34; : 14.782743, \u0026#34;_source\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34; : 103118, \u0026#34;play_name\u0026#34; : \u0026#34;Twelfth Night\u0026#34;, \u0026#34;speech_number\u0026#34; : 53, \u0026#34;line_number\u0026#34; : \u0026#34;1.3.95\u0026#34;, \u0026#34;speaker\u0026#34; : \u0026#34;SIR ANDREW\u0026#34;, \u0026#34;text_entry\u0026#34; : \u0026#34;will not be seen; or if she be, its four to one\u0026#34; } } ] } } ... 有关所有全文查询的列表，请参见 全文查询。\n如果您想在 speaker 字段中查询类似 “HAMLET” 的准确术语，并且不需要根据相关性得分对结果进行排序，则 term 查询更有效：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;speaker\u0026#34;: \u0026#34;HAMLET\u0026#34; } } } 响应示例 #  { \u0026#34;took\u0026#34; : 5, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1582, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 4.2540946, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;32700\u0026#34;, \u0026#34;_score\u0026#34; : 4.2540946, \u0026#34;_source\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34; : 32701, \u0026#34;play_name\u0026#34; : \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34; : 9, \u0026#34;line_number\u0026#34; : \u0026#34;1.2.66\u0026#34;, \u0026#34;speaker\u0026#34; : \u0026#34;HAMLET\u0026#34;, \u0026#34;text_entry\u0026#34; : \u0026#34;[Aside] A little more than kin, and less than kind.\u0026#34; } }, { \u0026#34;_index\u0026#34; : \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;32702\u0026#34;, \u0026#34;_score\u0026#34; : 4.2540946, \u0026#34;_source\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34; : 32703, \u0026#34;play_name\u0026#34; : \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34; : 11, \u0026#34;line_number\u0026#34; : \u0026#34;1.2.68\u0026#34;, \u0026#34;speaker\u0026#34; : \u0026#34;HAMLET\u0026#34;, \u0026#34;text_entry\u0026#34; : \u0026#34;Not so, my lord; I am too much i\u0026#39; the sun.\u0026#34; } }, { \u0026#34;_index\u0026#34; : \u0026#34;shakespeare\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;32709\u0026#34;, \u0026#34;_score\u0026#34; : 4.2540946, \u0026#34;_source\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;line\u0026#34;, \u0026#34;line_id\u0026#34; : 32710, \u0026#34;play_name\u0026#34; : \u0026#34;Hamlet\u0026#34;, \u0026#34;speech_number\u0026#34; : 13, \u0026#34;line_number\u0026#34; : \u0026#34;1.2.75\u0026#34;, \u0026#34;speaker\u0026#34; : \u0026#34;HAMLET\u0026#34;, \u0026#34;text_entry\u0026#34; : \u0026#34;Ay, madam, it is common.\u0026#34; } } ] } } ... Term 查询精确匹配。所以，如果你搜索 “Hamlet” ，你不会得到任何匹配，因为 “HAMLET” 是一个 keyword 字段，它是按字面意思存储在 Easysearch 中，而不是以分词的形式存储的。 搜索查询“HAMLET”也按字面搜索。因此，要在该字段上获得匹配，我们需要输入完全相同的字符。\n Term 查询 #  term 查询在字段中搜索精确的 term。\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;line_id\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;61809\u0026#34; } } } } Terms 查询 #  terms 查询在同一字段中搜索多个 term。\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;line_id\u0026#34;: [ \u0026#34;61809\u0026#34;, \u0026#34;61810\u0026#34; ] } } } 你会得到符合任何 term 的文档。\nIDs #  ids 查询搜索一个或多个文档 ID 值。\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;ids\u0026#34;: { \u0026#34;values\u0026#34;: [ 34229, 91296 ] } } } Range #  范围 查询搜索字段中的值范围。\n要搜索 line_id 值 \u0026gt;= 10 and \u0026lt;= 20 的文档，请执行以下操作：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;line_id\u0026#34;: { \u0026#34;gte\u0026#34;: 10, \u0026#34;lte\u0026#34;: 20 } } } }    参数 行为     gte 大于等于。   gt 大于。   lte 小于等于。   lt 小于。    假设您有一个 products 索引，并且希望查找 2023 年添加的所有产品：\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;created\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2023/01/01\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2023/12/31\u0026#34; } } } } 使用基本数学表达式指定相对日期。\n从指定日期减去 1 年零 1 天：\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;created\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2023/01/01||-1y-1d\u0026#34; } } } } 我们指定的第一个日期是定位日期或日期数学的起点。添加两个拖曳管道符号。然后你可以加一天（ +1d ）或减去两周（ -2w ）。此数学表达式与您指定的定位日期相关。\n您还可以通过在日期或时间单位中添加正斜杠来舍入日期。\n要查找上一年添加并按月份四舍五入的产品：\nGET products/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;created\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;now-1y/M\u0026#34; } } } } 关键字 now 指向当前日期和时间。\n前缀 Prefix #  prefix 查询搜索以特定前缀开头的 term。\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34;: { \u0026#34;speaker\u0026#34;: \u0026#34;KING\u0026#34; } } } 是否存在 Exists #  exists 查询搜索包含特定字段的文档。\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;speaker\u0026#34; } } } 通配符 Wildcards #  通配符查询搜索与通配符模式匹配的 term。\n   特征 行为     * 匹配所有有效值。   ? 匹配单个有效值。    要搜索以 H 开头并以 Y 结尾的 term ：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;speaker\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;H*Y\u0026#34; } } } } 如果我们将 * 更改为 ？，我们得不到匹配，因为 ？ 代表单个字符。\n通配符查询往往很慢，因为它们需要遍历大量术语。避免在查询的开头放置通配符，因为这可能是一个资源和时间都非常昂贵的操作。\n正则 Regex #  使用 regex 查询搜索与正则表达式匹配的 term。\n此正则表达式匹配任何单个大写或小写字母：\nGET shakespeare/_search { \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;: { \u0026#34;play_name\u0026#34;: \u0026#34;H[a-zA-Z]+mlet\u0026#34; } } } 正则表达式应用于字段中的 term ，而不是字段的整个值。\n正则表达式的效率在很大程度上取决于您编写的模式。确保使用前缀或后缀编写 regex 查询以提高性能。\n","subcategory":null,"summary":"","tags":null,"title":"词项查询","url":"/easysearch/v1.15.0/docs/references/search/term/"},{"category":null,"content":"脚本处理器（Script Processor） #  版本引入：1.14.0\nscript 搜索请求处理器用于拦截搜索请求，并在请求中添加一个内联的 Painless 脚本，该脚本会在接收到请求时执行。脚本仅能操作以下请求字段：\n from size explain version seq_no_primary_term track_scores track_total_hits min_score terminate_after profile  请求体字段 #  下表列出了该处理器支持的所有配置字段。\n   字段 数据类型 说明     source 内联脚本 要执行的脚本代码。必填。   lang 字符串 脚本语言。可选，默认为 painless，目前仅支持 painless。   tag 字符串 处理器的唯一标识符，用于调试或跟踪。可选。   description 字符串 对该处理器的描述信息。可选。   ignore_failure 布尔值 若为 true，当此处理器执行失败时，Easysearch 将忽略错误并继续执行后续处理器。可选，默认值为 false。    示例 #  以下请求创建一个名为 explain_one_result 的搜索管道，其中包含一个 script 请求处理器。该脚本的作用是：当请求返回多个结果时自动关闭 explain 功能，因为 explain 是一项开销较大的操作；仅在返回单个结果时启用。\nPUT /_search/pipeline/explain_one_result { \u0026quot;description\u0026quot;: \u0026quot;一个仅对单个结果启用 explain 操作的搜索管道\u0026quot;, \u0026quot;request_processors\u0026quot;: [ { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;painless\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;if (ctx._source['size'] \u0026gt; 1) { ctx._source['explain'] = false } else { ctx._source['explain'] = true }\u0026quot; } } ] } 说明 #   ctx._source 表示当前搜索请求的请求体内容。 上述脚本逻辑为：如果 size（返回结果数量）大于 1，则设置 \u0026quot;explain\u0026quot;: false，否则设置为 true。 这样可以避免在大量结果上执行高开销的评分解释（explain），从而提升性能。  使用搜索管道 #  在搜索请求中通过 search_pipeline 参数指定该管道：\nGET /my_index/_search?search_pipeline=explain_one_result { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;public\u0026quot; } }, \u0026quot;size\u0026quot;: 1 } ","subcategory":null,"summary":"","tags":null,"title":"脚本处理器","url":"/easysearch/v1.15.0/docs/references/search/search-pipelines/search-processors/script-processor/"},{"category":null,"content":"Easysearch Java API Client 使用文档 #  简介 #  Easysearch Java API Client 是 Easysearch 的官方 Java 客户端，提供了简洁、强大且类型安全的 API 接口。\n 全新重构的 2.0.x 版本，更轻量级的设计，移除冗余依赖。 兼容 Easysearch 各个版本。 为常用 Easysearch API 提供强类型的请求和响应。 API 均支持阻塞和异步方式。 使用流式构建器和函数式模式，以在创建复杂嵌套结构时编写简洁易读的代码。 通过使用 Jackson 无缝集成应用程序类。  快速开始 #  本页指导您完成Java客户端的安装过程，展示了如何实例化客户端，以及如何使用它执行基本的 Easysearch 操作。\n安装 #  easysearch-client 已经发布到 Maven https://mvnrepository.com/artifact/com.infinilabs/easysearch-client/2.0.2\n安装需要 jdk8 或以上版本\neasysearch-client 使用 Jackson 将业务代码和客户端 api 进行集成。\n在 Maven 项目中安装 #  相比 1.x 版本的客户端，新版客户端的安装更加简单，只需在您项目的 pom 文件的 dependencies 区域添加以下依赖以引入客户端\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.infinilabs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;easysearch-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 在 gradle 项目中安装 #\n 在您项目的 build.gradle 文件的 dependencies 区域添加以下依赖\ndependencies { implementation \u0026#39;com.infinilabs:easysearch-client:2.0.2\u0026#39; } 初始化客户端 #  假设您本地启动了 Easysearch 服务，并启用了安全通信加密和 security， 可以使用以下代码初始化客户端连接。\npublic static EasysearchClient create() throws NoSuchAlgorithmException, KeyStoreException, KeyManagementException { boolean https = true;  \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; HttpHost\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;[]\u0026lt;/span\u0026gt; hosts \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; HttpHost\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;[]{\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; HttpHost\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;localhost\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; 9200\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;https\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)};\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; SSLContext sslContext \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; SSLContextBuilder\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;create\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;loadTrustMaterial\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;null\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;chains\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; authType\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;true\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; SSLIOSessionStrategy sessionStrategy \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; SSLIOSessionStrategy\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sslContext\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; NoopHostnameVerifier\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;INSTANCE\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;final\u0026lt;/span\u0026gt; CredentialsProvider credentialsProvider \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; BasicCredentialsProvider\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; credentialsProvider\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;setCredentials\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;AuthScope\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;ANY\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; UsernamePasswordCredentials\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;username\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#e6db74\u0026quot;\u0026gt;\u0026amp;#34;passwowd\u0026amp;#34;\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;));\u0026lt;/span\u0026gt; RestClient restClient \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; RestClient\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;builder\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;hosts\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;setHttpClientConfigCallback\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;httpClientBuilder \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; httpClientBuilder\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;setDefaultCredentialsProvider\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;credentialsProvider\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;setSSLStrategy\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;sessionStrategy\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;disableAuthCaching\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;setRequestConfigCallback\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;requestConfigCallback \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;-\u0026amp;gt;\u0026lt;/span\u0026gt; requestConfigCallback\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;setConnectTimeout\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;30000\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;).\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;setSocketTimeout\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;300000\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#a6e22e\u0026quot;\u0026gt;build\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;();\u0026lt;/span\u0026gt; EasysearchTransport transport \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; RestClientTransport\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt; restClient\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; JacksonJsonpMapper\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;());\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;return\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#66d9ef\u0026quot;\u0026gt;new\u0026lt;/span\u0026gt; EasysearchClient\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;transport\u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;);\u0026lt;/span\u0026gt; \u0026lt;span style=\u0026quot;color:#f92672\u0026quot;\u0026gt;}\u0026lt;/span\u0026gt;  \n","subcategory":null,"summary":"","tags":null,"title":"快速开始","url":"/easysearch/v1.15.0/docs/references/client/java-client/"},{"category":null,"content":"基础查询 #  介绍 #  SQL 中的 SELECT 语句是从 Easysearch 索引中检索数据的最常见查询。在本文档中，只涵盖涉及单个索引和查询的简单 SELECT 语句。 SELECT 语句包括 SELECT、FROM、WHERE、GROUP BY、HAVING、ORDER BY 和 LIMIT 子句。其中，SELECT 和 FROM 是指定要获取哪些字段以及它们应该从哪个索引获取的基础。 其它所有子句都是可选的，根据您的需求使用。请继续阅读以了解它们的详细描述、语法和用例。\n语法 #  SELECT 语句的语法如下：\nSELECT [ALL | DISTINCT] (* | expression) [[AS] alias] [, ...] FROM index_name [WHERE predicates] [GROUP BY expression [, ...] [HAVING predicates]] [ORDER BY expression [ASC | DESC] [NULLS {FIRST | LAST}] [, ...]] [LIMIT [offset, ] size] 尽管不支持批量执行多个查询语句，但仍然允许以分号 ; 结束。例如，你可以运行 SELECT * FROM accounts; 而不会遇到问题。这对于支持其他工具生成的查询，如 Microsoft Excel 或 BI 工具，非常有用。\n基本原理 #  除了 SQL 语言的预定义关键字外，最基本的元素是字面量和标识符。字面量是数字、字符串、日期或布尔常量。 标识符表示 Easysearch 的索引或字段名称。通过应用算术运算符和 SQL 函数，基本的字面量和标识符可以构建成复杂的表达式。\n表达式原子 (expressionAtom) ：\n表达式可以通过逻辑运算符组合成谓词。通常，谓词用于 WHERE 和 HAVING 子句中，根据指定的条件过滤数据。\n表达式 (expression) ：\n谓词 (predicate) ：\n执行顺序 #  实际的执行顺序与语句的顺序大不相同：\nFROM index WHERE predicates GROUP BY expressions HAVING predicates SELECT expressions ORDER BY expressions LIMIT size SELECT #  SELECT 子句指定应检索 Easysearch 索引中的哪些字段。\n语法 #  selectElements:\n示例 1：查询所有字段 #  您可以用来获取索引中的所有字段，这在您只想快速查看数据时非常方便。*\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;SELECT * FROM accounts\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200 }  结果集：\n   Account Number First Name Gender City Balance Employer State Email Address Last Name Age     1 Amber M Brogan 39225 Pyrami IL amberduke@pyrami.com 880 Holmes Lane Duke 32   6 Hattie M Dante 5686 Netagy TN hattiebond@netagy.com 671 Bristol Street Bond 36   13 Nanette F Nogal 32838 Quility VA nanettebates@quility.com 789 Madison Street Bates 28   18 Dale M Orick 4180 null MD daleadams@boink.com 467 Hutchinson Court Adams 33    示例 2：查询特定字段 #  通常情况下，您会在子句中指定特定的字段名，以避免检索到大量不必要的数据。SELECT\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;SELECT firstname, lastname FROM accounts\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;firstname\u0026quot;, \u0026quot;lastname\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] } }  结果集：\n   firstname lastname     Amber Duke   Dale Adams   Hattie Bond   Nanette Bates    示例 3：使用字段别名 #  别名通常用于通过缩短字段名称来提高查询的可读性。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;SELECT account_number AS num FROM accounts\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] } }  结果集：\n   num     1   6   13   18    示例4：查询去重字段 #  默认情况下，SELECT ALL 生效以返回所有行。当您想要去重并获取唯一字段值时，DISTINCT 很有用。您可以提供一个或多个字段名（目前不支持 DISTINCT *）。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;SELECT DISTINCT age FROM accounts\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 0, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;age\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] }, \u0026quot;stored_fields\u0026quot; : \u0026quot;age\u0026quot;, \u0026quot;aggregations\u0026quot; : { \u0026quot;age\u0026quot; : { \u0026quot;terms\u0026quot; : { \u0026quot;field\u0026quot; : \u0026quot;age\u0026quot;, \u0026quot;size\u0026quot; : 200, \u0026quot;min_doc_count\u0026quot; : 1, \u0026quot;shard_min_doc_count\u0026quot; : 0, \u0026quot;show_term_doc_count_error\u0026quot; : false, \u0026quot;order\u0026quot; : [ { \u0026quot;_count\u0026quot; : \u0026quot;desc\u0026quot; }, { \u0026quot;_key\u0026quot; : \u0026quot;asc\u0026quot; } ] } } } }  结果集：\n   age     28   32   33   36    实际上，您可以在一个子句中使用任何表达式，例如：DISTINCT 。\nos\u0026gt; SELECT DISTINCT SUBSTRING(lastname, 1, 1) FROM accounts; fetched rows / total rows = 3/3 +-----------------------------+ | SUBSTRING(lastname, 1, 1) | |-----------------------------| | A | | B | | D | +-----------------------------+  FROM #  FROM 子句指定 Easysearch 索引，从中检索数据。在上一节中，您已经了解了如何在 FROM 子句中指定单个索引。这里我们提供更多用例示例。\n在 FROM 子句中支持子查询。有关更多详细信息，请查看文档。\n语法 #  tableName:\n示例1：使用索引别名 #  可以在 FROM 子句中给索引一个别名，并在查询的多个子句中使用它。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;SELECT acc.account_number FROM accounts acc\u0026quot; }  示例 2：按索引模式从多个索引中进行选择 #  或者，您可以通过索引模式从多个名称相似的索引中进行查询。这对于 Logstash 索引模板创建的以日期为后缀的索引来说非常方便。\nSQL query::\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;SELECT account_number FROM account*\u0026quot; }  WHERE #  WHERE 子句指定应受影响的 Easysearch 文档应符合的条件。它由使用 =, \u0026lt;\u0026gt;, \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=, IN, BETWEEN, LIKE, IS NULL 或 IS NOT NULL 的谓词组成。这些谓词可以通过逻辑运算符 NOT、AND 或 OR 结合起来，构建更复杂的表达式。\n对于 LIKE 和其他全文搜索主题，请参阅全文搜索文档。\n除了 SQL 查询，WHERE 子句也可以用于 SQL 语句，例如 DELETE。有关详细信息，请参阅数据操作语言文档。\n示例 1：比较运算符 #  基本比较运算符，例如 =, \u0026lt;\u0026gt;, \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=，可以用于数字、字符串或日期。 IN 和 BETWEEN 对于多个值或范围的比较非常方便。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT account_number FROM accounts WHERE account_number = 1 \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;must\u0026quot; : [ { \u0026quot;term\u0026quot; : { \u0026quot;account_number\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;boost\u0026quot; : 1.0 } } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } }, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] } }  结果集：\n   account_number     1    示例2：缺失的字段 #  Easysearch 允许灵活的模式，索引中的文档可能具有不同的字段。在这种情况下，您可以使用 IS NULL 或 IS NOT NULL 来检索缺失的字段或仅存在的字段。\n注意，目前我们不明确区分缺失字段和设置为 NULL 的字段。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT account_number, employer FROM accounts WHERE employer IS NULL \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;must\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;must_not\u0026quot; : [ { \u0026quot;exists\u0026quot; : { \u0026quot;field\u0026quot; : \u0026quot;employer.keyword\u0026quot;, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } }, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot;, \u0026quot;employer\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] } }  结果集：\n   account_number employer     18 null    GROUP BY #  GROUP BY 将具有相同字段值的文档分组到桶中。它通常与聚合函数一起使用，在每个桶内聚合。有关更多详细信息，请参阅 SQL 函数文档。\n请注意，WHERE 子句在 GROUP BY 子句之前应用。\n示例 1：按字段分组 #  SQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT age FROM accounts GROUP BY age \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 0, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;age\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] }, \u0026quot;stored_fields\u0026quot; : \u0026quot;age\u0026quot;, \u0026quot;aggregations\u0026quot; : { \u0026quot;age\u0026quot; : { \u0026quot;terms\u0026quot; : { \u0026quot;field\u0026quot; : \u0026quot;age\u0026quot;, \u0026quot;size\u0026quot; : 200, \u0026quot;min_doc_count\u0026quot; : 1, \u0026quot;shard_min_doc_count\u0026quot; : 0, \u0026quot;show_term_doc_count_error\u0026quot; : false, \u0026quot;order\u0026quot; : [ { \u0026quot;_count\u0026quot; : \u0026quot;desc\u0026quot; }, { \u0026quot;_key\u0026quot; : \u0026quot;asc\u0026quot; } ] } } } }  结果集：\n   age     28   32   33   36    示例 2：按字段别名分组 #  字段别名可在 GROUP BY 子句中访问。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT account_number AS num FROM accounts GROUP BY num \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 0, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] }, \u0026quot;stored_fields\u0026quot; : \u0026quot;account_number\u0026quot;, \u0026quot;aggregations\u0026quot; : { \u0026quot;num\u0026quot; : { \u0026quot;terms\u0026quot; : { \u0026quot;field\u0026quot; : \u0026quot;account_number\u0026quot;, \u0026quot;size\u0026quot; : 200, \u0026quot;min_doc_count\u0026quot; : 1, \u0026quot;shard_min_doc_count\u0026quot; : 0, \u0026quot;show_term_doc_count_error\u0026quot; : false, \u0026quot;order\u0026quot; : [ { \u0026quot;_count\u0026quot; : \u0026quot;desc\u0026quot; }, { \u0026quot;_key\u0026quot; : \u0026quot;asc\u0026quot; } ] } } } }  结果集：\n   num     1   6   13   18    示例 3：按序号分组 #  或者，SELECT 子句中的字段序号也可以用于分组。然而，这并不推荐，因为您的 GROUP BY 子句依赖于 SELECT 子句中的字段，并需要相应更改。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT age FROM accounts GROUP BY 1 \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 0, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;age\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] }, \u0026quot;stored_fields\u0026quot; : \u0026quot;age\u0026quot;, \u0026quot;aggregations\u0026quot; : { \u0026quot;age\u0026quot; : { \u0026quot;terms\u0026quot; : { \u0026quot;field\u0026quot; : \u0026quot;age\u0026quot;, \u0026quot;size\u0026quot; : 200, \u0026quot;min_doc_count\u0026quot; : 1, \u0026quot;shard_min_doc_count\u0026quot; : 0, \u0026quot;show_term_doc_count_error\u0026quot; : false, \u0026quot;order\u0026quot; : [ { \u0026quot;_count\u0026quot; : \u0026quot;desc\u0026quot; }, { \u0026quot;_key\u0026quot; : \u0026quot;asc\u0026quot; } ] } } } }  结果集：\n   age     28   32   33   36    示例 4：按标量函数分组 #  标量函数可以在 GROUP BY 子句中使用，并且必须在 SELECT 子句中出现。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT ABS(age) AS a FROM accounts GROUP BY ABS(age) \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 0, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;script\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] }, \u0026quot;stored_fields\u0026quot; : \u0026quot;abs(age)\u0026quot;, \u0026quot;script_fields\u0026quot; : { \u0026quot;abs(age)\u0026quot; : { \u0026quot;script\u0026quot; : { \u0026quot;source\u0026quot; : \u0026quot;def abs_1 = Math.abs(doc['age'].value);return abs_1;\u0026quot;, \u0026quot;lang\u0026quot; : \u0026quot;painless\u0026quot; }, \u0026quot;ignore_failure\u0026quot; : false } }, \u0026quot;aggregations\u0026quot; : { \u0026quot;abs(age)\u0026quot; : { \u0026quot;terms\u0026quot; : { \u0026quot;script\u0026quot; : { \u0026quot;source\u0026quot; : \u0026quot;def abs_1 = Math.abs(doc['age'].value);return abs_1;\u0026quot;, \u0026quot;lang\u0026quot; : \u0026quot;painless\u0026quot; }, \u0026quot;size\u0026quot; : 200, \u0026quot;min_doc_count\u0026quot; : 1, \u0026quot;shard_min_doc_count\u0026quot; : 0, \u0026quot;show_term_doc_count_error\u0026quot; : false, \u0026quot;order\u0026quot; : [ { \u0026quot;_count\u0026quot; : \u0026quot;desc\u0026quot; }, { \u0026quot;_key\u0026quot; : \u0026quot;asc\u0026quot; } ] } } } }  结果集：\n   a     28.0   32.0   33.0   36.0    HAVING #  HAVING 子句通过谓词过滤 GROUP BY 子句的结果。因此，聚合函数，甚至与 SELECT 子句中不同的聚合函数，也可以在谓词中使用。\n示例 #  SQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT age, MAX(balance) FROM accounts GROUP BY age HAVING MIN(balance) \u0026gt; 10000 \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 0, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;age\u0026quot;, \u0026quot;MAX\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] }, \u0026quot;stored_fields\u0026quot; : \u0026quot;age\u0026quot;, \u0026quot;aggregations\u0026quot; : { \u0026quot;age\u0026quot; : { \u0026quot;terms\u0026quot; : { \u0026quot;field\u0026quot; : \u0026quot;age\u0026quot;, \u0026quot;size\u0026quot; : 200, \u0026quot;min_doc_count\u0026quot; : 1, \u0026quot;shard_min_doc_count\u0026quot; : 0, \u0026quot;show_term_doc_count_error\u0026quot; : false, \u0026quot;order\u0026quot; : [ { \u0026quot;_count\u0026quot; : \u0026quot;desc\u0026quot; }, { \u0026quot;_key\u0026quot; : \u0026quot;asc\u0026quot; } ] }, \u0026quot;aggregations\u0026quot; : { \u0026quot;MAX_0\u0026quot; : { \u0026quot;max\u0026quot; : { \u0026quot;field\u0026quot; : \u0026quot;balance\u0026quot; } }, \u0026quot;min_0\u0026quot; : { \u0026quot;min\u0026quot; : { \u0026quot;field\u0026quot; : \u0026quot;balance\u0026quot; } }, \u0026quot;bucket_filter\u0026quot; : { \u0026quot;bucket_selector\u0026quot; : { \u0026quot;buckets_path\u0026quot; : { \u0026quot;min_0\u0026quot; : \u0026quot;min_0\u0026quot;, \u0026quot;MAX_0\u0026quot; : \u0026quot;MAX_0\u0026quot; }, \u0026quot;script\u0026quot; : { \u0026quot;source\u0026quot; : \u0026quot;params.min_0 \u0026gt; 10000\u0026quot;, \u0026quot;lang\u0026quot; : \u0026quot;painless\u0026quot; }, \u0026quot;gap_policy\u0026quot; : \u0026quot;skip\u0026quot; } } } } } }  结果集：\n   age MAX(balance)     28 32838   32 39225    ORDER BY #  ORDER BY 子句指定用于对结果进行排序的字段以及排序方向。\n示例 1：按字段排序 #  除了常规字段名称外，序号、别名或标量函数也可以像在 GROUP BY 中一样使用。可以附加 ASC（默认）或 DESC 表示按升序或降序排序。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;SELECT account_number FROM accounts ORDER BY account_number DESC\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] }, \u0026quot;sort\u0026quot; : [ { \u0026quot;account_number\u0026quot; : { \u0026quot;order\u0026quot; : \u0026quot;desc\u0026quot; } } ] }  结果集：\n   account_number     18   13   6   1    示例 2：按聚合函数排序\n聚合函数可以在 ORDER BY 子句中使用。您可以通过相同的函数调用、别名或在选择列表中的序号来引用它：\nos\u0026gt; SELECT gender, MAX(age) FROM accounts GROUP BY gender ORDER BY MAX(age) DESC; fetched rows / total rows = 2/2 +----------+------------+ | gender | MAX(age) | |----------+------------| | M | 36 | | F | 28 | +----------+------------+  即使在 SELECT 子句中不存在，它也可以按照以下方式使用：\nos\u0026gt; SELECT gender, MIN(age) FROM accounts GROUP BY gender ORDER BY MAX(age) DESC; fetched rows / total rows = 2/2 +----------+------------+ | gender | MIN(age) | |----------+------------| | M | 32 | | F | 28 | +----------+------------+  LIMIT #  为了防止将大量数据拉取到内存中，需要指定返回的文档数量上限。这种情况下，LIMIT 子句非常有用。 基本上，限制是设置在查询规划中的，所以不同的 LIMIT 和 OFFSET 可能会在结果中导致不可预测的子集。因此，建议在带有 limit 关键字的查询中使用 order by，以强制结果集中的固定排序。\n示例 1：限制结果大小 #  给定一个正数，LIMIT 将使用它作为页面大小，最多获取指定大小的结果。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT account_number FROM accounts ORDER BY account_number LIMIT 1 \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 1, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] }, \u0026quot;sort\u0026quot; : [ { \u0026quot;account_number\u0026quot; : { \u0026quot;order\u0026quot; : \u0026quot;asc\u0026quot; } } ] }  结果集：\n   account_number     1    示例2：按偏移量获取结果 #  偏移位置可以作为第一个参数给出，以指示从何处开始获取。虽然在大型索引上效率低下，但这可以用作简单的分页解决方案。 通常在这种情况下需要使用 ORDER BY，以确保页面之间具有相同的顺序。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT account_number FROM accounts ORDER BY account_number LIMIT 1, 1 \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 1, \u0026quot;size\u0026quot; : 1, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] }, \u0026quot;sort\u0026quot; : [ { \u0026quot;account_number\u0026quot; : { \u0026quot;order\u0026quot; : \u0026quot;asc\u0026quot; } } ] }  结果集：\n   account_number     1    偏移位置也可以在 OFFSET 关键字后面给出，以下是一个示例：\n\u0026gt;os SELECT age FROM accounts ORDER BY age LIMIT 2 OFFSET 1 fetched rows / total rows = 2/2 +-------+ | age | |-------| | 32 | | 33 | +-------+  ","subcategory":null,"summary":"","tags":null,"title":"基础查询","url":"/easysearch/v1.15.0/docs/references/sql/basics/"},{"category":null,"content":"版本发布日志 #  这里是 INFINI Easysearch 历史版本发布的相关说明。\nLatest (In development) #  Breaking changes #  Features #  Bug fix #  Improvements #  1.14.0 (2025-07-25) #  Breaking changes #   AI 模块 从 modules 迁移至 plugins 目录下，方便调用 knn 插件 旧的文本向量化接口 _ai/embed 已不再支持，将在后续版本删除  Features #   插件模块新增完整的文本嵌入模型集成功能，涵盖从数据导入到向量检索的全流程 新增语义检索 API，简化向量搜索使用流程 新增语义检索处理器配置大模型信息 新增搜索管道（Search pipelines），轻松地在 Easysearch 内部处理查询请求和查询结果 多模型集成支持  OpenAI 向量模型：直接调用 OpenAI 的嵌入接口（如 text-embedding-3-small） Ollama 本地模型：支持离线环境或私有化部署的向量生成   ik 分词器提供 reload API，能够对存量自定义词典进行完整更新 ik 分词器能够通过词库索引对默认词库进行自定义添加  Bug fix #  Improvements #   增强数据摄取管道（ingest pipeline）  在数据索引阶段支持文本向量化，文档可自动生成向量表示 导入数据时通过 ingest 管道进行向量化时支持单条和批量模式，适配大模型的请求限制场景   更新 Easysearch Docker 初始化文档 ik 分词器优化自定义词库加载逻辑，减少内存占用  1.13.1 (2025-06-29) #  Breaking changes #  Features #   插件模块新增 dependencyModule 配置项，用于声明共享的 common 模块依赖  Bug fix #   修复了前缀查询请求在只包含一个字符时的空指针错误  Improvements #   精简了部署后的 modules和 plugins 大小  1.13.0 (2025-06-16) #  Breaking changes #  Features #   Rollup 新增对已创建 job 的 interval 和 page_size 参数更新的 api Rollup 索引数据增加 unique 字段标识当前 job 的数据 Rollup 配置增加 window_start_time 字段，当重建 Rollup 时 会把历史 metadata 的最新时间戳写到 window_start_time 里  Bug fix #  Improvements #   Rollup 会自动检测源数据索引是否有新增字段，并更新到 metrics ILM 删除索引时 增加对 Rollup 的运行状态判断，未处理完的索引不会删除  1.12.3 (2025-05-31) #  Breaking changes #  Features #   Rollup Job 支持断点续跑： 当检测到历史 Rollup 索引及元数据时，新创建的 Rollup Job 会自动从中断的状态点继续处理，避免重复计算，显著节省计算资源  Bug fix #   修复前端 Rollup 查询时如果只有历史数据，不能正确返回 pipeline 聚合结果的错误  Improvements #   优化 Rollup job 的 top_hits 聚合效率 提升启用 source_reuse 时的文档获取性能  1.12.2 (2025-05-18) #  Breaking changes #  Features #   NodeStats 新增 rollup 的 运行状态，方便监控 更新 ik-analyzer 文档  Bug fix #  Improvements #   当设置了 JAVA_HOME 时，优先使用本地 JDK 新增安装为 linux 服务的文档  1.12.1 (2025-04-28) #  Breaking changes #  Features #   索引合并新增按照日期范围合并的策略：通过 index.merge.policy.time_range_field 配置项指定合并依据的字段  Bug fix #   修复 ollama_url 不能动态更新的错误 修复 ollama api 未正确兼容单个文本请求 索引生命周期管理 delete action 按文档最新时间删除时修正为按降序排序  Improvements #  1.12.0 (2025-03-28) #  Breaking changes #  Features #   Rollup 新增 write_optimization 配置项，启用后采用自动生成文档 ID 的策略，大幅提升写入速度 Rollup 现在支持针对 job 级别配置 rollover 的 max docs  Bug fix #   Rollup 修复带有内嵌的 pipeline 聚合时不能和原始索引聚合正常合并的问题  Improvements #   优化了 rollup 索引字段名长度，减小 rollup job 运行时的内存占用  1.11.1 (2025-03-14) #  Breaking changes #  Features #   新增 AI 模块，集成 Ollama embedding API，支持文本向量化  Bug fix #   修复 DateRange 聚合在 Rollup 查询中无法正确合并的问题  Improvements #  针对用户使用体验进行了多项改进，包括：\n 弃用 KNN 模块中的 index.knn 配置项，（此配置项和其他功能经常发生冲突） 简化配置逻辑，该配置项将在后续版本中移除 将 KNN 搜索功能从插件形式集成为内置功能，无需额外安装即可使用 将跨集群复制（CCR）功能从插件形式集成为内置功能，开箱即用 优化索引配置更新验证：增加非动态配置项的值比对，避免误报  1.11.0 (2025-02-28) #  Features #   新增 wildcard 数据类型 新增 Point in time 搜索 API 新增异步搜索 API 数值字段添加 doc-values 搜索支持 日期字段添加 doc-values 搜索支持 新增 IK 分词器自定义词典使用文档  Bug fix #  Improvements #   优化 Lucene flush 的 segment 大小，减少 I/O 开销  1.10.2 (2025-02-17) #  Features #    lucene 版本更新\n lucene 版本更新到 8.11.4，是 lucene8.x 系列的最后一个版本 jna 更新到 5.12.1    IK 分词器: 增强词典配置的灵活性和可扩展性\n 支持字段级别的词典配置，用户可通过自定义 tokenizer 为不同索引、不同字段配置专用词典 优化词典管理机制 支持自定义词典与 IK 默认词典合并使用 词库数据存储在可配置的索引中，支持实时更新 可使用内置词库索引或自定义词库索引(需保持相同结构)    索引生命周期管理\n delete action 支持同时基于索引创建时间和文档最新时间戳来执行删除操作    Bug fix #   Rollup  修复了平均值(avg)聚合计算错误    Improvements #   优化 rollup 索引的创建流程  1.10.1 (2025-01-24) #  Features #   Rollup 增加支持聚合的种类  增加支持 Filter aggregation，某些场景可以用来替代 query 增加针对个别字段自定义 special_metrics 指标的配置项 增加支持 Bucket sort aggregation   Rollup 查询 API 提供了 debug 参数，有助于调试  Bug fix #   修复数据节点和 master 节点角色分离时，Security 索引创建失败问题  Improvements #   Rollup 查询 增加 response 标识是否有 rollup 数据 Rollup response total hits 不再为 0 Rollup job 支持 更新操作，通过更新索引文档实现 rollup.hours_before 配置项只影响查询时间范围，不影响写入  1.10.0 (2025-01-11) #  Features #   Rollup 功能增强：新增并发限制、任务失败自动重启功能，支持批量启动、停止 Job， 并支持 date_range 聚合。 字段类型功能优化：新增 flattened_text 和 match_only_text 字段类型，支持更多查询场景；  1.9.0 (2024-10-17) #  Breaking changes #  Features #    发布 rollup 功能\n  支持自动对 rollup 索引进行滚动，无需外部触发\n  支持 avg sum max min value_count percentiles 指标类型的聚合\n  支持 terms 聚合\n  支持对指标聚合进行 Pipeline 聚合\n  支持聚合前先对数据进行过滤\n  进行聚合查询时支持直接搜索原始索引，不用更改搜索代码\n  增加适配 logstash8.x 的请求 header\n  _cat/templates 增加 lifecycle 和 rollover 列的展示\n  Bug fix #   修复 rest-api template 测试错误  1.8.3 (2024-08-13) #  Bug fix #   修复特定场景下 lucene 栈溢出问题 修复特定场景下字节处理空指针问题  Improvements #   更新依赖库至安全版本 优化开启 source_reuse 后内存使用性能 增加初始化密码环境变量,可手工设置 EASYSEARCH_INITIAL_ADMIN_PASSWORD 环境变量。  1.8.2 (2024-06-06) #  Breaking changes #  Features #  Bug fix #   修复 source_reuse 与 索引 mapping 的 enable: false 冲突  Improvements #   升级部分依赖包版本，commons-collections to 3.2.2, snakeyaml to 2.0 优化 CCR 同步性能及调整 CCR 全局配置参数 优化插件配置命名，去除\u0026quot;plugins.\u0026quot; 优化配置文件目录获取命名  1.8.0 (2024-04-30) #  Breaking changes #  Features #   增加写入限流功能，可针对节点级（数据节点和协调节点）、分片级  Bug fix #   修复查询数据结果为空时，聚合出错问题 修复 Bundle 包在 MacOS 环境下 JDK 路径出错问题  Improvements #  1.7.1 (2024-03-01) #  Breaking changes #  Features #  Bug fix #   修复 _meta 不为空且 启用 source_reuse 时的映射解析错误 修复 source_reuse 下对多值还原不正确的问题 修复 source_reuse 和 alias 类型字段的冲突  Improvements #   改进跨集群复制的数据加载，增加对 source_reuse 索引的支持 内存断路器在触发 GC 时增加 full GC 回退 针对 String 类型的 TermsAgg 增加分片级别的内存断路检测  1.7.0 (2023-12-15) #  Breaking changes #  Features #   发布快照搜索功能 Beta 版本，此功能旨在提高对已备份数据的使用效率。让用户利用对象存储（如 AWS S3、MinIO、Microsoft Azure Storage、Google Cloud Storage 等）技术来大幅降低存储成本。  Bug fix #   修复单个节点场景下，从快照恢复多个 shard 的索引时，恢复不完整的问题 修复无法删除索引已关联的 ILM 策略问题  Improvements #   初始化脚本优化，新增重复执行判断  1.6.2 (2023-12-01) #  Breaking changes #  Features #  Bug fix #   修复跨集群复制（CCR）不能对自动滚动生成的索引进行同步的问题  Improvements #   优化初始化脚本，增加-s/-slient 自动安装参数。 新增含 jdk/plugins 的 bundle 安装包  1.6.1 (2023-10-19) #  Breaking changes #  Features #   增加 analysis-icu 插件  Bug fix #   修复 JDK 17 及更高版本运行告警及异常  Improvements #   安装脚本优化，避免脚本使用不当出现错误 source_reuse 增加对 float，double，geo_point，half_float，ip 类型字段的压缩 优化启用 source_reuse 时的写入速度，压缩的字段越多，写入速度越快  1.6.0 (2023-09-22) #  Breaking changes #  Features #   增加 _field_usage_stats api，统计索引每个字段的访问次数 新增 _disk_usage api，可以分析指定索引每个字段的磁盘占用大小 增加 flattened 类型，将 json 对象作为字符串处理，可以减少嵌套 json 型的文档的大小  Bug fix #  Improvements #   source_reuse 增加对 _source 中数字类型的值进行复用压缩，可进一步降低 _source 磁盘占用 改进 source_reuse 筛选字段的逻辑  1.5.0 (2023-09-08) #  Breaking changes #  Features #   增加 sql 插件，支持使用 REST 接口和 JDBC 进行 SQL 查询 支持 sql 常用函数、包括数学函数、三角函数、日期函数、字符串函数、聚合函数等 sql 语句可以嵌入全文检索 增加 jdbc 驱动，可以通过用户密码或证书连接到集群  Bug fix #   修复 knn 插件的配置项导致非 knn 索引的 setting 不能正常解析的 bug  Improvements #  1.4.0 (2023-07-21) #  Breaking changes #  Features #   索引生命周期管理增加 wait_for_snapshot 操作，在删除索引之前，等待执行指定的快照管理策略，这样可以确保已删除索引的快照可用 增加 analysis-hanlp 分词插件 增加 jieba 分词插件  Bug fix #   修复启用 index.source_reuse 时，对复杂多层 json 的 source 字段 解析不正确的 bug  Improvements #   更新索引生命周期管理 api 文档，增加策略应用和更新说明，增加 wait_for_snapshot 说明 执行 initialize.sh 命令时增加初始化确认提示，是否将 admin 密码记录日志。  1.3.0 (2023-06-30) #  Breaking changes #  Features #   增加 kNN 检索插件：  新增 knn_nearest_neighbors query api Mapping 新增 knn_dense_float_vector 和 knn_sparse_bool_vector 数据类型 支持近似 kNN 搜索和精确 kNN 搜索    Bug fix #  Improvements #   admin 用户默认由 initialize.sh 脚本生成随机密码,增强了安全性 增加适配 Windows 平台 增加 Docker 镜像  1.2.0 (2023-06-08) #  Breaking changes #  Features #   正式发布快照生命周期管理 (SLM) API, 支持定时备份和删除快照，以及保留快照的个数 增加 跨集群复制 (Cross-cluster replication) 功能：  支持手动或自动复制索引 支持暂停和恢复复制索引 支持取消指定索引的跨集群复制    Bug fix #   security 模块修复缺少某些角色验证属性的问题  Improvements #   兼容 ES6.0 版本的索引  1.1.1 (2023-05-25) #  Breaking changes #  Features #  Bug fix #   修复模板别名在某些场景不生效的 bug 防止 BigArray 在某些场景发生内存泄漏 修复 SourceValueFetcher 可能遗漏字段的 bug  Improvements #   easysearch.yml 增加 elasticsearch.api_compatibility 配置项, 兼容 logstash-oss, filebeat-oss, apm-server-oss 等 Elasticsearch 的客户端  1.1.0 (2023-05-12) #  Breaking changes #   Lucene 版本升级到 8.11.2  Features #   增加 ZSTD codec，引入 ZSTD 压缩算法，对存储字段，doc_values，词典进行压缩。 增加 index.source_reuse 索引级别配置，对 _source 进一步压缩。 提供索引生命周期管理 ILM 模块的功能，绝大部分 API 兼容 Elasticsearch  Bug fix #  Improvements #   减少冗余日志输出。 减少 modules 模块整体大小  1.0.0 (2023-04-06) #  Features #   兼容 Elasticsearch7.x 支持加密传输，权限控制等 security 相关功能 相比 Elasticsearch 更加轻量级  Bug fix #  Improvements #  ","subcategory":null,"summary":"","tags":null,"title":"Easysearch","url":"/easysearch/v1.15.0/docs/release-notes/easysearch/"},{"category":null,"content":"重命名字段处理器（Rename Field Processor） #  版本引入：1.14.0\nrename_field 搜索响应处理器用于拦截搜索响应，并对指定字段进行重命名。当你索引中的字段名称与应用程序使用的名称不一致时，该功能非常有用。例如，当索引中使用了新的字段名称，但应用程序仍期望接收旧字段名称时，可通过 rename_field 处理器在返回响应前完成字段名的映射，实现平滑过渡和向后兼容。\n请求体字段 #  下表列出了该处理器支持的所有配置字段。\n   字段 数据类型 说明     field 字符串 要重命名的原始字段名。必填。   target_field 字符串 新的字段名称。必填。   tag 字符串 处理器的唯一标识符。可选。   description 字符串 对该处理器的描述信息。可选。   ignore_failure 布尔值 若为 true，当此处理器执行失败时，Easysearch 将忽略错误并继续执行搜索管道中的其余处理器。可选，默认值为 false。    示例 #  以下示例演示如何在搜索管道中使用 rename_field 处理器。\n准备工作 #  创建一个名为 my_index 的索引，并索引一个包含 message 字段的文档：\nPOST /my_index/_doc/1 { \u0026#34;message\u0026#34;: \u0026#34;This is a public message\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34; } 创建搜索管道 #  以下请求创建一个名为 my_pipeline 的搜索管道，其中包含一个 rename_field 响应处理器，用于将字段 message 重命名为 notification：\nPUT /_search/pipeline/my_pipeline { \u0026quot;response_processors\u0026quot;: [ { \u0026quot;rename_field\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;message\u0026quot;, \u0026quot;target_field\u0026quot;: \u0026quot;notification\u0026quot; } } ] } 使用搜索管道 #  在不使用搜索管道的情况下，对 my_index 索引执行搜索：\nGET /my_index/_search 响应中包含原始字段 message：\n 响应  { \u0026quot;took\u0026quot;: 1, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 2, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 1.0, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;my_index\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 1.0, \u0026quot;_source\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;This is a public message\u0026quot;, \u0026quot;visibility\u0026quot;: \u0026quot;public\u0026quot; } } ] } }  要使用搜索管道，请在请求中通过 search_pipeline 查询参数指定管道名称：\nGET /my_index/_search?search_pipeline=my_pipeline 此时，字段 message 已被重命名为 notification：\n 响应  { \u0026quot;took\u0026quot;: 2, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 1, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 0.0, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;my_index\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.0, \u0026quot;_source\u0026quot;: { \u0026quot;visibility\u0026quot;: \u0026quot;public\u0026quot;, \u0026quot;notification\u0026quot;: \u0026quot;This is a public message\u0026quot; } } ] } }  你也可以使用 fields 参数来搜索文档中的特定字段：\nPOST /my_index/_search?pretty\u0026amp;search_pipeline=my_pipeline { \u0026quot;fields\u0026quot;: [\u0026quot;visibility\u0026quot;, \u0026quot;message\u0026quot;] } 在响应中，message 字段已被重命名为 notification，包括在 fields 返回结果中：\n 响应  { \u0026quot;took\u0026quot;: 4, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 1, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 0.0, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;my_index\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.0, \u0026quot;_source\u0026quot;: { \u0026quot;visibility\u0026quot;: \u0026quot;public\u0026quot;, \u0026quot;notification\u0026quot;: \u0026quot;This is a public message\u0026quot; }, \u0026quot;fields\u0026quot;: { \u0026quot;visibility\u0026quot;: [ \u0026quot;public\u0026quot; ], \u0026quot;notification\u0026quot;: [ \u0026quot;This is a public message\u0026quot; ] } } ] } }  ","subcategory":null,"summary":"","tags":null,"title":"重命名字段处理器","url":"/easysearch/v1.15.0/docs/references/search/search-pipelines/search-processors/rename-field-processor/"},{"category":null,"content":"混合搜索 #  混合搜索结合了关键词搜索和语义搜索，以提升搜索相关性。要实现混合搜索，您需要建立一个在搜索时运行的搜索管道。 该管道会在中间阶段拦截搜索结果，并通过处理流程对文档分数进行归一化和组合处理。\n要使用混合搜索，您需要配置搜索管道，添加混合排序处理器。\n混合排序处理器 #  混合排序处理器 hybrid_ranker_processor 是一种基于排名的搜索阶段结果处理器，运行在搜索执行的查询阶段和获取阶段之间。它会拦截查询阶段的结果，然后使用 倒数排序融合算法（RRF，Reciprocal Rank Fusion） 来合并不同查询子句，最终生成排序后的搜索结果列表。\n适用场景：\n 需要融合不同搜索技术（如关键词和语义搜索）的结果 子查询的原始分数不可直接比较（如 BM25 和 KNN）  算法原理 #  RRF 是一种多查询融合方法，其核心计算逻辑为：\n  对每个文档在不同子结果集给出的排名取倒数（如排名第 k 则得分为 1/(k+60)） 将各子结果集的倒数得分相加，生成统一排序分数 按最终分数降序输出结果集   RRF 的通用计算公式如下（其中 k 为平滑常数，默认 60，query_j_rank 表示混合查询中某文档在第 j 种查询方法返回结果中的排名）：\nrankScore(document_i) = sum(1/(k + query_1_rank), 1/(k + query_2_rank), ..., 1/(k + query_j_rank)) 请求体字段 #  下表列出了所有可用的请求字段。\n   字段 数据类型 说明     combination.technique String 必填。指定分数组合方式，目前仅支持 rrf（倒数排序融合）。   combination.rank_constant Integer 可选。计算倒数得分前，加到文档排名的常量值（必须 ≥ 1）。默认值：60。\n- 较大值（如 100）会使分数更均匀，降低高排名结果的影响。\n- 较小值（如 10）会增大排名间的分数差异，使高排名结果更具优势。    （注：RRF 算法适用于混合搜索场景，能够平衡关键词搜索和语义搜索的排序结果，提升整体相关性。）\n创建混合排序处理器 #  以下请求创建一个搜索管道，其中包含混合排序处理器：\nPUT /_search/pipeline/rrf-pipeline { \u0026quot;phase_results_processors\u0026quot;: [ { \u0026quot;hybrid_ranker_processor\u0026quot;: { \u0026quot;combination\u0026quot;: { \u0026quot;technique\u0026quot;: \u0026quot;rrf\u0026quot;, \u0026quot;rank_constant\u0026quot;: 60 } } } ] } rrf-pipeline 仅用于演示，实际使用时建议配置混合搜索管道\n配置混合搜索管道 #  为充分发挥混合搜索的优势，建议配置语义查询增强处理器 semantic_query_enricher，通过结合关键词搜索的精确匹配能力和语义搜索的上下文理解能力，提升整体搜索效果。\n下面这个请求是在 Easysearch 中创建一个名为 search_model_aliyun 的搜索管道(search pipeline)。搜索管道允许你在搜索请求的不同阶段插入自定义逻辑。\nPUT /_search/pipeline/search_model_aliyun { \u0026quot;request_processors\u0026quot;: [ { \u0026quot;semantic_query_enricher\u0026quot; : { \u0026quot;tag\u0026quot;: \u0026quot;tag1\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;aliyun search embedding model\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings\u0026quot;, \u0026quot;vendor\u0026quot;: \u0026quot;openai\u0026quot;, \u0026quot;api_key\u0026quot;: \u0026quot;\u0026lt;api_key\u0026gt;\u0026quot;, \u0026quot;default_model_id\u0026quot;: \u0026quot;text-embedding-v4\u0026quot;, \u0026quot;vector_field_model_id\u0026quot;: { \u0026quot;text_vector\u0026quot;: \u0026quot;text-embedding-v4\u0026quot; } } } ], \u0026quot;phase_results_processors\u0026quot;: [ { \u0026quot;hybrid_ranker_processor\u0026quot;: { \u0026quot;combination\u0026quot;: { \u0026quot;technique\u0026quot;: \u0026quot;rrf\u0026quot;, \u0026quot;rank_constant\u0026quot;: 60 } } } ], \u0026quot;response_processors\u0026quot;: [ { \u0026quot;hybrid_score_explanation\u0026quot;: {} } ] } 搜索管道结构： #  请求处理器 (request_processors) #  { \u0026quot;semantic_query_enricher\u0026quot;: {} } 这部分配置了一个语义查询增强器，将查询文本转换为向量表示，用于后续的向量搜索。\n查询阶段结果处理器 (phase_results_processors) #  { \u0026quot;hybrid_ranker_processor\u0026quot;: {} } 这部分配置了一个混合排序处理器。\n响应处理器 (response_processors)，可选 #  { \u0026quot;hybrid_score_explanation\u0026quot;: {} } 这部分配置了一个混合分数解释器，它会：\n 在搜索结果中添加解释信息 帮助理解不同部分(如文本匹配和语义相似度)对最终得分的贡献  使用混合查询 API 进行搜索 #  将 search_model_aliyun 设置为默认搜索管道\nPUT /my-index/_settings { \u0026quot;index.search.default_pipeline\u0026quot; : \u0026quot;search_model_aliyun\u0026quot; } 执行混合搜索 以下示例请求组合了两个查询子句： 语义查询和匹配查询。它使用上一步设置的默认搜索管道进行查询\nGET /my-index/_search { \u0026quot;_source\u0026quot;: { \u0026quot;exclude\u0026quot;: [ \u0026quot;text_vector\u0026quot; ] }, \u0026quot;query\u0026quot;: { \u0026quot;hybrid\u0026quot;: { \u0026quot;queries\u0026quot;: [ { \u0026quot;match\u0026quot;: { \u0026quot;input_text\u0026quot;: \u0026quot;夏季旅游首选\u0026quot; } }, { \u0026quot;semantic\u0026quot;: { \u0026quot;text_vector\u0026quot;: { \u0026quot;query_text\u0026quot;: \u0026quot;夏季旅游首选\u0026quot;, \u0026quot;candidates\u0026quot;: 10, \u0026quot;query_strategy\u0026quot;: \u0026quot;LSH_COSINE\u0026quot; } } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"混合搜索","url":"/easysearch/v1.15.0/docs/references/ai-integration/hybrid-search/"},{"category":null,"content":"复杂查询 #  子查询 (Subquery) #  子查询 (subquery) 是一个完整的 SELECT 语句，它被用在另一个语句中，并用括号括起来。从 explain 输出中，您可以注意到一些子查询实际上被转换为等效的联接查询来执行。\n示例 1：表子查询 #  SQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT a1.firstname, a1.lastname, a1.balance FROM accounts a1 WHERE a1.account_number IN ( SELECT a2.account_number FROM accounts a2 WHERE a2.balance \u0026gt; 10000 ) \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;Physical Plan\u0026quot; : { \u0026quot;Project [ columns=[a1.balance, a1.firstname, a1.lastname] ]\u0026quot; : { \u0026quot;Top [ count=200 ]\u0026quot; : { \u0026quot;BlockHashJoin[ conditions=( a1.account_number = a2.account_number ), type=JOIN, blockSize=[FixedBlockSize with size=10000] ]\u0026quot; : { \u0026quot;Scroll [ accounts as a2, pageSize=10000 ]\u0026quot; : { \u0026quot;request\u0026quot; : { \u0026quot;size\u0026quot; : 200, \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;must\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;must\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;must_not\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;must_not\u0026quot; : [ { \u0026quot;exists\u0026quot; : { \u0026quot;field\u0026quot; : \u0026quot;account_number\u0026quot;, \u0026quot;boost\u0026quot; : 1 } } ], \u0026quot;boost\u0026quot; : 1 } } ], \u0026quot;boost\u0026quot; : 1 } }, { \u0026quot;range\u0026quot; : { \u0026quot;balance\u0026quot; : { \u0026quot;include_lower\u0026quot; : false, \u0026quot;include_upper\u0026quot; : true, \u0026quot;from\u0026quot; : 10000, \u0026quot;boost\u0026quot; : 1, \u0026quot;to\u0026quot; : null } } } ], \u0026quot;boost\u0026quot; : 1 } } ], \u0026quot;boost\u0026quot; : 1 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1 } }, \u0026quot;from\u0026quot; : 0 } }, \u0026quot;Scroll [ accounts as a1, pageSize=10000 ]\u0026quot; : { \u0026quot;request\u0026quot; : { \u0026quot;size\u0026quot; : 200, \u0026quot;from\u0026quot; : 0, \u0026quot;_source\u0026quot; : { \u0026quot;excludes\u0026quot; : [ ], \u0026quot;includes\u0026quot; : [ \u0026quot;firstname\u0026quot;, \u0026quot;lastname\u0026quot;, \u0026quot;balance\u0026quot;, \u0026quot;account_number\u0026quot; ] } } }, \u0026quot;useTermsFilterOptimization\u0026quot; : false } } } }, \u0026quot;description\u0026quot; : \u0026quot;Hash Join algorithm builds hash table based on result of first query, and then probes hash table to find matched rows for each row returned by second query\u0026quot;, \u0026quot;Logical Plan\u0026quot; : { \u0026quot;Project [ columns=[a1.balance, a1.firstname, a1.lastname] ]\u0026quot; : { \u0026quot;Top [ count=200 ]\u0026quot; : { \u0026quot;Join [ conditions=( a1.account_number = a2.account_number ) type=JOIN ]\u0026quot; : { \u0026quot;Group\u0026quot; : [ { \u0026quot;Project [ columns=[a1.balance, a1.firstname, a1.lastname, a1.account_number] ]\u0026quot; : { \u0026quot;TableScan\u0026quot; : { \u0026quot;tableAlias\u0026quot; : \u0026quot;a1\u0026quot;, \u0026quot;tableName\u0026quot; : \u0026quot;accounts\u0026quot; } } }, { \u0026quot;Project [ columns=[a2.account_number] ]\u0026quot; : { \u0026quot;Filter [ conditions=[AND ( AND account_number ISN null, AND balance GT 10000 ) ] ]\u0026quot; : { \u0026quot;TableScan\u0026quot; : { \u0026quot;tableAlias\u0026quot; : \u0026quot;a2\u0026quot;, \u0026quot;tableName\u0026quot; : \u0026quot;accounts\u0026quot; } } } } ] } } } } }  结果集：\n   a1.firstname a1.lastname a1.balance     Amber Duke 39225   Nanette Bates 32838    示例 2：FROM 子句中的子查询 #  SQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT a.f, a.l, a.a FROM ( SELECT firstname AS f, lastname AS l, age AS a FROM accounts WHERE age \u0026gt; 30 ) AS a \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;must\u0026quot; : [ { \u0026quot;range\u0026quot; : { \u0026quot;age\u0026quot; : { \u0026quot;from\u0026quot; : 30, \u0026quot;to\u0026quot; : null, \u0026quot;include_lower\u0026quot; : false, \u0026quot;include_upper\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } }, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;firstname\u0026quot;, \u0026quot;lastname\u0026quot;, \u0026quot;age\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] } }  结果集：\n   f l a     Amber Duke 32   Dale Adams 33   Hattie Bond 36    下面是子查询中使用聚合函数和 GROUP BY 的另一个示例：\nos\u0026gt; SELECT avg_balance FROM ( ... SELECT AVG(balance) AS avg_balance FROM accounts GROUP BY gender, age ... ) AS a; fetched rows / total rows = 4/4 +---------------+ | avg_balance | |---------------| | 32838.0 | | 39225.0 | | 4180.0 | | 5686.0 | +---------------+  多层子查询的查询也支持，以下是一个示例：\nos\u0026gt; SELECT name FROM ( ... SELECT lastname AS name, age FROM ( ... SELECT * FROM accounts WHERE gender = 'M' ... ) AS accounts WHERE age \u0026lt; 35 ... ) AS accounts fetched rows / total rows = 2/2 +--------+ | name | |--------| | Duke | | Adams | +--------+  JOIN #  JOIN 子句通过使用每个索引共有的值来组合一个或多个索引的列。\n语法 #  tableSource规则：\njoinPart规则：\n示例 1：内联接 (Inner Join) #  内联接是一种非常常用的联接方式，它根据指定的联接谓词，通过组合两个索引的列来创建一个新的结果集。它遍历两个索引，并比较每个文档以找到满足联接谓词的所有文档。使用 JOIN 关键字，并可以选择性地在其前面加上 INNER 关键字。联接谓词通过 ON 子句指定。\n注意，对于联接查询，explain API 输出看起来很复杂。这是因为一个联接查询关联了两个底层的 Easysearch DSL 查询，并在单独的查询计划框架中执行。你可以通过查看逻辑计划和物理计划来解释它。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT a.account_number, a.firstname, a.lastname, e.id, e.name FROM accounts a JOIN employees_nested e ON a.account_number = e.id \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;Physical Plan\u0026quot; : { \u0026quot;Project [ columns=[a.account_number, a.firstname, a.lastname, e.name, e.id] ]\u0026quot; : { \u0026quot;Top [ count=200 ]\u0026quot; : { \u0026quot;BlockHashJoin[ conditions=( a.account_number = e.id ), type=JOIN, blockSize=[FixedBlockSize with size=10000] ]\u0026quot; : { \u0026quot;Scroll [ employees_nested as e, pageSize=10000 ]\u0026quot; : { \u0026quot;request\u0026quot; : { \u0026quot;size\u0026quot; : 200, \u0026quot;from\u0026quot; : 0, \u0026quot;_source\u0026quot; : { \u0026quot;excludes\u0026quot; : [ ], \u0026quot;includes\u0026quot; : [ \u0026quot;id\u0026quot;, \u0026quot;name\u0026quot; ] } } }, \u0026quot;Scroll [ accounts as a, pageSize=10000 ]\u0026quot; : { \u0026quot;request\u0026quot; : { \u0026quot;size\u0026quot; : 200, \u0026quot;from\u0026quot; : 0, \u0026quot;_source\u0026quot; : { \u0026quot;excludes\u0026quot; : [ ], \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot;, \u0026quot;firstname\u0026quot;, \u0026quot;lastname\u0026quot; ] } } }, \u0026quot;useTermsFilterOptimization\u0026quot; : false } } } }, \u0026quot;description\u0026quot; : \u0026quot;Hash Join algorithm builds hash table based on result of first query, and then probes hash table to find matched rows for each row returned by second query\u0026quot;, \u0026quot;Logical Plan\u0026quot; : { \u0026quot;Project [ columns=[a.account_number, a.firstname, a.lastname, e.name, e.id] ]\u0026quot; : { \u0026quot;Top [ count=200 ]\u0026quot; : { \u0026quot;Join [ conditions=( a.account_number = e.id ) type=JOIN ]\u0026quot; : { \u0026quot;Group\u0026quot; : [ { \u0026quot;Project [ columns=[a.account_number, a.firstname, a.lastname] ]\u0026quot; : { \u0026quot;TableScan\u0026quot; : { \u0026quot;tableAlias\u0026quot; : \u0026quot;a\u0026quot;, \u0026quot;tableName\u0026quot; : \u0026quot;accounts\u0026quot; } } }, { \u0026quot;Project [ columns=[e.name, e.id] ]\u0026quot; : { \u0026quot;TableScan\u0026quot; : { \u0026quot;tableAlias\u0026quot; : \u0026quot;e\u0026quot;, \u0026quot;tableName\u0026quot; : \u0026quot;employees_nested\u0026quot; } } } ] } } } } }  结果集：\n   a.account_number a.firstname a.lastname e.id e.name     6 Hattie Bond 6 Jane Smith    示例 2：交叉联接 (Cross Join) #  交叉联接或笛卡尔联接将第一个索引中的每个文档与第二个索引中的每个文档进行组合。结果集是来自两个索引的文档的笛卡尔积。它看起来像是没有使用 ON 子句来指定联接条件的内联接。\n注意：即使是在两个中等大小的索引上进行交叉联接也是有风险的。这可能会触发我们的断路器终止查询，以避免内存溢出问题。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT a.account_number, a.firstname, a.lastname, e.id, e.name FROM accounts a JOIN employees_nested e \u0026quot;\u0026quot;\u0026quot; }  结果集：\n   a.account_number a.firstname a.lastname e.id e.name     1 Amber Duke 3 Bob Smith   1 Amber Duke 4 Susan Smith   1 Amber Duke 6 Jane Smith   6 Hattie Bond 3 Bob Smith   6 Hattie Bond 4 Susan Smith   6 Hattie Bond 6 Jane Smith   13 Nanette Bates 3 Bob Smith   13 Nanette Bates 4 Susan Smith   13 Nanette Bates 6 Jane Smith   18 Dale Adams 3 Bob Smith   18 Dale Adams 4 Susan Smith   18 Dale Adams 6 Jane Smith    示例 3：外联接 (Outer Join) #  外联接用于保留一个或两个索引中的文档，尽管它们不满足联接谓词。目前，只支持 LEFT OUTER JOIN，以保留来自第一个索引的行。请注意，OUTER 关键字是可选的。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT a.account_number, a.firstname, a.lastname, e.id, e.name FROM accounts a LEFT JOIN employees_nested e ON a.account_number = e.id \u0026quot;\u0026quot;\u0026quot; }  结果集：\n   a.account_number a.firstname a.lastname e.id e.name     1 Amber Duke     6 Hattie Bond 6 Jane Smith   13 Nanette Bates     18 Dale Adams      ","subcategory":null,"summary":"","tags":null,"title":"复杂查询","url":"/easysearch/v1.15.0/docs/references/sql/complex/"},{"category":null,"content":"任务管理 #  任务是在集群中运行的任何操作。例如，搜索图书数据集以查找标题或作者姓名是一项任务。将自动创建任务以监视集群的运行状况和性能。有关集群中当前执行的所有任务的详细信息，可以使用 tasks API 操作。\n以下请求返回有关所有任务的信息：\nGET _tasks 通过包含任务 ID，您可以获得特定任务的信息。请注意，任务 ID 由节点的标识字符串和任务的数字 ID 组成。例如，如果节点的标识串是 nodestring ，任务的数字标识是 1234 ，则任务 ID 是 nodestring:1234 。您可以通过运行 tasks 操作来查找此信息。\nGET _tasks/\u0026lt;task_id\u0026gt; 请注意，如果任务完成运行，它将不会作为请求的一部分返回。对于一个需要稍长时间才能完成的任务的示例，可以在较大的文档上运行 _reindex API 操作，然后运行 tasks 。\nSample Response\n{ \u0026#34;nodes\u0026#34;: { \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;easy-node1\u0026#34;, \u0026#34;transport_address\u0026#34;: \u0026#34;30.18.0.3:9300\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;30.18.0.3\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;30.18.0.3:9300\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;data\u0026#34;, \u0026#34;ingest\u0026#34;, \u0026#34;master\u0026#34;, \u0026#34;remote_cluster_client\u0026#34;], \u0026#34;tasks\u0026#34;: { \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:17416\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;, \u0026#34;id\u0026#34;: 17416, \u0026#34;type\u0026#34;: \u0026#34;transport\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;cluster:monitor/tasks/lists\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1613599752458, \u0026#34;running_time_in_nanos\u0026#34;: 994000, \u0026#34;cancellable\u0026#34;: false, \u0026#34;headers\u0026#34;: {} }, \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:17413\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;, \u0026#34;id\u0026#34;: 17413, \u0026#34;type\u0026#34;: \u0026#34;transport\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;indices:data/write/bulk\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1613599752286, \u0026#34;running_time_in_nanos\u0026#34;: 30846500, \u0026#34;cancellable\u0026#34;: false, \u0026#34;parent_task_id\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:17366\u0026#34;, \u0026#34;headers\u0026#34;: {} }, \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:17366\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;, \u0026#34;id\u0026#34;: 17366, \u0026#34;type\u0026#34;: \u0026#34;transport\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;indices:data/write/reindex\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1613599750929, \u0026#34;running_time_in_nanos\u0026#34;: 1529733100, \u0026#34;cancellable\u0026#34;: true, \u0026#34;headers\u0026#34;: {} } } } } } 您还可以在查询中使用以下参数。\n   参数 数据类型 描述     nodes List 以逗号分隔的节点 ID 或名称列表，用于限制返回的信息。使用 _local 从要连接的节点返回信息，指定节点名称以从特定节点获取信息，或将参数保持为空以从所有节点获取信息。   actions List 应返回的操作的逗号分隔列表。保留为空以返回全部。   detailed Boolean 返回详细的任务信息。（Default: false）   parent_task_id String 返回具有指定父任务 ID（node_id:task_number）的任务。保持为空或设置为 -1 以返回全部。   wait_for_completion Boolean 等待匹配的任务完成。（Default: false）   group_by Enum 按父/子关系或节点对任务进行分组。（Default: nodes）   timeout Time 显式操作超时。（Default: 30 seconds）   master_timeout Time 等待连接到主节点的时间。（Default: 30 seconds）    例如，此请求返回当前在名为 easy-node1 的节点上运行的任务。\n请求示例\nGET /_tasks?nodes=easy-node1 响应示例\n{ \u0026#34;nodes\u0026#34;: { \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;easy-node1\u0026#34;, \u0026#34;transport_address\u0026#34;: \u0026#34;sample_address\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;sample_host\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;sample_ip\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;data\u0026#34;, \u0026#34;ingest\u0026#34;, \u0026#34;master\u0026#34;, \u0026#34;remote_cluster_client\u0026#34;], \u0026#34;tasks\u0026#34;: { \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:24578\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;, \u0026#34;id\u0026#34;: 24578, \u0026#34;type\u0026#34;: \u0026#34;transport\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;cluster:monitor/tasks/lists\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1611612517044, \u0026#34;running_time_in_nanos\u0026#34;: 638700, \u0026#34;cancellable\u0026#34;: false, \u0026#34;headers\u0026#34;: {} }, \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:24579\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ\u0026#34;, \u0026#34;id\u0026#34;: 24579, \u0026#34;type\u0026#34;: \u0026#34;direct\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;cluster:monitor/tasks/lists[n]\u0026#34;, \u0026#34;start_time_in_millis\u0026#34;: 1611612517044, \u0026#34;running_time_in_nanos\u0026#34;: 222200, \u0026#34;cancellable\u0026#34;: false, \u0026#34;parent_task_id\u0026#34;: \u0026#34;Mgqdm0f9SEGClWxp_RdnaQ:24578\u0026#34;, \u0026#34;headers\u0026#34;: {} } } } } } 放弃任务 #  获取任务列表后，您可以使用以下请求取消所有可取消的任务：\nPOST _tasks/_cancel 请注意，并非所有任务都可以取消。要查看任务是否可取消，请参阅对 tasks API 请求的响应中的 cancellable 字段。\n您还可以通过包含特定任务 ID 来取消任务。\nPOST _tasks/\u0026lt;task_id\u0026gt;/_cancel cancel 操作支持与 tasks 操作相同的参数。以下示例显示如何取消多个节点上的所有可取消任务。\nPOST _tasks/_cancel?nodes=easy-node1,easy-node2 将头信息附加到任务 #  为了将请求与任务关联起来以便更好地跟踪，可以在 curl 命令的 HTTPS 请求读取器中提供 X-Opaque-Id:\u0026lt;Id_number\u0026gt; header 。API 将在返回的结果中附加指定的头。\nUsage:\ncurl -i -H \u0026#34;X-Opaque-Id: 111111\u0026#34; \u0026#34;https://localhost:9200/_tasks\u0026#34; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; --insecure _tasks 操作返回以下结果。\nHTTP/1.1 200 OK X-Opaque-Id: 111111 content-type: application/json; charset=UTF-8 content-length: 768 { \u0026quot;nodes\u0026quot;: { \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;easy-node1\u0026quot;, \u0026quot;transport_address\u0026quot;: \u0026quot;30.18.0.4:9300\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;30.18.0.4\u0026quot;, \u0026quot;ip\u0026quot;: \u0026quot;30.18.0.4:9300\u0026quot;, \u0026quot;roles\u0026quot;: [ \u0026quot;data\u0026quot;, \u0026quot;ingest\u0026quot;, \u0026quot;master\u0026quot;, \u0026quot;remote_cluster_client\u0026quot; ], \u0026quot;tasks\u0026quot;: { \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ:30072\u0026quot;: { \u0026quot;node\u0026quot;: \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ\u0026quot;, \u0026quot;id\u0026quot;: 30072, \u0026quot;type\u0026quot;: \u0026quot;direct\u0026quot;, \u0026quot;action\u0026quot;: \u0026quot;cluster:monitor/tasks/lists[n]\u0026quot;, \u0026quot;start_time_in_millis\u0026quot;: 161316670305, \u0026quot;running_time_in_nanos\u0026quot;: 245400, \u0026quot;cancellable\u0026quot;: false, \u0026quot;parent_task_id\u0026quot;: \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ:30071\u0026quot;, \u0026quot;headers\u0026quot;: { \u0026quot;X-Opaque-Id\u0026quot;: \u0026quot;111111\u0026quot; } }, \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ:30071\u0026quot;: { \u0026quot;node\u0026quot;: \u0026quot;Mgqdm0f9SEGClWxp_RdnaQ\u0026quot;, \u0026quot;id\u0026quot;: 30071, \u0026quot;type\u0026quot;: \u0026quot;transport\u0026quot;, \u0026quot;action\u0026quot;: \u0026quot;cluster:monitor/tasks/lists\u0026quot;, \u0026quot;start_time_in_millis\u0026quot;: 161316670305, \u0026quot;running_time_in_nanos\u0026quot;: 658200, \u0026quot;cancellable\u0026quot;: false, \u0026quot;headers\u0026quot;: { \u0026quot;X-Opaque-Id\u0026quot;: \u0026quot;111111\u0026quot; } } } } } } 此操作支持与 任务 操作相同的参数。以下示例显示了如何将 X-Opaque-Id 与特定任务相关联。\ncurl -i -H \u0026#34;X-Opaque-Id: 123456\u0026#34; \u0026#34;https://localhost:9200/_tasks?nodes=easy-node1\u0026#34; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; --insecure ","subcategory":null,"summary":"","tags":null,"title":"任务管理","url":"/easysearch/v1.15.0/docs/references/management/tasksapis/"},{"category":null,"content":"版本发布日志 #  这里是 INFINI Easysearch-client 历史版本发布的相关说明。\n2.0.2(2024-08-13) #  Improvements #   升级相关依赖项至安全版本  2.0.0(2024-04-17) #  Breaking changes #  Features #   发布全新的 Easysearch java 客户端 2.0 版本。 客户端经过完全重构，更加轻量级，避免冗余的第三方依赖。 为常用 Easysearch API 提供强类型的请求和响应。 API 均支持阻塞和异步方式。 使用流式构建器和函数式模式，以在创建复杂嵌套结构时编写简洁易读的代码。 通过使用 Jackson 无缝集成应用程序类。 自带 Java 低级别 REST 客户端，处理所有传输级别的问题：HTTP 连接池、重试、节点发现等。  Bug fix #  Improvements #  1.0.1(2023-11-14) #  Breaking changes #  Features #   正式发布 Easysearch Java 客户端。这一里程碑式的更新为开发人员带来了前所未有的便利性，使得与 Easysearch 集群的交互变得更加简洁和直观。现在，通过 Easysearch-client 客户端，开发者可以直接使用 Java 方法和数据结构来进行交互，而不再需要依赖于传统的 HTTP 方法和 JSON。这一变化大大简化了操作流程，使得数据管理和索引更加高效。高级客户端的功能范围包括处理数据操作，管理集群，包括查看和维护集群的健康状态，并对 Security 模块全面兼容。它提供了一系列 API，用于管理角色、用户、权限、角色映射和账户。这意味着安全性和访问控制现在可以更加细粒度地管理，确保了数据的安全性和合规性。  Bug fix #  Improvements #  ","subcategory":null,"summary":"","tags":null,"title":"Easysearch-client","url":"/easysearch/v1.15.0/docs/release-notes/client/"},{"category":null,"content":"过滤查询处理器（filter query processor） #  版本引入：1.14.0\nfilter_query 搜索请求处理器用于拦截搜索请求，并向该请求中添加一个额外的查询条件，从而对搜索结果进行过滤。当你不希望重写应用程序中已有的查询语句，但又需要对结果进行额外过滤时，此功能非常有用。\n请求体字段 #  下表列出了所有可用的请求字段。\n   字段 数据类型 说明     query 对象 使用 Easysearch 查询领域特定语言（DSL）编写的查询语句。必填。   tag 字符串 处理器的唯一标识符。可选。   description 字符串 对该处理器的描述信息。可选。   ignore_failure 布尔值 若为 true，则当此处理器执行失败时，Easysearch 将忽略该错误并继续执行搜索管道中的其余处理器。可选，默认值为 false。    示例 #  以下示例演示如何在搜索管道中使用 filter_query 处理器。\n准备工作 #  创建一个名为 my_index 的索引，并索引两个文档：一个公开，一个私有：\nPOST /my_index/_doc/1 { \u0026quot;message\u0026quot;: \u0026quot;This is a public message\u0026quot;, \u0026quot;visibility\u0026quot;: \u0026quot;public\u0026quot; } POST /my_index/_doc/2 { \u0026quot;message\u0026quot;: \u0026quot;This is a private message\u0026quot;, \u0026quot;visibility\u0026quot;: \u0026quot;private\u0026quot; } 创建搜索管道 #  以下请求创建一个名为 my_pipeline 的搜索管道，其中包含一个 filter_query 请求处理器。该处理器使用 term 查询，仅返回可见性为“public”的文档：\nPUT /_search/pipeline/my_pipeline { \u0026quot;request_processors\u0026quot;: [ { \u0026quot;filter_query\u0026quot;: { \u0026quot;tag\u0026quot;: \u0026quot;tag1\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;此处理器将限制只返回可见性为公开的文档\u0026quot;, \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;visibility\u0026quot;: \u0026quot;public\u0026quot; } } } } ] } 使用搜索管道 #  在不使用搜索管道的情况下，对 my_index 索引执行搜索：\nGET /my_index/_search 响应结果包含两个文档：\n 响应  { \u0026quot;took\u0026quot;: 47, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 2, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 1.0, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;my_index\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 1.0, \u0026quot;_source\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;This is a public message\u0026quot;, \u0026quot;visibility\u0026quot;: \u0026quot;public\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;my_index\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot;: 1.0, \u0026quot;_source\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;This is a private message\u0026quot;, \u0026quot;visibility\u0026quot;: \u0026quot;private\u0026quot; } } ] } }  要使用搜索管道，请在请求中通过 search_pipeline 查询参数指定管道名称：\nGET /my_index/_search?search_pipeline=my_pipeline 此时响应仅包含 visibility 为 public 的文档：\n 响应  { \u0026quot;took\u0026quot;: 19, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 1, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 0.0, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;my_index\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.0, \u0026quot;_source\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;This is a public message\u0026quot;, \u0026quot;visibility\u0026quot;: \u0026quot;public\u0026quot; } } ] } }   ","subcategory":null,"summary":"","tags":null,"title":"过滤查询处理器","url":"/easysearch/v1.15.0/docs/references/search/search-pipelines/search-processors/filter-query-processor/"},{"category":null,"content":"跨集群搜索 #  跨集群搜索正是它听起来的样子：它允许集群中的任何节点对其他集群执行搜索请求。Easysearch 支持开箱即用的跨集群搜索。\n身份验证流程 #  当跨集群搜索通过 协调集群 访问 远程集群 时：\n 安全模块对协调集群上的用户进行身份验证。 安全模块在协调集群上获取用户的后端角色。 请求调用（包括经过身份验证的用户）将转发到远程集群。 在远程群集上评估用户的权限。  远程群集和协调集群可以分别配置不同的身份验证和授权配置，但我们建议在两者上使用相同的设置。\n权限信息 #  要查询远程集群上的索引，除了 READ 或 SEARCH 权限外，用户还需要具有以下索引权限：\nindices:admin/shards/search_shards role.yml 样例配置 #  humanresources: cluster: - CLUSTER_COMPOSITE_OPS_RO indices: \u0026#34;humanresources\u0026#34;: \u0026#34;*\u0026#34;: - READ - indices:admin/shards/search_shards # needed for CCS 配置流程 #  分别启动两个集群，如下：\n➜ curl -k \u0026#39;https://localhost:9200/_cluster/health?pretty\u0026#39; -u admin:xxxxxxxxxxxx { \u0026#34;cluster_name\u0026#34; : \u0026#34;easysearch\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;green\u0026#34;, \u0026#34;timed_out\u0026#34; : false, \u0026#34;number_of_nodes\u0026#34; : 1, \u0026#34;number_of_data_nodes\u0026#34; : 1, \u0026#34;active_primary_shards\u0026#34; : 1, \u0026#34;active_shards\u0026#34; : 1, \u0026#34;relocating_shards\u0026#34; : 0, \u0026#34;initializing_shards\u0026#34; : 0, \u0026#34;unassigned_shards\u0026#34; : 0, \u0026#34;delayed_unassigned_shards\u0026#34; : 0, \u0026#34;number_of_pending_tasks\u0026#34; : 0, \u0026#34;number_of_in_flight_fetch\u0026#34; : 0, \u0026#34;task_max_waiting_in_queue_millis\u0026#34; : 0, \u0026#34;active_shards_percent_as_number\u0026#34; : 100.0 } ➜ curl -k \u0026#39;https://localhost:9201/_cluster/health?pretty\u0026#39; -u admin:xxxxxxxxxxxx { \u0026#34;cluster_name\u0026#34; : \u0026#34;my-application22\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;green\u0026#34;, \u0026#34;timed_out\u0026#34; : false, \u0026#34;number_of_nodes\u0026#34; : 1, \u0026#34;number_of_data_nodes\u0026#34; : 1, \u0026#34;active_primary_shards\u0026#34; : 1, \u0026#34;active_shards\u0026#34; : 1, \u0026#34;relocating_shards\u0026#34; : 0, \u0026#34;initializing_shards\u0026#34; : 0, \u0026#34;unassigned_shards\u0026#34; : 0, \u0026#34;delayed_unassigned_shards\u0026#34; : 0, \u0026#34;number_of_pending_tasks\u0026#34; : 0, \u0026#34;number_of_in_flight_fetch\u0026#34; : 0, \u0026#34;task_max_waiting_in_queue_millis\u0026#34; : 0, \u0026#34;active_shards_percent_as_number\u0026#34; : 100.0 } 在协调群集上，添加远程群集名称和 IP 地址（端口为 9300）：\ncurl -k -XPUT -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9200/_cluster/settings\u0026#39; -d \u0026#39; { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.remote\u0026#34;: { \u0026#34;cluster1\u0026#34;: { \u0026#34;seeds\u0026#34;: [\u0026#34;127.0.0.1:9300\u0026#34;] }, \u0026#34;cluster2\u0026#34;: { \u0026#34;seeds\u0026#34;: [\u0026#34;127.0.0.1:9301\u0026#34;] } } } }\u0026#39; 在远程集群内索引一个文档：\ncurl -XPUT -k -H \u0026#39;Content-Type: application/json\u0026#39; -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/books/_doc/1\u0026#39; -d \u0026#39;{\u0026#34;Dracula\u0026#34;: \u0026#34;Bram Stoker\u0026#34;}\u0026#39; At this point, cross-cluster search works. You can test it using the admin user:\n✗ curl -XGET -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9200/cluster2:books/_search?pretty\u0026#39; { \u0026#34;took\u0026#34; : 57, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;_clusters\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;cluster2:books\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;Dracula\u0026#34; : \u0026#34;Bram Stoker\u0026#34; } } ] } } To continue testing, create a new user on both clusters:\ncurl -XPUT -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9200/_security/user/booksuser\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;password\u0026#34;:\u0026#34;password\u0026#34;}\u0026#39; curl -XPUT -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; \u0026#39;https://localhost:9201/_security/user/booksuser\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;password\u0026#34;:\u0026#34;password\u0026#34;}\u0026#39; Then run the same search as before with booksuser :\ncurl -XGET -k -u booksuser:password \u0026#39;https://localhost:9200/cluster2:books/_search?pretty\u0026#39; { \u0026#34;error\u0026#34; : { \u0026#34;root_cause\u0026#34; : [ { \u0026#34;type\u0026#34; : \u0026#34;security_exception\u0026#34;, \u0026#34;reason\u0026#34; : \u0026#34;no permissions for [indices:admin/shards/search_shards, indices:data/read/search] and User [name=booksuser, roles=[], requestedTenant=null]\u0026#34; } ], \u0026#34;type\u0026#34; : \u0026#34;security_exception\u0026#34;, \u0026#34;reason\u0026#34; : \u0026#34;no permissions for [indices:admin/shards/search_shards, indices:data/read/search] and User [name=booksuser, roles=[], requestedTenant=null]\u0026#34; }, \u0026#34;status\u0026#34; : 403 } 请注意权限错误。在远程群集上，创建具有适当权限的角色，并将 booksuser 映射到该角色：\ncurl -XPUT -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; \u0026#39;https://localhost:9200/_security/role/booksrole\u0026#39; -d \u0026#39;{\u0026#34;indices\u0026#34;:[{\u0026#34;names\u0026#34;:[\u0026#34;books\u0026#34;],\u0026#34;privileges\u0026#34;:[\u0026#34;indices:admin/shards/search_shards\u0026#34;,\u0026#34;indices:data/read/search\u0026#34;]}]}\u0026#39; curl -XPUT -k -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; \u0026#39;https://localhost:9200/_security/role_mapping/booksrole\u0026#39; -d \u0026#39;{\u0026#34;users\u0026#34; : [\u0026#34;booksuser\u0026#34;]}\u0026#39; curl -XPUT -k -u 'admin:xxxxxxxxxxxx' -H 'Content-Type: application/json' 'https://localhost:9201/_security/role/booksrole' -d '{\u0026quot;indices\u0026quot;:[{\u0026quot;names\u0026quot;:[\u0026quot;books\u0026quot;],\u0026quot;privileges\u0026quot;:[\u0026quot;indices:admin/shards/search_shards\u0026quot;,\u0026quot;indices:data/read/search\u0026quot;]}]}' curl -XPUT -k -u 'admin:xxxxxxxxxxxx' -H 'Content-Type: application/json' 'https://localhost:9201/_security/role_mapping/booksrole' -d '{\u0026quot;users\u0026quot; : [\u0026quot;booksuser\u0026quot;]}'\n两个集群都必须具有该用户，但只有远程集群需要角色和映射；在这种情况下，协调群集处理身份验证（即 “此请求是否包含有效的用户凭据？”），远程群集处理授权（即 “此用户是否可以访问此数据？”）。\n重新搜索一次：\ncurl -XGET -k -u booksuser:password \u0026#39;https://localhost:9200/cluster2:books/_search?pretty\u0026#39; { \u0026#34;took\u0026#34; : 5, \u0026#34;timed_out\u0026#34; : false, \u0026#34;_shards\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0, \u0026#34;failed\u0026#34; : 0 }, \u0026#34;_clusters\u0026#34; : { \u0026#34;total\u0026#34; : 1, \u0026#34;successful\u0026#34; : 1, \u0026#34;skipped\u0026#34; : 0 }, \u0026#34;hits\u0026#34; : { \u0026#34;total\u0026#34; : { \u0026#34;value\u0026#34; : 1, \u0026#34;relation\u0026#34; : \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34; : 1.0, \u0026#34;hits\u0026#34; : [ { \u0026#34;_index\u0026#34; : \u0026#34;cluster2:books\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;Dracula\u0026#34; : \u0026#34;Bram Stoker\u0026#34; } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"跨集群搜索","url":"/easysearch/v1.15.0/docs/references/security/access-control/cross-cluster-search/"},{"category":null,"content":"[已废弃] 文本向量化 #  本文档描述的功能已不再支持，将在下个版本删除，请使用新的 写入数据文本向量化替代。\n本文档介绍如何在 Easysearch 中集成和使用预先部署的 Ollama 服务来生成文本嵌入向量。\n先决条件 #  需要预先部署好 Ollama 服务，现阶段集成的服务版本是 0.5.4。\n可以用以下命令测试服务是否正常：\ncurl http://localhost:11434/api/embed -d '{ \u0026quot;model\u0026quot;: \u0026quot;nomic-embed-text:latest\u0026quot;, \u0026quot;input\u0026quot;: \u0026quot;Why is the sky blue?\u0026quot; }' 配置 Ollama 服务 #  可以通过 ollama_url 配置项指定 Ollama 服务的地址。您可以通过以下 API 查看当前配置：\nGET _cluster/settings?flat_settings=true\u0026amp;include_defaults=true\u0026amp;filter_path=*.ollama_url 如果没有修改，会输出默认值：\n{ \u0026quot;defaults\u0026quot;: { \u0026quot;ollama_url\u0026quot;: \u0026quot;http://localhost:11434\u0026quot; } } REST API #  POST /_ai/embed { \u0026quot;model\u0026quot;: \u0026quot;模型名称\u0026quot;, \u0026quot;input\u0026quot;: \u0026quot;文本内容\u0026quot; } 请求示例 #  POST /_ai/embed { \u0026quot;model\u0026quot;: \u0026quot;nomic-embed-text:latest\u0026quot;, \u0026quot;input\u0026quot;: \u0026quot;Llamas are members of the camelid family\u0026quot; } 批量生成 Embeddings #  可以一次为多个文本生成嵌入向量。\n请求格式：\nPOST /_ai/embed { \u0026quot;model\u0026quot;: \u0026quot;模型名称\u0026quot;, \u0026quot;input\u0026quot;: [\u0026quot;文本1\u0026quot;, \u0026quot;文本2\u0026quot;, ...] } 示例响应 #  { \u0026quot;embeddings\u0026quot;: [ [ 0.00971285, 0.04449268, -0.14063065, 0.0013162489, ...... ], [ 0.010429025, 0.014321253, -0.12902334, -0.03530379, 0.046050403, 0.04550423, ...... ] ] } 对于批量处理，建议一次发送多个文本而不是多次调用。\n如果收到连接错误，请检查 ollama_url 配置是否正确。\n","subcategory":null,"summary":"","tags":null,"title":"文本向量化","url":"/easysearch/v1.15.0/docs/references/ai-integration/text-embeddings/"},{"category":null,"content":"全文搜索 #  全文搜索是对存储的单个文档进行搜索，这与基于数据库中的原始文本的常规搜索有所区别。全文搜索尝试通过检查每个文档中的所有单词来匹配搜索条件。 在 Easysearch 中，提供的全文查询使你能够搜索在索引过程中分析的文本字段。\nMatch Query #  在 Easysearch 中，Match 查询是执行全文搜索的标准查询。MATCHQUERY 和 MATCH_QUERY 都是用于执行匹配查询的函数。\n示例 1 #  这两个函数都可以接受字段名称作为第一个参数，文本作为第二个参数。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT account_number, address FROM accounts WHERE MATCH_QUERY(address, 'Holmes') \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;must\u0026quot; : [ { \u0026quot;match\u0026quot; : { \u0026quot;address\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;Holmes\u0026quot;, \u0026quot;operator\u0026quot; : \u0026quot;OR\u0026quot;, \u0026quot;prefix_length\u0026quot; : 0, \u0026quot;max_expansions\u0026quot; : 50, \u0026quot;fuzzy_transpositions\u0026quot; : true, \u0026quot;lenient\u0026quot; : false, \u0026quot;zero_terms_query\u0026quot; : \u0026quot;NONE\u0026quot;, \u0026quot;auto_generate_synonyms_phrase_query\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } }, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot;, \u0026quot;address\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] } }  结果集：\n   account_number address     1 880 Holmes Lane    示例 2 #  这两个函数也可以接受单个参数，并按以下方式使用。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT account_number, address FROM accounts WHERE address = MATCH_QUERY('Holmes') \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;must\u0026quot; : [ { \u0026quot;match\u0026quot; : { \u0026quot;address\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;Holmes\u0026quot;, \u0026quot;operator\u0026quot; : \u0026quot;OR\u0026quot;, \u0026quot;prefix_length\u0026quot; : 0, \u0026quot;max_expansions\u0026quot; : 50, \u0026quot;fuzzy_transpositions\u0026quot; : true, \u0026quot;lenient\u0026quot; : false, \u0026quot;zero_terms_query\u0026quot; : \u0026quot;NONE\u0026quot;, \u0026quot;auto_generate_synonyms_phrase_query\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } }, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot;, \u0026quot;address\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] } }  结果集：\n   account_number address     1 880 Holmes Lane    Multi-match Query #  除了针对单个字段进行匹配查询外，您还可以使用多个字段搜索文本。为此，提供了函数 MULTI_MATCH、 MULTIMATCH 和 MULTIMATCHQUERY。\n示例 #  每个前面提到的函数都接受一个 query 参数用于指定要搜索的文本，以及一个 fields 参数用于指定要搜索的字段名称或模式。例如，以下查询在名为 accounts 的索引中搜索名字或姓氏为 \u0026lsquo;Dale\u0026rsquo; 的文档：\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT firstname, lastname FROM accounts WHERE MULTI_MATCH('query'='Dale', 'fields'='*name') \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;must\u0026quot; : [ { \u0026quot;multi_match\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;Dale\u0026quot;, \u0026quot;fields\u0026quot; : [ \u0026quot;*name^1.0\u0026quot; ], \u0026quot;type\u0026quot; : \u0026quot;best_fields\u0026quot;, \u0026quot;operator\u0026quot; : \u0026quot;OR\u0026quot;, \u0026quot;slop\u0026quot; : 0, \u0026quot;prefix_length\u0026quot; : 0, \u0026quot;max_expansions\u0026quot; : 50, \u0026quot;zero_terms_query\u0026quot; : \u0026quot;NONE\u0026quot;, \u0026quot;auto_generate_synonyms_phrase_query\u0026quot; : true, \u0026quot;fuzzy_transpositions\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } }, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;firstname\u0026quot;, \u0026quot;lastname\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] } }  结果集：\n   firstname lastname     Dale Adams    Query String 查询 #  查询字符串查询是基于 Lucene 查询字符串语法解析和拆分提供的查询字符串。这种小语言支持逻辑连接词、通配符、正则表达式和邻近搜索。更多详细信息请参阅官方文档。请注意，如果在查询字符串中存在任何无效语法，则会抛出错误。\n示例 #  QUERY 函数接受查询字符串，并分别对符合查询字符串的文档返回true或false。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT account_number, address FROM accounts WHERE QUERY('address:Lane OR address:Street') \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;must\u0026quot; : [ { \u0026quot;query_string\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;address:Lane OR address:Street\u0026quot;, \u0026quot;fields\u0026quot; : [ ], \u0026quot;type\u0026quot; : \u0026quot;best_fields\u0026quot;, \u0026quot;default_operator\u0026quot; : \u0026quot;or\u0026quot;, \u0026quot;max_determinized_states\u0026quot; : 10000, \u0026quot;enable_position_increments\u0026quot; : true, \u0026quot;fuzziness\u0026quot; : \u0026quot;AUTO\u0026quot;, \u0026quot;fuzzy_prefix_length\u0026quot; : 0, \u0026quot;fuzzy_max_expansions\u0026quot; : 50, \u0026quot;phrase_slop\u0026quot; : 0, \u0026quot;escape\u0026quot; : false, \u0026quot;auto_generate_synonyms_phrase_query\u0026quot; : true, \u0026quot;fuzzy_transpositions\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } }, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot;, \u0026quot;address\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] } }  结果集：\n   account_number address     1 880 Holmes Lane   6 671 Bristol Street   13 789 Madison Street    Match Phrase 查询 #  匹配短语查询与匹配查询类似，但用于匹配确切的短语。为此，提供了函数 MATCHPHRASE、 MATCH_PHRASE 和 MATCHPHRASEQUERY。\n示例 #  SQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT account_number, address FROM accounts WHERE MATCH_PHRASE(address, '880 Holmes Lane') \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;filter\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;must\u0026quot; : [ { \u0026quot;match_phrase\u0026quot; : { \u0026quot;address\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;880 Holmes Lane\u0026quot;, \u0026quot;slop\u0026quot; : 0, \u0026quot;zero_terms_query\u0026quot; : \u0026quot;NONE\u0026quot;, \u0026quot;boost\u0026quot; : 1.0 } } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } }, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot;, \u0026quot;address\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] } }  结果集：\n   account_number address     1 880 Holmes Lane    Score Query #  Easysearch支持将过滤查询包装起来，以便在每个匹配的文档中返回相关性得分。可以使用 SCORE、 SCOREQUERY 和 SCORE_QUERY 来实现此功能。\n示例 #  第一个参数是匹配查询表达式，第二个参数是一个可选的浮点数，用于提高得分。默认值为1.0。除此之外，隐式变量_score也可用于返回每个文档的得分或用于排序。\nSQL query:\nPOST /_sql { \u0026quot;query\u0026quot; : \u0026quot;\u0026quot;\u0026quot; SELECT account_number, address, _score FROM accounts WHERE SCORE(MATCH_QUERY(address, 'Lane'), 0.5) OR SCORE(MATCH_QUERY(address, 'Street'), 100) ORDER BY _score \u0026quot;\u0026quot;\u0026quot; }  解释：\n{ \u0026quot;from\u0026quot; : 0, \u0026quot;size\u0026quot; : 200, \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot; : { \u0026quot;must\u0026quot; : [ { \u0026quot;bool\u0026quot; : { \u0026quot;should\u0026quot; : [ { \u0026quot;constant_score\u0026quot; : { \u0026quot;filter\u0026quot; : { \u0026quot;match\u0026quot; : { \u0026quot;address\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;Lane\u0026quot;, \u0026quot;operator\u0026quot; : \u0026quot;OR\u0026quot;, \u0026quot;prefix_length\u0026quot; : 0, \u0026quot;max_expansions\u0026quot; : 50, \u0026quot;fuzzy_transpositions\u0026quot; : true, \u0026quot;lenient\u0026quot; : false, \u0026quot;zero_terms_query\u0026quot; : \u0026quot;NONE\u0026quot;, \u0026quot;auto_generate_synonyms_phrase_query\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } }, \u0026quot;boost\u0026quot; : 0.5 } }, { \u0026quot;constant_score\u0026quot; : { \u0026quot;filter\u0026quot; : { \u0026quot;match\u0026quot; : { \u0026quot;address\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;Street\u0026quot;, \u0026quot;operator\u0026quot; : \u0026quot;OR\u0026quot;, \u0026quot;prefix_length\u0026quot; : 0, \u0026quot;max_expansions\u0026quot; : 50, \u0026quot;fuzzy_transpositions\u0026quot; : true, \u0026quot;lenient\u0026quot; : false, \u0026quot;zero_terms_query\u0026quot; : \u0026quot;NONE\u0026quot;, \u0026quot;auto_generate_synonyms_phrase_query\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } }, \u0026quot;boost\u0026quot; : 100.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } } ], \u0026quot;adjust_pure_negative\u0026quot; : true, \u0026quot;boost\u0026quot; : 1.0 } }, \u0026quot;_source\u0026quot; : { \u0026quot;includes\u0026quot; : [ \u0026quot;account_number\u0026quot;, \u0026quot;address\u0026quot;, \u0026quot;_score\u0026quot; ], \u0026quot;excludes\u0026quot; : [ ] }, \u0026quot;sort\u0026quot; : [ { \u0026quot;_score\u0026quot; : { \u0026quot;order\u0026quot; : \u0026quot;asc\u0026quot; } } ] }  结果集：\n   account_number address _score     1 880 Holmes Lane 0.5   6 671 Bristol Street 100   13 789 Madison Street 100    ","subcategory":null,"summary":"","tags":null,"title":"全文搜索","url":"/easysearch/v1.15.0/docs/references/sql/fulltext/"},{"category":null,"content":"重新索引数据 #  创建索引后，如果您需要进行广泛的更改，例如为每个文档添加一个新字段或合并多个索引以形成一个新的索引，而不是删除索引，使更改脱机，然后重新索引数据，则可以使用 reindex 操作。\n使用 reindex 操作，可以将通过查询选择的所有文档或文档子集复制到另一个索引。重新索引是一个 POST 操作。在最基本的形式中，指定源索引和目标索引。\n 重新编制索引可能是一项昂贵的操作，具体取决于源索引的大小。我们建议您通过将 number_of_replicas 设置为 0 来禁用目标索引中的副本，并在重新索引过程完成后重新启用它们。\n 重新索引所有文档 #  您可以将所有文档从一个索引复制到另一个索引。\n首先需要使用所需的字段映射和设置创建目标索引，或者可以从源索引中复制这些映射和设置：\nPUT destination { \u0026#34;mappings\u0026#34;:{ \u0026#34;Add in your desired mappings\u0026#34; }, \u0026#34;settings\u0026#34;:{ \u0026#34;Add in your desired settings\u0026#34; } } reindex 命令将所有文档从源索引复制到目标索引：\nPOST _reindex { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34; }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 如果尚未创建目标索引，则 reindex 操作将使用默认配置创建新的目标索引。\n从远程群集 reindex #  您可以从远程集群中的索引复制文档。使用 remote 选项指定远程主机名和所需的登录凭据。\n此命令会到达远程集群，使用用户名和密码登录，并将所有文档从该远程集群中的源索引复制到本地集群中的目标索引：\nPOST _reindex { \u0026#34;source\u0026#34;:{ \u0026#34;remote\u0026#34;:{ \u0026#34;host\u0026#34;:\u0026#34;https://\u0026lt;REST_endpoint_of_remote_cluster\u0026gt;:9200\u0026#34;, \u0026#34;username\u0026#34;:\u0026#34;YOUR_USERNAME\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;YOUR_PASSWORD\u0026#34; } }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 您可以指定以下选项：\n   选项 有效值 描述 必填     host String 远程集群的 REST 端点 Yes   username String 登录到远程集群的用户名 No   password String 登录到远程群集的密码 No   socket_timeout Time Unit 套接字读取的等待时间（默认为 30 秒） No   connect_timeout Time Unit 远程连接超时的等待时间（默认为 30 秒） No    重新索引文档子集 #  只能复制与搜索查询匹配的特定文档集。\n此命令仅将查询操作匹配的文档子集复制到目标索引：\nPOST _reindex { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;field_name\u0026#34;: \u0026#34;text\u0026#34; } } }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 有关所有查询操作的列表，请参见 全文查询。\n合并一个或多个索引 #  通过将源索引添加为列表，可以组合一个或多个索引中的文档。\n此命令将所有文档从两个源索引复制到一个目标索引：\nPOST _reindex { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:[ \u0026#34;source_1\u0026#34;, \u0026#34;source_2\u0026#34; ] }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 确保源索引和目标索引的碎片数量相同。\n仅重索引缺少的文档 #  通过将 op_type 选项设置为 create ，可以仅复制目标索引中缺少的文档。 在这种情况下，如果已经存在具有相同 ID 的文档，则操作将忽略源索引中的文档。 要忽略文档的所有版本冲突，请将 conflicts 选项设置为 proceed 。\nPOST _reindex { \u0026#34;conflicts\u0026#34;:\u0026#34;proceed\u0026#34;, \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34; }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34;, \u0026#34;op_type\u0026#34;:\u0026#34;create\u0026#34; } } 重新索引排序的文档 #  对文档中的特定字段进行排序后，可以复制某些文档。\n此命令基于 timestamp 字段复制最后 10 个文档：\nPOST _reindex { \u0026#34;size\u0026#34;:10, \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34;, \u0026#34;sort\u0026#34;:{ \u0026#34;timestamp\u0026#34;:\u0026#34;desc\u0026#34; } }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; } } 重新索引期间转换文档 #  您可以使用 script 选项在重新索引过程中转换数据。 我们建议在 Easysearch 中编写脚本时使用 Painless。\n此命令通过 Painless 脚本运行源索引，该脚本在将 account 对象复制到目标索引之前增加 number 字段：\nPOST _reindex { \u0026#34;source\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;source\u0026#34; }, \u0026#34;dest\u0026#34;:{ \u0026#34;index\u0026#34;:\u0026#34;destination\u0026#34; }, \u0026#34;script\u0026#34;:{ \u0026#34;lang\u0026#34;:\u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;:\u0026#34;ctx._account.number++\u0026#34; } } 您还可以指定一个摄取管道，以在重新索引过程中转换数据。\n首先必须创建一个定义了 processors 的管道。您可以在 ingest 管道中使用许多不同的 processors。\n这是一个示例摄取管道，它定义了一个 split 处理器，该处理器基于 space 分隔符拆分 text 字段，并将其存储在新的 word 字段中。 script 处理器是一个无痛脚本，它查找 word 字段的长度并将其存储在新的 word_count 字段中。 remove 处理器删除 test 字段。\nPUT _ingest/pipeline/pipeline-test { \u0026#34;description\u0026#34;: \u0026#34;Splits the text field into a list. Computes the length of the \u0026#39;word\u0026#39; field and stores it in a new \u0026#39;word_count\u0026#39; field. Removes the \u0026#39;test\u0026#39; field.\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;split\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;separator\u0026#34;: \u0026#34;\\\\s+\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;word\u0026#34; }, } { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;ctx.word_count = ctx.word.length\u0026#34; } }, { \u0026#34;remove\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;test\u0026#34; } } ] } 创建管道后，可以使用 reindex 操作：\nPOST _reindex { \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;source\u0026#34;, }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;destination\u0026#34;, \u0026#34;pipeline\u0026#34;: \u0026#34;pipeline-test\u0026#34; } } 更新当前索引中的文档 #  要更新当前索引中的数据而不将其复制到其他索引，请使用 update_by_query 操作。\nupdate_by_query 操作是一次可以对单个索引执行的 POST 操作。\nPOST \u0026lt;index_name\u0026gt;/_update_by_query 如果在没有参数的情况下运行此命令，则会增加索引中所有文档的版本号。\n源索引选项 #  可以为源索引指定以下选项：\n   选项 有效值 描述 必填     index String 源索引的名称。可以将多个源索引作为列表提供。 Yes   max_docs Integer 要重新索引的最大文档数。 No   query Object 用于重新索引操作的搜索查询。 No   size Integer 要重新索引的文档数。 No   slice String 指定手动或自动切片以并行化重新索引。 No   sort List 重新编制索引之前对文档中的特定字段进行排序。 No    目标索引选项 #  可以为目标索引指定以下选项：\n   选项 有效值 描述 必填     index String 目标索引的名称。 Yes   version_type Enum 索引操作的 version 类型。有效值：internal、external、extrnal_gt、external_gte。 No    ","subcategory":null,"summary":"","tags":null,"title":"重建数据","url":"/easysearch/v1.15.0/docs/references/management/reindex-data/"},{"category":null,"content":"身份模拟 #  用户模拟允许具备特定权限的用户以另外的身份来进行集群的访问。\n用户模拟可用于测试和故障排除，或允许系统服务安全地充当用户。\n在 REST 接口或 TCP 传输层上都可以进行用户模拟。\nREST 接口 #  要允许一个用户模拟另一个用户，请将以下内容添加到 easysearch.yml :\nsecurity.authcz.rest_impersonation_user: \u0026lt;AUTHENTICATED_USER\u0026gt;: - \u0026lt;IMPERSONATED_USER_1\u0026gt; - \u0026lt;IMPERSONATED_USER_2\u0026gt; 模拟用户字段支持通配符。将其设置为 * 允许 AUTHENTICATED_USER 来模拟任意用户。\n传输层配置 #  类似的配置方法如下：\nsecurity.authcz.impersonation_dn: \u0026#34;CN=spock,OU=client,O=client,L=Test,C=DE\u0026#34;: - worf 模拟其他用户 #  要模拟其他用户，请向系统提交请求，并将 HTTP 标头 security_run_as 设置为要模拟的用户的名称。例如：\ncurl -XGET -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; -k -H \u0026#34;security_run_as: user_1\u0026#34; https://localhost:9200/_security/authinfo?pretty ","subcategory":null,"summary":"","tags":null,"title":"身份模拟","url":"/easysearch/v1.15.0/docs/references/security/access-control/run-as/"},{"category":null,"content":"聚合查询 #  介绍 #  聚合函数作用于一组值。它们通常与GROUP BY子句一起使用，将值分组为子集。\nGROUP BY 子句 #  GROUP BY 表达式可以是：\n 标识符：Identifier 序数：Ordinal 表达式：Expression  标识符 #  group by 表达式可以是标识符：\nos\u0026gt; SELECT gender, sum(age) FROM accounts GROUP BY gender; fetched rows / total rows = 2/2 +----------+------------+ | gender | sum(age) | |----------+------------| | F | 28 | | M | 101 | +----------+------------+  序数 #  group by 表达式可以是序数：\nos\u0026gt; SELECT gender, sum(age) FROM accounts GROUP BY 1; fetched rows / total rows = 2/2 +----------+------------+ | gender | sum(age) | |----------+------------| | F | 28 | | M | 101 | +----------+------------+  group by 表达式可以是一个表达式。\nos\u0026gt; SELECT abs(account_number), sum(age) FROM accounts GROUP BY abs(account_number); fetched rows / total rows = 4/4 +-----------------------+------------+ | abs(account_number) | sum(age) | |-----------------------+------------| | 1 | 32 | | 13 | 28 | | 18 | 33 | | 6 | 36 | +-----------------------+------------+  聚合 #   聚合可以用于select。 聚合可以作为表达式的参数。 聚合可以包含表达式作为参数。  查询中的聚合 #  聚合可以用于select。\nos\u0026gt; SELECT gender, sum(age) FROM accounts GROUP BY gender; fetched rows / total rows = 2/2 +----------+------------+ | gender | sum(age) | |----------+------------| | F | 28 | | M | 101 | +----------+------------+  聚合表达式 #  聚合可以用作表达式的参数：\nos\u0026gt; SELECT gender, sum(age) * 2 as sum2 FROM accounts GROUP BY gender; fetched rows / total rows = 2/2 +----------+--------+ | gender | sum2 | |----------+--------| | F | 56 | | M | 202 | +----------+--------+  表达式作为聚合参数 #  聚合可以将表达式作为参数：\nos\u0026gt; SELECT gender, sum(age * 2) as sum2 FROM accounts GROUP BY gender; fetched rows / total rows = 2/2 +----------+--------+ | gender | sum2 | |----------+--------| | F | 56 | | M | 202 | +----------+--------+  COUNT 聚合 #  除了常规标识符，COUNT 聚合函数还接受诸如 * 或字面量如 1 的参数。这些不同形式的含义如下：\n COUNT(field) 只有当给定字段（或表达式）在输入行中不为 null 或缺失时才会计数。 COUNT(*) 将计算其所有输入行的数量。 COUNT(1) 与 COUNT(*) 相同，因为任何非 null 的字面量都会被计数。  聚合函数 #  COUNT #  用法：返回 SELECT 语句检索到的行中 expr 的计数。\nExample:\nos\u0026gt; SELECT gender, count(*) as countV FROM accounts GROUP BY gender; fetched rows / total rows = 2/2 +----------+----------+ | gender | countV | |----------+----------| | F | 1 | | M | 3 | +----------+----------+  SUM #  用法：SUM(expr). 返回 expr 的总和。\nExample:\nos\u0026gt; SELECT gender, sum(age) as sumV FROM accounts GROUP BY gender; fetched rows / total rows = 2/2 +----------+--------+ | gender | sumV | |----------+--------| | F | 28 | | M | 101 | +----------+--------+  AVG #  用法： AVG(expr). 返回 expr 的平均值。\nExample:\nos\u0026gt; SELECT gender, avg(age) as avgV FROM accounts GROUP BY gender; fetched rows / total rows = 2/2 +----------+--------------------+ | gender | avgV | |----------+--------------------| | F | 28.0 | | M | 33.666666666666664 | +----------+--------------------+  MAX #  用法: MAX(expr). 返回 expr 的最大值。\nExample:\nos\u0026gt; SELECT max(age) as maxV FROM accounts; fetched rows / total rows = 1/1 +--------+ | maxV | |--------| | 36 | +--------+  MIN #  用法: MIN(expr). 返回 expr 的最小值。\nExample:\nos\u0026gt; SELECT min(age) as minV FROM accounts; fetched rows / total rows = 1/1 +--------+ | minV | |--------| | 28 | +--------+  VAR_POP #  用法：VAR_POP(expr). 返回 expr 的总体标准方差。\nExample:\nos\u0026gt; SELECT var_pop(age) as varV FROM accounts; fetched rows / total rows = 1/1 +--------+ | varV | |--------| | 8.1875 | +--------+  VAR_SAMP #  用法：VAR_SAMP(expr). 返回 expr 的样本方差。\nExample:\nos\u0026gt; SELECT var_samp(age) as varV FROM accounts; fetched rows / total rows = 1/1 +--------------------+ | varV | |--------------------| | 10.916666666666666 | +--------------------+  VARIANCE #  用法：VARIANCE(expr). 返回 expr 的总体标准方差。VARIANCE() 是 VAR_POP() 函数的同义词。\nExample:\nos\u0026gt; SELECT variance(age) as varV FROM accounts; fetched rows / total rows = 1/1 +--------+ | varV | |--------| | 8.1875 | +--------+  STDDEV_POP #  用法：STDDEV_POP(expr). 返回 expr 的总体标准差。\nExample:\nos\u0026gt; SELECT stddev_pop(age) as stddevV FROM accounts; fetched rows / total rows = 1/1 +--------------------+ | stddevV | |--------------------| | 2.8613807855648994 | +--------------------+  STDDEV_SAMP #  用法：STDDEV_SAMP(expr). 返回 expr 的样本标准差。\nExample:\nos\u0026gt; SELECT stddev_samp(age) as stddevV FROM accounts; fetched rows / total rows = 1/1 +-------------------+ | stddevV | |-------------------| | 3.304037933599835 | +-------------------+  STD #  用法：STD(expr). 返回 expr 的总体标准差。STD() 是STDDEV_POP() 函数的同义词。\nExample:\nos\u0026gt; SELECT stddev_pop(age) as stddevV FROM accounts; fetched rows / total rows = 1/1 +--------------------+ | stddevV | |--------------------| | 2.8613807855648994 | +--------------------+  STDDEV #  用法：STDDEV(expr). 返回 expr 的总体标准差。STDDEV() 是 STDDEV_POP() 函数的同义词。\nExample:\nos\u0026gt; SELECT stddev(age) as stddevV FROM accounts; fetched rows / total rows = 1/1 +--------------------+ | stddevV | |--------------------| | 2.8613807855648994 | +--------------------+  DISTINCT COUNT 聚合 #  要获取一个字段不同值的计数，可以在count聚合中的字段前添加关键字 DISTINCT。\nExample:\nos\u0026gt; SELECT COUNT(DISTINCT gender), COUNT(gender) FROM accounts; fetched rows / total rows = 1/1 +--------------------------+-----------------+ | COUNT(DISTINCT gender) | COUNT(gender) | |--------------------------+-----------------| | 2 | 4 | +--------------------------+-----------------+  HAVING 子句 #  HAVING子句可以作为聚合过滤器，过滤掉不满足给定条件表达式的聚合值。\n带 GROUP BY 的 HAVING #  在 SELECT 子句中定义的聚合表达式或其别名可以在 HAVING 条件中使用。\n 尽管在 HAVING 子句中允许使用非聚合表达式，但建议在 WHERE 中使用非聚合表达式。 HAVING 子句中的聚合并不一定与选择列表中的聚合相同。作为对 SQL 标准的扩展，它也不限于只涉及 group by 列表上的标识符。  以下是一个典型使用 HAVING 子句的示例：\nos\u0026gt; SELECT ... gender, sum(age) ... FROM accounts ... GROUP BY gender ... HAVING sum(age) \u0026gt; 100; fetched rows / total rows = 1/1 +----------+------------+ | gender | sum(age) | |----------+------------| | M | 101 | +----------+------------+  以下是另一个在 HAVING 条件中使用别名的例子。请注意，如果一个标识符是模糊的，例如既作为选择别名又作为索引字段出现，那么优先考虑别名。这意味着标识符将被 SELECT 子句中的别名表达式替换：\nos\u0026gt; SELECT ... gender, sum(age) AS s ... FROM accounts ... GROUP BY gender ... HAVING s \u0026gt; 100; fetched rows / total rows = 1/1 +----------+-----+ | gender | s | |----------+-----| | M | 101 | +----------+-----+  不带 GROUP BY 的 HAVING #  此外，HAVING 子句也可以在没有GROUP BY 子句的情况下使用。这很有用，因为聚合表达式不能出现在 WHERE 子句中。\nos\u0026gt; SELECT ... 'Total of age \u0026gt; 100' ... FROM accounts ... HAVING sum(age) \u0026gt; 100; fetched rows / total rows = 1/1 +------------------------+ | 'Total of age \u0026gt; 100' | |------------------------| | Total of age \u0026gt; 100 | +------------------------+  FILTER 子句 #  FILTER 子句可以按照语法 aggregation_function(expr) FILTER(WHERE condition_expr) 为当前聚合存储桶设置特定条件。如果指定了过滤器，则只有过滤器子句中的条件计算结果为 true 的输入行才会馈送到聚合函数;其他行将被丢弃。 带筛选器子句的聚合只能在 SELECT 子句中使用。\n带 GROUP BY 的 FILTER #  带有 FILTER 子句的 group by 聚合可以为每个聚合桶设置不同的条件。以下是在 group by 聚合中使用 FILTER 的一个例子：\nos\u0026gt; SELECT avg(age) FILTER(WHERE balance \u0026gt; 10000) AS filtered, gender FROM accounts GROUP BY gender fetched rows / total rows = 2/2 +------------+----------+ | filtered | gender | |------------+----------| | 28.0 | F | | 32.0 | M | +------------+----------+  不带 GROUP BY 的 FILTER #  FILTER 子句也可以在没有 GROUP BY 的情况下用于聚合函数。例如：\nos\u0026gt; SELECT ... count(*) AS unfiltered, ... count(*) FILTER(WHERE age \u0026gt; 34) AS filtered ... FROM accounts fetched rows / total rows = 1/1 +--------------+------------+ | unfiltered | filtered | |--------------+------------| | 4 | 1 | +--------------+------------+  带有 FILTER 的 Distinct count 聚合\nFILTER 子句也用在 distinct count 中，在计算特定字段的不同值之前进行过滤。例如:\nos\u0026gt; SELECT COUNT(DISTINCT firstname) FILTER(WHERE age \u0026gt; 30) AS distinct_count FROM accounts fetched rows / total rows = 1/1 +------------------+ | distinct_count | |------------------| | 3 | +------------------+  ","subcategory":null,"summary":"","tags":null,"title":"聚合查询","url":"/easysearch/v1.15.0/docs/references/sql/aggregations/"},{"category":null,"content":"搜索请求文本向量化 #  Easysearch 使用搜索管道的 semantic_query_enricher 处理器，协助 semantic query，将文本转为向量。\n先决条件 #    服务兼容性 需满足以下任一条件：\n 支持与 OpenAI API 兼容的 embedding 接口 支持 Ollama embedding 接口    插件要求 必须安装 Easysearch 的以下插件：\nknn ai   数据准备 需预先完成：\n 创建向量索引 写入向量数据 参考 写入数据文本向量化    创建或更新 semantic_query_enricher 处理器 #  PUT /_search/pipeline/default_model_pipeline { \u0026quot;request_processors\u0026quot;: [ { \u0026quot;semantic_query_enricher\u0026quot; : { \u0026quot;tag\u0026quot;: \u0026quot;tag1\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Sets the default embedding model\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://api.openai.com/v1/embeddings\u0026quot;, \u0026quot;vendor\u0026quot;: \u0026quot;openai\u0026quot;, \u0026quot;api_key\u0026quot;: \u0026quot;\u0026lt;api_key\u0026gt;\u0026quot;, \u0026quot;default_model_id\u0026quot;: \u0026quot;text-embedding-3-small\u0026quot;, \u0026quot;vector_field_model_id\u0026quot;: { \u0026quot;text_vector\u0026quot;: \u0026quot;text-embedding-3-small\u0026quot; } } } ] } 请求体字段： #  下表列出了用于创建或更新管道的请求体字段。\n   参数 是否必填 类型 说明     request_processors 必填 数组 处理器列表，按顺序执行。示例中为 semantic_query_enricher。   tag 可选 字符串 处理器标签，用于调试或日志（如 \u0026quot;tag1\u0026quot;）。   description 可选 字符串 管道描述，用于说明用途（如 “Sets the default model ID at index and field levels”）。   url 必填 字符串 Embedding API 的完整 URL（如 https://poloai.top/v1/embeddings）。   vendor 必填 字符串 服务提供商标识，如 \u0026quot;openai\u0026quot;。   api_key 必填 字符串 API 密钥，需替换为实际值（如 \u0026quot;sk-xxx\u0026quot;）。   default_model_id 可选 字符串 全局默认模型 ID，当字段未在 vector_field_model_id 中配置时使用（如 text-embedding-3-small）。   vector_field_model_id 可选 对象 字段级模型映射，如 { \u0026quot;text_vector\u0026quot;: \u0026quot;text-embedding-3-small\u0026quot; }，优先级高于 default_model_id。    查看搜索管道 #  要查看刚创建的所有搜索管道，请使用以下请求：\nGET _search/pipeline/default_model_pipeline 响应包含您在上一节中设置的管道：\n{ \u0026quot;default_model_pipeline\u0026quot;: { \u0026quot;request_processors\u0026quot;: [ { \u0026quot;semantic_query_enricher\u0026quot;: { \u0026quot;api_key\u0026quot;: \u0026quot;*************************************************vE\u0026quot;, \u0026quot;vendor\u0026quot;: \u0026quot;openai\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Sets the default embedding model\u0026quot;, \u0026quot;tag\u0026quot;: \u0026quot;tag1\u0026quot;, \u0026quot;vector_field_model_id\u0026quot;: { \u0026quot;text_vector\u0026quot;: \u0026quot;text-embedding-3-small\u0026quot; }, \u0026quot;default_model_id\u0026quot;: \u0026quot;text-embedding-3-small\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://api.openai.com/v1/embeddings\u0026quot; } } ] } } api_key 会被遮掩，防止泄露\n设置默认搜索管道 #  可以将 default_model_pipeline 设置为索引默认的搜索管道，避免每次请求提供搜索管道参数。\nPUT /my-index/_settings { \u0026quot;index.search.default_pipeline\u0026quot; : \u0026quot;default_model_pipeline\u0026quot; } 使用 semantic query 进行搜索 #  GET /my-index/_search { \u0026quot;_source\u0026quot;: \u0026quot;input_text\u0026quot;, \u0026quot;query\u0026quot;: { \u0026quot;semantic\u0026quot;: { \u0026quot;text_vector\u0026quot;: { \u0026quot;query_text\u0026quot;: \u0026quot;这是另一个示例文本，这次使用 POST。\u0026quot;, \u0026quot;candidates\u0026quot;: 100, \u0026quot;query_strategy\u0026quot;: \u0026quot;LSH_COSINE\u0026quot; } } } } Semantic Query 参数说明 #     参数 类型 必填 默认值 说明     semantic string 是 - 查询类型标识，表示这是一个将文本自动转成向量的查询。   text_vector string 是 - 目标向量字段名（需在 mapping 中定义为 dense_vector 类型）   query_text string 是 - 需要编码为向量的文本内容   candidates integer 否 100 召回阶段每个 segment 考虑的候选文档数量，值越大召回率越高，性能开销越大   query_strategy enum 否 LSH_COSINE 近似最近邻算法策略，可选值：       - LSH_COSINE：局部敏感哈希 + 余弦相似度（model: lsh, similarity: cosine）       - LSH_L2：局部敏感哈希 + L2距离（model: lsh, similarity: l2）       - PERMUTATION_LSH：排列局部敏感哈希（model: permutation_lsh）    查询结果 #  { \u0026quot;took\u0026quot;: 3297, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 2, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;dewmA5gB0ulPrMzo2K-_\u0026quot;, \u0026quot;_score\u0026quot;: 2, \u0026quot;_source\u0026quot;: { \u0026quot;input_text\u0026quot;: \u0026quot;这是另一个示例文本，这次使用 POST。\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;dt2DAZgB6WXmvHRNYksX\u0026quot;, \u0026quot;_score\u0026quot;: 1.540483, \u0026quot;_source\u0026quot;: { \u0026quot;input_text\u0026quot;: \u0026quot;这是另一示例文本。\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;bulk_doc_2\u0026quot;, \u0026quot;_score\u0026quot;: 1.513369, \u0026quot;_source\u0026quot;: { \u0026quot;input_text\u0026quot;: \u0026quot;第二个批量处理的文本，指定了ID。\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;dd2DAZgB6WXmvHRNYksX\u0026quot;, \u0026quot;_score\u0026quot;: 1.4987004, \u0026quot;_source\u0026quot;: { \u0026quot;input_text\u0026quot;: \u0026quot;第一个批量处理的文本。\u0026quot; } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"搜索请求文本向量化","url":"/easysearch/v1.15.0/docs/references/ai-integration/search-text-embedding/"},{"category":null,"content":"雪球算法分词过滤器 #  雪球算法（snowball）分词过滤器是一种基于 雪球算法的词干提取过滤器。它支持多种语言，并且比波特词干提取算法更加高效和准确。\n参数说明 #  雪球分词过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：\n 阿拉伯语（Arabic） 亚美尼亚语（Armenian） 巴斯克语（Basque） 加泰罗尼亚语（Catalan） 丹麦语（Danish） 荷兰语（Dutch） 英语（English，默认值） 爱沙尼亚语（Estonian） 芬兰语（Finnish） 法语（French） 德语（German） 德语 2（German2） 匈牙利语（Hungarian） 意大利语（Italian） 爱尔兰语（Irish） Kp 立陶宛语（Lithuanian） 洛文斯（Lovins） 挪威语（Norwegian） 波特（Porter） 葡萄牙语（Portuguese） 罗马尼亚语（Romanian） 俄语（Russian） 西班牙语（Spanish） 瑞典语（Swedish） 土耳其语（Turkish）  参考样例 #  以下示例请求创建了一个名为 my-snowball-index 的新索引，并配置了一个带有雪球过滤器的分词器。\nPUT /my-snowball-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_snowball_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;snowball\u0026quot;, \u0026quot;language\u0026quot;: \u0026quot;English\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_snowball_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_snowball_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-snowball-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_snowball_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;running runners\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;run\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;runner\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"雪球算法分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/snowball/"},{"category":null,"content":"限制分词过滤器 #  限制（limit）分词过滤器用于限制分词链通过的词元数量。\n参数说明 #  限制分词过滤器可以使用以下参数进行配置。\n   参数 必填/可选 数据类型 描述     max_token_count 可选 整数 要生成的词元的最大数量。默认值为 1。   consume_all_tokens 可选 布尔值 （专家级设置）即使结果超过 max_token_count，也会使用来自词元生成器的所有词元。当设置此参数时，输出仍然只包含 max_token_count 指定数量的词元。不过，词元生成器生成的所有词元都会被处理。默认值为 false。    参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有限制过滤器的分析器：\nPUT my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;three_token_limit\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;custom_token_limit\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;custom_token_limit\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;limit\u0026quot;, \u0026quot;max_token_count\u0026quot;: 3 } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;three_token_limit\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch is a powerful and flexible search engine.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;a\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"限制分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/limit/"},{"category":null,"content":"长度分词过滤器 #  长度(length)分词过滤器用于从词元流中移除那些不符合指定长度标准（最小和最大长度值）的词元。\n参数说明 #  长度分词过滤器可以使用以下参数进行配置。\n   参数 必填/可选 数据类型 描述     min 可选 整数 词元的最小长度。默认值为 0。   max 可选 整数 词元的最大长度。默认值为整数类型的最大值（2147483647）。    参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有长度过滤器的分词器：\nPUT my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;only_keep_4_to_10_characters\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;whitespace\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;length_4_to_10\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;length_4_to_10\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;length\u0026quot;, \u0026quot;min\u0026quot;: 4, \u0026quot;max\u0026quot;: 10 } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;only_keep_4_to_10_characters\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch is a great tool!\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;great\u0026quot;, \u0026quot;start_offset\u0026quot;: 16, \u0026quot;end_offset\u0026quot;: 21, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;tool!\u0026quot;, \u0026quot;start_offset\u0026quot;: 22, \u0026quot;end_offset\u0026quot;: 27, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"长度分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/length/"},{"category":null,"content":"重点文本聚合 #  significant_text 聚合与 significant_terms 聚合类似，但它适用于原始文本字段。重要文本通过统计分析测量前景集和背景集之间流行度的变化。例如，当你搜索其股票缩写 TSLA 时，它可能会建议 Tesla。\nsignificant_text 聚合会动态重新分析源文本，过滤掉重复段落、模板化的页眉和页脚等噪声数据，这些数据可能会扭曲结果。\n重新分析高基数数据集可能是一项非常耗费 CPU 的操作。我们建议在采样聚合中使用 significant_text 聚合来将分析限制在少量最匹配文档中，例如 200。\n您可以设置以下参数：\n min_doc_count - 返回匹配超过配置数量顶部命中结果。我们不建议将 min_doc_count 设置为 1，因为它倾向于返回拼写错误或错别字。找到一个以上的词项实例有助于加强显著性不是偶然事件的结果。默认值 3 用于提供最小证据权重。 shard_size - 设置高值会增加稳定性（和准确性），但会牺牲计算性能。 shard_min_doc_count - 如果你的文本包含许多低频词，而你又不关心这些词（例如拼写错误），那么你可以将 shard_min_doc_count 参数设置为在分片级别上过滤候选词，以合理地确保即使合并本地显著文本频率也不会达到所需的 min_doc_count 。默认值为 1，直到你显式设置它之前没有影响。我们建议将此值设置得远低于 min_doc_count 值。  假设你在一个 Easysearch 集群中索引了莎士比亚的全部作品。你可以在 text_entry 字段中找到与“breathe”相关的显著文本：\nGET shakespeare/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;text_entry\u0026quot;: \u0026quot;breathe\u0026quot; } }, \u0026quot;aggregations\u0026quot;: { \u0026quot;my_sample\u0026quot;: { \u0026quot;sampler\u0026quot;: { \u0026quot;shard_size\u0026quot;: 100 }, \u0026quot;aggregations\u0026quot;: { \u0026quot;keywords\u0026quot;: { \u0026quot;significant_text\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;text_entry\u0026quot;, \u0026quot;min_doc_count\u0026quot;: 4 } } } } } } 返回内容\n\u0026quot;aggregations\u0026quot; : { \u0026quot;my_sample\u0026quot; : { \u0026quot;doc_count\u0026quot; : 59, \u0026quot;keywords\u0026quot; : { \u0026quot;doc_count\u0026quot; : 59, \u0026quot;bg_count\u0026quot; : 111396, \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;breathe\u0026quot;, \u0026quot;doc_count\u0026quot; : 59, \u0026quot;score\u0026quot; : 1887.0677966101694, \u0026quot;bg_count\u0026quot; : 59 }, { \u0026quot;key\u0026quot; : \u0026quot;air\u0026quot;, \u0026quot;doc_count\u0026quot; : 4, \u0026quot;score\u0026quot; : 2.641295376716233, \u0026quot;bg_count\u0026quot; : 189 }, { \u0026quot;key\u0026quot; : \u0026quot;dead\u0026quot;, \u0026quot;doc_count\u0026quot; : 4, \u0026quot;score\u0026quot; : 0.9665839666414213, \u0026quot;bg_count\u0026quot; : 495 }, { \u0026quot;key\u0026quot; : \u0026quot;life\u0026quot;, \u0026quot;doc_count\u0026quot; : 5, \u0026quot;score\u0026quot; : 0.9090787433467572, \u0026quot;bg_count\u0026quot; : 805 } ] } } } } 与 breathe 最相关的文本是 air 、 dead 和 life 。\nsignificant_text 聚合有以下限制：\n 不支持子聚合，因为子聚合会带来较高的内存成本。作为解决方案，您可以使用带包含子句和子聚合的 terms 聚合添加一个后续查询。 不支持嵌套对象，因为它基于文档的 JSON 源进行工作。 文档计数可能会有一些（通常很小）的不准确，因为它基于从每个分片返回的样本进行求和。您可以使用 shard_size 参数来微调准确性和性能之间的权衡。默认情况下， shard_size 设置为 -1 以自动估计分片和 size 参数的数量。  统计信息中背景词频的默认来源是整个索引。您可以使用背景过滤器缩小此范围，以便更聚焦：\nGET shakespeare/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;text_entry\u0026quot;: \u0026quot;breathe\u0026quot; } }, \u0026quot;aggregations\u0026quot;: { \u0026quot;my_sample\u0026quot;: { \u0026quot;sampler\u0026quot;: { \u0026quot;shard_size\u0026quot;: 100 }, \u0026quot;aggregations\u0026quot;: { \u0026quot;keywords\u0026quot;: { \u0026quot;significant_text\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;text_entry\u0026quot;, \u0026quot;background_filter\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;speaker\u0026quot;: \u0026quot;JOHN OF GAUNT\u0026quot; } } } } } } } } ","subcategory":null,"summary":"","tags":null,"title":"重点文本聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/significant-text/"},{"category":null,"content":"重点分组聚合 #  significant_terms 聚合功能可以帮助你在相对于索引中其他数据的过滤子集中识别不寻常或有趣的分组出现情况。\n前景集是指你进行过滤的文档集合，背景集是指索引中所有文档的集合。 significant_terms 聚合会检查前景集中的所有文档，并与背景集中的文档进行对比，从而为重要出现情况找到相应的分数。\n在示例网络日志数据中，每个文档都有一个包含访客 user-agent 的字段。此示例搜索来自 iOS 操作系统的所有请求。对这一前景集进行常规的 terms 聚合返回 Firefox，因为它在这个分组内有最多的文档数量。另一方面， significant_terms 聚合返回 Internet Explorer（IE），因为 IE 在前景集中的出现频率显著高于背景集。\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;query\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;machine.os.keyword\u0026quot;: [ \u0026quot;ios\u0026quot; ] } }, \u0026quot;aggs\u0026quot;: { \u0026quot;significant_response_codes\u0026quot;: { \u0026quot;significant_terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;agent.keyword\u0026quot; } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;significant_response_codes\u0026quot; : { \u0026quot;doc_count\u0026quot; : 2737, \u0026quot;bg_count\u0026quot; : 14074, \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\u0026quot;, \u0026quot;doc_count\u0026quot; : 818, \u0026quot;score\u0026quot; : 0.01462731514608217, \u0026quot;bg_count\u0026quot; : 4010 }, { \u0026quot;key\u0026quot; : \u0026quot;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\u0026quot;, \u0026quot;doc_count\u0026quot; : 1067, \u0026quot;score\u0026quot; : 0.009062566630410223, \u0026quot;bg_count\u0026quot; : 5362 } ] } } } 如果 significant_terms 聚合没有返回任何结果，你可能没有使用查询来过滤结果。或者，前景集中词条的分布可能与背景集相同，这意味着前景集中没有什么异常。\n背景词条频率统计信息的默认来源是整个索引。你可以使用背景过滤器来缩小这个范围，以便更聚焦\n","subcategory":null,"summary":"","tags":null,"title":"重点分组聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/significant-terms/"},{"category":null,"content":"采样聚合 #  如果你正在聚合大量文档，可以使用 sampler 聚合将范围缩小到一小部分文档，从而获得更快的响应。 sampler 聚合通过选择得分最高的文档来选取样本。\n结果是大致的，但能很好地反映真实数据的分布。 sampler 聚合显著提高了查询性能，但估计的响应并不完全可靠。\n基本语法是：\n“aggs”: { \u0026quot;SAMPLE\u0026quot;: { \u0026quot;sampler\u0026quot;: { \u0026quot;shard_size\u0026quot;: 100 }, \u0026quot;aggs\u0026quot;: {...} } } 分片大小属性 #  shard_size 属性告诉 Easysearch 每个分片最多收集多少文档。\n以下示例将每个分片上收集的文档数量限制为 1,000，然后使用 terms 聚合对文档进行分组：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;sample\u0026quot;: { \u0026quot;sampler\u0026quot;: { \u0026quot;shard_size\u0026quot;: 1000 }, \u0026quot;aggs\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;agent.keyword\u0026quot; } } } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;sample\u0026quot; : { \u0026quot;doc_count\u0026quot; : 1000, \u0026quot;terms\u0026quot; : { \u0026quot;doc_count_error_upper_bound\u0026quot; : 0, \u0026quot;sum_other_doc_count\u0026quot; : 0, \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\u0026quot;, \u0026quot;doc_count\u0026quot; : 368 }, { \u0026quot;key\u0026quot; : \u0026quot;Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.50 Safari/534.24\u0026quot;, \u0026quot;doc_count\u0026quot; : 329 }, { \u0026quot;key\u0026quot; : \u0026quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\u0026quot;, \u0026quot;doc_count\u0026quot; : 303 } ] } } } } ","subcategory":null,"summary":"","tags":null,"title":"采样聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/sampler/"},{"category":null,"content":"邻接矩阵聚合 #  adjacency_matrix 邻接矩阵聚合允许你定义过滤表达式，并返回一个交集矩阵，矩阵中的每个非空单元格代表一个分组。你可以找到落入任何过滤器组合中的文档数量。\n使用 adjacency_matrix 聚合通过将数据可视化为图形来发现概念之间的关联。\n例如，下面查询可以分析不同制造公司之间的关联关系：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;interactions\u0026quot;: { \u0026quot;adjacency_matrix\u0026quot;: { \u0026quot;filters\u0026quot;: { \u0026quot;grpA\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;manufacturer.keyword\u0026quot;: \u0026quot;Low Tide Media\u0026quot; } }, \u0026quot;grpB\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;manufacturer.keyword\u0026quot;: \u0026quot;Elitelligence\u0026quot; } }, \u0026quot;grpC\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;manufacturer.keyword\u0026quot;: \u0026quot;Oceanavigations\u0026quot; } } } } } } } 返回内容\n { ... \u0026quot;aggregations\u0026quot; : { \u0026quot;interactions\u0026quot; : { \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;grpA\u0026quot;, \u0026quot;doc_count\u0026quot; : 1553 }, { \u0026quot;key\u0026quot; : \u0026quot;grpA\u0026amp;grpB\u0026quot;, \u0026quot;doc_count\u0026quot; : 590 }, { \u0026quot;key\u0026quot; : \u0026quot;grpA\u0026amp;grpC\u0026quot;, \u0026quot;doc_count\u0026quot; : 329 }, { \u0026quot;key\u0026quot; : \u0026quot;grpB\u0026quot;, \u0026quot;doc_count\u0026quot; : 1370 }, { \u0026quot;key\u0026quot; : \u0026quot;grpB\u0026amp;grpC\u0026quot;, \u0026quot;doc_count\u0026quot; : 299 }, { \u0026quot;key\u0026quot; : \u0026quot;grpC\u0026quot;, \u0026quot;doc_count\u0026quot; : 1218 } ] } } } 让我们更仔细地查看结果\n { \u0026quot;key\u0026quot; : \u0026quot;grpA\u0026amp;grpB\u0026quot;, \u0026quot;doc_count\u0026quot; : 590 }  grpA ：由 Low Tide Media 制造的产品。 grpB ：由 Elitelligence 制造的产品。 590 : 同时生产的产品数量。  ","subcategory":null,"summary":"","tags":null,"title":"邻接矩阵聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/adjacency-matrix/"},{"category":null,"content":"过滤聚合 #  一个 filter 过滤聚合是一个查询子句，就像一个搜索查询一样 — match 或 term 或 range 。您可以使用 filter 聚合在创建分组之前将整个文档集缩小到特定的文档集。\n以下示例展示了 avg 聚合在过滤上下文中运行的情况。 avg 聚合仅聚合与 range 查询匹配的文档：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;low_value\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;range\u0026quot;: { \u0026quot;taxful_total_price\u0026quot;: { \u0026quot;lte\u0026quot;: 50 } } }, \u0026quot;aggs\u0026quot;: { \u0026quot;avg_amount\u0026quot;: { \u0026quot;avg\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot; } } } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;low_value\u0026quot; : { \u0026quot;doc_count\u0026quot; : 1633, \u0026quot;avg_amount\u0026quot; : { \u0026quot;value\u0026quot; : 38.363175998928355 } } } } ","subcategory":null,"summary":"","tags":null,"title":"过滤聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/filter/"},{"category":null,"content":"边缘 n 元分词过滤器 #  边缘 n 元（edge_ngram）分词过滤器与 n 元（ngram）分词过滤器非常相似，二者都会将特定字符串拆分成不同长度的子字符串。不过，边缘 n 元分词过滤器仅从词元的开头（边缘）生成 n 元子字符串。在自动补全或前缀匹配等场景中，它特别有用，因为在这些场景里，你希望在用户输入单词或短语时就能匹配词项的开头部分。\n参数说明 #  边缘 n 元分词过滤器可使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     min_gram 可选 整数 要生成的 n 元词项的最小长度。默认值为1。   max_gram 可选 整数 要生成的 n 元词项的最大长度。对于边缘 n 元分词过滤器，默认值为1；对于自定义分词过滤器，默认值为2。避免将此参数设置为较低的值。如果该值设置得过低，将只会生成非常短的 n 元词项，并且可能找不到搜索词。例如，将max_gram设置为 3，并且对单词“banana”建立索引，那么生成的最长词元将是“ban”。如果用户搜索“banana”，将不会返回任何匹配结果。你可以使用截断truncate分词过滤器作为搜索分析器来降低这种风险。   preserve_original 可选 布尔值 将原始词元包含在输出中。默认值为false。    参考样例 #  以下示例请求创建了一个名为 edge_ngram_example 的新索引，并配置了一个带有边缘 n 元过滤器的分词器：\nPUT /edge_ngram_example { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_edge_ngram\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;edge_ngram\u0026quot;, \u0026quot;min_gram\u0026quot;: 3, \u0026quot;max_gram\u0026quot;: 4 } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;, \u0026quot;my_edge_ngram\u0026quot;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /edge_ngram_example/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;slow green turtle\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;slo\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;slow\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;gre\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;gree\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;tur\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;turt\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"边缘 n 元分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/edge-n-gram/"},{"category":null,"content":"路径词元分词器 #  路径（path_hierarchy）词元分词器可以把文件路径（或类似的层次结构）的文本按照各个层级分解为词元。当处理诸如文件路径、网址或其他有分隔符的层次结构数据时，这个词元生成器特别有用。\n参考样例 #  以下示例请求创建一个名为 my_index 的新索引，并配置一个使用路径词元生成器的分词器：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_path_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;path_hierarchy\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_path_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_path_tokenizer\u0026quot; } } } } } 生成的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_path_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;/users/john/documents/report.txt\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;/users\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;/users/john\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;/users/john/documents\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 21, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;/users/john/documents/report.txt\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 32, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } 参数说明 #  路径词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     delimiter 可选 字符串 指定用于分隔路径组件的字符。默认值为 /。   replacement 可选 字符串 配置用于替换词元中分隔符的字符。默认值为 /。   buffer_size 可选 整数 指定缓冲区大小。默认值为 1024。   reverse 可选 布尔值 若为 true，则按逆序生成词元。默认值为 false。   skip 可选 整数 指定分词时要跳过的初始词元（层级）数量。默认值为 0。    使用分隔符和替换参数 #  以下示例请求配置了自定义的分隔符（delimiter）和替换(replacement)参数：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_path_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;path_hierarchy\u0026quot;, \u0026quot;delimiter\u0026quot;: \u0026quot;\\\\\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;/\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_path_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_path_tokenizer\u0026quot; } } } } } 使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_path_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;C:\\\\users\\\\john\\\\documents\\\\report.txt\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;C:\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 2, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;C:/users\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;C:/users/john\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;C:/users/john/documents\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 23, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;C:/users/john/documents/report.txt\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 34, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"路径词元分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/path-hierarchy/"},{"category":null,"content":"谓词分词过滤器 #  谓词分词过滤器(predicate_token_filter)会根据自定义脚本中定义的条件来评估词元是应该保留还是丢弃。词元的评估是在分析谓词上下文中进行的。此过滤器仅支持内联 Painless 脚本。\n参数说明 #  谓词分词过滤器有一个必需参数：script。该参数提供一个条件，用于评估词元是否应被保留。\n参考样例 #  以下示例请求创建了一个名为 predicate_index 的新索引，并配置了一个带有谓词分词过滤器的分词器。该过滤器指定仅输出长度超过 7 个字符的词元。\nPUT /predicate_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_predicate_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;predicate_token_filter\u0026quot;, \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;token.term.length() \u0026gt; 7\u0026quot; } } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;predicate_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_predicate_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /predicate_index/_analyze { \u0026quot;text\u0026quot;: \u0026quot;The Easysearch community is growing rapidly\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;predicate_analyzer\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 14, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;community\u0026quot;, \u0026quot;start_offset\u0026quot;: 15, \u0026quot;end_offset\u0026quot;: 24, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"谓词分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/predicate-token-filter/"},{"category":null,"content":"词片分词过滤器 #  词片(shingle)分词过滤器用于从输入文本中生成词 n 元组（即词片）。例如，对于字符串 “slow green turtle”，词片过滤器会创建以下一元词片和二元词片：“slow”、“slow green”、“green”、“green turtle” 以及 “turtle”。\n这个分词过滤器常常与其他过滤器结合使用，通过对短语而不是单个词元进行索引来提高搜索的准确性。\n参数说明 #  词片分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     min_shingle_size 可选 整数 要连接的词元的最小数量。默认值为 2。   max_shingle_size 可选 整数 要连接的词元的最大数量。默认值为 2。   output_unigrams 可选 布尔值 是否将一元词（单个词元）包含在输出中。默认值为 true。   output_unigrams_if_no_shingles 可选 布尔值 如果未生成词片，是否输出一元词。默认值为 false。   token_separator 可选 字符串 用于将词元连接成词片的分隔符。默认值为一个空格（ ）。   filler_token 可选 字符串 插入到词元之间的空位置或间隙中的词元。默认值为下划线（_）。     如果 output_unigrams 和 output_unigrams_if_no_shingles 都设置为 true，则 output_unigrams_if_no_shingles 将被忽略。\n 参数说明 #  以下示例请求创建了一个名为“my-shingle-index”的新索引，并配置了一个带有词片过滤器的分词器。\nPUT /my-shingle-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_shingle_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;shingle\u0026quot;, \u0026quot;min_shingle_size\u0026quot;: 2, \u0026quot;max_shingle_size\u0026quot;: 2, \u0026quot;output_unigrams\u0026quot;: true } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_shingle_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_shingle_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-shingle-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_shingle_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;slow green turtle\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;slow\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;slow green\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;shingle\u0026quot;, \u0026quot;position\u0026quot;: 0, \u0026quot;positionLength\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;green\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;green turtle\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;shingle\u0026quot;, \u0026quot;position\u0026quot;: 1, \u0026quot;positionLength\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"词片分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/shingle/"},{"category":null,"content":"词干覆盖分词过滤器 #  词干覆盖（stemmer_override）分词过滤器允许你定义自定义的词干提取规则，这些规则可以覆盖像波特（Porter）或雪球（Snowball）等默认词干提取器的行为。当你希望对某些特定的单词应用特殊的词干提取方式，而这些单词可能无法被标准的词干提取算法正确处理时，这个过滤器就会非常有用。\n参数说明 #  词干覆盖分词过滤器必须使用以下参数中的一个进行配置。\n   参数 数据类型 描述     rules 字符串 直接在设置中定义覆盖规则。   rules_path 字符串 指定包含自定义规则（映射）的文件的路径。该路径可以是绝对路径，也可以是相对于配置目录的相对路径。    参考样例 #  以下示例请求创建了一个名为 my-index 的新索引，并配置了一个带有词干覆盖过滤器的分词器。\nPUT /my-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_stemmer_override_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;stemmer_override\u0026quot;, \u0026quot;rules\u0026quot;: [ \u0026quot;running, runner =\u0026gt; run\u0026quot;, \u0026quot;bought =\u0026gt; buy\u0026quot;, \u0026quot;best =\u0026gt; good\u0026quot; ] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_stemmer_override_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;I am a runner and bought the best shoes\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;i\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 1, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;am\u0026quot;, \u0026quot;start_offset\u0026quot;: 2, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;a\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;run\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;and\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;buy\u0026quot;, \u0026quot;start_offset\u0026quot;: 18, \u0026quot;end_offset\u0026quot;: 24, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 }, { \u0026quot;token\u0026quot;: \u0026quot;the\u0026quot;, \u0026quot;start_offset\u0026quot;: 25, \u0026quot;end_offset\u0026quot;: 28, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 6 }, { \u0026quot;token\u0026quot;: \u0026quot;good\u0026quot;, \u0026quot;start_offset\u0026quot;: 29, \u0026quot;end_offset\u0026quot;: 33, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 7 }, { \u0026quot;token\u0026quot;: \u0026quot;shoes\u0026quot;, \u0026quot;start_offset\u0026quot;: 34, \u0026quot;end_offset\u0026quot;: 39, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 8 } ] } ","subcategory":null,"summary":"","tags":null,"title":"词干覆盖分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/stemmer-override/"},{"category":null,"content":"词干提取分词过滤器 #  词干提取(stemmer)分词过滤器会将单词缩减为其词根或基本形式（也称为词干stem）。\n参数说明 #  词干提取分词过滤器可以通过一个 language（语言）参数进行配置，该参数接受以下值：\n 阿拉伯语：arabic 亚美尼亚语：armenian 巴斯克语：basque 孟加拉语：bengali 巴西葡萄牙语：brazilian 保加利亚语：bulgarian 加泰罗尼亚语：catalan 捷克语：czech 丹麦语：danish 荷兰语：dutch、dutch_kp 英语：english（默认）、light_english、lovins、minimal_english、porter2、possessive_english 爱沙尼亚语：estonian 芬兰语：finnish、light_finnish 法语：light_french、french、minimal_french 加利西亚语：galician、minimal_galician（仅复数处理步骤） 德语：light_german、german、german2、minimal_german 希腊语：greek 印地语：hindi 匈牙利语：hungarian、light_hungarian 印尼语：indonesian 爱尔兰语：irish 意大利语：light_italian、italian 库尔德语（索拉尼方言）：sorani 拉脱维亚语：latvian 立陶宛语：lithuanian 挪威语（书面挪威语）：norwegian、light_norwegian、minimal_norwegian 挪威语（新挪威语）：light_nynorsk、minimal_nynorsk 葡萄牙语：light_portuguese、minimal_portuguese、portuguese、portuguese_rslp 罗马尼亚语：romanian 俄语：russian、light_russian 西班牙语：light_spanish、spanish 瑞典语：swedish、light_swedish 土耳其语：turkish   你也可以使用 name 参数作为 language 参数的别名。如果两个参数都被设置，name 参数将被忽略。\n 参考样例 #  以下示例请求创建了一个名为 my-stemmer-index 的新索引，并配置了一个带有词干提取过滤器的分词器。\nPUT /my-stemmer-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_english_stemmer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;stemmer\u0026quot;, \u0026quot;language\u0026quot;: \u0026quot;english\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_stemmer_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_english_stemmer\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-stemmer-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_stemmer_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;running runs\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;run\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;run\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 12, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"词干提取分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/stemmer/"},{"category":null,"content":"词干提取 #  词干提取是将单词还原为其词根或基本形式（即词干）的过程。这项技术可确保在搜索操作中，单词的不同变体都能匹配到相应结果。例如，单词 “running”（跑步，现在分词形式）、“runner”（跑步者，名词形式）和 “ran”（跑步，过去式）都可以还原为词干 “run”（跑步，原形），这样一来，搜索这些词中的任何一个都能返回相关结果。\n在自然语言中，由于动词变位、名词复数变化或词的派生等原因，单词常常以各种形式出现。词干提取在以下方面提升了搜索操作的效果：\n 提高搜索召回率：通过将不同的单词形式匹配到同一个词干，词干提取增加了检索到的相关文档的数量。 减小索引大小：仅存储单词的词干形式可以减少搜索索引的总体大小。  词干提取是通过在分词器中使用词元过滤器来配置的。一个分词器包含以下组件：\n 字符过滤器：在分词之前修改字符流。 词元生成器：将文本拆分为词元（通常是单词）。 词元过滤器：在分词之后修改词元，例如，应用词干提取操作。  使用内置词元过滤器进行词干提取的示例 #  要实现词干提取，你可以配置一个内置的词元过滤器，比如 porter_stem 或 kstem 过滤器。\n波特词干提取算法（ Porter stemming algorithm）是一种常用于英语的词干提取算法。\n创建带有自定义分词器的索引 #  以下示例请求创建了一个名为 my_stemming_index 的新索引，并配置了一个使用 porter_stem 词元过滤器的分词器：\nPUT /my_stemming_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_stemmer_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;porter_stem\u0026quot; ] } } } } } 此配置包含以下内容：\n 标准分词器：根据单词边界将文本拆分为词项。 小写字母过滤器：将所有词元转换为小写形式。 波特词干过滤器（porter_stem 过滤器）：将单词还原为它们的词根形式。  测试分词器 #  为了检验词干提取的效果，使用之前配置好的自定义分词器来分析一段示例文本：\nPOST /my_stemming_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_stemmer_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;The runners are running swiftly.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;the\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 3, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;runner\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;ar\u0026quot;, \u0026quot;start_offset\u0026quot;: 12, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;run\u0026quot;, \u0026quot;start_offset\u0026quot;: 16, \u0026quot;end_offset\u0026quot;: 23, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;swiftli\u0026quot;, \u0026quot;start_offset\u0026quot;: 24, \u0026quot;end_offset\u0026quot;: 31, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } 词干提取器的类别 #  你可以配置属于以下两类的词干提取器：\n 算法词干提取器 字典词干提取器  算法词干提取器 #  算法词干提取器运用预先设定的规则，系统地去除单词的词缀（前缀和后缀），将单词还原为词干。以下这些词元过滤器使用了算法词干提取器：\n porter_stem：应用波特词干提取算法，去除常见的后缀，把单词还原为词干。例如，“running”（跑步，现在分词）会变成“run”（跑步，原形）。 kstem：这是一款专为英语设计的轻量级词干提取器，它将算法词干提取与内置字典相结合。它能把复数形式还原为单数形式，将动词的各种时态转换为原形，并去除常见的派生词尾。 stemmer：为包括英语在内的多种语言提供算法词干提取功能，有像 light_english（轻型英语词干提取算法）、minimal_english（最简英语词干提取算法）和 porter2 等不同词干提取算法可供选择。 snowball：应用雪球算法，为包括英语、法语、德语等在内的多种语言提供高效且准确的词干提取服务。  字典词干提取器 #  字典词干提取器依赖于庞大的字典，将单词映射到它们的词根形式，有效地对不规则单词进行词干提取。它们会在预先编译好的列表中查找每个单词，以找到其对应的词干。这种操作对资源的消耗更大，但对于不规则单词以及那些看似词干相似但含义差异很大的单词，往往能得出更好的词干提取结果。\n字典词干提取器中最具代表性的例子是 hunspell 词元过滤器，它使用 Hunspell——一个在许多开源应用程序中都有使用的拼写检查引擎。\n注意事项 #  在选择词干提取器时，请注意以下几点：\n 当处理速度和内存效率是优先考虑的因素，并且所处理的语言具有相对规则的词形变化模式时，算法词干提取器是合适的选择。 当处理不规则单词形式时的准确性至关重要，并且有足够的资源来支持增加的内存使用和处理时间时，字典词干提取器是理想的选择。  额外的词干提取配置 #  尽管 “organize”（组织）和 “organic”（有机的）有共同的语言词根，这会导致词干提取器将它们都处理成 “organ”（器官；机构），但它们在概念上的差异是很大的。在实际的搜索场景中，这种共同的词根可能会导致在搜索结果中返回不相关的匹配项。\n你可以通过以下方法来应对这些挑战：\n 显式的词干提取覆盖：不必仅仅依赖算法词干提取，你可以定义特定的词干提取规则。使用 stemmer_override（词干提取覆盖），你可以确保 “organize” 保持不变，而 “organic” 被还原为 “organ”。这提供了对词项最终形式的精细控制。 关键词保留：为了保持重要词项的完整性，你可以使用 keyword_marker（关键词标记）词元过滤器。这个过滤器将特定的单词指定为关键词，防止后续的词干提取器过滤器对它们进行修改。在这个例子中，你可以将 “organize” 标记为关键词，确保它按照其原本的形式进行索引。 条件词干提取控制：condition（条件）词元过滤器使你能够建立规则来确定一个词项是否应该进行词干提取。这些规则可以基于各种标准，例如词项是否出现在预定义的列表中。 特定语言的词项排除：对于内置的语言分词器，stem_exclusion（词干提取排除）参数提供了一种指定应免于词干提取的单词的方法。例如，你可以将 “organize” 添加到 stem_exclusion 列表中，防止分词器对其进行词干提取。这对于在特定语言中保留特定词项的独特含义可能很有用。  ","subcategory":null,"summary":"","tags":null,"title":"词干提取","url":"/easysearch/v1.15.0/docs/references/text-analysis/stemming/"},{"category":null,"content":"词典复合词分词过滤器 #  词典复合词（dictionary_decompounder）分词过滤器用于根据预定义的词典将复合词拆分为其组成部分。该过滤器对于德语、荷兰语或芬兰语等复合词较为常见的语言特别有用，因为将复合词拆分可以提高搜索的相关性。词典复合词分词过滤器会根据已知词列表判断每个词元（单词）是否可以拆分为更小的词元。如果该词元可以拆分为已知单词，过滤器就会为该词元生成子词元。\n参数说明 #  词典复合词分词过滤器具有以下参数：\n   参数 必需/可选 数据类型 描述     word_list 除非配置了 word_list_path，否则为必需 字符串数组 过滤器用于拆分复合词的单词词典。   word_list_path 除非配置了 word_list，否则为必需 字符串 包含词典单词的文本文件的文件路径。可以接受绝对路径或相对于配置(config)目录的相对路径。词典文件必须采用 UTF-8 编码，且每个单词必须单独占一行。   min_word_size 可选 整数 会被考虑进行拆分的整个复合词的最小长度。如果复合词短于该值，则不会进行拆分。默认值为 5。   min_subword_size 可选 整数 任何子词的最小长度。如果子词短于该值，则不会包含在输出中。默认值为 2。   max_subword_size 可选 整数 任何子词的最大长度。如果子词长于该值，则不会包含在输出中。默认值为 15。   only_longest_match 可选 布尔值 如果设置为 true，则仅返回最长匹配的子词。默认值为 false。    参考样例 #  以下示例请求创建了一个名为 decompound_example 的新索引，并配置了一个带有词典复合词过滤器的分词器：\nPUT /decompound_example { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_dictionary_decompounder\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;dictionary_decompounder\u0026quot;, \u0026quot;word_list\u0026quot;: [\u0026quot;slow\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;turtle\u0026quot;] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;, \u0026quot;my_dictionary_decompounder\u0026quot;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /decompound_example/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;slowgreenturtleswim\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;slowgreenturtleswim\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;slow\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;green\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"词典复合词分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/dictionary-decompounder/"},{"category":null,"content":"词保留分词过滤器 #  词保留（keep_words）分词过滤器旨在在分析过程中仅保留特定的词。如果你有大量文本，但只对某些关键字或术语感兴趣，那么这个过滤器就会很有用。\n参数说明 #  词保留分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     keep_words 若未配置 keep_words_path 则为必需 字符串列表 要保留的词的列表。   keep_words_path 若未配置 keep_words 则为必需 字符串 包含要保留的词列表的文件的路径。   keep_words_case 可选 布尔值 在比较时是否将所有词转换为小写。默认值为 false。    参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有词保留过滤器的分词器：\nPUT my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_keep_word\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;keep_words_filter\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;keep_words_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keep\u0026quot;, \u0026quot;keep_words\u0026quot;: [\u0026quot;example\u0026quot;, \u0026quot;world\u0026quot;, \u0026quot;easysearch\u0026quot;], \u0026quot;keep_words_case\u0026quot;: true } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_keep_word\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Hello, world! This is an Easysearch example.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;world\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 12, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;Easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 25, \u0026quot;end_offset\u0026quot;: 35, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 }, { \u0026quot;token\u0026quot;: \u0026quot;example\u0026quot;, \u0026quot;start_offset\u0026quot;: 36, \u0026quot;end_offset\u0026quot;: 43, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 6 } ] } ","subcategory":null,"summary":"","tags":null,"title":"词保留分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/keep-words/"},{"category":null,"content":"规范化 #  规范化的功能与分词器类似，但它仅输出单个词元。它不包含分词器，并且只能包含特定类型的字符过滤器和词元过滤器。这些过滤器只能执行字符级别的操作，例如字符或模式替换，而不能对整个词元进行操作。这意味着不支持用同义词替换词元或进行词干提取。\n规范化在关键字搜索（即基于词项的查询）中很有用，因为它允许你对任何给定的输入运行词元过滤器和字符过滤器。例如，它使得能够将传入的查询 “Naïve” 与索引词项 “naive” 进行匹配。\n考虑以下示例：\n创建一个带有自定义规范化的新索引：\nPUT /sample-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;normalizer\u0026quot;: { \u0026quot;normalized_keyword\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;char_filter\u0026quot;: [], \u0026quot;filter\u0026quot;: [ \u0026quot;asciifolding\u0026quot;, \u0026quot;lowercase\u0026quot; ] } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;approach\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;normalizer\u0026quot;: \u0026quot;normalized_keyword\u0026quot; } } } } 索引一个文档\nPOST /sample-index/_doc/ { \u0026quot;approach\u0026quot;: \u0026quot;naive\u0026quot; } 以下查询与该文档匹配。这是预期的结果：\nGET /sample-index/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;approach\u0026quot;: \u0026quot;naive\u0026quot; } } } 但这个查询同样也与该文档匹配：\nGET /sample-index/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;approach\u0026quot;: \u0026quot;Naïve\u0026quot; } } } 要理解为什么会这样，我们来考虑一下规范化所产生的影响：\nGET /sample-index/_analyze { \u0026quot;normalizer\u0026quot; : \u0026quot;normalized_keyword\u0026quot;, \u0026quot;text\u0026quot; : \u0026quot;Naïve\u0026quot; } 在内部，规范化只接受 NormalizingTokenFilterFactory 或 NormalizingCharFilterFactory 实例的过滤器。以下是在作为 Easysearch 核心存储库一部分的模块和插件中找到的兼容过滤器列表。\n通用分词模块 #  该模块无需安装，默认即可使用。\n字符过滤器：pattern_replace（模式替换）、mapping（映射）\n词元过滤器：arabic_normalization（阿拉伯语规范化）、asciifolding（ASCII 折叠）、bengali_normalization（孟加拉语规范化）、cjk_width（中日韩字符宽度调整）、decimal_digit（十进制数字处理）、elision（省略）、german_normalization（德语规范化）、hindi_normalization（印地语规范化）、indic_normalization（印度语系规范化）、lowercase（转换为小写）、persian_normalization（波斯语规范化）、scandinavian_folding（斯堪的纳维亚语折叠）、scandinavian_normalization（斯堪的纳维亚语规范化）、serbian_normalization（塞尔维亚语规范化）、sorani_normalization（索拉尼语规范化）、trim（去除首尾空白字符）、uppercase（转换为大写）\n","subcategory":null,"summary":"","tags":null,"title":"规范化","url":"/easysearch/v1.15.0/docs/references/text-analysis/normalizers/"},{"category":null,"content":"范围聚合 #  range 范围聚合允许你为每个分组定义范围。\n例如，你可以找到在 1000 和 2000 之间、2000 和 3000 之间以及 3000 和 4000 之间的字节数。在 range 参数中，你可以将范围定义为数组对象。\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;number_of_bytes_distribution\u0026quot;: { \u0026quot;range\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;ranges\u0026quot;: [ { \u0026quot;from\u0026quot;: 1000, \u0026quot;to\u0026quot;: 2000 }, { \u0026quot;from\u0026quot;: 2000, \u0026quot;to\u0026quot;: 3000 }, { \u0026quot;from\u0026quot;: 3000, \u0026quot;to\u0026quot;: 4000 } ] } } } } 响应包含 from 键值，并排除 to 键值：\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;number_of_bytes_distribution\u0026quot; : { \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;1000.0-2000.0\u0026quot;, \u0026quot;from\u0026quot; : 1000.0, \u0026quot;to\u0026quot; : 2000.0, \u0026quot;doc_count\u0026quot; : 805 }, { \u0026quot;key\u0026quot; : \u0026quot;2000.0-3000.0\u0026quot;, \u0026quot;from\u0026quot; : 2000.0, \u0026quot;to\u0026quot; : 3000.0, \u0026quot;doc_count\u0026quot; : 1369 }, { \u0026quot;key\u0026quot; : \u0026quot;3000.0-4000.0\u0026quot;, \u0026quot;from\u0026quot; : 3000.0, \u0026quot;to\u0026quot; : 4000.0, \u0026quot;doc_count\u0026quot; : 1422 } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"范围聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/range/"},{"category":null,"content":"脚本指标聚合 #  scripted_metric 脚本指标聚合是一个多值指标聚合，它返回根据指定脚本计算的指标。脚本有四个阶段， init 、 map 、 combine 和 reduce ，每个聚合按顺序运行这些阶段，组合来自文档的结果。\n所有四个脚本共享一个可变对象，称为 state ，该对象由你定义。 state 在 init 、 map 和 combine 阶段时对每个分片是局部的。结果被传递到 states 数组中用于 reduce 阶段。因此，每个分片的 state 在分片在 reduce 步骤中组合之前是独立的。\n参数说明 #  scripted_metric 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     init_script 可选 String 一个脚本，在每个分片上处理任何文档之前执行一次。用于设置初始 state （例如，在 state 对象中初始化计数器或列表）。如果没有提供， state 在每个分片上开始时是一个空对象。   map_script 必需 String 一个脚本，针对聚合收集的每个文档执行。此脚本根据文档的数据更新 state 。例如，您可以检查字段的值，然后递增计数器或在 state 中计算运行总和。   combine_script 必需 String 一个脚本，在每个分片上处理完所有文档后由 map_script 执行一次。此脚本将分片的 state 聚合为单个结果发送回协调节点。此脚本用于完成一个分片的计算（例如，汇总存储在 state 中的计数器或总计）。脚本应返回其分片的汇总值或结构。   reduce_script 必需 String 一个脚本在接收到所有分片的合并结果后，在协调节点上执行一次。这个脚本接收一个特殊变量 states ，它是一个包含每个分片从 combine_script 输出的数组。 reduce_script 遍历状态并生成最终的聚合输出（例如，添加分片总和或合并计数的映射）。 reduce_script 返回的值是在聚合结果中报告的值。   params 可选 Object 除 reduce_script 外，所有脚本都可以访问用户定义的参数。    可返回的类型 #  脚本可以在内部使用任何有效的操作和对象。然而，存储在 state 或从任何脚本返回的数据必须属于允许的类型之一。这个限制存在是因为中间 state 需要在节点之间发送。允许的类型如下：\n 基本类型： int , long , float , double , boolean String 字符串 Map 映射（键和值仅允许为允许类型：基本类型、字符串、映射或数组） 数组（仅包含允许类型：基本类型、字符串、映射或数组）  state 可以是一个数字、字符串、 map （对象）或数组（列表），也可以是这些的组合。例如，你可以使用 map 来累积多个计数器，使用数组来收集值，或使用单个数字来保持累计总和。如果你需要返回多个指标，可以将它们存储在 map 或数组中。如果你从 reduce_script 返回 map 作为最终值，聚合结果包含一个对象。如果你返回单个数字或字符串，结果是一个单一值。\n在脚本中使用参数 #  您可以使用 params 字段可选地向脚本传递自定义参数。这是一个用户定义的对象，其内容成为 init_script 、 map_script 和 combine_script 中的变量。 reduce_script 不会直接接收 params ，因为在 reduce 阶段，所有需要的数据都必须在 states 数组中。如果您需要在 reduce 阶段使用常量，可以将其作为每个分片 state 的一部分，或使用存储的脚本。所有参数都必须在全局 params 对象中定义。这确保它们在不同脚本阶段之间共享。如果您未指定任何 params ，则 params 对象为空。\n例如，您可以在 params 中提供 threshold 或 field 名称，然后在脚本中引用 params.threshold 或 params.field ：\n\u0026quot;scripted_metric\u0026quot;: { \u0026quot;params\u0026quot;: { \u0026quot;threshold\u0026quot;: 100, \u0026quot;field\u0026quot;: \u0026quot;amount\u0026quot; }, \u0026quot;init_script\u0026quot;: \u0026quot;...\u0026quot;, \u0026quot;map_script\u0026quot;: \u0026quot;...\u0026quot;, \u0026quot;combine_script\u0026quot;: \u0026quot;...\u0026quot;, \u0026quot;reduce_script\u0026quot;: \u0026quot;...\u0026quot; } 参考样例 #\n 以下示例展示了使用 scripted_metric 的不同方法。\n计算交易净利润 #  以下示例展示了如何使用 scripted_metric 聚合来计算内置聚合不直接支持的定制指标。该数据集表示财务交易，其中每个文档被分类为 sale （收入）或 cost （支出），并包含一个 amount 字段。目标是通过从所有文档的总销售额中减去总成本来计算总净利润。\n创建索引：\nPUT transactions { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;type\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;amount\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot; } } } } 索引四笔交易，两笔销售（金额 80 和 130 ），以及两笔成本（ 10 和 30 ）：\nPUT transactions/_bulk?refresh=true { \u0026quot;index\u0026quot;: {} } { \u0026quot;type\u0026quot;: \u0026quot;sale\u0026quot;, \u0026quot;amount\u0026quot;: 80 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;type\u0026quot;: \u0026quot;cost\u0026quot;, \u0026quot;amount\u0026quot;: 10 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;type\u0026quot;: \u0026quot;cost\u0026quot;, \u0026quot;amount\u0026quot;: 30 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;type\u0026quot;: \u0026quot;sale\u0026quot;, \u0026quot;amount\u0026quot;: 130 } 要运行一个使用 scripted_metric 聚合来计算利润的搜索，请使用以下脚本：\n init_script 创建一个空列表，用于存储每个分片的事务值。 map_script 根据类型 sale 将每个文档的金额作为正数添加到 state.transactions 列表中，如果类型是 cost 则作为负数。在 map 阶段结束时，每个分片都有一个 state.transactions 列表，代表其收入和支出。 combine_script 处理 state.transactions 列表，并为分片计算一个 shardProfit 值。然后返回 shardProfit 作为分片的输出。 reduce_script 在协调节点上运行，接收 states 数组，其中包含每个分片的 shardProfit 值。它检查空条目，将这些值相加以计算总利润，并返回最终结果。  以下请求包含上面描述的脚本：\nGET transactions/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;total_profit\u0026quot;: { \u0026quot;scripted_metric\u0026quot;: { \u0026quot;init_script\u0026quot;: \u0026quot;state.transactions = []\u0026quot;, \u0026quot;map_script\u0026quot;: \u0026quot;state.transactions.add(doc['type'].value == 'sale' ? doc['amount'].value : -1 * doc['amount'].value)\u0026quot;, \u0026quot;combine_script\u0026quot;: \u0026quot;double shardProfit = 0; for (t in state.transactions) { shardProfit += t; } return shardProfit;\u0026quot;, \u0026quot;reduce_script\u0026quot;: \u0026quot;double totalProfit = 0; for (p in states) { if (p != null) { totalProfit += p; }} return totalProfit;\u0026quot; } } } } 返回 total_profit :\n{ ... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;total_profit\u0026quot;: { \u0026quot;value\u0026quot;: 170 } } } 给 HTTP 响应代码分类 #\n 以下示例展示了如何使用 scripted_metric 聚合在单个聚合中返回多个值。数据集由包含 HTTP 响应码的 Web 服务器日志条目组成。目标是将这些响应分为三类：成功响应（2xx 状态码）、客户端或服务器错误（4xx 或 5xx 状态码）以及其他响应（1xx 或 3xx 状态码）。这种分类是通过在基于映射的聚合 state 中维护计数器来实现的。\n创建一个示例索引：\nPUT logs { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;response\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; } } } } 添加具有各种响应代码的示例文档：\nPUT logs/_bulk?refresh=true { \u0026quot;index\u0026quot;: {} } { \u0026quot;response\u0026quot;: \u0026quot;200\u0026quot; } { \u0026quot;index\u0026quot;: {} } { \u0026quot;response\u0026quot;: \u0026quot;201\u0026quot; } { \u0026quot;index\u0026quot;: {} } { \u0026quot;response\u0026quot;: \u0026quot;404\u0026quot; } { \u0026quot;index\u0026quot;: {} } { \u0026quot;response\u0026quot;: \u0026quot;500\u0026quot; } { \u0026quot;index\u0026quot;: {} } { \u0026quot;response\u0026quot;: \u0026quot;304\u0026quot; } state （每个分片上）是一个 map ，包含三个计数器： error 、 success 和 other 。\n要运行一个用于统计类别的脚本指标聚合，请使用以下脚本：\n init_script 将 error 、 success 和 other 的计数器初始化为 0 。 map_script 检查每个文档的响应代码，并根据响应代码递增相应的计数器。 combine_script 返回该分片的 state.responses map 。 reduce_script 合并所有分片的 maps 数组 ( states )。因此，它创建一个新的组合 map ，并将每个分片的 map 中的 error 、 success 和 other 计数添加进去。这个组合的 map 作为最终结果返回。  以下请求包含了上面描述的脚本：\nGET logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;responses_by_type\u0026quot;: { \u0026quot;scripted_metric\u0026quot;: { \u0026quot;init_script\u0026quot;: \u0026quot;state.responses = new HashMap(); state.responses.put('success', 0); state.responses.put('error', 0); state.responses.put('other', 0);\u0026quot;, \u0026quot;map_script\u0026quot;: \u0026quot;\u0026quot;\u0026quot; String code = doc['response'].value; if (code.startsWith(\u0026quot;5\u0026quot;) || code.startsWith(\u0026quot;4\u0026quot;)) { // 4xx or 5xx -\u0026gt; count as error state.responses.error += 1; } else if (code.startsWith(\u0026quot;2\u0026quot;)) { // 2xx -\u0026gt; count as success state.responses.success += 1; } else { // anything else (e.g., 1xx, 3xx, etc.) -\u0026gt; count as other state.responses.other += 1; } \u0026quot;\u0026quot;\u0026quot;, \u0026quot;combine_script\u0026quot;: \u0026quot;return state.responses;\u0026quot;, \u0026quot;reduce_script\u0026quot;: \u0026quot;\u0026quot;\u0026quot; Map combined = new HashMap(); combined.error = 0; combined.success = 0; combined.other = 0; for (state in states) { if (state != null) { combined.error += state.error; combined.success += state.success; combined.other += state.other; } } return combined; \u0026quot;\u0026quot;\u0026quot; } } } } 在 value 对象中返回了三个值，展示了通过在 state 中使用 map ，脚本指标如何一次性返回多个指标\n{ ... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 5, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;responses_by_type\u0026quot;: { \u0026quot;value\u0026quot;: { \u0026quot;other\u0026quot;: 1, \u0026quot;success\u0026quot;: 2, \u0026quot;error\u0026quot;: 2 } } } } 管理空分组（没有文档） #\n 当使用 scripted_metric 聚合作为分组聚合（例如 terms ）的子聚合时，需要考虑某些分片上不包含文档的分组。在这种情况下，这些分片会为聚合 state 返回 null 值。在 reduce_script 阶段， states 数组可能因此包含对应这些分片的 null 条目。为确保可靠执行， reduce_script 必须设计为能够优雅地处理 null 值。常见的方法是在访问或操作每个 state 之前加入条件检查，例如 if (state != null) 。若未实施此类检查，在跨分片处理空分组时可能导致运行时错误。\n性能考量 #  由于脚本指标为每个文档运行自定义代码，因此可能会在内存中积累大量的 state ，所以它们可能比内置聚合慢。每个分片的中间 state 必须序列化才能发送到协调节点。因此，如果您的 state 非常大，它可能会消耗大量内存和网络带宽。为了保持搜索效率，请尽可能使您的脚本轻量，并避免在 state 中积累不必要的数据。在发送之前，使用合并阶段来缩减 state 数据，如“从交易中计算净利润”所示，并且仅收集您真正需要以生成最终指标的数据。\n","subcategory":null,"summary":"","tags":null,"title":"脚本指标聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/scripted-metric/"},{"category":null,"content":"缺省聚合 #  如果你的索引中的文档完全不包含聚合字段，或者聚合字段的值为 NULL，请使用 missing 参数指定这些文档应该放入的分组的名称。\n以下示例将任何缺失的值添加到名为“N/A”的分组中：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;response_codes\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;response.keyword\u0026quot;, \u0026quot;size\u0026quot;: 10, \u0026quot;missing\u0026quot;: \u0026quot;N/A\u0026quot; } } } } 由于 min_doc_count 参数的默认值为 1， missing 参数在其响应中不会返回任何分组。将 min_doc_count 参数设置为 0 以在响应中查看“N/A”分组：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;response_codes\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;response.keyword\u0026quot;, \u0026quot;size\u0026quot;: 10, \u0026quot;missing\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;min_doc_count\u0026quot;: 0 } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;response_codes\u0026quot; : { \u0026quot;doc_count_error_upper_bound\u0026quot; : 0, \u0026quot;sum_other_doc_count\u0026quot; : 0, \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;200\u0026quot;, \u0026quot;doc_count\u0026quot; : 12832 }, { \u0026quot;key\u0026quot; : \u0026quot;404\u0026quot;, \u0026quot;doc_count\u0026quot; : 801 }, { \u0026quot;key\u0026quot; : \u0026quot;503\u0026quot;, \u0026quot;doc_count\u0026quot; : 441 }, { \u0026quot;key\u0026quot; : \u0026quot;N/A\u0026quot;, \u0026quot;doc_count\u0026quot; : 0 } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"缺省聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/missing/"},{"category":null,"content":"统计聚合 #  stats 聚合是一个多值指标聚合，用于计算数值数据的汇总。这种聚合有助于快速了解数值字段的分布情况。它可以直接作用于字段，应用脚本来派生值，或处理缺少字段的文档。 stats 聚合返回五个值：\n count : 收集到的值的数量 min : 最低值 max : 最高值 sum : 所有值的总和 avg : 值的平均数（总和除以数量）  参数说明 #  stats 聚合支持以下可选参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 要聚合的字段。必须是数值字段。   script 可选 Object 用于计算聚合自定义值的脚本。可单独使用或与 field 一起使用。   missing 可选 Number 用于缺少目标字段的文档的默认值。    参考样例 #  以下示例计算 stats 聚合的电力使用情况。\n创建一个名为 power_usage 的索引，并添加包含给定小时内消耗的千瓦时 (kWh) 数量的文档：\nPUT /power_usage/_bulk?refresh=true {\u0026quot;index\u0026quot;: {}} {\u0026quot;device_id\u0026quot;: \u0026quot;A1\u0026quot;, \u0026quot;kwh\u0026quot;: 1.2} {\u0026quot;index\u0026quot;: {}} {\u0026quot;device_id\u0026quot;: \u0026quot;A2\u0026quot;, \u0026quot;kwh\u0026quot;: 0.7} {\u0026quot;index\u0026quot;: {}} {\u0026quot;device_id\u0026quot;: \u0026quot;A3\u0026quot;, \u0026quot;kwh\u0026quot;: 1.5} 要在所有文档中对 kwh 字段计算统计信息，使用一个名为 consumption_stats 的 stats 聚合，聚合字段为 kwh 。将 size 设置为 0 表示不应返回文档命中：\nGET /power_usage/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;consumption_stats\u0026quot;: { \u0026quot;stats\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;kwh\u0026quot; } } } } 返回内容为索引中的三个文档包含 count 、 min 、 max 、 avg 和 sum 值：\n{ ... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 3, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;consumption_stats\u0026quot;: { \u0026quot;count\u0026quot;: 3, \u0026quot;min\u0026quot;: 0.699999988079071, \u0026quot;max\u0026quot;: 1.5, \u0026quot;avg\u0026quot;: 1.1333333452542622, \u0026quot;sum\u0026quot;: 3.400000035762787 } } } 每个分组运行 stats 聚合 #  您可以通过在 device_id 字段中将 stats 聚合嵌套在 terms 聚合中来为每个设备计算单独的统计信息。 terms 聚合根据唯一的 device_id 值将文档分组，而 stats 聚合在每个分组内计算汇总统计信息：\nGET /power_usage/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;per_device\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;device_id.keyword\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;device_usage_stats\u0026quot;: { \u0026quot;stats\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;kwh\u0026quot; } } } } } } 返回为每个 device_id 返回一个分组，每个分组内包含计算出的 count 、 min 、 max 、 avg 和 sum 字段：\n{ ... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 3, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;per_device\u0026quot;: { \u0026quot;doc_count_error_upper_bound\u0026quot;: 0, \u0026quot;sum_other_doc_count\u0026quot;: 0, \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: \u0026quot;A1\u0026quot;, \u0026quot;doc_count\u0026quot;: 1, \u0026quot;device_usage_stats\u0026quot;: { \u0026quot;count\u0026quot;: 1, \u0026quot;min\u0026quot;: 1.2000000476837158, \u0026quot;max\u0026quot;: 1.2000000476837158, \u0026quot;avg\u0026quot;: 1.2000000476837158, \u0026quot;sum\u0026quot;: 1.2000000476837158 } }, { \u0026quot;key\u0026quot;: \u0026quot;A2\u0026quot;, \u0026quot;doc_count\u0026quot;: 1, \u0026quot;device_usage_stats\u0026quot;: { \u0026quot;count\u0026quot;: 1, \u0026quot;min\u0026quot;: 0.699999988079071, \u0026quot;max\u0026quot;: 0.699999988079071, \u0026quot;avg\u0026quot;: 0.699999988079071, \u0026quot;sum\u0026quot;: 0.699999988079071 } }, { \u0026quot;key\u0026quot;: \u0026quot;A3\u0026quot;, \u0026quot;doc_count\u0026quot;: 1, \u0026quot;device_usage_stats\u0026quot;: { \u0026quot;count\u0026quot;: 1, \u0026quot;min\u0026quot;: 1.5, \u0026quot;max\u0026quot;: 1.5, \u0026quot;avg\u0026quot;: 1.5, \u0026quot;sum\u0026quot;: 1.5 } } ] } } } 这使您能够通过单个查询比较不同设备的使用统计信息。\n使用脚本计算派生值 #  您也可以使用脚本计算 stats 聚合中使用的值。当指标来自文档字段或需要转换时，这很有用。\n例如，在运行 stats 聚合之前，将千瓦时（kWh）转换为瓦时（Wh），因为 1 kWh 等于 1,000 Wh ，你可以使用一个将每个值乘以 1,000 的脚本。以下脚本 doc['kwh'].value * 1000 用于推导每个文档的输入值：\nGET /power_usage/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;usage_wh_stats\u0026quot;: { \u0026quot;stats\u0026quot;: { \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;doc['kwh'].value * 1000\u0026quot; } } } } } 返回的 stats 聚合反映了 1200 、 700 和 1500 Wh 的值\n{ ... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 3, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;usage_wh_stats\u0026quot;: { \u0026quot;count\u0026quot;: 3, \u0026quot;min\u0026quot;: 699.999988079071, \u0026quot;max\u0026quot;: 1500, \u0026quot;avg\u0026quot;: 1133.3333452542622, \u0026quot;sum\u0026quot;: 3400.000035762787 } } } 使用带有字段的值脚本 #\n 当将字段与转换结合使用时，你可以同时指定 field 和 script 。这允许使用 _value 变量来在脚本中引用字段的值。\n以下示例在计算 stats 聚合之前将每个能量读数增加 5%：\nGET /power_usage/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;adjusted_usage\u0026quot;: { \u0026quot;stats\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;kwh\u0026quot;, \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;_value * 1.05\u0026quot; } } } } } 缺省值 #\n 如果某些文档不包含目标字段，它们默认会被排除在聚合之外。要使用默认值包含它们，你可以指定 missing 参数。\n以下请求将缺失的 kwh 值视为 0.0 ：\nGET /power_usage/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;consumption_with_default\u0026quot;: { \u0026quot;stats\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;kwh\u0026quot;, \u0026quot;missing\u0026quot;: 0.0 } } } } \n","subcategory":null,"summary":"","tags":null,"title":"统计聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/stats/"},{"category":null,"content":"统计分组聚合 #  stats_bucket 统计分组聚合是一个同级聚合，它为先前聚合的分组返回各种统计信息（ count 、 min 、 max 、 avg 和 sum ）。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n参数说明 #  stats_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。    参考样例 #  以下示例创建一个以一个月为间隔的日期直方图。 sum 子聚合计算每个月所有字节的总和。最后， stats_bucket 聚合从这些总和中返回 count 、 avg 、 sum 、 min 和 max 统计信息：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } } } }, \u0026quot;stats_monthly_bytes\u0026quot;: { \u0026quot;stats_bucket\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;visits_per_month\u0026gt;sum_of_bytes\u0026quot; } } } } 返回内容 #  该聚合返回每个分组的五个基本统计数据：\n{ \u0026quot;took\u0026quot;: 4, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 } } ] }, \u0026quot;stats_monthly_bytes\u0026quot;: { \u0026quot;count\u0026quot;: 3, \u0026quot;min\u0026quot;: 2804103, \u0026quot;max\u0026quot;: 39103067, \u0026quot;avg\u0026quot;: 26575229.666666668, \u0026quot;sum\u0026quot;: 79725689 } } } \n","subcategory":null,"summary":"","tags":null,"title":"统计分组聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/stats-bucket/"},{"category":null,"content":"经典词元生成器 #  经典（classic）词元生成器会解析文本，并应用英语语法规则将文本拆分为词元。它包含以下特定的逻辑来处理匹配规则：\n 首字母缩写词 电子邮件地址 域名 某些类型的标点符号   这种词元生成器最适合处理英语文本。对于其他语言，尤其是那些具有不同语法结构的语言，它可能无法产生最佳效果。\n 经典词元生成器按如下方式解析文本：\n 标点符号：在大多数标点符号处拆分文本，并移除标点字符。后面不跟空格的点号会被视为词元的一部分。 连字符：在连字符处拆分单词，但当词元中存在数字时除外。当词元中存在数字时，该词元不会被拆分，而是被当作产品编号处理。 电子邮件：识别电子邮件地址和主机名，并将它们作为单个词元保留。  参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用经典词元生成器的分词器：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_classic_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;classic\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_classic_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_classic_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;For product AB3423, visit X\u0026amp;Y at example.com, email info@example.com, or call the operator's phone number 1-800-555-1234. P.S. 你好.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;For\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 3, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;product\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;AB3423\u0026quot;, \u0026quot;start_offset\u0026quot;: 12, \u0026quot;end_offset\u0026quot;: 18, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;visit\u0026quot;, \u0026quot;start_offset\u0026quot;: 20, \u0026quot;end_offset\u0026quot;: 25, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;X\u0026amp;Y\u0026quot;, \u0026quot;start_offset\u0026quot;: 26, \u0026quot;end_offset\u0026quot;: 29, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;COMPANY\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;at\u0026quot;, \u0026quot;start_offset\u0026quot;: 30, \u0026quot;end_offset\u0026quot;: 32, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 }, { \u0026quot;token\u0026quot;: \u0026quot;example.com\u0026quot;, \u0026quot;start_offset\u0026quot;: 33, \u0026quot;end_offset\u0026quot;: 44, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;HOST\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 6 }, { \u0026quot;token\u0026quot;: \u0026quot;email\u0026quot;, \u0026quot;start_offset\u0026quot;: 46, \u0026quot;end_offset\u0026quot;: 51, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 7 }, { \u0026quot;token\u0026quot;: \u0026quot;info@example.com\u0026quot;, \u0026quot;start_offset\u0026quot;: 52, \u0026quot;end_offset\u0026quot;: 68, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;EMAIL\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 8 }, { \u0026quot;token\u0026quot;: \u0026quot;or\u0026quot;, \u0026quot;start_offset\u0026quot;: 70, \u0026quot;end_offset\u0026quot;: 72, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 9 }, { \u0026quot;token\u0026quot;: \u0026quot;call\u0026quot;, \u0026quot;start_offset\u0026quot;: 73, \u0026quot;end_offset\u0026quot;: 77, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 10 }, { \u0026quot;token\u0026quot;: \u0026quot;the\u0026quot;, \u0026quot;start_offset\u0026quot;: 78, \u0026quot;end_offset\u0026quot;: 81, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 11 }, { \u0026quot;token\u0026quot;: \u0026quot;operator's\u0026quot;, \u0026quot;start_offset\u0026quot;: 82, \u0026quot;end_offset\u0026quot;: 92, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;APOSTROPHE\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 12 }, { \u0026quot;token\u0026quot;: \u0026quot;phone\u0026quot;, \u0026quot;start_offset\u0026quot;: 93, \u0026quot;end_offset\u0026quot;: 98, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 13 }, { \u0026quot;token\u0026quot;: \u0026quot;number\u0026quot;, \u0026quot;start_offset\u0026quot;: 99, \u0026quot;end_offset\u0026quot;: 105, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 14 }, { \u0026quot;token\u0026quot;: \u0026quot;1-800-555-1234\u0026quot;, \u0026quot;start_offset\u0026quot;: 106, \u0026quot;end_offset\u0026quot;: 120, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 15 }, { \u0026quot;token\u0026quot;: \u0026quot;P.S.\u0026quot;, \u0026quot;start_offset\u0026quot;: 122, \u0026quot;end_offset\u0026quot;: 126, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ACRONYM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 16 }, { \u0026quot;token\u0026quot;: \u0026quot;你\u0026quot;, \u0026quot;start_offset\u0026quot;: 127, \u0026quot;end_offset\u0026quot;: 128, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;CJ\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 17 }, { \u0026quot;token\u0026quot;: \u0026quot;好\u0026quot;, \u0026quot;start_offset\u0026quot;: 128, \u0026quot;end_offset\u0026quot;: 129, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;CJ\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 18 } ] } 词元类型 #  经典（classic）词元生成器产生的词元有以下类型：\n词元类型及描述 #     词元类型 描述     \u0026lt;ALPHANUM\u0026gt; 由字母、数字或两者组合而成的字母数字词元。   \u0026lt;APOSTROPHE\u0026gt; 包含撇号的词元，常用于所有格或缩写形式（例如 John's）。   \u0026lt;ACRONYM\u0026gt; 首字母缩写词或缩写，通常以句点结尾（例如 P.S. 或 U.S.A.）。   \u0026lt;COMPANY\u0026gt; 代表公司名称的词元（例如 X\u0026amp;Y）。如果这些词元没有自动生成，你可能需要进行自定义配置或使用过滤器。   \u0026lt;EMAIL\u0026gt; 与电子邮件地址匹配的词元，包含 @ 符号和域名（例如 support@widgets.co 或 info@example.com）。   \u0026lt;HOST\u0026gt; 与网站或主机名匹配的词元，通常包含 www. 或诸如 .com 之类的域名后缀（例如 www.example.com 或 example.org）。   \u0026lt;NUM\u0026gt; 仅包含数字或类似数字序列的词元（例如 1-800、12345 或 3.14）。   \u0026lt;CJ\u0026gt; 代表中文或日文字符的词元。   \u0026lt;ACRONYM_DEP\u0026gt; 已弃用的首字母缩写词处理方式（例如，旧版本中具有不同解析规则的首字母缩写词）。很少使用，主要用于与旧版词元生成器规则保持向后兼容。    ","subcategory":null,"summary":"","tags":null,"title":"经典词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/classic/"},{"category":null,"content":"经典分词过滤器 #  经典（classic）分词过滤器的主要功能是与经典词元生成器配合使用。它通过应用以下常见的转换操作来处理词元，这些操作有助于文本分析和搜索：\n 移除所有格词尾，例如 “’s” 。比如，“John’s” 会变为 “John”。 从首字母缩略词中移除句点。例如，“D.A.R.P.A.” 会变为 “DARPA”。  参考样例 #  以下示例请求创建了一个名为 custom_classic_filter 的新索引，并配置了一个使用经典分词过滤器的分词器。\nPUT /custom_classic_filter { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_classic\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;classic\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;classic\u0026quot;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /custom_classic_filter/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_classic\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;John's co-operate was excellent.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;John\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;APOSTROPHE\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;co\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 9, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;operate\u0026quot;, \u0026quot;start_offset\u0026quot;: 10, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;was\u0026quot;, \u0026quot;start_offset\u0026quot;: 18, \u0026quot;end_offset\u0026quot;: 21, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;excellent\u0026quot;, \u0026quot;start_offset\u0026quot;: 22, \u0026quot;end_offset\u0026quot;: 31, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"经典分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/classic/"},{"category":null,"content":"累积和聚合 #  cumulative_sum 累积和聚合是一个父聚合，用于计算上一个聚合的存储分组的累积总和。\n累积和是给定序列的部分和的序列。例如，序列 {a，b，c,...} 的累积和为 a、a+b、a+b+c 等。您可以使用累积总和来可视化字段随时间的变化率。\n参数说明 #  cumulative_sum 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合存储分组的路径。请参阅存储分组路径 。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。    参考样例 #  以下示例创建间隔为一个月的日期直方图。sum 子聚合计算每个月所有字节的总和。最后，cumulative_sum 聚合计算每个月存储分组的累积字节数：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;no-of-bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;cumulative_bytes\u0026quot;: { \u0026quot;cumulative_sum\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;no-of-bytes\u0026quot; } } } } } } 返回内容\n{ \u0026quot;took\u0026quot;: 8, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;no-of-bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 }, \u0026quot;cumulative_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;no-of-bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 }, \u0026quot;cumulative_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 41907170 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;no-of-bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 }, \u0026quot;cumulative_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 79725689 } } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"累积和聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/cumulative-sum/"},{"category":null,"content":"索引模板 #  索引模板允许您使用预定义的映射和设置初始化新索引。例如，如果连续索引日志数据，可以定义一个索引模板，以便所有这些索引都具有相同数量的碎片和副本。\n创建模板 #  要创建索引模板，请使用 POST 请求：\nPOST _index_template 此命令创建一个名为 daily_logs 的模板，并将其应用于名称与正则表达式 logs-2023-01-* 匹配的任何新索引，还将其添加到 my_log 别名中：\nPUT _index_template/daily_logs { \u0026#34;index_patterns\u0026#34;: [ \u0026#34;logs-2023-01-*\u0026#34; ], \u0026#34;template\u0026#34;: { \u0026#34;aliases\u0026#34;: { \u0026#34;my_logs\u0026#34;: {} }, \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 2, \u0026#34;number_of_replicas\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; }, \u0026#34;value\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } } 您应该看到以下响应：\n{ \u0026#34;acknowledged\u0026#34;: true } 如果创建名为 logs-2023-01-01 的索引，可以看到它具有模板中的映射和设置：\nPUT logs-2023-01-01 GET logs-2023-01-01 { \u0026#34;logs-2023-01-01\u0026#34;: { \u0026#34;aliases\u0026#34;: { \u0026#34;my_logs\u0026#34;: {} }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; }, \u0026#34;value\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } }, \u0026#34;settings\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;creation_date\u0026#34;: \u0026#34;1673588860779\u0026#34;, \u0026#34;number_of_shards\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;number_of_replicas\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;uuid\u0026#34;: \u0026#34;S1vMSMDHSAuS2IzPcOHpOA\u0026#34;, \u0026#34;version\u0026#34;: { \u0026#34;created\u0026#34;: \u0026#34;7110199\u0026#34; }, \u0026#34;provided_name\u0026#34;: \u0026#34;logs-2023-01-01\u0026#34; } } } } 与此模式匹配的任何其他索引\u0026mdash; logs-2023-01-02 、 logs-2033-01-03等\u0026mdash; 都将继承相同的映射和设置。\n检索模板 #  要列出所有索引模板，请执行以下操作：\nGET _cat/templates 要按名称查找模板，请执行以下操作：\nGET _index_template/daily_logs 要获取所有模板的列表，请执行以下操作：\nGET _index_template/daily_logs 要获取与模式匹配的所有模板的列表，请执行以下操作：\nGET _index_template/daily* 要检查是否存在特定模板，请执行以下操作：\nHEAD _index_template/\u0026lt;name\u0026gt; 配置多个模板 #  您可以为索引创建多个索引模板。如果索引名称与多个模板匹配，Easysearch 将合并所有匹配模板中的所有映射和设置，并将其应用于索引。\n最近创建的索引模板中的设置将覆盖旧索引模板的设置。因此，您可以首先在通用模板中定义一些通用设置，这些通用模板可以充当一个包罗万象的模板，然后根据需要添加更多专用设置。\n更好的方法是使用 order 参数显式指定模板优先级。Easysearch 首先应用优先级较低的模板，然后使用优先级较高的模板覆盖它们。\n例如，假设您有以下两个模板都与 logs-2023-01-02 索引匹配，并且 number_of_shards 字段中存在冲突：\nTemplate 1 #  PUT _index_template/template-01 { \u0026#34;index_patterns\u0026#34;: [ \u0026#34;logs*\u0026#34; ], \u0026#34;priority\u0026#34;: 0, \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 2 } } } Template 2 #  PUT _index_template/template-02 { \u0026#34;index_patterns\u0026#34;: [ \u0026#34;logs-2023-01-*\u0026#34; ], \u0026#34;priority\u0026#34;: 1, \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3 } } } 由于 template-02 具有较高的 priority 值，因此它优先于 template-01 。 logs-2023-01-02 索引的 number_of_shards 值为 3 。\n删除模板 #  可以使用索引模板的名称删除索引模板，如以下命令所示：\nDELETE _index_template/daily_logs 索引模板选项 #  您可以指定下表中显示的选项：\n   Option Type Description Required     priority Number 指定索引模板的优先级。 No   create Boolean 指定此索引模板是否应替换现有模板。 No    ","subcategory":null,"summary":"","tags":null,"title":"索引模板","url":"/easysearch/v1.15.0/docs/references/management/index-templates/"},{"category":null,"content":"索引压缩 #  索引编码 #  索引编码决定索引的存储字段如何被压缩和存储在磁盘上。索引编码由静态的 index.codec 设置来控制，该设置指定压缩算法。这个设置会影响索引分片的大小和索引操作的性能。\nEasysearch 提供了多种基于索引编码的压缩方案，以降低索引的存储成本。\ndefault – 该编码使用LZ4算法和预设字典，优先考虑性能而非压缩比。与best_compression相比，它提供更快的索引和搜索操作，但可能导致更大的索引/分片大小。如果在索引设置中未提供编码，则默认使用LZ4作为压缩算法。\nbest_compression – 该编码底层使用zlib算法进行压缩。它能实现高压缩比，从而减小索引大小。然而，这可能会增加索引操作期间的额外CPU使用，并可能随后导致较高的索引和搜索延迟。\n从 Easysearch 1.1 开始，增加了基于 Zstandard 压缩算法的新编码方式。这种算法在压缩比和速度之间提供了良好的平衡。\nZSTD 与默认编解码器相比，该编解码器提供了与best_compression编解码器相当的压缩比，CPU使用合理，索引和搜索性能也有所提高。\nsource 复用 #  source_reuse： 启用 source_reuse 配置项能够去除 _source 字段中与 doc_values 或倒排索引重复的部分，从而有效减小索引总体大小，这个功能对日志类索引效果尤其明显。\nsource_reuse 支持对以下数据类型进行压缩：keyword，integer，long，short，boolean，float，half_float，double，geo_point，ip， 如果是 text 类型，需要默认启用 keyword 类型的 multi-field 映射。 以上类型必须启用 doc_values 映射（默认启用）才能压缩。\n使用限制 #  当索引里包含 nested 类型映射，或插件额外提供的数据类型时，不能启用 source_reuse，例如 knn 索引。\n压缩效果对比 #  Easysearch 压缩效果对比如下\n 使用 Nginx 日志作为数据样本 就 Elasticsearch 6.4.3 和 Easysearch 1.1 进行对比 默认未开启压缩 开启 best_compression 压缩 开启 ZSTD 压缩 开启 ZSTD 加 _source 优化  Easysearch 1.1 版本 相比 Elasticsearch 索引整体大小降低了40%~50%。\nElasticsearch v6.4.3\n   index pri rep docs.count store.size pri.store.size     nginxt_default 5 1 1000000 413.1mb 413.1mb   nginx_best 1 1 1000000 316.2mb 316.2mb    Easysearch v1.1\n   index pri rep docs.count store.size pri.store.size     nginxt_default 1 1 1000000 321.6mb 321.6mb   nginx_best 1 1 1000000 262.8mb 262.8mb   nginx_zstd 1 1 1000000 205.6mb 205.6mb   nginx_reuse 1 1 1000000 157mb 157mb    如何启用 #  单个索引配置示例 #  创建并设置索引的 codec 为ZSTD：\nPUT test-index { \u0026#34;settings\u0026#34;: { \u0026#34;index.codec\u0026#34;: \u0026#34;ZSTD\u0026#34; } } 创建并设置索引的 codec 为ZSTD，并启用 source_reuse：\nPUT test-index { \u0026#34;settings\u0026#34;: { \u0026#34;index.codec\u0026#34;: \u0026#34;ZSTD\u0026#34;, \u0026#34;index.source_reuse\u0026#34;: \u0026#34;true\u0026#34; } } 索引模板配置示例 #  在 index_template 配置 ZSTD 和 source_reuse：\nPUT _index_template/daily_logs { \u0026#34;index_patterns\u0026#34;: [ \u0026#34;logs-2020-*\u0026#34; ], \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;index.codec\u0026#34;: \u0026#34;ZSTD\u0026#34;, \u0026#34;index.source_reuse\u0026#34;: true }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;field1\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"索引压缩","url":"/easysearch/v1.15.0/docs/references/document/index-compression/"},{"category":null,"content":"索引分词器 #  索引分词器是在索引创建时指定的，在文档写入时对文本字段进行分词。\n写入索引分词器的生效流程 #  为了确定在对文档建立索引时对某个字段使用哪种分词器，Easysearch 会按以下顺序检查以下参数：\n 字段的分词器映射参数，即analyzer analysis.analyzer.default 索引设置 standard标准分词器（默认分词器）   在指定索引分词器时，请记住，在大多数情况下，为索引中的每个文本字段指定一个分词器效果最佳。使用相同的分词器在文本字段（在建立索引时）和查询字符串（在查询时），可确保搜索所使用的词项与存储在索引中的词项相同。\n 为字段指定索引分词器 #  在创建索引映射时，可以为每个文本字段提供 analyzer（分词器）参数。例如，以下请求为 text_entry 字段指定了 simple 简单分词器：\nPUT testindex { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;text_entry\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;simple\u0026quot; } } } } 为索引指定默认索引分词器 #  如果想对索引中的所有文本字段使用相同的分词器，则可以在索引设置中按如下方式进行指定analysis.analyzer.default：\nPUT testindex { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;simple\u0026quot; } } } } }  如果您未指定默认分词器，那么将使用standard标准分词器。\n ","subcategory":null,"summary":"","tags":null,"title":"索引分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/index-analyzers/"},{"category":null,"content":"系统日志 #  Easysearch 日志包含监控群集操作和故障排除问题的重要信息。日志的位置因安装类型而异：\n 在 Docker 上，Easysearch 将大多数日志写入控制台，并将其余日志存储在 Easysearch/logs/ 中。 tarball 安装也使用 easysearch/logs/ 。 在 RPM 和 Debian 安装上， Easysearch 将日志写入 /var/log/Easysearch/ 。  日志可作为 .log （纯文本）和 .json 文件使用。\n应用程序日志 #  对于其应用程序日志，Easysearch 使用 Apache Log4j 2 其内置日志级别（从最低到最高）为 TRACE 、 DEBUG 、 INFO 、 WARN 、 ERROR 和 FATAL 。默认 Easysearch 日志级别为 INFO 。\n您可以更改各个 Easysearch 模块的日志级别，而不是更改默认日志级别（ logger.level ）：\nPUT /_cluster/settings { \u0026#34;persistent\u0026#34; : { \u0026#34;logger.org.easysearch.index.reindex\u0026#34; : \u0026#34;DEBUG\u0026#34; } } 此示例更改后，Easysearch 在重新索引操作期间会发出更详细的日志：\n[2019-10-18T16:52:51,184][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: starting [2019-10-18T16:52:51,186][DEBUG][o.e.i.r.TransportReindexAction] [node1] executing initial scroll against [some-index] [2019-10-18T16:52:51,291][DEBUG][o.e.i.r.TransportReindexAction] [node1] scroll returned [3] documents with a scroll id of [DXF1Z==] [2019-10-18T16:52:51,292][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: got scroll response with [3] hits [2019-10-18T16:52:51,294][DEBUG][o.e.i.r.WorkerBulkByScrollTaskState] [node1] [1626]: preparing bulk request for [0s] [2019-10-18T16:52:51,297][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: preparing bulk request [2019-10-18T16:52:51,299][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: sending [3] entry, [222b] bulk request [2019-10-18T16:52:51,310][INFO ][o.e.c.m.MetaDataMappingService] [node1] [some-new-index/R-j3adc6QTmEAEb-eAie9g] create_mapping [_doc] [2019-10-18T16:52:51,383][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: got scroll response with [0] hits [2019-10-18T16:52:51,384][DEBUG][o.e.i.r.WorkerBulkByScrollTaskState] [node1] [1626]: preparing bulk request for [0s] [2019-10-18T16:52:51,385][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: preparing bulk request [2019-10-18T16:52:51,386][DEBUG][o.e.i.r.TransportReindexAction] [node1] [1626]: finishing without any catastrophic failures [2019-10-18T16:52:51,395][DEBUG][o.e.i.r.TransportReindexAction] [node1] Freed [1] contexts DEBUG 和 TRACE 级别非常冗长。如果启用其中一个来解决问题，请在完成后禁用它。\n还有其他方法可以更改日志级别：\n 向 easysearch.yml 添加行：  logger.org.easysearch.index.reindex: debug 如果您希望在多个集群中重用日志记录配置，或者使用单个节点调试启动问题，那么修改 easysearch.yml 最有意义。\nModify log4j2.properties:   # Define a new logger with unique ID of reindex logger.reindex.name = org.easysearch.index.reindex # Set the log level for that ID logger.reindex.level = debug 这种方法非常灵活，但需要熟悉 Log4j 2 属性文件语法. 通常，其他选项提供了更简单的配置体验。\n如果检查配置目录中的默认 log4j2.properties 文件，可以看到一些 Easysearch 特定的变量：\n appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n appender.rolling_old.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}.log  ${sys:es.logs.base_path} is the directory for logs (for example, /var/log/easysearch/ ). ${sys:es.logs.cluster_name} is the name of the cluster. [%node_name] is the name of the node.  Slow logs #  Easysearch 有两个 slow logs ，帮助您识别性能问题的日志：搜索慢日志和索引慢日志。\n这些日志依赖于阈值来定义什么是“慢”搜索或索引操作。例如，如果完成一个查询需要 15 秒以上的时间，您可能会认为它很慢。与为模块配置的应用程序日志不同，为索引配置慢日志。默认情况下，两个日志都被禁用（所有阈值都设置为“-1”）：\nGET \u0026lt;some-index\u0026gt;/_settings?include_defaults=true { \u0026quot;indexing\u0026quot;: { \u0026quot;slowlog\u0026quot;: { \u0026quot;reformat\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;threshold\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;warn\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;trace\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;debug\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;info\u0026quot;: \u0026quot;-1\u0026quot; } }, \u0026quot;source\u0026quot;: \u0026quot;1000\u0026quot;, \u0026quot;level\u0026quot;: \u0026quot;TRACE\u0026quot; } }, \u0026quot;search\u0026quot;: { \u0026quot;slowlog\u0026quot;: { \u0026quot;level\u0026quot;: \u0026quot;TRACE\u0026quot;, \u0026quot;threshold\u0026quot;: { \u0026quot;fetch\u0026quot;: { \u0026quot;warn\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;trace\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;debug\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;info\u0026quot;: \u0026quot;-1\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;warn\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;trace\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;debug\u0026quot;: \u0026quot;-1\u0026quot;, \u0026quot;info\u0026quot;: \u0026quot;-1\u0026quot; } } } } } 要启用这些日志，请增加一个或多个阈值：\nPUT \u0026lt;some-index\u0026gt;/_settings { \u0026#34;indexing\u0026#34;: { \u0026#34;slowlog\u0026#34;: { \u0026#34;threshold\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;warn\u0026#34;: \u0026#34;15s\u0026#34;, \u0026#34;trace\u0026#34;: \u0026#34;750ms\u0026#34;, \u0026#34;debug\u0026#34;: \u0026#34;3s\u0026#34;, \u0026#34;info\u0026#34;: \u0026#34;10s\u0026#34; } }, \u0026#34;source\u0026#34;: \u0026#34;500\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34; } } } 在此示例中，Easysearch 记录 WARN 级别需要 15 秒或更长时间的索引操作，以及 INFO 级别需要 10 到 14.x 秒的操作。如果将阈值设置为 0 秒，Easysearch 将记录所有操作，这对于测试是否确实启用了慢速日志非常有用。\n reformat 指定是将文档 _source 字段记录为单行（ true ）还是让它跨越多行（ false ）。 source 是要记录的文档 _source 字段的字符数。 level 是要包含的最低日志级别。  easysearch_index_indexing_slowlog.log 中的一行可能如下所示：\nnode1 | [2019-10-24T19:48:51,012][WARN][i.i.s.index] [node1] [some-index/i86iF5kyTyy-PS8zrdDeAA] took[3.4ms], took_millis[3], type[_doc], id[1], routing[], source[{\u0026quot;title\u0026quot;:\u0026quot;Your Name\u0026quot;, \u0026quot;Director\u0026quot;:\u0026quot;Makoto Shinkai\u0026quot;}] 如果将阈值或级别设置得太低，慢日志可能会占用大量磁盘空间。考虑暂时启用它们以进行故障排除或性能调整。要禁用慢速日志，请将所有阈值返回到 -1 。\n弃用日志 #  弃用日志记录客户端对集群进行弃用 API 调用的时间。这些日志可以帮助您在升级到新的主要版本之前识别并修复问题。默认情况下，Easysearch 在 WARN 级别记录不推荐的 API 调用，这几乎适用于所有用例。如果需要，可以使用 _cluster/settings 、 easysearch.yml 或 log4j2.properties 配置 logger.deprecation.level 。\n","subcategory":null,"summary":"","tags":null,"title":"系统日志","url":"/easysearch/v1.15.0/docs/references/management/logs/"},{"category":null,"content":"简繁转换分词器 #  stconvert-analyzer 简繁体转换分词器，可在索引与查询阶段将 简体中文 与 繁体中文 之间进行双向转换，解决两种文字体系混合检索的问题。\n参数说明 #     参数 说明 默认值     convert_type 转换方向，可选：s2t（简 → 繁），t2s（繁 → 简） s2t   keep_both 是否同时保留转换前后两种 token false   delimiter 当 keep_both=true 时，两种 token 之间的分隔符 ,    使用介绍 #  映射创建\nPUT /stconvert/ { \u0026#34;settings\u0026#34; : { \u0026#34;analysis\u0026#34; : { \u0026#34;analyzer\u0026#34; : { \u0026#34;tsconvert\u0026#34; : { \u0026#34;tokenizer\u0026#34; : \u0026#34;tsconvert\u0026#34; } }, \u0026#34;tokenizer\u0026#34; : { \u0026#34;tsconvert\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;stconvert\u0026#34;, \u0026#34;delimiter\u0026#34; : \u0026#34;#\u0026#34;, \u0026#34;keep_both\u0026#34; : false, \u0026#34;convert_type\u0026#34; : \u0026#34;t2s\u0026#34; } }, \u0026#34;filter\u0026#34;: { \u0026#34;tsconvert\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;stconvert\u0026#34;, \u0026#34;delimiter\u0026#34; : \u0026#34;#\u0026#34;, \u0026#34;keep_both\u0026#34; : false, \u0026#34;convert_type\u0026#34; : \u0026#34;t2s\u0026#34; } }, \u0026#34;char_filter\u0026#34; : { \u0026#34;tsconvert\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;stconvert\u0026#34;, \u0026#34;convert_type\u0026#34; : \u0026#34;t2s\u0026#34; } } } } } 分词测试\nGET stconvert/_analyze { \u0026#34;tokenizer\u0026#34; : \u0026#34;keyword\u0026#34;, \u0026#34;filter\u0026#34; : [\u0026#34;lowercase\u0026#34;], \u0026#34;char_filter\u0026#34; : [\u0026#34;tsconvert\u0026#34;], \u0026#34;text\u0026#34; : \u0026#34;国际國際\u0026#34; } # 返回 { \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;国际国际\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } 归一化方法\nPUT index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;tsconvert\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stconvert\u0026#34;, \u0026#34;convert_type\u0026#34;: \u0026#34;t2s\u0026#34; } }, \u0026#34;normalizer\u0026#34;: { \u0026#34;my_normalizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [ \u0026#34;tsconvert\u0026#34; ], \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34; ] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;foo\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;normalizer\u0026#34;: \u0026#34;my_normalizer\u0026#34; } } } } PUT index/_doc/1 { \u0026quot;foo\u0026quot;: \u0026quot;國際\u0026quot; }\nPUT index/_doc/2 { \u0026quot;foo\u0026quot;: \u0026quot;国际\u0026quot; }\nGET index/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;foo\u0026quot;: \u0026quot;国际\u0026quot; } } }\nGET index/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;foo\u0026quot;: \u0026quot;國際\u0026quot; } } } \n","subcategory":null,"summary":"","tags":null,"title":"简繁转换分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/stconvert-analyzer/"},{"category":null,"content":"简单分词器 #  简单（simple）分词器是一种非常基础的分词器，它会将文本中的非字母字符串拆分成词项，并将这些词项转换为小写形式。与标准分词器不同的是，简单分词器将除字母字符之外的所有内容都视为分隔符，这意味着它不会把数字、标点符号或特殊字符识别为词元的一部分。\n参考样例 #  以下命令创建一个名为 my_simple_index 并使用简单分词器的索引：\nPUT /my_simple_index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;simple\u0026quot; } } } } 配置自定义分词器 #  以下命令配置了一个索引，该索引带有一个自定义分词器，这个自定义分词器等同于添加了 html_strip 字符过滤器的简单分词器：\nPUT /my_custom_simple_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;char_filter\u0026quot;: { \u0026quot;html_strip\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;html_strip\u0026quot; } }, \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_lowercase_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;lowercase\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_simple_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;char_filter\u0026quot;: [\u0026quot;html_strip\u0026quot;], \u0026quot;tokenizer\u0026quot;: \u0026quot;my_lowercase_tokenizer\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;] } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_simple_analyzer\u0026quot; } } } } 产生的词元 #  以下请求用来检查分词器生成的词元：\nPOST /my_custom_simple_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_simple_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;\u0026lt;p\u0026gt;The slow turtle swims over to dogs \u0026amp;copy; 2024!\u0026lt;/p\u0026gt;\u0026quot; } 返回内容中包含了产生的词元\n{ \u0026quot;tokens\u0026quot;: [ {\u0026quot;token\u0026quot;: \u0026quot;the\u0026quot;,\u0026quot;start_offset\u0026quot;: 3,\u0026quot;end_offset\u0026quot;: 6,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;slow\u0026quot;,\u0026quot;start_offset\u0026quot;: 7,\u0026quot;end_offset\u0026quot;: 11,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 1}, {\u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;,\u0026quot;start_offset\u0026quot;: 12,\u0026quot;end_offset\u0026quot;: 18,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;swims\u0026quot;,\u0026quot;start_offset\u0026quot;: 19,\u0026quot;end_offset\u0026quot;: 24,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;over\u0026quot;,\u0026quot;start_offset\u0026quot;: 25,\u0026quot;end_offset\u0026quot;: 29,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;to\u0026quot;,\u0026quot;start_offset\u0026quot;: 30,\u0026quot;end_offset\u0026quot;: 32,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 5}, {\u0026quot;token\u0026quot;: \u0026quot;dogs\u0026quot;,\u0026quot;start_offset\u0026quot;: 33,\u0026quot;end_offset\u0026quot;: 37,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 6} ] } ","subcategory":null,"summary":"","tags":null,"title":"简单分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/simple-analyzer/"},{"category":null,"content":"简单分割匹配词元生成器 #  简单分割匹配（simple_pattern_split）词元生成器使用正则表达式将文本分割成词元。该正则表达式定义了用于确定文本分割位置的模式。文本中任何匹配该模式的部分都会被用作分隔符，分隔符之间的文本则成为一个词元。当你想定义分隔符，并根据某个模式对文本的其余部分进行分词时，可以使用此词元生成器。\n该词元生成器仅将输入文本中匹配到的部分（基于正则表达式）用作分隔符或边界，以将文本分割成词条。匹配到的部分不会包含在生成的词元中。例如，如果将词元生成器配置为在点号（.）处分割文本，输入文本为 one.two.three，那么生成的词元就是 one、two 和 three。点号本身不会包含在生成的词条中。\n参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用简单分割匹配词元生成器的分词器。该分词器被配置为在连字符（-）处分割文本：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_pattern_split_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;simple_pattern_split\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;-\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_pattern_split_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_pattern_split_tokenizer\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_pattern_split_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_pattern_split_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;2024-10-09\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;2024\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;10\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;09\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } 参数说明 #  简单分割匹配词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     max_token_length 可选 整数 设置生成词元的最大长度。若超过该长度，词元将按照 max_token_length 所配置的长度拆分为多个词元。默认值为 255。    ","subcategory":null,"summary":"","tags":null,"title":"简单分割匹配词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/simple-pattern-split/"},{"category":null,"content":"空格词元生成器 #  空格（whitespace）词元生成器会依据空白字符（如空格、制表符和换行符）对文本进行拆分。它将由空白字符分隔开的每个单词都视为一个词元，并且不会执行诸如转换为小写形式或去除标点符号等额外的规范化操作。\n参考样例 #  以下示例请求创建一个名为 my_index 的新索引，并配置一个使用空格词元生成器的分词器：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;whitespace_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;whitespace\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_whitespace_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;whitespace_tokenizer\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_whitespace_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_whitespace_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch is fast! Really fast.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;fast!\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;Really\u0026quot;, \u0026quot;start_offset\u0026quot;: 20, \u0026quot;end_offset\u0026quot;: 26, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;fast.\u0026quot;, \u0026quot;start_offset\u0026quot;: 27, \u0026quot;end_offset\u0026quot;: 32, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } 参数说明 #  空格词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     max_token_length 可选 整数 设置生成词元的最大长度。如果超过此长度，词元将按照 max_token_length 配置的长度拆分为多个词元。默认值为 255。    ","subcategory":null,"summary":"","tags":null,"title":"空格词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/whitespace/"},{"category":null,"content":"空格分词器 #  空格（whitespace）分词器仅基于空白字符（例如，空格和制表符）将文本拆分为词元。比如转换为小写形式或移除停用词这样的转换操作，它都不会应用，因此文本的原始大小写形式会被保留，并且标点符号也会作为词元的一部分包含在内。\n参考样例 #  以下命令创建一个名为 my_whitespace_index 并使用空格分词器的索引：\nPUT /my_whitespace_index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;whitespace\u0026quot; } } } } 配置自定义分词器 #  以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于空格分词器：\nPUT /my_custom_whitespace_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_whitespace_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;whitespace\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;] } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_whitespace_analyzer\u0026quot; } } } } 产生的词元 #  以下请求用来检查分词器生成的词元：\nPOST /my_custom_whitespace_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_whitespace_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;The SLOW turtle swims away! 123\u0026quot; } 返回内容中包含了产生的词元\n{ \u0026quot;tokens\u0026quot;: [ {\u0026quot;token\u0026quot;: \u0026quot;the\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 3,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;slow\u0026quot;,\u0026quot;start_offset\u0026quot;: 4,\u0026quot;end_offset\u0026quot;: 8,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 1}, {\u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;,\u0026quot;start_offset\u0026quot;: 9,\u0026quot;end_offset\u0026quot;: 15,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;swims\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 21,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;away!\u0026quot;,\u0026quot;start_offset\u0026quot;: 22,\u0026quot;end_offset\u0026quot;: 27,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;123\u0026quot;,\u0026quot;start_offset\u0026quot;: 28,\u0026quot;end_offset\u0026quot;: 31,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 5} ] } ","subcategory":null,"summary":"","tags":null,"title":"空格分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/whitespace-analyzer/"},{"category":null,"content":"稀有分组聚合 #  rare_terms 稀有分组聚合是一个分组聚合，用于识别数据集中的不常见词项。与 terms 聚合（查找最常见的词项）不同， rare_terms 聚合查找出现频率最低的词项。 rare_terms 聚合适用于异常检测、长尾分析和异常报告等应用。\n 可以使用 terms 通过按升序计数排序（ \u0026ldquo;order\u0026rdquo;: {\u0026ldquo;count\u0026rdquo;: \u0026ldquo;asc\u0026rdquo;} ）来搜索不常见的值。然而，我们强烈不建议这种做法，因为在涉及多个分片时，它可能导致不准确的结果。一个全局上不常见的词项可能不会在每个单个分片上显得不常见，或者可能完全不在某些分片返回的最不常见结果中。相反，一个在某个分片上出现频率较低的词项可能在另一个分片上很常见。在这两种情况下，分片级别的聚合可能会遗漏稀有词项，导致整体结果不正确。我们建议使用 rare_terms 聚合代替 terms 聚合，它专门设计用于更准确地处理这些情况。\n 近似结果 #  计算 rare_terms 聚合的精确结果需要编译所有分片上的值完整映射，这需要过多的运行时内存。因此， rare_terms 聚合结果被近似处理。\nrare_terms 计算中的大多数错误是假阴性或“遗漏”的值，这些值定义了聚合检测测试的灵敏度。 rare_terms 聚合使用 CuckooFilter 算法以实现适当的灵敏度和可接受的内存使用平衡。有关 CuckooFilter 算法的描述，请参阅这篇论文。\n控制灵敏度 #  rare_terms 聚合算法中的灵敏度误差被衡量为被遗漏的稀有值的比例，或 false negatives/target values 。例如，如果聚合在包含 5,000 个稀有值的数据集中遗漏了 100 个稀有值，灵敏度误差为 100/5000 = 0.02 ，或 2%。\n您可以调整 precision 参数在 rare_terms 聚合中来控制灵敏度和内存使用之间的权衡。\n这些因素也会影响灵敏度和内存的权衡：\n 唯一值的总数 数据集中稀有项的比例  以下指南可以帮助你决定使用哪个 precision 值。\n计算内存使用 #  运行时内存使用以绝对值描述，通常以 RAM 的 MB 为单位。\n内存使用随唯一项数量的增加而线性增长。线性扩展系数因 precision 参数而异，大致在每百万个唯一值 1.0 到 2.5 MB 之间。对于默认的 precision 设置为 0.001 时，内存成本约为每百万个唯一值 1.75 MB。\n管理灵敏度误差 #  敏感性误差随唯一值总数的增加而线性增长。有关估算唯一值数量的信息，请参阅基数聚合。\n在默认 precision 设置下，即使对于具有 1000 万至 2000 万个唯一值的集合，敏感性误差也极少超过 2.5%。对于 precision 设置为 0.00001 时，敏感性误差极少超过 0.6%。然而，非常低的稀有值绝对数量可能导致误差率出现较大波动（如果有两个稀有值，漏掉其中一个会导致 50% 的误差率）。\n与其他聚合的兼容性 #  rare_terms 聚合使用广度优先收集模式，与某些子聚合和嵌套配置中需要深度优先收集模式的聚合不兼容。\n参数说明 #  rare_terms 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必填 String 用于分析稀有词的字段。必须是数值类型或具有 keyword 映射的文本类型。   max_doc_count 可选 Integer 一个词被考虑为罕见词所需的最大文档数量。默认值是 1 。最大值是 100 。   precision 可选 Integer 控制用于识别罕见词的算法的精确度。较高的值提供更精确的结果，但会消耗更多内存。默认值是 0.001 。最小（最精确的允许值）是 0.00001 。   include 可选 Array/regex 结果中要包含的词。可以是正则表达式或值的数组。   exclude 可选 Array/regex 从结果中排除的词项。可以是正则表达式或值数组。   missing 可选 String 用于没有聚合字段值的文档的值。    参考样例 #  以下请求数据中仅出现一次的所有目的地机场代码：\nGET sample_data_flights/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rare_destination\u0026quot;: { \u0026quot;rare_terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;DestAirportID\u0026quot;, \u0026quot;max_doc_count\u0026quot;: 1 } } } } 返回内容显示，有两个机场符合仅在数据中出现一次的标准：\n{ \u0026quot;took\u0026quot;: 12, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;rare_destination\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: \u0026quot;ADL\u0026quot;, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key\u0026quot;: \u0026quot;BUF\u0026quot;, \u0026quot;doc_count\u0026quot;: 1 } ] } } } 文档数量限制 #\n 使用 max_doc_count 参数指定 rare_terms 聚合可以返回的最大文档数量。 rare_terms 返回的词项数量没有限制，因此较大的 max_doc_count 值可能会返回非常大的结果集。因此， 100 是允许的最大 max_doc_count 值。\n以下请求数据中最多出现两次的所有目的地机场代码：\nGET sample_data_flights/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rare_destination\u0026quot;: { \u0026quot;rare_terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;DestAirportID\u0026quot;, \u0026quot;max_doc_count\u0026quot;: 2 } } } } 响应显示，有七个目的地机场代码符合出现次数在两次或更少文档中的标准，包括前一个示例中的两个：\n{ \u0026quot;took\u0026quot;: 6, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;rare_destination\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: \u0026quot;ADL\u0026quot;, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key\u0026quot;: \u0026quot;BUF\u0026quot;, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key\u0026quot;: \u0026quot;ABQ\u0026quot;, \u0026quot;doc_count\u0026quot;: 2 }, { \u0026quot;key\u0026quot;: \u0026quot;AUH\u0026quot;, \u0026quot;doc_count\u0026quot;: 2 }, { \u0026quot;key\u0026quot;: \u0026quot;BIL\u0026quot;, \u0026quot;doc_count\u0026quot;: 2 }, { \u0026quot;key\u0026quot;: \u0026quot;BWI\u0026quot;, \u0026quot;doc_count\u0026quot;: 2 }, { \u0026quot;key\u0026quot;: \u0026quot;MAD\u0026quot;, \u0026quot;doc_count\u0026quot;: 2 } ] } } } 过滤 (include 和 exclude) #\n 使用 include 和 exclude 参数来过滤 rare_terms 聚合返回的值。这两个参数可以在同一个聚合中包含。 exclude 过滤器具有优先级；任何被排除的值都会从结果中移除，无论它们是否被明确包含。\ninclude 和 exclude 的参数可以是正则表达式（regex），包括字符串字面量，或数组。混合正则表达式和数组参数会导致错误。例如，以下组合是不允许的：\n\u0026quot;rare_terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;DestAirportID\u0026quot;, \u0026quot;max_doc_count\u0026quot;: 2, \u0026quot;exclude\u0026quot;: [\u0026quot;ABQ\u0026quot;, \u0026quot;AUH\u0026quot;], \u0026quot;include\u0026quot;: \u0026quot;A.*\u0026quot; } 示例：过滤 #  以下示例修改了前面的示例，以包含所有以“A”开头的机场代码，但排除“ABQ”机场代码：\nGET sample_data_flights/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rare_destination\u0026quot;: { \u0026quot;rare_terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;DestAirportID\u0026quot;, \u0026quot;max_doc_count\u0026quot;: 2, \u0026quot;include\u0026quot;: \u0026quot;A.*\u0026quot;, \u0026quot;exclude\u0026quot;: \u0026quot;ABQ\u0026quot; } } } } 响应显示了符合过滤条件的那两个机场代码：\n{ \u0026quot;took\u0026quot;: 4, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;rare_destination\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: \u0026quot;ADL\u0026quot;, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key\u0026quot;: \u0026quot;AUH\u0026quot;, \u0026quot;doc_count\u0026quot;: 2 } ] } } } 示例：使用数组输入进行过滤 #\n 以下示例数据中最多出现两次的 所有目的地机场代码，但指定了要排除的机场代码数组：\nGET sample_data_flights/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rare_destination\u0026quot;: { \u0026quot;rare_terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;DestAirportID\u0026quot;, \u0026quot;max_doc_count\u0026quot;: 2, \u0026quot;exclude\u0026quot;: [\u0026quot;ABQ\u0026quot;, \u0026quot;BIL\u0026quot;, \u0026quot;MAD\u0026quot;] } } } } 响应中排除了排除的机场代码：\n{ \u0026quot;took\u0026quot;: 6, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;rare_destination\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: \u0026quot;ADL\u0026quot;, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key\u0026quot;: \u0026quot;BUF\u0026quot;, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key\u0026quot;: \u0026quot;AUH\u0026quot;, \u0026quot;doc_count\u0026quot;: 2 }, { \u0026quot;key\u0026quot;: \u0026quot;BWI\u0026quot;, \u0026quot;doc_count\u0026quot;: 2 } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"稀有分组聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/rare-terms/"},{"category":null,"content":"移动平均聚合 #  一个 moving_avg 移动平均聚合是一个父级管道聚合，它计算有序数据集中窗口（相邻子集）内指标的一系列平均值。\n要创建一个 moving_avg 聚合，您首先创建一个 histogram 或 date_histogram 聚合。然后，您可以选择在直方图聚合中嵌入一个指标聚合。最后，您在直方图中嵌入 moving_avg 聚合，并将 buckets_path 参数设置为要跟踪的嵌入指标。\n窗口的大小是窗口中连续数据值的数量。在每次迭代中，算法计算窗口中所有数据点的平均值，然后向前滑动一个数据值，排除上一个窗口的第一个值，并包含下一个窗口的第一个值。\n例如，给定数据 [1, 5, 8, 23, 34, 28, 7, 23, 20, 19] ，一个窗口大小为 5 的移动平均如下：\n(1 + 5 + 8 + 23 + 34) / 5 = 14.2 (5 + 8 + 23 + 34 + 28) / 5 = 19.6 (8 + 23 + 34 + 28 + 7) / 5 = 20 ... moving_avg 聚合通常应用于时间序列数据，以平滑噪声或短期波动，并识别趋势。指定较小的窗口大小以平滑小规模波动。指定较大的窗口大小以平滑高频波动或随机噪声，使低频趋势更加明显。\n参数说明 #  moving_avg 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。   window 可选 Numerical 窗口中包含的数据点数量。默认为 5 。   model 可选 String 要使用的加权移动平均模型。选项为 ewma 、 holt 、 holt_winters 、 linear 和 simple 。默认为 simple 。参见模型。   settings 可选 Object 调整窗口的参数。参见模型。   predict 可选 Numerical 要追加到结果末尾的预测值数量。默认为 0 。    参考样例 #  以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， moving_avg 聚合计算这些总和的每月移动平均值：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;my_date_histogram\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;moving_avg\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;sum_of_bytes\u0026quot; } } } } } } 该聚合从第二个分组开始返回 moving_avg 值。第一个分组没有移动平均值，因为没有足够的前置数据点来计算它：\n{ \u0026quot;took\u0026quot;: 5, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;my_date_histogram\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 20953585 } } ] } } } 示例：预测数据 #\n 你可以使用 moving_avg 聚合来预测未来的分组。\n以下示例将上一个示例的间隔减少到一周，并在响应的末尾附加了五个预测的一周分组：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;my_date_histogram\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;week\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;moving_avg\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;sum_of_bytes\u0026quot;, \u0026quot;predict\u0026quot;: 5 } } } } } } 返回内容包含五个预测。请注意，预测的分组的 doc_count 是 0 :\n{ \u0026quot;took\u0026quot;: 5, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;my_date_histogram\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-24T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1742774400000, \u0026quot;doc_count\u0026quot;: 249, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 1531493 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-31T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743379200000, \u0026quot;doc_count\u0026quot;: 1617, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9213161 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 1531493 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-07T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743984000000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9188671 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 5372327 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-14T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1744588800000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9244851 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 6644441.666666667 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-21T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745193600000, \u0026quot;doc_count\u0026quot;: 1609, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9061045 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 7294544 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-28T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745798400000, \u0026quot;doc_count\u0026quot;: 1554, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 8713507 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 7647844.2 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-05T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746403200000, \u0026quot;doc_count\u0026quot;: 1710, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9544718 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9084247 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-12T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1747008000000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9155820 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9150558.4 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-19T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1747612800000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9025078 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9143988.2 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-26T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1748217600000, \u0026quot;doc_count\u0026quot;: 895, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 5047345 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9100033.6 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-06-02T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1748822400000, \u0026quot;doc_count\u0026quot;: 0, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 8297293.6 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-06-09T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1749427200000, \u0026quot;doc_count\u0026quot;: 0, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 8297293.6 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-06-16T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1750032000000, \u0026quot;doc_count\u0026quot;: 0, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 8297293.6 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-06-23T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1750636800000, \u0026quot;doc_count\u0026quot;: 0, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 8297293.6 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-06-30T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1751241600000, \u0026quot;doc_count\u0026quot;: 0, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 8297293.6 } } ] } } } 模型 #\n moving_avg 聚合支持五种模型，这些模型在如何对移动窗口中的值进行加权方面有所不同。\n使用 model 参数来指定要使用的模型。\n   模型名称 模型关键词 加权方式     简单 simple 窗口中所有值的未加权平均值。   线性 linear 使用线性权重衰减，更重视近期值。   指数加权移动平均 ewma 使用指数递减权重，更重视近期值。   Holt holt 使用第二个指数项来平滑长期趋势。   Holt-Winters holt_winters 使用第三个指数项来平滑周期（季节性）效应。    可以使用 settings 对象来设置模型的属性。下表显示了每个模型的可用设置。\n   模型 参数 允许的值 默认值 描述     simple 无 Numeric array 无 窗口中所有值的算术平均值。   linear 无 Numeric array 无 窗口中所有值的加权平均值，较新的值权重更大。   ewma alpha [0, 1] 0.3 衰减参数。更高的值会给最近的数据点赋予更大的权重。   holt alpha [0, 1] 0.3 用于级别组件的衰减参数。    beta [0, 1] 0.1 趋势成分的衰减参数。   holt_winters alpha [0, 1] 0.3 水平成分的衰减参数。    beta [0, 1] 0.3 趋势成分的衰减参数。    gamma [0, 1] 0.3 季节成分的衰减参数。    type add, mult add 定义季节性建模方式：加性或乘性。    period Integer 1 构成周期的分组的数量。    pad Boolean true 是否为 mult 类型模型中的 0 值添加一个小的偏移量，以避免除以零的错误。    示例：Holt 模型 #  holt 模型使用 alpha 和 beta 参数控制的指数衰减计算权重。\n以下请求使用 window 大小为 6 、 alpha 值为 0.4 、 beta 值为 0.2 的 Holt 模型计算总每周字节数的移动平均值：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;my_date_histogram\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;week\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;moving_avg\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;sum_of_bytes\u0026quot;, \u0026quot;window\u0026quot;: 6, \u0026quot;model\u0026quot;: \u0026quot;holt\u0026quot;, \u0026quot;settings\u0026quot;: { \u0026quot;alpha\u0026quot;: 0.4, \u0026quot;beta\u0026quot;: 0.2 } } } } } } } 移动平均数从第二个分组开始：\n{ \u0026quot;took\u0026quot;: 7, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;my_date_histogram\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-24T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1742774400000, \u0026quot;doc_count\u0026quot;: 249, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 1531493 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-31T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743379200000, \u0026quot;doc_count\u0026quot;: 1617, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9213161 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 1531493 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-07T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743984000000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9188671 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 4604160.2 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-14T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1744588800000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9244851 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 6806684.584000001 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-21T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745193600000, \u0026quot;doc_count\u0026quot;: 1609, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9061045 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 8341230.127680001 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-28T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745798400000, \u0026quot;doc_count\u0026quot;: 1554, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 8713507 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9260724.7236736 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-05T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746403200000, \u0026quot;doc_count\u0026quot;: 1710, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9544718 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9657431.903375873 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-12T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1747008000000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9155820 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9173999.55240704 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-19T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1747612800000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9025078 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9172040.511275519 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-26T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1748217600000, \u0026quot;doc_count\u0026quot;: 895, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 5047345 }, \u0026quot;moving_avg_of_sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9108804.964619776 } } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"移动平均聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/moving-avg/"},{"category":null,"content":"移动函数聚合 #  moving_fn 移动函数聚合是一个父级管道聚合，它在滑动窗口上执行脚本。滑动窗口在从父级 histogram 或 date histogram 聚合中提取的一系列值上移动。窗口一次向右移动一个分组； moving_fn 每次窗口移动时都会运行脚本。\n使用 moving_fn 聚合在滑动窗口内的数据上执行任何数值计算。你可以使用 moving_fn 用于以下目的：\n 趋势分析 异常值检测 自定义时间序列分析 自定义平滑算法 数字信号处理 (DSP)  参数说明 #  moving_fn 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   script 必需 String 或 Object 为每个数据窗口计算值的脚本。可以是内联脚本、存储脚本或脚本文件。该脚本可以访问在 buckets_path 参数中定义的变量名。   window 必需 Integer 滑动窗口中的分组的数量。必须是正整数。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。   shift 可选 Integer 窗口要移动的分组的数量。可以是正数（向未来的分组右移）或负数（向过去的分组左移）。默认是 0 ，将窗口立即放置在当前分组的左侧。参见移动窗口。    移动函数的工作原理 #  moving_fn 聚合操作在有序分组序列上的滑动窗口上。从父聚合中的第一个分组开始， moving_fn 执行以下操作：\n 收集由 window 和 shift 参数指定的分组中的子序列（窗口）的值。 将这些值作为数组传递给由 script 指定的函数。 使用 script 从数组中计算出一个值。 将此值作为当前分组的结果返回。 向前移动一个分组并重复此过程。   “过去”和“未来”值暗示时间序列数据，这是移动窗口函数最常见的用例。更一般地说，它们分别指任何有序数据序列中的先前值和即将到来的值。\n moving_fn 应用的脚本可以是一个预定义函数或自定义脚本。分组值通过 values 数组提供给脚本。脚本返回一个双精度值作为结果。结果值 NaN 和 +/- Inf 是允许的，但 null 是不允许的。\n窗口大小 #  window 参数指定定义窗口大小的分组的数量。\n传递给 script 函数的数组是零索引的。其值在脚本中通过 values[0] 到 values[n] 访问，其中 n = values.length - 1 。\n移动窗口 #  shift 参数控制移动窗口相对于当前分组的位置。根据您的分析需求设置 shift ，需要历史背景、当前数据还是未来预测。默认值是 0 ，仅显示过去值（不包括当前分组）。\nshift 的常用值如下：\n 0:仅过去值。不包括当前值。 1:过去值，包括当前值。 window/2:将窗口围绕当前值居中。 window:未来的值，包括当前值。  当窗口扩展到序列开头或结尾的可用数据之外时， window 会自动缩小，仅使用可用点：\n预定义函数 #  moving_fn 聚合支持多种预定义函数，可用于替代自定义脚本。这些函数可通过 MovingFunctions 上下文访问。例如，你可以通过 MovingFunctions.max(values) 访问 max 函数。\n下表描述了预定义函数。\n   函数 模型关键词 描述     最大值 max 窗口中的最大值。   最小值 min 窗口中的最小值。   求和 sum 窗口中值的总和。   无权平均 unweightedAvg 窗口内所有值的未加权平均值，等于 sum / window 。   线性加权平均 linearWeightedAvg 使用线性衰减权重的加权平均，更重视近期值。   指数加权移动平均 ewma 使用指数衰减权重的加权平均，更重视近期值。   Holt holt 使用第二个指数项的加权平均，用于平滑长期趋势。   Holt-Winters holt_wimnters\t 使用第三个指数项的加权平均，用于平滑周期性（季节性）效应。   标准差 stdDev 窗口中值的总和。    所有预定义函数都以 values 数组作为其第一个参数。对于需要额外参数的函数，按顺序在 values 后传递这些参数。例如，通过将 script 值设置为 MovingFunctions.stdDev(values, MovingFunctions.unweightedAvg(values)) 来调用 stdDev 函数。\n下表显示了每个模型所需的设置。\n   函数 参数 参数类型 默认值 描述     max 无 Numeric array 无 窗口的最大值。   min 无 Numeric array 无 窗口的最小值。   sum 无 Numeric array 无 窗口中所有值的总和。   unweightedAvg 无 Numeric array 无 窗口中所有值的算术平均值。   linearWeightedAvg 无 Numeric array 无 窗口中所有值的加权平均值，较新的值权重更大。   ewma alpha [0, 1] 0.3 衰减参数。更高的值会给最近的数据点赋予更大的权重。   holt alpha [0, 1] 0.3 用于级别组件的衰减参数。    beta [0, 1] 0.1 趋势成分的衰减参数。   holt_winters alpha [0, 1] 0.3 用于级别组件的衰减参数。    beta [0, 1] 0.3 趋势成分的衰减参数。    gamma [0, 1] 0.3 季节成分的衰减参数。    type add, mult add 定义季节性如何建模：加性或乘性。    period Integer 1 构成周期的分组数。    pad Boolean true 是否为 0 类型的模型对 mult 值添加一个小的偏移量以避免除以零的错误。   stdDev avg double 无 窗口的标准差。要计算有意义的标准差，请使用滑动窗口数组的平均值，通常为 MovingFunctions.unweightedAvg(values) 。     预定义函数不支持缺少参数的函数签名。因此，即使使用默认值，您也必须提供额外的参数。\n 示例：预定义函数 #  以下示例创建一个日期直方图，间隔为一周。 sum 子聚合计算每周所有记录的字节总和。最后， moving_fn 聚合使用 window 大小为 5 、默认 shift 为 0 ，以及无权重的平均值来计算字节总和的标准差：\nPOST sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;my_date_histo\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;week\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;the_sum\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;moving_fn\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;the_sum\u0026quot;, \u0026quot;window\u0026quot;: 5, \u0026quot;script\u0026quot;: \u0026quot;MovingFunctions.stdDev(values, MovingFunctions.unweightedAvg(values))\u0026quot; } } } } } } 返回内容显示了移动窗口的标准差，从第二个分组开始为零值。对于空窗口或只包含无效值（ null 或 NaN ）的窗口， stdDev 函数返回 0 。\n{ \u0026quot;took\u0026quot;: 15, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;my_date_histo\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-24T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1742774400000, \u0026quot;doc_count\u0026quot;: 249, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 1531493 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: null } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-31T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743379200000, \u0026quot;doc_count\u0026quot;: 1617, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9213161 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 0 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-07T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743984000000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9188671 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 3840834 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-14T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1744588800000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9244851 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 3615414.498228507 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-21T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745193600000, \u0026quot;doc_count\u0026quot;: 1609, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9061045 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 3327358.65618917 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-28T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745798400000, \u0026quot;doc_count\u0026quot;: 1554, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 8713507 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 3058812.9440705855 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-05T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746403200000, \u0026quot;doc_count\u0026quot;: 1710, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9544718 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 195603.33146038183 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-12T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1747008000000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9155820 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 270085.92336040025 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-19T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1747612800000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9025078 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 269477.75659701484 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-26T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1748217600000, \u0026quot;doc_count\u0026quot;: 895, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 5047345 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 267356.5422566652 } } ] } } } 自定义脚本 #\n 你可以提供一个任意的自定义脚本来计算 moving_fn 结果。自定义脚本使用 Painless 脚本语言。\n示例：使用自定义脚本 #  以下示例创建一个日期直方图，间隔为一周。 sum 子聚合计算每周所有应税收入的总额。 moving_fn 脚本返回当前值之前的两个值中较大的那个值，如果两个值不可用，则返回 NaN 。\nPOST sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;my_date_histo\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;order_date\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;week\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;the_sum\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot; } }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;moving_fn\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;the_sum\u0026quot;, \u0026quot;window\u0026quot;: 2, \u0026quot;script\u0026quot;: \u0026quot;return (values.length \u0026lt; 2 ? Double.NaN : (values[0]\u0026gt;values[1] ? values[0] : values[1]))\u0026quot; } } } } } } 该示例返回从第三个分组开始的计算结果，其中存在足够的前置数据来执行计算：\n{ \u0026quot;took\u0026quot;: 7, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;my_date_histo\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-24T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1742774400000, \u0026quot;doc_count\u0026quot;: 582, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 41455.5390625 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: null } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-31T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743379200000, \u0026quot;doc_count\u0026quot;: 1048, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 79448.60546875 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: null } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-07T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743984000000, \u0026quot;doc_count\u0026quot;: 1048, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 78208.4296875 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 79448.60546875 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-14T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1744588800000, \u0026quot;doc_count\u0026quot;: 1073, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 81277.296875 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 79448.60546875 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-21T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745193600000, \u0026quot;doc_count\u0026quot;: 924, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 70494.2578125 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 81277.296875 } } ] } } } 示例：移动平均聚合 #\n holt 模型是一种移动平均，它使用由 alpha 和 beta 参数控制的指数衰减权重。以下示例创建一个日期直方图，间隔为一周。 sum 子聚合计算每周所有字节的总和。最后， moving_fn 聚合使用 Holt 模型计算字节总和的加权平均值，模型大小为 window ，默认值为 shift ， alpha 值为 0.3 ， beta 值为 0.1 ：\nPOST sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;my_date_histogram\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;week\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;the_sum\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;moving_fn\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;the_sum\u0026quot;, \u0026quot;window\u0026quot;: 6, \u0026quot;script\u0026quot;: \u0026quot;MovingFunctions.holt(values, 0.3, 0.1)\u0026quot; } } } } } } 聚合返回从第二个分组开始的移动 holt 平均值：\n{ \u0026quot;took\u0026quot;: 16, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;my_date_histogram\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-24T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1742774400000, \u0026quot;doc_count\u0026quot;: 249, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 1531493 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: null } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-31T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743379200000, \u0026quot;doc_count\u0026quot;: 1617, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9213161 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 1531493 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-07T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743984000000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9188671 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 3835993.3999999994 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-14T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1744588800000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9244851 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 5603111.707999999 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-21T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745193600000, \u0026quot;doc_count\u0026quot;: 1609, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9061045 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 6964515.302359998 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-28T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745798400000, \u0026quot;doc_count\u0026quot;: 1554, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 8713507 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 7930766.089341199 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-05T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746403200000, \u0026quot;doc_count\u0026quot;: 1710, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9544718 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 8536788.607547803 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-12T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1747008000000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9155820 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 9172269.837272028 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-19T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1747612800000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 9025078 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 9166173.88436614 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-26T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1748217600000, \u0026quot;doc_count\u0026quot;: 895, \u0026quot;the_sum\u0026quot;: { \u0026quot;value\u0026quot;: 5047345 }, \u0026quot;the_movavg\u0026quot;: { \u0026quot;value\u0026quot;: 9123157.830417283 } } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"移动函数聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/moving-function/"},{"category":null,"content":"矩阵统计聚合 #  matrix_stats 矩阵统计聚合是一个多值指标聚合，以矩阵形式为两个或多个字段生成协方差统计。\n matrix_stats 聚合不支持脚本。\n 参数说明 #  matrix_stats 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 用于计算矩阵统计的一组字段。   missing 可选 Object 用于替代缺失值的值。默认情况下，会忽略缺失值。参见缺失值。   mode 可选 String 用作多值或数组字段样本的值。允许的值是 avg 、 min 、 max 、 sum 和 median 。默认是 avg 。    参考样例 #  以下示例返回数据中 taxful_total_price 和 products.base_price 字段的统计信息：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;matrix_stats_taxful_total_price\u0026quot;: { \u0026quot;matrix_stats\u0026quot;: { \u0026quot;fields\u0026quot;: [\u0026quot;taxful_total_price\u0026quot;, \u0026quot;products.base_price\u0026quot;] } } } } 返回内容包含聚合结果：\n{ \u0026quot;took\u0026quot;: 250, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;matrix_stats_taxful_total_price\u0026quot;: { \u0026quot;doc_count\u0026quot;: 4675, \u0026quot;fields\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;products.base_price\u0026quot;, \u0026quot;count\u0026quot;: 4675, \u0026quot;mean\u0026quot;: 34.99423943014724, \u0026quot;variance\u0026quot;: 360.5035285833702, \u0026quot;skewness\u0026quot;: 5.530161335032689, \u0026quot;kurtosis\u0026quot;: 131.1630632404217, \u0026quot;covariance\u0026quot;: { \u0026quot;products.base_price\u0026quot;: 360.5035285833702, \u0026quot;taxful_total_price\u0026quot;: 846.6489362233169 }, \u0026quot;correlation\u0026quot;: { \u0026quot;products.base_price\u0026quot;: 1, \u0026quot;taxful_total_price\u0026quot;: 0.8444765264325269 } }, { \u0026quot;name\u0026quot;: \u0026quot;taxful_total_price\u0026quot;, \u0026quot;count\u0026quot;: 4675, \u0026quot;mean\u0026quot;: 75.05542864304839, \u0026quot;variance\u0026quot;: 2788.1879749835425, \u0026quot;skewness\u0026quot;: 15.812149139923994, \u0026quot;kurtosis\u0026quot;: 619.1235507385886, \u0026quot;covariance\u0026quot;: { \u0026quot;products.base_price\u0026quot;: 846.6489362233169, \u0026quot;taxful_total_price\u0026quot;: 2788.1879749835425 }, \u0026quot;correlation\u0026quot;: { \u0026quot;products.base_price\u0026quot;: 0.8444765264325269, \u0026quot;taxful_total_price\u0026quot;: 1 } } ] } } } 下表描述了返回内容的字段。\n   统计 描述     count 用于聚合的文档数量。   mean 从样本计算得到的字段平均值。   variance 均值偏差的平方，衡量数据分布的离散程度。   skewness 衡量分布相对于均值的不对称性指标。   kurtosis 衡量分布尾部重量的指标。随着尾部变轻，峰度减小。通过评估峰度和偏度来判断一个总体是否可能呈正态分布。   covariance 衡量两个字段之间联合变差的指标。正值表示它们的值朝同一方向变动。   correlation 标准化协方差，用于衡量两个字段之间关系强度的指标。可能的值范围从-1 到 1（包含两端），表示完全负相关到完全正相关。值为 0 表示变量之间没有可识别的关系。    缺省值 #  要定义如何处理缺失值，请使用 missing 参数。默认情况下，会忽略缺失值。\n例如，创建一个索引，其中文档 1 缺少 gpa 和 class_grades 字段：\nPOST _bulk { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;students\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;John Doe\u0026quot; } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;students\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;Jonathan Powers\u0026quot;, \u0026quot;gpa\u0026quot;: 3.85, \u0026quot;class_grades\u0026quot;: [3.0, 3.9, 4.0] } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;students\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;Jane Doe\u0026quot;, \u0026quot;gpa\u0026quot;: 3.52, \u0026quot;class_grades\u0026quot;: [3.2, 2.1, 3.8] } 首先，运行不带 missing 参数的 matrix_stats 聚合：\nGET students/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;matrix_stats_taxful_total_price\u0026quot;: { \u0026quot;matrix_stats\u0026quot;: { \u0026quot;fields\u0026quot;: [ \u0026quot;gpa\u0026quot;, \u0026quot;class_grades\u0026quot; ], \u0026quot;mode\u0026quot;: \u0026quot;avg\u0026quot; } } } } 在计算矩阵统计时忽略缺失值：\n{ \u0026quot;took\u0026quot;: 5, \u0026quot;timed_out\u0026quot;: false, \u0026quot;terminated_early\u0026quot;: true, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 3, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;matrix_stats_taxful_total_price\u0026quot;: { \u0026quot;doc_count\u0026quot;: 2, \u0026quot;fields\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;gpa\u0026quot;, \u0026quot;count\u0026quot;: 2, \u0026quot;mean\u0026quot;: 3.684999942779541, \u0026quot;variance\u0026quot;: 0.05444997482300096, \u0026quot;skewness\u0026quot;: 0, \u0026quot;kurtosis\u0026quot;: 1, \u0026quot;covariance\u0026quot;: { \u0026quot;gpa\u0026quot;: 0.05444997482300096, \u0026quot;class_grades\u0026quot;: 0.09899998760223136 }, \u0026quot;correlation\u0026quot;: { \u0026quot;gpa\u0026quot;: 1, \u0026quot;class_grades\u0026quot;: 0.9999999999999991 } }, { \u0026quot;name\u0026quot;: \u0026quot;class_grades\u0026quot;, \u0026quot;count\u0026quot;: 2, \u0026quot;mean\u0026quot;: 3.333333333333333, \u0026quot;variance\u0026quot;: 0.1800000381469746, \u0026quot;skewness\u0026quot;: 0, \u0026quot;kurtosis\u0026quot;: 1, \u0026quot;covariance\u0026quot;: { \u0026quot;gpa\u0026quot;: 0.09899998760223136, \u0026quot;class_grades\u0026quot;: 0.1800000381469746 }, \u0026quot;correlation\u0026quot;: { \u0026quot;gpa\u0026quot;: 0.9999999999999991, \u0026quot;class_grades\u0026quot;: 1 } } ] } } } 要设置缺失字段为 0 ，请将 missing 参数作为键值映射提供。尽管 class_grades 是数组字段，但 matrix_stats 聚合会将多值数字字段展平为每个文档的平均值，因此您必须将单个数字作为缺失值提供：\nGET students/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;matrix_stats_taxful_total_price\u0026quot;: { \u0026quot;matrix_stats\u0026quot;: { \u0026quot;fields\u0026quot;: [\u0026quot;gpa\u0026quot;, \u0026quot;class_grades\u0026quot;], \u0026quot;mode\u0026quot;: \u0026quot;avg\u0026quot;, \u0026quot;missing\u0026quot;: { \u0026quot;gpa\u0026quot;: 0, \u0026quot;class_grades\u0026quot;: 0 } } } } } 在计算矩阵统计时，会用 0 替换任何缺失的 gpa 或 class_grades 值：\n{ \u0026quot;took\u0026quot;: 23, \u0026quot;timed_out\u0026quot;: false, \u0026quot;terminated_early\u0026quot;: true, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 3, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;matrix_stats_taxful_total_price\u0026quot;: { \u0026quot;doc_count\u0026quot;: 3, \u0026quot;fields\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;gpa\u0026quot;, \u0026quot;count\u0026quot;: 3, \u0026quot;mean\u0026quot;: 2.456666628519694, \u0026quot;variance\u0026quot;: 4.55363318017324, \u0026quot;skewness\u0026quot;: -0.688130006360758, \u0026quot;kurtosis\u0026quot;: 1.5, \u0026quot;covariance\u0026quot;: { \u0026quot;gpa\u0026quot;: 4.55363318017324, \u0026quot;class_grades\u0026quot;: 4.143944374667273 }, \u0026quot;correlation\u0026quot;: { \u0026quot;gpa\u0026quot;: 1, \u0026quot;class_grades\u0026quot;: 0.9970184390038257 } }, { \u0026quot;name\u0026quot;: \u0026quot;class_grades\u0026quot;, \u0026quot;count\u0026quot;: 3, \u0026quot;mean\u0026quot;: 2.2222222222222223, \u0026quot;variance\u0026quot;: 3.793703722777191, \u0026quot;skewness\u0026quot;: -0.6323693521730989, \u0026quot;kurtosis\u0026quot;: 1.5000000000000002, \u0026quot;covariance\u0026quot;: { \u0026quot;gpa\u0026quot;: 4.143944374667273, \u0026quot;class_grades\u0026quot;: 3.793703722777191 }, \u0026quot;correlation\u0026quot;: { \u0026quot;gpa\u0026quot;: 0.9970184390038257, \u0026quot;class_grades\u0026quot;: 1 } } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"矩阵统计聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/matrix-stats/"},{"category":null,"content":"省略词分词过滤器 #  省略词（Elision）分词过滤器用于从某些语言的单词中去除省略的字符。省略现象通常出现在像法语这样的语言中，在这些语言里，单词常常会发生缩合，并与后面的单词结合，常见的方式是省略一个元音字母，并用一个撇号来替代。\n 省略词分词过滤器已经在以下语言分词器中预先配置好了：加泰罗尼亚语/catalan、法语/french、爱尔兰语/irish和意大利语/italian。\n 参数说明 #  自定义省略词分词过滤器可使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     articles 若未配置 articles_path 则为必需 字符串数组 定义当某些冠词或短词作为省略形式的一部分出现时，哪些应该被移除。   articles_path 若未配置 articles 则为必需 字符串 指定在分析过程中应被移除的自定义冠词列表的路径。   articles_case 可选 布尔值 指定在匹配省略形式时，该过滤器是否区分大小写。默认值为 false。    参考样例 #  法语中默认的省略形式集合包括 l'、m'、t'、qu'、n'、s'、j'、d'、c'、jusqu'、quoiqu'、lorsqu' 和 puisqu'。你可以通过配置 french_elision 分词过滤器来更新这个集合。以下示例请求创建了一个名为 french_texts 的新索引，并配置了一个带有 french_elision 过滤器的分词器：\nPUT /french_texts { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;french_elision\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;elision\u0026quot;, \u0026quot;articles\u0026quot;: [ \u0026quot;l\u0026quot;, \u0026quot;t\u0026quot;, \u0026quot;m\u0026quot;, \u0026quot;d\u0026quot;, \u0026quot;n\u0026quot;, \u0026quot;s\u0026quot;, \u0026quot;j\u0026quot; ] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;french_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;, \u0026quot;french_elision\u0026quot;] } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;text\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;french_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /french_texts/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;french_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;L'étudiant aime l'école et le travail.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;étudiant\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;aime\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;école\u0026quot;, \u0026quot;start_offset\u0026quot;: 16, \u0026quot;end_offset\u0026quot;: 23, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;et\u0026quot;, \u0026quot;start_offset\u0026quot;: 24, \u0026quot;end_offset\u0026quot;: 26, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;le\u0026quot;, \u0026quot;start_offset\u0026quot;: 27, \u0026quot;end_offset\u0026quot;: 29, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;travail\u0026quot;, \u0026quot;start_offset\u0026quot;: 30, \u0026quot;end_offset\u0026quot;: 37, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 } ] } ","subcategory":null,"summary":"","tags":null,"title":"省略词分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/elision/"},{"category":null,"content":"省略符号分词过滤器 #  省略符号（apostrophe）分词过滤器的主要功能是移除所有格形式的撇号以及跟在撇号后面的所有内容。这在分析那些大量使用撇号的语言的文本时非常有用，比如土耳其语。在土耳其语中，撇号用于将词根与后缀分隔开来，这些后缀包括所有格后缀、格标记以及其他语法词尾。\n参考样例 #  以下示例请求创建了一个名为 custom_text_index 的新索引，在索引设置中配置了一个自定义分词器，并在映射中使用该分词器：\nPUT /custom_text_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;apostrophe\u0026quot; ] } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /custom_text_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;John's car is faster than Peter's bike\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;john\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;faster\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 20, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;than\u0026quot;, \u0026quot;start_offset\u0026quot;: 21, \u0026quot;end_offset\u0026quot;: 25, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;peter\u0026quot;, \u0026quot;start_offset\u0026quot;: 26, \u0026quot;end_offset\u0026quot;: 33, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 }, { \u0026quot;token\u0026quot;: \u0026quot;bike\u0026quot;, \u0026quot;start_offset\u0026quot;: 34, \u0026quot;end_offset\u0026quot;: 38, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 6 } ] }  内置的省略符号分词过滤器并不适用于像法语这样的语言，在法语中撇号会出现在单词的开头。例如，C'est l'amour de l'école 这句话使用该过滤器分词后将会得到四个词元：“C”、“l”、“de” 和 “l”。\n ","subcategory":null,"summary":"","tags":null,"title":"省略符号分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/apostrophe/"},{"category":null,"content":"直方图聚合 #  histogram 直方图聚合根据指定的间隔对文档进行分组。\n使用 histogram 聚合，您可以非常轻松地可视化给定范围内文档中值的分布。\n以下示例将 number_of_bytes 字段按 10,000 个间隔进行分组：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;number_of_bytes\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;interval\u0026quot;: 10000 } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;number_of_bytes\u0026quot; : { \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : 0.0, \u0026quot;doc_count\u0026quot; : 13372 }, { \u0026quot;key\u0026quot; : 10000.0, \u0026quot;doc_count\u0026quot; : 702 } ] } } 参数说明 #  histogram 聚合支持以下参数。\n   参数 必需/可选 数据类型 描述     interval\t 必填 Numeric 构造每个分组所使用的字段值宽度。    ","subcategory":null,"summary":"","tags":null,"title":"直方图聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/histogram/"},{"category":null,"content":"百分位聚合 #  percentiles 百分位聚合估计数值字段在给定百分位处的值。这对于理解分布边界很有用。\n例如， load_time 的 95th 百分位 = 120ms 表示 95% 的值小于或等于 120 毫秒。\n与 cardinality 指标类似， percentile 指标也是近似的。\n参数说明 #  percentiles 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 用于计算百分位数的数值字段。         percents 可选 Array of doubles 返回百分位数列表。默认为 [1, 5, 25, 50, 75, 95, 99] 。   keyed 可选 Boolean 如果设置为 false ，则将结果作为数组返回。否则将结果作为 JSON 对象返回。默认值为 true 。   tdigest.compression 可选 Double 控制 tdigest 算法的准确性和内存使用。参见使用 tdigest 进行精度调整。   hdr.number_of_significant_value_digits 可选 Integer HDR 直方图的精度设置。参见 HDR 直方图。   missing 可选 Numeric 当文档中目标字段缺失时使用的默认值。   script 可选 Object 用于计算自定义值而不是使用字段的脚本。支持内联和存储脚本。    参考样例 #  首先，创建一个索引：\nPUT /latency_data { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;load_time\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot; } } } } 添加示例数值以说明百分位数计算：\nPOST /latency_data/_bulk { \u0026quot;index\u0026quot;: {} } { \u0026quot;load_time\u0026quot;: 20 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;load_time\u0026quot;: 40 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;load_time\u0026quot;: 60 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;load_time\u0026quot;: 80 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;load_time\u0026quot;: 100 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;load_time\u0026quot;: 120 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;load_time\u0026quot;: 140 } 百分位数聚合 #\n 以下示例计算 load_time 字段的默认百分位数集：\nGET /latency_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;load_time_percentiles\u0026quot;: { \u0026quot;percentiles\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;load_time\u0026quot; } } } } 默认情况下，会返回第 1、5、25、50、75、95 和 99 个百分位数：\n{ ... \u0026quot;aggregations\u0026quot;: { \u0026quot;load_time_percentiles\u0026quot;: { \u0026quot;values\u0026quot;: { \u0026quot;1.0\u0026quot;: 20, \u0026quot;5.0\u0026quot;: 20, \u0026quot;25.0\u0026quot;: 40, \u0026quot;50.0\u0026quot;: 80, \u0026quot;75.0\u0026quot;: 120, \u0026quot;95.0\u0026quot;: 140, \u0026quot;99.0\u0026quot;: 140 } } } } 自定义分位数 #  您可以使用 percents 数组指定确切的百分位数：\nGET /latency_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;load_time_percentiles\u0026quot;: { \u0026quot;percentiles\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;load_time\u0026quot;, \u0026quot;percents\u0026quot;: [50, 90, 99] } } } } 返回内容仅包含所请求的三个百分位聚合：\n{ ... \u0026quot;aggregations\u0026quot;: { \u0026quot;load_time_percentiles\u0026quot;: { \u0026quot;values\u0026quot;: { \u0026quot;50.0\u0026quot;: 80, \u0026quot;90.0\u0026quot;: 140, \u0026quot;99.0\u0026quot;: 140 } } } } 键值响应 #\n 可以通过将 keyed 参数设置为 false 将返回的聚合格式从 JSON 对象更改为键值对列表：\nGET /latency_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;load_time_percentiles\u0026quot;: { \u0026quot;percentiles\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;load_time\u0026quot;, \u0026quot;keyed\u0026quot;: false } } } } 响应以值数组的形式提供百分位数：\n{ ... \u0026quot;aggregations\u0026quot;: { \u0026quot;load_time_percentiles\u0026quot;: { \u0026quot;values\u0026quot;: [ { \u0026quot;key\u0026quot;: 1, \u0026quot;value\u0026quot;: 20 }, { \u0026quot;key\u0026quot;: 5, \u0026quot;value\u0026quot;: 20 }, { \u0026quot;key\u0026quot;: 25, \u0026quot;value\u0026quot;: 40 }, { \u0026quot;key\u0026quot;: 50, \u0026quot;value\u0026quot;: 80 }, { \u0026quot;key\u0026quot;: 75, \u0026quot;value\u0026quot;: 120 }, { \u0026quot;key\u0026quot;: 95, \u0026quot;value\u0026quot;: 140 }, { \u0026quot;key\u0026quot;: 99, \u0026quot;value\u0026quot;: 140 } ] } } } 使用 tdigest 进行精确度调整 #\n tdigest 算法是计算百分位数的默认方法。它提供了一种内存高效的方式来估计百分位数排名，尤其是在处理浮点数据（如响应时间或延迟）时。\n与精确的百分位数计算不同， tdigest 使用概率方法将值分组到质心——即总结分布的小型簇。这种方法能够在无需将所有原始数据存储在内存中的情况下，为大多数百分位数提供准确的估计。\n该算法设计为在分布的尾部附近（即低百分位数（如第 1 位）和高百分位数（如第 99 位））具有高度准确性，这些通常是性能分析中最重要的一部分。您可以使用 compression 参数控制结果的精度。\n较高的 compression 值意味着使用更多的质心，这会增加准确性（尤其是在尾部），但需要更多的内存和 CPU。较低的 compression 值会减少内存使用并加快执行速度，但结果可能不够准确。\n适合使用 tdigest 的情况：\n 您的数据包含浮点值，例如响应时间、延迟或持续时间。 您需要在极端百分位数中获得准确的结果，例如第 1 位或第 99 位。  避免使用 tdigest 的情况：\n 您只处理整数数据，并希望获得最大速度。 您不太在意分布尾部的准确性，更倾向于快速聚合（可以考虑使用 hdr 代替）。  以下示例将 tdigest.compression 设置为 200 :\nGET /latency_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;load_time_percentiles\u0026quot;: { \u0026quot;percentiles\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;load_time\u0026quot;, \u0026quot;tdigest\u0026quot;: { \u0026quot;compression\u0026quot;: 200 } } } } } HDR 直方图 #\n 高动态范围（HDR）直方图是计算百分位的另一种方法。它特别适用于处理大型数据集和延迟测量。它专为速度而设计，同时支持宽动态范围值，并保持固定且可配置的精度水平。\n与 tdigest 不同，后者在分布的尾部（极端百分位）提供更高的准确性，HDR 优先考虑速度和范围内均匀的准确性。当分组的数量很大且不需要对稀有值进行极端精确度时，它效果最佳。\n例如，如果你正在测量从 1 微秒到 1 小时的范围内的响应时间，并使用 3 位有效数字配置 HDR，它将记录值精度为 ±1 微秒（直到 1 毫秒）和 ±3.6 秒（接近 1 小时）。\n这种权衡使得 HDR 比 tdigest 快得多，但内存消耗更大。\n下表展示了 HDR 显著数字的分解情况。\n   有效数字 相对精度（最大误差）     1 1 份在 10 份中 = 10%   2 1 份在 100 份中 = 1%   3 1 份在 1000 中 = 0.1%   4 1 份在 10000 中 = 0.01%   5 1 份在 100000 中 = 0.001%    如果您需要使用 HDR，则应：\n 正在跨多个分组进行聚合。 不需要在尾部的百分位数上要求极端的精确度。 确保有足够的内存可用。  你应该避免 HDR，如果：\n 尾部精度很重要。 你正在分析偏斜或稀疏的数据分布。  以下示例中将 hdr.number_of_significant_value_digits 设置为 3 ：\nGET /latency_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;load_time_percentiles\u0026quot;: { \u0026quot;percentiles\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;load_time\u0026quot;, \u0026quot;hdr\u0026quot;: { \u0026quot;number_of_significant_value_digits\u0026quot;: 3 } } } } } 缺省值 #  使用 missing 设置为不包含目标字段的文档配置备用值：\nGET /latency_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;load_time_percentiles\u0026quot;: { \u0026quot;percentiles\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;load_time\u0026quot;, \u0026quot;missing\u0026quot;: 0 } } } } Script 脚本 #\n 可以使用脚本动态计算值，而不是指定字段。当您需要应用转换（如货币转换或应用权重）时，这很有用。\n内联脚本 #  使用脚本计算派生值：\nGET /latency_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;adjusted_percentiles\u0026quot;: { \u0026quot;percentiles\u0026quot;: { \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;doc['load_time'].value * 1.2\u0026quot; }, \u0026quot;percents\u0026quot;: [50, 95] } } } } 存储脚本 #\n 首先，使用以下请求创建一个示例脚本：\nPOST _scripts/load_script { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;painless\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;doc[params.field].value * params.multiplier\u0026quot; } } 然后用 percentiles 聚合中的存储脚本，提供 params 存储脚本所需的：\nGET /latency_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;adjusted_percentiles\u0026quot;: { \u0026quot;percentiles\u0026quot;: { \u0026quot;script\u0026quot;: { \u0026quot;id\u0026quot;: \u0026quot;load_script\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;load_time\u0026quot;, \u0026quot;multiplier\u0026quot;: 1.2 } }, \u0026quot;percents\u0026quot;: [50, 95] } } } } ","subcategory":null,"summary":"","tags":null,"title":"百分位聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/percentile/"},{"category":null,"content":"百分位数分组聚合 #  percentiles_bucket 百分位数分组聚合是一个同级聚合，用于计算分位数的位置。\npercentiles_bucket 聚合精确计算分位数，不使用近似或插值。每个分位数都返回为目标分位数小于或等于的最近值。\npercentiles_bucket 聚合需要将整个值列表临时保存在内存中，即使对于大型数据集也是如此。相比之下， percentiles 指标聚合使用更少的内存，但会近似百分比。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n参数说明 #  percentiles_bucket聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。   percents 可选 List 一个包含任意数量数值百分比值的列表，这些值将被包含在输出中。有效值为 0.0 到 100.0（含）。默认为 [1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0] 。   keyed 可选 Boolean 是否将输出格式化为字典，而不是键值对对象数组。默认为 true （以键值对格式化输出）。    参考样例 #  以下示例创建一个以一周为间隔的日期直方图。 sum 子聚合为每周汇总 taxful_total_price 。最后， percentiles_bucket 聚合计算这些汇总的每周百分位数值：\nPOST sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;weekly_sales\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;order_date\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;week\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;total_price\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot; } } } }, \u0026quot;percentiles_monthly_sales\u0026quot;: { \u0026quot;percentiles_bucket\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;weekly_sales\u0026gt;total_price\u0026quot; } } } } 聚合返回每周价格总计的默认百分位数值：\n{ \u0026quot;took\u0026quot;: 4, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;weekly_sales\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-24T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1742774400000, \u0026quot;doc_count\u0026quot;: 582, \u0026quot;total_price\u0026quot;: { \u0026quot;value\u0026quot;: 41455.5390625 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-31T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743379200000, \u0026quot;doc_count\u0026quot;: 1048, \u0026quot;total_price\u0026quot;: { \u0026quot;value\u0026quot;: 79448.60546875 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-07T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743984000000, \u0026quot;doc_count\u0026quot;: 1048, \u0026quot;total_price\u0026quot;: { \u0026quot;value\u0026quot;: 78208.4296875 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-14T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1744588800000, \u0026quot;doc_count\u0026quot;: 1073, \u0026quot;total_price\u0026quot;: { \u0026quot;value\u0026quot;: 81277.296875 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-21T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745193600000, \u0026quot;doc_count\u0026quot;: 924, \u0026quot;total_price\u0026quot;: { \u0026quot;value\u0026quot;: 70494.2578125 } } ] }, \u0026quot;percentiles_monthly_sales\u0026quot;: { \u0026quot;values\u0026quot;: { \u0026quot;1.0\u0026quot;: 41455.5390625, \u0026quot;5.0\u0026quot;: 41455.5390625, \u0026quot;25.0\u0026quot;: 70494.2578125, \u0026quot;50.0\u0026quot;: 78208.4296875, \u0026quot;75.0\u0026quot;: 79448.60546875, \u0026quot;95.0\u0026quot;: 81277.296875, \u0026quot;99.0\u0026quot;: 81277.296875 } } } } 示例：选项修改 #\n 下一个示例使用与上一个示例相同的数据计算百分位数，但有以下不同：\n percents 参数指定仅计算第 25、50 和 75 个百分位数。 使用 format 参数追加字符串格式输出。 通过将 keyed 参数设置为 false ，结果以键值对对象形式显示（追加字符串值）。  POST sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;weekly_sales\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;order_date\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;week\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;total_price\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot; } } } }, \u0026quot;percentiles_monthly_sales\u0026quot;: { \u0026quot;percentiles_bucket\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;weekly_sales\u0026gt;total_price\u0026quot;, \u0026quot;percents\u0026quot;: [25.0, 50.0, 75.0], \u0026quot;format\u0026quot;: \u0026quot;$#,###.00\u0026quot;, \u0026quot;keyed\u0026quot;: false } } } } 选项修改的输出：\n{ \u0026quot;took\u0026quot;: 5, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;weekly_sales\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-24T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1742774400000, \u0026quot;doc_count\u0026quot;: 582, \u0026quot;total_price\u0026quot;: { \u0026quot;value\u0026quot;: 41455.5390625 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-31T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743379200000, \u0026quot;doc_count\u0026quot;: 1048, \u0026quot;total_price\u0026quot;: { \u0026quot;value\u0026quot;: 79448.60546875 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-07T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743984000000, \u0026quot;doc_count\u0026quot;: 1048, \u0026quot;total_price\u0026quot;: { \u0026quot;value\u0026quot;: 78208.4296875 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-14T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1744588800000, \u0026quot;doc_count\u0026quot;: 1073, \u0026quot;total_price\u0026quot;: { \u0026quot;value\u0026quot;: 81277.296875 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-21T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745193600000, \u0026quot;doc_count\u0026quot;: 924, \u0026quot;total_price\u0026quot;: { \u0026quot;value\u0026quot;: 70494.2578125 } } ] }, \u0026quot;percentiles_monthly_sales\u0026quot;: { \u0026quot;values\u0026quot;: [ { \u0026quot;key\u0026quot;: 25, \u0026quot;value\u0026quot;: 70494.2578125, \u0026quot;25.0_as_string\u0026quot;: \u0026quot;$70,494.26\u0026quot; }, { \u0026quot;key\u0026quot;: 50, \u0026quot;value\u0026quot;: 78208.4296875, \u0026quot;50.0_as_string\u0026quot;: \u0026quot;$78,208.43\u0026quot; }, { \u0026quot;key\u0026quot;: 75, \u0026quot;value\u0026quot;: 79448.60546875, \u0026quot;75.0_as_string\u0026quot;: \u0026quot;$79,448.61\u0026quot; } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"百分位数分组聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/percentiles-bucket/"},{"category":null,"content":"百分位排名聚合 #  percentile_ranks 百分比排名聚合估计低于或等于给定阈值的观测值百分比。这对于了解特定值在值分布中的相对位置很有用。 例如，您可以使用百分位排名聚合来学习交易金额 45 与数据集中其他交易值相比如何。百分位排名聚合返回一个值，如 82.3 ，这意味着 82.3% 的交易额低于或等于 45 。\n参数说明 #  percentile_ranks 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 用于计算百分位数的数值字段。   values 必需 Array of doubles 用于计算百分位数的值。   keyed 可选 Boolean 如果设置为 false ，则将结果作为数组返回。否则将结果作为 JSON 对象返回。默认值为 true 。   tdigest.compression 可选 Double 控制 tdigest 算法的准确性和内存使用。参见使用 tdigest 进行精度调整。   hdr.number_of_significant_value_digits 可选 Integer HDR 直方图的精度设置。参见 HDR 直方图。   missing 可选 Numeric 当文档中目标字段缺失时使用的默认值。   script 可选 Object 用于计算自定义值而不是使用字段的脚本。支持内联和存储脚本。    参考样例 #  首先，创建一个示例索引：\nPUT /transaction_data { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;amount\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot; } } } } 添加示例数值以说明百分位数排名计算：\nPOST /transaction_data/_bulk { \u0026quot;index\u0026quot;: {} } { \u0026quot;amount\u0026quot;: 10 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;amount\u0026quot;: 20 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;amount\u0026quot;: 30 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;amount\u0026quot;: 40 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;amount\u0026quot;: 50 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;amount\u0026quot;: 60 } { \u0026quot;index\u0026quot;: {} } { \u0026quot;amount\u0026quot;: 70 } 运行 percentile_ranks 聚合来计算某些值与整体分布的比较情况：\nGET /transaction_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rank_check\u0026quot;: { \u0026quot;percentile_ranks\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;amount\u0026quot;, \u0026quot;values\u0026quot;: [25, 55] } } } } 表明 28.6%的值小于或等于 25 ，71.4%的值小于或等于 55 ：\n{ ... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 7, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;rank_check\u0026quot;: { \u0026quot;values\u0026quot;: { \u0026quot;25.0\u0026quot;: 28.57142857142857, \u0026quot;55.0\u0026quot;: 71.42857142857143 } } } } 键值 #\n 可以通过将 keyed 参数设置为 false 来更改返回的聚合格式，从 JSON 对象更改为键值对列表：\nGET /transaction_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rank_check\u0026quot;: { \u0026quot;percentile_ranks\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;amount\u0026quot;, \u0026quot;values\u0026quot;: [25, 55], \u0026quot;keyed\u0026quot;: false } } } } 返回内容包含一个数组而不是对象：\n{ ... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 7, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;rank_check\u0026quot;: { \u0026quot;values\u0026quot;: [ { \u0026quot;key\u0026quot;: 25, \u0026quot;value\u0026quot;: 28.57142857142857 }, { \u0026quot;key\u0026quot;: 55, \u0026quot;value\u0026quot;: 71.42857142857143 } ] } } } 使用 tdigest 进行精确度调整 #  默认情况下，百分位数排名使用 tdigest 算法计算。您可以通过指定 tdigest.compression 参数来控制准确性和内存使用之间的权衡。更高的值提供更好的准确性，但需要更多的内存。有关 tdigest 工作原理的更多信息，请参阅使用 tdigest 进行精度调整。 以下示例中将 tdigest.compression 设置为 200 ：\nGET /transaction_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rank_check\u0026quot;: { \u0026quot;percentile_ranks\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;amount\u0026quot;, \u0026quot;values\u0026quot;: [25, 55], \u0026quot;tdigest\u0026quot;: { \u0026quot;compression\u0026quot;: 200 } } } } } HDR 直方图 #\n 作为 tdigest 的替代方案，您可以使用高动态范围（HDR）直方图算法，该算法更适合大量分组和快速处理。有关 HDR 直方图的工作原理的更多信息，请参阅 HDR 直方图。\n如果您需要使用 HDR，则应：\n 正在跨多个分组进行聚合。 不需要在尾部的百分位数上要求极端的精确度。 确保有足够的内存可用。  你应该避免 HDR，如果：\n 尾部精度很重要。 你正在分析偏斜或稀疏的数据分布。  以下示例中将 hdr.number_of_significant_value_digits 设置为 3 ：\nGET /transaction_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rank_check\u0026quot;: { \u0026quot;percentile_ranks\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;amount\u0026quot;, \u0026quot;values\u0026quot;: [25, 55], \u0026quot;hdr\u0026quot;: { \u0026quot;number_of_significant_value_digits\u0026quot;: 3 } } } } } 缺省值 #  如果某些文档缺少目标字段，你可以通过设置 missing 参数来指示查询使用备用值。以下示例确保没有 amount 字段的文档被视为其值为 0 ，并包含在百分位数计算中：\nGET /transaction_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rank_check\u0026quot;: { \u0026quot;percentile_ranks\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;amount\u0026quot;, \u0026quot;values\u0026quot;: [25, 55], \u0026quot;missing\u0026quot;: 0 } } } } Script 脚本 #  除了指定字段，您还可以使用脚本动态计算值。这适用于需要应用转换的情况，例如货币转换或应用权重。\n内联脚本 #  以下示例使用内联脚本计算转换后的值 30 和 60 与增加 10%的 amount 字段值的百分位数排名：\nGET /transaction_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rank_check\u0026quot;: { \u0026quot;percentile_ranks\u0026quot;: { \u0026quot;values\u0026quot;: [30, 60], \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;doc['amount'].value * 1.1\u0026quot; } } } } } 存储脚本 #\n 要使用存储脚本，首先使用以下请求创建它：\nPOST _scripts/percentile_script { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;painless\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;doc[params.field].value * params.multiplier\u0026quot; } } 然后用 percentile_ranks 聚合中的存储脚本：\nGET /transaction_data/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;rank_check\u0026quot;: { \u0026quot;percentile_ranks\u0026quot;: { \u0026quot;values\u0026quot;: [30, 60], \u0026quot;script\u0026quot;: { \u0026quot;id\u0026quot;: \u0026quot;percentile_script\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;amount\u0026quot;, \u0026quot;multiplier\u0026quot;: 1.1 } } } } } }   ","subcategory":null,"summary":"","tags":null,"title":"百分位排名聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/percentile-ranks/"},{"category":null,"content":"父级聚合 #  parent 父级聚合是一个分组聚合，根据您索引中定义的父子关系创建一个包含父级文档的分组。此聚合使您能够对具有相同匹配子级文档的父级文档执行分析，从而实现强大的层次结构数据分析。\nparent 聚合与 join 字段类型一起工作，该字段类型在同一个索引中的文档内建立父子关系。\nparent 聚合识别具有匹配子文档的父文档，而 children 聚合识别匹配特定子关系的子文档。这两种聚合都使用子关系名称作为输入。\n参数说明 #  parent 聚合具有以下参数：\n   参数 必需/可选 数据类型 描述     type\t 必填 String join 字段中的子类型名称。    参考样例 #  以下示例构建了一个包含三名员工的小公司数据库。每个员工记录都与一个父部门记录存在 join 子关系。\n首先，创建一个 company 索引，其中包含一个 join 字段，该字段将部门（父级）映射到员工（子级）：\nPUT /company { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;join_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;join\u0026quot;, \u0026quot;relations\u0026quot;: { \u0026quot;department\u0026quot;: \u0026quot;employee\u0026quot; } }, \u0026quot;department_name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;employee_name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;salary\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot; }, \u0026quot;hire_date\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot; } } } } 接下来，用三个部门和三个员工填充数据。父子关系在以下表格中展示。\n   部门（父关系） 员工（子关系）     Accounting Abel Anderson, Betty Billings   Engineering Carl Carter   HR none    routing 参数确保父级和子级文档存储在同一个分片上：\nPOST _bulk?routing=1 { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;department\u0026quot;, \u0026quot;department_name\u0026quot;: \u0026quot;Accounting\u0026quot;, \u0026quot;join_field\u0026quot;: \u0026quot;department\u0026quot; } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;department\u0026quot;, \u0026quot;department_name\u0026quot;: \u0026quot;Engineering\u0026quot;, \u0026quot;join_field\u0026quot;: \u0026quot;department\u0026quot; } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;department\u0026quot;, \u0026quot;department_name\u0026quot;: \u0026quot;HR\u0026quot;, \u0026quot;join_field\u0026quot;: \u0026quot;department\u0026quot; } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;employee_name\u0026quot;: \u0026quot;Abel Anderson\u0026quot;, \u0026quot;salary\u0026quot;: 120000, \u0026quot;hire_date\u0026quot;: \u0026quot;2024-04-04\u0026quot;, \u0026quot;join_field\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;parent\u0026quot;: \u0026quot;1\u0026quot; } } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;5\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;employee_name\u0026quot;: \u0026quot;Betty Billings\u0026quot;, \u0026quot;salary\u0026quot;: 140000, \u0026quot;hire_date\u0026quot;: \u0026quot;2023-05-05\u0026quot;, \u0026quot;join_field\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;parent\u0026quot;: \u0026quot;1\u0026quot; } } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;6\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;employee_name\u0026quot;: \u0026quot;Carl Carter\u0026quot;, \u0026quot;salary\u0026quot;: 140000, \u0026quot;hire_date\u0026quot;: \u0026quot;2020-06-06\u0026quot;, \u0026quot;join_field\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;parent\u0026quot;: \u0026quot;2\u0026quot; } } 最后，运行一个聚合操作，统计与一个或多个员工存在父子关系的所有部门：\nGET /company/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;all_departments\u0026quot;: { \u0026quot;parent\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;employee\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;departments\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;department_name\u0026quot; } } } } } } 返回内容\nall_departments 父聚合返回所有包含员工子文档的部门。请注意，人力资源部门没有体现：\n{ \u0026quot;took\u0026quot;: 3, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 6, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;all_departments\u0026quot;: { \u0026quot;doc_count\u0026quot;: 2, \u0026quot;departments\u0026quot;: { \u0026quot;doc_count_error_upper_bound\u0026quot;: 0, \u0026quot;sum_other_doc_count\u0026quot;: 0, \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: \u0026quot;Accounting\u0026quot;, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key\u0026quot;: \u0026quot;Engineering\u0026quot;, \u0026quot;doc_count\u0026quot;: 1 } ] } } } } ","subcategory":null,"summary":"","tags":null,"title":"父级聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/parent/"},{"category":null,"content":"波特词干分词过滤器 #  波特词干(porter_stem)分词过滤器会将单词还原为其基本（或词干）形式，并去除单词中常见的后缀，这有助于通过单词的词根来匹配相似的单词。例如，单词“running”会被词干提取为“run”。此分词过滤器主要用于英语，并基于 波特词干提取算法进行词干提取操作。\n参考样例 #  以下示例请求创建了一个名为 my_stem_index 的新索引，并配置了一个带有波特词干提取过滤器的分词器。\nPUT /my_stem_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_porter_stem\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;porter_stem\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;porter_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_porter_stem\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_stem_index/_analyze { \u0026quot;text\u0026quot;: \u0026quot;running runners ran\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;porter_analyzer\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;run\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;runner\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;ran\u0026quot;, \u0026quot;start_offset\u0026quot;: 16, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"波特词干分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/porter-stem/"},{"category":null,"content":"求和聚合 #  sum 求和聚合是一种单值指标聚合，计算字段中匹配文档中提取的数值的总和。此聚合常用于计算诸如收入、数量或持续时间等指标的总计。\n参数说明 #  sum 聚合接受以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 聚合的字段。必须是数值字段。   script 可选 Object 用于计算聚合自定义值的脚本。可以替代或与 field 结合使用。   missing 可选 Number 缺少目标字段时使用的默认值。    参考样例 #  以下示例演示了如何计算物流索引中记录的交付总重量。\n创建一个索引：\nPUT /deliveries { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;shipment_id\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;weight_kg\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot; } } } } 添加示例文档：\nPOST /deliveries/_bulk?refresh=true {\u0026quot;index\u0026quot;: {}} {\u0026quot;shipment_id\u0026quot;: \u0026quot;S001\u0026quot;, \u0026quot;weight_kg\u0026quot;: 12.5} {\u0026quot;index\u0026quot;: {}} {\u0026quot;shipment_id\u0026quot;: \u0026quot;S002\u0026quot;, \u0026quot;weight_kg\u0026quot;: 7.8} {\u0026quot;index\u0026quot;: {}} {\u0026quot;shipment_id\u0026quot;: \u0026quot;S003\u0026quot;, \u0026quot;weight_kg\u0026quot;: 15.0} {\u0026quot;index\u0026quot;: {}} {\u0026quot;shipment_id\u0026quot;: \u0026quot;S004\u0026quot;, \u0026quot;weight_kg\u0026quot;: 10.3} 以下请求计算 deliveries 索引中所有文档的总权重，通过将 size 设置为 0 来忽略文档命中，并返回 weight_kg 的总和：\nGET /deliveries/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;total_weight\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;weight_kg\u0026quot; } } } } 返回包含值 45.6 ，对应于 12.5 + 7.8 + 15.0 + 10.3 的总和：\n{ ... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;total_weight\u0026quot;: { \u0026quot;value\u0026quot;: 45.6 } } } 使用脚本来计算值 #\n 你可以提供一个脚本来计算聚合中的值，而不是直接指定一个字段。这在值必须经过推导或调整时非常有用。\n在以下示例中，每个重量在使用脚本求和之前都会从千克转换为克：\nGET /deliveries/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;total_weight_grams\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;doc['weight_kg'].value * 1000\u0026quot; } } } } } 内容中 total_weight_grams 为 45600 ：\n{ ... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;total_weight_grams\u0026quot;: { \u0026quot;value\u0026quot;: 45600 } } } 将字段与值脚本结合使用 #\n 你也可以同时指定 field 和 script ，使用特殊变量 _value 来引用字段的值。这在对现有字段值应用转换时非常有用。\n以下示例在求和之前将所有权重增加 10%：\nGET /deliveries/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;adjusted_weight\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;weight_kg\u0026quot;, \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;Math.round(_value * 110) / 100.0\u0026quot; } } } } } 反映了原始总重量增加了 10%：\n{ ... \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;adjusted_weight\u0026quot;: { \u0026quot;value\u0026quot;: 50.16 } } } 缺省值 #\n 缺少目标字段的文档默认会被忽略。要使用默认值包含它们，请使用 missing 参数。\n以下示例将默认值 0 分配给缺少的 weight_kg 字段。这确保了缺少该字段的文档被视为 weight_kg 设置为 0 并包含在聚合中。\nGET /deliveries/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;total_weight_with_missing\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;weight_kg\u0026quot;, \u0026quot;missing\u0026quot;: 0 } } } } \n","subcategory":null,"summary":"","tags":null,"title":"求和聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/sum/"},{"category":null,"content":"求和分组聚合 #  sum_bucket 聚合是一个同级聚合，用于计算先前聚合中每个分组中指标的总和。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n参数说明 #  sum_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。    参考样例 #  以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， sum_bucket 聚合通过汇总这些总和来计算每个月的总字节数：\nPOST sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } } } }, \u0026quot;sum_monthly_bytes\u0026quot;: { \u0026quot;sum_bucket\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;visits_per_month\u0026gt;sum_of_bytes\u0026quot; } } } } 返回内容 #  该聚合返回所有月度分组中的字节总和：\n{ \u0026quot;took\u0026quot;: 10, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 } } ] }, \u0026quot;sum_monthly_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 79725689 } } } ","subcategory":null,"summary":"","tags":null,"title":"求和分组聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/sum-bucket/"},{"category":null,"content":"标准词元生成器 #  标准（standard）词元生成器是 Easysearch 中的默认分词器。它基于单词边界，采用一种基于语法的方法对文本进行分词，这种方法能够识别字母、数字以及标点等其他字符。它具有高度的通用性，适用于多种语言，它使用了 Unicode 文本分割规则（ UAX#29）来将文本分割成词元。\n参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用标准词元生成器的分词器：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_standard_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;standard\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_standard_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_standard_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch is powerful, fast, and scalable.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;powerful\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 22, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;fast\u0026quot;, \u0026quot;start_offset\u0026quot;: 24, \u0026quot;end_offset\u0026quot;: 28, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;and\u0026quot;, \u0026quot;start_offset\u0026quot;: 30, \u0026quot;end_offset\u0026quot;: 33, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;scalable\u0026quot;, \u0026quot;start_offset\u0026quot;: 34, \u0026quot;end_offset\u0026quot;: 42, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 } ] } 参数说明 #  标准词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     max_token_length 可选 整数 设置生成词元的最大长度。若超过该长度，词元将按照 max_token_length 所配置的长度拆分为多个词元。默认值为 255。    ","subcategory":null,"summary":"","tags":null,"title":"标准词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/standard/"},{"category":null,"content":"标准分词器 #  标准（standard）分词器是在未指定其他分词器时默认使用的分词器。它旨在为通用文本处理提供一种基础且高效的方法。\n该分词器由以下词元生成器和词元过滤器组成：\n 标准（standard）词元生成器：去除大部分标点符号，并依据空格和其他常见分隔符对文本进行分割。 小写（lowercase）词元过滤器：将所有词元转换为小写，以确保匹配时不区分大小写。 停用词（stop）词元过滤器：从分词后的输出中移除常见的停用词，例如 “the”、“is” 和 “and”。  参考样例 #  以下命令创建一个名为 my_standard_index 并使用标准分词器的索引：\nPUT /my_standard_index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot; } } } } 参数说明 #  你可以使用以下参数来配置标准分词器。\n   参数 必填/可选 数据类型 描述     max_token_length 可选 整数 设置生成的词元的最大长度。如果超过此长度，词元将按照 max_token_length 中配置的长度拆分为多个词元。默认值为 255。   stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。   stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。    配置自定义分词器 #  以下命令配置一个索引，该索引带有一个等同于标准分词器的自定义分词器：\nPUT /my_custom_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;stop\u0026quot; ] } } } } } 产生的词元 #  以下请求用来检查分词器生成的词元：\nPOST /my_custom_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;The slow turtle swims away\u0026quot; } 返回内容中包含了产生的词元\n{ \u0026quot;tokens\u0026quot;: [ {\u0026quot;token\u0026quot;: \u0026quot;slow\u0026quot;,\u0026quot;start_offset\u0026quot;: 4,\u0026quot;end_offset\u0026quot;: 8,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 1}, {\u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;,\u0026quot;start_offset\u0026quot;: 9,\u0026quot;end_offset\u0026quot;: 15,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;swims\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 21,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;away\u0026quot;,\u0026quot;start_offset\u0026quot;: 22,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 4} ] } ","subcategory":null,"summary":"","tags":null,"title":"标准分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/standard-analyzer/"},{"category":null,"content":"条件分词过滤器 #  条件(condition)分词过滤器是一种特殊类型的过滤器，它允许你根据特定标准有条件地应用其他分词过滤器。这使得在文本分析过程中，你能更精准地控制何时应用特定的分词过滤器。你可以配置多个过滤器，并且只有当满足你定义的条件时，这些过滤器才会被应用。该分词过滤器在进行特定语言处理和特殊字符处理时非常有用。\n参数说明 #  要使用条件分词过滤器，必须配置两个参数，具体如下：\n   参数 必需/可选 数据类型 描述     filter 必需 数组 当满足由 script 参数定义的指定条件时，指定应对词元应用哪个分词过滤器。   script 必需 对象 配置一个内联脚本，该脚本定义了要应用 filter 参数中指定的过滤器所需满足的条件（仅接受内联脚本）。    参考样例 #  以下示例请求创建了一个名为 my_conditional_index 的新索引，并配置了一个带有条件过滤器的分词器。该过滤器会对任何包含字符序列 “um” 的词元应用小写（lowercase）字母过滤器。\nPUT /my_conditional_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_conditional_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;condition\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;], \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;token.getTerm().toString().contains('um')\u0026quot; } } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;my_conditional_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_conditional_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;THE BLACK CAT JUMPS OVER A LAZY DOG\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;THE\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 3, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;BLACK\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 9, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;CAT\u0026quot;, \u0026quot;start_offset\u0026quot;: 10, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;jumps\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;OVER\u0026quot;, \u0026quot;start_offset\u0026quot;: 20, \u0026quot;end_offset\u0026quot;: 24, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;A\u0026quot;, \u0026quot;start_offset\u0026quot;: 25, \u0026quot;end_offset\u0026quot;: 26, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 }, { \u0026quot;token\u0026quot;: \u0026quot;LAZY\u0026quot;, \u0026quot;start_offset\u0026quot;: 27, \u0026quot;end_offset\u0026quot;: 31, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 6 }, { \u0026quot;token\u0026quot;: \u0026quot;DOG\u0026quot;, \u0026quot;start_offset\u0026quot;: 32, \u0026quot;end_offset\u0026quot;: 35, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 7 } ] } ","subcategory":null,"summary":"","tags":null,"title":"条件分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/condition/"},{"category":null,"content":"最小哈希分词过滤器 #  最小哈希（min_hash）分词过滤器用于基于最小哈希近似算法为词元生成哈希值，这对于检测文档间的相似度很有用。最小哈希分词过滤器会为一组词元（通常来自一个经过分词的字段）生成哈希值。\n参数说明 #  最小哈希分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     hash_count 可选 整数 为每个词元生成的哈希值数量。增加此值通常会提高相似度估计的准确性，但会增加计算成本。默认值为 1。   bucket_count 可选 整数 要使用的哈希桶数量。这会影响哈希的粒度。更多的桶能提供更细的粒度并减少哈希冲突，但需要更多内存。默认值为 512。   hash_set_size 可选 整数 每个桶中要保留的哈希数量。这会影响哈希质量。更大的集合大小可能会带来更好的相似度检测效果，但会消耗更多内存。默认值为 1。   with_rotation 可选 布尔值 当设置为 true 时，如果 hash_set_size 为 1，过滤器会用从其右侧按环形顺序找到的第一个非空桶的值填充空桶。如果 bucket_count 参数大于 1，此设置会自动默认为 true；否则，默认为 false。    参考样例 #  以下示例请求创建了一个名为 minhash_index 的新索引，并配置了一个带有最小哈希过滤器的分词器：\nPUT /minhash_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;minhash_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;min_hash\u0026quot;, \u0026quot;hash_count\u0026quot;: 3, \u0026quot;bucket_count\u0026quot;: 512, \u0026quot;hash_set_size\u0026quot;: 1, \u0026quot;with_rotation\u0026quot;: false } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;minhash_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;minhash_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /minhash_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;minhash_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch is very powerful.\u0026quot; } 返回内容中包含了生成的词元（这些词元没有直观的可读性，因为它们代表的是哈希值）：\n{ \u0026quot;tokens\u0026quot; : [ { \u0026quot;token\u0026quot; : \u0026quot;\\u0000\\u0000㳠锯ੲ걌䐩䉵\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 27, \u0026quot;type\u0026quot; : \u0026quot;MIN_HASH\u0026quot;, \u0026quot;position\u0026quot; : 0 }, { \u0026quot;token\u0026quot; : \u0026quot;\\u0000\\u0000㳠锯ੲ걌䐩䉵\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 27, \u0026quot;type\u0026quot; : \u0026quot;MIN_HASH\u0026quot;, \u0026quot;position\u0026quot; : 0 }, ... ","subcategory":null,"summary":"","tags":null,"title":"最小哈希分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/min-hash/"},{"category":null,"content":"最小值聚合 #  min 最小值指标是一个单值指标，返回字段的最小值。\n min 聚合使用 double （双精度）表示来比较数值字段。对于包含 long 或 unsigned_long 且绝对值大于 2 53 的字段，结果应被视为近似值，因为 double 尾数中的有效位数是 53。\n 参数说明 #  min 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 计算最小值的字段名称。   missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，则包含缺失值的文档将从聚合中排除。    参考样例 #  以下示例请求在样本数据中查找最便宜的商品——即 base_unit_price 值最小的商品：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;min_base_unit_price\u0026quot;: { \u0026quot;min\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.base_unit_price\u0026quot; } } } } 返回内容 #\n 如以下示例返回所示，聚合返回了 products.base_unit_price 的最小值：\n{ \u0026quot;took\u0026quot;: 15, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;min_base_unit_price\u0026quot;: { \u0026quot;value\u0026quot;: 5.98828125 } } } 可以使用聚合名称（ min_base_unit_price ）作为检索聚合内容的键名。\n缺省值 #  可以为聚合字段的缺失实例分配一个值。有关更多信息，请参阅缺失聚合。\nmin 通常会忽略缺失值。如果您使用 missing 分配一个低于任何现有值的值， min 会将此替换值作为最小值返回。\n","subcategory":null,"summary":"","tags":null,"title":"最小值聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/minimum/"},{"category":null,"content":"最小值分组聚合 #  min_bucket 最小值分组聚合是一个同级聚合，用于计算先前聚合中每个分组中某个指标的最小值。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n参数说明 #  min_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。    参考样例 #  以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， min_bucket 聚合找到最小值——这些分组中最小的一个：\nPOST sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } } } }, \u0026quot;min_monthly_bytes\u0026quot;: { \u0026quot;min_bucket\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;visits_per_month\u0026gt;sum_of_bytes\u0026quot; } } } } 返回内容 #  min_bucket 聚合返回跨多个分组的指定指标的最小值。在这个示例中，它计算了 sum_of_bytes 指标在 visits_per_month 中的每月最小字节数。 value 字段显示了在所有分组中找到的最小值。 keys 数组包含观察到该最小值的分组的键。它是一个数组，因为多个分组可以具有相同的最小值。在这种情况下，所有匹配的分组键都会被包含。这确保了即使多个时间段（或分组项）具有相同的最小值，结果也是准确的：\n{ \u0026quot;took\u0026quot;: 7, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 } } ] }, \u0026quot;min_monthly_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103, \u0026quot;keys\u0026quot;: [ \u0026quot;2025-03-01T00:00:00.000Z\u0026quot; ] } } } \n","subcategory":null,"summary":"","tags":null,"title":"最小值分组聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/min-bucket/"},{"category":null,"content":"最大分组聚合 #  max_bucket 最大分组聚合是一个同级聚合，用于计算先前聚合中每个分组中某个指标的最大值。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n参数说明 #  max_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。    参考样例 #  以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， max_bucket 聚合找到最大值——这些分组中最大的那个：\nPOST sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } } } }, \u0026quot;max_monthly_bytes\u0026quot;: { \u0026quot;max_bucket\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;visits_per_month\u0026gt;sum_of_bytes\u0026quot; } } } } 返回内容 #\n max_bucket 聚合返回跨多个分组的指定指标的最大值。在这个示例中，它计算了 sum_of_bytes 指标在 visits_per_month 中的每月最大字节数。 value 字段显示了在所有分组中找到的最大值。 keys 数组包含观察到该最大值的分组的键。它是一个数组，因为多个分组可以具有相同最大值。在这种情况下，所有匹配的分组键都会被包含。这确保了即使多个时间段（或分组项）具有相同最大值，结果也是准确的：\n{ \u0026quot;took\u0026quot;: 6, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 } } ] }, \u0026quot;max_monthly_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067, \u0026quot;keys\u0026quot;: [ \u0026quot;2025-04-01T00:00:00.000Z\u0026quot; ] } } } \n","subcategory":null,"summary":"","tags":null,"title":"最大分组聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/max-bucket/"},{"category":null,"content":"最大值聚合 #  max 最大值指标是一个单值指标，返回字段的最大值。\n max 聚合使用 double （双精度）表示来比较数值字段。对于包含 long 或 unsigned_long 超过 2^53 的整数值的字段，结果应被视为近似值，因为 double 的尾数中的有效位数是 53。\n 参数说明 #  max 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 计算最大值的字段名称。   missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，则包含缺失值的文档将从聚合中排除。    参考样例 #  以下示例请求在数据中查找最昂贵的商品——即 base_unit_price 值最大的商品：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;max_base_unit_price\u0026quot;: { \u0026quot;max\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.base_unit_price\u0026quot; } } } } 返回内容 #  如以下示例返回内容所示，聚合返回 products.base_unit_price 的最大值：\n{ \u0026quot;took\u0026quot;: 24, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;max_base_unit_price\u0026quot;: { \u0026quot;value\u0026quot;: 540 } } } 您可以使用聚合名称（ max_base_unit_price ）作为键从响应中检索聚合。\n缺省值 #  您可以为聚合字段的缺失实例分配一个值。有关更多信息，请参阅缺失聚合。 缺省值通常被 max 忽略。如果您使用 missing 分配一个大于任何现有值的值， max 将返回此替换值作为最大值。\n","subcategory":null,"summary":"","tags":null,"title":"最大值聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/maximum/"},{"category":null,"content":"映射字符过滤器 #  映射（mapping）字符过滤器接受一个用于字符替换的键值对映射。每当该过滤器遇到与某个键匹配的字符串时，它就会用相应的值来替换这些字符。替换值可以是空字符串。\n该过滤器采用贪婪匹配方式，这意味着会匹配最长的匹配结果。\n在分词过程之前，需要进行特定文本替换的场景下，映射字符过滤器会很有帮助。\n参考样例 #  以下请求配置了一个映射字符过滤器，该过滤器可将罗马数字（如 I、II 或 III）转换为对应的阿拉伯数字（1、2 和 3）：\nGET /_analyze { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;char_filter\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;mapping\u0026quot;, \u0026quot;mappings\u0026quot;: [ \u0026quot;I =\u0026gt; 1\u0026quot;, \u0026quot;II =\u0026gt; 2\u0026quot;, \u0026quot;III =\u0026gt; 3\u0026quot;, \u0026quot;IV =\u0026gt; 4\u0026quot;, \u0026quot;V =\u0026gt; 5\u0026quot; ] } ], \u0026quot;text\u0026quot;: \u0026quot;I have III apples and IV oranges\u0026quot; } 返回内容中包含一个词元，其中罗马数字已被替换为阿拉伯数字：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;1 have 3 apples and 4 oranges\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 32, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } 参数说明 #  你可以使用以下任意一个参数来配置键值映射。\n   参数 必需/可选 数据类型 描述     mappings 可选 数组 格式为 key =\u0026gt; value 的键值对数组。在输入文本中找到的每个键都将被其对应的值替换。   mappings_path 可选 字符串 包含键值映射的 UTF-8 编码文件的路径。每个映射应在新的一行中以 key =\u0026gt; value 的格式呈现。该路径可以是绝对路径，也可以是相对于 Easysearch 配置目录的相对路径。    使用自定义映射字符过滤器 #  你可以通过定义自己的映射集来创建自定义映射字符过滤器。以下请求将创建一个自定义字符过滤器，用于替换文本中的常见缩写：\nPUT /test-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_abbr_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;char_filter\u0026quot;: [ \u0026quot;custom_abbr_filter\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;custom_abbr_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mapping\u0026quot;, \u0026quot;mappings\u0026quot;: [ \u0026quot;BTW =\u0026gt; By the way\u0026quot;, \u0026quot;IDK =\u0026gt; I don't know\u0026quot;, \u0026quot;FYI =\u0026gt; For your information\u0026quot; ] } } } } } 使用以下请求来检查使用该分词器生成的词元：\nGET /test-index/_analyze { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;char_filter\u0026quot;: [ \u0026quot;custom_abbr_filter\u0026quot; ], \u0026quot;text\u0026quot;: \u0026quot;FYI, updates to the workout schedule are posted. IDK when it takes effect, but we have some details. BTW, the finalized schedule will be released Monday.\u0026quot; } 返回内容显示这些缩写已被替换：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;For your information, updates to the workout schedule are posted. I don't know when it takes effect, but we have some details. By the way, the finalized schedule will be released Monday.\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 153, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"映射字符过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/character-filters/mapping/"},{"category":null,"content":"日期范围聚合 #  date_range 日期范围聚合在概念上与 range 聚合相同，只是它允许执行日期计算。例如，你可以获取过去 10 天内的所有文档。为了使日期更易读，可以使用 format 参数包含格式：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;number_of_bytes\u0026quot;: { \u0026quot;date_range\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;MM-yyyy\u0026quot;, \u0026quot;ranges\u0026quot;: [ { \u0026quot;from\u0026quot;: \u0026quot;now-10d/d\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;now\u0026quot; } ] } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;number_of_bytes\u0026quot; : { \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;03-2021-03-2021\u0026quot;, \u0026quot;from\u0026quot; : 1.6145568E12, \u0026quot;from_as_string\u0026quot; : \u0026quot;03-2021\u0026quot;, \u0026quot;to\u0026quot; : 1.615451329043E12, \u0026quot;to_as_string\u0026quot; : \u0026quot;03-2021\u0026quot;, \u0026quot;doc_count\u0026quot; : 0 } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"日期范围聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/date-range/"},{"category":null,"content":"日期直方图聚合 #  date_histogram 日期直方图聚合使用日期计算来为时间序列数据生成直方图。\n例如，您可以找到您的网站每月有多少次访问：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;logs_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;interval\u0026quot;: \u0026quot;month\u0026quot; } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;logs_per_month\u0026quot; : { \u0026quot;buckets\u0026quot; : [ { \u0026quot;key_as_string\u0026quot; : \u0026quot;2020-10-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot; : 1601510400000, \u0026quot;doc_count\u0026quot; : 1635 }, { \u0026quot;key_as_string\u0026quot; : \u0026quot;2020-11-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot; : 1604188800000, \u0026quot;doc_count\u0026quot; : 6844 }, { \u0026quot;key_as_string\u0026quot; : \u0026quot;2020-12-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot; : 1606780800000, \u0026quot;doc_count\u0026quot; : 5595 } ] } } 返回内容包含三个月的日志。如果你绘制这些值，你可以看到你的网站每月请求流量的峰值和低谷。\n参数说明 #  date_histogram 聚合支持以下参数。\n   参数 必需/可选 数据类型 描述     date_histogram\t 必填 Object 一个指定日期时间文档字段、间隔、可选格式和时区的对象。   calendar_interval\t 必填 时间间隔 构建每个分组所使用的日期字段。   format\t 可选 String 日期格式字符串。如果省略，日期将输出为 64 位自纪元以来的毫秒整数。   time_zone\t 可选 String 表示 UTC 时间偏移的字符串，可以是 ISO 8601 UTC 偏移（\u0026quot;-07:00\u0026quot;）或 tz 数据库标识符（\u0026ldquo;America/Los_Angeles\u0026rdquo;）。    ","subcategory":null,"summary":"","tags":null,"title":"日期直方图聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/date-histogram/"},{"category":null,"content":"数据脱敏 #  如果您的数据里面包含一些敏感信息，除了通过 字段级权限 来进行访问控制，您还可以通过混淆字段里面的内容来进行脱敏。目前，字段数据脱敏仅适用于基于字符串的字段，支持加密哈希和正则替换字段的内容。\n字段脱敏与字段级权限一起可以在相同的角色级别和索引级别上工作。您可以允许某些角色查看明文格式的敏感字段，并为其他角色脱敏这些字段。带有脱敏字段的搜索结果可能如下所示：\n{ \u0026#34;_index\u0026#34;: \u0026#34;movies\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;year\u0026#34;: 2013, \u0026#34;directors\u0026#34;: [\u0026#34;Ron Howard\u0026#34;], \u0026#34;title\u0026#34;: \u0026#34;ca998e768dd2e6cdd84c77015feb29975f9f498a472743f159bec6f1f1db109e\u0026#34; } } 设置盐值 #  可以在 easysearch.yml 设置一个随机字符串:\nsecurity.compliance.salt: abcdefghijklmnopqrstuvqxyz1234567890    属性 说明     security.compliance.salt 生成哈希值时要使用的盐值。必须至少为 32 个字符。仅允许使用 ASCII 字符。选填。    配置脱敏字段 #  role.yml #  masked_movie: cluster: [] indices: - names: - movies privileges: - \u0026#34;read\u0026#34; field_mask: - \u0026#34;genres\u0026#34; - \u0026#34;title\u0026#34; REST API #  参照 创建角色.\n使用其它哈希算法 #  默认情况下，安全模块使用 BLAKE2b 算法，但您可以使用 JVM 提供的任何哈希算法。此列表通常包括 MD5、SHA-1、SHA-384 和 SHA-512。\n要指定其它算法，请将其添加到脱敏字段之后：\nmasked_movie: cluster: [] indices: - names: - movies privileges: - \u0026#34;read\u0026#34; field_mask: - \u0026#34;genres::SHA-512\u0026#34; - \u0026#34;title\u0026#34; 基于规则的字段脱敏 #  除了使用哈希，您还可以使用一个或多个正则表达式来替换字符串从而达到字段脱敏的效果。语法是 \u0026lt;field\u0026gt;::/\u0026lt;regular-expression\u0026gt;/::\u0026lt;replacement-string\u0026gt; 。如果使用多个正则表达式，则结果将从左向右传递，就像 shell 中的管道操作一样：\nmasked_movie: cluster: [] indices: - names: - movies privileges: - \u0026#34;read\u0026#34; field_mask: - \u0026#34;genres::/^[a-zA-Z]{1,3}/::XXX::/[a-zA-Z]{1,3}$/::YYY\u0026#34; - \u0026#34;title::/./::*\u0026#34; title 字段里面的表达式将字段中的每个字符更改为 *，因此您仍然可以辨别脱敏后字符串的长度。genres 字段的表达式将字符串的前三个字符更改为 XXX，将最后三个字符更改为 YYY。\n完整测试脚本如下：\nPOST movies/_doc/1 { \u0026#34;year\u0026#34;: 2013, \u0026#34;title\u0026#34;: \u0026#34;Rush\u0026#34;, \u0026#34;actors\u0026#34;: [ \u0026#34;Daniel Brühl\u0026#34;, \u0026#34;Chris Hemsworth\u0026#34;, \u0026#34;Olivia Wilde\u0026#34; ] } POST movies/_doc/2 { \u0026quot;directors\u0026quot;: [ \u0026quot;Ron Howard\u0026quot; ], \u0026quot;plot\u0026quot;: \u0026quot;A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\u0026quot;, \u0026quot;genres\u0026quot;: [ \u0026quot;Action\u0026quot;, \u0026quot;Biography\u0026quot;, \u0026quot;Drama\u0026quot;, \u0026quot;Sport\u0026quot; ] }\nPUT _security/user/medcl { \u0026quot;password\u0026quot;: \u0026quot;pass\u0026quot;, \u0026quot;roles\u0026quot;: [\u0026quot;masked_movie\u0026quot;] }\ncurl -XGET -k 'https://localhost:9200/movies/_search?pretty' -u medcl:pass\n{ \u0026quot;took\u0026quot; : 27, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 2, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.0, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;movies\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;actors\u0026quot; : [ \u0026quot;Daniel Brühl\u0026quot;, \u0026quot;Chris Hemsworth\u0026quot;, \u0026quot;Olivia Wilde\u0026quot; ], \u0026quot;year\u0026quot; : 2013, \u0026quot;title\u0026quot; : \u0026quot;****\u0026quot; } }, { \u0026quot;_index\u0026quot; : \u0026quot;movies\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;plot\u0026quot; : \u0026quot;A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\u0026quot;, \u0026quot;genres\u0026quot; : [ \u0026quot;XXXYYY\u0026quot;, \u0026quot;XXXgraYYY\u0026quot;, \u0026quot;XXYYY\u0026quot;, \u0026quot;XXYYY\u0026quot; ], \u0026quot;directors\u0026quot; : [ \u0026quot;Ron Howard\u0026quot; ] } } ] } } 对审计日志的影响 #\n 读取历史记录可让您跟踪对文档中敏感字段的读取操作。例如，您可以跟踪对客户记录的电子邮件字段的访问。对脱敏字段的访问从读取历史记录中排除了，因为用户只能看到哈希值，而不是字段的明文值。\n","subcategory":null,"summary":"","tags":null,"title":"数据脱敏","url":"/easysearch/v1.15.0/docs/references/security/access-control/field-masking/"},{"category":null,"content":"搜索分词器 #  搜索分词器是在查询时指定的，当你对text字段进行全文匹配查询时，它会被用于分析被查询字符串。\n搜索分词器的生效流程 #  在查询时确定对查询字符串使用哪种分词器，Easysearch 会按以下顺序检查以下参数：\n 查询的 analyzer 参数 字段的 search_analyzer 映射参数 索引设置 analysis.analyzer.default_search 字段的 analyzer 映射参数 standard 分词器（默认分词器）   在大多数情况下，没有必要指定与索引分词器不同的搜索分词器，而且这样做可能会对搜索结果的相关性产生负面影响，或者导致出现意想不到的搜索结果。\n 为查询内容指定搜索分词器 #  在查询时，你可以在 analyzer 字段中指定想要使用的分词器：\nGET shakespeare/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;text_entry\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;speak the truth\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;english\u0026quot; } } } } 为字段指定搜索分词器 #  在创建索引映射时，你可以为每个文本字段提供 search_analyzer 参数。在提供 search_analyzer 时，你还必须提供 analyzer 参数，该参数指定在索引创建时的写入索引分词器内应用。\n例如，以下请求将 simple 分词器指定为 text_entry 字段的索引分词器，将 whitespace 分词器指定为该字段的搜索分词器：\nPUT testindex { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;text_entry\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;simple\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;whitespace\u0026quot; } } } } 为索引指定默认的搜索分词器 #  如果你希望在搜索时使用同一个分词器来解析所有的查询，你可以在 analysis.analyzer.default_search 设置中指定该搜索分词器。在提供 analysis.analyzer.default_search 时，你还必须提供 analysis.analyzer.default 参数，该参数指定了在创建索引时要使用的索引分词器。\n例如，以下请求将 simple 分词器指定为 testindex 索引的索引分词器，并将 whitespace 分词器指定为该索引的搜索分词器\nPUT testindex { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;simple\u0026quot; }, \u0026quot;default_search\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;whitespace\u0026quot; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"搜索分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/search-analyzers/"},{"category":null,"content":"捕获匹配分词过滤器 #  捕获匹配(pattern_capture)分词过滤器是一种功能强大的过滤器，它使用正则表达式根据特定模式来捕获和提取文本的部分内容。当你想要提取词元的特定部分，例如电子邮件域名、话题标签或数字，并将其重新用于进一步的分析或索引编制时，这个过滤器会非常有用。\n参数说明 #  捕获匹配分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     patterns 必需 字符串数组 用于捕获文本部分的正则表达式数组。   preserve_original 必需 布尔值 是否在输出中保留原始词元。默认值为 true。    参考样例 #  以下示例请求创建了一个名为 email_index 的新索引，并配置了一个带有捕获匹配过滤器的分词器，以便从电子邮件地址中提取本地部分和域名。\nPUT /email_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;email_pattern_capture\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern_capture\u0026quot;, \u0026quot;preserve_original\u0026quot;: true, \u0026quot;patterns\u0026quot;: [ \u0026quot;^([^@]+)\u0026quot;, \u0026quot;@(.+)$\u0026quot; ] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;email_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;uax_url_email\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;email_pattern_capture\u0026quot;, \u0026quot;lowercase\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /email_index/_analyze { \u0026quot;text\u0026quot;: \u0026quot;john.doe@example.com\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;email_analyzer\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;john.doe@example.com\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 20, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;EMAIL\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;john.doe\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 20, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;EMAIL\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;example.com\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 20, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;EMAIL\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"捕获匹配分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/pattern-capture/"},{"category":null,"content":"指纹分词过滤器 #  指纹（fingerprint）分词过滤器用于对文本进行标准化和去重处理。当文本处理的唯一性或者一致性至关重要时，这个过滤器特别有用。指纹分词过滤器通过以下步骤处理文本以实现这一目的：\n 小写转换：将所有文本转换为小写。 分词：把文本拆分成词元。 排序：按字母顺序排列这些词元。 去重：消除重复的词元。 合并词元：将这些词元合并成一个字符串，通常使用空格或其他指定的分隔符进行连接。  参数说明 #  指纹分词过滤器可以使用以下两个参数进行配置。\n   参数 必需/可选 数据类型 描述     max_output_size 可选 整数 限制生成的指纹字符串的长度。如果拼接后的字符串超过了 max_output_size，过滤器将不会产生任何输出，从而得到一个空词元。默认值是 255   separator 可选 字符串 定义在词元排序和去重后，用于将词元连接成单个字符串。默认是空格（\u0026quot; \u0026quot;）。    参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有指纹分词过滤器的分词器：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_fingerprint\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fingerprint\u0026quot;, \u0026quot;max_output_size\u0026quot;: 200, \u0026quot;separator\u0026quot;: \u0026quot;-\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_fingerprint\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch is a powerful search engine that scales easily\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;a-easily-engine-is-easysearch-powerful-scales-search-that\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 57, \u0026quot;type\u0026quot;: \u0026quot;fingerprint\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"指纹分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/fingerprint/"},{"category":null,"content":"指纹分词器 #  指纹（fingerprint）分词器会创建一个文本指纹。该分词器对从输入生成的词项（词元）进行排序和去重，然后使用一个分隔符将它们连接起来。它通常用于数据去重，对于包含相同单词的相似输入，无论单词顺序如何，它都可以产生相同的输出结果。\n指纹分词器由以下组件组成：\n 标准分词生成器 词元小写化过滤器 ASCII 词元过滤器 停用词词元过滤器 指纹词元过滤器  参数说明 #  指纹分词器可以使用以下参数进行配置。\n   参数 必填/可选 数据类型 描述     separator 可选 字符串 在对词项进行词元生成、排序和去重后，用于连接这些词项的字符。默认值为一个空格（ ）。   max_output_size 可选 整数 定义输出词元的最大大小。如果连接后的指纹超过此大小，将被截断。默认值为 255。   stopwords 可选 字符串或字符串列表 自定义的停用词列表。默认值为 _none_。   stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。    参考样例 #  以下命令创建一个名为 my_custom_fingerprint_index 带有指纹分词器的索引：\nPUT /my_custom_fingerprint_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_fingerprint_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fingerprint\u0026quot;, \u0026quot;separator\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;max_output_size\u0026quot;: 50, \u0026quot;stopwords\u0026quot;: [\u0026quot;to\u0026quot;, \u0026quot;the\u0026quot;, \u0026quot;over\u0026quot;, \u0026quot;and\u0026quot;] } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_fingerprint_analyzer\u0026quot; } } } } 产生的词元 #  以下请求用来检查使用该分词器生成的词元：\nPOST /my_custom_fingerprint_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_fingerprint_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;The slow turtle swims over to the dog\u0026quot; } 返回内容中包含了生成的词元：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;dog-slow-swims-turtle\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 37, \u0026quot;type\u0026quot;: \u0026quot;fingerprint\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } 深度定制 #  如果需要深度定制，你可以定义一个包含指纹分词器组件的分词器：\nPUT /custom_fingerprint_analyzer { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_fingerprint\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;asciifolding\u0026quot;, \u0026quot;fingerprint\u0026quot; ] } } } } } ","subcategory":null,"summary":"","tags":null,"title":"指纹分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/fingerprint-analyzer/"},{"category":null,"content":"拼音分词器 #  总体介绍 #  pinyin-analyzer 拼音分词器能够在索引阶段将 中文字符实时转写为拼音，并在检索阶段对拼音、汉字及混输查询进行统一匹配。借助该插件，你可以轻松实现：\n 支持 全拼、首字母、全拼拼接 等多种检索方式； 保留非中文字符，实现「中英混输」搜索； 借助 token filter 在分词链中灵活组合不同策略； 在联想输入、排序、聚合等场景下提升中文用户体验。  适用于人名、地名、品牌、歌曲、商品等多种中文文本检索场景。\n参数说明 #  下表整理了 pinyin 分词器 / 过滤器支持的全部可选参数及默认值。\n   参数 说明 默认值     keep_first_letter 仅保留每个汉字的拼音首字母。例如 刘德华 → ldh true   keep_separate_first_letter 将首字母分开存储，提高命中率。例如 刘德华 → l, d, h，注意：这可能会由于词频增加而增加查询模糊性。 false   limit_first_letter_length 首字母结果最长长度 16   keep_full_pinyin 保留每个汉字的全拼。如 刘德华 → liu, de, hua true   keep_joined_full_pinyin 将全拼连接在一起。如 刘德华 → liudehua false   keep_none_chinese 保留非中文字符（数字/字母等） true   keep_none_chinese_together 将连续非中文字符作为整体保留。例如， DJ 音乐家 变成 DJ 、 yin 、 yue 、 jia 。当设置为 false 时， DJ 音乐家 变成 D 、 J 、 yin 、 yue 、 jia 。注意： keep_none_chinese 应该先启用。 true   keep_none_chinese_in_first_letter 在首字母结果中保留非中文字。例如， 刘德华 AT2016 变成 ldhat2016 。符 true   keep_none_chinese_in_joined_full_pinyin 在拼接全部拼音时保留非中文字。例如， 刘德华 2016 变成 liudehua2016 。符 false   none_chinese_pinyin_tokenize 将非中文字符中可能的拼音继续拆分例如， liudehuaalibaba13zhuanghan 变成 liu 、 de 、 hua 、 a 、 li 、 ba 、 ba 、 13 、 zhuang 、 han 。注意： keep_none_chinese 和 keep_none_chinese_together 应该先启用。 true   keep_original 同时保留原始文本 false   lowercase 对非中文字符强制小写 true   trim_whitespace 去除首尾空格 true   remove_duplicated_term 开启时，移除重复术语以节省索引空间。例如， de 的 变为 de 。注意：位置相关的查询可能会受影响。积 false   ignore_pinyin_offset 忽略 offset 以允许重叠 token。在 6.0 版本之后，偏移量受到严格限制，不允许重叠的标记。使用此参数，通过忽略偏移量将允许重叠的标记。请注意，所有与位置相关的查询或高亮将变得不正确。您应该使用多字段，并为不同的查询目的指定不同的设置。如果您需要偏移量，请将其设置为 false。 true     备注：以上参数可按需自由组合。若使用 token filter，请将其放置于自定义分词链中。\n  常见用例 #  创建索引并配置自定义 Pinyin Analyzer #  PUT /medcl { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_pinyin\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pinyin\u0026#34;, \u0026#34;keep_separate_first_letter\u0026#34;: false, \u0026#34;keep_full_pinyin\u0026#34;: true, \u0026#34;keep_original\u0026#34;: true, \u0026#34;limit_first_letter_length\u0026#34;: 16, \u0026#34;lowercase\u0026#34;: true, \u0026#34;remove_duplicated_term\u0026#34;: true } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;pinyin_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;my_pinyin\u0026#34; } } } } } 测试分词效果\nGET /medcl/_analyze { \u0026#34;text\u0026#34;: [\u0026#34;刘德华\u0026#34;], \u0026#34;analyzer\u0026#34;: \u0026#34;pinyin_analyzer\u0026#34; } 预期返回：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;liu\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;de\u0026#34;, \u0026#34;start_offset\u0026#34;: 1, \u0026#34;end_offset\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;hua\u0026#34;, \u0026#34;start_offset\u0026#34;: 2, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 2 }, { \u0026#34;token\u0026#34;: \u0026#34;刘德华\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 3 }, { \u0026#34;token\u0026#34;: \u0026#34;ldh\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 4 } ] } 创建字段映射并建立文档 #  直接使用拼音分词器\nPOST /medcl/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;pinyin\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;store\u0026#34;: false, \u0026#34;term_vector\u0026#34;: \u0026#34;with_offsets\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;pinyin_analyzer\u0026#34;, \u0026#34;boost\u0026#34;: 10 } } } } }\n## 索引一个文档\nPOST /medcl/_create/andy {\u0026quot;name\u0026quot;:\u0026quot;刘德华\u0026quot;}\n## 查询测试\ncurl http://localhost:9200/medcl/_search?q=name:%E5%88%98%E5%BE%B7%E5%8D%8E curl http://localhost:9200/medcl/_search?q=name.pinyin:%e5%88%98%e5%be%b7 curl http://localhost:9200/medcl/_search?q=name.pinyin:liu curl http://localhost:9200/medcl/_search?q=name.pinyin:ldh curl http://localhost:9200/medcl/_search?q=name.pinyin:de+hua\n使用 Pinyin-TokenFilter #\n PUT /medcl1/ { \u0026quot;settings\u0026quot; : { \u0026quot;analysis\u0026quot; : { \u0026quot;analyzer\u0026quot; : { \u0026quot;user_name_analyzer\u0026quot; : { \u0026quot;tokenizer\u0026quot; : \u0026quot;whitespace\u0026quot;, \u0026quot;filter\u0026quot; : \u0026quot;pinyin_first_letter_and_full_pinyin_filter\u0026quot; } }, \u0026quot;filter\u0026quot; : { \u0026quot;pinyin_first_letter_and_full_pinyin_filter\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;pinyin\u0026quot;, \u0026quot;keep_first_letter\u0026quot; : true, \u0026quot;keep_full_pinyin\u0026quot; : false, \u0026quot;keep_none_chinese\u0026quot; : true, \u0026quot;keep_original\u0026quot; : false, \u0026quot;limit_first_letter_length\u0026quot; : 16, \u0026quot;lowercase\u0026quot; : true, \u0026quot;trim_whitespace\u0026quot; : true, \u0026quot;keep_none_chinese_in_first_letter\u0026quot; : true } } } } } Token 测试:刘德华 张学友 郭富城 黎明 四大天王\nGET /medcl1/_analyze { \u0026quot;text\u0026quot;: [\u0026quot;刘德华 张学友 郭富城 黎明 四大天王\u0026quot;], \u0026quot;analyzer\u0026quot;: \u0026quot;user_name_analyzer\u0026quot; } { \u0026quot;tokens\u0026quot; : [ { \u0026quot;token\u0026quot; : \u0026quot;ldh\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 3, \u0026quot;type\u0026quot; : \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot; : 0 }, { \u0026quot;token\u0026quot; : \u0026quot;zxy\u0026quot;, \u0026quot;start_offset\u0026quot; : 4, \u0026quot;end_offset\u0026quot; : 7, \u0026quot;type\u0026quot; : \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot; : 1 }, { \u0026quot;token\u0026quot; : \u0026quot;gfc\u0026quot;, \u0026quot;start_offset\u0026quot; : 8, \u0026quot;end_offset\u0026quot; : 11, \u0026quot;type\u0026quot; : \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot; : 2 }, { \u0026quot;token\u0026quot; : \u0026quot;lm\u0026quot;, \u0026quot;start_offset\u0026quot; : 12, \u0026quot;end_offset\u0026quot; : 14, \u0026quot;type\u0026quot; : \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot; : 3 }, { \u0026quot;token\u0026quot; : \u0026quot;sdtw\u0026quot;, \u0026quot;start_offset\u0026quot; : 15, \u0026quot;end_offset\u0026quot; : 19, \u0026quot;type\u0026quot; : \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot; : 4 } ] } 短语查询 #  场景 1:全拼音查询\nPUT /medcl2/ { \u0026quot;settings\u0026quot; : { \u0026quot;analysis\u0026quot; : { \u0026quot;analyzer\u0026quot; : { \u0026quot;pinyin_analyzer\u0026quot; : { \u0026quot;tokenizer\u0026quot; : \u0026quot;my_pinyin\u0026quot; } }, \u0026quot;tokenizer\u0026quot; : { \u0026quot;my_pinyin\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;pinyin\u0026quot;, \u0026quot;keep_first_letter\u0026quot;:false, \u0026quot;keep_separate_first_letter\u0026quot; : false, \u0026quot;keep_full_pinyin\u0026quot; : true, \u0026quot;keep_original\u0026quot; : false, \u0026quot;limit_first_letter_length\u0026quot; : 16, \u0026quot;lowercase\u0026quot; : true } } } } } POST /medcl2/_mapping { \u0026quot;properties\u0026quot;: { \u0026quot;name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;fields\u0026quot;: { \u0026quot;pinyin\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;store\u0026quot;: false, \u0026quot;term_vector\u0026quot;: \u0026quot;with_offsets\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;pinyin_analyzer\u0026quot;, \u0026quot;boost\u0026quot;: 10 } } } } }\nPOST /medcl2/_doc?refresh=true {\u0026quot;name\u0026quot;:\u0026quot;liudehua\u0026quot;}\nGET /medcl2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;刘德华\u0026quot; } } } 场景 2:全拼/缩写/混写等多场景匹配\nPUT /medcl3/ { \u0026quot;settings\u0026quot; : { \u0026quot;analysis\u0026quot; : { \u0026quot;analyzer\u0026quot; : { \u0026quot;pinyin_analyzer\u0026quot; : { \u0026quot;tokenizer\u0026quot; : \u0026quot;my_pinyin\u0026quot; } }, \u0026quot;tokenizer\u0026quot; : { \u0026quot;my_pinyin\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;pinyin\u0026quot;, \u0026quot;keep_first_letter\u0026quot;:true, \u0026quot;keep_separate_first_letter\u0026quot; : true, \u0026quot;keep_full_pinyin\u0026quot; : true, \u0026quot;keep_original\u0026quot; : false, \u0026quot;limit_first_letter_length\u0026quot; : 16, \u0026quot;lowercase\u0026quot; : true } } } } } POST /medcl3/_mapping { \u0026quot;properties\u0026quot;: { \u0026quot;name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;fields\u0026quot;: { \u0026quot;pinyin\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;store\u0026quot;: false, \u0026quot;term_vector\u0026quot;: \u0026quot;with_offsets\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;pinyin_analyzer\u0026quot;, \u0026quot;boost\u0026quot;: 10 } } } } } GET /medcl3/_analyze { \u0026quot;text\u0026quot;: [\u0026quot;刘德华\u0026quot;], \u0026quot;analyzer\u0026quot;: \u0026quot;pinyin_analyzer\u0026quot; } POST /medcl3/_create/andy {\u0026quot;name\u0026quot;:\u0026quot;刘德华\u0026quot;} GET /medcl3/_search { \u0026quot;query\u0026quot;: {\u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;刘德h\u0026quot; }} } GET /medcl3/_search { \u0026quot;query\u0026quot;: {\u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;刘dh\u0026quot; }} } GET /medcl3/_search { \u0026quot;query\u0026quot;: {\u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;liudh\u0026quot; }} } GET /medcl3/_search { \u0026quot;query\u0026quot;: {\u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;liudeh\u0026quot; }} } GET /medcl3/_search { \u0026quot;query\u0026quot;: {\u0026quot;match_phrase\u0026quot;: { \u0026quot;name.pinyin\u0026quot;: \u0026quot;liude华\u0026quot; }} }   ","subcategory":null,"summary":"","tags":null,"title":"拼音分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/pinyin-analyzer/"},{"category":null,"content":"扩展统计聚合 #  extended_stats 扩展统计聚合是统计数据 stats 聚合的更全面版本。除了统计数据提供的基本统计指标外，extended_stats 还计算以下内容：\n 平方和 方差 总体方差 抽样方差 标准差 总体标准差 抽样标准差 标准差界限： ** 上限 ** 下限 ** 总体上限 ** 总体下限 ** 抽样上限 ** 抽样下限  其中标准差和方差是总体统计量；它们分别始终等于总体标准差和方差。\nstd_deviation_bounds 对象定义了一个范围，该范围在均值（默认为两个标准差）的上方和下方跨越指定的标准差数量。此对象始终包含在输出中，但仅对正态分布的数据才有意义。在解释这些值之前，请验证您的数据集是否遵循正态分布。\n参数说明 #  extended_stats 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 返回扩展统计信息的字段名称。   sigma 可选 Double（非负） 计算 std_deviation_bounds 区间所使用的均值上下标准差的数量。默认值为 2 。   missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，包含缺失值的文档将不会出现在扩展统计中。    参考样例 #  以下示例请求数据中 taxful_total_price 的扩展统计信息：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;extended_stats_taxful_total_price\u0026quot;: { \u0026quot;extended_stats\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot; } } } } 返回内容 #  该内容包含 taxful_total_price 的扩展统计信息：\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;extended_stats_taxful_total_price\u0026quot; : { \u0026quot;count\u0026quot; : 4675, \u0026quot;min\u0026quot; : 6.98828125, \u0026quot;max\u0026quot; : 2250.0, \u0026quot;avg\u0026quot; : 75.05542864304813, \u0026quot;sum\u0026quot; : 350884.12890625, \u0026quot;sum_of_squares\u0026quot; : 3.9367749294174194E7, \u0026quot;variance\u0026quot; : 2787.59157113862, \u0026quot;variance_population\u0026quot; : 2787.59157113862, \u0026quot;variance_sampling\u0026quot; : 2788.187974983536, \u0026quot;std_deviation\u0026quot; : 52.79764740155209, \u0026quot;std_deviation_population\u0026quot; : 52.79764740155209, \u0026quot;std_deviation_sampling\u0026quot; : 52.80329511482722, \u0026quot;std_deviation_bounds\u0026quot; : { \u0026quot;upper\u0026quot; : 180.6507234461523, \u0026quot;lower\u0026quot; : -30.53986616005605, \u0026quot;upper_population\u0026quot; : 180.6507234461523, \u0026quot;lower_population\u0026quot; : -30.53986616005605, \u0026quot;upper_sampling\u0026quot; : 180.66201887270256, \u0026quot;lower_sampling\u0026quot; : -30.551161586606312 } } } } 定义计算边界 #  可以通过将 sigma 参数设置为任何非负值来定义用于计算 std_deviation_bounds 区间的标准偏差数。\n示例：定义计算边界 #  设置标准偏差的数量为 std_deviation_bounds 到 3 :\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;extended_stats_taxful_total_price\u0026quot;: { \u0026quot;extended_stats\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot;, \u0026quot;sigma\u0026quot;: 3 } } } } 这会改变标准差界限：\n{ ... \u0026quot;aggregations\u0026quot;: { ... \u0026quot;std_deviation_bounds\u0026quot;: { \u0026quot;upper\u0026quot;: 233.44837084770438, \u0026quot;lower\u0026quot;: -83.33751356160813, \u0026quot;upper_population\u0026quot;: 233.44837084770438, \u0026quot;lower_population\u0026quot;: -83.33751356160813, \u0026quot;upper_sampling\u0026quot;: 233.46531398752978, \u0026quot;lower_sampling\u0026quot;: -83.35445670143353 } } } } 缺省值处理 #  可以为聚合字段的缺失实例分配一个值。有关更多信息，请参阅缺失聚合。 准备一个示例索引，通过导入以下文档：\nPOST _bulk { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;students\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;John Doe\u0026quot;, \u0026quot;gpa\u0026quot;: 3.89, \u0026quot;grad_year\u0026quot;: 2022} { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;students\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;Jonathan Powers\u0026quot;, \u0026quot;grad_year\u0026quot;: 2025 } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;students\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;Jane Doe\u0026quot;, \u0026quot;gpa\u0026quot;: 3.52, \u0026quot;grad_year\u0026quot;: 2024 } 示例：替换缺省值 #  计算 extended_stats ，将缺失的 GPA 字段替换为 0 ：\nGET students/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;extended_stats_gpa\u0026quot;: { \u0026quot;extended_stats\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;gpa\u0026quot;, \u0026quot;missing\u0026quot;: 0 } } } } 在返回内容中， gpa 的所有缺失值被替换为 0 ：\n... \u0026quot;aggregations\u0026quot;: { \u0026quot;extended_stats_gpa\u0026quot;: { \u0026quot;count\u0026quot;: 3, \u0026quot;min\u0026quot;: 0, \u0026quot;max\u0026quot;: 3.890000104904175, \u0026quot;avg\u0026quot;: 2.4700000286102295, \u0026quot;sum\u0026quot;: 7.4100000858306885, \u0026quot;sum_of_squares\u0026quot;: 27.522500681877148, \u0026quot;variance\u0026quot;: 3.0732667526245145, \u0026quot;variance_population\u0026quot;: 3.0732667526245145, \u0026quot;variance_sampling\u0026quot;: 4.609900128936772, \u0026quot;std_deviation\u0026quot;: 1.7530735160353415, \u0026quot;std_deviation_population\u0026quot;: 1.7530735160353415, \u0026quot;std_deviation_sampling\u0026quot;: 2.147067797936705, \u0026quot;std_deviation_bounds\u0026quot;: { \u0026quot;upper\u0026quot;: 5.976147060680912, \u0026quot;lower\u0026quot;: -1.0361470034604534, \u0026quot;upper_population\u0026quot;: 5.976147060680912, \u0026quot;lower_population\u0026quot;: -1.0361470034604534, \u0026quot;upper_sampling\u0026quot;: 6.7641356244836395, \u0026quot;lower_sampling\u0026quot;: -1.8241355672631805 } } } } 示例：忽略缺省值 #\n 计算 extended_stats 但不分配 missing 参数：\nGET students/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;extended_stats_gpa\u0026quot;: { \u0026quot;extended_stats\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;gpa\u0026quot; } } } } 计算扩展统计信息，忽略包含缺失字段值的文档（默认行为）：\n... \u0026quot;aggregations\u0026quot;: { \u0026quot;extended_stats_gpa\u0026quot;: { \u0026quot;count\u0026quot;: 2, \u0026quot;min\u0026quot;: 3.5199999809265137, \u0026quot;max\u0026quot;: 3.890000104904175, \u0026quot;avg\u0026quot;: 3.7050000429153442, \u0026quot;sum\u0026quot;: 7.4100000858306885, \u0026quot;sum_of_squares\u0026quot;: 27.522500681877148, \u0026quot;variance\u0026quot;: 0.03422502293587115, \u0026quot;variance_population\u0026quot;: 0.03422502293587115, \u0026quot;variance_sampling\u0026quot;: 0.0684500458717423, \u0026quot;std_deviation\u0026quot;: 0.18500006198883057, \u0026quot;std_deviation_population\u0026quot;: 0.18500006198883057, \u0026quot;std_deviation_sampling\u0026quot;: 0.2616295967044675, \u0026quot;std_deviation_bounds\u0026quot;: { \u0026quot;upper\u0026quot;: 4.075000166893005, \u0026quot;lower\u0026quot;: 3.334999918937683, \u0026quot;upper_population\u0026quot;: 4.075000166893005, \u0026quot;lower_population\u0026quot;: 3.334999918937683, \u0026quot;upper_sampling\u0026quot;: 4.228259236324279, \u0026quot;lower_sampling\u0026quot;: 3.1817408495064092 } } } } 包含缺失 GPA 值的文档在此计算中被省略。注意 count 中的差异。\n","subcategory":null,"summary":"","tags":null,"title":"扩展统计聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/extended-stats/"},{"category":null,"content":"扩展统计分组聚合 #  extended_stats_bucket 扩展统计分组聚合是 stats_bucket 同级聚合的更全面的版本。除了 stats_bucket 提供的基本统计度量外， extended_stats_bucket 还计算以下指标：\n 平方和 方差 总体方差 抽样方差 标准差 总体标准差 抽样标准差 标准差界限： ** 上限 ** 下限 ** 种群上限 ** 种群下限 ** 采样上限 ** 采样下限  标准差和方差是总体统计量；它们分别始终等于总体标准差和方差。\nstd_deviation_bounds 对象定义了一个范围，该范围在均值（默认为两个标准差）的上方和下方跨越指定的标准差数量。此对象始终包含在输出中，但仅对正态分布的数据才有意义。在解释这些值之前，请验证您的数据集是否遵循正态分布。\n指定的指标必须是数值型，并且同级聚合必须是多分组聚合。\n参数说明 #  extended_stats_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。   sigma 可选 Double 非负） 用于计算 std_deviation_bounds 区间的均值上方和下方的标准差数量。默认值为 2 。参见 extended_stats 中定义范围。    参考样例 #  以下示例创建一个以一个月为间隔的日期直方图。 sum 子聚合计算每个月的字节总和。最后， extended_stats_bucket 聚合返回这些总和的扩展统计信息：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } } } }, \u0026quot;stats_monthly_bytes\u0026quot;: { \u0026quot;extended_stats_bucket\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;visits_per_month\u0026gt;sum_of_bytes\u0026quot;, \u0026quot;sigma\u0026quot;: 3, \u0026quot;format\u0026quot;: \u0026quot;0.##E0\u0026quot; } } } } 返回内容 #  响应包含所选分组的扩展统计信息。请注意，标准偏差界限适用于 3-sigma 范围；更改 sigma （或让其默认为 2 ）将返回不同的结果：\n{ \u0026quot;took\u0026quot;: 6, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 } } ] }, \u0026quot;stats_monthly_bytes\u0026quot;: { \u0026quot;count\u0026quot;: 3, \u0026quot;min\u0026quot;: 2804103, \u0026quot;max\u0026quot;: 39103067, \u0026quot;avg\u0026quot;: 26575229.666666668, \u0026quot;sum\u0026quot;: 79725689, \u0026quot;min_as_string\u0026quot;: \u0026quot;2.8E6\u0026quot;, \u0026quot;max_as_string\u0026quot;: \u0026quot;3.91E7\u0026quot;, \u0026quot;avg_as_string\u0026quot;: \u0026quot;2.66E7\u0026quot;, \u0026quot;sum_as_string\u0026quot;: \u0026quot;7.97E7\u0026quot;, \u0026quot;sum_of_squares\u0026quot;: 2967153221794459, \u0026quot;variance\u0026quot;: 282808242095406.25, \u0026quot;variance_population\u0026quot;: 282808242095406.25, \u0026quot;variance_sampling\u0026quot;: 424212363143109.4, \u0026quot;std_deviation\u0026quot;: 16816903.46334325, \u0026quot;std_deviation_population\u0026quot;: 16816903.46334325, \u0026quot;std_deviation_sampling\u0026quot;: 20596416.2694171, \u0026quot;std_deviation_bounds\u0026quot;: { \u0026quot;upper\u0026quot;: 77025940.05669643, \u0026quot;lower\u0026quot;: -23875480.72336309, \u0026quot;upper_population\u0026quot;: 77025940.05669643, \u0026quot;lower_population\u0026quot;: -23875480.72336309, \u0026quot;upper_sampling\u0026quot;: 88364478.47491796, \u0026quot;lower_sampling\u0026quot;: -35214019.141584635 }, \u0026quot;sum_of_squares_as_string\u0026quot;: \u0026quot;2.97E15\u0026quot;, \u0026quot;variance_as_string\u0026quot;: \u0026quot;2.83E14\u0026quot;, \u0026quot;variance_population_as_string\u0026quot;: \u0026quot;2.83E14\u0026quot;, \u0026quot;variance_sampling_as_string\u0026quot;: \u0026quot;4.24E14\u0026quot;, \u0026quot;std_deviation_as_string\u0026quot;: \u0026quot;1.68E7\u0026quot;, \u0026quot;std_deviation_population_as_string\u0026quot;: \u0026quot;1.68E7\u0026quot;, \u0026quot;std_deviation_sampling_as_string\u0026quot;: \u0026quot;2.06E7\u0026quot;, \u0026quot;std_deviation_bounds_as_string\u0026quot;: { \u0026quot;upper\u0026quot;: \u0026quot;7.7E7\u0026quot;, \u0026quot;lower\u0026quot;: \u0026quot;-2.39E7\u0026quot;, \u0026quot;upper_population\u0026quot;: \u0026quot;7.7E7\u0026quot;, \u0026quot;lower_population\u0026quot;: \u0026quot;-2.39E7\u0026quot;, \u0026quot;upper_sampling\u0026quot;: \u0026quot;8.84E7\u0026quot;, \u0026quot;lower_sampling\u0026quot;: \u0026quot;-3.52E7\u0026quot; } } } } ","subcategory":null,"summary":"","tags":null,"title":"扩展统计分组聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/extended-stats/"},{"category":null,"content":"截断分词过滤器 #  截断(truncate)分词过滤器用于缩短超过指定长度的词元。它会将词元修剪至最大字符数，确保超过该限制的词元被截断。\n参数说明 #  截断分词过滤器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     length 可选 整数 指定生成的词元的最大长度。默认值为 10。    参考样例 #  以下示例请求创建了一个名为 truncate_example 的新索引，并配置了一个带有截断过滤器的分词器。\nPUT /truncate_example { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;truncate_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;truncate\u0026quot;, \u0026quot;length\u0026quot;: 5 } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;truncate_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;truncate_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /truncate_example/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;truncate_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch is powerful and scalable\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;easys\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;power\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 22, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;and\u0026quot;, \u0026quot;start_offset\u0026quot;: 23, \u0026quot;end_offset\u0026quot;: 26, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;scala\u0026quot;, \u0026quot;start_offset\u0026quot;: 27, \u0026quot;end_offset\u0026quot;: 35, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"截断分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/truncate/"},{"category":null,"content":"归一化分词过滤器 #  归一化(normalization)分词过滤器旨在以减少文本差异（尤其是特殊字符差异）的方式对文本进行调整和简化。它主要用于通过对特定语言中的字符进行标准化处理，来应对书写上的差异。\n以下是可用的归一化分词过滤器：\n 阿拉伯语归一化: arabic_normalization 德语归一化: german_normalization 印地语归一化: hindi_normalization 印度语系归一化: indic_normalization 索拉尼语归一化: sorani_normalization 波斯语归一化: persian_normalization 斯堪的纳维亚语归一化: scandinavian_normalization 斯堪的纳维亚语折叠处理归一化: scandinavian_folding 塞尔维亚语归一化: serbian_normalization  参考样例 #  以下示例请求创建了一个名为 german_normalizer_example 的新索引，并配置了一个带有 german_normalization的分词器：\nPUT /german_normalizer_example { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;german_normalizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;german_normalization\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;german_normalizer_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;german_normalizer\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /german_normalizer_example/_analyze { \u0026quot;text\u0026quot;: \u0026quot;Straße München\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;german_normalizer_analyzer\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;strasse\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;munchen\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 14, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"归一化分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/normalization/"},{"category":null,"content":"序列差分聚合 #  serial_diff 序列差分聚合是一个父级管道聚合，用于计算当前分组中指标值与上一个分组中指标值之间的差值。它将结果存储在当前分组中。\n使用 serial_diff 聚合来计算具有指定滞后的时间段之间的变化。 lag 参数（一个正整数值）指定要从中减去当前值的哪个先前分组的值。默认的 lag 值是 1 ，这意味着 serial_diff 从当前分组中的值减去立即前一个分组中的值。\n参数说明 #  serial_diff 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as _string 属性中的格式化输出。   lag 可选 Integer 用于从当前数据分组中减去的历史数据分组。必须是正整数。默认为 1 。    参考样例 #  以下示例创建一个日期直方图，间隔为一个月。 sum 子聚合计算每个月的字节总和。最后， serial_diff 聚合计算这些总和之间的月度字节差异：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;monthly_bytes\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;total_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;monthly_bytes_change\u0026quot;: { \u0026quot;serial_diff\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;total_bytes\u0026quot;, \u0026quot;lag\u0026quot;: 1 } } } } } } 返回内容包含第二个月和第三个月的月度差异。（第一个月 serial_diff 无法计算，因为没有前一个月可以与之比较）：\n{ \u0026quot;took\u0026quot;: 3, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;monthly_bytes\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 }, \u0026quot;monthly_bytes_change\u0026quot;: { \u0026quot;value\u0026quot;: 36298964 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 }, \u0026quot;monthly_bytes_change\u0026quot;: { \u0026quot;value\u0026quot;: -1284548 } } ] } } } 示例：多周期差分 #\n 使用更大的 lag 值来比较每个分组与过去更早时间发生的分组。以下示例计算每周字节数据的差分，滞后为 4（即每个分组与 4 周前的分组进行比较）。这会消除任何周期为 4 周的波动：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;monthly_bytes\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;week\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;total_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;monthly_bytes_change\u0026quot;: { \u0026quot;serial_diff\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;total_bytes\u0026quot;, \u0026quot;lag\u0026quot;: 4 } } } } } } 返回内容包含每周分组的列表。请注意， serial_diff 聚合直到第五个分组才开始，当出现一个 lag 为 4 的分组时：\n{ \u0026quot;took\u0026quot;: 6, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;monthly_bytes\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-24T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1742774400000, \u0026quot;doc_count\u0026quot;: 249, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 1531493 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-31T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743379200000, \u0026quot;doc_count\u0026quot;: 1617, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9213161 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-07T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743984000000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9188671 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-14T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1744588800000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9244851 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-21T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745193600000, \u0026quot;doc_count\u0026quot;: 1609, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9061045 }, \u0026quot;monthly_bytes_change\u0026quot;: { \u0026quot;value\u0026quot;: 7529552 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-28T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1745798400000, \u0026quot;doc_count\u0026quot;: 1554, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 8713507 }, \u0026quot;monthly_bytes_change\u0026quot;: { \u0026quot;value\u0026quot;: -499654 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-05T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746403200000, \u0026quot;doc_count\u0026quot;: 1710, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9544718 }, \u0026quot;monthly_bytes_change\u0026quot;: { \u0026quot;value\u0026quot;: 356047 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-12T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1747008000000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9155820 }, \u0026quot;monthly_bytes_change\u0026quot;: { \u0026quot;value\u0026quot;: -89031 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-19T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1747612800000, \u0026quot;doc_count\u0026quot;: 1610, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 9025078 }, \u0026quot;monthly_bytes_change\u0026quot;: { \u0026quot;value\u0026quot;: -35967 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-26T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1748217600000, \u0026quot;doc_count\u0026quot;: 895, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 5047345 }, \u0026quot;monthly_bytes_change\u0026quot;: { \u0026quot;value\u0026quot;: -3666162 } } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"序列差分聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/serial-diff/"},{"category":null,"content":"平均聚合 #  avg 指标是一个单值指标，它返回某个字段的平均值。\n参数说明 #  avg 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 计算平均值的字段。   missing 可选 Float 要分配给字段缺失实例的值。默认情况下， avg 会在计算中忽略缺失值。    参考样例 #  以下示例请求计算示例数据中 taxful_total_price 字段的平均值：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;avg_taxful_total_price\u0026quot;: { \u0026quot;avg\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot; } } } } 返回内容 #  返回内容包含 taxful_total_price 的平均值：\n{ \u0026quot;took\u0026quot;: 85, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;avg_taxful_total_price\u0026quot;: { \u0026quot;value\u0026quot;: 75.05542864304813 } } } 可以使用聚合名称 avg_taxful_total_price 作为从聚合获取结果的键名。\n缺失值处理 #  通过提取以下文档来准备示例索引。请注意，第二个文档缺少 gpa 值：\nPOST _bulk { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;students\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;John Doe\u0026quot;, \u0026quot;gpa\u0026quot;: 3.89, \u0026quot;grad_year\u0026quot;: 2022} { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;students\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;Jonathan Powers\u0026quot;, \u0026quot;grad_year\u0026quot;: 2025 } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;students\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;Jane Doe\u0026quot;, \u0026quot;gpa\u0026quot;: 3.52, \u0026quot;grad_year\u0026quot;: 2024 } 示例：替换缺失值 #  取平均值，将缺失的 GPA 字段替换为 0 ：\nGET students/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;avg_gpa\u0026quot;: { \u0026quot;avg\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;gpa\u0026quot;, \u0026quot;missing\u0026quot;: 0 } } } } 返回内容如下。可以与下一个忽略了缺失值的示例做个比较：\n{ \u0026quot;took\u0026quot;: 12, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 3, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;avg_gpa\u0026quot;: { \u0026quot;value\u0026quot;: 2.4700000286102295 } } } 示例：忽略缺失值 #\n 取平均值但不分配 missing 参数：\nGET students/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;avg_gpa\u0026quot;: { \u0026quot;avg\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;gpa\u0026quot; } } } } 聚合器计算平均值，省略包含缺失字段值的文档（默认行为）：\n{ \u0026quot;took\u0026quot;: 255, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 3, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;avg_gpa\u0026quot;: { \u0026quot;value\u0026quot;: 3.7050000429153442 } } }   ","subcategory":null,"summary":"","tags":null,"title":"平均聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/average/"},{"category":null,"content":"平均分组聚合 #  avg_bucket 平均分组聚合是一个同级聚合，它计算上一个聚合的每个分组中的指标平均值。\n指定的指标必须是数值型的，且同级聚合必须是多分组聚合。\n参数说明 #  avg_bucket 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合存储分组的路径。请参阅存储分组路径 。   gap_policy 可选 String 要应用于缺失数据的策略。有效值为 skip 和 insert_zeros。默认值为 skip。请参阅数据差距 。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出 。    参考样例 #  以下示例创建间隔为一个月的日期直方图。sum 子聚合计算每个月的字节总和。最后，avg_bucket 聚合根据这些总和计算每月的平均字节数：\nPOST sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } } } }, \u0026quot;avg_monthly_bytes\u0026quot;: { \u0026quot;avg_bucket\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;visits_per_month\u0026gt;sum_of_bytes\u0026quot; } } } } 返回内容 #  聚合返回每月存储分组的平均字节数：\n{ \u0026quot;took\u0026quot;: 43, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;visits_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;sum_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 } } ] }, \u0026quot;avg_monthly_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 26575229.666666668 } } } ","subcategory":null,"summary":"","tags":null,"title":"平均分组聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/avg-bucket/"},{"category":null,"content":"平图化分词过滤器 #  平图化（flatten_graph）分词过滤器用于处理在图结构中同一位置生成多个词元时出现的复杂词元关系。一些分词过滤器，比如同义词图（synonym_graph）和词分隔符图（word_delimiter_graph），会生成多位置词元——即相互重叠或跨越多个位置的词元。这些词元图对于搜索查询很有用，但在索引过程中却不被直接支持。平图化分词过滤器会将多位置词元解析为一个线性的词元序列。对图进行扁平化处理可确保与索引过程兼容。\n 词元图的扁平化是一个有损耗的过程。只要有可能，就应避免使用平图化过滤器。相反，应仅在搜索分词器中应用图分词过滤器，这样就无需使用平图化分词过滤器了。\n 参考样例 #  以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有平图化分词过滤器的分词器：\nPUT /test_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_index_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;my_custom_filter\u0026quot;, \u0026quot;flatten_graph\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;my_custom_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;word_delimiter_graph\u0026quot;, \u0026quot;catenate_all\u0026quot;: true } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /test_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_index_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch helped many employers\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;helped\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;many\u0026quot;, \u0026quot;start_offset\u0026quot;: 18, \u0026quot;end_offset\u0026quot;: 22, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;employers\u0026quot;, \u0026quot;start_offset\u0026quot;: 23, \u0026quot;end_offset\u0026quot;: 32, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 } ] } ","subcategory":null,"summary":"","tags":null,"title":"平图化分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/flatten-graph/"},{"category":null,"content":"常用词组分词过滤器 #  常用词组(common_grams)分词过滤器通过保留文本中常见的短语（常用词组）来提高搜索的相关性。当处理某些语言或数据集时，特定的单词组合经常作为一个整体出现，如果将它们当作单独的词元处理，可能会影响搜索的相关性，此时该过滤器就非常有用。如果输入字符串中常用词，这个分词过滤器会同时生成它们的一元词组（单个词）和二元词组（两个词的组合）。\n使用这个分词过滤器可以保持常用短语的完整性，从而提高搜索的相关性。这有助于更准确地匹配查询内容，尤其是对于频繁出现的单词组合。它还能通过减少不相关的匹配结果来提高搜索的精度。\n 使用此过滤器时，你必须谨慎选择并维护常用词(common_words)列表。\n 参数说明 #  常用词组分词过滤器可通过以下参数进行配置：\n   参数 必需/可选 数据类型 描述     common_words 必需 字符串列表 一个应被视为常用词词组的单词列表。如果 common_words 参数是一个空列表，common_grams 分词过滤器将成为一个无操作过滤器，即它根本不会修改输入的词元。   ignore_case 可选 布尔值 指示过滤器在匹配常用词时是否应忽略大小写差异。默认值为 false。   query_mode 可选 布尔值 当设置为 true 时，应用以下规则：\n- 从 common_words 生成的一元词组（单个词）不包含在输出中。\n- 非常用词后跟常用词形成的二元词组会保留在输出中。\n- 如果非常用词的一元词组后面紧接着一个常用词，则该一元词组会被排除。\n- 如果非常用词出现在文本末尾且前面是一个常用词，其一元词组不包含在输出中。    参考样例 #  以下示例请求创建了一个名为 my_common_grams_index 的新索引，并配置了一个使用常用词组（common_grams）过滤器的分析器。\nPUT /my_common_grams_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_common_grams_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;common_grams\u0026quot;, \u0026quot;common_words\u0026quot;: [\u0026quot;a\u0026quot;, \u0026quot;in\u0026quot;, \u0026quot;for\u0026quot;], \u0026quot;ignore_case\u0026quot;: true, \u0026quot;query_mode\u0026quot;: true } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_common_grams_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_common_grams_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;A quick black cat jumps over the lazy dog in the park\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ {\u0026quot;token\u0026quot;: \u0026quot;a_quick\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 7,\u0026quot;type\u0026quot;: \u0026quot;gram\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;quick\u0026quot;,\u0026quot;start_offset\u0026quot;: 2,\u0026quot;end_offset\u0026quot;: 7,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 1}, {\u0026quot;token\u0026quot;: \u0026quot;black\u0026quot;,\u0026quot;start_offset\u0026quot;: 8,\u0026quot;end_offset\u0026quot;: 13,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;cat\u0026quot;,\u0026quot;start_offset\u0026quot;: 14,\u0026quot;end_offset\u0026quot;: 17,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;jumps\u0026quot;,\u0026quot;start_offset\u0026quot;: 18,\u0026quot;end_offset\u0026quot;: 23,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;over\u0026quot;,\u0026quot;start_offset\u0026quot;: 24,\u0026quot;end_offset\u0026quot;: 28,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 5}, {\u0026quot;token\u0026quot;: \u0026quot;the\u0026quot;,\u0026quot;start_offset\u0026quot;: 29,\u0026quot;end_offset\u0026quot;: 32,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 6}, {\u0026quot;token\u0026quot;: \u0026quot;lazy\u0026quot;,\u0026quot;start_offset\u0026quot;: 33,\u0026quot;end_offset\u0026quot;: 37,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 7}, {\u0026quot;token\u0026quot;: \u0026quot;dog_in\u0026quot;,\u0026quot;start_offset\u0026quot;: 38,\u0026quot;end_offset\u0026quot;: 44,\u0026quot;type\u0026quot;: \u0026quot;gram\u0026quot;,\u0026quot;position\u0026quot;: 8}, {\u0026quot;token\u0026quot;: \u0026quot;in_the\u0026quot;,\u0026quot;start_offset\u0026quot;: 42,\u0026quot;end_offset\u0026quot;: 48,\u0026quot;type\u0026quot;: \u0026quot;gram\u0026quot;,\u0026quot;position\u0026quot;: 9}, {\u0026quot;token\u0026quot;: \u0026quot;the\u0026quot;,\u0026quot;start_offset\u0026quot;: 45,\u0026quot;end_offset\u0026quot;: 48,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 10}, {\u0026quot;token\u0026quot;: \u0026quot;park\u0026quot;,\u0026quot;start_offset\u0026quot;: 49,\u0026quot;end_offset\u0026quot;: 53,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 11} ] } ","subcategory":null,"summary":"","tags":null,"title":"常用词组分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/common-grams/"},{"category":null,"content":"嵌套聚合 #  nested 聚合让你能够对嵌套对象内的字段进行聚合。 nested 类型是对象数据类型的特殊版本，它允许对象数组以独立于彼此的方式进行索引，从而可以独立于彼此进行查询\n使用 object 类型，所有数据都存储在同一个文档中，因此搜索匹配可以跨越子文档。例如，想象一个 logs 索引，其中 pages 映射为 object 数据类型：\nPUT logs/_doc/0 { \u0026quot;response\u0026quot;: \u0026quot;200\u0026quot;, \u0026quot;pages\u0026quot;: [ { \u0026quot;page\u0026quot;: \u0026quot;landing\u0026quot;, \u0026quot;load_time\u0026quot;: 200 }, { \u0026quot;page\u0026quot;: \u0026quot;blog\u0026quot;, \u0026quot;load_time\u0026quot;: 500 } ] } Easysearch 合并所有看起来像这样的实体关系的子属性：\n{ \u0026quot;logs\u0026quot;: { \u0026quot;pages\u0026quot;: [\u0026quot;landing\u0026quot;, \u0026quot;blog\u0026quot;], \u0026quot;load_time\u0026quot;: [\u0026quot;200\u0026quot;, \u0026quot;500\u0026quot;] } } 所以，如果你想要用 pages=landing 和 load_time=500 搜索这个索引，即使 load_time 的 landing 值为 200，这个文档也符合条件。\n如果你想要确保不会发生这种跨对象匹配，将字段映射为 nested 类型：\nPUT logs { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;pages\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;nested\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;page\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;load_time\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot; } } } } } } 嵌套文档允许你索引相同的 JSON 文档，但会保持你的页面在不同的 Lucene 文档中，使得只有 pages=landing 和 load_time=200 这样的搜索能返回预期结果。内部上，嵌套对象将数组中的每个对象索引为一个单独的隐藏文档，这意味着每个嵌套对象都可以独立于其他对象进行查询。\n您必须指定相对于父级包含嵌套文档的嵌套路径：\nGET logs/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;response\u0026quot;: \u0026quot;200\u0026quot; } }, \u0026quot;aggs\u0026quot;: { \u0026quot;pages\u0026quot;: { \u0026quot;nested\u0026quot;: { \u0026quot;path\u0026quot;: \u0026quot;pages\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;min_load_time\u0026quot;: { \u0026quot;min\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;pages.load_time\u0026quot; } } } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;pages\u0026quot; : { \u0026quot;doc_count\u0026quot; : 2, \u0026quot;min_load_time\u0026quot; : { \u0026quot;value\u0026quot; : 200 } } } }   ","subcategory":null,"summary":"","tags":null,"title":"嵌套聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/nested/"},{"category":null,"content":"小写词元生成器 #  小写（lowercase）词元生成器会在空白处将文本拆分成词条，然后把所有词条转换为小写形式。从功能上来说，这与配置一个字母词元生成器并搭配一个小写词元过滤器的效果是一样的。不过，使用小写词元生成器效率更高，因为分词操作是在一步之内完成的。\n参考样例 #  以下示例请求创建了一个名为 my-lowercase-index 的新索引，并配置了一个使用小写词元生成器的分词器：\nPUT /my-lowercase-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_lowercase_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;lowercase\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_lowercase_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_lowercase_tokenizer\u0026quot; } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my-lowercase-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_lowercase_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;This is a Test. Easysearch 123!\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;this\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;a\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 9, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;test\u0026quot;, \u0026quot;start_offset\u0026quot;: 10, \u0026quot;end_offset\u0026quot;: 14, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 16, \u0026quot;end_offset\u0026quot;: 26, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"小写词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/lowercase/"},{"category":null,"content":"小写分词过滤器 #  小写(lowercase)分词过滤器用于将分词流中的所有字符转换为小写，从而使搜索不区分大小写。\n参数 #  小写分词过滤器可以使用以下参数进行配置。\n   参数 必填/可选 描述     language 可选 指定一个特定语言的分词过滤器。有效值为：\n- 希腊语 greek\n- 爱尔兰语irish\n- 土耳其语turkish。\n默认值是 Lucene 的小写过滤器。    参考样例 #  以下示例请求创建了一个名为 custom_lowercase_example 的新索引。它配置了一个带有小写过滤器的分析器，并指定语言为希腊语。\nPUT /custom_lowercase_example { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;greek_lowercase_example\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;greek_lowercase\u0026quot;] } }, \u0026quot;filter\u0026quot;: { \u0026quot;greek_lowercase\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;lowercase\u0026quot;, \u0026quot;language\u0026quot;: \u0026quot;greek\u0026quot; } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /custom_lowercase_example/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;greek_lowercase_example\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Αθήνα ΕΛΛΑΔΑ\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;αθηνα\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 5, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;ελλαδα\u0026quot;, \u0026quot;start_offset\u0026quot;: 6, \u0026quot;end_offset\u0026quot;: 12, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"小写分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/lowercase/"},{"category":null,"content":"导数聚合 #  derivative 导数聚合是一个父聚合，用于计算聚合每个分组的一阶和二阶导数。\n对于有序的分组序列， derivative 将当前分组和前一个分组中的指标值之差近似为一阶导数。\n参数说明 #  derivative 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 String 要聚合的聚合分组的路径。参见分组路径。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认为 skip 。参见数据间隙。   format 可选 String DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。    示例：一阶导数 #  以下示例创建一个每月间隔的日期直方图。 sum 子聚合计算每个月所有字节的和。最后， derivative 聚合计算 sum 子聚合的一阶导数。一阶导数估计为当前月份和上个月字节数之间的差值：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;number_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;bytes_deriv\u0026quot;: { \u0026quot;derivative\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;number_of_bytes\u0026quot; } } } } } } 返回内容显示了为第二和第三个分组计算出的导数：\n{ \u0026quot;took\u0026quot;: 6, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;number_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;number_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 }, \u0026quot;bytes_deriv\u0026quot;: { \u0026quot;value\u0026quot;: 36298964 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;number_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 }, \u0026quot;bytes_deriv\u0026quot;: { \u0026quot;value\u0026quot;: -1284548 } } ] } } } 第一个分组没有计算导数，因为该分组没有可用的前一个分组。\n示例：二阶导数 #  要计算二阶导数，将一个导数聚合连接到另一个导数聚合上：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;number_of_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;bytes_1st_deriv\u0026quot;: { \u0026quot;derivative\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;number_of_bytes\u0026quot; } }, \u0026quot;bytes_2nd_deriv\u0026quot;: { \u0026quot;derivative\u0026quot;: { \u0026quot;buckets_path\u0026quot;: \u0026quot;bytes_1st_deriv\u0026quot; } } } } } } 返回内容\n{ \u0026quot;took\u0026quot;: 6, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 480, \u0026quot;number_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 2804103 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 6849, \u0026quot;number_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 39103067 }, \u0026quot;bytes_1st_deriv\u0026quot;: { \u0026quot;value\u0026quot;: 36298964 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 6745, \u0026quot;number_of_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 37818519 }, \u0026quot;bytes_1st_deriv\u0026quot;: { \u0026quot;value\u0026quot;: -1284548 }, \u0026quot;bytes_2nd_deriv\u0026quot;: { \u0026quot;value\u0026quot;: -37583512 } } ] } } } 由于该分组没有可用的前一个分组，因此第一个分组没有计算一阶导数。类似地，第一个和第二个分组也没有计算二阶导数。\n","subcategory":null,"summary":"","tags":null,"title":"导数聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/derivative/"},{"category":null,"content":"存储分组选择聚合 #  bucket_selector 存储分组选择聚合是一个父管道聚合，它评估脚本以确定直方图 （或 date_histogram ）聚合返回的存储分组是否应包含在最终结果中。\n与创建新值的管道聚合不同，bucket_selector 聚合充当筛选器，根据指定的条件保留或删除整个存储分组。使用此聚合可根据存储分组的计算指标筛选存储分组。\n参数说明 #  bucket_selector 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 Object 变量名称到分分组指标的映射，用于标识要在脚本中使用的指标。指标必须是数字。请参阅脚本变量 。   script 必需 String 或 Object 要执行的脚本。可以是内联脚本、存储脚本或脚本文件。该脚本可以访问 buckets_path 参数中定义的变量名称。必须返回布尔值。返回 false 的存储分组将从最终输出中删除。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认值为 skip 。详见 数据空缺。    参考样例 #  以下示例创建间隔为一周的日期直方图。sum 子聚合计算每周所有销售额的总和。最后，bucket_selector 聚合会筛选生成的每周存储分组，删除所有总值不超过 75,000 美元的存储分组：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;sales_per_week\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;order_date\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;week\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;weekly_sales\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;$#,###.00\u0026quot; } }, \u0026quot;avg_vendor_spend\u0026quot;: { \u0026quot;bucket_selector\u0026quot;: { \u0026quot;buckets_path\u0026quot;: { \u0026quot;weekly_sales\u0026quot;: \u0026quot;weekly_sales\u0026quot; }, \u0026quot;script\u0026quot;: \u0026quot;params.weekly_sales \u0026gt; 75000\u0026quot; } } } } } } 返回内容 #  聚合返回满足脚本条件的 sales_per_week 存储分组：\n{ \u0026quot;took\u0026quot;: 3, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;sales_per_week\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-31T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743379200000, \u0026quot;doc_count\u0026quot;: 1048, \u0026quot;weekly_sales\u0026quot;: { \u0026quot;value\u0026quot;: 79448.60546875, \u0026quot;value_as_string\u0026quot;: \u0026quot;$79,448.61\u0026quot; } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-07T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743984000000, \u0026quot;doc_count\u0026quot;: 1048, \u0026quot;weekly_sales\u0026quot;: { \u0026quot;value\u0026quot;: 78208.4296875, \u0026quot;value_as_string\u0026quot;: \u0026quot;$78,208.43\u0026quot; } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-14T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1744588800000, \u0026quot;doc_count\u0026quot;: 1073, \u0026quot;weekly_sales\u0026quot;: { \u0026quot;value\u0026quot;: 81277.296875, \u0026quot;value_as_string\u0026quot;: \u0026quot;$81,277.30\u0026quot; } } ] } } } \n由于它返回布尔值而不是数值，因此 buckets_selector 聚合不采用格式参数。在此示例中，格式化的指标由 sum 子聚合在 value_as_string 结果中返回。将此与 bucket_script 聚合中的示例进行对比。\n ","subcategory":null,"summary":"","tags":null,"title":"存储分组选择聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/bucket-selector/"},{"category":null,"content":"存储分组脚本聚合 #  bucket_script 存储分组脚本聚合是一个父管道聚合，它执行脚本以跨一组存储分组执行每个存储分组的数字计算。使用 bucket_script 聚合对分分组聚合中的多个指标执行自定义数值计算。例如，您可以：\n 计算派生指标和复合指标。 使用 if/else 语句应用条件逻辑。 计算特定于业务的 KPI，例如自定义评分指标。  参数说明 #  bucket_script 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     buckets_path 必需 Object 一个变量名称到分分组指标的映射，用于识别脚本中使用的指标。这些指标必须是数值型。参见脚本变量 。   script 必需 String 或 Object 要执行的脚本。可以是内联脚本、存储脚本或脚本文件。脚本可以访问通过 buckets_path 参数定义的变量名。必须返回一个数值。   gap_policy 可选 String 应用于缺失数据的策略。有效值为 skip 和 insert_zeros 。默认值为 skip 。详见 数据空缺。   format 可选 String 一个 DecimalFormat 格式化字符串。将在聚合的 value_as_string 参数中返回格式化后的输出。    脚本变量 #  buckets_path 参数将脚本变量名称映射到父聚合的指标。然后可以在脚本中使用这些变量。\n 对于 bucket_script 和 bucket_selector 聚合， buckets_path 参数是一个对象而不是字符串，因为它必须引用多个分组指标。有关 buckets_path 字符串版本的描述，请参阅管道聚合页面。\n 以下 buckets_path 将 sales_sum 指标映射到 total_sales 脚本变量，并将 item_count 指标映射到 item_count 脚本变量：\n\u0026quot;buckets_path\u0026quot;: { \u0026quot;total_sales\u0026quot;: \u0026quot;sales_sum\u0026quot;, \u0026quot;item_count\u0026quot;: \u0026quot;item_count\u0026quot; } 映射的变量可以从 params 上下文中访问。例如：\n params.total_sales params.item_count  参考样例 #  以下示例创建了一个按月份间隔的一组日期直方图。 total_sales 子聚合计算了每个月销售的所有商品的税后总价。 vendor_count 聚合计算了每个月的唯一供应商总数。最后， avg_vendor_spend 聚合使用内联脚本计算每个月每个供应商的平均消费金额：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;order_date\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;total_sales\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot; } }, \u0026quot;vendor_count\u0026quot;: { \u0026quot;cardinality\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.manufacturer.keyword\u0026quot; } }, \u0026quot;avg_vendor_spend\u0026quot;: { \u0026quot;bucket_script\u0026quot;: { \u0026quot;buckets_path\u0026quot;: { \u0026quot;sales\u0026quot;: \u0026quot;total_sales\u0026quot;, \u0026quot;vendors\u0026quot;: \u0026quot;vendor_count\u0026quot; }, \u0026quot;script\u0026quot;: \u0026quot;params.sales / params.vendors\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;$#,###.00\u0026quot; } } } } } } 返回内容 #\n 聚合返回格式化的每月平均供应商支出：\n{ \u0026quot;took\u0026quot;: 6, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-03-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1740787200000, \u0026quot;doc_count\u0026quot;: 721, \u0026quot;vendor_count\u0026quot;: { \u0026quot;value\u0026quot;: 21 }, \u0026quot;total_sales\u0026quot;: { \u0026quot;value\u0026quot;: 53468.1484375 }, \u0026quot;avg_vendor_spend\u0026quot;: { \u0026quot;value\u0026quot;: 2546.1023065476193, \u0026quot;value_as_string\u0026quot;: \u0026quot;$2,546.10\u0026quot; } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 3954, \u0026quot;vendor_count\u0026quot;: { \u0026quot;value\u0026quot;: 21 }, \u0026quot;total_sales\u0026quot;: { \u0026quot;value\u0026quot;: 297415.98046875 }, \u0026quot;avg_vendor_spend\u0026quot;: { \u0026quot;value\u0026quot;: 14162.665736607143, \u0026quot;value_as_string\u0026quot;: \u0026quot;$14,162.67\u0026quot; } } ] } } } ","subcategory":null,"summary":"","tags":null,"title":"存储分组脚本聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/bucket-script/"},{"category":null,"content":"字符组词元生成器 #  字符组（char_group）词元生成器使用特定字符作为分隔符将文本拆分为词元。它适用于需要简单直接进行分词的场景，为基于词元生成器的匹配模式提供了一种更简单的替代方案，避免了额外的复杂性。\n参考样例 #  以下示例请求创建了一个名为my_index的新索引，并配置了一个带有 char_group 字符组词元生成器的分词器。该词元生成器会依据空格、连字符 - 和冒号 : 来分割文本。\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_char_group_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;char_group\u0026quot;, \u0026quot;tokenize_on_chars\u0026quot;: [ \u0026quot;whitespace\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;:\u0026quot; ] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_char_group_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_char_group_tokenizer\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_char_group_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_char_group_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Fast-driving cars: they drive fast!\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Fast\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;driving\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 12, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;cars\u0026quot;, \u0026quot;start_offset\u0026quot;: 13, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;they\u0026quot;, \u0026quot;start_offset\u0026quot;: 19, \u0026quot;end_offset\u0026quot;: 23, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;drive\u0026quot;, \u0026quot;start_offset\u0026quot;: 24, \u0026quot;end_offset\u0026quot;: 29, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;fast!\u0026quot;, \u0026quot;start_offset\u0026quot;: 30, \u0026quot;end_offset\u0026quot;: 35, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 5 } ] } 参数说明 #  char_group 词元生成器可使用以下参数进行配置。\n   参数 必填/选填 数据类型 描述     tokenize_on_chars 必填 数组 指定用于对文本进行分词的一组字符。可以指定单个字符（例如 - 或 @），包括转义字符（例如 \\n ），或者字符类，如空白字符（whitespace）、字母（letter）、数字（digit）、标点符号（punctuation）或符号（symbol）。   max_token_length 选填 整数 设置生成词元的最大长度。如果超过此长度，词元将在max_token_length配置的长度处拆分为多个词元。默认值为 255。    ","subcategory":null,"summary":"","tags":null,"title":"字符组词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/character-group/"},{"category":null,"content":"字母词元生成器 #  字母(letter)词元生成器会根据任何非字母字符将文本拆分成单词。它在处理许多欧洲语种时效果良好，但对于一些亚洲语种却不太有效，因为在亚洲语种中，单词之间不是由空格来分隔的。\n参考样例 #  下面的示例请求创建了一个名为 my_index 的新索引，并配置了一个使用字母词元生成器的分词器。\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_letter_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;letter\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_letter_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST _analyze { \u0026quot;tokenizer\u0026quot;: \u0026quot;letter\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Cats 4EVER love chasing butterflies!\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Cats\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;EVER\u0026quot;, \u0026quot;start_offset\u0026quot;: 6, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;love\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;chasing\u0026quot;, \u0026quot;start_offset\u0026quot;: 16, \u0026quot;end_offset\u0026quot;: 23, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;butterflies\u0026quot;, \u0026quot;start_offset\u0026quot;: 24, \u0026quot;end_offset\u0026quot;: 35, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"字母词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/letter/"},{"category":null,"content":"子关联聚合 #  children 子关联聚合是一种存储分组聚合，它根据索引中定义的父子关系创建包含子文档的单个存储分组。\n子关联聚合与 join 字段类型配合使用，以聚合与父文档关联的子文档。\n子关联聚合标识与特定子关系名称匹配的子文档，而 parent 父聚合标识具有匹配子文档的父文档。这两个聚合都采用子关系名称作为输入。\n参数说明 #  children 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     type 必填 String join 字段中的子类型的名称。这标识了要使用的父子关系。    参考样例 #  以下示例构建一个包含三名员工的小型公司数据库。每个员工记录都与父部门记录具有子联接关系。\n首先，创建一个 company 索引，其中包含一个将部门（父级）映射到员工（子级）的联接字段：\nPUT /company { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;join_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;join\u0026quot;, \u0026quot;relations\u0026quot;: { \u0026quot;department\u0026quot;: \u0026quot;employee\u0026quot; } }, \u0026quot;department_name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;employee_name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;salary\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot; }, \u0026quot;hire_date\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot; } } } } 接下来，使用三个部门和三名员工填充数据。下表显示了父子分配。\n   部门（父关系） 员工（子关系）     Accounting Abel Anderson, Betty Billings   Engineering Carl Carter   HR none    routing 参数可确保父文档和子文档存储在同一分片上，这是父子关系在 Easysearch 中正常运行所必需的：\nPOST _bulk?routing=1 { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;department\u0026quot;, \u0026quot;department_name\u0026quot;: \u0026quot;Accounting\u0026quot;, \u0026quot;join_field\u0026quot;: \u0026quot;department\u0026quot; } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;department\u0026quot;, \u0026quot;department_name\u0026quot;: \u0026quot;Engineering\u0026quot;, \u0026quot;join_field\u0026quot;: \u0026quot;department\u0026quot; } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;department\u0026quot;, \u0026quot;department_name\u0026quot;: \u0026quot;HR\u0026quot;, \u0026quot;join_field\u0026quot;: \u0026quot;department\u0026quot; } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;employee_name\u0026quot;: \u0026quot;Abel Anderson\u0026quot;, \u0026quot;salary\u0026quot;: 120000, \u0026quot;hire_date\u0026quot;: \u0026quot;2024-04-04\u0026quot;, \u0026quot;join_field\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;parent\u0026quot;: \u0026quot;1\u0026quot; } } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;5\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;employee_name\u0026quot;: \u0026quot;Betty Billings\u0026quot;, \u0026quot;salary\u0026quot;: 140000, \u0026quot;hire_date\u0026quot;: \u0026quot;2023-05-05\u0026quot;, \u0026quot;join_field\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;parent\u0026quot;: \u0026quot;1\u0026quot; } } { \u0026quot;create\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;6\u0026quot; } } { \u0026quot;type\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;employee_name\u0026quot;: \u0026quot;Carl Carter\u0026quot;, \u0026quot;salary\u0026quot;: 140000, \u0026quot;hire_date\u0026quot;: \u0026quot;2020-06-06\u0026quot;, \u0026quot;join_field\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;employee\u0026quot;, \u0026quot;parent\u0026quot;: \u0026quot;2\u0026quot; } } 以下请求查询所有部门，然后筛选名为 Accounting 的部门。然后，它使用子关联聚合来选择与会计部门具有子关系的两个单据。最后，avg 子关联聚合返回会计员工工资的平均值：\nGET /company/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;filter\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;join_field\u0026quot;: \u0026quot;department\u0026quot; } }, { \u0026quot;term\u0026quot;: { \u0026quot;department_name\u0026quot;: \u0026quot;Accounting\u0026quot; } } ] } }, \u0026quot;aggs\u0026quot;: { \u0026quot;acc_employees\u0026quot;: { \u0026quot;children\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;employee\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;avg_salary\u0026quot;: { \u0026quot;avg\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;salary\u0026quot; } } } } } } 返回内容包含所选部门存储分组，查找部门的员工类型子级，并计算其工资的平均值 ：\n{ \u0026quot;took\u0026quot;: 379, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 1, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;acc_employees\u0026quot;: { \u0026quot;doc_count\u0026quot;: 2, \u0026quot;avg_salary\u0026quot;: { \u0026quot;value\u0026quot;: 110000 } } } } \n","subcategory":null,"summary":"","tags":null,"title":"子关联聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/children/"},{"category":null,"content":"大写分词过滤器 #  大写(uppercase)分词过滤器用于在分析过程中将所有词元（单词）转换为大写形式。\n参考样例 #  以下示例请求创建了一个名为 uppercase_example 的新索引，并配置了一个带有大写过滤器的分词器。\nPUT /uppercase_example { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;uppercase_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;uppercase\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;uppercase_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;uppercase_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /uppercase_example/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;uppercase_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch is powerful\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;EASYSEARCH\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;IS\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;POWERFUL\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 22, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"大写分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/uppercase/"},{"category":null,"content":"多过滤聚合 #  filters 聚合与 filter 聚合相同，但它允许你使用多个过滤器聚合。 filter 聚合结果为一个分组，而 filters 聚合会返回多个分组，每个定义的过滤器对应一个分组。\n要为所有未匹配任何过滤器查询的文档创建一个分组，将 other_bucket 属性设置为 true ：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;200_os\u0026quot;: { \u0026quot;filters\u0026quot;: { \u0026quot;other_bucket\u0026quot;: true, \u0026quot;filters\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;response.keyword\u0026quot;: \u0026quot;200\u0026quot; } }, { \u0026quot;term\u0026quot;: { \u0026quot;machine.os.keyword\u0026quot;: \u0026quot;osx\u0026quot; } } ] }, \u0026quot;aggs\u0026quot;: { \u0026quot;avg_amount\u0026quot;: { \u0026quot;avg\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } } } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;200_os\u0026quot; : { \u0026quot;buckets\u0026quot; : [ { \u0026quot;doc_count\u0026quot; : 12832, \u0026quot;avg_amount\u0026quot; : { \u0026quot;value\u0026quot; : 5897.852711970075 } }, { \u0026quot;doc_count\u0026quot; : 2825, \u0026quot;avg_amount\u0026quot; : { \u0026quot;value\u0026quot; : 5620.347256637168 } }, { \u0026quot;doc_count\u0026quot; : 1017, \u0026quot;avg_amount\u0026quot; : { \u0026quot;value\u0026quot; : 3247.0963618485744 } } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"多过滤聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/filters/"},{"category":null,"content":"多路复用分词过滤器 #  多路复用(multiplexer)分词过滤器允许你通过应用不同的过滤器来创建同一词元的多个版本。当你想要以多种方式分析同一个词元时，这非常有用。例如，你可能希望使用不同的词干提取、同义词或 n-gram 过滤器来分析一个词元，并将所有生成的词元一起使用。这个分词过滤器的工作方式是复制分词流，并对每个副本应用不同的过滤器。\n 多路复用分词过滤器会从分词流中移除重复的词元。\n  多路复用分词过滤器不支持多词同义词(synonym)过滤器、同义词图(synonym_graph)分词过滤器或组合词（shingle）分词过滤器，因为这些过滤器不仅需要分析当前词元，还需要分析后续的词元，以便正确确定如何转换输入内容。\n 参数说明 #  多路复用分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     filters 可选 字符串列表 要应用于分词流每个副本的分词过滤器的逗号分隔列表。默认值为空列表。   preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 true。    参考样例 #  以下示例请求创建了一个名为 multiplexer_index 的新索引，并配置了一个带有多路复用过滤器的分词器：\nPUT /multiplexer_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;english_stemmer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;stemmer\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;english\u0026quot; }, \u0026quot;synonym_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym\u0026quot;, \u0026quot;synonyms\u0026quot;: [ \u0026quot;quick,fast\u0026quot; ] }, \u0026quot;multiplexer_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;multiplexer\u0026quot;, \u0026quot;filters\u0026quot;: [\u0026quot;english_stemmer\u0026quot;, \u0026quot;synonym_filter\u0026quot;], \u0026quot;preserve_original\u0026quot;: true } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;multiplexer_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;multiplexer_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /multiplexer_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;multiplexer_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;The slow turtle hides from the quick dog\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;The\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 3, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;slow\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;, \u0026quot;start_offset\u0026quot;: 9, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;turtl\u0026quot;, \u0026quot;start_offset\u0026quot;: 9, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;hides\u0026quot;, \u0026quot;start_offset\u0026quot;: 16, \u0026quot;end_offset\u0026quot;: 21, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;hide\u0026quot;, \u0026quot;start_offset\u0026quot;: 16, \u0026quot;end_offset\u0026quot;: 21, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;from\u0026quot;, \u0026quot;start_offset\u0026quot;: 22, \u0026quot;end_offset\u0026quot;: 26, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;the\u0026quot;, \u0026quot;start_offset\u0026quot;: 27, \u0026quot;end_offset\u0026quot;: 30, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 }, { \u0026quot;token\u0026quot;: \u0026quot;quick\u0026quot;, \u0026quot;start_offset\u0026quot;: 31, \u0026quot;end_offset\u0026quot;: 36, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 6 }, { \u0026quot;token\u0026quot;: \u0026quot;fast\u0026quot;, \u0026quot;start_offset\u0026quot;: 31, \u0026quot;end_offset\u0026quot;: 36, \u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;, \u0026quot;position\u0026quot;: 6 }, { \u0026quot;token\u0026quot;: \u0026quot;dog\u0026quot;, \u0026quot;start_offset\u0026quot;: 37, \u0026quot;end_offset\u0026quot;: 40, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 7 } ] } ","subcategory":null,"summary":"","tags":null,"title":"多路复用分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/multiplexer/"},{"category":null,"content":"多样化采样聚合 #  diversified_sampler 多样化采样聚合允许你通过去重包含相同 field 的文档来减少样本池分布的偏差。它通过使用 max_docs_per_value 和 field 设置来实现，这些设置限制了在分片上收集的 field 的最大文档数。 max_docs_per_value 设置是一个可选参数，用于确定每个 field 将返回的最大文档数。此设置的默认值为 1 。\n与 sampler 聚合类似，你可以使用 shard_size 设置来控制在任何单个分片上收集的最大文档数，如下面的示例所示：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;sample\u0026quot;: { \u0026quot;diversified_sampler\u0026quot;: { \u0026quot;shard_size\u0026quot;: 1000, \u0026quot;field\u0026quot;: \u0026quot;response.keyword\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;agent.keyword\u0026quot; } } } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;sample\u0026quot; : { \u0026quot;doc_count\u0026quot; : 3, \u0026quot;terms\u0026quot; : { \u0026quot;doc_count_error_upper_bound\u0026quot; : 0, \u0026quot;sum_other_doc_count\u0026quot; : 0, \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\u0026quot;, \u0026quot;doc_count\u0026quot; : 2 }, { \u0026quot;key\u0026quot; : \u0026quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\u0026quot;, \u0026quot;doc_count\u0026quot; : 1 } ] } } } }   ","subcategory":null,"summary":"","tags":null,"title":"多样化采样聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/diversified-sampler/"},{"category":null,"content":"复合聚合 #  composite 复合聚合基于一个或多个文档字段或源创建分组。 composite 聚合为每个单独源值的组合创建一个分组。默认情况下，如果一个或多个字段中缺少值，则这些组合会从结果中省略。\n每个源有四种类型的聚合之一：\n terms 类型按唯一（通常是 String ）值分组。 histogram 类型按指定宽度数值分组。 date_histogram 类型按指定宽度的日期或时间范围分组。 geotile_grid 类型按指定分辨率将地理点分组到网格中。  composite 聚合通过将其源键组合到分组中来工作。生成的分组是有序的，跨源(Across)和源内部(Within)都是：\n Across：分组按聚合请求中源的顺序嵌套。 Within:每个源中值的顺序决定了该源的分组顺序。排序方式根据源类型适当选择，可以是字母顺序、数字顺序、日期时间顺序或地理切片顺序。  考虑一下来自马拉松参与者索引的这些字段：\n{... \u0026quot;city\u0026quot;: \u0026quot;Albuquerque\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Bronze\u0026quot; ...} {... \u0026quot;city\u0026quot;: \u0026quot;Boston\u0026quot;, ...} {... \u0026quot;city\u0026quot;: \u0026quot;Chicago\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Bronze\u0026quot; ...} {... \u0026quot;city\u0026quot;: \u0026quot;Albuquerque\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Gold\u0026quot; ...} {... \u0026quot;city\u0026quot;: \u0026quot;Chicago\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Silver\u0026quot; ...} {... \u0026quot;city\u0026quot;: \u0026quot;Boston\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Bronze\u0026quot; ...} {... \u0026quot;city\u0026quot;: \u0026quot;Chicago\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Gold\u0026quot; ...} 假设请求指定源如下：\n ... \u0026quot;sources\u0026quot;: [ { \u0026quot;marathon_city\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;city\u0026quot; }}}, { \u0026quot;participant_medal\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;place\u0026quot; }}} ], ...  你必须为每个源分配一个唯一的键名。\n 生成的 composite 包含以下分组，按顺序排列：\n{ \u0026quot;city\u0026quot;: \u0026quot;Albuquerque\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Bronze\u0026quot; } { \u0026quot;city\u0026quot;: \u0026quot;Albuquerque\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Gold\u0026quot; } { \u0026quot;city\u0026quot;: \u0026quot;Boston\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Bronze\u0026quot; } { \u0026quot;city\u0026quot;: \u0026quot;Boston\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Silver\u0026quot; } { \u0026quot;city\u0026quot;: \u0026quot;Chicago\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Bronze\u0026quot; } { \u0026quot;city\u0026quot;: \u0026quot;Chicago\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Gold\u0026quot; } { \u0026quot;city\u0026quot;: \u0026quot;Chicago\u0026quot;, \u0026quot;place\u0026quot;: \u0026quot;Silver\u0026quot; } 请注意， city 和 place 字段都是按字母顺序排列的。\n参数说明 #  composite 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     sources 必填 Array 源对象的数组。有效类型为 terms 、 histogram 、 date_histogram 和 geotile_grid 。   size 可选 Numeric 在结果中返回的 composite 分组的数量。默认值为 10 。参见 分页复合结果。   after 可选 String 一个键，用于指定从何处继续显示分页的 composite 分组。参见 分页复合结果。   order 可选 String 对于每个数据源，是否按升序或降序排列值。有效值为 asc 和 desc 。默认值为 asc 。   missing_bucket 可选 Boolean 对于每个数据源，是否包含缺失值文档。默认值为 false 。如果设置为 true ，Easysearch 会包含这些文档，并将 null 作为字段的键。空值在升序排列中排在最前面。     有关聚合特定参数，请参阅相应的聚合文档。\n Terms #  使用 terms 聚合来聚合字符串或布尔数据。更多信息，请参阅 Terms 聚合。\n 您可以使用 terms 源来为任何类型的数据创建复合分组。但是，由于 terms 源为每个唯一值创建分组，因此您通常使用 histogram 源来代替数值数据。\n 以下示例请求返回数据中星期几和客户性别的第一个 4 复合分组：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;composite\u0026quot;: { \u0026quot;sources\u0026quot;: [ { \u0026quot;day\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;day_of_week\u0026quot; }}}, { \u0026quot;gender\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;customer_gender\u0026quot; }}} ], \u0026quot;size\u0026quot;: 4 } } } } 由于此示例的数据集包含每个分组的有效数据，因此聚合会为性别和星期几的每一组合生成一个分组，最终产生 14 个总分组。\n由于请求指定了 size 个 4 ，响应包含前四个复合分组。由于源是 terms ，分组在跨源和源内部都是按升序字母顺序排列的：\n{ \u0026quot;took\u0026quot;: 51, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;after_key\u0026quot;: { \u0026quot;day\u0026quot;: \u0026quot;Monday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot; }, \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: { \u0026quot;day\u0026quot;: \u0026quot;Friday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;FEMALE\u0026quot; }, \u0026quot;doc_count\u0026quot;: 399 }, { \u0026quot;key\u0026quot;: { \u0026quot;day\u0026quot;: \u0026quot;Friday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot; }, \u0026quot;doc_count\u0026quot;: 371 }, { \u0026quot;key\u0026quot;: { \u0026quot;day\u0026quot;: \u0026quot;Monday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;FEMALE\u0026quot; }, \u0026quot;doc_count\u0026quot;: 320 }, { \u0026quot;key\u0026quot;: { \u0026quot;day\u0026quot;: \u0026quot;Monday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot; }, \u0026quot;doc_count\u0026quot;: 259 } ] } } } 您可以使用返回的 after_key 来查看更多结果。请参阅下一节的示例。\n直方图 #  使用 histogram 个数据源创建数值数据的组合聚合。更多信息，请参阅直方图聚合。\n对于 histogram 个数据源，每个 composite 分组键中使用的名称是该键的直方图间隔中的最低值。每个源直方图间隔包含 [lower_bound, lower_bound + interval) 范围内的值。第一个间隔的名称是源字段中的最低值（对于升序值源）。\n以下示例请求根据分组宽度分别为 1 和 50 ，返回数据中数量和基本单位价格的前 6 个组合分组：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;composite\u0026quot;: { \u0026quot;sources\u0026quot;: [ { \u0026quot;quantity\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.quantity\u0026quot;, \u0026quot;interval\u0026quot;: 1 }}}, { \u0026quot;unit_price\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.base_unit_price\u0026quot;, \u0026quot;interval\u0026quot;: 50 }}} ], \u0026quot;size\u0026quot;: 6 } } } } 聚合返回两个 histogram 源的第一个 6 分组键和文档计数。与 terms 示例一样，分组在源字段之间和内部按顺序排列。然而在此情况下，顺序是数值的，基于每个直方图宽度的包含下界：\n{ \u0026quot;took\u0026quot;: 11, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;after_key\u0026quot;: { \u0026quot;quantity\u0026quot;: 2, \u0026quot;unit_price\u0026quot;: 150 }, \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: { \u0026quot;quantity\u0026quot;: 1, \u0026quot;unit_price\u0026quot;: 0 }, \u0026quot;doc_count\u0026quot;: 17691 }, { \u0026quot;key\u0026quot;: { \u0026quot;quantity\u0026quot;: 1, \u0026quot;unit_price\u0026quot;: 50 }, \u0026quot;doc_count\u0026quot;: 5014 }, { \u0026quot;key\u0026quot;: { \u0026quot;quantity\u0026quot;: 1, \u0026quot;unit_price\u0026quot;: 100 }, \u0026quot;doc_count\u0026quot;: 482 }, { \u0026quot;key\u0026quot;: { \u0026quot;quantity\u0026quot;: 1, \u0026quot;unit_price\u0026quot;: 150 }, \u0026quot;doc_count\u0026quot;: 148 }, { \u0026quot;key\u0026quot;: { \u0026quot;quantity\u0026quot;: 1, \u0026quot;unit_price\u0026quot;: 200 }, \u0026quot;doc_count\u0026quot;: 32 }, { \u0026quot;key\u0026quot;: { \u0026quot;quantity\u0026quot;: 2, \u0026quot;unit_price\u0026quot;: 150 }, \u0026quot;doc_count\u0026quot;: 4 } ] } } } 每个字段的分组键是字段区间的下界。例如，第一个 composite 分组的 unit_price 键是 0 。\n要检索下一个 6 分组，请按如下方式将响应中的 after_key 对象提供给 after 参数：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;composite\u0026quot;: { \u0026quot;sources\u0026quot;: [ { \u0026quot;quantity\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.quantity\u0026quot;, \u0026quot;interval\u0026quot;: 1 }}}, { \u0026quot;unit_price\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.base_unit_price\u0026quot;, \u0026quot;interval\u0026quot;: 50 }}} ], \u0026quot;size\u0026quot;: 6, \u0026quot;after\u0026quot;: { \u0026quot;quantity\u0026quot;: 2, \u0026quot;unit_price\u0026quot;: 150 } } } } } 仅剩两个分组：\n{ \u0026quot;took\u0026quot;: 12, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;after_key\u0026quot;: { \u0026quot;quantity\u0026quot;: 2, \u0026quot;unit_price\u0026quot;: 500 }, \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: { \u0026quot;quantity\u0026quot;: 2, \u0026quot;unit_price\u0026quot;: 200 }, \u0026quot;doc_count\u0026quot;: 8 }, { \u0026quot;key\u0026quot;: { \u0026quot;quantity\u0026quot;: 2, \u0026quot;unit_price\u0026quot;: 500 }, \u0026quot;doc_count\u0026quot;: 4 } ] } } } 日期直方图 #\n 要创建日期范围的组合聚合，请使用 date_histogram 聚合。有关详细信息，请参阅日期直方图聚合。\nEasysearch 将日期（包括 date_interval 分组键）表示为 long 表示自 Unix 时间以来的毫秒数的整数。您可以使用 format 参数格式化日期输出。这不会改变键的顺序。\nEasysearch 以 UTC 存储日期和时间。您可以使用 time_zone 参数以不同的时区显示输出结果。\n以下示例请求返回数据中，每个售出产品创建年份的第一组 4 复合分组，以及售出日期，基于分组宽度分别为 1 年和 1 天：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;composite\u0026quot;: { \u0026quot;sources\u0026quot;: [ { \u0026quot;product_creation_date\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.created_on\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;1y\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;yyyy\u0026quot; }}}, { \u0026quot;order_date\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;order_date\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;1d\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;yyyy-MM-dd\u0026quot; }}} ], \u0026quot;size\u0026quot;: 4 } } } } 聚合返回格式化的基于日期的分组键和计数。对于 date_interval 复合聚合，字段排序按日期：\n{ \u0026quot;took\u0026quot;: 21, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;after_key\u0026quot;: { \u0026quot;product_creation_date\u0026quot;: \u0026quot;2016\u0026quot;, \u0026quot;order_date\u0026quot;: \u0026quot;2025-02-23\u0026quot; }, \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: { \u0026quot;product_creation_date\u0026quot;: \u0026quot;2016\u0026quot;, \u0026quot;order_date\u0026quot;: \u0026quot;2025-02-20\u0026quot; }, \u0026quot;doc_count\u0026quot;: 146 }, { \u0026quot;key\u0026quot;: { \u0026quot;product_creation_date\u0026quot;: \u0026quot;2016\u0026quot;, \u0026quot;order_date\u0026quot;: \u0026quot;2025-02-21\u0026quot; }, \u0026quot;doc_count\u0026quot;: 153 }, { \u0026quot;key\u0026quot;: { \u0026quot;product_creation_date\u0026quot;: \u0026quot;2016\u0026quot;, \u0026quot;order_date\u0026quot;: \u0026quot;2025-02-22\u0026quot; }, \u0026quot;doc_count\u0026quot;: 143 }, { \u0026quot;key\u0026quot;: { \u0026quot;product_creation_date\u0026quot;: \u0026quot;2016\u0026quot;, \u0026quot;order_date\u0026quot;: \u0026quot;2025-02-23\u0026quot; }, \u0026quot;doc_count\u0026quot;: 140 } ] } } } 地理网格 #\n 使用 geotile_grid 源将 geo_point 值聚合到代表地图瓦片的分组中。与其他复合聚合源一样，默认情况下，结果仅包括包含数据的分组。有关更多信息，请参阅 Geotile 网格聚合。\n每个单元格对应一个地图瓦片。单元格标签使用 {zoom}/{x}/{y} 格式。\n以下示例请求返回以 8 精度包含 geoip.location 字段中的位置的前 6 个瓦片：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;composite\u0026quot;: { \u0026quot;sources\u0026quot;: [ { \u0026quot;tile\u0026quot;: { \u0026quot;geotile_grid\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;geoip.location\u0026quot;, \u0026quot;precision\u0026quot;: 8 } } } ], \u0026quot;size\u0026quot;: 6 } } } } 聚合返回指定的 geo_tiles 和点计数：\n{ \u0026quot;took\u0026quot;: 3, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;after_key\u0026quot;: { \u0026quot;tile\u0026quot;: \u0026quot;8/122/104\u0026quot; }, \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: { \u0026quot;tile\u0026quot;: \u0026quot;8/43/102\u0026quot; }, \u0026quot;doc_count\u0026quot;: 310 }, { \u0026quot;key\u0026quot;: { \u0026quot;tile\u0026quot;: \u0026quot;8/75/96\u0026quot; }, \u0026quot;doc_count\u0026quot;: 896 }, { \u0026quot;key\u0026quot;: { \u0026quot;tile\u0026quot;: \u0026quot;8/75/124\u0026quot; }, \u0026quot;doc_count\u0026quot;: 178 }, { \u0026quot;key\u0026quot;: { \u0026quot;tile\u0026quot;: \u0026quot;8/122/104\u0026quot; }, \u0026quot;doc_count\u0026quot;: 408 } ] } } } 组合源 #\n 您可以组合两个或多个任何不同类型的来源。\n以下示例请求返回由三种不同来源类型组成的分组：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;composite\u0026quot;: { \u0026quot;sources\u0026quot;: [ { \u0026quot;order_date\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;order_date\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;1M\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;yyyy-MM\u0026quot; }}}, { \u0026quot;gender\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;customer_gender\u0026quot; }}}, { \u0026quot;unit_price\u0026quot;: { \u0026quot;histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.base_unit_price\u0026quot;, \u0026quot;interval\u0026quot;: 200 }}} ], \u0026quot;size\u0026quot;: 10 } } } } 聚合返回混合类型的 composite 分组和文档计数：\n{ \u0026quot;took\u0026quot;: 11, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;after_key\u0026quot;: { \u0026quot;order_date\u0026quot;: \u0026quot;2025-03\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot;, \u0026quot;unit_price\u0026quot;: 200 }, \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: { \u0026quot;order_date\u0026quot;: \u0026quot;2025-02\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;FEMALE\u0026quot;, \u0026quot;unit_price\u0026quot;: 0 }, \u0026quot;doc_count\u0026quot;: 1517 }, { \u0026quot;key\u0026quot;: { \u0026quot;order_date\u0026quot;: \u0026quot;2025-02\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot;, \u0026quot;unit_price\u0026quot;: 0 }, \u0026quot;doc_count\u0026quot;: 1369 }, { \u0026quot;key\u0026quot;: { \u0026quot;order_date\u0026quot;: \u0026quot;2025-02\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot;, \u0026quot;unit_price\u0026quot;: 200 }, \u0026quot;doc_count\u0026quot;: 6 }, { \u0026quot;key\u0026quot;: { \u0026quot;order_date\u0026quot;: \u0026quot;2025-02\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot;, \u0026quot;unit_price\u0026quot;: 400 }, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key\u0026quot;: { \u0026quot;order_date\u0026quot;: \u0026quot;2025-03\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;FEMALE\u0026quot;, \u0026quot;unit_price\u0026quot;: 0 }, \u0026quot;doc_count\u0026quot;: 3656 }, { \u0026quot;key\u0026quot;: { \u0026quot;order_date\u0026quot;: \u0026quot;2025-03\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;FEMALE\u0026quot;, \u0026quot;unit_price\u0026quot;: 200 }, \u0026quot;doc_count\u0026quot;: 1 }, { \u0026quot;key\u0026quot;: { \u0026quot;order_date\u0026quot;: \u0026quot;2025-03\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot;, \u0026quot;unit_price\u0026quot;: 0 }, \u0026quot;doc_count\u0026quot;: 3530 }, { \u0026quot;key\u0026quot;: { \u0026quot;order_date\u0026quot;: \u0026quot;2025-03\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot;, \u0026quot;unit_price\u0026quot;: 200 }, \u0026quot;doc_count\u0026quot;: 7 } ] } } } 子聚合 #  组合聚合在结合子聚合时最为有用，子聚合可以揭示 composite 分组中文档的信息。\n以下示例请求比较了数据中每周每天按性别划分的平均支出：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;composite\u0026quot;: { \u0026quot;sources\u0026quot;: [ { \u0026quot;weekday\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;day_of_week\u0026quot; }}}, { \u0026quot;gender\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;customer_gender\u0026quot; }}} ], \u0026quot;size\u0026quot;: 6 }, \u0026quot;aggs\u0026quot;: { \u0026quot;avg_spend\u0026quot;: { \u0026quot;avg\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot; } } } } } } 该聚合返回前 6 个分组的平均 taxful_total_price 值：\n{ \u0026quot;took\u0026quot;: 30, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;composite_buckets\u0026quot;: { \u0026quot;after_key\u0026quot;: { \u0026quot;weekday\u0026quot;: \u0026quot;Saturday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot; }, \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: { \u0026quot;weekday\u0026quot;: \u0026quot;Friday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;FEMALE\u0026quot; }, \u0026quot;doc_count\u0026quot;: 399, \u0026quot;avg_spend\u0026quot;: { \u0026quot;value\u0026quot;: 71.7733395989975 } }, { \u0026quot;key\u0026quot;: { \u0026quot;weekday\u0026quot;: \u0026quot;Friday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot; }, \u0026quot;doc_count\u0026quot;: 371, \u0026quot;avg_spend\u0026quot;: { \u0026quot;value\u0026quot;: 79.72514108827494 } }, { \u0026quot;key\u0026quot;: { \u0026quot;weekday\u0026quot;: \u0026quot;Monday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;FEMALE\u0026quot; }, \u0026quot;doc_count\u0026quot;: 320, \u0026quot;avg_spend\u0026quot;: { \u0026quot;value\u0026quot;: 72.1588623046875 } }, { \u0026quot;key\u0026quot;: { \u0026quot;weekday\u0026quot;: \u0026quot;Monday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot; }, \u0026quot;doc_count\u0026quot;: 259, \u0026quot;avg_spend\u0026quot;: { \u0026quot;value\u0026quot;: 86.1754946911197 } }, { \u0026quot;key\u0026quot;: { \u0026quot;weekday\u0026quot;: \u0026quot;Saturday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;FEMALE\u0026quot; }, \u0026quot;doc_count\u0026quot;: 365, \u0026quot;avg_spend\u0026quot;: { \u0026quot;value\u0026quot;: 73.53236301369863 } }, { \u0026quot;key\u0026quot;: { \u0026quot;weekday\u0026quot;: \u0026quot;Saturday\u0026quot;, \u0026quot;gender\u0026quot;: \u0026quot;MALE\u0026quot; }, \u0026quot;doc_count\u0026quot;: 371, \u0026quot;avg_spend\u0026quot;: { \u0026quot;value\u0026quot;: 72.78092360175202 } } ] } } } 分页复合结果 #\n 如果请求导致超过 size 个分组，则返回 size 个分组。在这种情况下，结果包含一个 after_key 对象，其中包含列表中下一个分组的键。要检索请求的下一个 size 个分组，请再次发送请求，并在 after 参数中提供 after_key 。例如，请参阅 Histogram 中的请求。\n 始终使用 after_key 继续分页响应，而不是复制最后一个分组。两者有时是不同的。\n 使用索引排序提升性能 #  为了加快在大数据集上的复合聚合速度，你可以使用与聚合源相同的字段和顺序来对索引进行排序。当 index.sort.field 和 index.sort.order 与复合聚合中使用的源字段和顺序相匹配时，Easysearch 可以更高效地返回结果，并减少内存使用。虽然索引排序在索引过程中会带来轻微的开销，但复合聚合的查询性能提升非常显著。\n以下示例请求为 my-sorted-index 索引中的每个字段设置了排序字段和排序顺序：\nPUT /my-sorted-index { \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;sort.field\u0026quot;: [\u0026quot;customer_id\u0026quot;, \u0026quot;timestamp\u0026quot;], \u0026quot;sort.order\u0026quot;: [\u0026quot;asc\u0026quot;, \u0026quot;desc\u0026quot;] } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;customer_id\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;timestamp\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot; }, \u0026quot;price\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot; } } } } 以下请求在 my-sorted-index 索引上创建复合聚合。由于该索引按 customer_id 升序排序，按 timestamp 降序排序，并且聚合源与该排序顺序匹配，因此此查询运行更快，且内存压力减小：\nGET /my-sorted-index/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;my_buckets\u0026quot;: { \u0026quot;composite\u0026quot;: { \u0026quot;size\u0026quot;: 1000, \u0026quot;sources\u0026quot;: [ { \u0026quot;customer\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;customer_id\u0026quot;, \u0026quot;order\u0026quot;: \u0026quot;asc\u0026quot; } } }, { \u0026quot;time\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;1d\u0026quot;, \u0026quot;order\u0026quot;: \u0026quot;desc\u0026quot; } } } ] } } } }   ","subcategory":null,"summary":"","tags":null,"title":"复合聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/composite/"},{"category":null,"content":"基数聚合 #  cardinality 聚合是一种单值度量聚合，用于计算字段的唯一值或不同值的数量。\n基数计数为近似值。有关更多信息，请参阅下面的控制精度 。\n参数说明 #  cardinality 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需的 String 估计基数的字段。   precision_threshold 可选 Numeric 阈值，低于该阈值的计数预计接近准确值。有关更多信息，请参阅控制精度 。   execution_hint 可选 String 如何运行聚合，该参数会影响资源使用和聚合效率。有效值为 ordinals 和 direct 。   missing 可选 与 field 类型相同 用于存储字段缺失实例的 bucket。如果未提供，则忽略缺失值。    参考样例 #  以下示例请求查找数据中唯一产品 ID 的数量：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;unique_products\u0026quot;: { \u0026quot;cardinality\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.product_id\u0026quot; } } } } 返回内容 #  如以下内容所示，聚合返回 unique_products 变量中的基数计数：\n{ \u0026quot;took\u0026quot;: 176, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;unique_products\u0026quot;: { \u0026quot;value\u0026quot;: 7033 } } } 控制精度 #  准确的基数计算需要将所有值加载到哈希集中并返回其大小。这种方法扩展性较差；它可能需要大量内存并导致高延迟。\n您可以使用 precision_threshold 设置来控制内存和准确度之间的权衡。此参数设置一个阈值，低于该阈值的计数预计会接近准确度。高于此值的计数可能会降低准确度。\nprecision_threshold 的默认值为 3000，支持的最大值为 40000。\n基数聚合使用 HyperLogLog++ 算法 。基数计数通常在精度阈值以下非常准确，并且在大多数情况下，即使阈值低至 100，其与真实计数的误差也在 6% 以内。\n预计算哈希 #  对于高基数字符串字段，存储索引字段的哈希值并计算哈希值的基数可以节省计算和内存资源。请谨慎使用此方法；它仅适用于长字符串和/或高基数的集合。数值字段和内存消耗较低的字符串集合最好直接处理。\n示例：控制精度 #  将精度阈值设置为 10000 唯一值：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;unique_products\u0026quot;: { \u0026quot;cardinality\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;products.product_id\u0026quot;, \u0026quot;precision_threshold\u0026quot;: 10000 } } } } 返回内容与使用默认阈值的结果类似，但返回值略有不同。可以调整 precision_threshold 参数，以查看它如何影响基数估计。\n缺省值处理 #  可以为聚合字段的缺失内容分配一个值。有关详细信息，请参阅缺省聚合。\n替换基数聚合中的缺失值会将替换值添加到唯一值列表中，从而将实际基数增加 1。\n","subcategory":null,"summary":"","tags":null,"title":"基数聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/cardinality/"},{"category":null,"content":"地理边界聚合 #  geo_bounds 地理边界聚合是一个多值聚合，用于计算包含一组 geo_point 或 geo_shape 对象的地理边界框。边界框以十进制编码的经纬度（lat-lon）对形式返回，作为矩形的左上角和右下角顶点。\n参数说明 #  geo_bounds 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 计算地理边界所使用的包含地理点或地理形状的字段的名称。   wrap_longitude 可选 Boolean 是否允许边界框与国际日期变更线重叠。默认值为 true 。    参考样例 #  以下示例查询数据中每个订单的 geo_bounds 的 geoip.location （每个 geoip.location 是一个地理点）：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;geo\u0026quot;: { \u0026quot;geo_bounds\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;geoip.location\u0026quot; } } } } 返回内容 #\n 如以下示例响应所示，聚合返回包含 geoip.location 字段中所有地理点的 geobounds ：\n{ \u0026quot;took\u0026quot;: 16, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;geo\u0026quot;: { \u0026quot;bounds\u0026quot;: { \u0026quot;top_left\u0026quot;: { \u0026quot;lat\u0026quot;: 52.49999997206032, \u0026quot;lon\u0026quot;: -118.20000001229346 }, \u0026quot;bottom_right\u0026quot;: { \u0026quot;lat\u0026quot;: 4.599999985657632, \u0026quot;lon\u0026quot;: 55.299999956041574 } } } } } ","subcategory":null,"summary":"","tags":null,"title":"地理边界聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/geobounds/"},{"category":null,"content":"地理距离聚合 #  geo_distance 地理距离聚合根据与一个起始 geo_point 字段距离将文档分组到同心圆中。它与 range 聚合相同，只是它作用于地理位置。\n例如，你可以使用 geo_distance 聚合来查找你 1 公里范围内的所有披萨店。搜索结果仅限于你指定的 1 公里半径范围内，但你还可以添加另一个在 2 公里范围内找到的结果。\n您只能对映射为 geo_point 的字段使用 geo_distance 聚合。\n点是一个单一的地理坐标，例如您智能手机显示的当前位置。在 OpenSearch 中，点表示如下：\n{ \u0026quot;location\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;point\u0026quot;, \u0026quot;coordinates\u0026quot;: { \u0026quot;lat\u0026quot;: 83.76, \u0026quot;lon\u0026quot;: -81.2 } } } 您还可以将纬度和经度指定为数组 [-81.20, 83.76] 或字符串 \u0026quot;83.76, -81.20\u0026quot;\n此表列出了 geo_distance 聚合的相关字段：\n   字段 必需/可选 描述     field 必需 指定您要处理的地理点字段。   origin 必需 指定用于计算距离的地理点。   ranges 必需 指定一组范围，根据文档与目标点的距离收集文档。   unit 可选 定义 ranges 数组中使用的单位。 unit 默认为 m （米），但你可以切换到其他单位，如 km （千米）、 mi （英里）、 in （英寸）、 yd （码）、 cm （厘米）和 mm （毫米）。   distance_type 可选 指定 OpenSearch 如何计算距离。默认值为 sloppy_arc （更快但精度较低），也可以设置为 arc （较慢但最精确）或 plane （最快但精度最低）。由于误差范围较大，仅适用于小地理区域使用 plane 。    语法如下：\n{ \u0026quot;aggs\u0026quot;: { \u0026quot;aggregation_name\u0026quot;: { \u0026quot;geo_distance\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;field_1\u0026quot;, \u0026quot;origin\u0026quot;: \u0026quot;x, y\u0026quot;, \u0026quot;ranges\u0026quot;: [ { \u0026quot;to\u0026quot;: \u0026quot;value_1\u0026quot; }, { \u0026quot;from\u0026quot;: \u0026quot;value_2\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;value_3\u0026quot; }, { \u0026quot;from\u0026quot;: \u0026quot;value_4\u0026quot; } ] } } } } 这个示例根据与 geo-point 字段的以下距离形成分组：\n 少于 10 公里 从 10 到 20 公里 从 20 到 50 公里 从 50 到 100 公里 超过 100 公里  GET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;position\u0026quot;: { \u0026quot;geo_distance\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;geo.coordinates\u0026quot;, \u0026quot;origin\u0026quot;: { \u0026quot;lat\u0026quot;: 83.76, \u0026quot;lon\u0026quot;: -81.2 }, \u0026quot;ranges\u0026quot;: [ { \u0026quot;to\u0026quot;: 10 }, { \u0026quot;from\u0026quot;: 10, \u0026quot;to\u0026quot;: 20 }, { \u0026quot;from\u0026quot;: 20, \u0026quot;to\u0026quot;: 50 }, { \u0026quot;from\u0026quot;: 50, \u0026quot;to\u0026quot;: 100 }, { \u0026quot;from\u0026quot;: 100 } ] } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;position\u0026quot; : { \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;*-10.0\u0026quot;, \u0026quot;from\u0026quot; : 0.0, \u0026quot;to\u0026quot; : 10.0, \u0026quot;doc_count\u0026quot; : 0 }, { \u0026quot;key\u0026quot; : \u0026quot;10.0-20.0\u0026quot;, \u0026quot;from\u0026quot; : 10.0, \u0026quot;to\u0026quot; : 20.0, \u0026quot;doc_count\u0026quot; : 0 }, { \u0026quot;key\u0026quot; : \u0026quot;20.0-50.0\u0026quot;, \u0026quot;from\u0026quot; : 20.0, \u0026quot;to\u0026quot; : 50.0, \u0026quot;doc_count\u0026quot; : 0 }, { \u0026quot;key\u0026quot; : \u0026quot;50.0-100.0\u0026quot;, \u0026quot;from\u0026quot; : 50.0, \u0026quot;to\u0026quot; : 100.0, \u0026quot;doc_count\u0026quot; : 0 }, { \u0026quot;key\u0026quot; : \u0026quot;100.0-*\u0026quot;, \u0026quot;from\u0026quot; : 100.0, \u0026quot;doc_count\u0026quot; : 14074 } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"地理距离聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/geo-distance/"},{"category":null,"content":"地理中心点 #  geo_centroid 地理中心点的聚合计算一组 geo_point 值的地理中心或焦点。它将中心位置作为纬度-经度对返回。\n参数说明 #  geo_centroid 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 包含计算中心的地理点的字段的名称。    参考样例 #  以下示例返回数据中每个订单的 geo_centroid 的 geoip.location 。每个 geoip.location 都是一个地理点：\nGET /sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;centroid\u0026quot;: { \u0026quot;geo_centroid\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;geoip.location\u0026quot; } } } } 返回内容 #  返回内容包括一个 centroid 对象，该对象具有 lat 和 lon 属性，表示所有索引数据点的中心位置：\n{ \u0026quot;took\u0026quot;: 35, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;centroid\u0026quot;: { \u0026quot;location\u0026quot;: { \u0026quot;lat\u0026quot;: 35.54990372113027, \u0026quot;lon\u0026quot;: -9.079764742533712 }, \u0026quot;count\u0026quot;: 4675 } } } 中心位置位于摩洛哥以北约的大西洋中。鉴于数据库中订单的广泛地理分布，这并不是很有意义。\n嵌套在其他聚合下使用 #  您可以将 geo_centroid 聚合嵌套在 bucket 聚合中，以计算数据子集的中心。\n示例：嵌套在分组聚合下 #  您可以在字符串字段的 terms 分组下嵌套 geo_centroid 聚合。\n要找到每个大洲的订单的 geoip 中心位置，请在 geoip.continent_name 字段内对中心进行子聚合：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;continents\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;geoip.continent_name\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;centroid\u0026quot;: { \u0026quot;geo_centroid\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;geoip.location\u0026quot; } } } } } } 这将返回每个大陆分组的中心位置：\n{ \u0026quot;took\u0026quot;: 34, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 4675, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;continents\u0026quot;: { \u0026quot;doc_count_error_upper_bound\u0026quot;: 0, \u0026quot;sum_other_doc_count\u0026quot;: 0, \u0026quot;buckets\u0026quot;: [ { \u0026quot;key\u0026quot;: \u0026quot;Asia\u0026quot;, \u0026quot;doc_count\u0026quot;: 1220, \u0026quot;centroid\u0026quot;: { \u0026quot;location\u0026quot;: { \u0026quot;lat\u0026quot;: 28.023606536509163, \u0026quot;lon\u0026quot;: 47.83377046025068 }, \u0026quot;count\u0026quot;: 1220 } }, { \u0026quot;key\u0026quot;: \u0026quot;North America\u0026quot;, \u0026quot;doc_count\u0026quot;: 1206, \u0026quot;centroid\u0026quot;: { \u0026quot;location\u0026quot;: { \u0026quot;lat\u0026quot;: 39.06542286878007, \u0026quot;lon\u0026quot;: -85.36152573149485 }, \u0026quot;count\u0026quot;: 1206 } }, { \u0026quot;key\u0026quot;: \u0026quot;Europe\u0026quot;, \u0026quot;doc_count\u0026quot;: 1172, \u0026quot;centroid\u0026quot;: { \u0026quot;location\u0026quot;: { \u0026quot;lat\u0026quot;: 48.125767892293325, \u0026quot;lon\u0026quot;: 2.7529009746915243 }, \u0026quot;count\u0026quot;: 1172 } }, { \u0026quot;key\u0026quot;: \u0026quot;Africa\u0026quot;, \u0026quot;doc_count\u0026quot;: 899, \u0026quot;centroid\u0026quot;: { \u0026quot;location\u0026quot;: { \u0026quot;lat\u0026quot;: 30.780756367941297, \u0026quot;lon\u0026quot;: 13.464182392125318 }, \u0026quot;count\u0026quot;: 899 } }, { \u0026quot;key\u0026quot;: \u0026quot;South America\u0026quot;, \u0026quot;doc_count\u0026quot;: 178, \u0026quot;centroid\u0026quot;: { \u0026quot;location\u0026quot;: { \u0026quot;lat\u0026quot;: 4.599999985657632, \u0026quot;lon\u0026quot;: -74.10000007599592 }, \u0026quot;count\u0026quot;: 178 } } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"地理中心点","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/geocentroid/"},{"category":null,"content":"唯一分词过滤器 #  唯一(unique)分词过滤器可确保在分词过程中仅保留唯一的词元，它会去除在单个字段或文本块中出现的重复词元。\n参数说明 #  唯一分词过滤器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     only_on_same_position 可选 布尔值 如果设置为 true，该分词过滤器将充当去重分词过滤器，仅移除位于相同位置的词元。默认值为 false。    参考样例 #  以下示例请求创建了一个名为 unique_example 的新索引，并配置了一个带有唯一过滤器的分词器。\nPUT /unique_example { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;unique_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;unique\u0026quot;, \u0026quot;only_on_same_position\u0026quot;: false } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;unique_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;unique_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /unique_example/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;unique_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch Easysearch is powerful powerful and scalable\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;, \u0026quot;start_offset\u0026quot;: 22, \u0026quot;end_offset\u0026quot;: 24, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;powerful\u0026quot;, \u0026quot;start_offset\u0026quot;: 25, \u0026quot;end_offset\u0026quot;: 33, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;and\u0026quot;, \u0026quot;start_offset\u0026quot;: 43, \u0026quot;end_offset\u0026quot;: 46, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;scalable\u0026quot;, \u0026quot;start_offset\u0026quot;: 47, \u0026quot;end_offset\u0026quot;: 55, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"唯一分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/unique/"},{"category":null,"content":"同义词图分词过滤器 #  同义词图(synonym_graph)分词过滤器是同义词分词过滤器的更高级版本。它支持多词同义词，并能跨多个词元处理同义词，这使得它非常适合处理短语或者那些词元之间关系很重要的场景。\n参数说明 #  同义词图分词过滤器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。   synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。   lenient 可选 布尔值 在加载规则配置时是否忽略异常。默认值为 false。   format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：\n- solr\n- wordnet\n默认值为 solr。   expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。\n例如：\n若同义词定义为 “quick, fast” 且 expand 设置为 true，则同义词规则配置如下：\n- quick 映射为 quick\n- quick 映射为 fast\n- fast 映射为 quick\n- fast 映射为 fast\n若 expand 设置为 false，则同义词规则配置如下：\n- quick 映射为 quick\n- fast 映射为 quick    参考样例： Solr 格式 #  以下示例请求创建了一个名为 my-index 的新索引，并配置了一个带有同义词图过滤器的分词器。该过滤器采用默认的 Solr 规则格式进行配置。\nPUT /my-car-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_synonym_graph_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym_graph\u0026quot;, \u0026quot;synonyms\u0026quot;: [ \u0026quot;sports car, race car\u0026quot;, \u0026quot;fast car, speedy vehicle\u0026quot;, \u0026quot;luxury car, premium vehicle\u0026quot;, \u0026quot;electric car, EV\u0026quot; ] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_synonym_graph_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_synonym_graph_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-car-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_synonym_graph_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;I just bought a sports car and it is a fast car.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ {\u0026quot;token\u0026quot;: \u0026quot;i\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 1,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;just\u0026quot;,\u0026quot;start_offset\u0026quot;: 2,\u0026quot;end_offset\u0026quot;: 6,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 1}, {\u0026quot;token\u0026quot;: \u0026quot;bought\u0026quot;,\u0026quot;start_offset\u0026quot;: 7,\u0026quot;end_offset\u0026quot;: 13,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;a\u0026quot;,\u0026quot;start_offset\u0026quot;: 14,\u0026quot;end_offset\u0026quot;: 15,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;race\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;sports\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 22,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 4,\u0026quot;positionLength\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 5,\u0026quot;positionLength\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;,\u0026quot;start_offset\u0026quot;: 23,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 6}, {\u0026quot;token\u0026quot;: \u0026quot;and\u0026quot;,\u0026quot;start_offset\u0026quot;: 27,\u0026quot;end_offset\u0026quot;: 30,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 7}, {\u0026quot;token\u0026quot;: \u0026quot;it\u0026quot;,\u0026quot;start_offset\u0026quot;: 31,\u0026quot;end_offset\u0026quot;: 33,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 8}, {\u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;,\u0026quot;start_offset\u0026quot;: 34,\u0026quot;end_offset\u0026quot;: 36,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 9}, {\u0026quot;token\u0026quot;: \u0026quot;a\u0026quot;,\u0026quot;start_offset\u0026quot;: 37,\u0026quot;end_offset\u0026quot;: 38,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 10}, {\u0026quot;token\u0026quot;: \u0026quot;speedy\u0026quot;,\u0026quot;start_offset\u0026quot;: 39,\u0026quot;end_offset\u0026quot;: 47,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 11}, {\u0026quot;token\u0026quot;: \u0026quot;fast\u0026quot;,\u0026quot;start_offset\u0026quot;: 39,\u0026quot;end_offset\u0026quot;: 43,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 11,\u0026quot;positionLength\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;vehicle\u0026quot;,\u0026quot;start_offset\u0026quot;: 39,\u0026quot;end_offset\u0026quot;: 47,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 12,\u0026quot;positionLength\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;,\u0026quot;start_offset\u0026quot;: 44,\u0026quot;end_offset\u0026quot;: 47,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 13} ] } 参考样例： WordNet 格式 #  以下示例请求创建了一个名为 my-wordnet-index 的新索引，并配置了一个带有同义词图过滤器的分词器。该过滤器采用 WordNet 规则格式进行配置。\nPUT /my-wordnet-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_synonym_graph_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym_graph\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;wordnet\u0026quot;, \u0026quot;synonyms\u0026quot;: [ \u0026quot;s(100000001, 1, 'sports car', n, 1, 0).\u0026quot;, \u0026quot;s(100000001, 2, 'race car', n, 1, 0).\u0026quot;, \u0026quot;s(100000001, 3, 'fast car', n, 1, 0).\u0026quot;, \u0026quot;s(100000001, 4, 'speedy vehicle', n, 1, 0).\u0026quot; ] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_synonym_graph_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_synonym_graph_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-wordnet-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_synonym_graph_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;I just bought a sports car and it is a fast car.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ {\u0026quot;token\u0026quot;: \u0026quot;i\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 1,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;just\u0026quot;,\u0026quot;start_offset\u0026quot;: 2,\u0026quot;end_offset\u0026quot;: 6,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 1}, {\u0026quot;token\u0026quot;: \u0026quot;bought\u0026quot;,\u0026quot;start_offset\u0026quot;: 7,\u0026quot;end_offset\u0026quot;: 13,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;a\u0026quot;,\u0026quot;start_offset\u0026quot;: 14,\u0026quot;end_offset\u0026quot;: 15,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;race\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;fast\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 4,\u0026quot;positionLength\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;speedy\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 4,\u0026quot;positionLength\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;sports\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 22,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 4,\u0026quot;positionLength\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 5,\u0026quot;positionLength\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 6,\u0026quot;positionLength\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;vehicle\u0026quot;,\u0026quot;start_offset\u0026quot;: 16,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 7,\u0026quot;positionLength\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;,\u0026quot;start_offset\u0026quot;: 23,\u0026quot;end_offset\u0026quot;: 26,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 8}, {\u0026quot;token\u0026quot;: \u0026quot;and\u0026quot;,\u0026quot;start_offset\u0026quot;: 27,\u0026quot;end_offset\u0026quot;: 30,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 9}, {\u0026quot;token\u0026quot;: \u0026quot;it\u0026quot;,\u0026quot;start_offset\u0026quot;: 31,\u0026quot;end_offset\u0026quot;: 33,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 10}, {\u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;,\u0026quot;start_offset\u0026quot;: 34,\u0026quot;end_offset\u0026quot;: 36,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 11}, {\u0026quot;token\u0026quot;: \u0026quot;a\u0026quot;,\u0026quot;start_offset\u0026quot;: 37,\u0026quot;end_offset\u0026quot;: 38,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 12}, {\u0026quot;token\u0026quot;: \u0026quot;sports\u0026quot;,\u0026quot;start_offset\u0026quot;: 39,\u0026quot;end_offset\u0026quot;: 47,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 13}, {\u0026quot;token\u0026quot;: \u0026quot;race\u0026quot;,\u0026quot;start_offset\u0026quot;: 39,\u0026quot;end_offset\u0026quot;: 47,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 13,\u0026quot;positionLength\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;speedy\u0026quot;,\u0026quot;start_offset\u0026quot;: 39,\u0026quot;end_offset\u0026quot;: 47,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 13,\u0026quot;positionLength\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;fast\u0026quot;,\u0026quot;start_offset\u0026quot;: 39,\u0026quot;end_offset\u0026quot;: 43,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 13,\u0026quot;positionLength\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;,\u0026quot;start_offset\u0026quot;: 39,\u0026quot;end_offset\u0026quot;: 47,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 14,\u0026quot;positionLength\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;,\u0026quot;start_offset\u0026quot;: 39,\u0026quot;end_offset\u0026quot;: 47,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 15,\u0026quot;positionLength\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;vehicle\u0026quot;,\u0026quot;start_offset\u0026quot;: 39,\u0026quot;end_offset\u0026quot;: 47,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 16,\u0026quot;positionLength\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;,\u0026quot;start_offset\u0026quot;: 44,\u0026quot;end_offset\u0026quot;: 47,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 17} ] } ","subcategory":null,"summary":"","tags":null,"title":"同义词图分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/synonym-graph/"},{"category":null,"content":"同义词分词过滤器 #  同义词(synonym)分词过滤器允许将多个术语映射到单个术语，或在单词之间创建等价组，从而提高搜索的灵活性。\n参数说明 #  同义词分词过滤器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     synonyms 必须指定 synonyms 或 synonyms_path 字符串 在配置中直接定义的同义词规则列表。   synonyms_path 必须指定 synonyms 或 synonyms_path 字符串 包含同义词规则的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。   lenient 可选 布尔值 加载规则配置时是否忽略异常。默认值为 false。   format 可选 字符串 指定用于确定 Easysearch 如何定义和解释同义词的格式。有效值为：\n- solr\n- wordnet\n默认值为 solr。   expand 可选 布尔值 是否扩展等效的同义词规则。默认值为 true。\n例如：\n如果同义词定义为 \u0026quot;quick, fast\u0026quot; 且 expand 设置为 true，则同义词规则配置如下：\n- quick =\u0026gt; quick\n- quick =\u0026gt; fast\n- fast =\u0026gt; quick\n- fast =\u0026gt; fast\n如果 expand 设置为 false，则同义词规则配置如下：\n- quick =\u0026gt; quick\n- fast =\u0026gt; quick    参考样例： Solr 格式 #  以下示例请求创建了一个名为 my-synonym-index 的新索引，并配置了一个带有同义词过滤器的分词器。该过滤器使用默认的 Solr 规则格式进行配置。\nPUT /my-synonym-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_synonym_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym\u0026quot;, \u0026quot;synonyms\u0026quot;: [ \u0026quot;car, automobile\u0026quot;, \u0026quot;quick, fast, speedy\u0026quot;, \u0026quot;laptop =\u0026gt; computer\u0026quot; ] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_synonym_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_synonym_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-synonym-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_synonym_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;The quick dog jumps into the car with a laptop\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;the\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 3, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;quick\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 9, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;fast\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 9, \u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;speedy\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 9, \u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;dog\u0026quot;, \u0026quot;start_offset\u0026quot;: 10, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;jumps\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;into\u0026quot;, \u0026quot;start_offset\u0026quot;: 20, \u0026quot;end_offset\u0026quot;: 24, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;the\u0026quot;, \u0026quot;start_offset\u0026quot;: 25, \u0026quot;end_offset\u0026quot;: 28, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 }, { \u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;, \u0026quot;start_offset\u0026quot;: 29, \u0026quot;end_offset\u0026quot;: 32, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 6 }, { \u0026quot;token\u0026quot;: \u0026quot;automobile\u0026quot;, \u0026quot;start_offset\u0026quot;: 29, \u0026quot;end_offset\u0026quot;: 32, \u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;, \u0026quot;position\u0026quot;: 6 }, { \u0026quot;token\u0026quot;: \u0026quot;with\u0026quot;, \u0026quot;start_offset\u0026quot;: 33, \u0026quot;end_offset\u0026quot;: 37, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 7 }, { \u0026quot;token\u0026quot;: \u0026quot;a\u0026quot;, \u0026quot;start_offset\u0026quot;: 38, \u0026quot;end_offset\u0026quot;: 39, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 8 }, { \u0026quot;token\u0026quot;: \u0026quot;computer\u0026quot;, \u0026quot;start_offset\u0026quot;: 40, \u0026quot;end_offset\u0026quot;: 46, \u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;, \u0026quot;position\u0026quot;: 9 } ] } 参考样例：WordNet 格式 #  以下示例请求创建了一个名为 my-wordnet-index 的新索引，并配置了一个带有同义词过滤器的分词器。该过滤器是按照 WordNet 规则格式配置的。\nPUT /my-wordnet-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_wordnet_synonym_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;wordnet\u0026quot;, \u0026quot;synonyms\u0026quot;: [ \u0026quot;s(100000001,1,'fast',v,1,0).\u0026quot;, \u0026quot;s(100000001,2,'quick',v,1,0).\u0026quot;, \u0026quot;s(100000001,3,'swift',v,1,0).\u0026quot; ] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_wordnet_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_wordnet_synonym_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-wordnet-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_wordnet_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;I have a fast car\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;i\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 1, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;have\u0026quot;, \u0026quot;start_offset\u0026quot;: 2, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;a\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;fast\u0026quot;, \u0026quot;start_offset\u0026quot;: 9, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;quick\u0026quot;, \u0026quot;start_offset\u0026quot;: 9, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;swift\u0026quot;, \u0026quot;start_offset\u0026quot;: 9, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"同义词分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/synonym/"},{"category":null,"content":"各类语种分词器 #  Easysearch 在分词器选项中支持以下语种：阿拉伯语（arabic）、亚美尼亚语（armenian）、巴斯克语（basque）、孟加拉语（bengali）、巴西葡萄牙语（brazilian）、保加利亚语（bulgarian）、加泰罗尼亚语（catalan）、捷克语（czech）、丹麦语（danish）、荷兰语（dutch）、英语（english）、爱沙尼亚语（estonian）、芬兰语（finnish）、法语（french）、加利西亚语（galician）、德语（german）、希腊语（greek）、印地语（hindi）、匈牙利语（hungarian）、印度尼西亚语（indonesian）、爱尔兰语（irish）、意大利语（italian）、拉脱维亚语（latvian）、立陶宛语（lithuanian）、挪威语（norwegian）、波斯语（persian）、葡萄牙语（portuguese）、罗马尼亚语（romanian）、俄语（russian）、索拉尼语（sorani）、西班牙语（spanish）、瑞典语（swedish）、土耳其语（turkish）以及泰语（thai）。\n当你映射索引时要使用该分词器，需在查询中指定相应的值。例如，要使用法语语种分词器来映射您的索引，为分词器字段指定值 french：\n \u0026quot;analyzer\u0026quot;: \u0026quot;french\u0026quot; 参考样例 #  以下请求指定了一个名为 my-index 的索引，其中 content 字段被配置为多字段，并且一个名为 french 的子字段配置了french语种分词器\nPUT my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;fields\u0026quot;: { \u0026quot;french\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;french\u0026quot; } } } } } } 也可以使用以下查询为整个索引配置默认的french分词器：\nPUT my-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;french\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;title\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } } } ","subcategory":null,"summary":"","tags":null,"title":"各类语种分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/language-analyzer/"},{"category":null,"content":"反转分词过滤器 #  反转（reverse）分词过滤器会反转每个词元中字符的顺序，这样在分析过程中，后缀信息就会位于反转后词元的开头。\n这对于基于后缀的搜索很有用：\n反转分词过滤器在你需要进行基于后缀的搜索时很有帮助，例如以下场景：\n 后缀匹配：根据单词的后缀来搜索单词，比如识别以特定后缀结尾的单词（例如 -tion 或 -ing）。 文件扩展名搜索：通过文件的扩展名来搜索文件，例如 .txt 或 .jpg。 自定义排序或排名：通过反转词元，你可以基于后缀实现独特的排序或排名逻辑。 后缀自动补全：实现基于后缀而非前缀的自动补全建议。  参考说明 #  以下示例请求创建了一个名为 my-reverse-index 的新索引，并配置了一个带有反转过滤器的分词器。\nPUT /my-reverse-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;reverse_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;reverse\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_reverse_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;reverse_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-reverse-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_reverse_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;hello world\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;olleh\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 5, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;dlrow\u0026quot;, \u0026quot;start_offset\u0026quot;: 6, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"反转分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/reverse/"},{"category":null,"content":"反向嵌套聚合 #  您可以将嵌套文档中的值聚合到其父文档中；这种聚合称为 reverse_nested 。您可以使用 reverse_nested 在按嵌套对象中的字段分组后，聚合父文档中的字段。 reverse_nested 聚合将“连接回”根页面，并为您的各种变体获取 load_time 。\nreverse_nested 聚合是嵌套聚合中的一个子聚合。它接受一个名为 path 的选项。此选项定义 Easysearch 在计算聚合时在文档层次结构中向后退多少步。\nGET logs/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;response\u0026quot;: \u0026quot;200\u0026quot; } }, \u0026quot;aggs\u0026quot;: { \u0026quot;pages\u0026quot;: { \u0026quot;nested\u0026quot;: { \u0026quot;path\u0026quot;: \u0026quot;pages\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;top_pages_per_load_time\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;pages.load_time\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;comment_to_logs\u0026quot;: { \u0026quot;reverse_nested\u0026quot;: {}, \u0026quot;aggs\u0026quot;: { \u0026quot;min_load_time\u0026quot;: { \u0026quot;min\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;pages.load_time\u0026quot; } } } } } } } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;pages\u0026quot; : { \u0026quot;doc_count\u0026quot; : 2, \u0026quot;top_pages_per_load_time\u0026quot; : { \u0026quot;doc_count_error_upper_bound\u0026quot; : 0, \u0026quot;sum_other_doc_count\u0026quot; : 0, \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : 200.0, \u0026quot;doc_count\u0026quot; : 1, \u0026quot;comment_to_logs\u0026quot; : { \u0026quot;doc_count\u0026quot; : 1, \u0026quot;min_load_time\u0026quot; : { \u0026quot;value\u0026quot; : null } } }, { \u0026quot;key\u0026quot; : 500.0, \u0026quot;doc_count\u0026quot; : 1, \u0026quot;comment_to_logs\u0026quot; : { \u0026quot;doc_count\u0026quot; : 1, \u0026quot;min_load_time\u0026quot; : { \u0026quot;value\u0026quot; : null } } } ] } } } } 返回内容显示日志索引有一页带有 load_time 200，还有一页带有 load_time 500。\n","subcategory":null,"summary":"","tags":null,"title":"反向嵌套聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/reverse-nested/"},{"category":null,"content":"去重分词过滤器 #  去重（remove_duplicates）分词过滤器用于去除在分词过程中在相同位置生成的重复词元。\n参考样例 #  以下示例请求创建了一个带有 keyword_repeat（关键词重复）分词过滤器的索引。该过滤器会在每个词元自身所在的相同位置添加该词元的关键词版本，然后使用 kstem（K 词干提取）过滤器来创建该词元的词干形式。\nPUT /example-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;keyword_repeat\u0026quot;, \u0026quot;kstem\u0026quot; ] } } } } } 使用以下请求来分析字符串 “Slower turtle”。\nGET /example-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Slower turtle\u0026quot; } 返回内容中在同一位置包含了两次词元 “turtle”。\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;slower\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;slow\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } 可以通过在索引设置中添加一个去重分词过滤器来移除重复的词元。\nPUT /index-remove-duplicate { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;keyword_repeat\u0026quot;, \u0026quot;kstem\u0026quot;, \u0026quot;remove_duplicates\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /index-remove-duplicate/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Slower turtle\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;slower\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;slow\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"去重分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/remove-duplicates/"},{"category":null,"content":"单词分隔符图分词过滤器 #  单词分隔符图(word_delimiter_graph)分词过滤器用于依据预定义的字符对词元进行拆分，还能基于可定制的规则对词元进行可选的规范化处理。\n 单词分隔符图分词过滤器可用于去除零件编号或产品 ID 这类复杂标识符中的标点符号。在这种情况下，它最好与关键字分词器搭配使用。对于带有连字符的单词，建议使用同义词图分词过滤器而非单词分隔符图分词过滤器，因为用户在搜索这些词时，往往既会搜索带连字符的形式，也会搜索不带连字符的形式。\n 默认情况下，该过滤器会应用以下规则：\n   描述 输入 输出     将非字母数字字符视为分隔符 ultra-fast ultra, fast   去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder   当字母大小写发生转换时拆分词元 Easysearch Easy, search   当字母和数字之间发生转换时拆分词元 T1000 T, 1000   去除词元末尾的所有格形式（\u0026lsquo;s） John's John     重要的是，不要将此过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并且会影响诸如 catenate_all 或 preserve_original 之类的选项。我们建议将此过滤器与关键字(keyword)分词器或空白(whitespace)分词器搭配使用。\n 参数说明 #  你可以使用以下参数来配置单词分隔符图分词过滤器。\n   参数 必需/可选 数据类型 描述     adjust_offsets 可选 布尔值 决定是否要为拆分或连接后的词元重新计算词元偏移量。若为 true，过滤器会调整词元偏移量，以准确呈现词元在词元流中的位置。这种调整能确保词元在文本中的位置与处理后的修改形式相匹配，这对高亮显示或短语查询等应用特别有用。若为 false，偏移量保持不变，这可能会导致处理后的词元映射回原始文本位置时出现错位。如果你的分词器使用了像 trim 这类会改变词元长度但不改变偏移量的过滤器，建议将此参数设为 false。默认值为 true。   catenate_all 可选 布尔值 从一系列字母数字部分生成连接后的词元。例如，quick-fast-200 会变成 [ quickfast200, quick, fast, 200 ]。默认值为 false。   catenate_numbers 可选 布尔值 连接数字序列。例如，10-20-30 会变成 [ 102030, 10, 20, 30 ]。默认值为 false。   catenate_words 可选 布尔值 连接字母单词。例如，high-speed-level 会变成 [ highspeedlevel, high, speed, level ]。默认值为 false。   generate_number_parts 可选 布尔值 若为 true，输出中会包含纯数字词元（仅由数字组成的词元）。默认值为 true。   generate_word_parts 可选 布尔值 若为 true，输出中会包含纯字母词元（仅由字母字符组成的词元）。默认值为 true。   ignore_keywords 可选 布尔值 是否处理标记为关键字的词元。默认值为 false。   preserve_original 可选 布尔值 在输出中，除了生成的词元外，还保留原始词元（可能包含非字母数字分隔符）。例如，auto-drive-300 会变成 [ auto - drive - 300, auto, drive, 300 ]。如果为 true，该过滤器会生成索引不支持的多位置词元，因此请勿在索引分词器中使用此过滤器，或者在该过滤器之后使用 flatten_graph 过滤器。默认值为 false。   protected_words 可选 字符串数组 指定不应被拆分的词元。   protected_words_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含不应被分隔的词元，词元需分行列出。   split_on_case_change 可选 布尔值 在连续字母大小写不同（一个为小写，另一个为大写）的位置拆分词元。例如，“EasySearch” 会变成 [ Easy, Search ]。默认值为 true。   split_on_numerics 可选 布尔值 在连续字母和数字的位置拆分词元。例如，v8engine 会变成 [ v, 8, engine ]。默认值为 true。   stem_english_possessive 可选 布尔值 去除英语所有格结尾，如 \u0026lsquo;s。默认值为 true。   type_table 可选 字符串数组 一个自定义映射，用于指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。例如，要将连字符（-）视为字母数字字符，可指定 [\u0026quot;- =\u0026gt; ALPHA\u0026quot;]，这样单词就不会在连字符处拆分。有效类型有：\n- ALPHA：字母\n- ALPHANUM：字母数字\n- DIGIT：数字\n- LOWER：小写字母\n- SUBWORD_DELIM：非字母数字分隔符\n- UPPER：大写字母   type_table_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含自定义字符映射。该映射指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。有关有效类型，请参阅 type_table。    参考样例 #  以下示例请求创建了一个名为 my-custom-index 的新索引，并配置了一个带有单词分隔符图过滤器（word_delimiter_graph）的分词器。\nPUT /my-custom-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;custom_word_delimiter_filter\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;custom_word_delimiter_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;word_delimiter_graph\u0026quot;, \u0026quot;split_on_case_change\u0026quot;: true, \u0026quot;split_on_numerics\u0026quot;: true, \u0026quot;stem_english_possessive\u0026quot;: true } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-custom-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;FastCar's Model2023\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Fast\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;Car\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;Model\u0026quot;, \u0026quot;start_offset\u0026quot;: 10, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;2023\u0026quot;, \u0026quot;start_offset\u0026quot;: 15, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 3 } ] } 单词分隔符图过滤器（word_delimiter_graph）和单词分隔符过滤器（word_delimiter）的区别 #  当以下任何参数设置为 true 时，单词分隔符图过滤器和单词分隔符分词过滤器都会生成跨越多个位置的词元：\n catenate_all catenate_numbers catenate_words preserve_original  为了说明这两种过滤器的区别，我们以输入文本 Pro-XT500 为例。\n单词分隔符图过滤器（word_delimiter_graph） #  单词分隔符图过滤器会为多位置词元分配一个 positionLength 属性，该属性表明一个词元跨越了多少个位置。这确保了该过滤器始终能生成有效的词元图，使其适用于高级词元图场景。虽然带有多位置词元的词元图不支持用于索引，但它们在搜索场景中仍然很有用。例如，像 match_phrase 这样的查询可以使用这些图从单个输入字符串生成多个子查询。对于示例输入文本，单词分隔符图过滤器会生成以下词元：\n Pro（位置 1） XT500（位置 2） ProXT500（位置 1，positionLength：2）  positionLength 属性使得生成的图可用于高级查询。\n单词分隔符过滤器（word_delimiter） #  相比之下，单词分隔符过滤器不会为多位置词元分配 positionLength 属性，当存在这些词元时，会导致生成无效的图。对于示例输入文本，单词分隔符过滤器会生成以下词元：\n Pro（位置 1） XT500（位置 2） ProXT500（位置 1，无 positionLength）  缺少 positionLength 属性会导致包含多位置词元的词元流所生成的词元图无效。\n","subcategory":null,"summary":"","tags":null,"title":"单词分隔符图分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/word-delimiter-graph/"},{"category":null,"content":"单词分隔符分词过滤器 #  单词分隔符(word_delimiter)分词过滤器用于根据预定义的字符拆分词元，还能根据可定制规则对词元进行可选的规范化处理。\n 我们建议尽可能使用 word_delimiter_graph 过滤器而非 word_delimiter 过滤器，因为 word_delimiter 过滤器有时会生成无效的词元图。\n  word_delimiter 过滤器可用于从零件编号或产品 ID 等复杂标识符中去除标点符号。在这种情况下，它最好与关键字分词器配合使用。对于带连字符的单词，建议使用同义词图(synonym_graph)分词过滤器而非 word_delimiter 过滤器，因为用户搜索这些词项时，常常既会搜索带连字符的形式，也会搜索不带连字符的形式。\n 默认情况下，该过滤器应用以下规则：\n   描述 输入 输出     将非字母数字字符视为分隔符 ultra-fast ultra, fast   去除词元开头或结尾的分隔符 Z99++'Decoder' Z99, Decoder   当字母大小写发生转换时拆分词元 Easysearch Easy, search   当字母和数字之间发生转换时拆分词元 T1000 T, 1000   去除词元末尾的所有格形式（\u0026lsquo;s） John's John     重要提示：不要将该过滤器与会去除标点符号的分词器（如标准分词器）一起使用。这样做可能会导致词元无法正确拆分，并影响诸如 catenate_all 或 preserve_original 等选项的效果。我们建议将此过滤器与关键(keyword)字分词器或空白(whitespace)分词器配合使用。\n 参数说明 #  你可以使用以下参数配置单词分隔符分词过滤器。\n   参数 必需/可选 数据类型 描述     catenate_all 可选 布尔值 从一系列字母数字部分生成连接后的词元。例如，quick-fast-200 会变成 [ quickfast200, quick, fast, 200 ]。默认值为 false。   catenate_numbers 可选 布尔值 连接数字序列。例如，10-20-30 会变成 [ 102030, 10, 20, 30 ]。默认值为 false。   catenate_words 可选 布尔值 连接字母单词。例如，high-speed-level 会变成 [ highspeedlevel, high, speed, level ]。默认值为 false。   generate_number_parts 可选 布尔值 如果为 true，输出中会包含纯数字词元（仅由数字组成的词元）。默认值为 true。   generate_word_parts 可选 布尔值 如果为 true，输出中会包含纯字母词元（仅由字母字符组成的词元）。默认值为 true。   preserve_original 可选 布尔值 在输出中，除了生成的词元外，还保留原始词元（可能包含非字母数字分隔符）。例如，auto-drive-300 会变成 [ auto - drive - 300, auto, drive, 300 ]。如果为 true，该过滤器会生成索引不支持的多位置词元，因此请勿在索引分词器中使用此过滤器，或者在该过滤器之后使用 flatten_graph 过滤器。默认值为 false。   protected_words 可选 字符串数组 指定不应被拆分的词元。   protected_words_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含不应被分隔的词元，词元需分行列出。   split_on_case_change 可选 布尔值 在连续字母大小写不同（一个为小写，另一个为大写）的位置拆分词元。例如，Easysearch 会变成 [ Easy, search ]。默认值为 true。   split_on_numerics 可选 布尔值 在连续字母和数字的位置拆分词元。例如，v8engine 会变成 [ v, 8, engine ]。默认值为 true。   stem_english_possessive 可选 布尔值 去除英语所有格结尾，如 \u0026lsquo;s。默认值为 true。   type_table 可选 字符串数组 一个自定义映射，用于指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。例如，要将连字符（-）视为字母数字字符，可指定 [\u0026quot;- =\u0026gt; ALPHA\u0026quot;]，这样单词就不会在连字符处拆分。有效类型有：\n- ALPHA：字母\n- ALPHANUM：字母数字\n- DIGIT：数字\n- LOWER：小写字母\n- SUBWORD_DELIM：非字母数字分隔符\n- UPPER：大写字母   type_table_path 可选 字符串 指定一个文件的路径（绝对路径或相对于配置目录的相对路径），该文件包含自定义字符映射。该映射指定如何处理字符以及是否将其视为分隔符，以避免不必要的拆分。有关有效类型，请参阅 type_table。    参考样例 #  以下示例请求创建了一个名为 my-custom-index 的新索引，并配置了一个带有单词分隔符过滤器（word_delimiter）的分词器。\nPUT /my-custom-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;custom_word_delimiter_filter\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;custom_word_delimiter_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;word_delimiter\u0026quot;, \u0026quot;split_on_case_change\u0026quot;: true, \u0026quot;split_on_numerics\u0026quot;: true, \u0026quot;stem_english_possessive\u0026quot;: true } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-custom-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;FastCar's Model2023\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Fast\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;Car\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;Model\u0026quot;, \u0026quot;start_offset\u0026quot;: 10, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;2023\u0026quot;, \u0026quot;start_offset\u0026quot;: 15, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 3 } ] } ","subcategory":null,"summary":"","tags":null,"title":"单词分隔符分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/word-delimiter/"},{"category":null,"content":"十进制数字分词过滤器 #  十进制数字（decimal_digit）分词过滤器用于将各种字符集中的十进制数字字符（0 到 9）规范化为它们对应的 ASCII 字符。当你希望在文本分析中确保所有数字都能被统一处理，而不管这些数字是以何种字符集书写时，这个过滤器就非常有用。\n参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有十进制数字过滤器的分词器：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_decimal_digit_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;decimal_digit\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;my_decimal_digit_filter\u0026quot;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;123 ١٢٣ १२३\u0026quot; } text分词：\n “123”（ASCII 数字） “١٢٣”（阿拉伯 - 印度数字） “१२३”（梵文数字）  返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;123\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 3, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;123\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;123\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"十进制数字分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/decimal-digit/"},{"category":null,"content":"匹配词元生成器 #  匹配（pattern）词元生成器是一种高度灵活的词元生成器，它允许你根据自定义的 Java 正则表达式将文本拆分为词元。与使用 Lucene 正则表达式的简单匹配（simple_pattern）词元生成器和简单分割匹配（simple_pattern_split）词元生成器不同，匹配词元生成器能够处理更复杂、更细致的正则表达式模式，从而让你对文本的分词方式拥有更强的掌控力。\n参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用匹配词元生成器的分词器。该分词器会在 -、_ 或 . 字符处对文本进行分割。\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_pattern_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;[-_.]\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_pattern_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_pattern_tokenizer\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_pattern_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_pattern_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch-2024_v1.2\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;2024\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;start_offset\u0026quot;: 16, \u0026quot;end_offset\u0026quot;: 18, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;start_offset\u0026quot;: 19, \u0026quot;end_offset\u0026quot;: 20, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 3 } ] } 参数说明 #  匹配词元生成器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述       pattern 可选 字符串 用于将文本拆分为词元的模式，需使用 Java 正则表达式指定。默认值是 \\W+。     flags 可选 字符串 配置以竖线分隔的标志，用于应用于正则表达式。例如，\u0026quot;CASE_INSENSITIVE | MULTILINE | DOTALL\u0026quot;。     group 可选 整数 指定用作词元的捕获组。默认值是 -1（在匹配处进行拆分）。      使用分组参数 #  以下示例请求配置了一个 group 参数，该参数仅捕获第二个捕获组。\nPUT /my_index_group2 { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_pattern_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;([a-zA-Z]+)(\\\\d+)\u0026quot;, \u0026quot;group\u0026quot;: 2 } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_pattern_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_pattern_tokenizer\u0026quot; } } } } } 使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index_group2/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_pattern_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;abc123def456ghi\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;123\u0026quot;, \u0026quot;start_offset\u0026quot;: 3, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;456\u0026quot;, \u0026quot;start_offset\u0026quot;: 9, \u0026quot;end_offset\u0026quot;: 12, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"匹配词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/pattern/"},{"category":null,"content":"匹配模式分词器 #  匹配模式（pattern）分词器允许你定义一个自定义分词器，该分词器使用正则表达式（regex）将输入文本分割成词元。它还提供了应用正则表达式、将词元转换为小写以及过滤掉停用词的功能。\n参数说明 #  匹配模式分词器可以使用以下参数进行配置。\n   参数 必填/可选 数据类型 描述     pattern 可选 字符串 用于对输入内容进行分词匹配的 Java 正则表达式。默认值是 \\W+。   flags 可选 字符串 以竖线分隔的 Java 正则表达式的字符串标志，这些标志会修改正则表达式的行为。   lowercase 可选 布尔值 是否将词元转换为小写。默认值为 true。   stopwords 可选 字符串或字符串列表 个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。   stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。    参考样例 #  以下命令创建一个名为 my_pattern_index 并使用匹配模式分词器的索引：\nPUT /my_pattern_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_pattern_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;\\\\W+\u0026quot;, \u0026quot;lowercase\u0026quot;: true, \u0026quot;stopwords\u0026quot;: [\u0026quot;and\u0026quot;, \u0026quot;is\u0026quot;] } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_pattern_analyzer\u0026quot; } } } } 产生的词元 #  以下请求用来检查分词器生成的词元：\nPOST /my_pattern_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_pattern_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch is fast and scalable\u0026quot; } 返回内容中包含了产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;fast\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 18, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;scalable\u0026quot;, \u0026quot;start_offset\u0026quot;: 23, \u0026quot;end_offset\u0026quot;: 31, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } ","subcategory":null,"summary":"","tags":null,"title":"匹配模式分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/pattern-analyzer/"},{"category":null,"content":"匹配替换字符过滤器 #  匹配替换（pattern_replace）字符过滤器使你能够使用正则表达式来定义文本匹配替换的模式。对于文本转换的高阶需求场景，尤其是在处理复杂的字符串模式时，它是一种的灵活工具。\n这个过滤器会用替换符合匹配模式的所有匹配项，从而可以轻松地对输入文本进行替换、删除或复杂的修改。你可以在分词之前使用它对输入内容进行规范化处理。\n参考样例 #  为了规范电话号码，你可以使用正则表达式 [\\\\s()-]+去替换号码里的特殊格式：\n []：定义一个字符类，意味着它将匹配方括号内的任意一个字符。 \\\\s：匹配任何空白字符，如空格、制表符或换行符。 ()：匹配字面意义上的括号（( 或 )）。 -：匹配字面意义上的连字符（-）。 +：指定该模式应匹配前面字符的一次或多次出现。  模式 [\\\\s()-]+ 将匹配由一个或多个空白字符、括号或连字符组成的任意序列，并将其从输入文本中移除。这确保了电话号码得到规范处理，结果将仅包含数字。\n以下请求通过移除空格、连字符和括号来规范电话号码：\nGET /_analyze { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;char_filter\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;pattern_replace\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;[\\\\s()-]+\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;\u0026quot; } ], \u0026quot;text\u0026quot;: \u0026quot;(555) 123-4567\u0026quot; } 返回内容中包含生成的词元：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;5551234567\u0026quot;, \u0026quot;start_offset\u0026quot;: 1, \u0026quot;end_offset\u0026quot;: 14, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } 参数说明 #  pattern_replace 字符过滤器必须使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     pattern 必需 字符串 用于匹配输入文本部分内容的正则表达式。过滤器会识别并匹配此模式以执行替换操作。   replacement 可选 字符串 用于替换匹配内容的字符串。使用空字符串（\u0026quot;\u0026quot;）可移除匹配到的文本。默认值为空字符串（\u0026quot;\u0026quot;）。    创建自定义分词器 #  以下请求创建一个索引，该索引带有一个配置了 pattern_replace 字符过滤器的自定义分词器。此过滤器会从数字中移除货币符号以及千位分隔符（包括欧洲的 “.” 和美国的 “,”）：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;char_filter\u0026quot;: [ \u0026quot;pattern_char_filter\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;pattern_char_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern_replace\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;[$€,.]\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;\u0026quot; } } } } } 使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Total: $ 1,200.50 and € 1.100,75\u0026quot; } 返回内容中包含了生成的词元：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Total\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 5, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;120050\u0026quot;, \u0026quot;start_offset\u0026quot;: 9, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;and\u0026quot;, \u0026quot;start_offset\u0026quot;: 18, \u0026quot;end_offset\u0026quot;: 21, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;110075\u0026quot;, \u0026quot;start_offset\u0026quot;: 24, \u0026quot;end_offset\u0026quot;: 32, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 } ] } 使用捕获组 #  你可以在 replacement 参数中使用捕获组。例如，以下请求创建了一个自定义分词器，该分词器使用匹配替换字符过滤器将电话号码中的连字符替换为点号：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;char_filter\u0026quot;: [ \u0026quot;pattern_char_filter\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;pattern_char_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern_replace\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;(\\\\d+)-(?=\\\\d)\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;$1.\u0026quot; } } } } } 使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Call me at 555-123-4567 or 555-987-6543\u0026quot; } 返回内容中包含了生成的词元：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Call\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;me\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;at\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;555.123.4567\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 23, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;or\u0026quot;, \u0026quot;start_offset\u0026quot;: 24, \u0026quot;end_offset\u0026quot;: 26, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;555.987.6543\u0026quot;, \u0026quot;start_offset\u0026quot;: 27, \u0026quot;end_offset\u0026quot;: 39, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 } ] } ","subcategory":null,"summary":"","tags":null,"title":"匹配替换字符过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/character-filters/pattern-replace/"},{"category":null,"content":"匹配替换分词过滤器 #  匹配替换（pattern_replace）分词过滤器允许您使用正则表达式来修改词元。此过滤器会将词元中的模式替换为指定的值，这使您在对词元进行索引之前，能够灵活地转换或规范化词元。当您在分析过程中需要清理或规范文本时，该过滤器尤其有用。\n参数说明 #  匹配替换分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     pattern 必需 字符串 一个正则表达式模式，用于匹配需要被替换的文本。   all 可选 布尔值 是否替换所有匹配的模式。如果为 false，则仅替换第一个匹配项。默认值为 true。   replacement 可选 字符串 用于替换匹配模式的字符串。默认值为空字符串。    参考样例 #  以下示例请求创建了一个名为 text_index 的新索引，并配置了一个带有匹配替换过滤器的分词器，用于将包含数字的词元替换为字符串 [NUM]：\nPUT /text_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;number_replace_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern_replace\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;\\\\d+\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;[NUM]\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;number_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;number_replace_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /text_index/_analyze { \u0026quot;text\u0026quot;: \u0026quot;Visit us at 98765 Example St.\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;number_analyzer\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;visit\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 5, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;us\u0026quot;, \u0026quot;start_offset\u0026quot;: 6, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;at\u0026quot;, \u0026quot;start_offset\u0026quot;: 9, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;[NUM]\u0026quot;, \u0026quot;start_offset\u0026quot;: 12, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;example\u0026quot;, \u0026quot;start_offset\u0026quot;: 18, \u0026quot;end_offset\u0026quot;: 25, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;st\u0026quot;, \u0026quot;start_offset\u0026quot;: 26, \u0026quot;end_offset\u0026quot;: 28, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 } ] } ","subcategory":null,"summary":"","tags":null,"title":"匹配替换分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/pattern-replace/"},{"category":null,"content":"加权平均聚合 #  weighted_avg 聚合计算跨文档数值的加权平均值。当您想计算平均值，但希望某些数据点的权重大于其他数据点时，此功能非常有用。\n加权平均值使用公式 $\\frac{\\sum_{i=1}^n value_i \\cdot weight_i}{\\sum_{i=1}^n weight_i}$ 计算。\n参数说明 #  weighted_avg 聚合采用以下参数。\n   参数 必需/可选 描述     value 必需 定义如何获取要计算平均值的数值。需要 field 或 script 。   weight 必需 定义如何获取每个值的权重。需要 field 或 script 。   format 可选 DecimalFormat 格式字符串。返回聚合的 value_as_string 属性中的格式化输出。   value_type 可选 使用脚本或未映射字段时的值的类型提示。    可以在 value 或 weight 内指定以下参数。\n   参数 必需/可选 描述     field 可选 用于值或权重的文档字段。   missing 可选 字段缺失时使用的默认值或权重。请参阅缺失值。    参考样例 #  首先，创建索引并索引一些数据。请注意，产品 C 缺少 rating 和 num_reviews 字段：\nPOST _bulk { \u0026quot;index\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;products\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;Product A\u0026quot;, \u0026quot;rating\u0026quot;: 4.5, \u0026quot;num_reviews\u0026quot;: 100 } { \u0026quot;index\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;products\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;Product B\u0026quot;, \u0026quot;rating\u0026quot;: 3.8, \u0026quot;num_reviews\u0026quot;: 50 } { \u0026quot;index\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;products\u0026quot; } } { \u0026quot;name\u0026quot;: \u0026quot;Product C\u0026quot;} 以下请求使用 weighted_avg 聚合来计算加权平均产品评分。在此上下文中，每个产品的评分都由其 num_reviews 加权。这意味着，评论较多的产品对最终平均值的影响将大于评论较少的产品：\nGET /products/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;weighted_rating\u0026quot;: { \u0026quot;weighted_avg\u0026quot;: { \u0026quot;value\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;rating\u0026quot; }, \u0026quot;weight\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;num_reviews\u0026quot; }, \u0026quot;format\u0026quot;: \u0026quot;#.##\u0026quot; } } } } 返回内容 #\n 响应包含 weighted_rating ，计算结果为 weighted_avg = (4.5 * 100 + 3.8 * 50) / (100 + 50) = 4.27 。仅考虑同时包含 rating 和 num_reviews 值的文档 1 和 2：\n{ \u0026quot;took\u0026quot;: 18, \u0026quot;timed_out\u0026quot;: false, \u0026quot;terminated_early\u0026quot;: true, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 3, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;weighted_rating\u0026quot;: { \u0026quot;value\u0026quot;: 4.266666650772095, \u0026quot;value_as_string\u0026quot;: \u0026quot;4.27\u0026quot; } } } 缺省值 #  使用 missing 参数，您可以为缺少 value 字段或 weight 字段的文档指定默认值，而不是将它们从计算中排除。\n例如，您可以为没有评级的产品分配 3.0 的“平均”评级，并将 num_reviews 设置为 1，以赋予它们较小的非零权重：\nGET /products/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;weighted_rating\u0026quot;: { \u0026quot;weighted_avg\u0026quot;: { \u0026quot;value\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;rating\u0026quot;, \u0026quot;missing\u0026quot;: 3.0 }, \u0026quot;weight\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;num_reviews\u0026quot;, \u0026quot;missing\u0026quot;: 1 }, \u0026quot;format\u0026quot;: \u0026quot;#.##\u0026quot; } } } } 新的加权平均值计算为 weighted_avg = (4.5 * 100 + 3.8 * 50 + 3.0 * 1) / (100 + 50 + 1) = 4.26 ：\n{ \u0026quot;took\u0026quot;: 27, \u0026quot;timed_out\u0026quot;: false, \u0026quot;terminated_early\u0026quot;: true, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 3, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;weighted_rating\u0026quot;: { \u0026quot;value\u0026quot;: 4.258278129906055, \u0026quot;value_as_string\u0026quot;: \u0026quot;4.26\u0026quot; } } }   ","subcategory":null,"summary":"","tags":null,"title":"加权平均聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/weighted-avg/"},{"category":null,"content":"前缀 n 元词元生成器 #  前缀 n 元词元生成器会从每个单词的开头生成部分单词词元，也就是 n 元组。它会根据指定的字符分割文本，并在定义的最小和最大长度范围内生成词元。这种词元生成器在实现即输即搜（search-as-you-type）功能时特别有用。\n前缀 n 元组非常适合用于自动补全搜索，在这种搜索中单词的顺序可能会有所不同，例如在搜索产品名称或地址时。有关更多信息，请参阅 “自动补全” 相关内容。不过，对于像电影或歌曲标题这种顺序固定的文本，完成建议器(completion suggester)可能会更准确。\n默认情况下，前缀 n 元词元生成器生成的词元最小长度为 1，最大长度为 2。例如，当分析文本 Easysearch 时，默认配置会生成 “E” 和 “Ea” 这两个 n 元组。这些短的 n 元组常常会匹配到太多不相关的词条，所以调整 n 元组的长度，对词元生成器进行优化是很有必要的。\n参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用前缀 n 元词元生成器的分词器。该词元生成器生成长度为 3 到 6 个字符的词元，且将字母和符号都视为有效的词元字符。\nPUT /edge_n_gram_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;my_custom_tokenizer\u0026quot; } }, \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_custom_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;edge_ngram\u0026quot;, \u0026quot;min_gram\u0026quot;: 3, \u0026quot;max_gram\u0026quot;: 6, \u0026quot;token_chars\u0026quot;: [ \u0026quot;letter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /edge_n_gram_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Code 42 rocks!\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Cod\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 3, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;Code\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;roc\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;rock\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 12, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;rocks\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } 参数说明 #     参数 必填/选填 数据类型 描述     min_gram 选填 整数 词元的最小长度。默认值为 1。   max_gram 选填 整数 词元的最大长度。默认值为 2。   custom_token_chars 选填 字符串 被视为词元一部分的自定义字符（例如，+-_）。   token_chars 选填 字符串数组 定义要包含在词元中的字符类。词元会字符类规则进行拆分。默认是所有字符。可用的字符类包括：\n- letter：字母字符（例如，a、ç 或 “京”）\n- digit：数字字符（例如，3 或 7）\n- punctuation：标点符号（例如，! 或 ?）\n- symbol：其他符号（例如，$ 或 √）\n- whitespace：空格或换行符\n- custom：允许您在 custom_token_chars 设置中指定自定义字符。    最大词元长度（max_gram）参数的限制 #  最大词元长度（max_gram）参数设置了生成的词元的最大长度。当一个查询词项的长度超过这个设定时，它可能无法匹配索引中的任何词条。\n例如，如果将 max_gram 设置为 4，那么在索引过程中，查询词 explore 会被分词为 expl。这样一来，当搜索完整的词条 explore 时，就无法匹配到已索引的词元 expl 了。\n为了解决这个限制问题，你可以应用一个截断(truncate)词元过滤器，将搜索词缩短到最大词元长度。不过，这种方法也存在权衡取舍。将 explore 截断为 expl 可能会导致与一些不相关的词条匹配，比如 explosion 或 explicit，从而降低搜索精度。\n我们建议仔细权衡 max_gram 的值，以确保在进行高效分词的同时，尽量减少不相关的匹配。如果搜索精度至关重要，可考虑其他策略，比如调整查询分词器或微调过滤器。\n最佳实践 #  我们建议仅在索引创建阶段使用前缀 n 元词元生成器，以确保部分单词词元得以存储。在执行搜索时，应使用基础分词器来匹配所有的查询词条。\n配置“即输即搜”功能 #  要实现即输即搜（search-as-you-type）功能，可在索引创建时使用前缀 n 元词元生成器，在搜索时使用一个执行最少处理操作的分词器。以下示例展示了这种实现方法。\n使用前缀 n 元词元生成器创建一个索引：\nPUT /my-autocomplete-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;autocomplete\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;autocomplete\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot; ] }, \u0026quot;autocomplete_search\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;lowercase\u0026quot; } }, \u0026quot;tokenizer\u0026quot;: { \u0026quot;autocomplete\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;edge_ngram\u0026quot;, \u0026quot;min_gram\u0026quot;: 2, \u0026quot;max_gram\u0026quot;: 10, \u0026quot;token_chars\u0026quot;: [ \u0026quot;letter\u0026quot; ] } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;title\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;autocomplete\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;autocomplete_search\u0026quot; } } } } 索引一个包含 product 字段的文档，并刷新该索引：\nPUT my-autocomplete-index/_doc/1?refresh { \u0026quot;title\u0026quot;: \u0026quot;Laptop Pro\u0026quot; } 这种配置确保了前缀 n 元词元生成器会将诸如 “Laptop” 这样的词条拆分成 “La”、“Lap” 和 “Lapt” 等词元，从而在搜索时能够实现部分匹配。在搜索阶段，standard词元生成器会简化查询操作，并且由于有小写字母过滤器的存在，还能确保匹配操作不区分大小写。\n现在，当搜索 “laptop Pr” 或 “lap pr” 时，就会基于部分匹配检索到相关的文档：\nGET my-autocomplete-index/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;title\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;lap pr\u0026quot;, \u0026quot;operator\u0026quot;: \u0026quot;and\u0026quot; } } } } 了解更多信息，请参阅  Search as you type 相关内容。\n","subcategory":null,"summary":"","tags":null,"title":"前缀 n 元词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/edge-n-gram/"},{"category":null,"content":"索引别名 #  别名是可以指向一个或多个索引的虚拟索引名称。\n如果数据分布在多个索引中，而不是跟踪要查询的索引，则可以创建别名并进行查询。\n例如，如果要将日志存储到基于月份的索引中，并且经常查询前两个月的日志，则可以创建一个 last_2_months 别名，并每月更新其指向的索引。\n您可以随时更改别名指向的索引，所以在应用程序中使用别名引用索引可以让您在不停机的情况下重新索引数据。\n创建索引别名 #  要创建索引别名，请使用 POST 请求:\nPOST _aliases 使用 actions 方法指定要执行的操作列表。此命令创建名为 alias1 的别名，并将 index-1 添加到此别名：\nPOST _aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index-1\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias1\u0026#34; } } ] } 您应该看到以下响应:\n{ \u0026#34;acknowledged\u0026#34;: true } 如果此请求失败，请确保要添加到别名的索引已存在.\n要检查 alias1 是否引用 index-1 ，请运行以下命令\nGET alias1 索引别名添加与删除操作 #  您可以在同一 别名 操作中执行多个操作。\n例如，以下命令删除 index-1 并将 index-2 添加到 alias1 ：\nPOST _aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;remove\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index-1\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias1\u0026#34; } }, { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index-2\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias1\u0026#34; } } ] } add 和 remove 操作以原子方式发生，这意味着 alias1 不会同时指向 index-1 和 index-2 .\n还可以基于索引模式添加索引:\nPOST _aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index*\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias1\u0026#34; } } ] } 索引别名管理 #  要列出别名到索引的映射，请运行以下命令:\nGET _cat/aliases?v 响应示例 #  alias index filter routing.index routing.search alias1 index-1 * - - 要检查别名指向的索引，请运行以下命令:\nGET _alias/alias1 响应示例 #  { \u0026#34;index-2\u0026#34;: { \u0026#34;aliases\u0026#34;: { \u0026#34;alias1\u0026#34;: {} } } } 相反，要查找指向特定索引的别名，请运行以下命令：\nGET /index-2/_alias/* 要检查别名是否存在，请运行以下命令：\nHEAD /alias1/_alias/ 创建索引时添加别名 #  您可以在创建索引时向别名添加索引：\nPUT index-1 { \u0026#34;aliases\u0026#34;: { \u0026#34;alias1\u0026#34;: {} } } 根据筛选创建别名 #  您可以创建一个过滤的别名来访问基础索引中的文档或字段子集。\n此命令仅向 alias1 添加特定的时间戳字段:\nPOST _aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index-1\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias1\u0026#34;, \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;timestamp\u0026#34;: \u0026#34;1574641891142\u0026#34; } } } } ] } 索引别名选项 #  您可以指定下表中显示的选项:\n   选项 类型 描述 必填     index String 别名指向的索引名 Yes   alias String 别名名称 No   filter Object 过滤条件 No   routing String 路由到特定分片，可以分别设置 search_routing 或者 index_routing 参数 No   is_write_index String 别名是否允许接收写入操作，如果不指定则表示不允许 No    ","subcategory":null,"summary":"","tags":null,"title":"别名操作","url":"/easysearch/v1.15.0/docs/references/document/index-alias/"},{"category":null,"content":"创建一个自定义分词器 #  要创建一个自定义分词器，需要指定以下组成内容：\n 字符过滤器（零个或多个） 词元生成器（一个） 词元过滤器（零个或多个）  相关配置 #  以下参数可用于配置自定义分词器。\n   参数 必填/可选 描述     type 可选 分词器类型。默认值为custom。你也可以给这个参数指定一个预构建的分词器。   tokenizer 必填 每个分词器必须要有一个词元生成器。   char_filter 可选 要包含在分词器中的字符过滤器列表。   filter 可选 要包含在分词器中的分词过滤器列表。   position_increment_gap 可选 在对包含多个词项内容的文本字段建立索引时，应用于各词项之间的额外间隔。有关更多信息，请参阅位置间隔增量。默认值为 100。    参考样例 #  以下示例展示了各种自定义分词器的配置。\n自定义分词器用于去除 HTML 格式标签 #  以下示例中的分词器在分词之前会从文本中移除 HTML 格式标签：\nPUT simple_html_strip_analyzer_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;html_strip_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;char_filter\u0026quot;: [\u0026quot;html_strip\u0026quot;], \u0026quot;tokenizer\u0026quot;: \u0026quot;whitespace\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;] } } } } } 使用以下请求来查看使用该分词器生成的词元：\nGET simple_html_strip_analyzer_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;html_strip_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;\u0026lt;p\u0026gt;Easysearch is \u0026lt;strong\u0026gt;awesome\u0026lt;/strong\u0026gt;!\u0026lt;/p\u0026gt;\u0026quot; } 返回内容中包含生成的词元：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 3, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;, \u0026quot;start_offset\u0026quot;: 14, \u0026quot;end_offset\u0026quot;: 16, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;awesome!\u0026quot;, \u0026quot;start_offset\u0026quot;: 25, \u0026quot;end_offset\u0026quot;: 42, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } 自定义分词器实现同义词替换和字符串映射 #  以下示例中的分词器可以在同义词过滤之前替换特定的字符：\nPUT mapping_analyzer_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;synonym_mapping_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;char_filter\u0026quot;: [\u0026quot;underscore_to_space\u0026quot;], \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;, \u0026quot;stop\u0026quot;, \u0026quot;synonym_filter\u0026quot;] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;underscore_to_space\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mapping\u0026quot;, \u0026quot;mappings\u0026quot;: [\u0026quot;_ =\u0026gt; ' '\u0026quot;] } }, \u0026quot;filter\u0026quot;: { \u0026quot;synonym_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym\u0026quot;, \u0026quot;synonyms\u0026quot;: [ \u0026quot;quick, fast, speedy\u0026quot;, \u0026quot;big, large, huge\u0026quot; ] } } } } } 使用以下请求来查看使用该分词器生成的词元：\nGET mapping_analyzer_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;synonym_mapping_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;The slow_green_turtle is very large\u0026quot; } 返回内容中包含生成的词元：\n{ \u0026quot;tokens\u0026quot;: [ {\u0026quot;token\u0026quot;: \u0026quot;slow\u0026quot;,\u0026quot;start_offset\u0026quot;: 4,\u0026quot;end_offset\u0026quot;: 8,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 1}, {\u0026quot;token\u0026quot;: \u0026quot;green\u0026quot;,\u0026quot;start_offset\u0026quot;: 9,\u0026quot;end_offset\u0026quot;: 14,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;,\u0026quot;start_offset\u0026quot;: 15,\u0026quot;end_offset\u0026quot;: 21,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;very\u0026quot;,\u0026quot;start_offset\u0026quot;: 25,\u0026quot;end_offset\u0026quot;: 29,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 5}, {\u0026quot;token\u0026quot;: \u0026quot;large\u0026quot;,\u0026quot;start_offset\u0026quot;: 30,\u0026quot;end_offset\u0026quot;: 35,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 6}, {\u0026quot;token\u0026quot;: \u0026quot;big\u0026quot;,\u0026quot;start_offset\u0026quot;: 30,\u0026quot;end_offset\u0026quot;: 35,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 6}, {\u0026quot;token\u0026quot;: \u0026quot;huge\u0026quot;,\u0026quot;start_offset\u0026quot;: 30,\u0026quot;end_offset\u0026quot;: 35,\u0026quot;type\u0026quot;: \u0026quot;SYNONYM\u0026quot;,\u0026quot;position\u0026quot;: 6} ] } 自定义分词器实现数字规范化 #  以下示例中的分词器会通过去除破折号和空格的方式来规范化电话号码，并对规范化后的文本应用边缘 edge n-gram 以支持部分匹配：\nPUT advanced_pattern_replace_analyzer_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;phone_number_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;char_filter\u0026quot;: [\u0026quot;phone_normalization\u0026quot;], \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;, \u0026quot;edge_ngram\u0026quot;] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;phone_normalization\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern_replace\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;[-\\\\s]\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;\u0026quot; } }, \u0026quot;filter\u0026quot;: { \u0026quot;edge_ngram\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;edge_ngram\u0026quot;, \u0026quot;min_gram\u0026quot;: 3, \u0026quot;max_gram\u0026quot;: 10 } } } } } 使用以下请求来查看使用该分词器生成的词元：\nGET advanced_pattern_replace_analyzer_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;phone_number_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;123-456 7890\u0026quot; } 返回内容中包含生成的词元：\n{ \u0026quot;tokens\u0026quot;: [ {\u0026quot;token\u0026quot;: \u0026quot;123\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 12,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;1234\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 12,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;12345\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 12,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;123456\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 12,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;1234567\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 12,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;12345678\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 12,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;123456789\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 12,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;1234567890\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 12,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0} ] } 处理正则表达式模式中的特殊字符 #  当在分词器中使用自定义正则表达式模式时，要确保能正确处理特殊字符或非英文字符。默认情况下，Java 的正则表达式仅将 [A-Za-z0-9_] 视为单词字符（\\w）。这在使用 \\w 或 \\b（用于匹配单词和非单词字符之间的边界）时可能会导致意外行为。 例如，以下分词器尝试使用模式 (\\b\\p{L}+\\b) 来匹配各语种（\\p{L}）的一个或多个字母字符：\nPUT /buggy_custom_analyzer { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;capture_words\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern_capture\u0026quot;, \u0026quot;patterns\u0026quot;: [ \u0026quot;(\\\\b\\\\p{L}+\\\\b)\u0026quot; ] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;filter_only_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;capture_words\u0026quot; ] } } } } } 然而，这个分词器会错误地将 él-empezó-a-reír 分词为 l、empez、a 和 reír，因为 \\b 无法匹配带重音符号的字符与字符串开头或结尾之间的边界。\n为了正确处理特殊字符，你需要在模式中添加 Unicode 大小写标志 (?U)：\nPUT /fixed_custom_analyzer { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;capture_words\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern_capture\u0026quot;, \u0026quot;patterns\u0026quot;: [ \u0026quot;(?U)(\\\\b\\\\p{L}+\\\\b)\u0026quot; ] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;filter_only_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;capture_words\u0026quot; ] } } } } } 位置间隔增量 #  position_increment_gap（位置间隔增量）参数用于在对多值字段（如数组）建立索引时，设置词项之间的位置间隔。除非有明确的允许，这个间隔参数可确保短语查询不会出现跨词项匹配。例如，默认间隔值为 100，表示不同数组条目中的词项之间相隔 100 个位置距离，从而在短语搜索中防止出现意外的匹配情况。你可以调整这个值，或者将其设置为 0，以便让短语能够跨越词项进行匹配。\n下面的示例通过使用 match_phrase（匹配短语）查询来演示 position_increment_gap 的作用。\n 在 test-index 索引中索引一个文档：  PUT test-index/_doc/1 { \u0026quot;names\u0026quot;: [ \u0026quot;Slow green\u0026quot;, \u0026quot;turtle swims\u0026quot;] } 使用短语匹配查询（match_phrase）来查询文档：  GET test-index/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;names\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;green turtle\u0026quot; } } } } 返回内容是空的，这是因为 green 和 turtle 这两个词项之间的距离是 100（即默认的 position_increment_gap 值）。\n现在，使用带有 slop 参数的 match_phrase 查询来查询文档，我们把该参数值设置成大于 position_increment_gap 的 101：  GET test-index/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;names\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;green turtle\u0026quot;, \u0026quot;slop\u0026quot;: 101 } } } } 返回内容包含了需要匹配的文档\n { \u0026quot;took\u0026quot;: 4, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 1, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 0.010358453, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;test-index\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.010358453, \u0026quot;_source\u0026quot;: { \u0026quot;names\u0026quot;: [ \u0026quot;Slow green\u0026quot;, \u0026quot;turtle swims\u0026quot; ] } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"创建一个自定义分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/creating-a-custom-analyzer/"},{"category":null,"content":"分隔式负载分词过滤器 #  分隔式负载（delimited_payload）分词过滤器用于在分词过程中解析包含负载的词元。例如，字符串 red|1.5 fast|2.0 car|1.0 会被解析为词元 red（负载为 1.5）、fast（负载为 2.0）和 car（负载为 1.0）。当你的词元包含额外的关联数据（如权重、分数或其他数值），且你打算将这些数据用于评分或自定义查询逻辑时，这个过滤器就特别有用。该过滤器可以处理不同类型的负载，包括整数、浮点数和字符串，并将负载（额外的元数据）附加到词元上。\n在文本分词时，分隔式负载分词过滤器会解析每个词元，提取负载并将其附加到词元上。后续在查询中可以使用这个负载来影响评分、提升权重或实现其他自定义行为。\n负载以 Base64 编码的字符串形式存储。默认情况下，负载不会随词元一起在查询响应中返回。若要返回负载，你必须配置额外的参数。更多信息，请参阅“带有负载存储的分词示例”。\n参数说明 #  分隔式负载分词过滤器有两个参数：\n   参数 必需/可选 数据类型 描述     encoding 可选 字符串 指定附加到词元的负载的数据类型。这决定了在分析和查询期间如何解释负载数据。\n有效值为：\n- float：使用 IEEE 754 格式将负载解释为 32 位浮点数（例如，car\\|2.5 中的 2.5）。\n- identity：将负载解释为字符序列（例如，在 user\\|admin 中，admin 被解释为字符串）。\n- int：将负载解释为 32 位整数（例如，priority \\| 1中的1）。\n默认值为 float。   delimiter 可选 字符串 指定在输入文本中分隔词元及其负载的字符。默认值为竖线字符（\\|）。    不带负载存储的分词示例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有分隔式负载过滤器的分词器：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_payload_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;delimited_payload\u0026quot;, \u0026quot;delimiter\u0026quot;: \u0026quot;|\u0026quot;, \u0026quot;encoding\u0026quot;: \u0026quot;float\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;whitespace\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;my_payload_filter\u0026quot;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;red|1.5 fast|2.0 car|1.0\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;red\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;fast\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 16, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;car\u0026quot;, \u0026quot;start_offset\u0026quot;: 17, \u0026quot;end_offset\u0026quot;: 24, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } 带有负载存储的分词示例 #  若要在返回内容中带有负载，需创建一个存储词项向量的索引，并在索引映射中将 term_vector 设置为 with_positions_payloads 或 with_positions_offsets_payloads。例如，以下索引被配置为将负载存储到词项向量内容：\nPUT /visible_payloads { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;text\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_payloads\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot; } } }, \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_payload_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;delimited_payload\u0026quot;, \u0026quot;delimiter\u0026quot;: \u0026quot;|\u0026quot;, \u0026quot;encoding\u0026quot;: \u0026quot;float\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;whitespace\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;my_payload_filter\u0026quot; ] } } } } } 你可以使用以下请求将一个文档索引到这个索引中：\nPUT /visible_payloads/_doc/1 { \u0026quot;text\u0026quot;: \u0026quot;red|1.5 fast|2.0 car|1.0\u0026quot; } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /visible_payloads/_termvectors/1 { \u0026quot;fields\u0026quot;: [\u0026quot;text\u0026quot;] } 返回内容中包含生成的词元，其中包括负载信息：\n{ \u0026quot;_index\u0026quot;: \u0026quot;visible_payloads\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_version\u0026quot;: 1, \u0026quot;found\u0026quot;: true, \u0026quot;took\u0026quot;: 5, \u0026quot;term_vectors\u0026quot;: { \u0026quot;text\u0026quot;: { \u0026quot;field_statistics\u0026quot;: { \u0026quot;sum_doc_freq\u0026quot;: 3, \u0026quot;doc_count\u0026quot;: 1, \u0026quot;sum_ttf\u0026quot;: 3 }, \u0026quot;terms\u0026quot;: { \u0026quot;car\u0026quot;: { \u0026quot;term_freq\u0026quot;: 1, \u0026quot;tokens\u0026quot;: [ { \u0026quot;position\u0026quot;: 2, \u0026quot;payload\u0026quot;: \u0026quot;P4AAAA==\u0026quot; } ] }, \u0026quot;fast\u0026quot;: { \u0026quot;term_freq\u0026quot;: 1, \u0026quot;tokens\u0026quot;: [ { \u0026quot;position\u0026quot;: 1, \u0026quot;payload\u0026quot;: \u0026quot;QAAAAA==\u0026quot; } ] }, \u0026quot;red\u0026quot;: { \u0026quot;term_freq\u0026quot;: 1, \u0026quot;tokens\u0026quot;: [ { \u0026quot;position\u0026quot;: 0, \u0026quot;payload\u0026quot;: \u0026quot;P8AAAA==\u0026quot; } ] } } } } } ","subcategory":null,"summary":"","tags":null,"title":"分隔式负载分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/delimited-payload/"},{"category":null,"content":"分组聚合 #  terms 聚合会动态为字段中的每个唯一词条创建一个分组。\n以下示例使用 terms 聚合来查找网络日志数据中每个响应代码的文档数量：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;response_codes\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;response.keyword\u0026quot;, \u0026quot;size\u0026quot;: 10 } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;response_codes\u0026quot; : { \u0026quot;doc_count_error_upper_bound\u0026quot; : 0, \u0026quot;sum_other_doc_count\u0026quot; : 0, \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;200\u0026quot;, \u0026quot;doc_count\u0026quot; : 12832 }, { \u0026quot;key\u0026quot; : \u0026quot;404\u0026quot;, \u0026quot;doc_count\u0026quot; : 801 }, { \u0026quot;key\u0026quot; : \u0026quot;503\u0026quot;, \u0026quot;doc_count\u0026quot; : 441 } ] } } } 值以 key 键返回。 doc_count 指定每个分组中的文档数量。默认情况下，分组按 doc-count 的降序排列。\n 可以使用 terms 通过按升序计数排序（ \u0026ldquo;order\u0026rdquo;: {\u0026ldquo;count\u0026rdquo;: \u0026ldquo;asc\u0026rdquo;} ）来搜索不常见的值。然而，我们强烈不建议这种做法，因为在涉及多个分片时，它可能导致不准确的结果。一个全局上不常见的词项可能不会在每个单个分片上显得不常见，或者可能完全不在某些分片返回的最不常见结果中。相反，一个在某个分片上出现频率较低的词项可能在另一个分片上很常见。在这两种情况下，分片级别的聚合可能会遗漏稀有词项，导致整体结果不正确。我们建议使用 rare_terms 聚合代替 terms 聚合，它专门设计用于更准确地处理这些情况。\n 返回文档数量和分片大小参数 #  terms 聚合返回的分组数量由 size 参数控制，默认值为 10。\n此外，负责聚合的协调节点会提示每个分片返回其顶部唯一词项。每个分片返回的分组数量由 shard_size 参数控制。此参数与 size 参数不同，它作为增加分组文档计数准确性的机制存在。\n例如，假设 size 和 shard_size 参数的值都为 3。 terms 聚合会向每个分片请求其前三个唯一词项。协调节点会汇总结果以计算最终结果。如果一个分片包含的某个对象不在前三个中，那么它就不会出现在响应中。然而，通过增加此请求的 shard_size 值，每个分片可以返回更多的唯一词项，从而提高协调节点收到所有相关结果的可能性。\n默认情况下， shard_size 参数设置为 size * 1.5 + 10 。\n在使用并发分片搜索时， shard_size 参数也会应用于每个分片切片。\nshard_size 参数作为平衡 terms 聚合的性能和文档计数准确性的方式。较高的 shard_size 值将确保更高的文档计数准确性，但会导致更高的内存和计算使用。较低的 shard_size 值将更高效，但会导致较低的文档计数准确性。\n文档计数错误 #  返回内容还包括两个名为 doc_count_error_upper_bound 和 sum_other_doc_count 的键。\nterms 聚合返回最顶端的唯一词。因此，如果数据包含许多唯一词，那么其中一些可能不会出现在结果中。 sum_other_doc_count 字段表示被排除在响应之外的文档总和。在这种情况下，数字是 0，因为所有唯一值都出现在响应中。\ndoc_count_error_upper_bound 字段表示最终结果中排除的唯一值的最大可能计数。使用此字段来估计计数的误差范围。\ndoc_count_error_upper_bound 值和准确性的概念仅适用于使用默认排序方式（按文档数量降序）进行的聚合。这是因为当你按文档数量降序排序时，未返回的任何词项都保证包含等于或少于已返回词项的文档数。基于这一点，你可以计算 doc_count_error_upper_bound 。\nmin_doc_count 和 shard_min_doc_count 参数 #  您可以使用 min_doc_count 参数过滤掉任何出现次数少于 min_doc_count 的唯一词项。 min_doc_count 阈值仅应用于合并从所有分片中检索到的结果之后。每个分片都不知道某个词项的全局文档数量。如果全局最频繁的 shard_size 个词项与某个分片本地最频繁的词项之间存在显著差异，在使用 min_doc_count 参数时您可能会收到意外结果。\n另外， shard_min_doc_count 参数用于过滤掉分片返回给协调器且结果少于 shard_min_doc_count 的唯一词项。\n收集模式 #  有两种收集模式可用： depth_first 和 breadth_first 。 depth_first 收集模式以深度优先方式扩展聚合树的所有分支，并在扩展完成后才进行剪枝。\n然而，在使用嵌套的聚合时，返回的分组数量 cardinality 会被每个嵌套级别的字段 cardinality 相乘，这使得随着聚合的嵌套，分组数量容易出现组合爆炸。\n你可以使用 breadth_first 集合模式来解决这个问题。在这种情况下，聚合树的第一级将在展开到下一级之前应用剪枝，可能会大大减少计算分组的数量。\n此外，执行 breadth_first 收集还会带来内存开销，该开销与匹配文档的数量呈线性关系。这是因为 breadth_first 收集的工作方式是通过缓存并重放从父级层级修剪后的分组集。\n考虑预聚合数据 #  虽然 doc_count 字段提供了关于分组中聚合的独立文档数量的表示，但 doc_count 单独无法正确地增加存储预聚合数据的文档。要考虑预聚合数据并准确计算分组中的文档数量，您可以使用 _doc_count 字段来添加单个汇总字段中的文档数量。当文档包含 _doc_count 字段时，所有分组聚合都会识别其值并累积增加分组 doc_count 。在使用 _doc_count 字段时，请记住这些注意事项：\n 该字段不支持嵌套数组；只能使用正整数。 如果文档不包含 _doc_count 字段，聚合会使用该文档将计数增加 1。  参考样例 #  PUT /my_index/_doc/1 { \u0026quot;response_code\u0026quot;: 404, \u0026quot;date\u0026quot;:\u0026quot;2022-08-05\u0026quot;, \u0026quot;_doc_count\u0026quot;: 20 } PUT /my_index/_doc/2 { \u0026quot;response_code\u0026quot;: 404, \u0026quot;date\u0026quot;:\u0026quot;2022-08-06\u0026quot;, \u0026quot;_doc_count\u0026quot;: 10 }\nPUT /my_index/_doc/3 { \u0026quot;response_code\u0026quot;: 200, \u0026quot;date\u0026quot;:\u0026quot;2022-08-06\u0026quot;, \u0026quot;_doc_count\u0026quot;: 300 }\nGET /my_index/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;response_codes\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot; : \u0026quot;response_code\u0026quot; } } } } 返回内容\n{ \u0026quot;took\u0026quot; : 20, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 3, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] }, \u0026quot;aggregations\u0026quot; : { \u0026quot;response_codes\u0026quot; : { \u0026quot;doc_count_error_upper_bound\u0026quot; : 0, \u0026quot;sum_other_doc_count\u0026quot; : 0, \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : 200, \u0026quot;doc_count\u0026quot; : 300 }, { \u0026quot;key\u0026quot; : 404, \u0026quot;doc_count\u0026quot; : 30 } ] } } }   ","subcategory":null,"summary":"","tags":null,"title":"分组聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/terms/"},{"category":null,"content":"分组排序聚合 #  bucket_sort 分组排序聚合是一个父聚合，它对其父多存储分组聚合生成的存储分组进行排序或截断。\n在 bucket_sort 聚合中，您可以按多个字段对存储分组进行排序，每个字段都有自己的排序顺序。可以按存储分组的键、文档计数或子聚合中的值进行排序。您还可以使用 from 和 size 参数来截断结果，无论是否进行排序。\n有关指定排序顺序的信息，请参阅排序结果 。\n参数说明 #  bucket_sort 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     gap_policy 可选 String 要应用于缺失数据的策略。有效值为 skip 和 insert_zeros。默认值为 skip。请参阅数据差距 。   sort 可选 String 要排序的字段列表。请参阅排序结果 。   from 可选 String 要返回的第一个结果的索引。必须是非负整数。默认值为 0。请参阅 from 和 size 参数 。   size 可选 String 要返回的最大结果数。必须是正整数。请参阅 from 和 size 参数 。     您必须至少提供一个 sort、from 和 size。\n 参考样例 #  以下示例创建间隔为一个月的日期直方图。sum 子聚合计算每个月所有字节的总和。最后，聚合按字节数降序对存储分组进行排序：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;total_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;bytes_bucket_sort\u0026quot;: { \u0026quot;bucket_sort\u0026quot;: { \u0026quot;sort\u0026quot;: [ { \u0026quot;total_bytes\u0026quot;: { \u0026quot;order\u0026quot;: \u0026quot;desc\u0026quot; } } ] } } } } } } 返回内容 #\n 聚合按总字节数降序对存储分组重新排序：\n{ \u0026quot;took\u0026quot;: 3, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-05-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1746057600000, \u0026quot;doc_count\u0026quot;: 7072, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 40124337 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-06-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1748736000000, \u0026quot;doc_count\u0026quot;: 6056, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 34123131 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 946, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 5478221 } } ] } } } 示例：截取结果 #\n 要截取结果，请提供 from 和/或 size 参数。以下示例执行相同的排序，但返回两个存储分组，从第二个存储分组开始：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;total_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;bytes_bucket_sort\u0026quot;: { \u0026quot;bucket_sort\u0026quot;: { \u0026quot;sort\u0026quot;: [ { \u0026quot;total_bytes\u0026quot;: { \u0026quot;order\u0026quot;: \u0026quot;desc\u0026quot; } } ], \u0026quot;from\u0026quot;: 1, \u0026quot;size\u0026quot;: 2 } } } } } } 聚合返回两个排序的存储分组：\n{ \u0026quot;took\u0026quot;: 2, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;buckets\u0026quot;: [ { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-06-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1748736000000, \u0026quot;doc_count\u0026quot;: 6056, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 34123131 } }, { \u0026quot;key_as_string\u0026quot;: \u0026quot;2025-04-01T00:00:00.000Z\u0026quot;, \u0026quot;key\u0026quot;: 1743465600000, \u0026quot;doc_count\u0026quot;: 946, \u0026quot;total_bytes\u0026quot;: { \u0026quot;value\u0026quot;: 5478221 } } ] } } } 要截取结果而不进行排序，请省略 sort 参数：\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;sales_per_month\u0026quot;: { \u0026quot;date_histogram\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;@timestamp\u0026quot;, \u0026quot;calendar_interval\u0026quot;: \u0026quot;month\u0026quot; }, \u0026quot;aggs\u0026quot;: { \u0026quot;total_bytes\u0026quot;: { \u0026quot;sum\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;bytes\u0026quot; } }, \u0026quot;bytes_bucket_sort\u0026quot;: { \u0026quot;bucket_sort\u0026quot;: { \u0026quot;from\u0026quot;: 1, \u0026quot;size\u0026quot;: 2 } } } } } }   ","subcategory":null,"summary":"","tags":null,"title":"分组排序聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/pipeline-aggregations/bucket-sort/"},{"category":null,"content":"写入数据文本向量化 #  Easysearch 使用摄取管道 ingest pipeline 中的一系列处理器，可以对写入的数据进行处理，并且支持对文本进行向量化，本文档介绍如何在 Easysearch 中使用 text_embedding 处理器对写入数据进行向量化。\n先决条件 #  支持与 OpenAI API 兼容的 embedding 接口，支持 Ollama embedding 接口。\n需要安装 Easysearch 的 knn 和 ai 插件。\n在生产环境中使用数据采集时，您的集群应至少包含一个节点，且该节点的节点角色权限设置为 ingest 。\n创建带有向量字段的索引 #  首先，需要创建一个包含 knn mapping 的索引，text_vector 是存储向量的字段，向量维度是 768。\nPUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;text_vector\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;knn_dense_float_vector\u0026quot;, \u0026quot;knn\u0026quot;: { \u0026quot;dims\u0026quot;: 768, \u0026quot;model\u0026quot;: \u0026quot;lsh\u0026quot;, \u0026quot;similarity\u0026quot;: \u0026quot;cosine\u0026quot;, \u0026quot;L\u0026quot;: 99, \u0026quot;k\u0026quot;: 1 } } } } } 创建或更新 text_embedding 处理器 #  请求路径：\nPUT _ingest/pipeline/\u0026lt;pipeline-id\u0026gt; 请求示例：\nPUT _ingest/pipeline/text-embedding-pipeline { \u0026quot;description\u0026quot;: \u0026quot;用于生成文本嵌入向量的管道\u0026quot;, \u0026quot;processors\u0026quot;: [ { \u0026quot;text_embedding\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;https://api.openai.com/v1/embeddings\u0026quot;, \u0026quot;vendor\u0026quot;: \u0026quot;openai\u0026quot;, \u0026quot;api_key\u0026quot;: \u0026quot;\u0026lt;api_key\u0026gt;\u0026quot;, \u0026quot;text_field\u0026quot;: \u0026quot;input_text\u0026quot;, \u0026quot;vector_field\u0026quot;: \u0026quot;text_vector\u0026quot;, \u0026quot;model_id\u0026quot;: \u0026quot;text-embedding-3-small\u0026quot;, \u0026quot;dims\u0026quot;: 768, \u0026quot;ignore_missing\u0026quot;: false, \u0026quot;ignore_failure\u0026quot;: false } } ] } 请求体字段： #  下表列出了用于创建或更新管道的请求体字段。\n   参数 是否必填 类型 说明     processors 必填 数组 处理器列表，按顺序执行。示例中仅包含 text_embedding 处理器。   description 可选 字符串 管道的描述信息，用于说明用途（如“生成文本嵌入向量”）。   url 必填 字符串 Embedding API 的完整 URL（如 https://api.openai.com/v1/embeddings ）。   vendor 必填 字符串 服务提供商标识，固定为 \u0026quot;openai\u0026quot; 或 \u0026quot;ollama\u0026quot;。   api_key 必填 字符串 API 密钥，需替换为实际值（如 \u0026quot;sk-xxx\u0026quot;）。   text_field 必填 字符串 输入文本的字段名（如 \u0026quot;input_text\u0026quot;），需与索引中的字段匹配。   vector_field 必填 字符串 存储向量的目标字段名（如 \u0026quot;text_vector\u0026quot;），必须包含在索引的 knn mapping 里。   model_id 必填 字符串 模型名称（如 \u0026quot;text-embedding-3-small\u0026quot; 或 \u0026quot;text-embedding-v3\u0026quot;）。   dims 必填 整数 向量维度（需与所选模型匹配，如 768 对应 text-embedding-3-small）,并与 mapping 里的 dims 保持一致。   ignore_missing 可选 布尔值 若 text_field 不存在是否跳过处理（默认 false）。   ignore_failure 可选 布尔值 若处理失败是否继续执行后续处理器（默认 false）。    若使用国内大模型，例如阿里云的千问，需替换为\nurl: https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings model_id: text-embedding-v3 dims: 根据模型确定\n路径参数： #     参数 是否必填 类型 说明     pipeline-id 必填 字符串 分配给管道的唯一标识符，即管道 ID。    若已部署 Ollama 服务，按下面示例使用：\nPUT _ingest/pipeline/ollama-embedding-pipeline { \u0026quot;description\u0026quot;: \u0026quot;Ollama embedding 示例\u0026quot;, \u0026quot;processors\u0026quot;: [ { \u0026quot;text_embedding\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://localhost:11434/api/embed\u0026quot;, \u0026quot;vendor\u0026quot;: \u0026quot;ollama\u0026quot;, \u0026quot;text_field\u0026quot;: \u0026quot;input_text\u0026quot;, \u0026quot;vector_field\u0026quot;: \u0026quot;text_vector\u0026quot;, \u0026quot;model_id\u0026quot;: \u0026quot;nomic-embed-text:latest\u0026quot;, \u0026quot;ignore_missing\u0026quot;: false, \u0026quot;ignore_failure\u0026quot;: false } } ] } 查看 text_embedding 处理器 #  查看 text_embedding 处理器与查看其他处理器的 api 一致：\nGET _ingest/pipeline 返回输出：\n{ \u0026quot;ollama-embedding-pipeline\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;Ollama embedding 示例\u0026quot;, \u0026quot;processors\u0026quot;: [ { \u0026quot;text_embedding\u0026quot;: { \u0026quot;ignore_failure\u0026quot;: false, \u0026quot;vendor\u0026quot;: \u0026quot;ollama\u0026quot;, \u0026quot;vector_field\u0026quot;: \u0026quot;text_vector\u0026quot;, \u0026quot;text_field\u0026quot;: \u0026quot;input_text\u0026quot;, \u0026quot;ignore_missing\u0026quot;: false, \u0026quot;model_id\u0026quot;: \u0026quot;nomic-embed-text:latest\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://localhost:11434/api/embed\u0026quot; } } ] }, \u0026quot;text-embedding-pipeline\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;用于生成文本嵌入向量的管道\u0026quot;, \u0026quot;processors\u0026quot;: [ { \u0026quot;text_embedding\u0026quot;: { \u0026quot;ignore_failure\u0026quot;: false, \u0026quot;dims\u0026quot;: 768, \u0026quot;api_key\u0026quot;: \u0026quot;*************************************************vE\u0026quot;, \u0026quot;vendor\u0026quot;: \u0026quot;openai\u0026quot;, \u0026quot;vector_field\u0026quot;: \u0026quot;text_vector\u0026quot;, \u0026quot;text_field\u0026quot;: \u0026quot;input_text\u0026quot;, \u0026quot;ignore_missing\u0026quot;: false, \u0026quot;model_id\u0026quot;: \u0026quot;text-embedding-3-small\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://poloai.top/v1/embeddings\u0026quot; } } ] } } api_key 会被遮掩，防止泄露\n使用 text_embedding 处理器进行写入时转换 #  与使用其他处理器一致,\nPOST /_bulk?pipeline=text-embedding-pipeline\u0026amp;pretty\u0026amp;refresh=wait_for { \u0026quot;index\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;my-index\u0026quot; } } { \u0026quot;input_text\u0026quot;: \u0026quot;第一个批量处理的文本。\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;bulk api example 1\u0026quot; } { \u0026quot;index\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;my-index\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;bulk_doc_2\u0026quot; } } { \u0026quot;input_text\u0026quot;: \u0026quot;第二个批量处理的文本，指定了ID。\u0026quot;, \u0026quot;priority\u0026quot;: 1 } { \u0026quot;index\u0026quot;: { \u0026quot;_index\u0026quot;: \u0026quot;my-index\u0026quot; } } { \u0026quot;input_text\u0026quot;: \u0026quot;这是另一示例文本。\u0026quot;, \u0026quot;tags\u0026quot;: [\u0026quot;bulk\u0026quot;, \u0026quot;test\u0026quot;] } 查看写入结果\nGET my-index/_search 返回输出：\n{ \u0026quot;took\u0026quot;: 1, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 3, \u0026quot;relation\u0026quot;: \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot;: 1, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;dd2DAZgB6WXmvHRNYksX\u0026quot;, \u0026quot;_score\u0026quot;: 1, \u0026quot;_source\u0026quot;: { \u0026quot;input_text\u0026quot;: \u0026quot;第一个批量处理的文本。\u0026quot;, \u0026quot;text_vector\u0026quot;: [ 0.012978158, 0.007739224, -0.015867598, 0.005287578, ...... 删除 text_embedding 处理器 #\n 与删除其他处理器一致\nDELETE _ingest/pipeline/text-embedding-pipeline ","subcategory":null,"summary":"","tags":null,"title":"写入数据文本向量化","url":"/easysearch/v1.15.0/docs/references/ai-integration/ingest-text-embedding/"},{"category":null,"content":"关键词重复分词过滤器 #  关键词重复（keyword_repeat）分词过滤器会将词元的关键词版本发松到词元流中。当你希望在经过进一步的词元转换（例如词干提取或同义词扩展）之后，既保留原始词元，又保留其修改后的版本时，通常会使用这个过滤器。这些重复的词元使得原始的、未改变的词元版本能够与修改后的版本一起保留在最终的分析结果中。\n 关键词重复分词过滤器应该放置在词干提取过滤器之前。词干提取并非应用于每个词元，因此在词干提取之后，你可能会在同一位置得到重复的词元。为了去除重复词元，可以在词干提取器之后使用 remove_duplicates 分词过滤器。\n 参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词重复过滤器的分词器：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_kstem\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;kstem\u0026quot; }, \u0026quot;my_lowercase\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;lowercase\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;my_lowercase\u0026quot;, \u0026quot;keyword_repeat\u0026quot;, \u0026quot;my_kstem\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Stopped quickly\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;stop\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;quickly\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;quick\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } 你可以通过在 _analyze 查询中添加以下参数，进一步研究关键词重复分词过滤器的影响：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Stopped quickly\u0026quot;, \u0026quot;explain\u0026quot;: true, \u0026quot;attributes\u0026quot;: \u0026quot;keyword\u0026quot; } 返回内容中包含了详细信息，例如分词情况、过滤操作以及特定分词过滤器的应用情况：\n{ \u0026quot;detail\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: true, \u0026quot;charfilters\u0026quot;: [], \u0026quot;tokenizer\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Stopped\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;quickly\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] }, \u0026quot;tokenfilters\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;my_lowercase\u0026quot;, \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;quickly\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] }, { \u0026quot;name\u0026quot;: \u0026quot;keyword_repeat\u0026quot;, \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0, \u0026quot;keyword\u0026quot;: true }, { \u0026quot;token\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0, \u0026quot;keyword\u0026quot;: false }, { \u0026quot;token\u0026quot;: \u0026quot;quickly\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1, \u0026quot;keyword\u0026quot;: true }, { \u0026quot;token\u0026quot;: \u0026quot;quickly\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1, \u0026quot;keyword\u0026quot;: false } ] }, { \u0026quot;name\u0026quot;: \u0026quot;my_kstem\u0026quot;, \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;stopped\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0, \u0026quot;keyword\u0026quot;: true }, { \u0026quot;token\u0026quot;: \u0026quot;stop\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0, \u0026quot;keyword\u0026quot;: false }, { \u0026quot;token\u0026quot;: \u0026quot;quickly\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1, \u0026quot;keyword\u0026quot;: true }, { \u0026quot;token\u0026quot;: \u0026quot;quick\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1, \u0026quot;keyword\u0026quot;: false } ] } ] } } ","subcategory":null,"summary":"","tags":null,"title":"关键词重复分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/keyword-repeat/"},{"category":null,"content":"关键词词元生成器 #  关键字（keyword）词元生成器接收文本并将其原封不动地作为单个词元输出。当你希望输入内容保持完整时，比如在管理像姓名、产品代码或电子邮件地址这类结构化数据时，这个词元生成器就特别有用。\n关键字词元生成器可以与词元过滤器搭配使用来处理文本。例如，对文本进行规范化处理或去除多余的字符。\n参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用关键字词元生成器的分词器：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_keyword_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_keyword_analyzer\u0026quot; } } } } 生成的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_keyword_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Easysearch Example\u0026quot; } 返回内容会是包含原始内容的单个词元：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Easysearch Example\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 18, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } 参数说明 #  关键字词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     buffer_size 可选 整数 确定字符缓冲区的大小。默认值为 256。通常无需更改此设置。    将关键字词元生成器与词元过滤器结合使用 #  为了增强关键字词元生成器的功能，你可以将它与词元过滤器结合起来。词元过滤器能够对文本进行转换，例如将文本转换为小写形式或者移除不需要的字符。\n示例：使用匹配替换（pattern_replace）词元过滤器和关键字词元生成器 #  在这个示例中，匹配替换（pattern_replace）词元过滤器使用正则表达式，会将所有非字母数字字符替换为空字符串：\nPOST _analyze { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;pattern_replace\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;[^a-zA-Z0-9]\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;\u0026quot; } ], \u0026quot;text\u0026quot;: \u0026quot;Product#1234-XYZ\u0026quot; } pattern_replace词元过滤器会移除非字母数字字符，并返回以下词元：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Product1234XYZ\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 16, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"关键词词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/keyword/"},{"category":null,"content":"关键词标记分词过滤器 #  关键词标记（keyword_marker）分词过滤器用于防止某些词元被词干提取器或其他过滤器修改。该过滤器通过将指定的词元标记为关键词来实现这一点，这样就能避免词干提取或其他方式的处理。这确保了特定的单词能保持其原始形式。\n参数说明 #  关键词标记分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     ignore_case 可选 布尔值 在匹配关键词时是否忽略字母大小写。默认值为false。   keywords 若未设置 keywords_path 或 keywords_pattern 则为必需 字符串列表 要标记为关键词的词元列表。   keywords_path 若未设置 keywords 或 keywords_pattern 则为必需 字符串 关键词列表的路径（相对于配置目录或绝对路径）。   keywords_pattern 若未设置 keywords 或 keywords_path 则为必需 字符串 用于匹配要标记为关键词的词元的正则表达式。    参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个带有关键词标记过滤器的分词器。该过滤器将单词 example 标记为关键词：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;, \u0026quot;keyword_marker_filter\u0026quot;, \u0026quot;stemmer\u0026quot;] } }, \u0026quot;filter\u0026quot;: { \u0026quot;keyword_marker_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword_marker\u0026quot;, \u0026quot;keywords\u0026quot;: [\u0026quot;example\u0026quot;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Favorite example\u0026quot; } 返回中包含了生成的词元。请注意，虽然单词 favorite 进行了词干提取，但单词 example 未进行词干提取，因为它被标记为了关键词。\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;favorit\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;example\u0026quot;, \u0026quot;start_offset\u0026quot;: 9, \u0026quot;end_offset\u0026quot;: 16, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } 你可以通过在 _analyze 查询中添加以下参数，进一步研究关键词标记分词过滤器的影响：\nGET /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;This is an Easysearch example demonstrating keyword marker.\u0026quot;, \u0026quot;explain\u0026quot;: true, \u0026quot;attributes\u0026quot;: \u0026quot;keyword\u0026quot; } 这将在返回内容中生成类似如下的更多详细信息：\n{ \u0026quot;name\u0026quot;: \u0026quot;porter_stem\u0026quot;, \u0026quot;tokens\u0026quot;: [ ... { \u0026quot;token\u0026quot;: \u0026quot;example\u0026quot;, \u0026quot;start_offset\u0026quot;: 22, \u0026quot;end_offset\u0026quot;: 29, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4, \u0026quot;keyword\u0026quot;: true }, { \u0026quot;token\u0026quot;: \u0026quot;demonstr\u0026quot;, \u0026quot;start_offset\u0026quot;: 30, \u0026quot;end_offset\u0026quot;: 43, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5, \u0026quot;keyword\u0026quot;: false }, ... ] } ","subcategory":null,"summary":"","tags":null,"title":"关键词标记分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/keyword-marker/"},{"category":null,"content":"关键词分词器 #  关键词（keyword）分词器根本不会对文本进行分词。相反，它将整个输入视为单个词元，不会将其拆分成单个的词项。关键词分词器常用于包含电子邮件地址、网址或产品 ID 的字段，以及其他不需要进行分词的情况。\n参考样例 #  以下命令创建一个名为 my_keyword_index 使用关键词分词器的索引：\nPUT /my_keyword_index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;keyword\u0026quot; } } } } 配置自定义分词器 #  以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于关键词分词器：\nPUT /my_custom_keyword_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_keyword_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot; } } } } } 产生的词元 #  以下请求来检查使用该分词器生成的词元：\nPOST /my_custom_keyword_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_keyword_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Just one token\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Just one token\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 14, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"关键词分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/keyword-analyzer/"},{"category":null,"content":"全局聚合 #  global 全局聚合让你能跳出过滤聚合的聚合上下文。即使你包含了一个缩小文档集的过滤查询， global 聚合仍然对所有文档进行聚合，就好像过滤查询不存在一样。它忽略 filter 聚合，并隐式地假设 match_all 查询。\n以下示例返回索引中所有文档的 taxful_total_price 字段的 avg 值：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;query\u0026quot;: { \u0026quot;range\u0026quot;: { \u0026quot;taxful_total_price\u0026quot;: { \u0026quot;lte\u0026quot;: 50 } } }, \u0026quot;aggs\u0026quot;: { \u0026quot;total_avg_amount\u0026quot;: { \u0026quot;global\u0026quot;: {}, \u0026quot;aggs\u0026quot;: { \u0026quot;avg_price\u0026quot;: { \u0026quot;avg\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot; } } } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;total_avg_amount\u0026quot; : { \u0026quot;doc_count\u0026quot; : 4675, \u0026quot;avg_price\u0026quot; : { \u0026quot;value\u0026quot; : 75.05542864304813 } } } } 你可以看到， taxful_total_price 字段的平均值是 75.05，而不是 filter 示例中查询匹配时看到的 38.36。\n","subcategory":null,"summary":"","tags":null,"title":"全局聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/global/"},{"category":null,"content":"停用词分词过滤器 #  停用词（stop）分词过滤器用于在分词过程中从词元流中去除常见词汇（也称为停用词）。停用词通常是冠词和介词，比如“a” 或 “for” 。这些词在搜索查询中意义不大，并且常常被排除在外，以提高搜索效率和相关性。\n默认的英语停用词列表包含以下单词：a、an、and、are、as、at、be、but、by、for、if、in、into、is、it、no、not、of、on、or、such、that、the、their、then、there、these、they、this、to、was、will 以及 with。\n参数说明 #  停用词分词过滤器可以使用以下参数进行配置：\n   参数 必需/可选 数据类型 描述     stopwords 可选 字符串 既可以指定自定义的停用词数组，也可以指定一种语言以获取预定义的 Lucene 停用词列表。可指定的语言如下：\n- _arabic_\n- _armenian_\n- _basque_\n- _bengali_\n- _brazilian_（巴西葡萄牙语）\n- _bulgarian_\n- _catalan_\n- _cjk_（中文、日语和韩语）\n- _czech_\n- _danish_\n- _dutch_\n- _english_（默认值）\n- _estonian_\n- _finnish_\n- _french_\n- _galician_\n- _german_\n- _greek_\n- _hindi_\n- _hungarian_\n- _indonesian_\n- _irish_\n- _italian_\n- _latvian_\n- _lithuanian_\n- _norwegian_\n- _persian_\n- _portuguese_\n- _romanian_\n- _russian_\n- _sorani_\n- _spanish_\n- _swedish_\n- _thai_\n- _turkish_   stopwords_path 可选 字符串 指定包含自定义停用词的文件的路径（可以是绝对路径，也可以是相对于配置目录的相对路径）。   ignore_case 可选 布尔值 如果设置为 true，则在匹配停用词时不区分大小写。默认值为 false。   remove_trailing 可选 布尔值 如果设置为 true，则在分析过程中会移除末尾的停用词。默认值为 true。    参考样例 #  以下示例请求创建了一个名为 my-stopword-index 的新索引，并配置了一个带有停用词过滤器的分词器，该过滤器使用预定义的英语停用词列表。\nPUT /my-stopword-index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_stop_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;stop\u0026quot;, \u0026quot;stopwords\u0026quot;: \u0026quot;_english_\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_stop_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_stop_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my-stopword-index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_stop_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;A quick dog jumps over the turtle\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;quick\u0026quot;, \u0026quot;start_offset\u0026quot;: 2, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;dog\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;jumps\u0026quot;, \u0026quot;start_offset\u0026quot;: 12, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;over\u0026quot;, \u0026quot;start_offset\u0026quot;: 18, \u0026quot;end_offset\u0026quot;: 22, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;, \u0026quot;start_offset\u0026quot;: 27, \u0026quot;end_offset\u0026quot;: 33, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 6 } ] } ","subcategory":null,"summary":"","tags":null,"title":"停用词分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/stop/"},{"category":null,"content":"停用词分词器 #  停用词（stop）分词器会在文本中移除预定义的停用词。该分词器由一个小写词元生成器和一个停用词词元过滤器组成。\n参数说明 #  你可以使用以下参数来配置一个停用词分词器。\n   参数 必填/可选 数据类型 描述     stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。   stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。    参考样例 #  以下命令创建一个名为 my_stop_index 并使用停词器分词器的索引：\nPUT /my_stop_index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;stop\u0026quot; } } } } 配置自定义分词器 #  以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于停用词分词器：\nPUT /my_custom_stop_analyzer_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_stop_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;lowercase\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;stop\u0026quot; ] } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;my_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_stop_analyzer\u0026quot; } } } } 产生的词元 #  以下请求用来检查分词器生成的词元：\nPOST /my_custom_stop_analyzer_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_stop_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;The large turtle is green and brown\u0026quot; } 返回内容中包含了产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;large\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 9, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;turtle\u0026quot;, \u0026quot;start_offset\u0026quot;: 10, \u0026quot;end_offset\u0026quot;: 16, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;green\u0026quot;, \u0026quot;start_offset\u0026quot;: 20, \u0026quot;end_offset\u0026quot;: 25, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;brown\u0026quot;, \u0026quot;start_offset\u0026quot;: 30, \u0026quot;end_offset\u0026quot;: 35, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 6 } ] } 指定停用词 #  下面演示了怎么去指定一个自定义的停用词列表：\nPUT /my_new_custom_stop_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_stop_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;stop\u0026quot;, \u0026quot;stopwords_path\u0026quot;: \u0026quot;stopwords.txt\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_stop_analyzer\u0026quot; } } } } 在这个例子中，该文件位于配置目录中。你也可以指定该文件的完整路径。\n","subcategory":null,"summary":"","tags":null,"title":"停用词分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/stop-analyzer/"},{"category":null,"content":"值计数聚合 #  value_count 值计数聚合是一个单值指标聚合，用于计算聚合所基于的值的数量。 例如，您可以将 value_count 指标与 avg 指标一起使用来查找聚合使用多少个数字来计算平均值。\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;number_of_values\u0026quot;: { \u0026quot;value_count\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;taxful_total_price\u0026quot; } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;number_of_values\u0026quot; : { \u0026quot;value\u0026quot; : 4675 } } }   ","subcategory":null,"summary":"","tags":null,"title":"值计数聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/value-count/"},{"category":null,"content":"修剪分词过滤器 #  修剪(trim)分词过滤器会从词元中去除前导和尾随的空白字符。\n 许多常用的分词器，例如标准(standard)分词器、关键字(keyword)分词器和空白(whitespace)分词器，在分词过程中会自动去除前导和尾随的空白字符。当使用这些分词器时，无需额外配置修剪分词过滤器。\n 参考样例 #  以下示例请求创建了一个名为 my_pattern_trim_index 的新索引，并配置了一个带有修剪过滤器和匹配词元生成器的分词器。其中，匹配词元生成器不会去除词元的前导和尾随空白字符。\nPUT /my_pattern_trim_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;my_trim_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;trim\u0026quot; } }, \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_pattern_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;,\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_pattern_trim_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_pattern_tokenizer\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_trim_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /my_pattern_trim_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_pattern_trim_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot; Easysearch , is , powerful \u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 12, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;, \u0026quot;start_offset\u0026quot;: 13, \u0026quot;end_offset\u0026quot;: 18, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;powerful\u0026quot;, \u0026quot;start_offset\u0026quot;: 19, \u0026quot;end_offset\u0026quot;: 32, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"修剪分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/trim/"},{"category":null,"content":"保留类型分词过滤器 #  保留类型（keep_types）分词过滤器是一种用于文本分析的分词过滤器，它可用于控制保留或丢弃哪些词元类型。不同的分词器会产生不同的词元类型，例如 \u0026lt;HOST\u0026gt;、\u0026lt;NUM\u0026gt; 或 \u0026lt;ALPHANUM\u0026gt;。\n 分词器keyword）、简单匹配（simple_pattern）和简单匹配拆分（simple_pattern_split）分词器不支持保留类型分词过滤器，因为这些分词器不支持词元类型属性。\n 参数说明 #  保留类型分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     types 必需 字符串列表 要保留或丢弃的词元类型列表（由 mode 参数决定）。   mode 可选 字符串 是要包含(include)还是排除(exclude) types 中指定的词元类型。默认值为 include（包含）。    参考样例 #  以下示例请求创建了一个名为 test_index 的新索引，并配置了一个带有保留类型过滤器的分析器：\nPUT /test_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;, \u0026quot;keep_types_filter\u0026quot;] } }, \u0026quot;filter\u0026quot;: { \u0026quot;keep_types_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keep_types\u0026quot;, \u0026quot;types\u0026quot;: [\u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nGET /test_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Hello 2 world! This is an example.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;hello\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 5, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;world\u0026quot;, \u0026quot;start_offset\u0026quot;: 8, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;this\u0026quot;, \u0026quot;start_offset\u0026quot;: 15, \u0026quot;end_offset\u0026quot;: 19, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;is\u0026quot;, \u0026quot;start_offset\u0026quot;: 20, \u0026quot;end_offset\u0026quot;: 22, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;an\u0026quot;, \u0026quot;start_offset\u0026quot;: 23, \u0026quot;end_offset\u0026quot;: 25, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 }, { \u0026quot;token\u0026quot;: \u0026quot;example\u0026quot;, \u0026quot;start_offset\u0026quot;: 26, \u0026quot;end_offset\u0026quot;: 33, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 6 } ] } ","subcategory":null,"summary":"","tags":null,"title":"保留类型分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/keep-types/"},{"category":null,"content":"中位数绝对偏差聚合 #  median_absolute_deviation 中位数绝对偏差聚合是一个单值指标聚合。中位数绝对偏差是一种变异性指标，用于衡量相对于中位数的离散程度。\n与依赖平方误差项的标准偏差相比，中位数绝对偏差受异常值的影响较小，适用于描述非正态分布的数据。\n中位数绝对偏差按以下方式计算：\nmedian_absolute_deviation = median( | x\u0026lt;sub\u0026gt;i\u0026lt;/sub\u0026gt; - median(x\u0026lt;sub\u0026gt;i\u0026lt;/sub\u0026gt;) | )\n由于内存限制，Easysearch 估计 median_absolute_deviation ，而不是直接计算它。这种估计在计算上很昂贵。您可以调整估计精度和性能之间的权衡。有关更多信息，请参阅调整估计精度。\n参数说明 #  median_absolute_deviation 聚合采用以下参数。\n   参数 必需/可选 数据类型 描述     field 必需 String 要计算中位数绝对偏差的数值字段的名称。   missing 可选 Numeric 分配给字段缺失实例的值。如果未提供，则从估计中排除具有缺失值的文档。   compression 可选 Numeric 一个调整估计精度和性能之间平衡的参数。 compression 的值必须大于 0 。默认值为 1000 。    参考样例 #  以下示例计算数据集中 DistanceMiles 字段的绝对中位数偏差：\nGET sample_data_flights/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;median_absolute_deviation_DistanceMiles\u0026quot;: { \u0026quot;median_absolute_deviation\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;DistanceMiles\u0026quot; } } } } 返回内容 #  如以下返回内容所示，聚合返回了 median_absolute_deviation_DistanceMiles 变量中绝对中位数偏差的估计值：\n{ \u0026quot;took\u0026quot;: 490, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;median_absolute_deviation_DistanceMiles\u0026quot;: { \u0026quot;value\u0026quot;: 1830.917892238693 } } } 缺省值 #\n 在计算 median_absolute_deviation 时会忽略缺失值和空值。你可以为聚合字段的缺失实例分配一个值。有关更多信息，请参阅缺失聚合。\n调整估计精度 #  中位数绝对偏差使用 t-digest 数据结构进行计算，该结构采用一个 compression 参数来平衡性能和估计精度。 compression 的较低值可以提高性能，但可能会降低估计精度，如下面的请求所示：\nGET sample_data_flights/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;median_absolute_deviation_DistanceMiles\u0026quot;: { \u0026quot;median_absolute_deviation\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;DistanceMiles\u0026quot;, \u0026quot;compression\u0026quot;: 10 } } } } 估计误差取决于数据集，但通常低于 5%，即使对于值低至 100 的 compression 也是如此。（此处使用的低示例值 10 是为了说明权衡效应，并不推荐。）\n请注意，在以下返回中，计算时间（ took 时间）有所减少，并且估计参数的值略有下降。\n作为参考，Easysearch 的最佳估计（将 compression 任意设置得非常高）对于 DistanceMiles 的中值绝对偏差是 1831.076904296875 ：\n{ \u0026quot;took\u0026quot;: 1, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;aggregations\u0026quot;: { \u0026quot;median_absolute_deviation_DistanceMiles\u0026quot;: { \u0026quot;value\u0026quot;: 1836.265614211182 } } } ","subcategory":null,"summary":"","tags":null,"title":"中位数绝对偏差聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/median-absolute-deviation/"},{"category":null,"content":"UAX URL 邮件词元生成器 #  除了常规文本之外，UAX URL 邮件（uax_url_email）词元生成器还专门用于处理网址、电子邮件地址和域名。它基于 Unicode 文本分割算法（ UAX #29），这使得它能够正确地对复杂文本（包括网址和电子邮件地址）进行分词处理。\n参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 UAX URL 邮件词元生成器的分词器：\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;uax_url_email_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;uax_url_email\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_uax_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;uax_url_email_tokenizer\u0026quot; } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_uax_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_uax_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Contact us at support@example.com or visit https://example.com for details.\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ {\u0026quot;token\u0026quot;: \u0026quot;Contact\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 7,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;us\u0026quot;,\u0026quot;start_offset\u0026quot;: 8,\u0026quot;end_offset\u0026quot;: 10,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 1}, {\u0026quot;token\u0026quot;: \u0026quot;at\u0026quot;,\u0026quot;start_offset\u0026quot;: 11,\u0026quot;end_offset\u0026quot;: 13,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;support@example.com\u0026quot;,\u0026quot;start_offset\u0026quot;: 14,\u0026quot;end_offset\u0026quot;: 33,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;EMAIL\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;or\u0026quot;,\u0026quot;start_offset\u0026quot;: 34,\u0026quot;end_offset\u0026quot;: 36,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;visit\u0026quot;,\u0026quot;start_offset\u0026quot;: 37,\u0026quot;end_offset\u0026quot;: 42,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 5}, {\u0026quot;token\u0026quot;: \u0026quot;https://example.com\u0026quot;,\u0026quot;start_offset\u0026quot;: 43,\u0026quot;end_offset\u0026quot;: 62,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;URL\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 6}, {\u0026quot;token\u0026quot;: \u0026quot;for\u0026quot;,\u0026quot;start_offset\u0026quot;: 63,\u0026quot;end_offset\u0026quot;: 66,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 7}, {\u0026quot;token\u0026quot;: \u0026quot;details\u0026quot;,\u0026quot;start_offset\u0026quot;: 67,\u0026quot;end_offset\u0026quot;: 74,\u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;,\u0026quot;position\u0026quot;: 8} ] } 参数说明 #  UAX URL 邮件词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     max_token_length 可选 整数 设置生成词元的最大长度。若超过该长度，词元将按照 max_token_length 所配置的长度拆分为多个词元。默认值为 255。    ","subcategory":null,"summary":"","tags":null,"title":"UAX URL 邮件词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/uax-url-email/"},{"category":null,"content":"Top 汇总 #  top_hits 指标是一种多值指标汇总，它根据聚合字段的相关性评分对匹配文档进行排名。\n您可以指定以下选项：\n from : 命中的起始位置。 size : 返回命中的最大数量。默认值为 3。 sort : 匹配命中的排序方式。默认情况下，命中的排序依据聚合查询的相关性得分。  以下示例返回数据中的前 5 个产品：\nGET sample_data_ecommerce/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;top_hits_products\u0026quot;: { \u0026quot;top_hits\u0026quot;: { \u0026quot;size\u0026quot;: 5 } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;top_hits_products\u0026quot; : { \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 4675, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.0, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;sample_data_ecommerce\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;glMlwXcBQVLeQPrkHPtI\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;category\u0026quot; : [ \u0026quot;Women's Accessories\u0026quot;, \u0026quot;Women's Clothing\u0026quot; ], \u0026quot;currency\u0026quot; : \u0026quot;EUR\u0026quot;, \u0026quot;customer_first_name\u0026quot; : \u0026quot;rania\u0026quot;, \u0026quot;customer_full_name\u0026quot; : \u0026quot;rania Evans\u0026quot;, \u0026quot;customer_gender\u0026quot; : \u0026quot;FEMALE\u0026quot;, \u0026quot;customer_id\u0026quot; : 24, \u0026quot;customer_last_name\u0026quot; : \u0026quot;Evans\u0026quot;, \u0026quot;customer_phone\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;day_of_week\u0026quot; : \u0026quot;Sunday\u0026quot;, \u0026quot;day_of_week_i\u0026quot; : 6, \u0026quot;email\u0026quot; : \u0026quot;rania@evans-family.zzz\u0026quot;, \u0026quot;manufacturer\u0026quot; : [ \u0026quot;Tigress Enterprises\u0026quot; ], \u0026quot;order_date\u0026quot; : \u0026quot;2021-02-28T14:16:48+00:00\u0026quot;, \u0026quot;order_id\u0026quot; : 583581, \u0026quot;products\u0026quot; : [ { \u0026quot;base_price\u0026quot; : 10.99, \u0026quot;discount_percentage\u0026quot; : 0, \u0026quot;quantity\u0026quot; : 1, \u0026quot;manufacturer\u0026quot; : \u0026quot;Tigress Enterprises\u0026quot;, \u0026quot;tax_amount\u0026quot; : 0, \u0026quot;product_id\u0026quot; : 19024, \u0026quot;category\u0026quot; : \u0026quot;Women's Accessories\u0026quot;, \u0026quot;sku\u0026quot; : \u0026quot;ZO0082400824\u0026quot;, \u0026quot;taxless_price\u0026quot; : 10.99, \u0026quot;unit_discount_amount\u0026quot; : 0, \u0026quot;min_price\u0026quot; : 5.17, \u0026quot;_id\u0026quot; : \u0026quot;sold_product_583581_19024\u0026quot;, \u0026quot;discount_amount\u0026quot; : 0, \u0026quot;created_on\u0026quot; : \u0026quot;2016-12-25T14:16:48+00:00\u0026quot;, \u0026quot;product_name\u0026quot; : \u0026quot;Snood - white/grey/peach\u0026quot;, \u0026quot;price\u0026quot; : 10.99, \u0026quot;taxful_price\u0026quot; : 10.99, \u0026quot;base_unit_price\u0026quot; : 10.99 }, { \u0026quot;base_price\u0026quot; : 32.99, \u0026quot;discount_percentage\u0026quot; : 0, \u0026quot;quantity\u0026quot; : 1, \u0026quot;manufacturer\u0026quot; : \u0026quot;Tigress Enterprises\u0026quot;, \u0026quot;tax_amount\u0026quot; : 0, \u0026quot;product_id\u0026quot; : 19260, \u0026quot;category\u0026quot; : \u0026quot;Women's Clothing\u0026quot;, \u0026quot;sku\u0026quot; : \u0026quot;ZO0071900719\u0026quot;, \u0026quot;taxless_price\u0026quot; : 32.99, \u0026quot;unit_discount_amount\u0026quot; : 0, \u0026quot;min_price\u0026quot; : 17.15, \u0026quot;_id\u0026quot; : \u0026quot;sold_product_583581_19260\u0026quot;, \u0026quot;discount_amount\u0026quot; : 0, \u0026quot;created_on\u0026quot; : \u0026quot;2016-12-25T14:16:48+00:00\u0026quot;, \u0026quot;product_name\u0026quot; : \u0026quot;Cardigan - grey\u0026quot;, \u0026quot;price\u0026quot; : 32.99, \u0026quot;taxful_price\u0026quot; : 32.99, \u0026quot;base_unit_price\u0026quot; : 32.99 } ], \u0026quot;sku\u0026quot; : [ \u0026quot;ZO0082400824\u0026quot;, \u0026quot;ZO0071900719\u0026quot; ], \u0026quot;taxful_total_price\u0026quot; : 43.98, \u0026quot;taxless_total_price\u0026quot; : 43.98, \u0026quot;total_quantity\u0026quot; : 2, \u0026quot;total_unique_products\u0026quot; : 2, \u0026quot;type\u0026quot; : \u0026quot;order\u0026quot;, \u0026quot;user\u0026quot; : \u0026quot;rani\u0026quot;, \u0026quot;geoip\u0026quot; : { \u0026quot;country_iso_code\u0026quot; : \u0026quot;EG\u0026quot;, \u0026quot;location\u0026quot; : { \u0026quot;lon\u0026quot; : 31.3, \u0026quot;lat\u0026quot; : 30.1 }, \u0026quot;region_name\u0026quot; : \u0026quot;Cairo Governorate\u0026quot;, \u0026quot;continent_name\u0026quot; : \u0026quot;Africa\u0026quot;, \u0026quot;city_name\u0026quot; : \u0026quot;Cairo\u0026quot; }, \u0026quot;event\u0026quot; : { \u0026quot;dataset\u0026quot; : \u0026quot;sample_ecommerce\u0026quot; } } ... } ] } } } } ","subcategory":null,"summary":"","tags":null,"title":"Top 汇总","url":"/easysearch/v1.15.0/docs/references/aggregation/metric-aggregations/top-hits/"},{"category":null,"content":"Text 字段类型 #  text 字段类型包含经过分词器分析的字符串。它用于全文搜索，因为它允许部分匹配。对多个词条的搜索可以匹配其中的一部分而不是全部。根据分词器的不同，搜索结果可以是大小写不敏感的、词干化的、去除停用词的、应用同义词的等等。\n 如果您需要进行精确值搜索，请将字段映射为 keyword 类型。\n 代码样例 #  创建一个带有 text 字段的映射：\nPUT movies { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;title\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34; } } } } 参数说明 #  下表列出了 text 字段类型接受的参数。所有参数都是可选的。\n   参数 描述     analyzer 用于此字段的分词器。默认情况下，它将在索引时和搜索时使用。要在搜索时覆盖它，请设置 search_analyzer 参数。默认是 standard 分词器，它使用基于语法的分词，并基于 Unicode 文本分段算法。   boost 浮点值，指定此字段对相关性分数的权重。大于 1.0 的值会增加字段的相关性。0.0 到 1.0 之间的值会降低字段的相关性。默认值为 1.0。   eager_global_ordinals 指定是否应在刷新时立即加载全局序号。如果该字段经常用于聚合，此参数应设置为 true。默认值为 false。   fielddata 布尔值，指定是否访问此字段的已分析标记以进行排序、聚合和脚本编写。默认值为 false。   fielddata_frequency_filter JSON 对象，指定仅将文档频率在 min 和 max 值之间的已分析标记加载到内存中（以绝对数字或百分比提供）。频率按段计算。参数：min、max、min_segment_size。默认加载所有已分析的标记。   fields 要以多种方式索引同一个字符串（例如，作为 keyword 和 text），请提供 fields 参数。您可以指定字段的一个版本用于搜索，另一个版本用于排序和聚合。   index 布尔值，指定字段是否应可搜索。默认值为 true。   index_options 指定要存储在索引中用于搜索和突出显示的信息。有效值：docs（仅文档编号）、freqs（文档编号和词频）、positions（文档编号、词频和词位置）、offsets（文档编号、词频、词位置以及开始和结束字符偏移量）。默认值为 positions。   index_phrases 布尔值，指定是否单独索引 2-gram。2-gram 是此字段字符串中两个连续单词的组合。导致精确短语查询更快但索引更大。当不删除停用词时效果最好。默认值为 false。   index_prefixes JSON 对象，指定单独索引词条前缀。前缀中的字符数在 min_chars 和 max_chars 之间（包含）。导致前缀搜索更快但索引更大。可选参数：min_chars、max_chars。默认 min_chars 为 2，max_chars 为 5。   meta 接受此字段的元数据。   norms 布尔值，指定在计算相关性分数时是否应使用字段长度。默认值为 false。   position_increment_gap 当文本字段被分析时，它们被分配位置。如果一个字段包含一个字符串数组，并且这些位置是连续的，这将导致可能跨不同数组元素匹配。为防止这种情况，在连续的数组元素之间插入一个人工间隙。您可以通过指定整数 position_increment_gap 来更改此间隙。注意：如果 slop 大于 position_element_gap，可能会发生跨不同数组元素的匹配。默认值为 100。   similarity 用于计算相关性分数的排名算法。默认值为 BM25。   term_vector 布尔值，指定是否应存储此字段的词向量。默认值为 no。    词向量参数 #  词向量在分析过程中产生。它包含：\n 词条列表 每个词条的序数位置 搜索字符串在字段中的起始和结束字符偏移量 负载 Payloads（如果可用）。每个词条可以有与词条位置相关的自定义二进制数据  term_vector 字段包含一个接受以下参数的 JSON 对象：\n   参数 存储的值     no 无。这是默认值。   yes 字段中的词条。   with_offsets 词条和字符偏移量。   with_positions_offsets 词条、位置和字符偏移量。   with_positions_offsets_payloads 词条、位置、字符偏移量和负载。   with_positions 词条和位置。   with_positions_payloads 词条、位置和负载。     存储位置对于邻近查询很有用。存储字符偏移量对于突出显示很有用。\n 词向量参数示例 #  创建一个带有存储字符偏移量的词向量的文本字段的映射：\nPUT testindex { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;dob\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;text\u0026#34;, \u0026#34;term_vector\u0026#34;: \u0026#34;with_positions_offsets\u0026#34; } } } } 索引一个带有文本字段的文档：\nPUT testindex/_doc/1 { \u0026#34;dob\u0026#34; : \u0026#34;The patient\u0026#39;s date of birth.\u0026#34; } 查询\u0026quot;date of birth\u0026quot;并在原始字段中突出显示它：\nGET testindex/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;dob\u0026#34;: \u0026#34;date of birth\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;dob\u0026#34;: {} } } } 返回内容中的\u0026quot;date of birth\u0026quot;被突出显示：\n{ \u0026#34;took\u0026#34;: 854, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 1, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.8630463, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.8630463, \u0026#34;_source\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;The patient\u0026#39;s date of birth.\u0026#34; }, \u0026#34;highlight\u0026#34;: { \u0026#34;text\u0026#34;: [\u0026#34;The patient\u0026#39;s \u0026lt;em\u0026gt;date\u0026lt;/em\u0026gt; \u0026lt;em\u0026gt;of\u0026lt;/em\u0026gt; \u0026lt;em\u0026gt;birth\u0026lt;/em\u0026gt;.\u0026#34;] } } ] } } ","subcategory":null,"summary":"","tags":null,"title":"Text 字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/string-field-type/text/"},{"category":null,"content":"SQL-JDBC 驱动 #  Easysearch 的 SQL jdbc 驱动程序是一个独立、直连的纯 Java 驱动程序，可将 JDBC 调用转换为 Easysearch SQL。\n安装 #  JDBC 驱动 可以从官网下载：https://release.infinilabs.com/easysearch/archive/plugins/sql-jdbc-1.7.1.jar\n在 gradle 项目中安装 #  需要把sql-jdbc-1.x.x.jar 集成到本地 gradle 项目的libs目录，假设项目名称叫 jdbc-test\n将下载的sql-jdbc jar 包放到 jdbc-test/libs/ 下：\n在 项目 build.gradle 添加依赖\nimplementation files(\u0026lsquo;libs/sql-jdbc-1.0.0.jar\u0026rsquo;)\n完整的build.gradle 配置：\nplugins { id 'java' } group \u0026lsquo;org.example\u0026rsquo; version \u0026lsquo;1.0-SNAPSHOT\u0026rsquo;\nrepositories { mavenCentral() flatDir { dirs \u0026lsquo;libs\u0026rsquo; } }\ndependencies { implementation files(\u0026lsquo;libs/sql-jdbc-1.0.0.jar\u0026rsquo;) testImplementation \u0026lsquo;org.junit.jupiter:junit-jupiter-api:5.7.0\u0026rsquo; testRuntimeOnly \u0026lsquo;org.junit.jupiter:junit-jupiter-engine:5.7.0\u0026rsquo; }\ntest { useJUnitPlatform() } 初始化 #\n  String url = \u0026quot;jdbc:easysearch://https://localhost:9210\u0026quot;; Properties properties = new Properties(); properties.put(\u0026quot;trustStoreLocation\u0026quot;, \u0026quot;/Users/xxxx/instance.jks\u0026quot;); properties.put(\u0026quot;trustStorePassword\u0026quot;, \u0026quot;123456\u0026quot;); properties.put(\u0026quot;hostnameVerification\u0026quot;, \u0026quot;false\u0026quot;); properties.put(\u0026quot;user\u0026quot;, \u0026quot;admin\u0026quot;); properties.put(\u0026quot;password\u0026quot;, \u0026quot;admin\u0026quot;); properties.put(\u0026quot;ssl\u0026quot;, \u0026quot;true\u0026quot;); Connection con = DriverManager.getConnection(url, properties); Statement st = con.createStatement();  查询示例 #\n 事先创建 accounts 索引并插入几条示例数据\nPOST accounts/_bulk {\u0026quot;index\u0026quot;:{\u0026quot;_id\u0026quot;:\u0026quot;1\u0026quot;}} {\u0026quot;account_number\u0026quot;:1,\u0026quot;balance\u0026quot;:39225,\u0026quot;firstname\u0026quot;:\u0026quot;Amber\u0026quot;,\u0026quot;lastname\u0026quot;:\u0026quot;Duke\u0026quot;,\u0026quot;age\u0026quot;:32,\u0026quot;gender\u0026quot;:\u0026quot;M\u0026quot;,\u0026quot;address\u0026quot;:\u0026quot;880 Holmes Lane\u0026quot;,\u0026quot;employer\u0026quot;:\u0026quot;Pyrami\u0026quot;,\u0026quot;email\u0026quot;:\u0026quot;amberduke@pyrami.com\u0026quot;,\u0026quot;city\u0026quot;:\u0026quot;Brogan\u0026quot;,\u0026quot;state\u0026quot;:\u0026quot;IL\u0026quot;} {\u0026quot;index\u0026quot;:{\u0026quot;_id\u0026quot;:\u0026quot;6\u0026quot;}} {\u0026quot;account_number\u0026quot;:6,\u0026quot;balance\u0026quot;:5686,\u0026quot;firstname\u0026quot;:\u0026quot;Hattie\u0026quot;,\u0026quot;lastname\u0026quot;:\u0026quot;Bond\u0026quot;,\u0026quot;age\u0026quot;:36,\u0026quot;gender\u0026quot;:\u0026quot;M\u0026quot;,\u0026quot;address\u0026quot;:\u0026quot;671 Bristol Street\u0026quot;,\u0026quot;employer\u0026quot;:\u0026quot;Netagy\u0026quot;,\u0026quot;email\u0026quot;:\u0026quot;hattiebond@netagy.com\u0026quot;,\u0026quot;city\u0026quot;:\u0026quot;Dante\u0026quot;,\u0026quot;state\u0026quot;:\u0026quot;TN\u0026quot;} {\u0026quot;index\u0026quot;:{\u0026quot;_id\u0026quot;:\u0026quot;13\u0026quot;}} {\u0026quot;account_number\u0026quot;:13,\u0026quot;balance\u0026quot;:32838,\u0026quot;firstname\u0026quot;:\u0026quot;Nanette\u0026quot;,\u0026quot;lastname\u0026quot;:\u0026quot;Bates\u0026quot;,\u0026quot;age\u0026quot;:28,\u0026quot;gender\u0026quot;:\u0026quot;F\u0026quot;,\u0026quot;address\u0026quot;:\u0026quot;789 Madison Street\u0026quot;,\u0026quot;employer\u0026quot;:\u0026quot;Quility\u0026quot;,\u0026quot;email\u0026quot;:\u0026quot;nanettebates@quility.com\u0026quot;,\u0026quot;city\u0026quot;:\u0026quot;Nogal\u0026quot;,\u0026quot;state\u0026quot;:\u0026quot;VA\u0026quot;} {\u0026quot;index\u0026quot;:{\u0026quot;_id\u0026quot;:\u0026quot;18\u0026quot;}} {\u0026quot;account_number\u0026quot;:18,\u0026quot;balance\u0026quot;:4180,\u0026quot;firstname\u0026quot;:\u0026quot;Dale\u0026quot;,\u0026quot;lastname\u0026quot;:\u0026quot;Adams\u0026quot;,\u0026quot;age\u0026quot;:33,\u0026quot;gender\u0026quot;:\u0026quot;M\u0026quot;,\u0026quot;address\u0026quot;:\u0026quot;467 Hutchinson Court\u0026quot;,\u0026quot;employer\u0026quot;:\u0026quot;Boink\u0026quot;,\u0026quot;email\u0026quot;:\u0026quot;daleadams@boink.com\u0026quot;,\u0026quot;city\u0026quot;:\u0026quot;Orick\u0026quot;,\u0026quot;state\u0026quot;:\u0026quot;MD\u0026quot;} 执行查询\n Connection con = DriverManager.getConnection(url, properties); Statement st = con.createStatement(); ResultSet rs = null; //rs = st.executeQuery(\u0026quot;SELECT firstname, lastname FROM accounts\u0026quot;); rs = st.executeQuery(\u0026quot;SELECT firstname, lastname, age FROM accounts WHERE age \u0026gt; 20 ORDER BY state ASC\u0026quot;); while (rs.next()) { String firstname = rs.getString(\u0026quot;firstname\u0026quot;); String lastname = rs.getString(\u0026quot;lastname\u0026quot;); int age = rs.getInt(\u0026quot;age\u0026quot;); System.out.println(\u0026quot;firstname: \u0026quot; + firstname + \u0026quot; lastname:\u0026quot; + lastname + \u0026quot; age:\u0026quot; + age); } if (rs != null) rs.close(); if (st != null) st.close(); con.close();  输出\n firstname: Amber lastname:Duke age:32 firstname: Dale lastname:Adams age:33 firstname: Hattie lastname:Bond age:36 firstname: Nanette lastname:Bates age:28   ","subcategory":null,"summary":"","tags":null,"title":"SQL-JDBC","url":"/easysearch/v1.15.0/docs/references/sql/sql-jdbc/"},{"category":null,"content":"Rank 字段类型 #  下表列出了 Easysearch 支持的所有 rank 字段类型。\n   字段数据类型 描述     rank_feature 提升或降低文档的相关性得分。   rank_features 提升或降低文档的相关性得分。用于特征列表稀疏的情况。     Rank feature 和 rank features 字段只能使用 rank feature 查询进行查询。它们不支持聚合或排序。\n Rank feature 字段类型 #  Rank feature 字段类型使用正浮点值来提升或降低文档在 rank_feature 查询中的相关性得分。默认情况下，该值会提升相关性得分。要降低相关性得分，请将可选参数 positive_score_impact 设置为 false。\n示例 #  创建一个包含 rank feature 字段的映射：\nPUT chessplayers { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;rating\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;rank_feature\u0026#34; }, \u0026#34;age\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;rank_feature\u0026#34;, \u0026#34;positive_score_impact\u0026#34;: false } } } } 索引三个文档，其中包含一个提升得分的 rank_feature 字段（rating）和一个降低得分的 rank_feature 字段（age）：\nPUT chessplayers/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;rating\u0026#34;: 2554, \u0026#34;age\u0026#34;: 75 } PUT chessplayers/_doc/2 { \u0026quot;name\u0026quot;: \u0026quot;Kwaku Mensah\u0026quot;, \u0026quot;rating\u0026quot;: 2067, \u0026quot;age\u0026quot;: 10 }\nPUT chessplayers/_doc/3 { \u0026quot;name\u0026quot;: \u0026quot;Nikki Wolf\u0026quot;, \u0026quot;rating\u0026quot;: 1864, \u0026quot;age\u0026quot;: 22 } Rank feature 查询 #\n 使用 rank feature 查询，您可以按评分、年龄或同时按评分和年龄对选手进行排名。如果按评分排名，评分较高的选手将获得更高的相关性得分。如果按年龄排名，年龄较小的选手将获得更高的相关性得分。\n使用 rank feature 查询根据年龄和评分搜索选手：\nGET chessplayers/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;rating\u0026#34; } }, { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;age\u0026#34; } } ] } } } 当同时按年龄和评分排名时，年龄较小和评分较高的选手得分更好：\n{ \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 3, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 1.2093145, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;chessplayers\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1.2093145, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Kwaku Mensah\u0026#34;, \u0026#34;rating\u0026#34;: 1967, \u0026#34;age\u0026#34;: 10 } }, { \u0026#34;_index\u0026#34;: \u0026#34;chessplayers\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1.0150313, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Nikki Wolf\u0026#34;, \u0026#34;rating\u0026#34;: 1864, \u0026#34;age\u0026#34;: 22 } }, { \u0026#34;_index\u0026#34;: \u0026#34;chessplayers\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.8098284, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;rating\u0026#34;: 2554, \u0026#34;age\u0026#34;: 75 } } ] } } Rank features 字段类型 #  Rank features 字段类型与 rank feature 字段类型类似，但它更适合用于稀疏特征列表。Rank features 字段可以索引数值特征向量，这些向量后续用于在 rank_feature 查询中提升或降低文档的相关性得分。\n示例 #  创建一个包含 rank features 字段的映射：\nPUT testindex1 { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;correlations\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;rank_features\u0026#34; } } } } 要索引包含 rank features 字段的文档，请使用带有字符串键和正浮点值的哈希映射：\nPUT testindex1/_doc/1 { \u0026#34;correlations\u0026#34;: { \u0026#34;young kids\u0026#34;: 1, \u0026#34;older kids\u0026#34;: 15, \u0026#34;teens\u0026#34;: 25.9 } } PUT testindex1/_doc/2 { \u0026quot;correlations\u0026quot;: { \u0026quot;teens\u0026quot;: 10, \u0026quot;adults\u0026quot;: 95.7 } } 使用 rank feature 查询检索文档：\nGET testindex1/_search { \u0026#34;query\u0026#34;: { \u0026#34;rank_feature\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;correlations.teens\u0026#34; } } } 响应按相关性得分排序：\n{ \u0026#34;took\u0026#34;: 123, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: { \u0026#34;value\u0026#34;: 2, \u0026#34;relation\u0026#34;: \u0026#34;eq\u0026#34; }, \u0026#34;max_score\u0026#34;: 0.6258503, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.6258503, \u0026#34;_source\u0026#34;: { \u0026#34;correlations\u0026#34;: { \u0026#34;young kids\u0026#34;: 1, \u0026#34;older kids\u0026#34;: 15, \u0026#34;teens\u0026#34;: 25.9 } } }, { \u0026#34;_index\u0026#34;: \u0026#34;testindex1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 0.39263803, \u0026#34;_source\u0026#34;: { \u0026#34;correlations\u0026#34;: { \u0026#34;teens\u0026#34;: 10, \u0026#34;adults\u0026#34;: 95.7 } } } ] } }  Rank feature 和 rank features 字段使用前 9 个有效位来保证精度，导致大约 0.4% 的相对误差。值的存储相对精度为 2^−8 = 0.00390625。\n ","subcategory":null,"summary":"","tags":null,"title":"Rank 字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/rank/"},{"category":null,"content":"N-gram 词元生成器 #  N-gram 词元生成器会将文本拆分为指定长度的重叠 n-gram（固定长度为 n 的字符序列）。当你希望实现部分单词匹配或自动补全搜索功能时，这个分词器特别有用，因为它会生成原始输入文本的子字符串（ n-gram 字符串）。\n参考样例 #  以下示例请求创建了一个名为 my_index 的新索引，并配置了一个使用 n-gram 词元生成器的分词器。\nPUT /my_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_ngram_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;ngram\u0026quot;, \u0026quot;min_gram\u0026quot;: 3, \u0026quot;max_gram\u0026quot;: 4, \u0026quot;token_chars\u0026quot;: [\u0026quot;letter\u0026quot;, \u0026quot;digit\u0026quot;] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_ngram_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_ngram_tokenizer\u0026quot; } } } } } 生成的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_ngram_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Search\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ {\u0026quot;token\u0026quot;: \u0026quot;Sea\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 3,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 0}, {\u0026quot;token\u0026quot;: \u0026quot;Sear\u0026quot;,\u0026quot;start_offset\u0026quot;: 0,\u0026quot;end_offset\u0026quot;: 4,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 1}, {\u0026quot;token\u0026quot;: \u0026quot;ear\u0026quot;,\u0026quot;start_offset\u0026quot;: 1,\u0026quot;end_offset\u0026quot;: 4,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 2}, {\u0026quot;token\u0026quot;: \u0026quot;earc\u0026quot;,\u0026quot;start_offset\u0026quot;: 1,\u0026quot;end_offset\u0026quot;: 5,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 3}, {\u0026quot;token\u0026quot;: \u0026quot;arc\u0026quot;,\u0026quot;start_offset\u0026quot;: 2,\u0026quot;end_offset\u0026quot;: 5,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 4}, {\u0026quot;token\u0026quot;: \u0026quot;arch\u0026quot;,\u0026quot;start_offset\u0026quot;: 2,\u0026quot;end_offset\u0026quot;: 6,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 5}, {\u0026quot;token\u0026quot;: \u0026quot;rch\u0026quot;,\u0026quot;start_offset\u0026quot;: 3,\u0026quot;end_offset\u0026quot;: 6,\u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;,\u0026quot;position\u0026quot;: 6} ] } 参数说明 #  N-gram 词元生成器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     min_gram 可选 整数 n-gram 的最小长度。默认值为 1。   max_gram 可选 整数 n-gram 的最大长度。默认值为 2。   token_chars 可选 字符串列表 分词时要包含的字符类。有效值为：\n- letter（字母）\n- digit（数字）\n- whitespace（空白字符）\n- punctuation（标点符号）\n- symbol（符号）\n- custom（自定义，你还必须指定 custom_token_chars 参数）\n默认值为空列表 []，即保留所有字符。   custom_token_chars 可选 字符串 要包含在词元中的自定义字符。    关于 min_gram 与 max_gram 的最大差值 #  min_gram 和 max_gram 之间的最大差值可通过索引级别的 index.max_ngram_diff 设置进行配置，其默认值为 1。\n以下示例请求创建了一个带有自定义 index.max_ngram_diff 设置的索引：\nPUT /my-index { \u0026quot;settings\u0026quot;: { \u0026quot;index.max_ngram_diff\u0026quot;: 2, \u0026quot;analysis\u0026quot;: { \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_ngram_tokenizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;ngram\u0026quot;, \u0026quot;min_gram\u0026quot;: 3, \u0026quot;max_gram\u0026quot;: 5, \u0026quot;token_chars\u0026quot;: [\u0026quot;letter\u0026quot;, \u0026quot;digit\u0026quot;] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_ngram_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_ngram_tokenizer\u0026quot; } } } } } ","subcategory":null,"summary":"","tags":null,"title":"N-gram 词元生成器","url":"/easysearch/v1.15.0/docs/references/text-analysis/tokenizers/n-gram/"},{"category":null,"content":"n-gram 分词过滤器 #  n-gram 分词过滤器是一种强大的工具，用于将文本拆分为更小的组件，即 n-gram，这有助于提升部分匹配和模糊搜索能力。它通过将一个词元拆分成指定长度的子字符串来工作。这些过滤器在搜索应用程序中很常用，可用于支持自动补全、部分匹配以及容错拼写搜索。\n参数说明 #  n-gram 分词过滤器可以使用以下参数进行配置。\n   参数 必需/可选 数据类型 描述     min_gram 可选 整数 n-gram 的最小长度。默认值为 1。   max_gram 可选 整数 n-gram 的最大长度。默认值为 2。   preserve_original 可选 布尔值 是否将原始词元保留为输出之一。默认值为 false。    参考样例 #  以下示例请求创建了一个名为“ngram_example_index”的新索引，并配置了一个带有 n-gram 过滤器的分词器：\nPUT /ngram_example_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;ngram_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;ngram\u0026quot;, \u0026quot;min_gram\u0026quot;: 2, \u0026quot;max_gram\u0026quot;: 3 } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;ngram_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;ngram_filter\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /ngram_example_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;ngram_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Search\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;se\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;sea\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;ea\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;ear\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;ar\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;arc\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;rc\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;rch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;ch\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"n-gram 分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/n-gram/"},{"category":null,"content":"KStem 分词过滤器 #  KStem 分词过滤器是一种词干提取过滤器，用于将单词还原为其词根形式。该过滤器是一种专为英语设计的轻量级算法词干提取器，它执行以下词干提取操作：\n 将复数形式的单词还原为单数形式。 将不同的动词时态转换为其基本形式。 去除常见的派生词尾，例如 “-ing” 或 “-ed”。  KStem 分词过滤器等同于配置了 light_english 语言的词干提取器过滤器。与其他词干提取过滤器（如词干提取器 porter_stem）相比，它提供了更为保守的词干提取方式。\nKStem 分词过滤器基于 Lucene 的 KStemFilter。如需了解更多信息，请参阅 Lucene 文档。\n参考样例 #  以下示例请求创建了一个名为 my_kstem_index 的新索引，并配置了一个带有 KStem 过滤器的分词器：\nPUT /my_kstem_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;kstem_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;kstem\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;my_kstem_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;kstem_filter\u0026quot; ] } } } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_kstem_analyzer\u0026quot; } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /my_kstem_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_kstem_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;stops stopped\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;stop\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 5, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;stop\u0026quot;, \u0026quot;start_offset\u0026quot;: 6, \u0026quot;end_offset\u0026quot;: 13, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 } ] } ","subcategory":null,"summary":"","tags":null,"title":"KStem 分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/kstem/"},{"category":null,"content":"Keyword 字段类型 #  keyword 字段类型包含未经分析的字符串。它只允许精确的大小写敏感匹配。\n默认情况下，keyword 字段既被索引（因为 index 已启用）也存储在磁盘上（因为 doc_values 已启用）。为了减少磁盘空间，您可以通过将 index 设置为 false 来指定不索引 keyword 字段。\n 如果您需要对字段进行全文搜索，请将其映射为 text 类型。\n 代码样例 #  以下查询创建了一个带有 keyword 字段的映射。将 index 设置为 false 指定将 genre 字段存储在磁盘上，并使用 doc_values 检索它：\nPUT movies { \u0026#34;mappings\u0026#34; : { \u0026#34;properties\u0026#34; : { \u0026#34;genre\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;keyword\u0026#34;, \u0026#34;index\u0026#34; : false } } } } 参数说明 #  下表列出了 keyword 字段类型接受的参数。所有参数都是可选的。\n   参数 描述     boost 浮点值，指定此字段对相关性分数的权重。大于 1.0 的值会增加字段的相关性。0.0 到 1.0 之间的值会降低字段的相关性。默认值为 1.0。   doc_values 布尔值，指定是否应将字段存储在磁盘上，以便可以用于聚合、排序或脚本。默认值为 true。   eager_global_ordinals 指定是否应在刷新时立即加载全局序号。如果该字段经常用于聚合，此参数应设置为 true。默认值为 false。   fields 要以多种方式索引同一个字符串（例如，作为 keyword 和 text），请提供 fields 参数。您可以指定字段的一个版本用于搜索，另一个版本用于排序和聚合。   ignore_above 任何长度超过此整数值的字符串都不应被索引。默认值为 2147483647。默认动态映射会创建一个 ignore_above 设置为 256 的 keyword 子字段。   index 布尔值，指定字段是否应可搜索。默认值为 true。要减少磁盘空间，请将 index 设置为 false。   index_options 要存储在索引中的信息，将在计算相关性分数时考虑。可以设置为 freqs 以获取词频。默认值为 docs。   meta 接受此字段的元数据。   normalizer 指定在索引之前如何预处理此字段（例如，转换为小写）。默认值为 null（无预处理）。   norms 布尔值，指定在计算相关性分数时是否应使用字段长度。默认值为 false。   null_value 用于替代 null 的值。必须与字段类型相同。如果未指定此参数，当值为 null 时，该字段将被视为缺失。默认值为 null。   similarity 用于计算相关性分数的排名算法。默认值为 BM25。   split_queries_on_whitespace 布尔值，指定全文查询是否应在空格上分割。默认值为 false。   store 布尔值，指定字段值是否应该被存储并且可以与 _source 字段分开检索。默认值为 false。    ","subcategory":null,"summary":"","tags":null,"title":"Keyword 字段类型","url":"/easysearch/v1.15.0/docs/references/mappings-and-field-types/field-types/string-field-type/keyword/"},{"category":null,"content":"IP 范围聚合 #  ip_range IP 范围聚合用于 IP 地址。它适用于 ip 类型字段。您可以在 CIDR 表示法中定义 IP 范围和掩码。\nGET sample_data_logs/_search { \u0026quot;size\u0026quot;: 0, \u0026quot;aggs\u0026quot;: { \u0026quot;access\u0026quot;: { \u0026quot;ip_range\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;ip\u0026quot;, \u0026quot;ranges\u0026quot;: [ { \u0026quot;from\u0026quot;: \u0026quot;1.0.0.0\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;126.158.155.183\u0026quot; }, { \u0026quot;mask\u0026quot;: \u0026quot;1.0.0.0/8\u0026quot; } ] } } } } 返回内容\n... \u0026quot;aggregations\u0026quot; : { \u0026quot;access\u0026quot; : { \u0026quot;buckets\u0026quot; : [ { \u0026quot;key\u0026quot; : \u0026quot;1.0.0.0/8\u0026quot;, \u0026quot;from\u0026quot; : \u0026quot;1.0.0.0\u0026quot;, \u0026quot;to\u0026quot; : \u0026quot;2.0.0.0\u0026quot;, \u0026quot;doc_count\u0026quot; : 98 }, { \u0026quot;key\u0026quot; : \u0026quot;1.0.0.0-126.158.155.183\u0026quot;, \u0026quot;from\u0026quot; : \u0026quot;1.0.0.0\u0026quot;, \u0026quot;to\u0026quot; : \u0026quot;126.158.155.183\u0026quot;, \u0026quot;doc_count\u0026quot; : 7184 } ] } } } 如果您向映射中将 ip_range 设置为 false 的索引添加了字段格式不正确的文档，OpenSearch 会拒绝整个文档。您可以将 ignore_malformed 设置为 true 以指定 OpenSearch 应忽略格式不正确的字段。默认值为 false 。\n... \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;ips\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;ip_range\u0026quot;, \u0026quot;ignore_malformed\u0026quot;: true } } }   ","subcategory":null,"summary":"","tags":null,"title":"IP 范围聚合","url":"/easysearch/v1.15.0/docs/references/aggregation/bucket-aggregations/ip-range/"},{"category":null,"content":"IK 分词器 #  IK 分词器是一款专为处理中文文本设计的分词器，高效且智能。支持 ik_smart 和 ik_max_word 两种分词模式。\nIK 分词器安装 #  IK 分词插件安装命令如下：\nbin/easysearch-plugin install analysis-ik 同时也需要安装 ingest-common 插件：\nbin/easysearch-plugin install ingest-common 如果觉得比较麻烦，也可以直接使用 Easysearch 的 bundle 包安装部署。\n使用样例 #  下面的命令样例展示了 IK 的使用方式。\n# 1.创建索引 PUT index_ik\n2.创建映射关系 POST index_ik/_mapping { \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;ik_max_word\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;ik_smart\u0026quot; } } }\n3.写入文档 POST index_ik/_create/1 {\u0026quot;content\u0026quot;:\u0026quot;美国留给伊拉克的是个烂摊子吗\u0026quot;}\nPOST index_ik/_create/2 {\u0026quot;content\u0026quot;:\u0026quot;公安部：各地校车将享最高路权\u0026quot;}\nPOST index_ik/_create/3 {\u0026quot;content\u0026quot;:\u0026quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船\u0026quot;}\nPOST index_ik/_create/4 {\u0026quot;content\u0026quot;:\u0026quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首\u0026quot;}\n4.高亮查询 POST index_ik/_search { \u0026quot;query\u0026quot; : { \u0026quot;match\u0026quot; : { \u0026quot;content\u0026quot; : \u0026quot;中国\u0026quot; }}, \u0026quot;highlight\u0026quot; : { \u0026quot;pre_tags\u0026quot; : [\u0026quot;\u0026lt;tag1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;tag2\u0026gt;\u0026quot;], \u0026quot;post_tags\u0026quot; : [\u0026quot;\u0026lt;/tag1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;/tag2\u0026gt;\u0026quot;], \u0026quot;fields\u0026quot; : { \u0026quot;content\u0026quot; : {} } } }\n返回内容 { \u0026quot;took\u0026quot;: 14, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 5, \u0026quot;successful\u0026quot;: 5, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: 2, \u0026quot;max_score\u0026quot;: 2, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;fulltext\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 2, \u0026quot;_source\u0026quot;: { \u0026quot;content\u0026quot;: \u0026quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首\u0026quot; }, \u0026quot;highlight\u0026quot;: { \u0026quot;content\u0026quot;: [ \u0026quot;\u0026lt;tag1\u0026gt;中国\u0026lt;/tag1\u0026gt;驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首 \u0026quot; ] } }, { \u0026quot;_index\u0026quot;: \u0026quot;index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;fulltext\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 2, \u0026quot;_source\u0026quot;: { \u0026quot;content\u0026quot;: \u0026quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船\u0026quot; }, \u0026quot;highlight\u0026quot;: { \u0026quot;content\u0026quot;: [ \u0026quot;均每天扣1艘\u0026lt;tag1\u0026gt;中国\u0026lt;/tag1\u0026gt;渔船 \u0026quot; ] } } ] } } 关于 ik_smart 和 ik_max_word #\n ik_smart 和 ik_max_word 是 IK 分词器的两个分词模式。\nik_max_word：将文本根据词典内容做最细粒度的切分。例如,ik_max_word 会将”中华人民共和国国歌”切分到“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国、共和,和,国国,国歌”,详尽生成各种可能的组合。\nik_smart：执行文本粗粒度的切分。例如：ik_smart 会把 ”中华人民共和国国歌”到“中华人民共和国,国歌”。\nik_smart 比较适合短语查询，而 ik_max_word 更合适精准匹配。值得注意的是，由于 ik_smart 做了分词切割的优化，其的分词结果并不是 ik_max_word 的分词结果的子集。\n字段级别词典设置 #  Easysearch 在 IK 分词器原有的功能上增加了自定义字段级别词典的功能。\n自定义字段级别词典的功能支持用户对不同的字段设置不同的分词词库，用户既可以指定 IK 完全使用自有的词库，也支持在 IK 默认的词库上增加自定义的词库内容。\n自定义词库内容 #  默认的词库索引是 .analysis_ik 索引，IK 插件自动初始化的 .analysis_ik 索引。\n用户可以自定义使用某个索引替代 .analysis_ik（设置参数下面会提及），但是要保持和 .analysis_ik 一个的 mapping 结构和使用预设的 pipeline：ik_dicts_default_date_pipeline。\n.analysis_ik 词库需要存储的格式如下：\nPOST .analysis_ik/_doc { \u0026#34;dict_key\u0026#34;: \u0026#34;test_dic\u0026#34;, \u0026#34;dict_type\u0026#34;: \u0026#34;main_dicts\u0026#34;, \u0026#34;dict_content\u0026#34;: \u0026#34;\u0026#34;\u0026#34;中华人民共和国 中文万岁 秋水共长天\u0026#34;\u0026#34;\u0026#34; } 主要使用字段\n dict_content：词典内容字段。各个词典以换行符分隔。 dict_key：自定义词典名。对应自定义词典中设置的 dict_key。 dict_type：字典类型，可选 \u0026ldquo;main_dicts\u0026rdquo;, \u0026ldquo;stopwords_dicts\u0026rdquo;, \u0026ldquo;quantifier_dicts\u0026rdquo; 三个值。其中任意 dict_key 的\u0026quot;main_dicts\u0026quot;必须存在。  如果是对默认词库的自定义增加，dict_key 就写\u0026quot;default\u0026quot;，同时需要使用默认的词库索引 .analysis_ik 。\n设置自定义词库 #  自定义词库的生效主要通过自定义 tokenizer 进行设置。\nPUT my-index-000001 { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;my_tokenizer\u0026#34; } }, \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;custom_dict_enable\u0026#34;: true, \u0026#34;load_default_dicts\u0026#34;:true, \u0026#34;lowcase_enable\u0026#34;: true, \u0026#34;dict_key\u0026#34;: \u0026#34;test_dic\u0026#34;, \u0026#34;dict_index\u0026#34;:\u0026#34;custom_index\u0026#34; } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;test_ik\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34; } } } } 其中\n custom_dict_enable：布尔值，默认 false，true 则可以定制词典读取路径，否则 load_default_dicts / dict_key / dict_index 均失效。 load_default_dicts：布尔值，默认 true，定制的词典是否包含默认的词典库。 lowcase_enable：布尔值，默认为 true，是否大小写敏感，false 则保留原来文本的大小写。 dict_key：string。对应词库索引中的 dict_key 字段内容。如果词典名不匹配，则会装载错误或者直接报错 。 dict_index: string。词库索引名称，默认是 .analysis_ik。可以自定义，但是要保持和 mapping 结构以及 pipeline 一致。  词库内容的自动新增 #  词库的追加内容是能自动被程序探测的，这个主要依赖于 .analysis_ik 的时间戳字段和 pipeline 执行。\nik 只对词库索引的新增词典内容进行自动追加，不会对存量词典的修改进修改或者删除。如果需要对某个存量自定义词典进行修改或者删除，请进行全量 reload。\n# 词典索引写入需要的默认时间戳 pipeline GET _ingest/pipeline/ik_dicts_default_date_pipeline { \u0026#34;ik_dicts_default_date_pipeline\u0026#34;: { \u0026#34;processors\u0026#34;: [ { \u0026#34;set\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;upload_dicts_timestamp\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{_ingest.timestamp}}\u0026#34;, \u0026#34;override\u0026#34;: true } } ] } } # 词典索引的结构 GET .analysis.ik { \u0026quot;.analysis.ik\u0026quot;: { \u0026quot;aliases\u0026quot;: {}, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;dict_content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;custom_analyzer\u0026quot; }, \u0026quot;dict_key\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;dict_type\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;upload_dicts_timestamp\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot; } } }, \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;number_of_shards\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;provided_name\u0026quot;: \u0026quot;.analysis.ik\u0026quot;, \u0026quot;default_pipeline\u0026quot;: \u0026quot;ik_dicts_default_date_pipeline\u0026quot;, \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;pattern_tokenizer\u0026quot; } }, \u0026quot;tokenizer\u0026quot;: { \u0026quot;pattern_tokenizer\u0026quot;: { \u0026quot;pattern\u0026quot;: \u0026quot;\\n\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;pattern\u0026quot; } } }, \u0026quot;number_of_replicas\u0026quot;: \u0026quot;1\u0026quot; } } } } 这里 ik_dicts_default_date_pipeline 会对每一条写入词库的数据赋予当前 upload_dicts_timestamp 时间戳。ik 会记录当前词库的最大时间戳，然后每分钟都会去查询一次词库索引现有的最大时间戳。如果查到词库索引的最大的时间戳大于上次记录到的时间戳，则对这段时间内的词库内容都进行加载。\n全量 reload #  reload API 通过对词典库的全量重新加载来实现词典库的更新或者删除。用户可以通过下面的命令实现：\n# 测试索引准备 PUT my-index-000001 { \u0026quot;settings\u0026quot;: { \u0026quot;number_of_shards\u0026quot;: 3, \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;my_custom_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;my_tokenizer\u0026quot; } }, \u0026quot;tokenizer\u0026quot;: { \u0026quot;my_tokenizer\u0026quot;: {\n \u0026amp;quot;type\u0026amp;quot;: \u0026amp;quot;ik_smart\u0026amp;quot;, \u0026amp;quot;custom_dict_enable\u0026amp;quot;: true, \u0026amp;quot;load_default_dicts\u0026amp;quot;:false, # 这里不包含默认词库 \u0026amp;quot;lowcase_enable\u0026amp;quot;: true, \u0026amp;quot;dict_key\u0026amp;quot;: \u0026amp;quot;test_dic\u0026amp;quot; } } }  }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;test_ik\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot; } } } }\n原来词库分词效果，只预置了分词“自强不息” GET my-index-000001/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;:\u0026quot;自强不息，杨树林\u0026quot; }\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;自强不息\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;CN_WORD\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;杨\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;树\u0026quot;, \u0026quot;start_offset\u0026quot;: 6, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;林\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 3 } ] }\n更新词库 POST .analysis_ik/_doc { \u0026quot;dict_key\u0026quot;: \u0026quot;test_dic\u0026quot;, \u0026quot;dict_type\u0026quot;: \u0026quot;main_dicts\u0026quot;, \u0026quot;dict_content\u0026quot;:\u0026quot;杨树林\u0026quot; }\n删除词库，词库文档的id为coayoJcBFHNnLYAKfTML DELETE .analysis_ik/_doc/coayoJcBFHNnLYAKfTML?refresh=true\n重载词库 POST _ik/_reload {}\n更新后的词库效果 GET my-index-000001/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;my_custom_analyzer\u0026quot;, \u0026quot;text\u0026quot;:\u0026quot;自强不息，杨树林\u0026quot; }\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;自\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 1, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;强\u0026quot;, \u0026quot;start_offset\u0026quot;: 1, \u0026quot;end_offset\u0026quot;: 2, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;不\u0026quot;, \u0026quot;start_offset\u0026quot;: 2, \u0026quot;end_offset\u0026quot;: 3, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;息\u0026quot;, \u0026quot;start_offset\u0026quot;: 3, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;type\u0026quot;: \u0026quot;CN_CHAR\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;杨树林\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;CN_WORD\u0026quot;, \u0026quot;position\u0026quot;: 4 } ] } 这里是实现索引里全部的词库更新。\n也可以实现某个单独的词库全量更新\nPOST _ik/_reload {\u0026quot;dict_key\u0026quot;:\u0026quot;test_dic”} ","subcategory":null,"summary":"","tags":null,"title":"IK 分词器","url":"/easysearch/v1.15.0/docs/references/text-analysis/analyzers/ik-analyzer/"},{"category":null,"content":"HTML 剥离字符过滤器 #  HTML 剥离（html_strip）字符过滤器会从输入文本中移除 HTML 标签（例如 \u0026lt;div\u0026gt;、\u0026lt;p\u0026gt; 和 \u0026lt;a\u0026gt; 等）并输出纯文本。该过滤器可以配置保留某些标签，或者配置把特定的 HTML 标签实体（如 \u0026amp;nbsp;）解码为空格。\n参考样例 #  以下请求展示将 html_strip 字符过滤器应用于文本：\nGET /_analyze { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;char_filter\u0026quot;: [ \u0026quot;html_strip\u0026quot; ], \u0026quot;text\u0026quot;: \u0026quot;\u0026lt;p\u0026gt;Commonly used calculus symbols include \u0026amp;alpha;, \u0026amp;beta; and \u0026amp;theta; \u0026lt;/p\u0026gt;\u0026quot; } 返回内容中包含的词元里，可以看到 HTML 字符已被转换为它们的解码后的值：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;\\nCommonly used calculus symbols include α, β and θ \\n\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 74, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } 参数说明 #  html_strip 字符过滤器可以使用以下参数进行配置。\n   参数 必填/可选 数据类型 描述     escaped_tags 可选 字符串数组 一个 HTML 元素名称列表，指定时不带包围的尖括号（\u0026lt; \u0026gt;）。当从文本中去除 HTML 标签时，过滤器不会移除该列表中的元素。例如，将该配置设置为 [\u0026quot;b\u0026quot;, \u0026quot;i\u0026quot;]时, 将防止 \u0026lt;b\u0026gt; 和 \u0026lt;i\u0026gt; 元素被去除。    示例：带有小写过滤器的自定义分词器 #  以下请求创建了一个自定义分词器，该分词器通过使用 html_strip 字符过滤器来去除 HTML 标签，并通过 lowercase 词元过滤器将纯文本转换为小写形式：\nPUT /html_strip_and_lowercase_analyzer { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;char_filter\u0026quot;: { \u0026quot;html_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;html_strip\u0026quot; } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;html_strip_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;char_filter\u0026quot;: [\u0026quot;html_filter\u0026quot;], \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;lowercase\u0026quot;] } } } } } 使用以下请求来检查使用该分词器生成的词元：\nGET /html_strip_and_lowercase_analyzer/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;html_strip_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;\u0026lt;h1\u0026gt;Welcome to \u0026lt;strong\u0026gt;Easysearch\u0026lt;/strong\u0026gt;!\u0026lt;/h1\u0026gt;\u0026quot; } 在返回内容中，HTML 标签已被移除，并且纯文本已被转换为小写形式：\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;welcome\u0026quot;, \u0026quot;start_offset\u0026quot;: 4, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;to\u0026quot;, \u0026quot;start_offset\u0026quot;: 12, \u0026quot;end_offset\u0026quot;: 14, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;easysearch\u0026quot;, \u0026quot;start_offset\u0026quot;: 23, \u0026quot;end_offset\u0026quot;: 42, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } 示例：保留 HTML 标签的自定义分词器 #  以下示例请求创建了一个能保留 HTML 标签的自定义分词器：\nPUT /html_strip_preserve_analyzer { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;char_filter\u0026quot;: { \u0026quot;html_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;html_strip\u0026quot;, \u0026quot;escaped_tags\u0026quot;: [\u0026quot;b\u0026quot;, \u0026quot;i\u0026quot;] } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;html_strip_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;char_filter\u0026quot;: [\u0026quot;html_filter\u0026quot;], \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot; } } } } } 使用以下请求来检查使用该分词器生成的词元：\nGET /html_strip_preserve_analyzer/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;html_strip_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;\u0026lt;p\u0026gt;This is a \u0026lt;b\u0026gt;bold\u0026lt;/b\u0026gt; and \u0026lt;i\u0026gt;italic\u0026lt;/i\u0026gt; text.\u0026lt;/p\u0026gt;\u0026quot; } 在返回内容中，正如在自定义分词器请求中所指定的那样，斜体 italic 标签和加粗 bold 标签已被保留。\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;\\nThis is a \u0026lt;b\u0026gt;bold\u0026lt;/b\u0026gt; and \u0026lt;i\u0026gt;italic\u0026lt;/i\u0026gt; text.\\n\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 52, \u0026quot;type\u0026quot;: \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot;: 0 } ] } ","subcategory":null,"summary":"","tags":null,"title":"HTML 剥离字符过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/character-filters/html-strip/"},{"category":null,"content":"CJK 宽度分词过滤器 #  CJK 宽度(cjk_width)分词过滤器通过将全角 ASCII 字符转换为其标准（半角）ASCII 等效字符，并将半角片假名字符转换为全角等效字符，来对中文、日文和韩文（CJK）词元进行规范化处理。\n转换全角 ASCII 字符 #  在 CJK 文本中，ASCII 字符（如字母和数字）可能会以全角形式出现，占据两个半角字符的空间。全角 ASCII 字符在东亚排版中通常用于与 CJK 字符的宽度对齐。然而，出于索引和搜索的目的，这些全角字符需要规范化为其标准（半角）ASCII 等效字符。\n以下示例说明了 ASCII 字符的规范化过程：\n 全角: ＡＢＣＤＥ １２３４５ 规范化 (半角): ABCDE 12345 转换半角片假名字符 #  CJK 宽度分词过滤器会将半角片假名字符转换为对应的全角片假名字符，全角片假名是日语文本中使用的标准形式。如下例所示，这种规范化处理对于文本处理和搜索的一致性至关重要：\n 半角 katakana: ｶﾀｶﾅ 规范化 (全角) katakana: カタカナ 参考样例 #  以下示例请求创建了一个名为 cjk_width_example_index 的新索引，并定义了一个包含 cjk_width 过滤器的分词器：\nPUT /cjk_width_example_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;cjk_width_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [\u0026quot;cjk_width\u0026quot;] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /cjk_width_example_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;cjk_width_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Ｔｏｋｙｏ 2024 ｶﾀｶﾅ\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;Tokyo\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 5, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;2024\u0026quot;, \u0026quot;start_offset\u0026quot;: 6, \u0026quot;end_offset\u0026quot;: 10, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;NUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;カタカナ\u0026quot;, \u0026quot;start_offset\u0026quot;: 11, \u0026quot;end_offset\u0026quot;: 15, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;KATAKANA\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 } ] } ","subcategory":null,"summary":"","tags":null,"title":"CJK 宽度分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/cjk-width/"},{"category":null,"content":"CJK 二元分词过滤器 #  CJK 二元(cjk_bigram)分词过滤器是专门为处理东亚语言（如中文、日文和韩文，简称 CJK）而设计的，这些语言通常不使用空格来分隔单词。二元组是指在词元字符串中两个相邻元素的序列，这些元素可以是字符或单词。对于 CJK 语言来说，二元组有助于近似确定单词边界，并捕捉那些能够传达意义的重要字符对。\n参数说明 #  CJK 二元分词过滤器可以通过两个参数进行配置：ignore_scripts（忽略字符集）和 output_unigrams（输出一元组）。\nignore_scripts（忽略字符集） #  CJK 二元分词过滤器会忽略所有非 CJK 字符集（如拉丁字母或西里尔字母等书写系统），并且仅将 CJK 文本分词为二元组。可使用此选项指定要忽略的 CJK 字符集。该选项有以下有效值：\n han（汉字）：汉字字符集处理汉字。汉字是用于中国、日本和韩国书面语言的意符文字。该过滤器可帮助处理文本相关任务，例如对用中文、日文汉字或韩文汉字书写的文本进行分词、规范化或词干提取。 hangul（韩文）：韩文（谚文）字符集处理韩文字符，这些字符是韩语所特有的，不存在于其他东亚字符集中。 hiragana（平假名）：平假名字符集处理平假名，平假名是日语书写系统中使用的两种音节文字之一。平假名通常用于日语的本土词汇、语法元素以及某些形式的标点符号。 katakana（片假名）：片假名字符集处理片假名，片假名是日语的另一种音节文字。片假名主要用于外来借词、拟声词、科学名称以及某些日语词汇。  output_unigrams（输出一元组） #  当此选项设置为 true 时，会同时输出一元组（单个字符）和二元组。默认值为 false。\n参考样例 #  以下示例请求创建了一个名为 devanagari_example_index 的新索引，并定义了一个分词器，该分词器使用了 cjk_bigram_filter 过滤器，且将 ignored_scripts 参数设置为 katakana（片假名）：\nPUT /cjk_bigram_example { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;cjk_bigrams_no_katakana\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;cjk_bigrams_no_katakana_filter\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;cjk_bigrams_no_katakana_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;cjk_bigram\u0026quot;, \u0026quot;ignored_scripts\u0026quot;: [ \u0026quot;katakana\u0026quot; ], \u0026quot;output_unigrams\u0026quot;: true } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /cjk_bigram_example/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;cjk_bigrams_no_katakana\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;東京タワーに行く\u0026quot; } 测试文本：“東京タワーに行く”\n東京 (Kanji for \u0026quot;Tokyo\u0026quot;) タワー (Katakana for \u0026quot;Tower\u0026quot;) に行く (Hiragana and Kanji for \u0026quot;go to\u0026quot;) 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;東\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 1, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;SINGLE\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;東京\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 2, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;DOUBLE\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0, \u0026quot;positionLength\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;京\u0026quot;, \u0026quot;start_offset\u0026quot;: 1, \u0026quot;end_offset\u0026quot;: 2, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;SINGLE\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;タワー\u0026quot;, \u0026quot;start_offset\u0026quot;: 2, \u0026quot;end_offset\u0026quot;: 5, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;KATAKANA\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;に\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;SINGLE\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;に行\u0026quot;, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;DOUBLE\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3, \u0026quot;positionLength\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;行\u0026quot;, \u0026quot;start_offset\u0026quot;: 6, \u0026quot;end_offset\u0026quot;: 7, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;SINGLE\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4 }, { \u0026quot;token\u0026quot;: \u0026quot;行く\u0026quot;, \u0026quot;start_offset\u0026quot;: 6, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;DOUBLE\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 4, \u0026quot;positionLength\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;く\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 8, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;SINGLE\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 5 } ] } ","subcategory":null,"summary":"","tags":null,"title":"CJK 二元分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/cjk-bigram/"},{"category":null,"content":"ASCII 折叠分词过滤器 #  ASCII 折叠(asciifolding)分词过滤器将非 ASCII 字符转换为与其最接近的 ASCII 等效字符。例如，“é” 变为 “e”，“ü” 变为 “u”，“ñ” 变为 “n”。这个过程被称为\u0026quot;音译\u0026quot;。\nASCII 折叠分词过滤器有许多优点：\n 增强搜索灵活性：用户在输入查询内容时常常会省略重音符号或特殊字符。ASCII 折叠分词过滤器可确保即使是这样的查询也仍然能返回相关结果。 规范化：通过确保重音字符始终被转换为其 ASCII 等效字符，使索引编制过程标准化。 国际化：对于包含多种语言和字符集的应用程序特别有用。   尽管 ASCII 折叠分词过滤器可以简化搜索，但它也可能导致特定信息的丢失，尤其是当数据集中重音字符和非重音字符之间的区别很重要的时候。\n 参数说明 #  你可以使用 preserve_original 参数来配置 ASCII 折叠分词过滤器。将此参数设置为 true 时，会在词元流中同时保留原始词元及其 ASCII 折叠后的版本。当你希望在搜索查询中同时匹配一个词项的原始版本（带重音符号）和规范化版本（不带重音符号）时，这一点会特别有用。该参数的默认值为 false。\n参考样例 #  以下示例请求创建了一个名为 example_index 的新索引，并定义了一个分词器，该分词器使用了 ASCII 折叠过滤器，且将 preserve_original 参数设置为 true：\nPUT /example_index { \u0026quot;settings\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;filter\u0026quot;: { \u0026quot;custom_ascii_folding\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;asciifolding\u0026quot;, \u0026quot;preserve_original\u0026quot;: true } }, \u0026quot;analyzer\u0026quot;: { \u0026quot;custom_ascii_analyzer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;custom_ascii_folding\u0026quot; ] } } } } } 产生的词元 #  使用以下请求来检查使用该分词器生成的词元：\nPOST /example_index/_analyze { \u0026quot;analyzer\u0026quot;: \u0026quot;custom_ascii_analyzer\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;Résumé café naïve coördinate\u0026quot; } 返回内容包含产生的词元\n{ \u0026quot;tokens\u0026quot;: [ { \u0026quot;token\u0026quot;: \u0026quot;resume\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;résumé\u0026quot;, \u0026quot;start_offset\u0026quot;: 0, \u0026quot;end_offset\u0026quot;: 6, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 0 }, { \u0026quot;token\u0026quot;: \u0026quot;cafe\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;café\u0026quot;, \u0026quot;start_offset\u0026quot;: 7, \u0026quot;end_offset\u0026quot;: 11, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 1 }, { \u0026quot;token\u0026quot;: \u0026quot;naive\u0026quot;, \u0026quot;start_offset\u0026quot;: 12, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;naïve\u0026quot;, \u0026quot;start_offset\u0026quot;: 12, \u0026quot;end_offset\u0026quot;: 17, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 2 }, { \u0026quot;token\u0026quot;: \u0026quot;coordinate\u0026quot;, \u0026quot;start_offset\u0026quot;: 18, \u0026quot;end_offset\u0026quot;: 28, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 }, { \u0026quot;token\u0026quot;: \u0026quot;coördinate\u0026quot;, \u0026quot;start_offset\u0026quot;: 18, \u0026quot;end_offset\u0026quot;: 28, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot;: 3 } ] } ","subcategory":null,"summary":"","tags":null,"title":"ASCII 折叠分词过滤器","url":"/easysearch/v1.15.0/docs/references/text-analysis/token-filters/ascii-folding/"},{"category":null,"content":"字段级权限 #  字段级权限允许您控制用户可以查看哪些文档的字段。就像 文档级权限，您可以通过角色中的索引控制访问。\n包括或排除字段 #  配置字段级权限时，有两个选项：包括或排除字段。如果包含字段，则用户在检索文档时 只能看到 这些字段。例如，如果您包含 actors、title 和 year 字段，则搜索结果可能如下所示：\nPOST movies/_doc/1 { \u0026#34;year\u0026#34;: 2013, \u0026#34;title\u0026#34;: \u0026#34;Rush\u0026#34;, \u0026#34;actors\u0026#34;: [ \u0026#34;Daniel Brühl\u0026#34;, \u0026#34;Chris Hemsworth\u0026#34;, \u0026#34;Olivia Wilde\u0026#34; ] } 如果是排除字段，则用户在检索文档时会看到除这些字段之外的所有内容。例如，如果排除这些相同的字段，则相同的搜索结果可能如下所示：\nPOST movies/_doc/2 { \u0026#34;directors\u0026#34;: [ \u0026#34;Ron Howard\u0026#34; ], \u0026#34;plot\u0026#34;: \u0026#34;A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\u0026#34;, \u0026#34;genres\u0026#34;: [ \u0026#34;Action\u0026#34;, \u0026#34;Biography\u0026#34;, \u0026#34;Drama\u0026#34;, \u0026#34;Sport\u0026#34; ] } 您可以使用配置文件 role.yml 和 REST API 来指定字段级安全设置。\n 要排除字段，可以在配置 role.yml 或通过 REST API，在字段名称前添加 ~ 。 字段名称支持通配符 (*).  role.yml #  limited_movie: cluster: [] indices: - names: - movies privileges: - \u0026#34;read\u0026#34; field_security: - \u0026#34;~actors\u0026#34; - \u0026#34;~title\u0026#34; - \u0026#34;~year\u0026#34; REST API #  参照 创建角色.\n注意多个角色 #  如果将用户映射到多个角色，我们建议这些角色对每个索引使用 include or exclude 语句。安全模块使用 AND 运算符来评估字段级权限的设置，因此组合包含和排除语句可能会导致这两种行为都无法正常工作。\n例如，在 movies 索引中，如果您在一个角色中包含 actors 、 title 和 year，在另一个角色中排除 actors 、 title 和 genres，然后将这两个角色映射到同一用户，则搜索结果可能如下所示：\n{ \u0026#34;_index\u0026#34;: \u0026#34;movies\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;year\u0026#34;: 2013, \u0026#34;directors\u0026#34;: [\u0026#34;Ron Howard\u0026#34;], \u0026#34;plot\u0026#34;: \u0026#34;A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\u0026#34; } } 与文档级权限的交互 #   文档级权限 依赖于 Easysearch 查询，这意味着查询中的所有相关字段都必须可见才能正常工作。如果将字段级权限与文档级权限结合使用，请确保不要限制对文档级权限字段的访问。\n","subcategory":null,"summary":"","tags":null,"title":"字段权限","url":"/easysearch/v1.15.0/docs/references/security/access-control/field-level-security/"},{"category":null,"content":"增删改查 #  您可以使用 REST API 对数据进行索引。存在两个 API：索引 API 和 _bulk API。\n对于新数据增量到达的情况（例如，来自小型企业的客户订单），您可以使用索引 API 在文档到达时单独添加文档。对于数据流不太频繁的情况（例如，每周更新一次营销网站），您可能更希望生成一个文件并将其发送到 _bulk API。对于大量文档，将请求汇总在一起并使用 _bulk API 可提供优异的性能。然而，如果您的文档非常庞大，您可能需要单独对它们进行索引。\n索引介绍 #  在搜索数据之前，必须对其进行索引。索引是搜索引擎组织数据以便快速检索的方法。生成的结构称为索引。\n在 Easysearch 中，数据的基本单位是 JSON 文档。在索引中，Easysearch 使用唯一的 ID 标识每个文档。\n对索引 API 的请求如下所示：\nPUT \u0026lt;index\u0026gt;/_doc/\u0026lt;id\u0026gt; { \u0026#34;A JSON\u0026#34;: \u0026#34;document\u0026#34; } 对 _bulk API 的请求看起来有点不同，因为您在批量数据中指定了索引和 ID：\nPOST _bulk { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;\u0026lt;index\u0026gt;\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;\u0026lt;id\u0026gt;\u0026#34; } } { \u0026#34;A JSON\u0026#34;: \u0026#34;document\u0026#34; } 批量数据采用 _bulk API 操作必须符合特定的格式，该格式要求在每行（包括最后一行）的末尾都有一个换行符（ \\n ）。这是基本格式：\nAction and metadata\\n Optional document\\n Action and metadata\\n Optional document\\n  文档是可选的，因为 删除 操作不需要文档。其他操作（ 索引 、 创建 和 更新 ）都需要文档。如果您特别希望在文档已存在的情况下操作失败，请使用 创建 操作而不是 索引 操作。\n 要使用 curl 命令对批量数据采用 _bulk API 进行索引，请导航到 data.json 文件的所在文件夹，然后运行以下命令：\ncurl -H \u0026#34;Content-Type: application/x-ndjson\u0026#34; -POST https://localhost:9200/data/_bulk -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; --insecure --data-binary \u0026#34;@data.json\u0026#34; 如果 _bulk API 中的任何一个操作失败，Easysearch 将继续执行其他操作。检查响应中的 items 数组，找出问题所在。 items 数组中的条目与请求中指定的操作的顺序相同。\nEasysearch 的特点是当您将文档添加到不存在的索引时自动创建索引。如果不在请求中指定 ID，它还具有自动生成 ID 的功能。这个简单的示例会自动创建 movies 索引，为文档编制索引，并为其分配一个唯一的 ID：\nPOST movies/_doc { \u0026#34;title\u0026#34;: \u0026#34;Spirited Away\u0026#34; } 自动生成 ID 有一个明显的缺点：因为索引请求没有指定文档 ID，所以以后无法轻松更新文档。此外，如果您运行此请求 10 次，Easysearch 会将此文档索引为具有唯一 ID 的 10 个不同文档。要指定 ID 为 1，请使用以下请求，并注意使用 PUT 而不是 POST:\nPUT movies/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;Spirited Away\u0026#34; } 因为您必须指定一个 ID，所以如果您运行此命令 10 次，仍然只有一个文档被索引， _version 字段的值增加到 10。\n索引默认为一个主分片和一个副本。如果要指定非默认设置，请在添加文档之前创建索引：\nPUT more-movies { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 6, \u0026#34;number_of_replicas\u0026#34;: 2 } } 命名限制 #  Easysearch 索引具有以下命名限制：\n 所有字母必须小写 索引名称不能以_（下划线）或-（连字符）开头 索引名称不能包含空格、逗号或以下字符:  : , \u0026quot; , * , + , / , \\ , | , ? , # , \u0026gt; , or \u0026lt;\n检索数据 #  索引文档后，可以通过向用于索引的同一端点发送 GET 请求来检索文档：\nGET movies/_doc/1 { \u0026quot;_index\u0026quot; : \u0026quot;movies\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_version\u0026quot; : 1, \u0026quot;_seq_no\u0026quot; : 0, \u0026quot;_primary_term\u0026quot; : 1, \u0026quot;found\u0026quot; : true, \u0026quot;_source\u0026quot; : { \u0026quot;title\u0026quot; : \u0026quot;Spirited Away\u0026quot; } } 您可以在 _source 对象中查看文档。如果未找到文档，则 found 键为 false ， _source 对象不是响应的一部分。\n要使用单个命令检索多个文档，请使用 _mget 操作。\n检索多个文档的格式类似于 _bulk 操作，其中必须在请求正文中指定索引和 ID：\nGET _mget { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;\u0026lt;index\u0026gt;\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;\u0026lt;id\u0026gt;\u0026#34; }, { \u0026#34;_index\u0026#34;: \u0026#34;\u0026lt;index\u0026gt;\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;\u0026lt;id\u0026gt;\u0026#34; } ] } 仅返回文档中的特定字段:\nGET _mget { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;\u0026lt;index\u0026gt;\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;\u0026lt;id\u0026gt;\u0026#34;, \u0026#34;_source\u0026#34;: \u0026#34;field1\u0026#34; }, { \u0026#34;_index\u0026#34;: \u0026#34;\u0026lt;index\u0026gt;\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;\u0026lt;id\u0026gt;\u0026#34;, \u0026#34;_source\u0026#34;: \u0026#34;field2\u0026#34; } ] } 检查文档是否存在:\nHEAD movies/_doc/\u0026lt;doc-id\u0026gt; 如果文档存在，则返回 200 OK 响应，如果不存在，则会返回 404-Not Found 错误。\n更新数据 #  要更新现有字段或添加新字段，请向 _update 操作发送 POST 请求，其中包含对 doc 对象的更改：\nPOST movies/_update/1 { \u0026#34;doc\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Castle in the Sky\u0026#34;, \u0026#34;genre\u0026#34;: [\u0026#34;Animation\u0026#34;, \u0026#34;Fantasy\u0026#34;] } } 验证更新的 title 字段和新的 genre 字段：\nGET movies/_doc/1 { \u0026quot;_index\u0026quot; : \u0026quot;movies\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_version\u0026quot; : 2, \u0026quot;_seq_no\u0026quot; : 1, \u0026quot;_primary_term\u0026quot; : 1, \u0026quot;found\u0026quot; : true, \u0026quot;_source\u0026quot; : { \u0026quot;title\u0026quot; : \u0026quot;Castle in the Sky\u0026quot;, \u0026quot;genre\u0026quot; : [ \u0026quot;Animation\u0026quot;, \u0026quot;Fantasy\u0026quot; ] } } 文档还具有递增的 _version 字段。使用此字段可跟踪文档的更新次数。\nPOST 请求对文档进行部分更新。要完全替换文档，请使用 PUT 请求：\nPUT movies/_doc/1 { \u0026#34;title\u0026#34;: \u0026#34;Spirited Away\u0026#34; } ID 为 1 的文档将仅包含 title 字段，因为整个文档将被此 PUT 请求中索引的文档替换。\n使用 upsert 对象根据文档是否已存在有条件地更新文档。在这里，如果文档存在，其 title 字段将更改为 Castle in the Sky 。如果没有，Easysearch 将在 upsert 对象中为文档编制索引。\nPOST movies/_update/2 { \u0026#34;doc\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Castle in the Sky\u0026#34; }, \u0026#34;upsert\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Only Yesterday\u0026#34;, \u0026#34;genre\u0026#34;: [\u0026#34;Animation\u0026#34;, \u0026#34;Fantasy\u0026#34;], \u0026#34;date\u0026#34;: 1993 } } 响应示例 #  { \u0026#34;_index\u0026#34;: \u0026#34;movies\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 3, \u0026#34;_primary_term\u0026#34;: 1 } 文档的每个更新操作都具有 _seq_no 和 _primary_term 值的唯一组合。\nEasysearch 首先将更新写入主分片，然后将此更改发送到所有副本分片。如果基于 Easysearch 的应用程序的多个用户对同一索引中的现有文档进行更新，则会出现一个不常见的问题。在这种情况下，另一个用户可以在从主分片接收您的更新之前从副本读取并更新文档。然后，更新操作将更新文档的旧版本。在最佳情况下，您和其他用户进行了相同的更改，并且文档保持准确。在最坏的情况下，返回文档包含过期信息。\n要防止这种情况，请在请求头中使用 _seq_no 和 _primary_term 值：\nPOST movies/_update/2?if_seq_no=3\u0026amp;if_primary_term=1 { \u0026#34;doc\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Castle in the Sky\u0026#34;, \u0026#34;genre\u0026#34;: [\u0026#34;Animation\u0026#34;, \u0026#34;Fantasy\u0026#34;] } } 如果在检索文档后更新文档，则 _seq_no 和 _primary_term 值不同，更新操作将失败，并出现 409-Conflict 错误。\n使用 _bulk API 时，请在操作元数据中指定 _seq_no 和 _primary_term 值。\n删除数据 #  要从索引中删除文档，请使用 delete 请求:\nDELETE movies/_doc/1 DELETE 操作使 _version 字段递增。如果将文档添加回相同的 ID，则 _version 字段将再次递增。出现这种行为是因为 Easysearch 删除了文档 _source ，但保留了其元数据。\n","subcategory":null,"summary":"","tags":null,"title":"增删改查","url":"/easysearch/v1.15.0/docs/references/document/index-data/"},{"category":null,"content":"cat API #  您可以使用紧凑且对齐的文本 （CAT） API 以易于理解的表格格式获取有关集群的基本统计信息。cat API 是一个人类可读的接口，它返回纯文本而不是传统的 JSON。\n使用 cat API，您可以回答诸如哪个节点是选定的主节点、集群处于什么状态、每个索引中有多少文档等问题。\n要查看 cat API 中的可用操作，请使用以下命令：\nGET _cat 还可以在查询中使用以下字符串参数。\n   参数 描述     ?v 通过向列添加标题使输出更详细。它还添加了一些格式，以帮助将每列对齐在一起。此页面上的所有示例都包含 v 参数。   ?help 列出给定操作的默认标头和其他可用标头。   ?h 将输出限制为特定标头。   ?format 以 JSON、YAML 或 CBOR 格式输出结果。   ?sort 按指定列对输出进行排序。    要查看每列表示的内容，请使用 ?v 参数：\nGET _cat/\u0026lt;operation_name\u0026gt;?v 要查看所有可用的标头，请使用 ?help 参数：\nGET _cat/\u0026lt;operation_name\u0026gt;?help 要将输出限制为标头的子集，请使用 ?h 参数：\nGET _cat/\u0026lt;operation_name\u0026gt;?h=\u0026lt;header_name_1\u0026gt;,\u0026lt;header_name_2\u0026gt;\u0026amp;v 通常，对于任何操作，您可以使用 ?help 参数找出可用的标头，然后使用 ?h 参数将输出限制为您关注的标头。 Typically, for any operation you can find out what headers are available using the ?help parameter, and then use the ?h parameter to limit the output to only the headers that you care about.\n别名 Aliases #  列出别名到索引的映射，以及路由和筛选信息。\nGET _cat/aliases?v 若要将信息限制为特定别名，请在查询后添加别名。\nGET _cat/aliases/\u0026lt;alias\u0026gt;?v 分配 Allocation #  列出索引的磁盘空间分配以及每个节点上的分片数。 Default request:\nGET _cat/allocation?v 文档数 Count #  列出群集中的文档数。\nGET _cat/count?v 若要查看特定索引中的文档数，请在查询后添加索引名称。\nGET _cat/count/\u0026lt;index\u0026gt;?v 字段数据 Field data #  列出每个节点的每个字段使用的内存大小。\nGET _cat/fielddata?v 若要将信息限制为特定字段，请在查询后添加字段名称。\nGET _cat/fielddata/\u0026lt;fields\u0026gt;?v 集群健康状态 Health #  列出群集的状态、群集已启动的时间、节点数以及有助于分析群集运行状况的其他有用信息。\nGET _cat/health?v 索引信息 Indices #  列出与索引相关的信息 - 索引使用的磁盘空间量、分片数、运行状况等。\nGET _cat/indices?v 若要将信息限制为特定索引，请在查询后添加索引名称。\nGET _cat/indices/\u0026lt;index\u0026gt;?v 主节点 Master #  列出有助于识别选定主节点的信息。\nGET _cat/master?v 节点属性 Node attributes #  列出自定义节点的属性。\nGET _cat/nodeattrs?v 节点信息 Nodes #  列出节点级别信息，包括节点角色和负载指标。\n一些重要的节点指标是 pid ， name ， master ， ip ， port ， version ， build ， jdk ，以及 disk ， heap ， ram 和 file_desc 。\nGET _cat/nodes?v 待处理任务 Pending tasks #  列出所有待处理任务的进度，包括任务优先级和队列中的时间。\nGET _cat/pending_tasks?v 已安装插件 Plugins #  列出已安装插件的名称、组件和版本。\nGET _cat/plugins?v 恢复 Recovery #  列出所有已完成和正在进行的索引和分片的恢复。\nGET _cat/recovery?v 若要仅查看特定索引的恢复情况，请在查询后添加索引名称。\nGET _cat/recovery/\u0026lt;index\u0026gt;?v 存储库 Repositories #  列出所有快照存储库及其类型。\nGET _cat/repositories?v 段信息 Segments #  列出每个索引的 Lucene 段级别信息。\nGET _cat/segments?v 若要仅查看有关特定索引段的信息，请在查询后添加索引名称。\nGET _cat/segments/\u0026lt;index\u0026gt;?v 分片 Shards #  列出所有主分片和副本分片的状态及其分布方式。\nGET _cat/shards?v 若要仅查看特定索引的分片信息，请在查询后添加索引名称。\nGET _cat/shards/\u0026lt;index\u0026gt;?v 快照 Snapshots #  列出存储库的所有快照。\nGET _cat/snapshots/\u0026lt;repository\u0026gt;?v 任务 Tasks #  列出群集上当前运行的所有任务的进度。\nGET _cat/tasks?v 索引模板 Templates #  列出索引模板的名称、模式、订单号和版本号。\nGET _cat/templates?v 线程池状态 Thread pool #  列出每个节点上不同线程池的活动线程、排队线程和拒绝线程。\nGET _cat/thread_pool?v 若要将信息限制为特定线程池，请在查询后添加线程池名称。\nGET _cat/thread_pool/\u0026lt;thread_pool\u0026gt;?v ","subcategory":null,"summary":"","tags":null,"title":"CAT API","url":"/easysearch/v1.15.0/docs/references/management/catapis/"},{"category":null,"content":"文档级权限 #  文档级权限允许您将角色限制为索引中文档的一部分子集。\n参考设置 #  文档级权限使用 Easysearch 查询 DSL 来定义角色授予对哪些文档的访问权限。\n{ \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;public\u0026#34;: \u0026#34;true\u0026#34; } } } } 上面的查询指定了该角色访问的文档里面，其字段 public 必须匹配 true。\n指定字段 query 并设置为将上面的查询，并对查询字符串进行转义，最后的角色设置如下：\nPUT _security/role/public_data { \u0026#34;cluster\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;indices\u0026#34;: [{ \u0026#34;names\u0026#34;: [ \u0026#34;pub*\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;{\\\u0026#34;term\\\u0026#34;: { \\\u0026#34;public\\\u0026#34;: true}}\u0026#34;, \u0026#34;privileges\u0026#34;: [ \u0026#34;read\u0026#34; ] }] } 上面的查询也可以根据需要写的很复杂，但是我们建议保持简单，以最大程度地减少文档级安全功能对集群的性能影响。\n参数替换 #  查询过程中可以利用上下文变量，可根据当前用户的属性来强制实施规则替换。例如 ${user.name} 将替换为当前用户的名称。\n如下规则允许用户读取字段 readable_by 为其用户名值的任何文档：\nPUT _security/role/user_data { \u0026#34;cluster\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;indices\u0026#34;: [{ \u0026#34;names\u0026#34;: [ \u0026#34;pub*\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;{\\\u0026#34;term\\\u0026#34;: { \\\u0026#34;readable_by\\\u0026#34;: \\\u0026#34;${user.name}\\\u0026#34;}}\u0026#34;, \u0026#34;privileges\u0026#34;: [ \u0026#34;read\u0026#34; ] }] } 支持以下变量：\n   名称 描述     ${user.name} 用户名.   ${user.roles} 用户的角色列表，使用逗号分割。   ${attr.\u0026lt;TYPE\u0026gt;.\u0026lt;NAME\u0026gt;} 用户的自定义属性， \u0026lt;TYPE\u0026gt; 支持 internal 和 ldap。    基于属性的权限控制 #  可以将角色和参数替换与 terms_set 查询一起使用，以启用基于属性的权限控制。\n定义角色 #  PUT _security/role/abac { \u0026#34;indices\u0026#34;: [{ \u0026#34;names\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;query\u0026#34;: \u0026#34;{\\\u0026#34;terms_set\\\u0026#34;: {\\\u0026#34;security_attributes\\\u0026#34;: {\\\u0026#34;terms\\\u0026#34;: [${attr.internal.permissions}], \\\u0026#34;minimum_should_match_script\\\u0026#34;: {\\\u0026#34;source\\\u0026#34;: \\\u0026#34;doc[\u0026#39;security_attributes\u0026#39;].length\\\u0026#34;}}}}\u0026#34;, \u0026#34;privileges\u0026#34;: [ \u0026#34;read\u0026#34; ] }] } 定义用户 #  PUT _security/user/user1 { \u0026#34;password\u0026#34;: \u0026#34;asdf\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;abac\u0026#34;], \u0026#34;attributes\u0026#34;: { \u0026#34;permissions\u0026#34;: \u0026#34;\\\u0026#34;att1\\\u0026#34;, \\\u0026#34;att2\\\u0026#34;, \\\u0026#34;att3\\\u0026#34;\u0026#34; } } 定义文档 #  POST easysearch/_doc/1 { \u0026quot;security_attributes\u0026quot;:\u0026quot;att1\u0026quot; }  注意，索引的 security_attributes 必须是 keyword 类型。\n ","subcategory":null,"summary":"","tags":null,"title":"文档权限","url":"/easysearch/v1.15.0/docs/references/security/access-control/document-level-security/"},{"category":null,"content":"搭建集群 #  在深入研究 Easysearch 以及搜索和聚合数据之前，你首先需要创建一个 Easysearch 集群。\nEasysearch 可以作为一个单节点或多节点集群运行。一般来说，配置两者的步骤是非常相似的。本页演示了如何创建和配置一个多节点集群，但只需做一些小的调整，你可以按照同样的步骤创建一个单节点集群。\n要根据你的要求创建和部署一个 Easysearch 集群，重要的是要了解节点发现和集群形成是如何工作的，以及哪些设置对它们有影响。\n有许多方法可以设计一个集群。下面的插图显示了一个基本架构。\n这是一个四节点的集群，有一个专用的主节点，一个专用的协调节点，还有两个数据节点，这两个节点是主节点，也是用来摄取数据的。\n下表提供了节点类型的简要描述。\n   节点类型 描述 最佳实践     Master 管理集群的整体运作并跟踪集群的状态。这包括创建和删除索引，跟踪加入和离开集群的节点，检查集群中每个节点的健康状况（通过运行 ping 请求），并将分片分配给节点。 在三个不同区域的三个专用主节点是几乎所有生产用例的正确方法。这可以确保你的集群永远不会失去法定人数。两个节点在大部分时间都是空闲的，除非一个节点宕机或需要一些维护。   Data 存储和搜索数据。在本地分片上执行所有与数据有关的操作（索引、搜索、聚合）。这些是你的集群的工作节点，需要比其他任何节点类型更多的磁盘空间。 当你添加数据节点时，保持它们在各区之间的平衡。例如，如果你有三个区，以三的倍数添加数据节点，每个区一个。我们建议使用存储和内存重的节点。    默认情况下，每个节点是一个主节点和数据节点。决定节点的数量，分配节点类型，并为每个节点类型选择硬件，取决于你的使用情况。你必须考虑到一些因素，如你想保留数据的时间，你的文件的平均大小，你的典型工作负载（索引、搜索、聚合），你的预期性价比，你的风险容忍度，等等。\n在你评估所有这些要求之后，我们建议你使用一个管理工具。要开始使用 INFINI Console，请参阅 INFINI Console 文档。\n本页演示了如何处理不同的节点类型。它假设你有一个类似于前面插图的四节点集群。\n前提条件 #  在你开始之前，你必须在你的所有节点上安装和配置 Easysearch。有关可用选项的信息，请参见 安装和配置。\n完成后，使用 SSH 连接到每个节点，然后打开 config/easysearch.yml 文件。\n你可以在这个文件中为你的集群设置所有的配置。\nStep 1: 命名集群 #  为集群指定一个唯一的名字。如果你不指定集群名称，它将被默认设置为 easysearch。设置一个描述性的集群名称很重要，特别是如果你想在一个网络内运行多个集群。\n要指定集群名称，请修改下面一行。\n#cluster.name: my-application to\ncluster.name: easy-cluster 在所有的节点上做同样的修改，以确保它们会加入形成一个集群。\nStep 2: 为集群中的每个节点设置节点属性 #  在你命名集群后，为集群中的每个节点设置节点属性。\nMaster node #  给你的主节点一个名字。如果你不指定一个名字，Easysearch 会分配一个机器生成的名字，这使得节点难以监控和排除故障。\nnode.name: easy-master 你也可以明确地指定这个节点是一个主节点。这在默认情况下已经是 true ，但添加它可以更容易识别主节点。\nnode.master: true 然后使该节点成为专用的主节点，不会作为数据节点执行双重任务。\nnode.data: false Data nodes #  将两个节点的名称分别改为 easy-d1 和 easy-d2 。\nnode.name: easy-d1 node.name: easy-d2 你可以让它们既成为主节点，也作为数据节点。\nnode.master: true node.data: true Step 3: 将集群与特定的 IP 地址绑定 #  network_host 定义了用于绑定节点的 IP 地址。默认情况下，Easysearch 在本地主机上监听，这将集群限制在一个节点上。你也可以使用 _local_ 和 _site_ 来绑定任何环回或站点本地地址，无论是 IPv4 还是 IPv6。\nnetwork.host: [_local_, _site_] 要形成一个多节点的集群，指定节点的 IP 地址。\nnetwork.host: \u0026lt;IP address of the node\u0026gt; 请确保在你的所有节点上配置这些设置。\nStep 4: 为集群配置发现主机 #  现在你已经配置了网络主机，你需要配置发现主机。\nZen Discovery 是内置的、默认的机制，使用 单播来寻找集群中的其他节点。\n一般来说，你可以直接将所有符合主控条件的节点添加到 discovery.seed_hosts 数组中。当一个节点启动时，它会找到其他符合主控条件的节点，确定哪一个是主控，并要求加入集群。\n例如，对于 easy-master ，这一行看起来是这样的。\ndiscovery.seed_hosts: [\u0026#34;\u0026lt;private IP of easy-d1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;private IP of easy-d2\u0026gt;\u0026#34;, \u0026#34;\u0026lt;private IP of easy-c1\u0026gt;\u0026#34;] Step 5: 启动集群 #  设置好配置后，在所有节点上启动 Easysearch。\nsudo systemctl start easysearch.service 请参阅 以服务的形式运行 Easysearch ，了解如何创建和启动服务。\n然后去看日志文件，看看集群的形成情况。\nless /var/log/easysearch/easy-cluster.log 在任何节点上执行以下 _cat 查询，以查看作为集群的所有节点。\ncurl -XGET https://\u0026lt;private-ip\u0026gt;:9200/_cat/nodes?v -u \u0026#39;admin:xxxxxxxxxxxx\u0026#39; --insecure ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name x.x.x.x 13 61 0 0.02 0.04 0.05 mi * easy-master x.x.x.x 16 60 0 0.06 0.05 0.05 md - easy-d1 x.x.x.x 34 38 0 0.12 0.07 0.06 md - easy-d2 x.x.x.x 23 38 0 0.12 0.07 0.06 md - easy-c1 为了更好地了解和监控你的集群，使用 cat API。\n(高级) 第 6 步：配置分片分配意识或强制意识 #  如果你的节点分布在几个地理区域，你可以配置分片分配意识，将所有的复制分片分配到一个与主分片不同的区域。\n有了分片分配意识，如果你的一个区域的节点发生故障，你可以保证你的复制分片分布在你的其他区域。它增加了一个容错层，以确保你的数据在区域故障中幸存下来，而不仅仅是单个节点的故障。\n要配置碎片分配意识，请分别向 easy-d1 和 easy-d2 添加区域属性。\nnode.attr.zone: zoneA node.attr.zone: zoneB 更新集群设置。\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.awareness.attributes\u0026#34;: \u0026#34;zone\u0026#34; } } 你可以使用 persistent 或 transient 设置。我们推荐使用 persistent 设置，因为它在集群重启后仍然有效。瞬时设置不会在集群重启时持续存在。\n碎片分配意识试图在多个区中分离主碎片和复制碎片。但是，如果只有一个区是可用的（比如在一个区发生故障后），Easysearch 会将复制分片分配到唯一剩下的区。\n另一个选择是要求主副 shards 永远不被分配到同一个区。这被称为强制意识。\n要配置强制意识，为你的区属性指定所有可能的值。\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.awareness.attributes\u0026#34;: \u0026#34;zone\u0026#34;, \u0026#34;cluster.routing.allocation.awareness.force.zone.values\u0026#34;:[\u0026#34;zoneA\u0026#34;, \u0026#34;zoneB\u0026#34;] } } 现在，如果一个数据节点发生故障，强制意识不会将副本分配给同一区域的节点。相反，集群进入黄色状态，只有当另一个区的节点上线时才会分配副本。\n在我们的两区架构中，如果 easy-d1 和 easy-d2 的利用率低于 50% ，我们就可以使用分配意识，这样他们每个人都有存储容量来分配同一区的复制。 如果不是这样，并且 easy-d1 和 easy-d2 没有容量来容纳所有的主分片和复制分片，我们可以使用强制意识。这种方法有助于确保在发生故障时，Easysearch 不会因为缺乏存储而使你最后剩下的区域超载并锁定你的集群。\n选择分配意识或强制意识取决于你在每个区可能需要多少空间来平衡你的主副分片。\n(高级）第 7 步：建立一个 hot-warm 架构 #  你可以设计一个 hot-warm 架构，首先将你的数据索引到热节点上\u0026ndash;快速而昂贵\u0026ndash;然后在一段时间后将它们转移到暖节点上\u0026ndash;慢速而廉价。\n如果你分析的是很少更新的时间序列数据，并希望将较旧的数据转移到较便宜的存储上，这种架构就很适合。\n这种架构有助于节省存储成本。与其增加热节点的数量并使用快速、昂贵的存储，你可以为你不经常访问的数据增加暖节点。\n要配置一个 hot-warm 的存储架构，分别给 easy-d1 和 easy-d2 添加 temp 属性。\nnode.attr.temp: hot node.attr.temp: warm 你可以将属性名称和值设置为任何你想要的东西，只要它对你所有的热节点和温节点是一致的。\n要给热节点添加一个索引 newindex 。\nPUT newindex { \u0026#34;settings\u0026#34;: { \u0026#34;index.routing.allocation.require.temp\u0026#34;: \u0026#34;hot\u0026#34; } } 请看下面的 newindex 的分片分配。\nGET _cat/shards/newindex?v index shard prirep state docs store ip node new_index 2 p STARTED 0 230b 10.0.0.225 easy-d1 new_index 2 r UNASSIGNED new_index 3 p STARTED 0 230b 10.0.0.225 easy-d1 new_index 3 r UNASSIGNED new_index 4 p STARTED 0 230b 10.0.0.225 easy-d1 new_index 4 r UNASSIGNED new_index 1 p STARTED 0 230b 10.0.0.225 easy-d1 new_index 1 r UNASSIGNED new_index 0 p STARTED 0 230b 10.0.0.225 easy-d1 new_index 0 r UNASSIGNED 在这个例子中，所有的主碎片都被分配到 easy-d1 ，这是我们的热节点。所有的副本碎片都没有分配，因为我们强迫这个索引只分配给热节点。\n要将索引 oldindex 添加到热节点上。\nPUT oldindex { \u0026#34;settings\u0026#34;: { \u0026#34;index.routing.allocation.require.temp\u0026#34;: \u0026#34;warm\u0026#34; } } oldindex 的分片分配：\nGET _cat/shards/oldindex?v index shard prirep state docs store ip node old_index 2 p STARTED 0 230b 10.0.0.74 easy-d2 old_index 2 r UNASSIGNED old_index 3 p STARTED 0 230b 10.0.0.74 easy-d2 old_index 3 r UNASSIGNED old_index 4 p STARTED 0 230b 10.0.0.74 easy-d2 old_index 4 r UNASSIGNED old_index 1 p STARTED 0 230b 10.0.0.74 easy-d2 old_index 1 r UNASSIGNED old_index 0 p STARTED 0 230b 10.0.0.74 easy-d2 old_index 0 r UNASSIGNED 在这种情况下，所有的主分片都被分配到 easy-d2 。同样地，所有的复制分片都没有被分配，因为我们只有一个温暖的节点。\n一个流行的方法是配置你的 index templates，将 index.routing.allocation.require.temp 值设置为 hot 。这样，Easysearch 将你的最新数据存储在热节点上。\n","subcategory":null,"summary":"","tags":null,"title":"搭建集群","url":"/easysearch/v1.15.0/docs/references/management/cluster/"},{"category":null,"content":"角色与用户 #  安全模块包括一个内部用户数据库。使用此数据库代替外部身份验证系统（如 LDAP 或 Active Directory）或作为外部身份验证系统的补充。\n角色是控制对群集的访问的核心方式。角色包含集群范围权限、特定于索引的权限、文档和字段级安全性以及租户的任意组合。然后，将用户映射到这些角色，以便用户获得这些权限。\n除非您需要创建新的 只读或隐藏用户，我们强烈建议使用 REST API 来创建新的用户、角色和角色映射。.yml 文件用于初始设置，而不是持续使用。\n创建用户 #  user.yml #  参照 本地文件配置。\nREST API #  参照 创建用户。\n创建角色 #  role.yml #  参照 本地文件配置。\nREST API #  参照 创建角色。\n映射用户到角色 #  role_mapping.yml #  参照 本地文件配置。\nREST API #  参照 创建角色映射。\n","subcategory":null,"summary":"","tags":null,"title":"用户角色","url":"/easysearch/v1.15.0/docs/references/security/access-control/users-roles/"},{"category":null,"content":"部署Easysearch Operator #  这里我们准备部署一个 3 节点的Easysearch 集群，准备 three-nodes-easysearch-cluster.yaml 文件，文件内容如下所示，并对关键字段都进行了注释。\napiVersion: infinilabs.infinilabs.com/v1 kind: SearchCluster # 自定义的资源类型 metadata: name: threenodes # Easysearch 集群的名称 namespace: default # Easysearch 集群所在的命名空间 spec: # 规格 security: # 安全相关 config: adminSecret: # admin证书配置 name: easysearch-admin-certs adminCredentialsSecret: # 账户密码配置 name: threenodes-admin-password tls: # tls 协议配置，包括节点间的transport，以及访问集群的http http: # 访问集群http配置 generate: false # 是否需要集群自动生成证书 secret: # 自定义证书配置 name: easysearch-certs transport: # 集群间访问配置 generate: false perNode: false # 是否给每一个节点配置证书 secret: # 自定义证书配置 name: easysearch-certs nodesDn: [\u0026#34;CN=Easysearch_Node\u0026#34;] adminDn: [\u0026#34;CN=Easysearch_Admin\u0026#34;] general: # 通用配置 snapshotRepositories: # s3 快照配置 - name: s3_repository # 配置的s3快照的名称 type: s3 # 快照类型 settings: # 快照配置 bucket: es-operator-bucket # s3中的桶，需提前建好 access_key: minioadmin # 访问s3密钥 secret_key: minioadmin endpoint: http://192.168.3.185:19000 # s3 访问地址 # compress: true version: \u0026#34;1.7.0-223\u0026#34; # Easysearch 版本 httpPort: 9200 # Easysearch 监听http端口 vendor: Easysearch serviceAccount: controller-manager # 访问k8s的service account，需要提前配置好，主要用于访问k8s资源 serviceName: threenodes monitoring: enable: false pluginsList: [] drainDataNodes: false # 在删除节点之前，是否需要先将节点上的数据迁移出去 nodePools: # Easysearch 节点池 - component: masters # 组件名称，作为标识，比如可以是masters，snapshotconfig securityconfig replicas: 3 # 配置集群节点个数 diskSize: \u0026#34;30Gi\u0026#34; # 每个节点占多少磁盘空间 jvm: -Xmx4G -Xms4G # jvm 参数 nodeSelector: # 调度时选择node resources: # 节点所需资源配置，包括申请值和最大值 requests: memory: \u0026#34;4Gi\u0026#34; cpu: \u0026#34;1\u0026#34; limits: memory: \u0026#34;5Gi\u0026#34; cpu: \u0026#34;2\u0026#34; roles: # 节点的角色，一般有master主节点，data数据节点，以及两者可 - \u0026#34;master\u0026#34; - \u0026#34;data\u0026#34; 编写好上述的 Operator yaml 文件，执行命令：\nkubectl create -f three-nodes-easysearch-cluster.yaml 为了快速组建集群，首先会创建一个 bootstrap 节点，然后各个节点同时并发启动，等待一段时间后集群创建成功，分别创建 3 个节点：\nthreenodes-masters-0 threenodes-masters-1 threenodes-masters-2 查看 pvc，对应到 3 个节点分别创建了 3 个 pvc:\ndata-threenodes-masters-0 data-threenodes-masters-1 data-threenodes-masters-2 如下图：\n查看 secret，已经创建了 threenodes-admin-password，用于保存账号密码\n进入 console-0，查看集群状态\ncurl -ku admin:xxxxxxxxxxxx https://threenodes.default.svc.cluster.local:9200 { \u0026quot;name\u0026quot; : \u0026quot;threenodes-masters-0\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;threenodes\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;WIuPcQ2gR4aG2hFrTLF4NQ\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;distribution\u0026quot; : \u0026quot;easysearch\u0026quot;, \u0026quot;number\u0026quot; : \u0026quot;1.7.0\u0026quot;, \u0026quot;distributor\u0026quot; : \u0026quot;INFINI Labs\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;b8a4c52487d245a65679fa4dc45db166cb919c4d\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2023-12-15T01:52:39.584409Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;8.11.2\u0026quot;, \u0026quot;minimum_wire_lucene_version\u0026quot; : \u0026quot;7.7.0\u0026quot;, \u0026quot;minimum_lucene_index_compatibility_version\u0026quot; : \u0026quot;7.7.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, For Easy Search!\u0026quot; } 可继续通过 console web 端操作集群\n至此，说明整个集群成功运行起来。\n  autoplay=\u0026quot;1\u0026quot; preload=\u0026quot;1\u0026quot; start-at=\u0026quot;0\u0026quot; speed=\u0026quot;2\u0026quot; \u0026gt;\u0026lt;/asciinema-player\u0026gt;  https://asciinema.org/a/cvWISVzr23CPiHYeZwexF4DnP\n","subcategory":null,"summary":"","tags":null,"title":"部署 Easysearch","url":"/easysearch/v1.15.0/docs/getting-started/install/operator/deploy_easysearch/"},{"category":null,"content":"系统索引 #  Easysearch 默认的身份信息存放在一个受保护的系统索引里面，名称为：.security， 将索引设置为系统索引可以对该索引的数据进行额外的保护，因为即使您的用户帐户对所有索引具有读取权限，也无法直接访问此系统索引中的数据。\n您可以在 easysearch.yml 中添加其它您希望需要受到保护的索引。\nsecurity.system_indices.enabled: true security.system_indices.indices: [\u0026#34;.infini-*\u0026#34;] 如果要访问系统索引，必须使用管理员证书的方式来进行： 配置管理证书:\ncurl -k --cert ./admin.crt --key ./admin.key -XGET \u0026#39;https://localhost:9200/.security/_search\u0026#39; 另一种方法是从每个节点上的 security.system_indices.index 列表中删除该索引，然后重新启动 Easysearch 即可正常操作该索引。\n","subcategory":null,"summary":"","tags":null,"title":"系统索引","url":"/easysearch/v1.15.0/docs/references/security/configuration/system-indices/"},{"category":null,"content":"配置 TLS 证书 #  Easysearch 可以通过启用 TLS 传输加密来保护您数据的网络传输安全。 TLS 的相关设置要在配置文件 easysearch.yml 里面进行。主要包括两个部分的配置：传输层和 HTTP 层。传输层的 TLS 是必需的，HTTP 层的 TLS 的配置是可选的。\n默认的配置如下：\nsecurity.ssl.transport.cert_file: instance.crt security.ssl.transport.key_file: instance.key security.ssl.transport.ca_file: ca.crt security.ssl.http.enabled: true security.ssl.http.cert_file: instance.crt security.ssl.http.key_file: instance.key security.ssl.http.ca_file: ca.crt 一键生成证书 #  启用 TLS 需要设置证书才能工作，通过执行命令 ./bin/initialize.sh 可以一键生成 TLS 证书，如下：\n➜ ./bin/initialize.sh Generating RSA private key, 2048 bit long modulus .......................+++ ...............................+++ e is 65537 (0x10001) Generating RSA private key, 2048 bit long modulus .......................................................................................................................................................................................+++ ......................................................+++ e is 65537 (0x10001) Signature ok subject=/C=IN/ST=FI/L=NI/O=ORG/OU=UNIT/CN=infini.cloud Getting CA Private Key DNS:infini.cloud, DNS:*.infini.cloud ➜ ls config ca.crt easysearch.yml instance.key log4j2.properties ca.key easysearch.yml.example jvm.options security easysearch.keystore instance.crt jvm.options.d X.509 PEM 证书和 PKCS#8 密钥 #  下面是 Easysearch 用于配置基于 PEM 证书和私钥的详细参数说明。\n传输层 TLS #     名称 说明     security.ssl.transport.key_file 证书密钥文件 （PKCS #8） 的路径（PEM 格式），该文件必须位于 config 目录下，使用相对路径指定。必填。   security.ssl.transport.key_secret 密钥文件的密码。如果密钥没有密码，请忽略此设置。选填。   security.ssl.transport.cert_file 证书公钥文件路径（PEM 格式），必须位于 config 目录下，使用相对路径指定。必填。   security.ssl.transport.ca_file 根证书（CA）的路径（PEM 格式），必须位于 config 目录下，使用相对路径指定。必填。    HTTP 层 TLS #     名称 说明     security.ssl.http.key_file 证书密钥文件 （PKCS #8） 的路径（PEM 格式），该文件必须位于 config 目录下，使用相对路径指定。必填。   security.ssl.http.key_secret 密钥文件的密码。如果密钥没有密码，请忽略此设置。选填。   security.ssl.http.cert_file 证书公钥文件路径（PEM 格式），必须位于 config 目录下，使用相对路径指定。必填。   security.ssl.http.ca_file 根证书（CA）的路径（PEM 格式），必须位于 config 目录下，使用相对路径指定。必填。    配置节点证书 #  安全模块需要识别集群节点间的请求（即节点之间的通信）。配置节点证书的最简单方法是在 easysearch.yml 中列出这些证书的可分辨名称 (DN)。所有 DN 都必须包含在所有节点上的 easysearch.yml 中。安全模块支持通配符和正则表达式：\nsecurity.nodes_dn: - \u0026#34;CN=node.other.com,OU=SSL,O=Test,L=Test,C=DE\u0026#34; - \u0026#34;CN=*.example.com,OU=SSL,O=Test,L=Test,C=DE\u0026#34; - \u0026#34;CN=elk-devcluster*\u0026#34; - \u0026#34;/CN=.*regex/\u0026#34; 如果您的节点证书在 SAN 部分中具有 OID 标识符，则可以省略此配置。\n配置管理证书 #  管理员证书是具有执行管理任务的提升权限的常规客户端证书。您需要管理员证书才能使用 HTTP API 更改安全配置。管理员证书在 easysearch.yml 中通过声明其 DN 进行配置：\nsecurity.authcz.admin_dn: - CN=admin,OU=SSL,O=Test,L=Test,C=DE 出于安全原因，您不能在此处使用通配符或正则表达式。\n主机名验证和 DNS 查找 #  除了根据根 CA 和/或中间 CA 验证 TLS 证书外，安全插件还可以对传输层应用其他检查。\n禁用 skip_domain_verify 后，安全模块将不会验证通信伙伴的主机名是否与证书中的主机名匹配。\n主机名取自证书的 subject 或 SAN 条目。例如，如果节点的主机名为 node-0.example.com ，则 TLS 证书中的主机名也必须设置为 node-0.example.com。否则，将引发错误：\n[ERROR][c.a.o.s.s.t.SecuritySSLNettyTransport] [WX6omJY] SSL Problem No name matching \u0026lt;hostname\u0026gt; found [ERROR][c.a.o.s.s.t.SecuritySSLNettyTransport] [WX6omJY] SSL Problem Received fatal alert: certificate_unknown 此外，启用 resolve_hostnames 后，安全模块会根据您的 DNS 解析（经过验证的）主机名。如果主机名未解析，则会引发错误：\n   名称 描述     security.ssl.transport.skip_domain_verify 是否跳过验证传输层上的主机名。默认为 true。   security.ssl.transport.resolve_hostname 是否在传输层上根据 DNS 解析主机名。默认为 true。仅在同时启用主机名验证时才有效。    客户端认证 #  启用 TLS 客户端身份验证后，HTTP 客户端可以将 TLS 证书与 HTTP 请求一起发送，以向安全模块提供身份信息。 TLS 客户端认证主要有 3 种使用场景：\n 在使用 HTTP 管理 API 时提供管理员证书。 根据客户端证书配置角色和权限。 为 Kibana、Logstash 或 Beats 等工具提供身份信息。  TLS 客户端认证有三种模式：\n NONE: 安全模块不接受 TLS 客户端证书。如果发送了一个，则将其丢弃。 OPTIONAL: 安全模块接受 TLS 客户端证书（如果已发送），但不需要它们。 REQUIRE: 安全模块仅在发送有效的客户端 TLS 证书时接受 HTTP 请求。  对于 HTTP 管理 API，客户端身份验证模式至少必须是 OPTIONAL。\n可以使用以下设置配置客户端身份验证模式：\n   名称 说明     security.ssl.http.clientauth_mode 要使用的 TLS 客户端身份验证模式。可以是以下之一：NONE 、 OPTIONAL (默认) 和 REQUIRE。可选参数。    样例 #  curl -k --cert config/instance.crt --key config/instance.key -XDELETE 'https://localhost:9200/.infini-*/' -u admin:xxxxxxxxxxxx 加密算法和协议 #  您可以限制 HTTP 层允许的加密算法和 TLS 协议。例如，您只能允许强密码，并将 TLS 版本限制为最新版本。\n如果未启用此设置，密码和 TLS 版本将在浏览器和安全插件之间自动协商，这在某些情况下会导致使用较弱的密码套件。您可以使用以下设置配置密码和协议。\n   名称 说明     security.ssl.http.enabled_ciphers 数组，HTTP 层启用的 TLS 密码套件。仅支持 Java 格式。   security.ssl.http.enabled_protocols 数组，HTTP 层启用的 TLS 协议。仅支持 Java 格式。   security.ssl.transport.enabled_ciphers 数组，为传输层启用了 TLS 密码套件。仅支持 Java 格式。   security.ssl.transport.enabled_protocols 数组，为传输层启用了 TLS 协议。仅支持 Java 格式。    样例 #  security.ssl.http.enabled_ciphers: - \u0026#34;TLS_AES_128_GCM_SHA256\u0026#34; - \u0026#34;TLS_AES_256_GCM_SHA384\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384\u0026#34; - \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\u0026#34; security.ssl.http.enabled_protocols: - \u0026#34;TLSv1.1\u0026#34; - \u0026#34;TLSv1.2\u0026#34; - \u0026#34;TLSv1.3\u0026#34; 安全模块默认禁用了不安全的 TLSv1 协议。如果您需要使用 TLSv1 并接受潜在的风险，您仍然可以启用它：\nsecurity.ssl.http.enabled_protocols: - \u0026#34;TLSv1\u0026#34; - \u0026#34;TLSv1.1\u0026#34; - \u0026#34;TLSv1.2\u0026#34; 配置 Kibana #  如果 Easysearch 开启了 TLS，Kibana 也需要进行相应的调整，您有两个选项：禁用 SSL 验证或添加 CA 根证书到 Kibana 配置。\n 直接禁用 SSL 校验：  easysearch.ssl.verificationMode: none  添加 CA 证书信息：  根据您在 kibana.yml 中的设置，将 ca.crt 添加到您的 Kibana 节点。\neasysearch.ssl.certificateAuthorities: [\u0026#34;/usr/share/kibana/config/ca.crt\u0026#34;] easysearch.ssl.verificationMode: full ","subcategory":null,"summary":"","tags":null,"title":"证书配置","url":"/easysearch/v1.15.0/docs/references/security/configuration/tls/"},{"category":null,"content":"后端配置 #  配置安全模块的第一步是确定如何验证用户身份。尽管 Easysearch 本身可以充当一个内部用户数据库，但许多人更喜欢集成企业现有的身份认证体系，例如 LDAP 服务器，或两者组合。设置身份验证和授权服务端的主要配置文件位于 config/security/config.yml。它定义了安全模块如何检索用户凭据、如何验证这些凭据以及如何从后端系统获取其他角色（可选）。\nconfig.yml 主要包含三大部分：\nsecurity: dynamic: http: ... authc: ... authz: ... HTTP #  配置 http 具有以下格式：\nanonymous_auth_enabled: \u0026lt;true|false\u0026gt; 可以选择是否开启匿名访问，如果禁用匿名身份验证，则至少在 authc里面提供一个认证后端，否则安全模块将不予初始化，默认为 false。\n认证 #  认证配置 authc 具有以下格式：\n\u0026lt;name\u0026gt;: http_enabled: \u0026lt;true|false\u0026gt; transport_enabled: \u0026lt;true|false\u0026gt; order: \u0026lt;integer\u0026gt; http_authenticator: ... authentication_backend: ... 配置 authc 里面的每一项被称为 身份验证域。它指定来在何处获取用户凭据以及应针对哪个后端对它们进行身份验证。\n您可以使用多个身份验证域。每个身份验证域都有一个名称（例如，basic_auth_internal）、enabled 开关和排序参数 order。该顺序使将身份验证域链接在一起成为可能。安全模块按您提供的顺序依次使用它们。如果用户成功通过一个域进行了身份验证，安全模块将跳过剩余的验证域。\nhttp_authenticator 指定要在 HTTP 层上使用的身份验证方法。\n以下是在 HTTP 层上定义身份验证器的语法：\nhttp_authenticator: type: \u0026lt;type\u0026gt; challenge: \u0026lt;true|false\u0026gt; config: ... 参数 type 支持以下几种类型:\n basic: HTTP 基本身份验证。无需其他配置。 clientcert: 通过客户端 TLS 证书进行身份验证。此证书必须受节点信任库中的根 CA 之一的信任。  设置 HTTP 身份验证器后，必须指定要针对哪个后端系统对用户进行身份验证：\nauthentication_backend: type: \u0026lt;type\u0026gt; config: ... 参数 type 支持以下几种类型:\n noop: 无需进一步的身份验证。设置 noop 如果 HTTP 身份验证器已经完全对用户进行身份验证，例如 JWT、Kerberos 或客户端证书身份验证。 internal: 使用本地配置 user.yml 里面定义的用户和角色来进行认证。  授权 #  对用户进行身份验证后，安全模块可以选择从后端权限系统收集用户的其他角色。\n授权配置 authz 具有以下格式：\nauthz: \u0026lt;name\u0026gt;: http_enabled: \u0026lt;true|false\u0026gt; transport_enabled: \u0026lt;true|false\u0026gt; authorization_backend: type: \u0026lt;type\u0026gt; config: ... 您可以在本节中定义多个条目，方法与定义身份验证条目的方式相同。在这种情况下，执行顺序不相关，因此没有 order 字段。\n参数 type 支持以下几种类型:\n noop: 即跳过这一步操作。  ","subcategory":null,"summary":"","tags":null,"title":"后端配置","url":"/easysearch/v1.15.0/docs/references/security/configuration/backend/"},{"category":null,"content":"本地配置 #  通过安全模块的本地 YAML 配置文件可以方便的管理默认的内置用户或 隐藏的保留资源，例如 admin 管理员用户。不过通过 INFINI Console 或者 REST API 来创建其他用户、角色、映射、操作组和租户可能更容易。\nuser.yml #  此文件包含您要添加到内部用户数据库的默认初始用户。\n配置文件里面的密码不能是明文，必须使用 Hash 之后的密码，通过命令 ./bin/hash_password.sh -p \u0026lt;new-password\u0026gt; 可以生成一个密码哈希。\n_meta: type: \u0026#34;user\u0026#34; config_version: 2 admin: hash: \u0026quot;$2y$12$ZXx5R8NfuW2TYPOdGNY7a.43WKKBMCtN9aJywYWjAz9i11w7SrkqG\u0026quot; reserved: true external_roles: - \u0026quot;admin\u0026quot; description: \u0026quot;Default admin user\u0026quot;\nreadonly: hash: \u0026quot;$2y$12$d9I16.5qpYhhsbiGN4zqdeA4k6BeMl/yEKRvTo3gzxFp8UC57EgJ.\u0026quot; reserved: false external_roles: - \u0026quot;readall\u0026quot; description: \u0026quot;Default readonly user\u0026quot; role.yml #\n 此文件包含要添加到内置数据库的默认初始角色。除了一些元数据之外，这个文件默认是是空的，因为系统已经了内置了若干角色，可以根据需要进行角色扩展。\ncomplex-role: reserved: false hidden: false cluster: - \u0026#34;read\u0026#34; - \u0026#34;cluster:monitor/nodes/stats\u0026#34; - \u0026#34;cluster:monitor/task/get\u0026#34; indices: - names: - \u0026#34;kibana_sample_data_*\u0026#34; query: \u0026#39;{\u0026#34;match\u0026#34;: {\u0026#34;FlightDelay\u0026#34;: true}}\u0026#39; field_security: - \u0026#34;~FlightNum\u0026#34; field_mask: - \u0026#34;Carrier\u0026#34; privileges: - \u0026#34;read\u0026#34; static: false _meta: type: \u0026#34;roles\u0026#34; config_version: 2 role_mapping.yml #  此文件主要设置用户角色的映射关系。\nmanage_snapshots: reserved: true hidden: false external_roles: - \u0026#34;snapshotrestore\u0026#34; hosts: [] users: [] privilege.yml #  此文件包含您要添加到安全模块的默认权限集合。\n除了一些元数据之外，该文件默认为空，因为安全模块已经内置不少了权限集合。这些集合基本上涵盖了常用的场景，这里的配置根据需要自行扩展。\nmy-action-group: reserved: false hidden: false privileges: - \u0026#34;indices:data/write/index*\u0026#34; - \u0026#34;indices:data/write/update*\u0026#34; - \u0026#34;indices:admin/mapping/put\u0026#34; - \u0026#34;indices:data/write/bulk*\u0026#34; - \u0026#34;read\u0026#34; - \u0026#34;write\u0026#34; static: false _meta: type: \u0026#34;privilege\u0026#34; config_version: 2 nodes_dn.yml #  _meta: type: \u0026#34;nodesdn\u0026#34; config_version: 2 # Define nodesdn mapping name and corresponding values # cluster1: # nodes_dn: # - CN=*.example.com 注意 #  任何对权限配置文件的修改: 如 user.yml,role.yml,role_mapping.yml 修改后必须以管理员身份删除.security 索引，然后重启服务，才能生效。 删除命令： curl -XDELETE -k \u0026ndash;cert admin.crt \u0026ndash;key admin.key \u0026lsquo;https://localhost:9200/.security\u0026rsquo;\n","subcategory":null,"summary":"","tags":null,"title":"本地配置","url":"/easysearch/v1.15.0/docs/references/security/configuration/yaml/"},{"category":null,"content":"部署 Easysearch Operator #  Easysearch Operator 只能在 k8s 环境下部署安装，请准备好一套 k8s 环境\n部署前准备 #   k8s 环境\n要求Kubernetes 1.9以上版本，自 1.9 版本以后，StatefulSet成为了在Kubernetes中管理有状态应用的标准方式。 StorageClass\nStorageClass 允许集群管理员定义多种存储方案，如快速的 SSD、标准的硬盘，或者其他的存储系统。无需手动预先创建存储资源，用户只需要在 PersistentVolumeClaim (PVC) 中指定需要的 StorageClass，存储资源就可以根据需求动态地创建。 ServiceAccount\n创建一个 ServiceAccount 用于 Easysearch Operator 获取和操作 k8s 资源 apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/name: serviceaccount app.kubernetes.io/instance: controller-manager-sa app.kubernetes.io/component: rbac app.kubernetes.io/created-by: k8s-operator app.kubernetes.io/part-of: k8s-operator app.kubernetes.io/managed-by: kustomize name: controller-manager # ServiceAccount 的名字是 controller-manager namespace: default  ClusterRole\n创建 ClusterRole，用于定义访问 k8s 集群的角色权限  展开查看完整代码 ...  apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: manager-role namespace: default rules: - apiGroups: - apps resources: - deployments verbs: - create - delete - get - list - patch - update - watch - apiGroups: - apps resources: - statefulsets verbs: - create - delete - get - list - patch - update - watch - apiGroups: - batch resources: - jobs verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - update - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - secrets verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - services verbs: - create - delete - get - list - patch - update - watch - apiGroups: - infinilabs.infinilabs.com resources: - events verbs: - create - patch - apiGroups: - infinilabs.infinilabs.com resources: - searchclusters verbs: - create - delete - get - list - patch - update - watch - apiGroups: - infinilabs.infinilabs.com resources: - searchclusters/finalizers verbs: - update - apiGroups: - infinilabs.infinilabs.com resources: - searchclusters/status verbs: - get - patch - update - apiGroups: - policy resources: - poddisruptionbudgets verbs: - create - delete - get - list - patch - update - watch - apiGroups: - monitoring.coreos.com resources: - servicemonitors verbs: - create - delete - get - list - patch - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - persistentvolumeclaims verbs: - create - delete - get - list - patch - update - watch  ClusterRoleBinding\n创建 ClusterRoleBinding，将 service account（controller-manager） 和 role （manager-role）绑定起来 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/name: clusterrolebinding app.kubernetes.io/instance: manager-rolebinding app.kubernetes.io/component: rbac app.kubernetes.io/created-by: k8s-operator app.kubernetes.io/part-of: k8s-operator app.kubernetes.io/managed-by: kustomize name: manager-rolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: manager-role subjects: - kind: ServiceAccount name: controller-manager namespace: default   \u0026lt;/div\u0026gt;     cert-manager\n部署 cert-manager 来管理 Easysearch 整个集群的证书  至此，准备工作完毕，以上的流程都是一劳永逸的，在第一次部署 Operator 的时候才需要，后续不再需要重新部署。\n部署 Easysearch Operator #  下载镜像 #  Easysearch Operator 的镜像发布在 Docker 的官方仓库，地址如下：\nhttps://hub.docker.com/r/infinilabs/operator\n使用下面的命令即可获取最新的容器镜像：\ndocker pull infinilabs/operator:latest 验证镜像 #  将镜像下载到本地之后，可以看到 Easysearch Operator 容器镜像非常小，只有大概 60MB，所以下载的速度应该是非常快的。\n➜ docker images REPOSITORY TAG IMAGE ID CREATED SIZE infinilabs/operator latest 5da94b93e7eb 8 minutes ago 61.2MB 部署 Operator #  在已有的 k8s 环境中部署 Operator，Operator 采用 deployment 方式部署，编辑对应的 yaml 文件，并 apply 即可。\nkind: Deployment apiVersion: apps/v1 metadata: name: k8s-operator namespace: default labels: app: k8s-operator spec: replicas: 1 selector: matchLabels: app: k8s-operator template: metadata: labels: app: k8s-operator spec: containers: - name: k8s-operator image: \u0026#39;infinilabs/operator:latest\u0026#39; imagePullPolicy: IfNotPresent restartPolicy: Always serviceAccount: controller-manager # 指定账号 查看部署情况 #  ik get deployment NAME READY UP-TO-DATE AVAILABLE AGE k8s-operator 1/1 1 1 2m6s 至此，Easysearch Operator 已经部署完成。\n","subcategory":null,"summary":"","tags":null,"title":"部署 Operator","url":"/easysearch/v1.15.0/docs/getting-started/install/operator/deploy_operator/"}]