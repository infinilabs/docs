<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>分词器 on INFINI Easysearch</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/</link><description>Recent content in 分词器 on INFINI Easysearch</description><generator>Hugo -- gohugo.io</generator><atom:link href="/easysearch/v1.15.0/docs/references/text-analysis/analyzers/index.xml" rel="self" type="application/rss+xml"/><item><title>IK 分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/ik-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/ik-analyzer/</guid><description>IK 分词器 # IK 分词器是一款专为处理中文文本设计的分词器，高效且智能。支持 ik_smart 和 ik_max_word 两种分词模式。
IK 分词器安装 # IK 分词插件安装命令如下：
bin/easysearch-plugin install analysis-ik 同时也需要安装 ingest-common 插件：
bin/easysearch-plugin install ingest-common 如果觉得比较麻烦，也可以直接使用 Easysearch 的 bundle 包安装部署。
使用样例 # 下面的命令样例展示了 IK 的使用方式。
# 1.创建索引 PUT index_ik # 2.创建映射关系 POST index_ik/_mapping { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;ik_max_word&amp;quot;, &amp;quot;search_analyzer&amp;quot;: &amp;quot;ik_smart&amp;quot; } } } # 3.写入文档 POST index_ik/_create/1 {&amp;quot;content&amp;quot;:&amp;quot;美国留给伊拉克的是个烂摊子吗&amp;quot;} POST index_ik/_create/2 {&amp;quot;content&amp;quot;:&amp;quot;公安部：各地校车将享最高路权&amp;quot;} POST index_ik/_create/3 {&amp;quot;content&amp;quot;:&amp;quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&amp;quot;} POST index_ik/_create/4 {&amp;quot;content&amp;quot;:&amp;quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&amp;quot;} # 4.</description></item><item><title>停用词分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/stop-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/stop-analyzer/</guid><description>停用词分词器 # 停用词（stop）分词器会在文本中移除预定义的停用词。该分词器由一个小写词元生成器和一个停用词词元过滤器组成。
参数说明 # 你可以使用以下参数来配置一个停用词分词器。
参数 必填/可选 数据类型 描述 stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_stop_index 并使用停词器分词器的索引：
PUT /my_stop_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;stop&amp;quot; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于停用词分词器：
PUT /my_custom_stop_analyzer_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_stop_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;lowercase&amp;quot;, &amp;quot;filter&amp;quot;: [ &amp;quot;stop&amp;quot; ] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_stop_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>关键词分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/keyword-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/keyword-analyzer/</guid><description>关键词分词器 # 关键词（keyword）分词器根本不会对文本进行分词。相反，它将整个输入视为单个词元，不会将其拆分成单个的词项。关键词分词器常用于包含电子邮件地址、网址或产品 ID 的字段，以及其他不需要进行分词的情况。
参考样例 # 以下命令创建一个名为 my_keyword_index 使用关键词分词器的索引：
PUT /my_keyword_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;keyword&amp;quot; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于关键词分词器：
PUT /my_custom_keyword_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_keyword_analyzer&amp;quot;: { &amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot; } } } } } 产生的词元 # 以下请求来检查使用该分词器生成的词元：
POST /my_custom_keyword_index/_analyze { &amp;quot;analyzer&amp;quot;: &amp;quot;my_keyword_analyzer&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Just one token&amp;quot; } 返回内容包含产生的词元
{ &amp;quot;tokens&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;Just one token&amp;quot;, &amp;quot;start_offset&amp;quot;: 0, &amp;quot;end_offset&amp;quot;: 14, &amp;quot;type&amp;quot;: &amp;quot;word&amp;quot;, &amp;quot;position&amp;quot;: 0 } ] }</description></item><item><title>创建一个自定义分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/creating-a-custom-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/creating-a-custom-analyzer/</guid><description>创建一个自定义分词器 # 要创建一个自定义分词器，需要指定以下组成内容：
字符过滤器（零个或多个） 词元生成器（一个） 词元过滤器（零个或多个） 相关配置 # 以下参数可用于配置自定义分词器。
参数 必填/可选 描述 type 可选 分词器类型。默认值为custom。你也可以给这个参数指定一个预构建的分词器。 tokenizer 必填 每个分词器必须要有一个词元生成器。 char_filter 可选 要包含在分词器中的字符过滤器列表。 filter 可选 要包含在分词器中的分词过滤器列表。 position_increment_gap 可选 在对包含多个词项内容的文本字段建立索引时，应用于各词项之间的额外间隔。有关更多信息，请参阅位置间隔增量。默认值为 100。 参考样例 # 以下示例展示了各种自定义分词器的配置。
自定义分词器用于去除 HTML 格式标签 # 以下示例中的分词器在分词之前会从文本中移除 HTML 格式标签：
PUT simple_html_strip_analyzer_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;html_strip_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;char_filter&amp;quot;: [&amp;quot;html_strip&amp;quot;], &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;] } } } } } 使用以下请求来查看使用该分词器生成的词元：</description></item><item><title>匹配模式分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/pattern-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/pattern-analyzer/</guid><description>匹配模式分词器 # 匹配模式（pattern）分词器允许你定义一个自定义分词器，该分词器使用正则表达式（regex）将输入文本分割成词元。它还提供了应用正则表达式、将词元转换为小写以及过滤掉停用词的功能。
参数说明 # 匹配模式分词器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 pattern 可选 字符串 用于对输入内容进行分词匹配的 Java 正则表达式。默认值是 \W+。 flags 可选 字符串 以竖线分隔的 Java 正则表达式的字符串标志，这些标志会修改正则表达式的行为。 lowercase 可选 布尔值 是否将词元转换为小写。默认值为 true。 stopwords 可选 字符串或字符串列表 个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_pattern_index 并使用匹配模式分词器的索引：
PUT /my_pattern_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_pattern_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;pattern&amp;quot;, &amp;quot;pattern&amp;quot;: &amp;quot;\\W+&amp;quot;, &amp;quot;lowercase&amp;quot;: true, &amp;quot;stopwords&amp;quot;: [&amp;quot;and&amp;quot;, &amp;quot;is&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_pattern_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>各类语种分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/language-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/language-analyzer/</guid><description>各类语种分词器 # Easysearch 在分词器选项中支持以下语种：阿拉伯语（arabic）、亚美尼亚语（armenian）、巴斯克语（basque）、孟加拉语（bengali）、巴西葡萄牙语（brazilian）、保加利亚语（bulgarian）、加泰罗尼亚语（catalan）、捷克语（czech）、丹麦语（danish）、荷兰语（dutch）、英语（english）、爱沙尼亚语（estonian）、芬兰语（finnish）、法语（french）、加利西亚语（galician）、德语（german）、希腊语（greek）、印地语（hindi）、匈牙利语（hungarian）、印度尼西亚语（indonesian）、爱尔兰语（irish）、意大利语（italian）、拉脱维亚语（latvian）、立陶宛语（lithuanian）、挪威语（norwegian）、波斯语（persian）、葡萄牙语（portuguese）、罗马尼亚语（romanian）、俄语（russian）、索拉尼语（sorani）、西班牙语（spanish）、瑞典语（swedish）、土耳其语（turkish）以及泰语（thai）。
当你映射索引时要使用该分词器，需在查询中指定相应的值。例如，要使用法语语种分词器来映射您的索引，为分词器字段指定值 french：
&amp;quot;analyzer&amp;quot;: &amp;quot;french&amp;quot; 参考样例 # 以下请求指定了一个名为 my-index 的索引，其中 content 字段被配置为多字段，并且一个名为 french 的子字段配置了french语种分词器
PUT my-index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;fields&amp;quot;: { &amp;quot;french&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;french&amp;quot; } } } } } } 也可以使用以下查询为整个索引配置默认的french分词器：
PUT my-index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;default&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;french&amp;quot; } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;content&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;title&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; }, &amp;quot;description&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot; } } } }</description></item><item><title>拼音分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/pinyin-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/pinyin-analyzer/</guid><description>拼音分词器 # 总体介绍 # pinyin-analyzer 拼音分词器能够在索引阶段将 中文字符实时转写为拼音，并在检索阶段对拼音、汉字及混输查询进行统一匹配。借助该插件，你可以轻松实现：
支持 全拼、首字母、全拼拼接 等多种检索方式； 保留非中文字符，实现「中英混输」搜索； 借助 token filter 在分词链中灵活组合不同策略； 在联想输入、排序、聚合等场景下提升中文用户体验。 适用于人名、地名、品牌、歌曲、商品等多种中文文本检索场景。
参数说明 # 下表整理了 pinyin 分词器 / 过滤器支持的全部可选参数及默认值。
参数 说明 默认值 keep_first_letter 仅保留每个汉字的拼音首字母。例如 刘德华 → ldh true keep_separate_first_letter 将首字母分开存储，提高命中率。例如 刘德华 → l, d, h，注意：这可能会由于词频增加而增加查询模糊性。 false limit_first_letter_length 首字母结果最长长度 16 keep_full_pinyin 保留每个汉字的全拼。如 刘德华 → liu, de, hua true keep_joined_full_pinyin 将全拼连接在一起。如 刘德华 → liudehua false keep_none_chinese 保留非中文字符（数字/字母等） true keep_none_chinese_together 将连续非中文字符作为整体保留。例如， DJ 音乐家 变成 DJ 、 yin 、 yue 、 jia 。当设置为 false 时， DJ 音乐家 变成 D 、 J 、 yin 、 yue 、 jia 。注意： keep_none_chinese 应该先启用。 true keep_none_chinese_in_first_letter 在首字母结果中保留非中文字。例如， 刘德华 AT2016 变成 ldhat2016 。符 true keep_none_chinese_in_joined_full_pinyin 在拼接全部拼音时保留非中文字。例如， 刘德华 2016 变成 liudehua2016 。符 false none_chinese_pinyin_tokenize 将非中文字符中可能的拼音继续拆分例如， liudehuaalibaba13zhuanghan 变成 liu 、 de 、 hua 、 a 、 li 、 ba 、 ba 、 13 、 zhuang 、 han 。注意： keep_none_chinese 和 keep_none_chinese_together 应该先启用。 true keep_original 同时保留原始文本 false lowercase 对非中文字符强制小写 true trim_whitespace 去除首尾空格 true remove_duplicated_term 开启时，移除重复术语以节省索引空间。例如， de 的 变为 de 。注意：位置相关的查询可能会受影响。积 false ignore_pinyin_offset 忽略 offset 以允许重叠 token。在 6.</description></item><item><title>指纹分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/fingerprint-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/fingerprint-analyzer/</guid><description>指纹分词器 # 指纹（fingerprint）分词器会创建一个文本指纹。该分词器对从输入生成的词项（词元）进行排序和去重，然后使用一个分隔符将它们连接起来。它通常用于数据去重，对于包含相同单词的相似输入，无论单词顺序如何，它都可以产生相同的输出结果。
指纹分词器由以下组件组成：
标准分词生成器 词元小写化过滤器 ASCII 词元过滤器 停用词词元过滤器 指纹词元过滤器 参数说明 # 指纹分词器可以使用以下参数进行配置。
参数 必填/可选 数据类型 描述 separator 可选 字符串 在对词项进行词元生成、排序和去重后，用于连接这些词项的字符。默认值为一个空格（ ）。 max_output_size 可选 整数 定义输出词元的最大大小。如果连接后的指纹超过此大小，将被截断。默认值为 255。 stopwords 可选 字符串或字符串列表 自定义的停用词列表。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 参考样例 # 以下命令创建一个名为 my_custom_fingerprint_index 带有指纹分词器的索引：
PUT /my_custom_fingerprint_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_fingerprint_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;fingerprint&amp;quot;, &amp;quot;separator&amp;quot;: &amp;quot;-&amp;quot;, &amp;quot;max_output_size&amp;quot;: 50, &amp;quot;stopwords&amp;quot;: [&amp;quot;to&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;over&amp;quot;, &amp;quot;and&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_fingerprint_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查使用该分词器生成的词元：</description></item><item><title>搜索分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/search-analyzers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/search-analyzers/</guid><description>搜索分词器 # 搜索分词器是在查询时指定的，当你对text字段进行全文匹配查询时，它会被用于分析被查询字符串。
搜索分词器的生效流程 # 在查询时确定对查询字符串使用哪种分词器，Easysearch 会按以下顺序检查以下参数：
查询的 analyzer 参数 字段的 search_analyzer 映射参数 索引设置 analysis.analyzer.default_search 字段的 analyzer 映射参数 standard 分词器（默认分词器） 在大多数情况下，没有必要指定与索引分词器不同的搜索分词器，而且这样做可能会对搜索结果的相关性产生负面影响，或者导致出现意想不到的搜索结果。
为查询内容指定搜索分词器 # 在查询时，你可以在 analyzer 字段中指定想要使用的分词器：
GET shakespeare/_search { &amp;quot;query&amp;quot;: { &amp;quot;match&amp;quot;: { &amp;quot;text_entry&amp;quot;: { &amp;quot;query&amp;quot;: &amp;quot;speak the truth&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;english&amp;quot; } } } } 为字段指定搜索分词器 # 在创建索引映射时，你可以为每个文本字段提供 search_analyzer 参数。在提供 search_analyzer 时，你还必须提供 analyzer 参数，该参数指定在索引创建时的写入索引分词器内应用。
例如，以下请求将 simple 分词器指定为 text_entry 字段的索引分词器，将 whitespace 分词器指定为该字段的搜索分词器：
PUT testindex { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text_entry&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;simple&amp;quot;, &amp;quot;search_analyzer&amp;quot;: &amp;quot;whitespace&amp;quot; } } } } 为索引指定默认的搜索分词器 # 如果你希望在搜索时使用同一个分词器来解析所有的查询，你可以在 analysis.</description></item><item><title>标准分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/standard-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/standard-analyzer/</guid><description>标准分词器 # 标准（standard）分词器是在未指定其他分词器时默认使用的分词器。它旨在为通用文本处理提供一种基础且高效的方法。
该分词器由以下词元生成器和词元过滤器组成：
标准（standard）词元生成器：去除大部分标点符号，并依据空格和其他常见分隔符对文本进行分割。 小写（lowercase）词元过滤器：将所有词元转换为小写，以确保匹配时不区分大小写。 停用词（stop）词元过滤器：从分词后的输出中移除常见的停用词，例如 “the”、“is” 和 “and”。 参考样例 # 以下命令创建一个名为 my_standard_index 并使用标准分词器的索引：
PUT /my_standard_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot; } } } } 参数说明 # 你可以使用以下参数来配置标准分词器。
参数 必填/可选 数据类型 描述 max_token_length 可选 整数 设置生成的词元的最大长度。如果超过此长度，词元将按照 max_token_length 中配置的长度拆分为多个词元。默认值为 255。 stopwords 可选 字符串或字符串列表 一个包含自定义停用词列表（如 _english）的字符串，或者一个自定义停用词列表的数组。默认值为 _none_。 stopwords_path 可选 字符串 包含停用词列表的文件的路径（相对于配置目录的绝对路径或相对路径）。 配置自定义分词器 # 以下命令配置一个索引，该索引带有一个等同于标准分词器的自定义分词器：</description></item><item><title>空格分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/whitespace-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/whitespace-analyzer/</guid><description>空格分词器 # 空格（whitespace）分词器仅基于空白字符（例如，空格和制表符）将文本拆分为词元。比如转换为小写形式或移除停用词这样的转换操作，它都不会应用，因此文本的原始大小写形式会被保留，并且标点符号也会作为词元的一部分包含在内。
参考样例 # 以下命令创建一个名为 my_whitespace_index 并使用空格分词器的索引：
PUT /my_whitespace_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;whitespace&amp;quot; } } } } 配置自定义分词器 # 以下命令来配置一个自定义分词器的索引，该自定义分词器的作用等同于空格分词器：
PUT /my_custom_whitespace_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_whitespace_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;tokenizer&amp;quot;: &amp;quot;whitespace&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_whitespace_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>简单分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/simple-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/simple-analyzer/</guid><description>简单分词器 # 简单（simple）分词器是一种非常基础的分词器，它会将文本中的非字母字符串拆分成词项，并将这些词项转换为小写形式。与标准分词器不同的是，简单分词器将除字母字符之外的所有内容都视为分隔符，这意味着它不会把数字、标点符号或特殊字符识别为词元的一部分。
参考样例 # 以下命令创建一个名为 my_simple_index 并使用简单分词器的索引：
PUT /my_simple_index { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;simple&amp;quot; } } } } 配置自定义分词器 # 以下命令配置了一个索引，该索引带有一个自定义分词器，这个自定义分词器等同于添加了 html_strip 字符过滤器的简单分词器：
PUT /my_custom_simple_index { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;char_filter&amp;quot;: { &amp;quot;html_strip&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;html_strip&amp;quot; } }, &amp;quot;tokenizer&amp;quot;: { &amp;quot;my_lowercase_tokenizer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;lowercase&amp;quot; } }, &amp;quot;analyzer&amp;quot;: { &amp;quot;my_custom_simple_analyzer&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;, &amp;quot;char_filter&amp;quot;: [&amp;quot;html_strip&amp;quot;], &amp;quot;tokenizer&amp;quot;: &amp;quot;my_lowercase_tokenizer&amp;quot;, &amp;quot;filter&amp;quot;: [&amp;quot;lowercase&amp;quot;] } } } }, &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;my_field&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;my_custom_simple_analyzer&amp;quot; } } } } 产生的词元 # 以下请求用来检查分词器生成的词元：</description></item><item><title>简繁转换分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/stconvert-analyzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/stconvert-analyzer/</guid><description>简繁转换分词器 # stconvert-analyzer 简繁体转换分词器，可在索引与查询阶段将 简体中文 与 繁体中文 之间进行双向转换，解决两种文字体系混合检索的问题。
参数说明 # 参数 说明 默认值 convert_type 转换方向，可选：s2t（简 → 繁），t2s（繁 → 简） s2t keep_both 是否同时保留转换前后两种 token false delimiter 当 keep_both=true 时，两种 token 之间的分隔符 , 使用介绍 # 映射创建
PUT /stconvert/ { &amp;#34;settings&amp;#34; : { &amp;#34;analysis&amp;#34; : { &amp;#34;analyzer&amp;#34; : { &amp;#34;tsconvert&amp;#34; : { &amp;#34;tokenizer&amp;#34; : &amp;#34;tsconvert&amp;#34; } }, &amp;#34;tokenizer&amp;#34; : { &amp;#34;tsconvert&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;stconvert&amp;#34;, &amp;#34;delimiter&amp;#34; : &amp;#34;#&amp;#34;, &amp;#34;keep_both&amp;#34; : false, &amp;#34;convert_type&amp;#34; : &amp;#34;t2s&amp;#34; } }, &amp;#34;filter&amp;#34;: { &amp;#34;tsconvert&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;stconvert&amp;#34;, &amp;#34;delimiter&amp;#34; : &amp;#34;#&amp;#34;, &amp;#34;keep_both&amp;#34; : false, &amp;#34;convert_type&amp;#34; : &amp;#34;t2s&amp;#34; } }, &amp;#34;char_filter&amp;#34; : { &amp;#34;tsconvert&amp;#34; : { &amp;#34;type&amp;#34; : &amp;#34;stconvert&amp;#34;, &amp;#34;convert_type&amp;#34; : &amp;#34;t2s&amp;#34; } } } } } 分词测试</description></item><item><title>索引分词器</title><link>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/index-analyzers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/easysearch/v1.15.0/docs/references/text-analysis/analyzers/index-analyzers/</guid><description>索引分词器 # 索引分词器是在索引创建时指定的，在文档写入时对文本字段进行分词。
写入索引分词器的生效流程 # 为了确定在对文档建立索引时对某个字段使用哪种分词器，Easysearch 会按以下顺序检查以下参数：
字段的分词器映射参数，即analyzer analysis.analyzer.default 索引设置 standard标准分词器（默认分词器） 在指定索引分词器时，请记住，在大多数情况下，为索引中的每个文本字段指定一个分词器效果最佳。使用相同的分词器在文本字段（在建立索引时）和查询字符串（在查询时），可确保搜索所使用的词项与存储在索引中的词项相同。
为字段指定索引分词器 # 在创建索引映射时，可以为每个文本字段提供 analyzer（分词器）参数。例如，以下请求为 text_entry 字段指定了 simple 简单分词器：
PUT testindex { &amp;quot;mappings&amp;quot;: { &amp;quot;properties&amp;quot;: { &amp;quot;text_entry&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;analyzer&amp;quot;: &amp;quot;simple&amp;quot; } } } } 为索引指定默认索引分词器 # 如果想对索引中的所有文本字段使用相同的分词器，则可以在索引设置中按如下方式进行指定analysis.analyzer.default：
PUT testindex { &amp;quot;settings&amp;quot;: { &amp;quot;analysis&amp;quot;: { &amp;quot;analyzer&amp;quot;: { &amp;quot;default&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;simple&amp;quot; } } } } } 如果您未指定默认分词器，那么将使用standard标准分词器。</description></item></channel></rss>